[
  {
    "title": "The Byzantine Generals Problem",
    "doi": "https://doi.org/10.1145/357172.357176",
    "publication_date": "1982-07-01",
    "publication_year": 1982,
    "authors": "Leslie Lamport; Robert E. Shostak; Marshall C. Pease",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on The Byzantine Generals Problem Authors: Leslie Lamport Computer Science Laboratory, SRI International, 333 Ravenswood Avenue, Menlo Park, CA Computer Science Laboratory, SRI International, 333 Ravenswood Avenue, Menlo Park, CAView Profile , Robert Shostak Computer Science Laboratory, SRI International, 333 Ravenswood Avenue, Menlo Park, CA Computer Science Laboratory, SRI International, 333 Ravenswood Avenue, Menlo Park, CAView Profile , Marshall Pease Computer Science Laboratory, SRI International, 333 Ravenswood Avenue, Menlo Park, CA Computer Science Laboratory, SRI International, 333 Ravenswood Avenue, Menlo Park, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 3pp 382–401https://doi.org/10.1145/357172.357176Published:01 July 1982Publication History 3,984citation25,820DownloadsMetricsTotal Citations3,984Total Downloads25,820Last 12 Months3,557Last 6 weeks636 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 5865,
    "openalex_id": "https://openalex.org/W3137092842",
    "type": "article"
  },
  {
    "title": "Automatic verification of finite-state concurrent systems using temporal logic specifications",
    "doi": "https://doi.org/10.1145/5397.5399",
    "publication_date": "1986-04-01",
    "publication_year": 1986,
    "authors": "E. M. Clarke; E. Allen Emerson; A. Prasad Sistla",
    "corresponding_authors": "",
    "abstract": "We give an efficient procedure for verifying that a finite-state concurrent system meets a specification expressed in a (propositional, branching-time) temporal logic. Our algorithm has complexity linear in both the size of the specification and the size of the global state graph for the concurrent system. We also show how this approach can be adapted to handle fairness. We argue that our technique can provide a practical alternative to manual proof construction or use of a mechanical theorem prover for verifying many finite-state concurrent systems. Experimental results show that state machines with several hundred states can be checked in a matter of seconds.",
    "cited_by_count": 3600,
    "openalex_id": "https://openalex.org/W2117189826",
    "type": "article"
  },
  {
    "title": "Linearizability: a correctness condition for concurrent objects",
    "doi": "https://doi.org/10.1145/78969.78972",
    "publication_date": "1990-07-01",
    "publication_year": 1990,
    "authors": "Maurice Herlihy; Jeannette M. Wing",
    "corresponding_authors": "",
    "abstract": "A concurrent object is a data object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract data types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object's operations can be given by pre- and post-conditions. This paper defines linearizability, compares it to other correctness conditions, presents and demonstrates a method for proving the correctness of implementations, and shows how to reason about concurrent objects, given they are linearizable.",
    "cited_by_count": 3209,
    "openalex_id": "https://openalex.org/W2101939036",
    "type": "article"
  },
  {
    "title": "The program dependence graph and its use in optimization",
    "doi": "https://doi.org/10.1145/24039.24041",
    "publication_date": "1987-07-01",
    "publication_year": 1987,
    "authors": "Jeanne Ferrante; Karl J. Ottenstein; Joe Warren",
    "corresponding_authors": "",
    "abstract": "In this paper we present an intermediate program representation, called the program dependence graph ( PDG ), that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The PDG supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.",
    "cited_by_count": 2738,
    "openalex_id": "https://openalex.org/W2144344516",
    "type": "article"
  },
  {
    "title": "Virtual time",
    "doi": "https://doi.org/10.1145/3916.3988",
    "publication_date": "1985-07-01",
    "publication_year": 1985,
    "authors": "David Jefferson",
    "corresponding_authors": "David Jefferson",
    "abstract": "Virtual time is a new paradigm for organizing and synchronizing distributed systems which can be applied to such problems as distributed discrete event simulation and distributed database concurrency control. Virtual time provides a flexible abstraction of real time in much the same way that virtual memory provides an abstraction of real memory. It is implemented using the Time Warp mechanism, a synchronization protocol distinguished by its reliance on lookahead-rollback, and by its implementation of rollback via antimessages.",
    "cited_by_count": 2449,
    "openalex_id": "https://openalex.org/W2296636214",
    "type": "article"
  },
  {
    "title": "Generative communication in Linda",
    "doi": "https://doi.org/10.1145/2363.2433",
    "publication_date": "1985-01-02",
    "publication_year": 1985,
    "authors": "David Gelernter",
    "corresponding_authors": "David Gelernter",
    "abstract": "Generative communication is the basis of a new distributed programming langauge that is intended for systems programming in distributed settings generally and on integrated network computers in particular. It differs from previous interprocess communication models in specifying that messages be added in tuple-structured form to the computation environment, where they exist as named, independent entities until some process chooses to receive them. Generative communication results in a number of distinguishing properties in the new language, Linda, that is built around it. Linda is fully distributed in space and distributed in time; it allows distributed sharing, continuation passing, and structured naming. We discuss these properties and their implications, then give a series of examples. Linda presents novel implementation problems that we discuss in Part II. We are particularly concerned with implementation of the dynamic global name space that the generative communication model requires.",
    "cited_by_count": 2330,
    "openalex_id": "https://openalex.org/W2161307885",
    "type": "article"
  },
  {
    "title": "Efficiently computing static single assignment form and the control dependence graph",
    "doi": "https://doi.org/10.1145/115372.115320",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Ron K. Cytron; Jeanne Ferrante; Barry K. Rosen; Mark N. Wegman; F. Kenneth Zadeck",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Efficiently computing static single assignment form and the control dependence graph Authors: Ron Cytron IBM Research Division, Yorktown Heights, NY IBM Research Division, Yorktown Heights, NYView Profile , Jeanne Ferrante IBM Research Division, Yorktown Heights, NY IBM Research Division, Yorktown Heights, NYView Profile , Barry K. Rosen IBM Research Division, Yorktown Heights, NY IBM Research Division, Yorktown Heights, NYView Profile , Mark N. Wegman IBM Research Division, Yorktown Heights, NY IBM Research Division, Yorktown Heights, NYView Profile , F. Kenneth Zadeck Brown Univ., Providence, RI Brown Univ., Providence, RIView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 4Oct. 1991 pp 451–490https://doi.org/10.1145/115372.115320Online:01 October 1991Publication History 1,649citation12,158DownloadsMetricsTotal Citations1,649Total Downloads12,158Last 12 Months599Last 6 weeks81 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 2273,
    "openalex_id": "https://openalex.org/W1982205631",
    "type": "article"
  },
  {
    "title": "Wait-free synchronization",
    "doi": "https://doi.org/10.1145/114005.102808",
    "publication_date": "1991-01-01",
    "publication_year": 1991,
    "authors": "Maurice Herlihy",
    "corresponding_authors": "Maurice Herlihy",
    "abstract": "A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. First, we introduce a simple and general technique, based on reduction to a concensus protocol, for proving statements of the form, “there is no wait-free implementation of X by Y.” We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that atomic read/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: thay cannot be used to construct wait-free implementations of many simple and familiar data types. Moreover, classical synchronization primitives such as test&amp;set and fetch&amp;add , while more powerful than read and write , are also computationally weak, as are the standard message-passing primitives. Second, nevertheless, we show that there do exist simple universal objects from which one can construct a wait-free implementation of any sequential object.",
    "cited_by_count": 1874,
    "openalex_id": "https://openalex.org/W2085407655",
    "type": "article"
  },
  {
    "title": "The temporal logic of actions",
    "doi": "https://doi.org/10.1145/177492.177726",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Leslie Lamport",
    "corresponding_authors": "Leslie Lamport",
    "abstract": "The temporal logic of actions (TLA) is a logic for specifying and reasoning about concurrent systems. Systems and their properties are represented in the same logic, so the assertion that a system meets its specification and the assertion that one system implements another are both expressed by logical implication. TLA is very simple; its syntax and complete formal semantics are summarized in about a page. Yet, TLA is not just a logician's toy; it is extremely powerful, both in principle and in practice. This report introduces TLA and describes how it is used to specify and verify concurrent algorithms. The use of TLA to specify and reason about open systems will be described elsewhere.",
    "cited_by_count": 1838,
    "openalex_id": "https://openalex.org/W2015688007",
    "type": "article"
  },
  {
    "title": "Interprocedural slicing using dependence graphs",
    "doi": "https://doi.org/10.1145/77606.77608",
    "publication_date": "1990-01-03",
    "publication_year": 1990,
    "authors": "Susan Horwitz; Thomas Reps; David Binkley",
    "corresponding_authors": "",
    "abstract": "The notion of a program slice , originally introduced by Mark Weiser, is useful in program debugging, automatic parallelization, and program integration. A slice of a program is taken with respect to a program point p and a variable x ; the slice consists of all statements of the program that might affect the value of x at point p . This paper concerns the problem of interprocedural slicing—generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph , which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation. (It should be noted that our work concerns a somewhat restricted kind of slice: rather than permitting a program to b e sliced with respect to program point p and an arbitrary variable, a slice must be taken with respect to a variable that is defined or used at p .) The chief difficulty in interprocedural slicing is correctly accounting for the calling context of a called procedure. To handle this problem, system dependence graphs include some data dependence edges that represent transitive dependences due to the effects of procedure calls, in addition to the conventional direct-dependence edges. These edges are constructed with the aid of an auxiliary structure that represents calling and parameter-linkage relationships. This structure takes the form of an attribute grammar. The step of computing the required transitive-dependence edges is reduced to the construction of the subordinate characteristic graphs for the grammar's nonterminals.",
    "cited_by_count": 1501,
    "openalex_id": "https://openalex.org/W2092483417",
    "type": "article"
  },
  {
    "title": "Model checking and abstraction",
    "doi": "https://doi.org/10.1145/186025.186051",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Edmund M. Clarke; Orna Grümberg; David E. Long",
    "corresponding_authors": "",
    "abstract": "We describe a method for using abstraction to reduce the complexity of temporal-logic model checking. Using techniques similar to those involved in abstract interpretation, we construct an abstract model of a program without ever examining the corresponding unabstracted model. We show how this abstract model can be used to verify properties of the original program. We have implemented a system based on these techniques, and we demonstrate their practicality using a number of examples, including a program representing a pipelined ALU circuit with over 10 1300 states.",
    "cited_by_count": 1234,
    "openalex_id": "https://openalex.org/W2080593426",
    "type": "article"
  },
  {
    "title": "A Distributed Algorithm for Minimum-Weight Spanning Trees",
    "doi": "https://doi.org/10.1145/357195.357200",
    "publication_date": "1983-01-01",
    "publication_year": 1983,
    "authors": "Robert G. Gallager; P.A. Humblet; Philip M. Spira",
    "corresponding_authors": "",
    "abstract": "article Free AccessA Distributed Algorithm for Minimum-Weight Spanning Trees Authors: R. G. Gallager Room 35-206, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA Room 35-206, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MAView Profile , P. A. Humblet Room 35-203, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA Room 35-203, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MAView Profile , P. M. Spira Apple Computer Company, 10260 Bandley Drive, Cupertino, CA Apple Computer Company, 10260 Bandley Drive, Cupertino, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 1Jan. 1983 pp 66–77https://doi.org/10.1145/357195.357200Published:01 January 1983Publication History 825citation6,397DownloadsMetricsTotal Citations825Total Downloads6,397Last 12 Months555Last 6 weeks76 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 1172,
    "openalex_id": "https://openalex.org/W2077371116",
    "type": "article"
  },
  {
    "title": "A behavioral notion of subtyping",
    "doi": "https://doi.org/10.1145/197320.197383",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Barbara Liskov; Jeannette M. Wing",
    "corresponding_authors": "",
    "abstract": "The use of hierarchy is an important component of object-oriented design. Hierarchy allows the use of type families, in which higher level supertypes capture the behavior that all of their subtypes have in common. For this methodology to be effective, it is necessary to have a clear understanding of how subtypes and supertypes are related. This paper takes the position that the relationship should ensure that any property proved about supertype objects also holds for its subtype objects. It presents two ways of defining the subtype relation, each of which meets this criterion, and each of which is easy for programmers to use. The subtype relation is based on the specifications of the sub- and supertypes; the paper presents a way of specifying types that makes it convenient to define the subtype relation. The paper also discusses the ramifications of this notion of subtyping on the design of type families.",
    "cited_by_count": 1155,
    "openalex_id": "https://openalex.org/W1995008247",
    "type": "article"
  },
  {
    "title": "MULTILISP: a language for concurrent symbolic computation",
    "doi": "https://doi.org/10.1145/4472.4478",
    "publication_date": "1985-10-01",
    "publication_year": 1985,
    "authors": "Robert H. Halstead",
    "corresponding_authors": "Robert H. Halstead",
    "abstract": "Multilisp is a version of the Lisp dialect Scheme extended with constructs for parallel execution. Like Scheme, Multilisp is oriented toward symbolic computation. Unlike some parallel programming languages, Multilisp incorporates constructs for causing side effects and for explicitly introducing parallelism. The potential complexity of dealing with side effects in a parallel context is mitigated by the nature of the parallelism constructs and by support for abstract data types: a recommended Multilisp programming style is presented which, if followed, should lead to highly parallel, easily understandable programs. Multilisp is being implemented on the 32-processor Concert multiprocessor; however, it is ultimately intended for use on larger multiprocessors. The current implementation, called Concert Multilisp , is complete enough to run the Multilisp compiler itself and has been run on Concert prototypes including up to eight processors. Concert Multilisp uses novel techniques for task scheduling and garbage collection. The task scheduler helps control excessive resource utilization by means of an unfair scheduling policy; the garbage collector uses a multiprocessor algorithm based on the incremental garbage collector of Baker.",
    "cited_by_count": 1090,
    "openalex_id": "https://openalex.org/W1983587324",
    "type": "article"
  },
  {
    "title": "Featherweight Java",
    "doi": "https://doi.org/10.1145/503502.503505",
    "publication_date": "2001-05-01",
    "publication_year": 2001,
    "authors": "Atsushi Igarashi; Benjamin C. Pierce; Philip Wadler",
    "corresponding_authors": "",
    "abstract": "Several recent studies have introduced lightweight versions of Java: reduced languages in which complex features like threads and reflection are dropped to enable rigorous arguments about key properties such as type safety. We carry this process a step further, omitting almost all features of the full language (including interfaces and even assignment) to obtain a small calculus, Featherweight Java, for which rigorous proofs are not only possible but easy. Featherweight Java bears a similar relation to Java as the lambda-calculus does to languages such as ML and Haskell. It offers a similar computational \"feel,\" providing classes, methods, fields, inheritance, and dynamic typecasts with a semantics closely following Java's. A proof of type safety for Featherweight Java thus illustrates many of the interesting features of a safety proof for the full language, while remaining pleasingly compact. The minimal syntax, typing rules, and operational semantics of Featherweight Java make it a handy tool for studying the consequences of extensions and variations. As an illustration of its utility in this regard, we extend Featherweight Java with generic classes in the style of GJ (Bracha, Odersky, Stoutamire, and Wadler) and give a detailed proof of type safety. The extended system formalizes for the first time some of the key features of GJ.",
    "cited_by_count": 981,
    "openalex_id": "https://openalex.org/W2033348393",
    "type": "article"
  },
  {
    "title": "Simplification by Cooperating Decision Procedures",
    "doi": "https://doi.org/10.1145/357073.357079",
    "publication_date": "1979-10-01",
    "publication_year": 1979,
    "authors": "Greg Nelson; Derek C. Oppen",
    "corresponding_authors": "",
    "abstract": "A method for combining decision procedures for several theories into a single decision procedure for their combination is described, and a simplifier based on this method is discussed. The simplifier finds a normal form for any expression formed from individual variables, the usual Boolean connectives, the equality predicate =, the conditional function if-then-else, the integers, the arithmetic functions and predicates +, -, and ≤, the Lisp functions and predicates car, cdr, cons, and atom, the functions store and select for storing into and selecting from arrays, and uninterpreted function symbols. If the expression is a theorem it is simplified to the constant true, so the simplifier can be used as a decision procedure for the quantifier-free theory containing these functions and predicates. The simplifier is currently used in the Stanford Pascal Verifier.",
    "cited_by_count": 903,
    "openalex_id": "https://openalex.org/W2164778826",
    "type": "article"
  },
  {
    "title": "Parametric shape analysis via 3-valued logic",
    "doi": "https://doi.org/10.1145/514188.514190",
    "publication_date": "2002-05-01",
    "publication_year": 2002,
    "authors": "Mooly Sagiv; Thomas Reps; Reinhard Wilhelm",
    "corresponding_authors": "",
    "abstract": "Shape analysis concerns the problem of determining \"shape invariants\" for programs that perform destructive updating on dynamically allocated storage. This article presents a parametric framework for shape analysis that can be instantiated in different ways to create different shape-analysis algorithms that provide varying degrees of efficiency and precision. A key innovation of the work is that the stores that can possibly arise during execution are represented (conservatively) using 3-valued logical structures. The framework is instantiated in different ways by varying the predicates used in the 3-valued logic. The class of programs to which a given instantiation of the framework can be applied is not limited a priori (i.e., as in some work on shape analysis, to programs that manipulate only lists, trees, DAGS, etc.); each instantiation of the framework can be applied to any program, but may produce imprecise results (albeit conservative ones) due to the set of predicates employed.",
    "cited_by_count": 799,
    "openalex_id": "https://openalex.org/W1991837261",
    "type": "article"
  },
  {
    "title": "An Efficient Unification Algorithm",
    "doi": "https://doi.org/10.1145/357162.357169",
    "publication_date": "1982-04-01",
    "publication_year": 1982,
    "authors": "Alberto Martelli; Ugo Montanari",
    "corresponding_authors": "",
    "abstract": "article Open Access Share on An Efficient Unification Algorithm Authors: Alberto Martelli Istituto di Scienze della Informazione, Università di Torino, Corso M. d'Azeglio 42, I-10125 Torino, Italy Istituto di Scienze della Informazione, Università di Torino, Corso M. d'Azeglio 42, I-10125 Torino, ItalyView Profile , Ugo Montanari Istituto di Scienze della Informazione, Università di Pisa, Corso Italia 40, I-56100 Pisa, Italy Istituto di Scienze della Informazione, Università di Pisa, Corso Italia 40, I-56100 Pisa, ItalyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 2pp 258–282https://doi.org/10.1145/357162.357169Published:01 April 1982Publication History 608citation4,441DownloadsMetricsTotal Citations608Total Downloads4,441Last 12 Months983Last 6 weeks111 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 785,
    "openalex_id": "https://openalex.org/W2113722134",
    "type": "article"
  },
  {
    "title": "Automatic translation of FORTRAN programs to vector form",
    "doi": "https://doi.org/10.1145/29873.29875",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Randy Allen; Ken Kennedy",
    "corresponding_authors": "",
    "abstract": "The recent success of vector computers such as the Cray-1 and array processors such as those manufactured by Floating Point Systems has increased interest in making vector operations available to the FORTRAN programmer. The FORTRAN standards committee is currently considering a successor to FORTRAN 77, usually called FORTRAN 8x, that will permit the programmer to explicitly specify vector and array operations. Although FORTRAN 8x will make it convenient to specify explicit vector operations in new programs, it does little for existing code. In order to benefit from the power of vector hardware, existing programs will need to be rewritten in some language (presumably FORTRAN 8x) that permits the explicit specification of vector operations. One way to avoid a massive manual recoding effort is to provide a translator that discovers the parallelism implicit in a FORTRAN program and automatically rewrites that program in FORTRAN 8x. Such a translation from FORTRAN to FORTRAN 8x is not straightforward because FORTRAN DO loops are not always semantically equivalent to the corresponding FORTRAN 8x parallel operation. The semantic difference between these two constructs is precisely captured by the concept of dependence . A translation from FORTRAN to FORTRAN 8x preserves the semantics of the original program if it preserves the dependences in that program. The theoretical background is developed here for employing data dependence to convert FORTRAN programs to parallel form. Dependence is defined and characterized in terms of the conditions that give rise to it; accurate tests to determine dependence are presented; and transformations that use dependence to uncover additional parallelism are discussed.",
    "cited_by_count": 725,
    "openalex_id": "https://openalex.org/W2135736783",
    "type": "article"
  },
  {
    "title": "A fast algorithm for finding dominators in a flowgraph",
    "doi": "https://doi.org/10.1145/357062.357071",
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "Thomas Lengauer; Robert E. Tarjan",
    "corresponding_authors": "",
    "abstract": "A fast algorithm for finding dominators in a flowgraph is presented. The algorithm uses depth-first search and an efficient method of computing functions defined on paths in trees. A simple implementation of the algorithm runs in O ( m log n ) time, where m is the number of edges and n is the number of vertices in the problem graph. A more sophisticated implementation runs in O ( m α( m , n )) time, where α( m , n ) is a functional inverse of Ackermann's function. Both versions of the algorithm were implemented in Algol W, a Stanford University version of Algol, and tested on an IBM 370/168. The programs were compared with an implementation by Purdom and Moore of a straightforward O ( mn )-time algorithm, and with a bit vector algorithm described by Aho and Ullman. The fast algorithm beat the straightforward algorithm and the bit vector algorithm on all but the smallest graphs tested.",
    "cited_by_count": 634,
    "openalex_id": "https://openalex.org/W2111379929",
    "type": "article"
  },
  {
    "title": "The Programming Language Aspects of ThingLab, a Constraint-Oriented Simulation Laboratory",
    "doi": "https://doi.org/10.1145/357146.357147",
    "publication_date": "1981-10-01",
    "publication_year": 1981,
    "authors": "Alan Borning",
    "corresponding_authors": "Alan Borning",
    "abstract": "article Free Access Share on The Programming Language Aspects of ThingLab, a Constraint-Oriented Simulation Laboratory Author: Alan Borning Department of Computer Science, FR-35, University of Washington, Seattle, WA Department of Computer Science, FR-35, University of Washington, Seattle, WAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 4Oct. 1981 pp 353–387https://doi.org/10.1145/357146.357147Published:01 October 1981Publication History 570citation1,242DownloadsMetricsTotal Citations570Total Downloads1,242Last 12 Months119Last 6 weeks41 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 629,
    "openalex_id": "https://openalex.org/W2078404830",
    "type": "article"
  },
  {
    "title": "Tentative steps toward a development method for interfering programs",
    "doi": "https://doi.org/10.1145/69575.69577",
    "publication_date": "1983-10-01",
    "publication_year": 1983,
    "authors": "Cliff B. Jones",
    "corresponding_authors": "Cliff B. Jones",
    "abstract": "article Free Access Share on Tentative steps toward a development method for interfering programs Author: C. B. Jones Manchester University Manchester UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 4Oct. 1983 pp 596–619https://doi.org/10.1145/69575.69577Published:01 October 1983Publication History 475citation1,338DownloadsMetricsTotal Citations475Total Downloads1,338Last 12 Months102Last 6 weeks26 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 617,
    "openalex_id": "https://openalex.org/W2090551028",
    "type": "article"
  },
  {
    "title": "Constant propagation with conditional branches",
    "doi": "https://doi.org/10.1145/103135.103136",
    "publication_date": "1991-04-01",
    "publication_year": 1991,
    "authors": "Mark N. Wegman; F. Kenneth Zadeck",
    "corresponding_authors": "",
    "abstract": "Constant propagation is a well-known global flow analysis problem. The goal of constant propagation is to discover values that are constant on all possible executions of a program and to propagate these constant values as far foward through the program as possible. Expressions whose operands are all constants can be evaluated at compile time and the results propagated further. Using the algorithms presented in this paper can produce smaller and faster compiled programs. The same algorithms can be used for other kinds of analyses (e.g., type of determination). We present four algorithms in this paper, all conservitive in the sense that all constants may not be found, but each constant found is constant over all possible executions of the program. These algorithms are among the simplest, fastest, and most powerful global constant propagation algorithms known. We also present a new algorithm that performs a form of interprocedural data flow analysis in which aliasing information is gathered in conjunction with constant progagation. Several variants of this algorithm are considered.",
    "cited_by_count": 602,
    "openalex_id": "https://openalex.org/W2102890180",
    "type": "article"
  },
  {
    "title": "From system F to typed assembly language",
    "doi": "https://doi.org/10.1145/319301.319345",
    "publication_date": "1999-05-01",
    "publication_year": 1999,
    "authors": "Greg Morrisett; David Walker; Karl Crary; Neal Glew",
    "corresponding_authors": "",
    "abstract": "We motivate the design of typed assembly language (TAL) and present a type-preserving ttranslation from Systemn F to TAL. The typed assembly language we pressent is based on a conventional RISC assembly language, but its static type sytem provides support for enforcing high-level language abstratctions, such as closures, tuples, and user-defined abstract data types. The type system ensures that well-typed programs cannot violatet these abstractionsl In addition, the typing constructs admit many low-level compiler optimiztaions. Our translation to TAL is specified as a sequence of type-preserving transformations, including CPS and closure conversion phases; type-correct source programs are mapped to type-correct assembly language. A key contribution is an approach to polymorphic closure conversion that is considerably simpler than previous work. The compiler and typed assembly lanugage provide a fully automatic way to produce certified code, suitable for use in systems where unstrusted and potentially malicious code must be checked for safety before execution.",
    "cited_by_count": 595,
    "openalex_id": "https://openalex.org/W2069107692",
    "type": "article"
  },
  {
    "title": "A calculus for access control in distributed systems",
    "doi": "https://doi.org/10.1145/155183.155225",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Martı́n Abadi; Michael T. Burrows; Butler Lampson; Gordon Plotkin",
    "corresponding_authors": "",
    "abstract": "We study some of the concepts, protocols, and algorithms for access control in distributed systems, from a logical perspective. We account for how a principal may come to believe that another principal is making a request, either on his own or on someone else's behalf. We also provide a logical language for accesss control lists and theories for deciding whether requests should be granted.",
    "cited_by_count": 587,
    "openalex_id": "https://openalex.org/W1987339920",
    "type": "article"
  },
  {
    "title": "Proving Liveness Properties of Concurrent Programs",
    "doi": "https://doi.org/10.1145/357172.357178",
    "publication_date": "1982-07-01",
    "publication_year": 1982,
    "authors": "Susan Owicki; Leslie Lamport",
    "corresponding_authors": "",
    "abstract": "article Free AccessProving Liveness Properties of Concurrent Programs Authors: Susan Owicki Computer Systems Laboratory, Stanford Electronics Laboratories, Department of Electrical Engineering, Stanford University, Stanford, CA Computer Systems Laboratory, Stanford Electronics Laboratories, Department of Electrical Engineering, Stanford University, Stanford, CAView Profile , Leslie Lamport Computer Science Laboratory, SRI International, 333 Ravenswood Avenue, Menlo Park, CA Computer Science Laboratory, SRI International, 333 Ravenswood Avenue, Menlo Park, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 3pp 455–495https://doi.org/10.1145/357172.357178Published:01 July 1982Publication History 453citation1,565DownloadsMetricsTotal Citations453Total Downloads1,565Last 12 Months149Last 6 weeks24 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 586,
    "openalex_id": "https://openalex.org/W2166656159",
    "type": "article"
  },
  {
    "title": "A Deductive Approach to Program Synthesis",
    "doi": "https://doi.org/10.1145/357084.357090",
    "publication_date": "1980-01-01",
    "publication_year": 1980,
    "authors": "Zohar Manna; Richard Waldinger",
    "corresponding_authors": "",
    "abstract": "Program synthesis is the systematic derivation of a program from a given specification. A deductive approach to program synthesis is presented for the construction of recursive programs. This approach regards program synthesis as a theorem-proving task and relies on a theorem-proving method that combines the features of transformation rules, unification, and mathematical induction within a single framework.",
    "cited_by_count": 583,
    "openalex_id": "https://openalex.org/W2012436850",
    "type": "article"
  },
  {
    "title": "Guardians and Actions: Linguistic Support for Robust, Distributed Programs",
    "doi": "https://doi.org/10.1145/2166.357215",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Barbara Liskov; Robert W. Scheifler",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Guardians and Actions: Linguistic Support for Robust, Distributed Programs Authors: Barbara Liskov Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square, Cambridge, MA Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square, Cambridge, MAView Profile , Robert Scheifler Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square, Cambridge, MA Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square, Cambridge, MAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3pp 381–404https://doi.org/10.1145/2166.357215Published:01 July 1983Publication History 403citation685DownloadsMetricsTotal Citations403Total Downloads685Last 12 Months50Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 569,
    "openalex_id": "https://openalex.org/W2009712165",
    "type": "article"
  },
  {
    "title": "Protocol specifications and component adaptors",
    "doi": "https://doi.org/10.1145/244795.244801",
    "publication_date": "1997-03-03",
    "publication_year": 1997,
    "authors": "Daniel M. Yellin; Robert E. Strom",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Protocol specifications and component adaptors Authors: Daniel M. Yellin IBM T. J. Watson Research Center, Yorktown Heights, NY IBM T. J. Watson Research Center, Yorktown Heights, NYView Profile , Robert E. Strom IBM T. J. Watson Research Center, Yorktown Heights, NY IBM T. J. Watson Research Center, Yorktown Heights, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 19Issue 2pp 292–333https://doi.org/10.1145/244795.244801Published:03 March 1997Publication History 436citation2,145DownloadsMetricsTotal Citations436Total Downloads2,145Last 12 Months54Last 6 weeks8 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 564,
    "openalex_id": "https://openalex.org/W1975455609",
    "type": "article"
  },
  {
    "title": "Improving data locality with loop transformations",
    "doi": "https://doi.org/10.1145/233561.233564",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "Kathryn S. McKinley; Steve Carr; Chau‐Wen Tseng",
    "corresponding_authors": "",
    "abstract": "In the past decade, processor speed has become significantly faster than memory speed. Small, fast cache memories are designed to overcome this discrepancy, but they are only effective when programs exhibit data locality . In the this article, we present compiler optimizations to improve data locality based on a simple yet accurate cost model. The model computes both temporal and spatial reuse of cache lines to find desirable loop organizations. The cost model drives the application of compound transformations consisting of loop permutation, loop fusion, loop distribution, and loop reversal. To validate our optimization strategy, we implemented our algorithms and ran experiments on a large collection of scientific programs and kernels. Experiments illustrate that for kernels our model and algorithm can select and achieve the best loop structure for a nest. For over 30 complete applications, we executed the original and transformed versions and simulated cache hit rates. We collected statistics about the inherent characteristics of these programs and our ability to improve their data locality. To our knowledge, these studies are the first of such breadth and depth. We found performance improvements were difficult to achieve bacause benchmark programs typically have high hit rates even for small data caches; however, our optimizations significanty improved several programs.",
    "cited_by_count": 558,
    "openalex_id": "https://openalex.org/W2108315152",
    "type": "article"
  },
  {
    "title": "The concurrency workbench",
    "doi": "https://doi.org/10.1145/151646.151648",
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "Rance Cleaveland; Joachim Parrow; Bernhard Steffen",
    "corresponding_authors": "",
    "abstract": "The Concurrency Workbench is an automated tool for analyzing networks of finite-state processes expressed in Milner's Calculus of Communicating Systems. Its key feature is its breadth: a variety of different verification methods, including equivalence checking, preorder checking, and model checking, are supported for several different process semantics. One experience from our work is that a large number of interesting verification methods can be formulated as combinations of a small number of primitive algorithms. The Workbench has been applied to the verification of communications protocols and mutual exclusion algorithms and has proven a valuable aid in teaching and research.",
    "cited_by_count": 556,
    "openalex_id": "https://openalex.org/W1971107784",
    "type": "article"
  },
  {
    "title": "Model checking and modular verification",
    "doi": "https://doi.org/10.1145/177492.177725",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Orna Grümberg; David E. Long",
    "corresponding_authors": "",
    "abstract": "We describe a framework for compositional verification of finite-state processes. The framework is based on two ideas: a subset of the logic CTL for which satisfaction is preserved under composition, and a preorder on structures which captures the relation between a component and a system containing the component. Satisfaction of a formula in the logic corresponds to being below a particular structure (a tableau for the formula) in the preorder. We show how to do assume-guarantee-style reasoning within this framework. Additionally, we demonstrate efficient methods for model checking in the logic and for checking the preorder in several special cases. We have implemented a system based on these methods, and we use it to give a compositional verification of a CPU controller.",
    "cited_by_count": 539,
    "openalex_id": "https://openalex.org/W2009965218",
    "type": "article"
  },
  {
    "title": "Specifying Concurrent Program Modules",
    "doi": "https://doi.org/10.1145/69624.357207",
    "publication_date": "1983-04-01",
    "publication_year": 1983,
    "authors": "Leslie Lamport",
    "corresponding_authors": "Leslie Lamport",
    "abstract": "article Free AccessSpecifying Concurrent Program Modules Author: Leslie Lamport Computer Science Laboratory, SRI International, 333 Ravenswood Avenue, Menlo Park, CA Computer Science Laboratory, SRI International, 333 Ravenswood Avenue, Menlo Park, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 2April 1983 pp 190–222https://doi.org/10.1145/69624.357207Published:01 April 1983Publication History 396citation1,279DownloadsMetricsTotal Citations396Total Downloads1,279Last 12 Months82Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 533,
    "openalex_id": "https://openalex.org/W2086070079",
    "type": "article"
  },
  {
    "title": "The CLP( ℛ ) language and system",
    "doi": "https://doi.org/10.1145/129393.129398",
    "publication_date": "1992-05-01",
    "publication_year": 1992,
    "authors": "Joxan Jaffar; Spiro Michaylov; Peter J. Stuckey; Roland H. C. Yap",
    "corresponding_authors": "",
    "abstract": "The CLP( ℛ ) programming language is defined, its underlying philosophy and programming methodology are discussed, important implementation issues are explored in detail, and finally, a prototype interpreter is described. CLP( ℛ ) is designed to be an instance of the Constraint Logic Programming Scheme, a family of rule-based constraint programming languages defined by Jaffar and Lassez. The domain of computation ℛ of this particular instance is the algebraic structure consisting of uninterpreted functors over real numbers. An important property of CLP( ℛ )is that the constraints are treated uniformly in the sense that they are used to specify the input parameters to a program, they are the only primitives used in the execution of a program, and they are used to describe the output of a program. Implementation of a CLP language, and of CLP( ℛ ) in particular, raises new problems in the design of a constraint-solver. For example, the constraint solver must be incremental in the sense that solving additional constraints must not entail the resolving of old constraints. In our system, constraints are filtered through an inference engine, an engine/solver interface, an equation solver and an inequality solver. This sequence of modules reflects a classification and prioritization of the classes of constraints. Modules solving higher priority constraints are isolated from the complexities of modules solving lower priority constraints. This multiple-phase solving of constraints, together with a set of associated algorithms, gives rise to a practical system.",
    "cited_by_count": 530,
    "openalex_id": "https://openalex.org/W2060498389",
    "type": "article"
  },
  {
    "title": "Ten Years of Hoare's Logic: A Survey—Part I",
    "doi": "https://doi.org/10.1145/357146.357150",
    "publication_date": "1981-10-01",
    "publication_year": 1981,
    "authors": "Krzysztof R. Apt",
    "corresponding_authors": "Krzysztof R. Apt",
    "abstract": "article Free AccessTen Years of Hoare's Logic: A Survey—Part I Author: Krzysztof R. Apt LITP, Université Paris 7, 2, place Jussieu, 75221 Paris, France LITP, Université Paris 7, 2, place Jussieu, 75221 Paris, FranceView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 4Oct. 1981 pp 431–483https://doi.org/10.1145/357146.357150Published:01 October 1981Publication History 387citation1,839DownloadsMetricsTotal Citations387Total Downloads1,839Last 12 Months126Last 6 weeks58 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 520,
    "openalex_id": "https://openalex.org/W2111619838",
    "type": "article"
  },
  {
    "title": "Kleene algebra with tests",
    "doi": "https://doi.org/10.1145/256167.256195",
    "publication_date": "1997-05-01",
    "publication_year": 1997,
    "authors": "Dexter Kozen",
    "corresponding_authors": "Dexter Kozen",
    "abstract": "We introduce Kleene algebra with tests, an equational system for manipulating programs. We give a purely equational proof, using Kleene algebra with tests and commutativity conditions, of the following classical result: every while program can be simulated by a while program can be simulated by a while program with at most one while loop. The proof illustrates the use of Kleene algebra with tests and commutativity conditions in program equivalence proofs.",
    "cited_by_count": 518,
    "openalex_id": "https://openalex.org/W1998368317",
    "type": "article"
  },
  {
    "title": "Synthesis of Communicating Processes from Temporal Logic Specifications",
    "doi": "https://doi.org/10.1145/357233.357237",
    "publication_date": "1984-01-01",
    "publication_year": 1984,
    "authors": "Zohar Manna; Pierre Wolper",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Synthesis of Communicating Processes from Temporal Logic Specifications Authors: Zohar Manna Department of Computer Science, Stanford University, Stanford, CA Department of Computer Science, Stanford University, Stanford, CAView Profile , Pierre Wolper Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 1pp 68–93https://doi.org/10.1145/357233.357237Published:01 January 1984Publication History 338citation1,058DownloadsMetricsTotal Citations338Total Downloads1,058Last 12 Months71Last 6 weeks8 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 504,
    "openalex_id": "https://openalex.org/W2040127143",
    "type": "article"
  },
  {
    "title": "Composing specifications",
    "doi": "https://doi.org/10.1145/151646.151649",
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "Martı́n Abadi; Leslie Lamport",
    "corresponding_authors": "",
    "abstract": "A rigorous modular specification method requires a proof rule asserting that if each component behaves correctly in isolation, then it behaves correctly in concert with other components. Such a rule is subtle because a component need behave correctly only when its environment does, and each component is part of the others' environments. We examine the precise distinction between a system and its environment, and provide the requisite proof rule when modules are specified with safety and liveness properties.",
    "cited_by_count": 469,
    "openalex_id": "https://openalex.org/W2293400264",
    "type": "article"
  },
  {
    "title": "A methodology for implementing highly concurrent data objects",
    "doi": "https://doi.org/10.1145/161468.161469",
    "publication_date": "1993-11-01",
    "publication_year": 1993,
    "authors": "Maurice Herlihy",
    "corresponding_authors": "Maurice Herlihy",
    "abstract": "A concurrent object is a data structure shared by concurrent processes. Conventional techniques for implementing concurrent objects typically rely on critical sections ; ensuring that only one process at a time can operate on the object. Nevertheless, critical sections are poorly suited for asynchronous systems: if one process is halted or delayed in a critical section, other, nonfaulty processes will be unable to progress. By contrast, a concurrent object implementation is lock free if it always guarantees that some process will complete an operation in a finite number of steps, and it is wait free if it guarantees that each process will complete an operation in a finite number of steps. This paper proposes a new methodology for constructing lock-free and wait-free implementations of concurrent objects. The object's representation and operations are written as stylized sequential programs, with no explicit synchronization. Each sequential operation is atutomatically transformed into a lock-free or wait-free operation using novel synchronization and memory management algorithms. These algorithms are presented for a multiple instruction/multiple data (MIMD) architecture in which n processes communicate by applying atomic read, write, load_linked, and store_conditional operations to a shared memory.",
    "cited_by_count": 464,
    "openalex_id": "https://openalex.org/W1996931099",
    "type": "article"
  },
  {
    "title": "Conjoining specifications",
    "doi": "https://doi.org/10.1145/203095.201069",
    "publication_date": "1995-05-01",
    "publication_year": 1995,
    "authors": "Martı́n Abadi; Leslie Lamport",
    "corresponding_authors": "",
    "abstract": "We show how to specify components of concurrent systems. The specification of a system is the conjunction of its components' specifications. Properties of the system are proved by reasoning about its components. We consider both the decomposition of a given system into parts, and the composition of given parts to form a system.",
    "cited_by_count": 459,
    "openalex_id": "https://openalex.org/W2296232480",
    "type": "article"
  },
  {
    "title": "Abstract types have existential type",
    "doi": "https://doi.org/10.1145/44501.45065",
    "publication_date": "1988-07-01",
    "publication_year": 1988,
    "authors": "John C. Mitchell; Gordon Plotkin",
    "corresponding_authors": "",
    "abstract": "Abstract data type declarations appear in typed programming languages like Ada, Alphard, CLU and ML. This form of declaration binds a list of identifiers to a type with associated operations, a composite “value” we call a data algebra . We use a second-order typed lambda calculus SOL to show how data algebras may be given types, passed as parameters, and returned as results of function calls. In the process, we discuss the semantics of abstract data type declarations and review a connection between typed programming languages and constructive logic.",
    "cited_by_count": 438,
    "openalex_id": "https://openalex.org/W2104204098",
    "type": "article"
  },
  {
    "title": "Efficient and correct execution of parallel programs that share memory",
    "doi": "https://doi.org/10.1145/42190.42277",
    "publication_date": "1988-04-01",
    "publication_year": 1988,
    "authors": "Dennis Shasha; Marc Snir",
    "corresponding_authors": "",
    "abstract": "In this paper we consider an optimization problem that arises in the execution of parallel programs on shared-memory multiple-instruction-stream, multiple-data-stream (MIMD) computers. A program on such machines consists of many sequential program segments, each executed by a single processor. These segments interact as they access shared variables. Access to memory is asynchronous, and memory accesses are not necessarily executed in the order they were issued. An execution is correct if it is sequentially consistent: It should seem as if all the instructions were executed sequentially, in an order obtained by interleaving the instruction streams of the processors. Sequential consistency can be enforced by delaying each access to shared memory until the previous access of the same processor has terminated. For performance reasons, however, we want to allow several accesses by the same processor to proceed concurrently. Our analysis finds a minimal set of delays that enforces sequential consistency. The analysis extends to interprocessor synchronization constraints and to code where blocks of operations have to execute atomically. We use a conflict graph similar to that used to schedule transactions in distributed databases. Our graph incorporates the order on operations given by the program text, enabling us to do without locks even when database conflict graphs would suggest that locks are necessary. Our work has implications for the design of multiprocessors; it offers new compiler optimization techniques for parallel languages that support shared variables.",
    "cited_by_count": 427,
    "openalex_id": "https://openalex.org/W2039509099",
    "type": "article"
  },
  {
    "title": "Integrating noninterfering versions of programs",
    "doi": "https://doi.org/10.1145/65979.65980",
    "publication_date": "1989-07-01",
    "publication_year": 1989,
    "authors": "Susan Horwitz; Jan F. Prins; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "The need to integrate several versions of a program into a common one arises frequently, but it is a tedious and time consuming task to integrate programs by hand. To date, the only available tools for assisting with program integration are variants of text-based differential file comparators; these are of limited utility because one has no guarantees about how the program that is the product of an integration behaves compared to the programs that were integrated. This paper concerns the design of a semantics-based tool for automatically integrating program versions. The main contribution of the paper is an algorithm that takes as input three programs A , B , and Base , where A and B are two variants of Base . Whenever the changes made to Base to create A and B do not “interfere” (in a sense defined in the paper), the algorithm produces a program M that integrates A and B . The algorithm is predicated on the assumption that differences in the behavior of the variant programs from that of Base , rather than differences in the text , are significant and must be preserved in M . Although it is undecidable whether a program modification actually leads to such a difference, it is possible to determine a safe approximation by comparing each of the variants with Base . To determine this information, the integration algorithm employs a program representation that is similar (although not identical) to the dependence graphs that have been used previously in vectorizing and parallelizing compilers. The algorithm also makes use of the notion of a program slice to find just those statements of a program that determine the values of potentially affected variables. The program-integration problem has not been formalized previously. It should be noted, however, that the integration problem examined here is a greatly simplified one; in particular, we assume that expressions contain only scalar variables and constants, and that the only statements used in programs are assignment statements, conditional statements, and while-loops.",
    "cited_by_count": 424,
    "openalex_id": "https://openalex.org/W2064625489",
    "type": "article"
  },
  {
    "title": "Reasoning about naming systems",
    "doi": "https://doi.org/10.1145/161468.161471",
    "publication_date": "1993-11-01",
    "publication_year": 1993,
    "authors": "Mic Bowman; Saumya Debray; Larry Peterson",
    "corresponding_authors": "",
    "abstract": "article Free AccessReasoning about naming systems Authors: Mic Bowman Pennsylvania State Univ., University Park Pennsylvania State Univ., University ParkView Profile , Saumya K. Debray Univ. of Arizona, Tucson Univ. of Arizona, TucsonView Profile , Larry L. Peterson Univ. of Arizona, Tucson Univ. of Arizona, TucsonView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 5pp 795–825https://doi.org/10.1145/161468.161471Published:01 November 1993Publication History 167citation1,612DownloadsMetricsTotal Citations167Total Downloads1,612Last 12 Months112Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 419,
    "openalex_id": "https://openalex.org/W2043539602",
    "type": "article"
  },
  {
    "title": "Combinators for bidirectional tree transformations",
    "doi": "https://doi.org/10.1145/1232420.1232424",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "J. Nathan Foster; Michael B. Greenwald; Jonathan T. Moore; Benjamin C. Pierce; Alan Schmitt",
    "corresponding_authors": "",
    "abstract": "We propose a novel approach to the view-update problem for tree-structured data: a domain-specific programming language in which all expressions denote bidirectional transformations on trees. In one direction, these transformations---dubbed lenses ---map a concrete tree into a simplified abstract view; in the other, they map a modified abstract view, together with the original concrete tree, to a correspondingly modified concrete tree. Our design emphasizes both robustness and ease of use, guaranteeing strong well-behavedness and totality properties for well-typed lenses. We begin by identifying a natural space of well-behaved bidirectional transformations over arbitrary structures, studying definedness and continuity in this setting. We then instantiate this semantic framework in the form of a collection of lens combinators that can be assembled to describe bidirectional transformations on trees. These combinators include familiar constructs from functional programming (composition, mapping, projection, conditionals, recursion) together with some novel primitives for manipulating trees (splitting, pruning, merging, etc.). We illustrate the expressiveness of these combinators by developing a number of bidirectional list-processing transformations as derived forms. An extended example shows how our combinators can be used to define a lens that translates between a native HTML representation of browser bookmarks and a generic abstract bookmark format.",
    "cited_by_count": 418,
    "openalex_id": "https://openalex.org/W2000616678",
    "type": "article"
  },
  {
    "title": "Improvements to graph coloring register allocation",
    "doi": "https://doi.org/10.1145/177492.177575",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Preston Briggs; Keith D. Cooper; Linda Torczon",
    "corresponding_authors": "",
    "abstract": "We describe two improvements to Chaitin-style graph coloring register allocators. The first, optimistic coloring , uses a stronger heuristic to find a k -coloring for the interference graph. The second extends Chaitin's treatment of rematerialization to handle a larger class of values. These techniques are complementary. Optimistic coloring decreases the number of procedures that require spill code and reduces the amount of spill code when spilling is unavoidable. Rematerialization lowers the cost of spilling some values. This paper describes both of the techniques and our experience building and using register allocators that incorporate them. It provides a detailed description of optimistic coloring and rematerialization. It presents experimental data to show the performance of several versions of the register allocator on a suite of FORTRAN programs. It discusses several insights that we discovered only after repeated implementation of these allocators.",
    "cited_by_count": 395,
    "openalex_id": "https://openalex.org/W2044636417",
    "type": "article"
  },
  {
    "title": "The drinking philosophers problem",
    "doi": "https://doi.org/10.1145/1780.1804",
    "publication_date": "1984-10-01",
    "publication_year": 1984,
    "authors": "K. Mani Chandy; Jayadev Misra",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on The drinking philosophers problem Authors: K. M. Chandy University of Texas at Austin University of Texas at AustinView Profile , J. Misra University of Texas at Austin University of Texas at AustinView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 401 October 1984pp 632–646https://doi.org/10.1145/1780.1804Published:01 October 1984Publication History 317citation2,429DownloadsMetricsTotal Citations317Total Downloads2,429Last 12 Months99Last 6 weeks9 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 390,
    "openalex_id": "https://openalex.org/W1985091167",
    "type": "article"
  },
  {
    "title": "Abstract interpretation of reactive systems",
    "doi": "https://doi.org/10.1145/244795.244800",
    "publication_date": "1997-03-03",
    "publication_year": 1997,
    "authors": "Dennis Dams; Rob Gerth; Orna Grümberg",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Abstract interpretation of reactive systems Authors: Dennis Dams Eindhoven Univ. of Technology, Eindhoven, The Netherlands Eindhoven Univ. of Technology, Eindhoven, The NetherlandsView Profile , Rob Gerth Eindhoven Univ. of Technology, Eindhoven, The Netherlands Eindhoven Univ. of Technology, Eindhoven, The NetherlandsView Profile , Orna Grumberg Technion–Israel Institute of Technology, Haifa, Israel Technion–Israel Institute of Technology, Haifa, IsraelView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 19Issue 2pp 253–291https://doi.org/10.1145/244795.244800Published:03 March 1997Publication History 270citation1,677DownloadsMetricsTotal Citations270Total Downloads1,677Last 12 Months84Last 6 weeks11 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 389,
    "openalex_id": "https://openalex.org/W2090106217",
    "type": "article"
  },
  {
    "title": "I-structures: data structures for parallel computing",
    "doi": "https://doi.org/10.1145/69558.69562",
    "publication_date": "1989-10-01",
    "publication_year": 1989,
    "authors": "Arvind Arvind; Rishiyur S. Nikhil; Keshav Pingali",
    "corresponding_authors": "",
    "abstract": "It is difficult to achieve elegance, efficiency, and parallelism simultaneously in functional programs that manipulate large data structures. We demonstrate this through careful analysis of program examples using three common functional data-structuring approaches-lists using Cons, arrays using Update (both fine-grained operators), and arrays using make-array (a “bulk” operator). We then present I-structure as an alternative and show elegant, efficient, and parallel solutions for the program examples in Id, a language with I-structures. The parallelism in Id is made precise by means of an operational semantics for Id as a parallel reduction system. I-structures make the language nonfunctional, but do not lose determinacy. Finally, we show that even in the context of purely functional languages, I-structures are invaluable for implementing functional data abstractions.",
    "cited_by_count": 368,
    "openalex_id": "https://openalex.org/W1986804682",
    "type": "article"
  },
  {
    "title": "The concept of a supercompiler",
    "doi": "https://doi.org/10.1145/5956.5957",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Valentin F. Turchin",
    "corresponding_authors": "Valentin F. Turchin",
    "abstract": "A supercompiler is a program transformer of a certain type. It traces the possible generalized histories of computation by the original program, and compiles an equivalent program, reducing in the process the redundancy that could be present in the original program. The nature of the redundancy that can be eliminated by supercompilation may be various, e.g., some variables might have predefined values (as in partial evaluation), or the structure of control transfer could be made more efficient (as in lazy evaluation), or it could simply be the fact that the same variable is used more than once. The general principles of supercompilation are described and compared with the usual approach to program transformation as a stepwise application of a number of equivalence rules. It is argued that the language Refal serves the needs of supercompilation best. Refal is formally defined and compared with Prolog and other languages. Examples are given of the operation of a Refal supercompiler implemented at CCNY on an IBM/370.",
    "cited_by_count": 368,
    "openalex_id": "https://openalex.org/W2020024606",
    "type": "article"
  },
  {
    "title": "CCured: type-safe retrofitting of legacy software",
    "doi": "https://doi.org/10.1145/1065887.1065892",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "George C. Necula; Jeremy Condit; Matthew Harren; Scott McPeak; Westley Weimer",
    "corresponding_authors": "",
    "abstract": "This article describes CCured, a program transformation system that adds type safety guarantees to existing C programs. CCured attempts to verify statically that memory errors cannot occur, and it inserts run-time checks where static verification is insufficient.CCured extends C's type system by separating pointer types according to their usage, and it uses a surprisingly simple type inference algorithm that is able to infer the appropriate pointer kinds for existing C programs. CCured uses physical subtyping to recognize and verify a large number of type casts at compile time. Additional type casts are verified using run-time type information. CCured uses two instrumentation schemes, one that is optimized for performance and one in which metadata is stored in a separate data structure whose shape mirrors that of the original user data. This latter scheme allows instrumented programs to invoke external functions directly on the program's data without the use of a wrapper function.We have used CCured on real-world security-critical network daemons to produce instrumented versions without memory-safety vulnerabilities, and we have found several bugs in these programs. The instrumented code is efficient enough to be used in day-to-day operations.",
    "cited_by_count": 358,
    "openalex_id": "https://openalex.org/W2098806455",
    "type": "article"
  },
  {
    "title": "Using Time Instead of Timeout for Fault-Tolerant Distributed Systems.",
    "doi": "https://doi.org/10.1145/2993.2994",
    "publication_date": "1984-04-01",
    "publication_year": 1984,
    "authors": "Leslie Lamport",
    "corresponding_authors": "Leslie Lamport",
    "abstract": "article Free Access Share on Using Time Instead of Timeout for Fault-Tolerant Distributed Systems. Author: Leslie Lamport SRI International SRI InternationalView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 2April 1984 pp 254–280https://doi.org/10.1145/2993.2994Published:01 April 1984Publication History 198citation2,006DownloadsMetricsTotal Citations198Total Downloads2,006Last 12 Months241Last 6 weeks35 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 337,
    "openalex_id": "https://openalex.org/W2061819031",
    "type": "article"
  },
  {
    "title": "A Proof System for Communicating Sequential Processes",
    "doi": "https://doi.org/10.1145/357103.357110",
    "publication_date": "1980-07-01",
    "publication_year": 1980,
    "authors": "Krzysztof R. Apt; Nissim Francez; Willem P. de Roever",
    "corresponding_authors": "",
    "abstract": "An axiomatic proof system is presented for proving partial correctness and absence of deadlock (and failure) of communicating sequential processes. The key (meta) rule introduces cooperation between proofs, a new concept needed to deal with proofs about synchronization by message passing. CSP's new convention for distributed termination of loops is dealt with. Applications of the method involve correctness proofs for two algorithms, one for distributed partitioning of sets, the other for distributed computation of the greatest common divisor of n numbers.",
    "cited_by_count": 328,
    "openalex_id": "https://openalex.org/W1984831616",
    "type": "article"
  },
  {
    "title": "PARLOG: parallel programming in logic",
    "doi": "https://doi.org/10.1145/5001.5390",
    "publication_date": "1986-01-02",
    "publication_year": 1986,
    "authors": "Keith Clark; Steven Gregory",
    "corresponding_authors": "",
    "abstract": "PARLOG is a logic programming language in the sense that nearly every definition and query can be read as a sentence of predicate logic. It differs from PROLOG in incorporating parallel modes of evaluation. For reasons of efficient implementation, it distinguishes and separates and-parallel and or-parallel evaluation. PARLOG relations are divided into two types: single-solution relations and all-solutions relations. A conjunction of single-solution relation calls can be evaluated in parallel with shared variables acting as communication channels for the passing of partial bindings. Only one solution to each call is computed, using committed choice nondeterminism. A conjunction of all-solutions relation calls is evaluated without communication of partial bindings, but all the solutions may be found by an or-parallel exploration of the different evaluation paths. A set constructor provides the main interface between single-solution relations and all-solutions relations. This paper is a tutorial introduction to PARLOG. It assumes familiarity with logic programming. Categories and Subject Descriptors: D.l.l [Programming Techniques]: Applicative (Functional)",
    "cited_by_count": 325,
    "openalex_id": "https://openalex.org/W2143326699",
    "type": "article"
  },
  {
    "title": "Herding Cats",
    "doi": "https://doi.org/10.1145/2627752",
    "publication_date": "2014-07-01",
    "publication_year": 2014,
    "authors": "Jade Alglave; Luc Maranget; Michael Tautschnig",
    "corresponding_authors": "",
    "abstract": "We propose an axiomatic generic framework for modelling weak memory. We show how to instantiate this framework for Sequential Consistency (SC), Total Store Order (TSO), C++ restricted to release-acquire atomics, and Power. For Power, we compare our model to a preceding operational model in which we found a flaw. To do so, we define an operational model that we show equivalent to our axiomatic model. We also propose a model for ARM. Our testing on this architecture revealed a behaviour later acknowledged as a bug by ARM, and more recently, 31 additional anomalies. We offer a new simulation tool, called herd, which allows the user to specify the model of his choice in a concise way. Given a specification of a model, the tool becomes a simulator for that model. The tool relies on an axiomatic description; this choice allows us to outperform all previous simulation tools. Additionally, we confirm that verification time is vastly improved, in the case of bounded model checking. Finally, we put our models in perspective, in the light of empirical data obtained by analysing the C and C++ code of a Debian Linux distribution. We present our new analysis tool, called mole, which explores a piece of code to find the weak memory idioms that it uses.",
    "cited_by_count": 321,
    "openalex_id": "https://openalex.org/W2738891045",
    "type": "article"
  },
  {
    "title": "Ultracomputers",
    "doi": "https://doi.org/10.1145/357114.357116",
    "publication_date": "1980-10-01",
    "publication_year": 1980,
    "authors": "Jacob T. Schwartz",
    "corresponding_authors": "Jacob T. Schwartz",
    "abstract": "A class of parallel processors potentially involving thousands of individual processing elements is described. The architecture is based on the perfect shuffle connection and has two favorable characteristics: (1) Each processor communicates with a fixed number of other processors. (2) Important communication functions can be accomplished in time proportional to the logarithm of the number of processors. A number of basic algorithms for these “ultracomputers” are presented, and physical design considerations are discussed in a preliminary fashion.",
    "cited_by_count": 287,
    "openalex_id": "https://openalex.org/W4206658214",
    "type": "article"
  },
  {
    "title": "WYSINWYX",
    "doi": "https://doi.org/10.1145/1749608.1749612",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Gogul Balakrishnan; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "Over the last seven years, we have developed static-analysis methods to recover a good approximation to the variables and dynamically allocated memory objects of a stripped executable, and to track the flow of values through them. The article presents the algorithms that we developed, explains how they are used to recover Intermediate Representations (IRs) from executables that are similar to the IRs that would be available if one started from source code, and describes their application in the context of program understanding and automated bug hunting. Unlike algorithms for analyzing executables that existed prior to our work, the ones presented in this article provide useful information about memory accesses, even in the absence of debugging information. The ideas described in the article are incorporated in a tool for analyzing Intel x86 executables, called CodeSurfer/x86. CodeSurfer/x86 builds a system dependence graph for the program, and provides a GUI for exploring the graph by (i) navigating its edges, and (ii) invoking operations, such as forward slicing, backward slicing, and chopping, to discover how parts of the program can impact other parts. To assess the usefulness of the IRs recovered by CodeSurfer/x86 in the context of automated bug hunting, we built a tool on top of CodeSurfer/x86, called Device-Driver Analyzer for x86 (DDA/x86), which analyzes device-driver executables for bugs. Without the benefit of either source code or symbol-table/debugging information, DDA/x86 was able to find known bugs (that had been discovered previously by source-code analysis tools), along with useful error traces, while having a low false-positive rate. DDA/x86 is the first known application of program analysis/verification techniques to industrial executables.",
    "cited_by_count": 275,
    "openalex_id": "https://openalex.org/W2030906223",
    "type": "article"
  },
  {
    "title": "Optimally profiling and tracing programs",
    "doi": "https://doi.org/10.1145/183432.183527",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Thomas Ball; James R. Larus",
    "corresponding_authors": "",
    "abstract": "This paper describes algorithms for inserting monitoring code to profile and trace programs. These algorithms greatly reduce the cost of measuring programs with respect to the commonly used technique of placing code in each basic block. Program profiling counts the number of times each basic block in a program executes. Instruction tracing records the sequence of basic blocks traversed in a program execution. The algorithms optimize the placement of counting/tracing code with respect to the expected or measured frequency of each block or edge in a program's control-flow graph. We have implemented the algorithms in a profiling/tracing tool, and they substantially reduce the overhead of profiling and tracing. We also define and study the hierarchy of profiling problems. These problems have two dimensions: what is profiled (i.e., vertices (basic blocks) or edges in a control-flow graph) and where the instrumentation code is placed (in blocks or along edges). We compare the optimal solutions to the profiling problems and describe a new profiling problem: basic-block profiling with edge counters. This problem is important because an optimal solution to any other profiling problem (for a given control-flow graph) is never better than an optimal solution to this problem. Unfortunately, finding an optimal placement of edge counters for vertex profiling appears to be a hard problem in general. However, our work shows that edge profiling with edge counters works well in practice because it is simple and efficient and finds optimal counter placements in most cases. Furthermore, it yields more information than a vertex profile. Tracing also benefits from placing instrumentation code along edges rather than on vertices.",
    "cited_by_count": 367,
    "openalex_id": "https://openalex.org/W2031487553",
    "type": "article"
  },
  {
    "title": "Linear scan register allocation",
    "doi": "https://doi.org/10.1145/330249.330250",
    "publication_date": "1999-09-01",
    "publication_year": 1999,
    "authors": "Massimiliano Poletto; Vivek Sarkar",
    "corresponding_authors": "",
    "abstract": "We describe a new algorithm for fast global register allocation called linear scan . This algorithm is not based on graph coloring, but allocates registers to variables in a single linear-time scan of the variables' live ranges. The linear scan algorithm is considerably faster than algorithms based on graph coloring, is simple to implement, and results in code that is almost as efficient as that obtained using more complex and time-consuming register allocators based on graph coloring. The algorithm is of interest in applications where compile time is a concern, such as dynamic compilation systems, “just-in-time” compilers, and interactive development environments.",
    "cited_by_count": 361,
    "openalex_id": "https://openalex.org/W2140191557",
    "type": "article"
  },
  {
    "title": "Subtyping recursive types",
    "doi": "https://doi.org/10.1145/155183.155231",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Roberto M. Amadio; Luca Cardelli",
    "corresponding_authors": "",
    "abstract": "We investigate the interactions of subtyping and recursive types, in a simply typed λ-calculus. The two fundamental questions here are whether two (recursive)types are in the subtype relation and whether a term has a type. To address the first question, we relate various definitions of type equivalence and subtyping that are induced by a model, an ordering on infinite trees, an algorithm, and a set of type rules. We show soundness and completeness among the rules, the algorithm, and the tree semantics. We also prove soundness and a restricted form of completeness for the model. To address the second question, we show that to every pair of types in the subtype relation we can associate a term whose denotation is the uniquely determined coercion map between the two types. Moreover, we derive an algorithm that, when given a term with implicit coercions, can infer its least type whenever possible.",
    "cited_by_count": 357,
    "openalex_id": "https://openalex.org/W2123727486",
    "type": "article"
  },
  {
    "title": "An indexed model of recursive types for foundational proof-carrying code",
    "doi": "https://doi.org/10.1145/504709.504712",
    "publication_date": "2001-09-01",
    "publication_year": 2001,
    "authors": "Andrew W. Appel; David McAllester",
    "corresponding_authors": "",
    "abstract": "The proofs of \"traditional\" proof carrying code (PCC) are type-specialized in the sense that they require axioms about a specific type system. In contrast, the proofs of foundational PCC explicitly define all required types and explicitly prove all the required properties of those types assuming only a fixed foundation of mathematics such as higher-order logic. Foundational PCC is both more flexible and more secure than type-specialized PCC.For foundational PCC we need semantic models of type systems on von Neumann machines. Previous models have been either too weak (lacking general recursive types and first-class function-pointers), too complex (requiring machine-checkable proofs of large bodies of computability theory), or not obviously applicable to von Neumann machines. Our new model is strong, simple, and works either in λ-calculus or on Pentiums.",
    "cited_by_count": 352,
    "openalex_id": "https://openalex.org/W1991984504",
    "type": "article"
  },
  {
    "title": "The priority-based coloring approach to register allocation",
    "doi": "https://doi.org/10.1145/88616.88621",
    "publication_date": "1990-10-01",
    "publication_year": 1990,
    "authors": "Fred Chow; John L. Hennessy",
    "corresponding_authors": "",
    "abstract": "Global register allocation plays a major role in determining the efficacy of an optimizing compiler. Graph coloring has been used as the central paradigm for register allocation in modern compilers. A straightforward coloring approach can suffer from several shortcomings. These shortcomings are addressed in this paper by coloring the graph using a priority ordering. A natural method for dealing with the spilling emerges from this approach. The detailed algorithms for a priority-based coloring approach are presented and are contrasted with the basic graph-coloring algorithm. Various extensions to the basic algorithms are also presented. Measurements of large programs are used to determine the effectiveness of the algorithm and its extensions, as well as the causes of an imperfect allocation. Running time of the allocator and the impact of heuristics aimed at reducing that time are also measured.",
    "cited_by_count": 343,
    "openalex_id": "https://openalex.org/W2023200270",
    "type": "article"
  },
  {
    "title": "Solving shape-analysis problems in languages with destructive updating",
    "doi": "https://doi.org/10.1145/271510.271517",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Mooly Sagiv; Thomas Reps; Reinhard Wilhelm",
    "corresponding_authors": "",
    "abstract": "This article concerns the static analysis of programs that perform destructive updating on heap-allocated storage. We give an algorithm that uses finite shape graphs to approximate conservatively the possible “shapes” that heap-allocated structures in a program can take on. For certain programs, our technique is able to determine such properties as (1) when the input to the program is a list, the output is also a list and (2) when the input to the program is a tree, the output is also a tree. For example, the method can determine that “listness” is preserved by (1) a program that performs list reversal via destructive updating of the input list and (2) a program that searches a list and splices a new element into the list. None of the previously known methods that use graphs to model the program's store are capable of determining that “listness” is preserved on these examples (or examples of similar complexity). In contrast with most previous work, our shape analysis algorithm is even accurate for certain programs that update cyclic data structures; that is, it is sometimes able to show that when the input to the program is a circular list, the output is also a circular list. For example, the shape-analysis algorithm can determine that an insertion into a circular list preserves “circular listness.”",
    "cited_by_count": 328,
    "openalex_id": "https://openalex.org/W2030697178",
    "type": "article"
  },
  {
    "title": "Code generation using tree matching and dynamic programming",
    "doi": "https://doi.org/10.1145/69558.75700",
    "publication_date": "1989-10-01",
    "publication_year": 1989,
    "authors": "Alfred V. Aho; Mahadevan Ganapathi; Steven Tjiang",
    "corresponding_authors": "",
    "abstract": "Compiler-component generators, such as lexical analyzer generators and parser generators, have long been used to facilitate the construction of compilers. A tree-manipulation language called twig has been developed to help construct efficient code generators. Twig transforms a tree-translation scheme into a code generator that combines a fast top-down tree-pattern matching algorithm with dynamic programming. Twig has been used to specify and construct code generators for several experimental compilers targeted for different machines.",
    "cited_by_count": 324,
    "openalex_id": "https://openalex.org/W2057455870",
    "type": "article"
  },
  {
    "title": "Probabilistic predicate transformers",
    "doi": "https://doi.org/10.1145/229542.229547",
    "publication_date": "1996-05-01",
    "publication_year": 1996,
    "authors": "Carroll Morgan; Annabelle McIver; Karen Seidel",
    "corresponding_authors": "",
    "abstract": "Probabilistic predicates generalize standard predicates over a state space; with probabilistic predicate transformers one thus reasons about imperative programs in terms of probabilistic pre- and postconditions. Probabilistic healthiness conditions generalize the standard ones, characterizing “real” probabilistic programs, and are based on a connection with an underlying relational model for probabilistic execution; in both contexts demonic nondeterminism coexists with probabilistic choice. With the healthiness conditions, the associated weakest-precondition calculus seems suitable for exploring the rigorous derivation of small probabilistic programs.",
    "cited_by_count": 308,
    "openalex_id": "https://openalex.org/W2141670510",
    "type": "article"
  },
  {
    "title": "The undecidability of aliasing",
    "doi": "https://doi.org/10.1145/186025.186041",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "G. Ramalingam",
    "corresponding_authors": "G. Ramalingam",
    "abstract": "Alias analysis is a prerequisite for performing most of the common program analyses such as reaching-definitions analysis or live-variables analysis. Landi [1992] recently established that it is impossible to compute statically precise alias information—either may-alias or must-alias—in languages with if statements, loops, dynamic storage, and recursive data structures: more precisely, he showed that the may-alias relation is not recursive, while the must-alias relation is not even recursively enumerable. This article presents simpler proofs of the same results.",
    "cited_by_count": 302,
    "openalex_id": "https://openalex.org/W2055084740",
    "type": "article"
  },
  {
    "title": "Dynamic typing in a statically typed language",
    "doi": "https://doi.org/10.1145/103135.103138",
    "publication_date": "1991-04-01",
    "publication_year": 1991,
    "authors": "Martı́n Abadi; Luca Cardelli; Benjamin C. Pierce; Gordon Plotkin",
    "corresponding_authors": "",
    "abstract": "Statically typed programming languages allow earlier error checking, better enforcement of diciplined programming styles, and the generation of more efficient object code than languages where all type consistency checks are performed at run time. However, even in statically typed languages, there is often the need to deal with datawhose type cannot be determined at compile time. To handle such situations safely, we propose to add a type Dynamic whose values are pairs of a value v and a type tag T where v has the type denoted by T . Instances of Dynamic are built with an explicit tagging construct and inspected with a type safe typecase construct. This paper explores the syntax, operational semantics, and denotational semantics of a simple language that includes the type Dynamic . We give examples of how dynamically typed values can be used in programming. Then we discuss an operational semantics for our language and obtain a soundness theorem. We present two formulations of the denotational semantics of this language and relate them to the operational semantics. Finally, we consider the implications of polymorphism and some implementation issues.",
    "cited_by_count": 291,
    "openalex_id": "https://openalex.org/W1987203566",
    "type": "article"
  },
  {
    "title": "Supporting dynamic data structures on distributed-memory machines",
    "doi": "https://doi.org/10.1145/201059.201065",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Anne Rogers; Martin C. Carlisle; John Reppy; Laurie Hendren",
    "corresponding_authors": "",
    "abstract": "Compiling for distributed-memory machines has been a very active research area in recent years. Much of this work has concentrated on programs that use arrays as their primary data structures. To date, little work has been done to address the problem of supporting programs that use pointer-based dynamic data structures. The techniques developed for supporting SPMD execution of array-based programs rely on the fact that arrays are statically defined and directly addressable. Recursive data structures do not have these properties, so new techniques must be developed. In this article, we describe an execution model for supporting programs that use pointer-based dynamic data structures. This model uses a simple mechanism for migrating a thread of control based on the layout of heap-allocated data and introduces parallelism using a technique based on futures and lazy task creation. We intend to exploit this execution model using compiler analyses and automatic parallelization techniques. We have implemented a prototype system, which we call Olden , that runs on the Intel iPSC/860 and the Thinking Machines CM-5. We discuss our implementation and report on experiments with five benchmarks.",
    "cited_by_count": 287,
    "openalex_id": "https://openalex.org/W2154554979",
    "type": "article"
  },
  {
    "title": "Finite Differencing of Computable Expressions",
    "doi": "https://doi.org/10.1145/357172.357177",
    "publication_date": "1982-07-01",
    "publication_year": 1982,
    "authors": "Robert Paige; Shaye Koenig",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Finite Differencing of Computable Expressions Authors: Robert Paige Department of Computer Science, Hill Center for the Mathematical Sciences, Rutgers--The State University of New Jersey, Busch Campus, New Brunswick, NJ Department of Computer Science, Hill Center for the Mathematical Sciences, Rutgers--The State University of New Jersey, Busch Campus, New Brunswick, NJView Profile , Shaye Koenig Department of Computer Science, Hill Center for the Mathematical Sciences, Rutgers--The State University of New Jersey, Busch Campus, New Brunswick, NJ Department of Computer Science, Hill Center for the Mathematical Sciences, Rutgers--The State University of New Jersey, Busch Campus, New Brunswick, NJView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 3pp 402–454https://doi.org/10.1145/357172.357177Published:01 July 1982Publication History 231citation1,031DownloadsMetricsTotal Citations231Total Downloads1,031Last 12 Months64Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 286,
    "openalex_id": "https://openalex.org/W1992808564",
    "type": "article"
  },
  {
    "title": "Context-sensitive synchronization-sensitive analysis is undecidable",
    "doi": "https://doi.org/10.1145/349214.349241",
    "publication_date": "2000-03-01",
    "publication_year": 2000,
    "authors": "G. Ramalingam",
    "corresponding_authors": "G. Ramalingam",
    "abstract": "Static program analysis is concerned with the computation of approximations of the runtime behavior of programs. Precise information about a program's runtime behavior is, in general, uncomputable for various different reasons, and each reason may necessitate making certain approximations in the information computed. This article illustrates one source of difficulty in static analysis of concurrent programs. Specifically, the article shows that an analysis that is simultaneously both context-sensitive and synchronization-sensitive (that is, a context-sensitive analysis that precisely takes into account the constraints on execution order imposed by the synchronization statements in the program) is impossible even for the simplest of analysis problems.",
    "cited_by_count": 286,
    "openalex_id": "https://openalex.org/W2060345611",
    "type": "article"
  },
  {
    "title": "A generalization of Dijkstra's calculus",
    "doi": "https://doi.org/10.1145/69558.69559",
    "publication_date": "1989-10-01",
    "publication_year": 1989,
    "authors": "Greg Nelson",
    "corresponding_authors": "Greg Nelson",
    "abstract": "Dijsktra's calculus of guarded commands can be generalized and simplified by dropping the law of the excluded miracle. This paper gives a self-contained account of the generalized calculus from first principles through the semantics of recursion. The treatment of recursion uses the fixpoint method from denotational semantics. The paper relies only on the algebraic properties of predicates; individual states are not mentioned (except for motivation). To achieve this, we apply the correspondence between programs and predicates that underlies predicative programming. The paper is written from the axiomatic semantic point of view, but its contents can be described from the denotational semantic point of view roughly as follows: The Plotkin-Apt correspondence between wp semantics and the Smyth powerdomain is extended to a correspondence between the full wp/wlp semantics and the Plotkin powerdomain extended with the empty set.",
    "cited_by_count": 284,
    "openalex_id": "https://openalex.org/W2030865387",
    "type": "article"
  },
  {
    "title": "Basic Techniques for the Efficient Coordination of Very Large Numbers of Cooperating Sequential Processors",
    "doi": "https://doi.org/10.1145/69624.357206",
    "publication_date": "1983-04-01",
    "publication_year": 1983,
    "authors": "Allan Gottlieb; Boris D. Lubachevsky; Larry Rudolph",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Basic Techniques for the Efficient Coordination of Very Large Numbers of Cooperating Sequential Processors Authors: Allan Gottlieb Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NY Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NYView Profile , Boris D. Lubachevsky Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NY Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NYView Profile , Larry Rudolph Department of Computer Science, Carnegie-Mellon University, Pittsburgh, PA Department of Computer Science, Carnegie-Mellon University, Pittsburgh, PAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 2April 1983 pp 164–189https://doi.org/10.1145/69624.357206Published:01 April 1983Publication History 220citation669DownloadsMetricsTotal Citations220Total Downloads669Last 12 Months31Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 283,
    "openalex_id": "https://openalex.org/W2047434043",
    "type": "article"
  },
  {
    "title": "Local type inference",
    "doi": "https://doi.org/10.1145/345099.345100",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Benjamin C. Pierce; David N. Turner",
    "corresponding_authors": "",
    "abstract": "We study two partial type inference methods for a language combining subtyping and impredicative polymorphism. Both methods are local in the sense that missing annotations are recovered using only information from adjacent nodes in the syntax tree, without long-distance constraints such as unification variables. One method infers type arguments in polymorphic applications using a local constraint solver. The other infers annotations on bound variables in function abstractions by propagating type constraints downward from enclosing application nodes. We motivate our design choices by a statistical analysis of the uses of type inference in a sizable body of existing ML code.",
    "cited_by_count": 282,
    "openalex_id": "https://openalex.org/W2036676170",
    "type": "article"
  },
  {
    "title": "The POLYLITH software bus",
    "doi": "https://doi.org/10.1145/174625.174629",
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "James Purtilo",
    "corresponding_authors": "James Purtilo",
    "abstract": "We describe a system called POLYLITH that helps programmers prepare and interconnect mixed-language software components for execution in heterogeneous environments. POLYLITH's principal benefit is that programmers are free to implement functional requirements separately from their treatment of interfacing requirements; this means that once an application has been developed for use in one execution environment (such as a distributed network) it can be adapted for reuse in other environments (such as a shared-memory multiprocessor) by automatic techniques. This flexibility is provided without loss of performance. We accomplish this by creating a new run-time organization for software. An abstract decoupling agent, called the software bus , is introduced between the system components. Heterogeneity in language and architecture is accommodated since program units are prepared to interface directly to the bus and not to other program units. Programmers specify application structure in terms of a module interconnection language (MIL); POLYLITH uses this specification to guide packaging (static interfacing activities such as stub generation, source program adaptation, compilation, and linking). At run time, an implementation of the bus abstraction may assist in message delivery, name service, or system reconfiguration.",
    "cited_by_count": 281,
    "openalex_id": "https://openalex.org/W2125294767",
    "type": "article"
  },
  {
    "title": "Compiler techniques for code compaction",
    "doi": "https://doi.org/10.1145/349214.349233",
    "publication_date": "2000-03-01",
    "publication_year": 2000,
    "authors": "Saumya Debray; William Evans; Robert Muth; Bjorn De Sutter",
    "corresponding_authors": "",
    "abstract": "In recent years there has been an increasing trend toward the incorpor ation of computers into a variety of devices where the amount of memory available is limited. This makes it desirable to try to reduce the size of applications where possible. This article explores the use of compiler techniques to accomplish code compaction to yield smaller executables. The main contribution of this article is to show that careful, aggressive, interprocedural optimization, together with procedural abstraction of repeated code fragments, can yield significantly better reductions in code size than previous approaches, which have generally focused on abstraction of repeated instruction sequences. We also show how “equivalent” code fragments can be detected and factored out using conventional compiler techniques, and without having to resort to purely linear treatments of code sequences as in suffix-tree-based approaches, thereby setting up a framework for code compaction that can be more flexible in its treatment of what code fragments are considered equivalent. Our ideas have been implemented in the form of a binary-rewriting tool that reduces the size of executables by about 30% on the average.",
    "cited_by_count": 279,
    "openalex_id": "https://openalex.org/W1969031936",
    "type": "article"
  },
  {
    "title": "Postpass Code Optimization of Pipeline Constraints",
    "doi": "https://doi.org/10.1145/2166.357217",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "John L. Hennessy; Thomas R. Gross",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Postpass Code Optimization of Pipeline Constraints Authors: John L. Hennessy Computer Systems Laboratory, Departments of Electrical Engineering and Computer Science, Stanford University, Stanford, CA Computer Systems Laboratory, Departments of Electrical Engineering and Computer Science, Stanford University, Stanford, CAView Profile , Thomas Gross Computer Systems Laboratory, Departments of Electrical Engineering and Computer Science, Stanford University, Stanford, CA Computer Systems Laboratory, Departments of Electrical Engineering and Computer Science, Stanford University, Stanford, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3pp 422–448https://doi.org/10.1145/2166.357217Published:01 July 1983Publication History 269citation1,332DownloadsMetricsTotal Citations269Total Downloads1,332Last 12 Months136Last 6 weeks20 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 275,
    "openalex_id": "https://openalex.org/W2048936605",
    "type": "article"
  },
  {
    "title": "Cache miss equations",
    "doi": "https://doi.org/10.1145/325478.325479",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "Somnath Ghosh; Margaret Martonosi; Sharad Malik",
    "corresponding_authors": "",
    "abstract": "With the ever-widening performance gap between processors and main memory, cache memory, which is used to bridge this gap, is becoming more and more significant. Caches work well for programs that exhibit sufficient locality. Other programs, however, have reference patterns that fail to exploit the cache, thereby suffering heavily from high memory latency. In order to get high cache efficiency and achieve good program performance, efficient memory accessing behavior is necessary. In fact, for many programs, program transformations or source-code changes can radically alter memory access patterns, significantly improving cache performance. Both hand-tuning and compiler optimization techniques are often used to transform codes to improve cache utilization. Unfortunately, cache conflicts are difficult to predict and estimate, precluding effective transformations. Hence, effective transformations require detailed knowledge about the frequency and causes of cache misses in the code. This article describes methods for generating and solving Cache Miss Equations (CMEs) that give a detailed representation of cache behavior, including conflict misses, in loop-oriented scientific code. Implemented within the SUIF compiler framework, our approach extends traditional compiler reuse analysis to generate linear Diophantine equations that summarize each loop's memory behavior. While solving these equations is in general difficult, we show that is also unnecessary, as mathematical techniques for manipulating Diophantine equations allow us to relatively easily compute and/or reduce the number of possible solutions, where each solution corresponds to a potential cache miss. The mathematical precision of CMEs allows us to find true optimal solutions for transformations such as blocking or padding. The generality of CMEs also allows us to reason about interactions between transformations applied in concert. The article also gives examples of their use to determine array padding and offset amounts that minimize cache misses, and to determine optimal blocking factors for tiled code. Overall, these equations represent an analysis framework that offers the generality and precision needed for detailed compiler optimizations.",
    "cited_by_count": 271,
    "openalex_id": "https://openalex.org/W2158737060",
    "type": "article"
  },
  {
    "title": "Iterated register coalescing",
    "doi": "https://doi.org/10.1145/229542.229546",
    "publication_date": "1996-05-01",
    "publication_year": 1996,
    "authors": "Lal George; Andrew W. Appel",
    "corresponding_authors": "",
    "abstract": "An important function of any register allocator is to target registers so as to eliminate copy instructions. Graph-coloring register allocation is an elegant approach to this problem. If the source and destination of a move instruction do not interfere, then their nodes can be coalesced in the interference graph. Chaitin's coalescing heuristic could make a graph uncolorable (i.e., introduce spills); Briggs et al. demonstrated a conservative coalescing heuristic that preserves colorability. But Briggs's algorithm is too conservative and leaves too many move instructions in our programs. We show how to interleave coloring reductions with Briggs's coalescing heuristic, leading to an algorithm that is safe but much more aggressive.",
    "cited_by_count": 270,
    "openalex_id": "https://openalex.org/W2142550124",
    "type": "article"
  },
  {
    "title": "The specification statement",
    "doi": "https://doi.org/10.1145/44501.44503",
    "publication_date": "1988-07-01",
    "publication_year": 1988,
    "authors": "Carroll Morgan",
    "corresponding_authors": "Carroll Morgan",
    "abstract": "Dijkstra's programming language is extended by specification statements , which specify parts of a program “yet to be developed.” A weakest precondition semantics is given for these statements so that the extended language has a meaning as precise as the original. The goal is to improve the development of programs, making it closer to manipulations within a single calculus. The extension does this by providing one semantic framework for specifications and programs alike: Developments begin with a program (a single specification statement) and end with a program (in the executable language). And the notion of refinement or satisfaction , which normally relates a specification to its possible implementations, is automatically generalized to act between specifications and between programs as well. A surprising consequence of the extension is the appearance of miracles : program fragments that do not satisfy Dijkstra's Law of the Excluded Miracle . Uses for them are suggested.",
    "cited_by_count": 265,
    "openalex_id": "https://openalex.org/W2011444209",
    "type": "article"
  },
  {
    "title": "Information flow inference for ML",
    "doi": "https://doi.org/10.1145/596980.596983",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "François Pottier; Vincent Simonet",
    "corresponding_authors": "",
    "abstract": "This paper presents a type-based information flow analysis for a call-by-value λ-calculus equipped with references, exceptions and let-polymorphism, which we refer to as ML. The type system is constraint-based and has decidable type inference. Its noninterference proof is reasonably light-weight, thanks to the use of a number of orthogonal techniques. First, a syntactic segregation between values and expressions allows a lighter formulation of the type system. Second, noninterference is reduced to subject reduction for a nonstandard language extension. Lastly, a semi-syntactic approach to type soundness allows dealing with constraint-based polymorphism separately.",
    "cited_by_count": 262,
    "openalex_id": "https://openalex.org/W2061056245",
    "type": "article"
  },
  {
    "title": "Distributed cooperation with action systems",
    "doi": "https://doi.org/10.1145/48022.48023",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "R. J. R. Back; F. Kurki-Suonio",
    "corresponding_authors": "",
    "abstract": "Action systems provide a method to program distributed systems that emphasizes the overall behavior of the system. System behavior is described in terms of the possible interactions (actions) that the processes can engage in, rather than in terms of the sequential code that the processes execute. The actions provide a symmetric communication mechanism that permits an arbitrary number of processes to be synchronized by a common handshake. This is a generalization of the usual approach, employed in languages like CSP and Ada, in which communication is asymmetric and restricted to involve only two processes. Two different execution models are given for action systems: a sequential one and a concurrent one. The sequential model is easier to use for reasoning, and is essentially equivalent to the guarded iteration statement by Dijkstra. It is well suited for reasoning about system properties in temporal logic, but requires a stronger fairness notion than it is reasonable to assume a distributed implementation will support. The concurrent execution model reflects the true concurrency that is present in a distributed execution, and corresponds to the way in which the system is actually implemented. An efficient distributed implementation of action systems on a local area network is described. The fairness assumptions of the concurrent model can be guaranteed in this implementation. The relationship between the two execution models is studied in detail in the paper. For systems that will be called fairly serializable, the two models are shown to be equivalent. Proof methods are given for verifying this property of action systems. It is shown that for fairly serializable systems, properties that hold for any concurrent execution of the system can be established by temporal proofs that are conducted entirely within the simpler sequential execution model.",
    "cited_by_count": 261,
    "openalex_id": "https://openalex.org/W2005952505",
    "type": "article"
  },
  {
    "title": "Incremental Context-Dependent Analysis for Language-Based Editors",
    "doi": "https://doi.org/10.1145/2166.357218",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Thomas Reps; Tim Teitelbaum; Alan Demers",
    "corresponding_authors": "",
    "abstract": "article Free AccessIncremental Context-Dependent Analysis for Language-Based Editors Authors: Thomas Reps Department of Computer Science, Upson Hall, Cornell University, Ithaca, NY Department of Computer Science, Upson Hall, Cornell University, Ithaca, NYView Profile , Tim Teitelbaum Department of Computer Science, Upson Hall, Cornell University, Ithaca, NY Department of Computer Science, Upson Hall, Cornell University, Ithaca, NYView Profile , Alan Demers Department of Computer Science, Upson Hall, Cornell University, Ithaca, NY Department of Computer Science, Upson Hall, Cornell University, Ithaca, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3July 1983 pp 449–477https://doi.org/10.1145/2166.357218Published:01 July 1983Publication History 260citation734DownloadsMetricsTotal Citations260Total Downloads734Last 12 Months32Last 6 weeks13 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 259,
    "openalex_id": "https://openalex.org/W2029068974",
    "type": "article"
  },
  {
    "title": "Type classes in Haskell",
    "doi": "https://doi.org/10.1145/227699.227700",
    "publication_date": "1996-03-01",
    "publication_year": 1996,
    "authors": "Cordelia Hall; Kevin Hammond; Simon Jones; Philip Wadler",
    "corresponding_authors": "",
    "abstract": "This article defines a set of type inference rules for resolving overloading introduced by type classes, as used in the functional programming language Haskell. Programs including type classes are transformed into ones which may be typed by standard Hindley-Milner inference rules. In contrast to other work on type classes, the rules presented here relate directly to Haskell programs. An innovative aspect of this work is the use of second-order lambda calculus to record type information in the transformed program.",
    "cited_by_count": 255,
    "openalex_id": "https://openalex.org/W2095136535",
    "type": "article"
  },
  {
    "title": "A framework for call graph construction algorithms",
    "doi": "https://doi.org/10.1145/506315.506316",
    "publication_date": "2001-11-01",
    "publication_year": 2001,
    "authors": "David Grove; Craig Chambers",
    "corresponding_authors": "",
    "abstract": "A large number of call graph construction algorithms for object-oriented and functional languages have been proposed, each embodying different tradeoffs between analysis cost and call graph precision. In this article we present a unifying framework for understanding call graph construction algorithms and an empirical comparison of a representative set of algorithms. We first present a general parameterized algorithm that encompasses many well-known and novel call graph construction algorithms. We have implemented this general algorithm in the Vortex compiler infrastructure, a mature, multilanguage, optimizing compiler. The Vortex implementation provides a \"level playing field\" for meaningful cross-algorithm performance comparisons. The costs and benefits of a number of call graph construction algorithms are empirically assessed by applying their Vortex implementation to a suite of sizeable (5,000 to 50,000 lines of code) Cecil and Java programs. For many of these applications, interprocedural analysis enabled substantial speed-ups over an already highly optimized baseline. Furthermore, a significant fraction of these speed-ups can be obtained through the use of a scalable, near-linear time call graph construction algorithm.",
    "cited_by_count": 254,
    "openalex_id": "https://openalex.org/W2117426803",
    "type": "article"
  },
  {
    "title": "Traits",
    "doi": "https://doi.org/10.1145/1119479.1119483",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Sté́phane Ducasse; Oscar Nierstrasz; Nathanael Schärli; Roel Wuyts; Andrew P. Black",
    "corresponding_authors": "",
    "abstract": "Inheritance is well-known and accepted as a mechanism for reuse in object-oriented languages. Unfortunately, due to the coarse granularity of inheritance, it may be difficult to decompose an application into an optimal class hierarchy that maximizes software reuse. Existing schemes based on single inheritance, multiple inheritance, or mixins, all pose numerous problems for reuse. To overcome these problems we propose traits , pure units of reuse consisting only of methods. We develop a formal model of traits that establishes how traits can be composed, either to form other traits, or to form classes. We also outline an experimental validation in which we apply traits to refactor a nontrivial application into composable units.",
    "cited_by_count": 250,
    "openalex_id": "https://openalex.org/W2111898165",
    "type": "article"
  },
  {
    "title": "A machine-checked model for a Java-like language, virtual machine, and compiler",
    "doi": "https://doi.org/10.1145/1146809.1146811",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Gerwin Klein; Tobias Nipkow",
    "corresponding_authors": "",
    "abstract": "We introduce Jinja, a Java-like programming language with a formal semantics designed to exhibit core features of the Java language architecture. Jinja is a compromise between the realism of the language and the tractability and clarity of its formal semantics. The following aspects are formalised: a big and a small step operational semantics for Jinja and a proof of their equivalence, a type system and a definite initialisation analysis, a type safety proof of the small step semantics, a virtual machine (JVM), its operational semantics and its type system, a type safety proof for the JVM; a bytecode verifier, that is, a data flow analyser for the JVM, a correctness proof of the bytecode verifier with respect to the type system, and a compiler and a proof that it preserves semantics and well-typedness. The emphasis of this work is not on particular language features but on providing a unified model of the source language, the virtual machine, and the compiler. The whole development has been carried out in the theorem prover Isabelle/HOL.",
    "cited_by_count": 246,
    "openalex_id": "https://openalex.org/W2052735108",
    "type": "article"
  },
  {
    "title": "An old-fashioned recipe for real time",
    "doi": "https://doi.org/10.1145/186025.186058",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Martı́n Abadi; Leslie Lamport",
    "corresponding_authors": "",
    "abstract": "Traditional methods for specifying and reasoning about concurrent systems work for real-time systems. Using TLA (the temporal logic of actions), we illustrate how they work with the examples of a queue and of a mutual-exclusion protocol. In general, two problems must be addressed: avoiding the real-time programming version of Zeno's paradox, and coping with circularities when composing real-time assumption/guarantee specifications. Their solutions rest on properties of machine closure and realizability.",
    "cited_by_count": 245,
    "openalex_id": "https://openalex.org/W2076513239",
    "type": "article"
  },
  {
    "title": "Distributed Termination",
    "doi": "https://doi.org/10.1145/357084.357087",
    "publication_date": "1980-01-01",
    "publication_year": 1980,
    "authors": "Nissim Francez",
    "corresponding_authors": "Nissim Francez",
    "abstract": "Discussed is a distributed system based on communication among disjoint processes, where each process is capable of achieving a post-condition of its local space in such a way that the conjunction of local post-conditions implies a global post-condition of the whole system. The system is then augmented with extra control communication in order to achieve distributed termination, without adding new channels of communication. The algorithm is applied to a problem of constructing a sorted partition.",
    "cited_by_count": 244,
    "openalex_id": "https://openalex.org/W2296352387",
    "type": "article"
  },
  {
    "title": "Linearity and the pi-calculus",
    "doi": "https://doi.org/10.1145/330249.330251",
    "publication_date": "1999-09-01",
    "publication_year": 1999,
    "authors": "Naoki Kobayashi; Benjamin C. Pierce; David N. Turner",
    "corresponding_authors": "",
    "abstract": "The economy and flexibility of the pi-calculus make it an attractive object of theoretical study and a clean basis for concurrent language design and implementation. However, such generality has a cost: encoding higher-level features like functional computation in pi-calculus throws away potentially useful information. We show how a linear type system can be used to recover important static information about a process's behavior. In particular, we can guarantee that two processes communicating over a linear channel cannot interfere with other communicating processes. After developing standard results such as soundness of typing, we focus on equivalences, adapting the standard notion of barbed bisimulation to the linear setting and showing how reductions on linear channels induce a useful “partial confluence” of process behaviors. For an extended example of the theory, we prove the validity of a tail-call optimization for higher-order functions represented as processes.",
    "cited_by_count": 243,
    "openalex_id": "https://openalex.org/W2042360145",
    "type": "article"
  },
  {
    "title": "A semantics for advice and dynamic join points in aspect-oriented programming",
    "doi": "https://doi.org/10.1145/1018203.1018208",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Mitchell Wand; Gregor Kiczales; Christopher Dutchyn",
    "corresponding_authors": "",
    "abstract": "A characteristic of aspect-oriented programming, as embodied in Aspect J, is the use of advice and point cuts to define behavior that crosscuts the structure of the rest of the code. The events during execution at which advice may execute are called join points . A pointcut is a set of join points. An advice is an action to be taken at the join points in a particular pointcut. In this model of aspect-oriented programming, join points are dynamic in that they refer to events during the flow of execution of the program.We give a denotational semantics for a minilanguage that embodies the key features of dynamic join points, pointcuts, and advice. This is the first semantics for aspect-oriented programming that handles dynamic join points and recursive procedures. It is intended as a baseline semantics against which future correctness results may be measured.",
    "cited_by_count": 241,
    "openalex_id": "https://openalex.org/W1965418329",
    "type": "article"
  },
  {
    "title": "A structural view of the Cedar programming environment",
    "doi": "https://doi.org/10.1145/6465.6466",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Daniel C. Swinehart; Polle T. Zellweger; Richard J. Beach; Robert B. Hagmann",
    "corresponding_authors": "",
    "abstract": "This paper presents an overview of the Cedar programming environment, focusing on its overall structure—that is, the major components of Cedar and the way they are organized. Cedar supports the development of programs written in a single programming language, also called Cedar. Its primary purpose is to increase the productivity of programmers whose activities include experimental programming and the development of prototype software systems for a high-performance personal computer. The paper emphasizes the extent to which the Cedar language, with run-time support, has influenced the organization, flexibility, usefulness, and stability of the Cedar environment. It highlights the novel system features of Cedar, including automatic storage management of dynamically allocated typed values, a run-time type system that provides run-time access to Cedar data type definitions and allows interpretive manipulation of typed values, and a powerful device-independent imaging model that supports the user interface facilities. Using these discussions to set the context, the paper addresses the language and system features and the methodologies used to facilitate the integration of Cedar applications. A comparison of Cedar with other programming environments further identifies areas where Cedar excels and areas where work remains to be done.",
    "cited_by_count": 235,
    "openalex_id": "https://openalex.org/W2087747577",
    "type": "article"
  },
  {
    "title": "Modern concurrency abstractions for C#",
    "doi": "https://doi.org/10.1145/1018203.1018205",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Nick Benton; Luca Cardelli; Cédric Fournet",
    "corresponding_authors": "",
    "abstract": "Polyphonic C ♯ is an extension of the C ♯ language with new asynchronous concurrency constructs, based on the join calculus. We describe the design and implementation of the language and give examples of its use in addressing a range of concurrent programming problems.",
    "cited_by_count": 233,
    "openalex_id": "https://openalex.org/W2118317839",
    "type": "article"
  },
  {
    "title": "Types for safe locking",
    "doi": "https://doi.org/10.1145/1119479.1119480",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Martı́n Abadi; Cormac Flanagan; Stephen N. Freund",
    "corresponding_authors": "",
    "abstract": "This article presents a static race-detection analysis for multithreaded shared-memory programs, focusing on the Java programming language. The analysis is based on a type system that captures many common synchronization patterns. It supports classes with internal synchronization, classes that require client-side synchronization, and thread-local classes. In order to demonstrate the effectiveness of the type system, we have implemented it in a checker and applied it to over 40,000 lines of hand-annotated Java code. We found a number of race conditions in the standard Java libraries and other test programs. The checker required fewer than 20 additional type annotations per 1,000 lines of code. This article also describes two improvements that facilitate checking much larger programs: an algorithm for annotation inference and a user interface that clarifies warnings generated by the checker. These extensions have enabled us to use the checker for identifying race conditions in large-scale software systems with up to 500,000 lines of code.",
    "cited_by_count": 232,
    "openalex_id": "https://openalex.org/W2166615267",
    "type": "article"
  },
  {
    "title": "An <i>O</i> ( <i>n</i> log <i>n</i> ) Unidirectional Algorithm for the Circular Extrema Problem",
    "doi": "https://doi.org/10.1145/69622.357194",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "Gary L. Peterson",
    "corresponding_authors": "Gary L. Peterson",
    "abstract": "article Free AccessAn O(nlog n) Unidirectional Algorithm for the Circular Extrema Problem Author: Gary L. Peterson Department of Computer Science, University of Rochester, Mathematical Sciences Building, Rochester, NY Department of Computer Science, University of Rochester, Mathematical Sciences Building, Rochester, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 758–762https://doi.org/10.1145/69622.357194Published:01 October 1982Publication History 197citation1,118DownloadsMetricsTotal Citations197Total Downloads1,118Last 12 Months63Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 228,
    "openalex_id": "https://openalex.org/W2017534864",
    "type": "article"
  },
  {
    "title": "Computability classes for enforcement mechanisms",
    "doi": "https://doi.org/10.1145/1111596.1111601",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Kevin W. Hamlen; Greg Morrisett; Fred B. Schneider",
    "corresponding_authors": "",
    "abstract": "A precise characterization of those security policies enforceable by program rewriting is given. This also exposes and rectifies problems in prior work, yielding a better characterization of those security policies enforceable by execution monitors as well as a taxonomy of enforceable security policies. Some but not all classes can be identified with known classes from computational complexity theory.",
    "cited_by_count": 228,
    "openalex_id": "https://openalex.org/W2156456150",
    "type": "article"
  },
  {
    "title": "Type inference with polymorphic recursion",
    "doi": "https://doi.org/10.1145/169701.169692",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Fritz Henglein",
    "corresponding_authors": "Fritz Henglein",
    "abstract": "article Free Access Share on Type inference with polymorphic recursion Author: Fritz Henglein University of Copenhagen University of CopenhagenView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 2April 1993 pp 253–289https://doi.org/10.1145/169701.169692Published:01 April 1993Publication History 139citation1,309DownloadsMetricsTotal Citations139Total Downloads1,309Last 12 Months118Last 6 weeks11 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 227,
    "openalex_id": "https://openalex.org/W1964952791",
    "type": "article"
  },
  {
    "title": "Refinement types for secure implementations",
    "doi": "https://doi.org/10.1145/1890028.1890031",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Jesper Bengtson; Karthikeyan Bhargavan; Cédric Fournet; Andrew D. Gordon; Sergio Maffeis",
    "corresponding_authors": "",
    "abstract": "We present the design and implementation of a typechecker for verifying security properties of the source code of cryptographic protocols and access control mechanisms. The underlying type theory is a λ-calculus equipped with refinement types for expressing pre- and post-conditions within first-order logic. We derive formal cryptographic primitives and represent active adversaries within the type theory. Well-typed programs enjoy assertion-based security properties, with respect to a realistic threat model including key compromise. The implementation amounts to an enhanced typechecker for the general-purpose functional language F # ; typechecking generates verification conditions that are passed to an SMT solver. We describe a series of checked examples. This is the first tool to verify authentication properties of cryptographic protocols by typechecking their source code.",
    "cited_by_count": 226,
    "openalex_id": "https://openalex.org/W2036961426",
    "type": "article"
  },
  {
    "title": "Interprocedural pointer alias analysis",
    "doi": "https://doi.org/10.1145/325478.325519",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "Michael Hind; Michael Burke; Paul Carini; Jong-Deok Choi",
    "corresponding_authors": "",
    "abstract": "We present practical approximation methods for computing and representing interprocedural aliases for a program written in a language that includes pointers, reference parameters, and recursion. We present the following contributions: (1) a framework for interprocedural pointer alias analysis that handles function pointers by constructing the program call graph while alias analysis is being performed; (2) a flow-sensitive interprocedural pointer alias analysis algorithm; (3) a flow-insensitive interprocedural pointer alias analysis algorithm; (4) a flow-insensitive interprocedural pointer alias analysis algorithm that incorporates kill information to improve precision; (5) empirical measurements of the efficiency and precision of the three interprocedural alias analysis algorithms.",
    "cited_by_count": 222,
    "openalex_id": "https://openalex.org/W1996146601",
    "type": "article"
  },
  {
    "title": "An Improved Context-Free Recognizer",
    "doi": "https://doi.org/10.1145/357103.357112",
    "publication_date": "1980-07-01",
    "publication_year": 1980,
    "authors": "Susan L. Graham; Michael A. Harrison; Walter L. Ruzzo",
    "corresponding_authors": "",
    "abstract": "A new algorithm for recognizing and parsing arbitrary context-free languages is presented, and several new results are given on the computational complexity of these problems. The new algorithm is of both practical and theoretical interest. It is conceptually simple and allows a variety of efficient implementations, which are worked out in detail. Two versions are given which run in faster than cubic time. Surprisingly close connections between the Cocke-Kasami-Younger and Earley algorithms are established which reveal that the two algorithms are “almost” identical.",
    "cited_by_count": 222,
    "openalex_id": "https://openalex.org/W2047092246",
    "type": "article"
  },
  {
    "title": "Data Abstraction, Implementation, Specification, and Testing",
    "doi": "https://doi.org/10.1145/357139.357140",
    "publication_date": "1981-07-01",
    "publication_year": 1981,
    "authors": "John D. Gannon; Paul R. McMullin; Richard Hamlet",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Data Abstraction, Implementation, Specification, and Testing Authors: John Gannon Department of Computer Science, University of Maryland, College Park, MD Department of Computer Science, University of Maryland, College Park, MDView Profile , Paul McMullin Department of Computer Science, University of Maryland, College Park, MD Department of Computer Science, University of Maryland, College Park, MDView Profile , Richard Hamlet Department of Computer Science, University of Maryland, College Park, MD Department of Computer Science, University of Maryland, College Park, MDView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 3pp 211–223https://doi.org/10.1145/357139.357140Published:01 July 1981Publication History 215citation1,003DownloadsMetricsTotal Citations215Total Downloads1,003Last 12 Months109Last 6 weeks10 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 222,
    "openalex_id": "https://openalex.org/W2162661340",
    "type": "article"
  },
  {
    "title": "Remote evaluation",
    "doi": "https://doi.org/10.1145/88616.88631",
    "publication_date": "1990-10-01",
    "publication_year": 1990,
    "authors": "James W. Stamos; David K. Gifford",
    "corresponding_authors": "",
    "abstract": "A new technique for computer-to-computer communication is presented that can increase the performance of distributed systems. This technique, called remote evaluation, lets one computer send another computer a request in the form of a program. A computer that receives such a request executes the program in the request and returns the results to the sending computer. Remote evaluation provides a new degree of flexibility in the design of distributed systems. In present distributed systems that use remote procedure calls, server computers are designed to offer a fixed set of services. In a system that uses remote evaluation, server computers are more properly viewed as programmable processors. One consequence of this flexibility is that remote evaluation can reduce the amount of communication that is required to accomplish a given task. In this paper we discuss the semantics of remote evaluation and its effect on distributed system design. We also summarize our experience with a prototype implementation.",
    "cited_by_count": 221,
    "openalex_id": "https://openalex.org/W2296073022",
    "type": "article"
  },
  {
    "title": "Analysis of recursive state machines",
    "doi": "https://doi.org/10.1145/1075382.1075387",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Rajeev Alur; Michael Benedikt; Kousha Etessami; Patrice Godefroid; Thomas Reps; Mihalis Yannakakis",
    "corresponding_authors": "",
    "abstract": "Recursive state machines (RSMs) enhance the power of ordinary state machines by allowing vertices to correspond either to ordinary states or to potentially recursive invocations of other state machines. RSMs can model the control flow in sequential imperative programs containing recursive procedure calls. They can be viewed as a visual notation extending Statecharts-like hierarchical state machines, where concurrency is disallowed but recursion is allowed. They are also related to various models of pushdown systems studied in the verification and program analysis communities.After introducing RSMs and comparing their expressiveness with other models, we focus on whether verification can be efficiently performed for RSMs. Our first goal is to examine the verification of linear time properties of RSMs. We begin this study by dealing with two key components for algorithmic analysis and model checking, namely, reachability (Is a target state reachable from initial states?) and cycle detection (Is there a reachable cycle containing an accepting state?). We show that both these problems can be solved in time O ( n θ 2 ) and space O ( n θ), where n is the size of the recursive machine and θ is the maximum, over all component state machines, of the minimum of the number of entries and the number of exits of each component. From this, we easily derive algorithms for linear time temporal logic model checking with the same complexity in the model. We then turn to properties in the branching time logic CTL*, and again demonstrate a bound linear in the size of the state machine, but only for the case of RSMs with a single exit node.",
    "cited_by_count": 219,
    "openalex_id": "https://openalex.org/W2151670874",
    "type": "article"
  },
  {
    "title": "Information-flow and data-flow analysis of while-programs",
    "doi": "https://doi.org/10.1145/2363.2366",
    "publication_date": "1985-01-02",
    "publication_year": 1985,
    "authors": "Jean-Francois Bergeretti; Bernard Carré",
    "corresponding_authors": "",
    "abstract": "Until recently, information-flow analysis has been used primarily to verify that information transmission between program variables cannot violate security requirements. Here, the notion of information flow is explored as an aid to program development and validation. Information-flow relations are presented for while-programs, which identify those program statements whose execution may cause information to be transmitted from or to particular input, internal, or output values. It is shown with examples how these flow relations can be helpful in writing, testing, and updating programs; they also usefully extend the class of errors which can be detected automatically in the “static analysis” of a program.",
    "cited_by_count": 210,
    "openalex_id": "https://openalex.org/W1980767016",
    "type": "article"
  },
  {
    "title": "A theory of contracts for Web services",
    "doi": "https://doi.org/10.1145/1538917.1538920",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Giuseppe Castagna; Nils Gesbert; Luca Padovani",
    "corresponding_authors": "",
    "abstract": "Contracts are behavioral descriptions of Web services. We devise a theory of contracts that formalizes the compatibility of a client with a service, and the safe replacement of a service with another service. The use of contracts statically ensures the successful completion of every possible interaction between compatible clients and services. The technical device that underlies the theory is the filter , which is an explicit coercion preventing some possible behaviors of services and, in doing so, make services compatible with different usage scenarios. We show that filters can be seen as proofs of a sound and complete subcontracting deduction system which simultaneously refines and extends Hennessy's classical axiomatization of the must testing preorder. The relation is decidable, and the decision algorithm is obtained via a cut-elimination process that proves the coherence of subcontracting as a logical system. Despite the richness of the technical development, the resulting approach is based on simple ideas and basic intuitions. Remarkably, its application is mostly independent of the language used to program the services or the clients. We outline the practical aspects of our theory by studying two different concrete syntaxes for contracts and applying each of them to Web services languages. We also explore implementation issues of filters and discuss the perspectives of future research this work opens.",
    "cited_by_count": 209,
    "openalex_id": "https://openalex.org/W2032399648",
    "type": "article"
  },
  {
    "title": "Dynamic software updating",
    "doi": "https://doi.org/10.1145/1108970.1108971",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Michael Hicks; Scott Nettles",
    "corresponding_authors": "",
    "abstract": "Many important applications must run continuously and without interruption, and yet also must be changed to fix bugs or upgrade functionality. No prior general-purpose methodology for dynamic updating achieves a practical balance between flexibility, robustness, low overhead, ease of use, and low cost.We present an approach for C-like languages that provides type-safe dynamic updating of native code in an extremely flexible manner---code, data, and types may be updated, at programmer-determined times---and permits the use of automated tools to aid the programmer in the updating process. Our system is based on dynamic patches that contain both the updated code and the code needed to transition from the old version to the new. A novel aspect of our patches is that they consist of verifiable native code (e.g. Proof-Carrying Code or Typed Assembly Language), which is native code accompanied by annotations that allow online verification of the code's safety. We discuss how patches are generated mostly automatically, how they are applied using dynamic-linking technology, and how code is compiled to make it updateable.To concretely illustrate our system, we have implemented a dynamically updateable web server, FlashEd. We discuss our experience building and maintaining FlashEd, and generalize to present observations about updateable software development. Performance experiments show that for FlashEd, the overhead due to updating is low: typically less than 1 percent.",
    "cited_by_count": 209,
    "openalex_id": "https://openalex.org/W2047401676",
    "type": "article"
  },
  {
    "title": "Optimal code motion",
    "doi": "https://doi.org/10.1145/183432.183443",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Jens Knoop; Oliver Rüthing; Bernhard Steffen",
    "corresponding_authors": "",
    "abstract": "An implementation-oriented algorithm for lazy code motion is presented that minimizes the number of computations in programs while suppressing any unnecessary code motion in order to avoid superfluous register pressure. In particular, this variant of the original algorithm for lazy code motion works on flowgraphs whose nodes are basic blocks rather than single statements, since this format is standard in optimizing compilers. The theoretical foundations of the modified algorithm are given in the first part, where t -refined flowgraphs are introduced for simplifying the treatment of flow graphs whose nodes are basic blocks. The second part presents the “basic block” algorithm in standard notation and gives directions for its implementation in standard compiler environments.",
    "cited_by_count": 205,
    "openalex_id": "https://openalex.org/W2058360616",
    "type": "article"
  },
  {
    "title": "The pitfalls of verifying floating-point computations",
    "doi": "https://doi.org/10.1145/1353445.1353446",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "David Monniaux",
    "corresponding_authors": "David Monniaux",
    "abstract": "Current critical systems often use a lot of floating-point computations, and thus the testing or static analysis of programs containing floating-point operators has become a priority. However, correctly defining the semantics of common implementations of floating-point is tricky, because semantics may change according to many factors beyond source-code level, such as choices made by compilers. We here give concrete examples of problems that can appear and solutions for implementing in analysis software.",
    "cited_by_count": 204,
    "openalex_id": "https://openalex.org/W2130084210",
    "type": "article"
  },
  {
    "title": "Efficient implementation of lattice operations",
    "doi": "https://doi.org/10.1145/59287.59293",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "Hassan Aı̈t-Kaci; Robert S. Boyer; Patrick Lincoln; Roger Nasr",
    "corresponding_authors": "",
    "abstract": "Lattice operations such as greatest lower bound (GLB), least upper bound (LUB), and relative complementation (BUTNOT) are becoming more and more important in programming languages supporting object inheritance. We present a general technique for the efficient implementation of such operations based on an encoding method. The effect of the encoding is to plunge the given ordering into a boolean lattice of binary words, leading to an almost constant time complexity of the lattice operations. A first method is described based on a transitive closure approach. Then a more space-efficient method minimizing code-word length is described. Finally a powerful grouping technique called modulation is presented, which drastically reduces code space while keeping all three lattice operations highly efficient. This technique takes into account idiosyncrasies of the topology of the poset being encoded that are quite likely to occur in practice. All methods are formally justified. We see this work as an original contribution towards using semantic (vz., in this case, taxonomic) information in the engineering pragmatics of storage and retrieval of (vz., partially or quasi-ordered) information.",
    "cited_by_count": 202,
    "openalex_id": "https://openalex.org/W1999978255",
    "type": "article"
  },
  {
    "title": "The design, implementation, and evaluation of Jade",
    "doi": "https://doi.org/10.1145/291889.291893",
    "publication_date": "1998-05-01",
    "publication_year": 1998,
    "authors": "Martin Rinard; Monica S. Lam",
    "corresponding_authors": "",
    "abstract": "Jade is a portable, implicitly parallel language designed for exploiting task-level concurrency.Jade programmers start with a program written in a standard serial, imperative language, then use Jade constructs to declare how parts of the program access data. The Jade implementation uses this data access information to automatically extract the concurrency and map the application onto the machine at hand. The resulting parallel execution preserves the semantics of the original serial program. We have implemented Jade as an extension to C, and Jade implementations exist for s hared-memory multiprocessors, homogeneous message-passing machines, and heterogeneous networks of workstations. In this atricle we discuss the design goals and decisions that determined the final form of Jade and present an overview of the Jade implementation. We also present our experience using Jade to implement several complete scientific and engineering applications. We use this experience to evaluate how the different Jade language features were used in practice and how well Jade as a whole supports the process of developing parallel applications. We find that the basic idea of preserving the serial semantics simplifies the program development process, and that the concept of using data access specifications to guide the parallelization offers significant advantages over more traditional control-based approaches. We also find that the Jade data model can interact poorly with concurrency patterns that write disjoint pieces of a single aggregate data structure, although this problem arises in only one of the applications.",
    "cited_by_count": 201,
    "openalex_id": "https://openalex.org/W2028267160",
    "type": "article"
  },
  {
    "title": "Compiling language definitions",
    "doi": "https://doi.org/10.1145/567097.567099",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Mark van den Brand; Jan Heering; Paul Klint; P.A.S. Olivier",
    "corresponding_authors": "",
    "abstract": "The ASF+SDF Meta-Environment is an interactive language development environment whose main application areas are definition and implementation of domain-specific languages, generation of program analysis and transformation tools, and production of software renovation tools. It uses conditional rewrite rules to define the dynamic semantics and other tool-oriented aspects of languages, so the effectiveness of the generated tools is critically dependent on the quality of the rewrite rule implementation. The ASF+SDF rewrite rule compiler generates C code, thus taking advantage of C's portability and the sophisticated optimization capabilities of current C compilers as well as avoiding potential abstract machine interface bottlenecks. It can handle large (10,000+ rule) language definitions and uses an efficient run-time storage scheme capable of handling large (1,000,000+ node) terms. Term storage uses maximal subterm sharing (hash-consing), which turns out to be more effective in the case of ASF+SDF than in Lisp or SML. Extensive benchmarking has shown the time and space performance of the generated code to be as good as or better than that of the best current rewrite rule and functional language compilers.",
    "cited_by_count": 201,
    "openalex_id": "https://openalex.org/W2094080019",
    "type": "article"
  },
  {
    "title": "Proofs as programs",
    "doi": "https://doi.org/10.1145/2363.2528",
    "publication_date": "1985-01-02",
    "publication_year": 1985,
    "authors": "Joseph L. Bates; Robert L. Constable",
    "corresponding_authors": "",
    "abstract": "The significant intellectual cost of programming is for problem solving and explaining, not for coding. Yet programming systems offer mechanical assistance for the coding process exclusively. We illustrate the use of an implemented program development system, called PRL (\"pearl\"), that provides automated assistance with the difficult part. The problem and its explained solution are seen as formal objects in a constructive logic of the data domains. These formal explanations can be executed at various stages of completion. The most incomplete explanations resemble applicative programs, the most complete are formal proofs.",
    "cited_by_count": 193,
    "openalex_id": "https://openalex.org/W2021790140",
    "type": "article"
  },
  {
    "title": "DIB—a distributed implementation of backtracking",
    "doi": "https://doi.org/10.1145/22719.24067",
    "publication_date": "1987-03-20",
    "publication_year": 1987,
    "authors": "Raphael A. Finkel; Udi Manber",
    "corresponding_authors": "",
    "abstract": "DIB is a general-purpose package that allows a wide range of applications such as recursive backtrack, branch and bound, and alpha-beta search to be implemented on a multicomputer. It is very easy to use. The application program needs to specify only the root of the recursion tree, the computation to be performed at each node, and how to generate children at each node. In addition, the application program may optionally specify how to synthesize values of tree nodes from their children's values and how to disseminate information (such as bounds) either globally or locally in the tree. DIB uses a distributed algorithm, transparent to the application programmer, that divides the problem into subproblems and dynamically allocates them to any number of (potentially nonhomogeneous) machines. This algorithm requires only minimal support from the distributed operating system. DIB can recover from failures of machines even if they are not detected. DIB currently runs on the Crystal multicomputer at the University of Wisconsin-Madison. Many applications have been implemented quite easily, including exhaustive traversal ( N queens, knight's tour, negamax tree evaluation), branch and bound (traveling salesman) and alpha-beta search (the game of NIM). Speedup is excellent for exhaustive traversal and quite good for branch and bound.",
    "cited_by_count": 192,
    "openalex_id": "https://openalex.org/W1984263429",
    "type": "article"
  },
  {
    "title": "User Recovery and Reversal in Interactive Systems",
    "doi": "https://doi.org/10.1145/357233.357234",
    "publication_date": "1984-01-01",
    "publication_year": 1984,
    "authors": "James E. Archer; Richard Conway; Fred B. Schneider",
    "corresponding_authors": "",
    "abstract": "article Free AccessUser Recovery and Reversal in Interactive Systems Authors: James E. Archer Rational Machines Inc., 1500 Salado Drive, Mountainview, CA Rational Machines Inc., 1500 Salado Drive, Mountainview, CAView Profile , Richard Conway Department of Computer Science, Cornell University, Ithaca, NY Department of Computer Science, Cornell University, Ithaca, NYView Profile , Fred B. Schneider Department of Computer Science, Cornell University, Ithaca, NY Department of Computer Science, Cornell University, Ithaca, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 1Jan. 1984 pp 1–19https://doi.org/10.1145/357233.357234Published:01 January 1984Publication History 96citation725DownloadsMetricsTotal Citations96Total Downloads725Last 12 Months100Last 6 weeks16 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 192,
    "openalex_id": "https://openalex.org/W2056198286",
    "type": "article"
  },
  {
    "title": "On the origins of bisimulation and coinduction",
    "doi": "https://doi.org/10.1145/1516507.1516510",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Davide Sangiorgi",
    "corresponding_authors": "Davide Sangiorgi",
    "abstract": "The origins of bisimulation and bisimilarity are examined, in the three fields where they have been independently discovered: Computer Science, Philosophical Logic (precisely, Modal Logic), Set Theory. Bisimulation and bisimilarity are coinductive notions, and as such are intimately related to fixed points, in particular greatest fixed points. Therefore also the appearance of coinduction and fixed points is discussed, though in this case only within Computer Science. The paper ends with some historical remarks on the main fixed-point theorems (such as Knaster-Tarski) that underpin the fixed-point theory presented.",
    "cited_by_count": 184,
    "openalex_id": "https://openalex.org/W2116322260",
    "type": "article"
  },
  {
    "title": "An overview of the SR language and implementation",
    "doi": "https://doi.org/10.1145/42192.42324",
    "publication_date": "1988-01-01",
    "publication_year": 1988,
    "authors": "Gregory R. Andrews; Michael H. Coffin; I. J. P. Elshoff; Kelvin Nilsen; Gregg M. Townsend; Ronald A. Olsson; Titus D. M. Purdin",
    "corresponding_authors": "",
    "abstract": "SR is a language for programming distributed systems ranging from operating systems to application programs. On the basis of our experience with the initial version, the language has evolved considerably. In this paper we describe the current version of SR and give an overview of its implementation. The main language constructs are still resources and operations. Resources encapsulate processes and variables that they share; operations provide the primary mechanism for process interaction. One way in which SR has changed is that both resources and processes are now created dynamically. Another change is that inheritance is supported. A third change is that the mechanisms for operation invocation—call and send—and operation implementation—proc and in—have been extended and integrated. Consequently, all of local and remote procedure call, rendezvous, dynamic process creation, asynchronous message passing, multicast, and semaphores are supported. We have found this flexibility to be very useful for distributed programming. Moreover, by basing SR on a small number of well-integrated concepts, the language has proved easy to learn and use, and it has a reasonably efficient implementation.",
    "cited_by_count": 184,
    "openalex_id": "https://openalex.org/W2116936037",
    "type": "article"
  },
  {
    "title": "The trace partitioning abstract domain",
    "doi": "https://doi.org/10.1145/1275497.1275501",
    "publication_date": "2007-08-02",
    "publication_year": 2007,
    "authors": "Xavier Rival; Laurent Mauborgne",
    "corresponding_authors": "",
    "abstract": "In order to achieve better precision of abstract interpretation-based static analysis, we introduce a new generic abstract domain, the trace partitioning abstract domain. We develop a theoretical framework allowing a wide range of instantiations of the domain, proving that all these instantiations give correct results. From this theoretical framework, we go into implementation details of a particular instance developed in the Astrée static analyzer. We show how the domain is automatically configured in Astrée and the gain and cost in terms of performance and precision.",
    "cited_by_count": 180,
    "openalex_id": "https://openalex.org/W2044870852",
    "type": "article"
  },
  {
    "title": "Uniform self-stabilizing rings",
    "doi": "https://doi.org/10.1145/63264.63403",
    "publication_date": "1989-04-01",
    "publication_year": 1989,
    "authors": "J. Burns; Jan Pachl",
    "corresponding_authors": "",
    "abstract": "A self-stabilizing system has the property that, no matter how it is perturbed, it eventually returns to a legitimate configuration. Dijkstra originally introduced the self-stabilization problem and gave several solutions for a ring of processors in his 1974 Communications of the ACM paper. His solutions use a distinguished processor in the ring, which effectively acts as a controlling element to drive the system toward stability. Dijkstra has observed that a distinguished processor is essential if the number of processors in the ring is composite. We show, by presenting a protocol and proving its correctness, that there is a self-stabilizing system with no distinguished processor if the size of the ring is prime. The basic protocol uses Θ ( n 2 ) states in each processor when n is the size of the ring. We modify the basic protocol to obtain one that uses Θ ( n 2 /ln n ) states.",
    "cited_by_count": 177,
    "openalex_id": "https://openalex.org/W1970096085",
    "type": "article"
  },
  {
    "title": "A superimposition control construct for distributed systems",
    "doi": "https://doi.org/10.1145/169701.169682",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Shmuel Katz",
    "corresponding_authors": "Shmuel Katz",
    "abstract": "article Free Access Share on A superimposition control construct for distributed systems Author: Shmuel Katz Technion–Israel Institute of Technology, Haifa, Israel Technion–Israel Institute of Technology, Haifa, IsraelView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 2April 1993 pp 337–356https://doi.org/10.1145/169701.169682Published:01 April 1993Publication History 122citation463DownloadsMetricsTotal Citations122Total Downloads463Last 12 Months32Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 177,
    "openalex_id": "https://openalex.org/W2030828698",
    "type": "article"
  },
  {
    "title": "Programming with Equations",
    "doi": "https://doi.org/10.1145/357153.357158",
    "publication_date": "1982-01-01",
    "publication_year": 1982,
    "authors": "Christoph M. Hoffmann; Michael J. O’Donnell",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Programming with Equations Authors: Christoph M. Hoffmann Department of Computer Sciences, Purdue University, West Lafayette, IN Department of Computer Sciences, Purdue University, West Lafayette, INView Profile , Michael J. O'Donnell Department of Computer Sciences, Purdue University, West Lafayette, IN Department of Computer Sciences, Purdue University, West Lafayette, INView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 1Jan. 1982 pp 83–112https://doi.org/10.1145/357153.357158Published:01 January 1982Publication History 109citation524DownloadsMetricsTotal Citations109Total Downloads524Last 12 Months24Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 173,
    "openalex_id": "https://openalex.org/W2036748304",
    "type": "article"
  },
  {
    "title": "Concurrent Reading While Writing",
    "doi": "https://doi.org/10.1145/357195.357198",
    "publication_date": "1983-01-01",
    "publication_year": 1983,
    "authors": "Gary L. Peterson",
    "corresponding_authors": "Gary L. Peterson",
    "abstract": "article Free Access Share on Concurrent Reading While Writing Author: Gary L. Peterson Department of Computer Science, The University of Rochester, Rochester, NY Department of Computer Science, The University of Rochester, Rochester, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 1Jan. 1983 pp 46–55https://doi.org/10.1145/357195.357198Published:01 January 1983Publication History 130citation1,109DownloadsMetricsTotal Citations130Total Downloads1,109Last 12 Months118Last 6 weeks18 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 173,
    "openalex_id": "https://openalex.org/W2058618358",
    "type": "article"
  },
  {
    "title": "A Value Transmission Method for Abstract Data Types",
    "doi": "https://doi.org/10.1145/69622.357182",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "Maurice Herlihy; Barbara Liskov",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A Value Transmission Method for Abstract Data Types Authors: Maurice P. Herlihy Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square, Cambridge, MA Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square, Cambridge, MAView Profile , Barbara Liskov Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square, Cambridge, MA Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square, Cambridge, MAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 527–551https://doi.org/10.1145/69622.357182Online:01 October 1982Publication History 139citation673DownloadsMetricsTotal Citations139Total Downloads673Last 12 Months115Last 6 weeks16 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 171,
    "openalex_id": "https://openalex.org/W2135032959",
    "type": "article"
  },
  {
    "title": "Techniques for debugging parallel programs with flowback analysis",
    "doi": "https://doi.org/10.1145/115372.115324",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Jong-Deok Choi; Barton P. Miller; Robert H. B. Netzer",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Techniques for debugging parallel programs with flowback analysis Authors: Jong-Deok Choi IBM T. J. Watson Research Center, Yorktown Heights, NY IBM T. J. Watson Research Center, Yorktown Heights, NYView Profile , Barton P. Miller Univ. of Wisconsin, Madison Univ. of Wisconsin, MadisonView Profile , Robert H. B. Netzer Univ. of Wisconsin, Madison Univ. of Wisconsin, MadisonView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 401 October 1991pp 491–530https://doi.org/10.1145/115372.115324Published:01 October 1991Publication History 119citation776DownloadsMetricsTotal Citations119Total Downloads776Last 12 Months37Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 170,
    "openalex_id": "https://openalex.org/W2047229718",
    "type": "article"
  },
  {
    "title": "Symbolic Debugging of Optimized Code",
    "doi": "https://doi.org/10.1145/357172.357173",
    "publication_date": "1982-07-01",
    "publication_year": 1982,
    "authors": "John L. Hennessy",
    "corresponding_authors": "John L. Hennessy",
    "abstract": "article Free Access Share on Symbolic Debugging of Optimized Code Author: John Hennessy Computer Systems Laboratory, Departments of Electrical Engineering and Computer Science, Stanford University, Stanford, CA Computer Systems Laboratory, Departments of Electrical Engineering and Computer Science, Stanford University, Stanford, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 3July 1982 pp 323–344https://doi.org/10.1145/357172.357173Published:01 July 1982Publication History 176citation1,187DownloadsMetricsTotal Citations176Total Downloads1,187Last 12 Months403Last 6 weeks53 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 170,
    "openalex_id": "https://openalex.org/W2118450335",
    "type": "article"
  },
  {
    "title": "Local atomicity properties: modular concurrency control for abstract data types",
    "doi": "https://doi.org/10.1145/63264.63518",
    "publication_date": "1989-04-01",
    "publication_year": 1989,
    "authors": "William E. Weihl",
    "corresponding_authors": "William E. Weihl",
    "abstract": "Atomic actions (or transactions) are useful for coping with concurrency and failures. One way of ensuring atomicity of actions is to implement applications in terms of atomic data types : abstract data types whose objects ensure serializability and recoverability of actions using them. Many atomic types can be implemented to provide high levels of concurrency by taking advantage of algebraic properties of the type's operations, for example, that certain operations commute. In this paper we analyze the level of concurrency permitted by an atomic type. We introduce several local constraints on individual objects that suffice to ensure global atomicity of actions; we call these constraints local atomicity properties . We present three local atomicity properties, each of which is optimal : no strictly weaker local constraint on objects suffices to ensure global atomicity for actions. Thus, the local atomicity properties define precise limits on the amount of concurrency that can be permitted by an atomic type.",
    "cited_by_count": 169,
    "openalex_id": "https://openalex.org/W1991878751",
    "type": "article"
  },
  {
    "title": "Program locality analysis using reuse distance",
    "doi": "https://doi.org/10.1145/1552309.1552310",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Yutao Zhong; Xipeng Shen; Chen Ding",
    "corresponding_authors": "",
    "abstract": "On modern computer systems, the memory performance of an application depends on its locality. For a single execution, locality-correlated measures like average miss rate or working-set size have long been analyzed using reuse distance —the number of distinct locations accessed between consecutive accesses to a given location. This article addresses the analysis problem at the program level, where the size of data and the locality of execution may change significantly depending on the input. The article presents two techniques that predict how the locality of a program changes with its input. The first is approximate reuse-distance measurement, which is asymptotically faster than exact methods while providing a guaranteed precision. The second is statistical prediction of locality in all executions of a program based on the analysis of a few executions. The prediction process has three steps: dividing data accesses into groups, finding the access patterns in each group, and building parameterized models. The resulting prediction may be used on-line with the help of distance-based sampling. When evaluated on fifteen benchmark applications, the new techniques predicted program locality with good accuracy, even for test executions that are orders of magnitude larger than the training executions. The two techniques are among the first to enable quantitative analysis of whole-program locality in general sequential code. These findings form the basis for a unified understanding of program locality and its many facets. Concluding sections of the article present a taxonomy of related literature along five dimensions of locality and discuss the role of reuse distance in performance modeling, program optimization, cache and virtual memory management, and network traffic analysis.",
    "cited_by_count": 169,
    "openalex_id": "https://openalex.org/W2091250014",
    "type": "article"
  },
  {
    "title": "Floyd--hoare logic for quantum programs",
    "doi": "https://doi.org/10.1145/2049706.2049708",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Mingsheng Ying",
    "corresponding_authors": "Mingsheng Ying",
    "abstract": "Floyd--Hoare logic is a foundation of axiomatic semantics of classical programs, and it provides effective proof techniques for reasoning about correctness of classical programs. To offer similar techniques for quantum program verification and to build a logical foundation of programming methodology for quantum computers, we develop a full-fledged Floyd--Hoare logic for both partial and total correctness of quantum programs. It is proved that this logic is (relatively) complete by exploiting the power of weakest preconditions and weakest liberal preconditions for quantum programs.",
    "cited_by_count": 162,
    "openalex_id": "https://openalex.org/W2044357043",
    "type": "article"
  },
  {
    "title": "Termination of Probabilistic Concurrent Program",
    "doi": "https://doi.org/10.1145/2166.357214",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Sergiu Hart; Micha Sharir; Amir Pnueli",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Termination of Probabilistic Concurrent Program Authors: Sergiu Hart Department of Statistics, Tel Aviv University, Ramat-Aviv, Tel Aviv 69978, Israel Department of Statistics, Tel Aviv University, Ramat-Aviv, Tel Aviv 69978, IsraelView Profile , Micha Sharir Department of Computer Science, School of Mathematical Sciences, Tel Aviv University, Ramat-Aviv, Tel Aviv 69978, Israel Department of Computer Science, School of Mathematical Sciences, Tel Aviv University, Ramat-Aviv, Tel Aviv 69978, IsraelView Profile , Amir Pnueli Department of Applied Mathematics, The Weizmann Institute of Science, Rehovot, Israel Department of Applied Mathematics, The Weizmann Institute of Science, Rehovot, IsraelView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3July 1983 pp 356–380https://doi.org/10.1145/2166.357214Published:01 July 1983Publication History 118citation513DownloadsMetricsTotal Citations118Total Downloads513Last 12 Months46Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 162,
    "openalex_id": "https://openalex.org/W2091188761",
    "type": "article"
  },
  {
    "title": "Language support for the specification and development of composite systems",
    "doi": "https://doi.org/10.1145/22719.22947",
    "publication_date": "1987-03-20",
    "publication_year": 1987,
    "authors": "Martin S. Feather",
    "corresponding_authors": "Martin S. Feather",
    "abstract": "When a complex system is to be realized as a combination of interacting components, development of those components should commence from a specification of the behavior required of the composite system. A separate specification should be used to describe the decomposition of that system into components. The first phase of implementation from a specification in this style is the derivation of the individual component behaviors implied by these specifications. The virtues of this approach to specification are expounded, and specification language features that are supportive of it are presented. It is shown how these are incorporated in the specification language Gist, which our group has developed. These issues are illustrated in a development of a controller for elevators serving passengers in a multistory building.",
    "cited_by_count": 160,
    "openalex_id": "https://openalex.org/W1975565207",
    "type": "article"
  },
  {
    "title": "CIRCAL and the representation of communication, concurrency, and time",
    "doi": "https://doi.org/10.1145/3318.3322",
    "publication_date": "1985-04-01",
    "publication_year": 1985,
    "authors": "George Milne",
    "corresponding_authors": "George Milne",
    "abstract": "The CIRCAL calculus is presented as a mathematical framework in which to describe and analyze concurrent systems, whether hardware or software. The dot operator is used to compose CIRCAL descriptions, and it is this operator which permits the natural modeling of asynchronous and simultaneous behavior, thus allowing the representation and analysis of system timing properties such as those found in circuits. The CIRCAL framework uses an abstraction operator to permit the modeling of a system at different levels of detail. Behavioral complexity of real systems makes abstraction crucial when producing a tractable model, and we illustrate how abstraction introduces nondeterminisim into system representations. An operational semantics, acceptance semantics, is introduced, and it is in terms of this active experimentation that meaning is given to the CIRCAL syntax, thus allowing proof of system properties to be constructed.",
    "cited_by_count": 154,
    "openalex_id": "https://openalex.org/W2085912345",
    "type": "article"
  },
  {
    "title": "An Axiomatic Approach to Information Flow in Programs",
    "doi": "https://doi.org/10.1145/357084.357088",
    "publication_date": "1980-01-01",
    "publication_year": 1980,
    "authors": "Gregory R. Andrews; Richard P. Reitman",
    "corresponding_authors": "",
    "abstract": "A new approach to information flow in sequential and parallel programs is presented. Flow proof rules that capture the information flow semantics of a variety of statements are given and used to construct program flow proofs. The method is illustrated by examples. The applications of flow proofs to certifying information flow policies and to solving the confinement problem are considered. It is also shown that flow rules and correctness rules can be combined to form an even more powerful proof system.",
    "cited_by_count": 152,
    "openalex_id": "https://openalex.org/W1993302007",
    "type": "article"
  },
  {
    "title": "On the Construction of Submodule Specifications and Communication Protocols",
    "doi": "https://doi.org/10.1145/357195.357196",
    "publication_date": "1983-01-01",
    "publication_year": 1983,
    "authors": "Philip M. Merlin; Gregor von Bochmann",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on On the Construction of Submodule Specifications and Communication Protocols Authors: Philip Merlin The Technion The TechnionView Profile , Gregor V. Bochmann Département d'informatique et de recherche opérationnelle, Université de Montréal, Case postale 6128, Succursale \"A,\" Montréal, Québec H3C 3J7, Canada Département d'informatique et de recherche opérationnelle, Université de Montréal, Case postale 6128, Succursale \"A,\" Montréal, Québec H3C 3J7, CanadaView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 1pp 1–25https://doi.org/10.1145/357195.357196Published:01 January 1983Publication History 134citation439DownloadsMetricsTotal Citations134Total Downloads439Last 12 Months32Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 152,
    "openalex_id": "https://openalex.org/W2126736658",
    "type": "article"
  },
  {
    "title": "Synchronization in Distributed Programs",
    "doi": "https://doi.org/10.1145/357162.357163",
    "publication_date": "1982-04-01",
    "publication_year": 1982,
    "authors": "Fred B. Schneider",
    "corresponding_authors": "Fred B. Schneider",
    "abstract": "article Free Access Share on Synchronization in Distributed Programs Author: Fred B. Schneider Department of Computer Science, Cornell University, Ithaca, NY Department of Computer Science, Cornell University, Ithaca, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 2April 1982 pp 125–148https://doi.org/10.1145/357162.357163Published:01 April 1982Publication History 107citation1,574DownloadsMetricsTotal Citations107Total Downloads1,574Last 12 Months305Last 6 weeks32 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 145,
    "openalex_id": "https://openalex.org/W2053374874",
    "type": "article"
  },
  {
    "title": "Multivariate amortized resource analysis",
    "doi": "https://doi.org/10.1145/2362389.2362393",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Jan Hoffmann; Klaus Aehlig; Martin Hofmann",
    "corresponding_authors": "",
    "abstract": "We study the problem of automatically analyzing the worst-case resource usage of procedures with several arguments. Existing automatic analyses based on amortization or sized types bound the resource usage or result size of such a procedure by a sum of unary functions of the sizes of the arguments. In this article we generalize this to arbitrary multivariate polynomial functions thus allowing bounds of the form mn which had to be grossly overestimated by m 2 + n 2 before. Our framework even encompasses bounds like ∑ i,j≤ n m i m j where the m i are the sizes of the entries of a list of length n . This allows us for the first time to derive useful resource bounds for operations on matrices that are represented as lists of lists and to considerably improve bounds on other superlinear operations on lists such as longest common subsequence and removal of duplicates from lists of lists. Furthermore, resource bounds are now closed under composition which improves accuracy of the analysis of composed programs when some or all of the components exhibit superlinear resource or size behavior. The analysis is based on a novel multivariate amortized resource analysis. We present it in form of a type system for a simple first-order functional language with lists and trees, prove soundness, and describe automatic type inference based on linear programming. We have experimentally validated the automatic analysis on a wide range of examples from functional programming with lists and trees. The obtained bounds were compared with actual resource consumption. All bounds were asymptotically tight, and the constants were close or even identical to the optimal ones.",
    "cited_by_count": 143,
    "openalex_id": "https://openalex.org/W2073629816",
    "type": "article"
  },
  {
    "title": "Verification of a Cryptographic Primitive",
    "doi": "https://doi.org/10.1145/2701415",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "Andrew W. Appel",
    "corresponding_authors": "Andrew W. Appel",
    "abstract": "This article presents a full formal machine-checked verification of a C program: the OpenSSL implementation of SHA-256. This is an interactive proof of functional correctness in the Coq proof assistant, using the Verifiable C program logic. Verifiable C is a separation logic for the C language, proved sound with respect to the operational semantics for C, connected to the CompCert verified optimizing C compiler.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W2067081213",
    "type": "article"
  },
  {
    "title": "Structured Communication-Centered Programming for Web Services",
    "doi": "https://doi.org/10.1145/2220365.2220367",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Marco Carbone; Kohei Honda; Nobuko Yoshida",
    "corresponding_authors": "",
    "abstract": "This article relates two different paradigms of descriptions of communication behavior, one focusing on global message flows and another on end-point behaviors, using formal calculi based on session types. The global calculus, which originates from a Web service description language (W3C WS-CDL), describes an interaction scenario from a vantage viewpoint; the end-point calculus, an applied typed π -calculus, precisely identifies a local behavior of each participant. We explore a theory of end-point projection, by which we can map a global description to its end-point counterparts preserving types and dynamics. Three principles of well-structured description and the type structures play a fundamental role in the theory.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W1991621238",
    "type": "article"
  },
  {
    "title": "Rigorous Estimation of Floating-Point Round-Off Errors with Symbolic Taylor Expansions",
    "doi": "https://doi.org/10.1145/3230733",
    "publication_date": "2018-12-11",
    "publication_year": 2018,
    "authors": "Alexey Solovyev; Marek Baranowski; Ian Briggs; Charles Jacobsen; Zvonimir Rakamarić; Ganesh Gopalakrishnan",
    "corresponding_authors": "",
    "abstract": "Rigorous estimation of maximum floating-point round-off errors is an important capability central to many formal verification tools. Unfortunately, available techniques for this task often provide very pessimistic overestimates, causing unnecessary verification failure. We have developed a new approach called Symbolic Taylor Expansions that avoids these problems, and implemented a new tool called FPTaylor embodying this approach. Key to our approach is the use of rigorous global optimization, instead of the more familiar interval arithmetic, affine arithmetic, and/or SMT solvers. FPTaylor emits per-instance analysis certificates in the form of HOL Light proofs that can be machine checked. In this article, we present the basic ideas behind Symbolic Taylor Expansions in detail. We also survey as well as thoroughly evaluate six tool families, namely, Gappa (two tool options studied), Fluctuat, PRECiSA, Real2Float, Rosa, and FPTaylor (two tool options studied) on 24 examples, running on the same machine, and taking care to find the best options for running each of these tools. This study demonstrates that FPTaylor estimates round-off errors within much tighter bounds compared to other tools on a significant number of case studies. We also release FPTaylor along with our benchmarks, thus contributing to future studies and tool development in this area.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2904117297",
    "type": "article"
  },
  {
    "title": "CFLOBDDs: Context-Free-Language Ordered Binary Decision Diagrams",
    "doi": "https://doi.org/10.1145/3651157",
    "publication_date": "2024-03-04",
    "publication_year": 2024,
    "authors": "Meghana Aparna Sistla; Swarat Chaudhuri; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "This article presents a new compressed representation of Boolean functions, called CFLOBDDs (for Context-Free-Language Ordered Binary Decision Diagrams). They are essentially a plug-compatible alternative to BDDs (Binary Decision Diagrams), and hence are useful for representing certain classes of functions, matrices, graphs, relations, and so forth in a highly compressed fashion. CFLOBDDs share many of the good properties of BDDs, but—in the best case—the CFLOBDD for a Boolean function can be exponentially smaller than any BDD for that function . Compared with the size of the decision tree for a function, a CFLOBDD—again, in the best case—can give a double-exponential reduction in size . They have the potential to permit applications to (i) execute much faster and (ii) handle much larger problem instances than has been possible heretofore. We applied CFLOBDDs in quantum-circuit simulation and found that for several standard problems, the improvement in scalability, compared to BDDs, is quite dramatic. With a 15-minute timeout, the number of qubits that CFLOBDDs can handle are 65,536 for Greenberger-Horne-Zellinger, 524,288 for Bernstein-Vazirani, 4,194,304 for Deutsch-Jozsa, and 4,096 for Grover’s algorithm, besting BDDs by factors of 128×, 1,024×, 8,192×, and 128×, respectively.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4392376954",
    "type": "article"
  },
  {
    "title": "Combining analyses, combining optimizations",
    "doi": "https://doi.org/10.1145/201059.201061",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Cliff Click; Keith D. Cooper",
    "corresponding_authors": "",
    "abstract": "Modern optimizing compilers use several passes over a program's intermediate representation to generate good code. Many of these optimizations exhibit a phase-ordering problem. Getting the best code may require iterating optimizations until a fixed point is reached. Combining these phases can lead to the discovery of more facts about the program, exposing more opportunities for optimization. This article presents a framework for describing optimizations. It shows how to combine two such frameworks and how to reason about the properties of the resulting framework. The structure of the frame work provides insight into when a combination yields better results. To make the ideas more concrete, this article presents a framework for combining constant propagation, value numbering, and unreachable-code elimination. It is an open question as to what other frameworks can be combined in this way.",
    "cited_by_count": 178,
    "openalex_id": "https://openalex.org/W2152885483",
    "type": "article"
  },
  {
    "title": "Covariance and contravariance",
    "doi": "https://doi.org/10.1145/203095.203096",
    "publication_date": "1995-05-01",
    "publication_year": 1995,
    "authors": "Giuseppe Castagna",
    "corresponding_authors": "Giuseppe Castagna",
    "abstract": "In type-theoretic research on object-oriented programming, the issue of “covariance versus contravariance” is a topic of continuing debate. In this short note we argue that covariance and contravariance appropriately characterize two distinct and independent mechanisms. The so-called contravariance rule correctly captures the subtyping relation (that relation which establishes which sets of functions can replace another given set in every context ). A covariant relation, instead, characterizes the specialization of code (i.e., the definition of new code which replaces old definitions in some particular cases ). Therefore, covariance and contravariance are not opposing views, but distinct concepts that each have their place in object-oriented systems. Both can (and should) be integrated in a type-safe manner in object-oriented languages. We also show that the independence of the two mechanisms is not characteristic of a particular model but is valid in general, since covariant specialization is present in record-based models, although it is hidden by a deficiency of all existing calculi that realize this model. As an aside, we show that the λ&amp;-calculus can be taken as the basic calculus for both an overloading-based and a record-based model. Using this approach, one not only obtains a more uniform vision of object-oriented type theories, but in the case of the record-based approach, one also gains multiple dispatching, a feature that existing record-based models do not capture",
    "cited_by_count": 172,
    "openalex_id": "https://openalex.org/W2116793756",
    "type": "article"
  },
  {
    "title": "Regular expression types for XML",
    "doi": "https://doi.org/10.1145/1053468.1053470",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Haruo Hosoya; Jérôme Vouillon; Benjamin C. Pierce",
    "corresponding_authors": "",
    "abstract": "We propose regular expression types as a foundation for statically typed XML processing languages. Regular expression types, like most schema languages for XML, introduce regular expression notations such as repetition (*), alternation (|), etc., to describe XML documents. The novelty of our type system is a semantic presentation of subtyping, as inclusion between the sets of documents denoted by two types. We give several examples illustrating the usefulness of this form of subtyping in XML processing.The decision problem for the subtype relation reduces to the inclusion problem between tree automata, which is known to be EXPTIME-complete. To avoid this high complexity in typical cases, we develop a practical algorithm that, unlike classical algorithms based on determinization of tree automata, checks the inclusion relation by a top-down traversal of the original type expressions. The main advantage of this algorithm is that it can exploit the property that type expressions being compared often share portions of their representations. Our algorithm is a variant of Aiken and Murphy's set-inclusion constraint solver, to which are added several new implementation techniques, correctness proofs, and preliminary performance measurements on some small programs in the domain of typed XML processing.",
    "cited_by_count": 165,
    "openalex_id": "https://openalex.org/W2157821258",
    "type": "article"
  },
  {
    "title": "Parallel execution of prolog programs",
    "doi": "https://doi.org/10.1145/504083.504085",
    "publication_date": "2001-07-01",
    "publication_year": 2001,
    "authors": "Gopal Gupta; Enrico Pontelli; Khayri A. M. Ali; Mats Carlsson; Manuel V. Hermenegildo",
    "corresponding_authors": "",
    "abstract": "Since the early days of logic programming, researchers in the field realized the potential for exploitation of parallelism present in the execution of logic programs. Their high-level nature, the presence of nondeterminism, and their referential transparency, among other characteristics, make logic programs interesting candidates for obtaining speedups through parallel execution. At the same time, the fact that the typical applications of logic programming frequently involve irregular computations, make heavy use of dynamic data structures with logical variables, and involve search and speculation, makes the techniques used in the corresponding parallelizing compilers and run-time systems potentially interesting even outside the field. The objective of this article is to provide a comprehensive survey of the issues arising in parallel execution of logic programming languages along with the most relevant approaches explored to date in the field. Focus is mostly given to the challenges emerging from the parallel execution of Prolog programs. The article describes the major techniques used for shared memory implementation of Or-parallelism, And-parallelism, and combinations of the two. We also explore some related issues, such as memory management, compile-time analysis, and execution visualization.",
    "cited_by_count": 163,
    "openalex_id": "https://openalex.org/W1985039455",
    "type": "article"
  },
  {
    "title": "Cost analysis of logic programs",
    "doi": "https://doi.org/10.1145/161468.161472",
    "publication_date": "1993-11-01",
    "publication_year": 1993,
    "authors": "Saumya Debray; Nai-Wei Lin",
    "corresponding_authors": "",
    "abstract": "article Free AccessCost analysis of logic programs Authors: Saumya K. Debray View Profile , Nai-Wei Lin View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 5pp 826–875https://doi.org/10.1145/161468.161472Published:01 November 1993Publication History 117citation614DownloadsMetricsTotal Citations117Total Downloads614Last 12 Months71Last 6 weeks17 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 159,
    "openalex_id": "https://openalex.org/W2012069736",
    "type": "article"
  },
  {
    "title": "Commutativity analysis",
    "doi": "https://doi.org/10.1145/267959.269969",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "Martin Rinard; Pedro C. Diniz",
    "corresponding_authors": "",
    "abstract": "This article presents a new analysis technique, commutativity analysis, for automatically parallelizing computations that manipulate dynamic, pointer-based data structures. Commutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e., generate the same final result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code. We have implemented a prototype compilation system that uses commutativity analysis as its primary analysis technique. We have used this system to automatically parallelize three complete scientific computations: the Barnes-Hut N-body solver, the Water liquid simulation code, and the String seismic simulation code. This article presents performance results for the generated parallel code running on the Stanford DASH machine. These results provide encouraging evidence that commutativity analysis can serve as the basis for a successful parallelizing compiler.",
    "cited_by_count": 157,
    "openalex_id": "https://openalex.org/W2104251622",
    "type": "article"
  },
  {
    "title": "Model checking of hierarchical state machines",
    "doi": "https://doi.org/10.1145/503502.503503",
    "publication_date": "2001-05-01",
    "publication_year": 2001,
    "authors": "Rajeev Alur; Mihalis Yannakakis",
    "corresponding_authors": "",
    "abstract": "Model checking is emerging as a practical tool for detecting logical errors in early stages of system design. We investigate the model checking of sequential hierarchical (nested) systems, i.e., finite-state machines whose states themselves can be other machines. This nesting ability is common in various software design methodologies, and is available in several commercial modeling tools. The straightforward way to analyze a hierarchical machine is to flatten it (thus incurring an exponential blow up) and apply a model-checking tool on the resulting ordinary FSM. We show that this flattening can be avoided. We develop algorithms for verifying linear-time requirements whose complexity is polynomial in the size of the hierarchical machine. We also address the verification of branching time requirements and provide efficient algorithms and matching lower bounds.",
    "cited_by_count": 153,
    "openalex_id": "https://openalex.org/W2002644257",
    "type": "article"
  },
  {
    "title": "Improving the ratio of memory operations to floating-point operations in loops",
    "doi": "https://doi.org/10.1145/197320.197366",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Steve Carr; Ken Kennedy",
    "corresponding_authors": "",
    "abstract": "Over the past decade, microprocessor design strategies have focused on increasing the computational power on a single chip. Because computations often require more data from cache per floating-point operation than a machine can deliver and because operations are pipelined, idle computational cycles are common when scientific applications are executed. To overcome these bottlenecks, programmers have learned to use a coding style that ensures a better balance between memory references and floating-point operations. In our view, this is a step in the wrong direction because it makes programs more machine-specific. A programmer should not be required to write a new program version for each new machine; instead, the task of specializing a program to a target machine should be left to the compiler. But is our view practical? Can a sophisticated optimizing compiler obviate the need for the myriad of programming tricks that have found their way into practice to improve the performance of the memory hierarchy? In this paper we attempt to answer that question. To do so, we develop and evaluate techniques that automatically restructure program loops to achieve high performance on specific target architectures. These methods attempt to balance computation and memory accesses and seek to eliminate or reduce pipeline interlock. To do this, they estimate statically the balance between memory operations and floating-point operations for each loop in a particular program and use these estimates to determine whether to apply various loop transformations. Experiments with our automatic techniques show that integer-factor speedups are possible on kernels. Additionally, the estimate of the balance between memory operations and computation, and the application of the estimate are very accurate—experiments reveal little difference between the balance achieved by our automatic system that is made possible by hand optimization.",
    "cited_by_count": 152,
    "openalex_id": "https://openalex.org/W2138007781",
    "type": "article"
  },
  {
    "title": "Beyond induction variables",
    "doi": "https://doi.org/10.1145/200994.201003",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Michael P. Gerlek; Eric Stoltz; Michael Wolfe",
    "corresponding_authors": "",
    "abstract": "Linear induction variable detection is usually associated with the strength reduction optimization. For restructuring compilers, effective data dependence analysis requires that the compiler detect and accurately describe linear and nonlinear induction variables as well as more general sequences. In this article we present a practical technique for detecting a broader class of linear induction variables than is usually recognized, as well as several other sequence forms, including periodic, polynomial, geometric, monotonic, and wrap-around variables. Our method is based on Factored Use-Def (FUD) chains, a demand-driven representation of the popular Static Single Assignment (SSA) form. In this form, strongly connected components of the associated SSA graph correspond to sequences in the source program: we describe a simple yet efficient algorithm for detecting and classifying these sequences. We have implemented this algorithm in Nascent, our restructuring Fortran 90+ compiler, and we present some results showing the effectiveness of our approach.",
    "cited_by_count": 151,
    "openalex_id": "https://openalex.org/W2078314687",
    "type": "article"
  },
  {
    "title": "Saturn",
    "doi": "https://doi.org/10.1145/1232420.1232423",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Yichen Xie; Alex Aiken",
    "corresponding_authors": "",
    "abstract": "This article presents Saturn, a general framework for building precise and scalable static error detection systems. Saturn exploits recent advances in Boolean satisfiability (SAT) solvers and is path sensitive, precise down to the bit level, and models pointers and heap data. Our approach is also highly scalable, which we achieve using two techniques. First, for each program function, several optimizations compress the size of the Boolean formulas that model the control flow and data flow and the heap locations accessed by a function. Second, summaries in the spirit of type signatures are computed for each function, allowing interprocedural analysis without a dramatic increase in the size of the Boolean constraints to be solved. We have experimentally validated our approach by conducting two case studies involving a Linux lock checker and a memory leak checker. Results from the experiments show that our system scales well, parallelizes well, and finds more errors with fewer false positives than previous static error detection systems.",
    "cited_by_count": 146,
    "openalex_id": "https://openalex.org/W2124377830",
    "type": "article"
  },
  {
    "title": "Data Type Specification: Parameterization and the Power of Specification Techniques",
    "doi": "https://doi.org/10.1145/69622.357192",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "J. W. Thatcher; Eric G. Wagner; Jesse B. Wright",
    "corresponding_authors": "",
    "abstract": "article Free AccessData Type Specification: Parameterization and the Power of Specification Techniques Authors: J. W. Thatcher Mathematical Sciences Department, IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY Mathematical Sciences Department, IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NYView Profile , E. G. Wagner Mathematical Sciences Department, IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY Mathematical Sciences Department, IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NYView Profile , J. B. Wright Mathematical Sciences Department, IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY Mathematical Sciences Department, IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 711–732https://doi.org/10.1145/69622.357192Published:01 October 1982Publication History 87citation400DownloadsMetricsTotal Citations87Total Downloads400Last 12 Months34Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 141,
    "openalex_id": "https://openalex.org/W2035625685",
    "type": "article"
  },
  {
    "title": "The VAL Language: Description and Analysis",
    "doi": "https://doi.org/10.1145/357153.357157",
    "publication_date": "1982-01-01",
    "publication_year": 1982,
    "authors": "James R. McGraw",
    "corresponding_authors": "James R. McGraw",
    "abstract": "article Free AccessThe VAL Language: Description and Analysis Author: James R. McGraw Lawrence Livermore National Laboratory, P. O. Box 808, Livermore, CA Lawrence Livermore National Laboratory, P. O. Box 808, Livermore, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 1Jan. 1982 pp 44–82https://doi.org/10.1145/357153.357157Published:01 January 1982Publication History 103citation1,172DownloadsMetricsTotal Citations103Total Downloads1,172Last 12 Months315Last 6 weeks31 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 141,
    "openalex_id": "https://openalex.org/W2062182248",
    "type": "article"
  },
  {
    "title": "Efficient synchronization of multiprocessors with shared memory",
    "doi": "https://doi.org/10.1145/48022.48024",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "Clyde P. Kruskal; Larry Rudolph; Marc Snir",
    "corresponding_authors": "",
    "abstract": "A new formalism is given for read-modify-write (RMW) synchronization operations. This formalism is used to extend the memory reference combining mechanism introduced in the NYU Ultracomputer, to arbitrary RMW operations. A formal correctness proof of this combining mechanism is given. General requirements for the practicality of combining are discussed. Combining is shown to be practical for many useful memory access operations. This includes memory updates of the form mem _ val := mem _ val op val , where op need not be associative, and a variety of synchronization primitives. The computation involved is shown to be closely related to parallel prefix evaluation.",
    "cited_by_count": 140,
    "openalex_id": "https://openalex.org/W2071226137",
    "type": "article"
  },
  {
    "title": "Static inference of modes and data dependencies in logic programs",
    "doi": "https://doi.org/10.1145/65979.65983",
    "publication_date": "1989-07-01",
    "publication_year": 1989,
    "authors": "Saumya Debray",
    "corresponding_authors": "Saumya Debray",
    "abstract": "Mode and data dependency analyses find many applications in the generation of efficient executable code for logic programs. For example, mode information can be used to generate specialized unification instructions where permissible, to detect determinacy and functionality of programs, generate index structures more intelligently, reduce the amount of runtime tests in systems that support goal suspension, and in the integration of logic and functional languages. Data dependency information can be used for various source-level optimizing transformations, to improve backtracking behavior and to parallelize logic programs. This paper describes and proves correct an algorithm for the static inference of modes and data dependencies in a program. The algorithm is shown to be quite efficient for programs commonly encountered in practice.",
    "cited_by_count": 139,
    "openalex_id": "https://openalex.org/W2017860618",
    "type": "article"
  },
  {
    "title": "C and tcc",
    "doi": "https://doi.org/10.1145/316686.316697",
    "publication_date": "1999-03-01",
    "publication_year": 1999,
    "authors": "Massimiliano Poletto; Wilson C. Hsieh; Dawson Engler; M. Frans Kaashoek",
    "corresponding_authors": "",
    "abstract": "Dynamic code generation allows programmers to use run-time information in order to achieve performance and expressiveness superior to those of static code. The 'C( Tick C ) language is a superset of ANSI C that supports efficient and high-level use of dynamic code generation. 'C provides dynamic code generation at the level of C expressions and statements and supports the composition of dynamic code at run time. These features enable programmers to add dynamic code generation to existing C code incrementally and to write important applications (such as “just-in-time” compilers) easily. The article presents many examples of how 'C can be used to solve practical problems. The tcc compiler is an efficient, portable, and freely available implementation of 'C. tcc allows programmers to trade dynamic compilation speed for dynamic code quality: in some aplications, it is most important to generate code quickly, while in others code quality matters more than compilation speed. The overhead of dynamic compilation is on the order of 100 to 600 cycles per generated instruction, depending on the level of dynamic optimizaton. Measurements show that the use of dynamic code generation can improve performance by almost an order of magnitude; two- to four-fold speedups are common. In most cases, the overhead of dynamic compilation is recovered in under 100 uses of the dynamic code; sometimes it can be recovered within one use.",
    "cited_by_count": 138,
    "openalex_id": "https://openalex.org/W2048423324",
    "type": "article"
  },
  {
    "title": "ACE: an automatic complexity evaluator",
    "doi": "https://doi.org/10.1145/42190.42347",
    "publication_date": "1988-04-01",
    "publication_year": 1988,
    "authors": "Daniel Le Métayer",
    "corresponding_authors": "Daniel Le Métayer",
    "abstract": "There has been a great deal of research done on the evaluation of the complexity of particular algorithms; little effort, however, has been devoted to the mechanization of this evaluation. The ACE (Automatic Complexity Evaluator) system is able to analyze reasonably large programs, like sorting programs, in a fully mechanical way. A time-complexity function is derived from the initial functional program. This function is transformed into its nonrecursive equivalent according to MacCarthy's recursion induction principle, using a predefined library of recursive definitions. As the complexity is not a decidable property, this transformation will not be possible in all cases. The richer the predefined library is, the more likely the system is to succeed. The operations performed by ACE are described and the use of the system is illustrated with the analysis of a sorting algorithm. Related works and further improvements are discussed in the conclusion.",
    "cited_by_count": 138,
    "openalex_id": "https://openalex.org/W2060213695",
    "type": "article"
  },
  {
    "title": "The concurrent language, Shared Prolog",
    "doi": "https://doi.org/10.1145/114005.102807",
    "publication_date": "1991-01-01",
    "publication_year": 1991,
    "authors": "Antonio Brogi; Paolo Ciancarini",
    "corresponding_authors": "",
    "abstract": "Shared Prolog is a new concurrent logic language. A Shared Prolog system is composed of a set of parallel agents that are Prolog programs extended by a guard mechanism. The programmer controls the granularity of parallelism, coordinating communication and synchronization of the agents via a centralized data structure. The communication mechanism is inherited from the blackboard model of problem solving. Intuitively, the granularity of the logic processes to be elaborated in parallel is large, while the resources shared on the blackboard can be very fined grained. An operational semantics for Shared Prolog is given in terms of a distributed model. Through an abstract notion of computation, the kinds of parallelism supported by the language, as well as properties of infinite computations, such as local deadlocks, are studied. The expressiveness of the language is shown with respect to the specification of two classes of applications: metaprogramming and blackboard systems.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W2006090282",
    "type": "article"
  },
  {
    "title": "Axioms for memory access in asynchronous hardware systems",
    "doi": "https://doi.org/10.1145/5001.5007",
    "publication_date": "1986-01-02",
    "publication_year": 1986,
    "authors": "Jayadev Misra",
    "corresponding_authors": "Jayadev Misra",
    "abstract": "The problem of concurrent accesses to registers by asynchronous components is considered. A set of axioms about the values in a register during concurrent accesses is proposed. It is shown that if these axioms are met by a register, then concurrent accesses to it may be viewed as nonconcurrent, thus making it possible to analyze asynchronous algorithms without elaborate timing analysis of operations. These axioms are shown, in a certain sense, to be the weakest. Motivation for this work came from analyzing low-level hardware components in a VLSI chip which concurrently accesses a flip-flop.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W2070356883",
    "type": "article"
  },
  {
    "title": "An approach for exploring code improving transformations",
    "doi": "https://doi.org/10.1145/267959.267960",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "Deborah Whitfield; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "Although code transformations are routinely applied to improve the performance of programs for both scalar and parallel machines, the properties of code-improving transformations are not well understood. In this article we present a framework that enables the exploration, both analytically and experimentally, of properties of code-improving transformations. The major component of the framework is a specification language, Gospel, for expressing the conditions needed to safely apply a transformation and the actions required to change the code to implement the transformation. The framework includes a technique that facilitates an analytical investigation of code-improving transformations using the Gospel specifications. It also contains a tool, Genesis, that automatically produces a transformer that implements the transformations specified in Gospel. We demonstrate the usefulness of the framework by exploring the enabling and disabling properties of transformations. We first present analytical results on the enabling and disabling properties of a set of code transformations, including both traditional and parallelizing transformations, and then describe experimental results showing the types of transformations and the enabling and disabling interactions actually found in a set of programs.",
    "cited_by_count": 136,
    "openalex_id": "https://openalex.org/W2088056808",
    "type": "article"
  },
  {
    "title": "The PSG system: from formal language definitions to interactive programming environments",
    "doi": "https://doi.org/10.1145/6465.20890",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Rolf Bahlke; Gregor Snelting",
    "corresponding_authors": "",
    "abstract": "The PSG programming system generator developed at the Technical University of Darmstadt produces interactive, language-specific programming environments from formal language definitions. All language-dependent parts of the environment are generated from an entirely nonprocedural specification of the language's syntax, context conditions, and dynamic semantics. The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The major component of the environment is a full-screen editor, which allows both structure and text editing. In structure mode the editor guarantees prevention of both syntactic and semantic errors, whereas in textual mode it guarantees their immediate recognition. PSG editors employ a novel algorithm for incremental semantic analysis which is based on unification. The algorithm will immediately detect semantic errors even in incomplete program fragments. The dynamic semantics of the language are defined in denotational style using a functional language based on the lambda calculus. Program fragments are compiled to terms of the functional language which are executed by an interpreter. The PSG generator has been used to produce environments for Pascal, ALGOL 60, MODULA-2, and the formal language definition language itself.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W2051504145",
    "type": "article"
  },
  {
    "title": "Efficient computation of interprocedural definition-use chains",
    "doi": "https://doi.org/10.1145/174662.174663",
    "publication_date": "1994-03-01",
    "publication_year": 1994,
    "authors": "Mary Jean Harrold; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "The dependencies that exist among definitions and uses of variables in a program are required by many language-processing tools. This paper considers the computation of definition-use and use-definition chains that extend across procedure boundaries at call and return sites. Intraprocedural definition and use information is abstracted for each procedure and is used to construct an interprocedural flow graph. This intraprocedural data-flow information is then propagated throughout the program via the interprocedural flow graph to obtain sets of reaching definitions and/or reachable uses for reach interprocedural control point, including procedure entry, exit, call, and return. Interprocedural definition-use and/or use-definition chains are computed from this reaching information. The technique handles the interprocedural effects of the data flow caused by both reference parameters and global variables, while preserving the calling context of called procedures. Additionally, recursion, aliasing, and separate compilation are handled. The technique has been implemented using a Sun-4 Workstation and incorporated into an interprocedural data-flow tester. Results from experiments indicate the practicality of the technique, both in terms of the size of the interprocedural flow graph and the size of the data-flow sets.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W2103326816",
    "type": "article"
  },
  {
    "title": "The promotion and accumulation strategies in transformational programming",
    "doi": "https://doi.org/10.1145/1780.1781",
    "publication_date": "1984-10-01",
    "publication_year": 1984,
    "authors": "Richard Bird",
    "corresponding_authors": "Richard Bird",
    "abstract": "article Free Access Share on The promotion and accumulation strategies in transformational programming Author: R. S. Bird Oxford University Oxford UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 4Oct. 1984 pp 487–504https://doi.org/10.1145/1780.1781Online:01 October 1984Publication History 109citation583DownloadsMetricsTotal Citations109Total Downloads583Last 12 Months52Last 6 weeks8 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 134,
    "openalex_id": "https://openalex.org/W1993075518",
    "type": "article"
  },
  {
    "title": "Specifying representations of machine instructions",
    "doi": "https://doi.org/10.1145/256167.256225",
    "publication_date": "1997-05-01",
    "publication_year": 1997,
    "authors": "Norman F. Ramsey; Mary Fernández",
    "corresponding_authors": "",
    "abstract": "We present SLED, a specification language for Encoding and Decoding, which describes, abstract, binary, and assembly-language representations of machine instructions. Guided by a SLED specification, the New Jersey Machine-Code Toolkit generates bit-manipulating code for use in applications that process machine code. Programmers can write such applications at an assembly language level of abstraction, and the toolkit enables the applications to recognize and emit the binary representations used by the hardware. SLED is suitable for describing both CISC and RISC machines; we have specified representations of MIPS R3000, SPARC, Alpha, and Intel Pentium instructions, and toolkit users have written specifications for the Power PC and Motorola 68000. The article includes representative excerpts from our SPARC and Pentium specifications. SLED uses four elements; fields and tokens describe parts of instructions; patterns describe binary representations of instructions or group of instructions; and constructors map between the abstract and binary levels. By combining the elements in different ways, SLED supports machine-independent implementations of machine-level concepts like conditional assembly, span-dependent instructions, relocatable addresses, object code, sections, and relocation. SLED specifications can be checked automatically for consistency with existing assemblers. The implementation of the toolkit is largely determined by our representations of patterns and constructors. We use a normal form that facilitates construction of encoders and decoders. The article describes the normal form and its use. The toolkit has been used to help build several applications. We have built a retargetable debugger and a retargetable, optimizing linker. Colleagues have built a dynamic code generator, a decompiler, and an execution-time analyzer. The toolkit generates efficient code; for example, the linker emits binary up to 15% faster than it emits assembly language, making it 1.7-2 times faster to produce an a.out directly than by using the assembler.",
    "cited_by_count": 134,
    "openalex_id": "https://openalex.org/W2024347417",
    "type": "article"
  },
  {
    "title": "A region inference algorithm",
    "doi": "https://doi.org/10.1145/291891.291894",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Mads Tofte; Lars Birkedal",
    "corresponding_authors": "",
    "abstract": "Region Inference is a program analysis which infers lifetimes of values. It is targeted at a runtime model in which the store consists of a stack of regions and memory management predominantly consists of pushing and popping regions, rather than performing garbage collection. Region Inference has previously been specified by a set of inference rules which formalize when regions may be allocated and deallocated. This article presents an algorithm which implements the specification. We prove that the algorithm is sound with respect to the region inference rules and that it always terminates even though the region inference rules permit polymorphic recursion in regions. The algorithm is the result of several years of experiments with region inference algorithms in the ML Kit, a compiler from Standard ML to assembly language. We report on practical experience with the algorithm and give hints on how to implement it.",
    "cited_by_count": 133,
    "openalex_id": "https://openalex.org/W2006638707",
    "type": "article"
  },
  {
    "title": "Data abstraction and information hiding",
    "doi": "https://doi.org/10.1145/570886.570888",
    "publication_date": "2002-09-01",
    "publication_year": 2002,
    "authors": "K. Rustan M. Leino; Greg Nelson",
    "corresponding_authors": "",
    "abstract": "This article describes an approach for verifying programs in the presence of data abstraction and information hiding, which are key features of modern programming languages with objects and modules. This article draws on our experience building and using an automatic program checker, and focuses on the property of modular soundness : that is, the property that the separate verifications of the individual modules of a program suffice to ensure the correctness of the composite program. We found this desirable property surprisingly difficult to achieve. A key feature of our methodology for modular soundness is a new specification construct: the abstraction dependency , which reveals which concrete variables appear in the representation of a given abstract variable, without revealing the abstraction function itself. This article discusses in detail two varieties of abstraction dependencies: static and dynamic. The article also presents a new technical definition of modular soundness as a monotonicity property of verifiability with respect to scope and uses this technical definition to formally prove the modular soundness of a programming discipline for static dependencies.",
    "cited_by_count": 132,
    "openalex_id": "https://openalex.org/W2136371406",
    "type": "article"
  },
  {
    "title": "Lazy caching",
    "doi": "https://doi.org/10.1145/151646.151651",
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "Yehuda Afek; Geoffrey Brown; Michael Merritt",
    "corresponding_authors": "",
    "abstract": "This paper examines cache consistency conditions for multiprocessor shared memory systems. It states and motivates a weaker condition than is normally implemented. An algorithm is presented that exploits the weaker condition to achieve greater concurrency. The algorithm is shown to satisfy the weak consistency condition. Other properties of the algorithm and possible extensions are discussed.",
    "cited_by_count": 130,
    "openalex_id": "https://openalex.org/W2295903464",
    "type": "article"
  },
  {
    "title": "Code selection through object code optimization",
    "doi": "https://doi.org/10.1145/1780.1783",
    "publication_date": "1984-10-01",
    "publication_year": 1984,
    "authors": "Jack W. Davidson; Christopher W. Fraser",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Code selection through object code optimization Authors: Jack W. Davidson University of Virginia University of VirginiaView Profile , Christopher W. Fraser University of Arizona University of ArizonaView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 401 October 1984pp 505–526https://doi.org/10.1145/1780.1783Published:01 October 1984Publication History 95citation1,115DownloadsMetricsTotal Citations95Total Downloads1,115Last 12 Months161Last 6 weeks19 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 129,
    "openalex_id": "https://openalex.org/W2054572543",
    "type": "article"
  },
  {
    "title": "An interval-based approach to exhaustive and incremental interprocedural data-flow analysis",
    "doi": "https://doi.org/10.1145/78969.78963",
    "publication_date": "1990-07-01",
    "publication_year": 1990,
    "authors": "Michael Burke",
    "corresponding_authors": "Michael Burke",
    "abstract": "We reformulate interval analysis so that it can he applied to any monotone data-flow problem, including the nonfast problems of flow-insensitive interprocedural analysis. We then develop an incremental interval analysis technique that can be applied to the same class of problems. When applied to flow-insensitive interprocedural data-flow problems, the resulting algorithms are simple, practical, and efficient. With a single update, the incremental algorithm can accommodate any sequence of program changes that does not alter the structure of the program call graph. It can also accommodate a large class of structural changes. For alias analysis, we develop an incremental algorithm that obtains the exact solution as computed by an exhaustive algorithm. Finally, we develop a transitive closure algorithm that is particularly well suited to the very sparse matrices associated with the problems we address.",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W1984914680",
    "type": "article"
  },
  {
    "title": "Writing Larch interface language specifications",
    "doi": "https://doi.org/10.1145/9758.10500",
    "publication_date": "1987-01-01",
    "publication_year": 1987,
    "authors": "Jeannette M. Wing",
    "corresponding_authors": "Jeannette M. Wing",
    "abstract": "Current research in specifications is emphasizing the practical use of formal specifications in program design. One way to encourage their use in practice is to provide specification languages that are accessible to both designers and programmers. With this goal in mind, the Larch family of formal specification languages has evolved to support a two-tiered approach to writing specifications. This approach separates the specification of state transformations and programming language dependencies from the specification of underlying abstractions. Thus, each member of the Larch family has a subset derived from a programming language and another subset independent of any programming languages. We call the former interface languages, and the latter the Larch Shared Language. This paper focuses on Larch interface language specifications. Through examples, we illustrate some salient features of Larch/CLU, a Larch interface language for the programming language CLU. We give an example of writing an interface specification following the two-tiered approach and discuss in detail issues involved in writing interface specifications and their interaction with their Shared Language components.",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W2013492611",
    "type": "article"
  },
  {
    "title": "Automatic Derivation of Code Generators from Machine Descriptions",
    "doi": "https://doi.org/10.1145/357094.357097",
    "publication_date": "1980-04-01",
    "publication_year": 1980,
    "authors": "R. G. G. Cattell",
    "corresponding_authors": "R. G. G. Cattell",
    "abstract": "Work with compiler compilers has dealt principally with automatic generation of parsers and lexical analyzers. Until recently, little work has been done on formalizing and generating the back end of a compiler, particularly an optimizing compiler. This paper describes formalizations of machines and code generators and describes a scheme for the automatic derivation of code generators from machine descriptions. It was possible to separate all machine dependence from the code generation algorithms for a wide range of typical architectures (IBM-360, PDP-11, PDP-10, Intel 8080) while retaining good code quality. Heuristic search methods from work in artificial intelligence were found to be both fast and general enough for use in generation of code generators with the machine representation proposed. A scheme is proposed to perform as much analysis as possible at code generator generation time, resulting in a fast pattern-matching code generator. The algorithms and representations were implemented to test their practicality in use.",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W2087585770",
    "type": "article"
  },
  {
    "title": "Smart recompilation",
    "doi": "https://doi.org/10.1145/5956.5959",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Walter F. Tichy",
    "corresponding_authors": "Walter F. Tichy",
    "abstract": "With current compiler technology, changing a single line in a large software system may trigger massive recompilations. If the change occurs in a file with shared declarations, all compilation units depending upon that file must be recompiled to assure consistency. However, many of those recompilations may be redundant, because the change may affect only a small fraction of the overall system. Smart recompilation is a method for reducing the set of modules that must be recompiled after a change. The method determines whether recompilation is necessary by isolating the differences among program modules and analyzing the effect of changes. The method is applicable to languages with and without overloading. A prototype demonstrates that the method is efficient and can be added with modest effort to existing compilers.",
    "cited_by_count": 127,
    "openalex_id": "https://openalex.org/W2294979290",
    "type": "article"
  },
  {
    "title": "Implementation of resilient, atomic data types",
    "doi": "https://doi.org/10.1145/3318.3319",
    "publication_date": "1985-04-01",
    "publication_year": 1985,
    "authors": "William E. Weihl; Barbara Liskov",
    "corresponding_authors": "",
    "abstract": "A major issue in many applications is how to preserve the consistency of data in the presence of concurrency and hardware failures. We suggest addressing this problem by implementing applications in terms of abstract data types with two properties: Their objects are atomic (they provide serializability and recoverability for activities using them) and resilient (they survive hardware failures with acceptably high probability). We define what it means for abstract data types to be atomic and resilient. We also discuss issues that arise in implementing such types, and describe a particular linguistic mechanism provided in the Argus programming language.",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W1998735458",
    "type": "article"
  },
  {
    "title": "Experimental evaluation of a generic abstract interpretation algorithm for PROLOG",
    "doi": "https://doi.org/10.1145/174625.174627",
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Baudouin Le Charlier; Pascal Van Hentenryck",
    "corresponding_authors": "",
    "abstract": "Abstract interpretation of PROLOG programs has attracted many researchers in recent years, partly because of the potential for optimization in PROLOG compilers and partly because of the declarative nature of logic programming languages that make them more amenable to optimization than procedural languages. Most of the work, however, has remained at the theoretical level, focusing on the developments of frameworks and the definition of abstract domains. This paper reports our effort to verify experimentally the practical value of this area of research. It describes the design and implementation of the generic abstract interpretation algorithm GAIA that we originally proposed in Le Charlier et al. [1991], its instantiation to a sophisticated abstract domain (derived from Bruynooghe and Janssens [1988]) containing modes, types, sharing, and aliasing, and its evaluation both in terms of performance and accuracy. The overall implementation (over 5000 lines of Pascal) has been systematically analyzed on a variety of programs and compared with the complexity analysis of Le Charlie et al. [1991] and the specific analysis systems of Hickey and Mudambi [1989], Taylor [1989; 1990], Van Roy and Despain [1990], and Warren et al. [1988].",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W2024886390",
    "type": "article"
  },
  {
    "title": "Synchronizing Resources",
    "doi": "https://doi.org/10.1145/357146.357149",
    "publication_date": "1981-10-01",
    "publication_year": 1981,
    "authors": "Gregory R. Andrews",
    "corresponding_authors": "Gregory R. Andrews",
    "abstract": "article Free Access Share on Synchronizing Resources Author: Gregory R. Andrews Department of Computer Science, The University of Arizona, Tucson, AZ Department of Computer Science, The University of Arizona, Tucson, AZView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 401 October 1981pp 405–430https://doi.org/10.1145/357146.357149Published:01 October 1981Publication History 93citation589DownloadsMetricsTotal Citations93Total Downloads589Last 12 Months30Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 125,
    "openalex_id": "https://openalex.org/W2294581346",
    "type": "article"
  },
  {
    "title": "Heap reference analysis using access graphs",
    "doi": "https://doi.org/10.1145/1290520.1290521",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Uday P. Khedker; Amitabha Sanyal; Amey Karkare",
    "corresponding_authors": "",
    "abstract": "Despite significant progress in the theory and practice of program analysis, analyzing properties of heap data has not reached the same level of maturity as the analysis of static and stack data. The spatial and temporal structure of stack and static data is well understood while that of heap data seems arbitrary and is unbounded. We devise bounded representations that summarize properties of the heap data. This summarization is based on the structure of the program that manipulates the heap. The resulting summary representations are certain kinds of graphs called access graphs . The boundedness of these representations and the monotonicity of the operations to manipulate them make it possible to compute them through data flow analysis. An important application that benefits from heap reference analysis is garbage collection, where currently liveness is conservatively approximated by reachability from program variables. As a consequence, current garbage collectors leave a lot of garbage uncollected, a fact that has been confirmed by several empirical studies. We propose the first ever end-to-end static analysis to distinguish live objects from reachable objects. We use this information to make dead objects unreachable by modifying the program. This application is interesting because it requires discovering data flow information representing complex semantics. In particular, we formulate the following new analyses for heap data: liveness, availability, and anticipability and propose solution methods for them. Together, they cover various combinations of directions of analysis (i.e., forward and backward) and confluence of information (i.e. union and intersection). Our analysis can also be used for plugging memory leaks in C/C++ languages.",
    "cited_by_count": 125,
    "openalex_id": "https://openalex.org/W3124619212",
    "type": "article"
  },
  {
    "title": "Creating user interfaces using programming by example, visual programming, and constraints",
    "doi": "https://doi.org/10.1145/78942.78943",
    "publication_date": "1990-04-01",
    "publication_year": 1990,
    "authors": "Brad A. Myers",
    "corresponding_authors": "Brad A. Myers",
    "abstract": "Peridot is an experimental tool that allows designers to create user interface components without conventional programming. The designer draws pictures of what the interface should look like and then uses the mouse and other input devices to demonstrate how the interface should operate. Peridot generalizes from these example pictures and actions to create parameterized procedures, such as those found in conventional user interface libraries such as the Macintosh Toolbox. Peridot uses visual programming, programming by example, constraints, and plausible inferencing to allow nonprogrammers to create menus, buttons, scroll bars, and many other interaction techniques easily and quickly. Peridot created its own interface and can create almost all of the interaction techniques in the Macintosh Toolbox. Therefore, Peridot demonstrates that it is possible to provide sophisticated programming capabilities to nonprogrammers in an easy-to-use manner and still have sufficient power to generate interesting and useful programs.",
    "cited_by_count": 123,
    "openalex_id": "https://openalex.org/W2079566271",
    "type": "article"
  },
  {
    "title": "The Design and Application of a Retargetable Peephole Optimizer",
    "doi": "https://doi.org/10.1145/357094.357098",
    "publication_date": "1980-04-01",
    "publication_year": 1980,
    "authors": "Jack W. Davidson; Christopher W. Fraser",
    "corresponding_authors": "",
    "abstract": "Peephole optimizers improve object code by replacing certain sequences of instructions with better sequences. This paper describes PO, a peephole optimizer that uses a symbolic machine description to simulate pairs of adjacent instructions, replacing them, where possible, with an equivalent single instruction. As a result of this organization, PO is machine independent and can be described formally and concisely: when PO is finished, no instruction, and no pair of adjacent instructions, can be replaced with a cheaper single instruction that has the same effect. This thoroughness allows PO to relieve code generators of much case analysis; for example, they might produce only load/add-register sequences and rely on PO to, where possible, discard them in favor or add-memory, add-immediate, or increment instructions. Experiments indicate that naive code generators can give good code if used with PO.",
    "cited_by_count": 122,
    "openalex_id": "https://openalex.org/W2065496085",
    "type": "article"
  },
  {
    "title": "Adaptive functional programming",
    "doi": "https://doi.org/10.1145/1186632.1186634",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Umut A. Acar; Guy E. Blelloch; Robert Harper",
    "corresponding_authors": "",
    "abstract": "We present techniques for incremental computing by introducing adaptive functional programming. As an adaptive program executes, the underlying system represents the data and control dependences in the execution in the form of a dynamic dependence graph . When the input to the program changes, a change propagation algorithm updates the output and the dynamic dependence graph by propagating changes through the graph and re-executing code where necessary. Adaptive programs adapt their output to any change in the input, small or large.We show that adaptivity techniques are practical by giving an efficient implementation as a small ML library. The library consists of three operations for making a program adaptive, plus two operations for making changes to the input and adapting the output to these changes. We give a general bound on the time it takes to adapt the output, and based on this, show that an adaptive Quicksort adapts its output in logarithmic time when its input is extended by one key.To show the safety and correctness of the mechanism we give a formal definition of AFL, a call-by-value functional language extended with adaptivity primitives. The modal type system of AFL enforces correct usage of the adaptivity mechanism, which can only be checked at run time in the ML library. Based on the AFL dynamic semantics, we formalize thechange-propagation algorithm and prove its correctness.",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W1971597822",
    "type": "article"
  },
  {
    "title": "A termination analyzer for Java bytecode based on path-length",
    "doi": "https://doi.org/10.1145/1709093.1709095",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Fausto Spoto; Fred Mesnard; Étienne Payet",
    "corresponding_authors": "",
    "abstract": "It is important to prove that supposedly terminating programs actually terminate, particularly if those programs must be run on critical systems or downloaded into a client such as a mobile phone. Although termination of computer programs is generally undecidable, it is possible and useful to prove termination of a large, nontrivial subset of the terminating programs. In this article, we present our termination analyzer for sequential Java bytecode, based on a program property called path-length . We describe the analyses which are needed before the path-length can be computed such as sharing, cyclicity, and aliasing. Then we formally define the path-length analysis and prove it correct with respect to a reference denotational semantics of the bytecode. We show that a constraint logic program P CLP can be built from the result of the path-length analysis of a Java bytecode program P and formally prove that if P CLP terminates, then P also terminates. Hence a termination prover for constraint logic programs can be applied to prove the termination of P . We conclude with some discussion of the possibilities and limitations of our approach. Ours is the first existing termination analyzer for Java bytecode dealing with any kind of data structures dynamically allocated on the heap and which does not require any help or annotation on the part of the user.",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W1996245589",
    "type": "article"
  },
  {
    "title": "An Effective Implementation for the Generalized Input-Output Construct of CSP",
    "doi": "https://doi.org/10.1145/69624.357208",
    "publication_date": "1983-04-01",
    "publication_year": 1983,
    "authors": "Gael N. Buckley; Abraham Silberschatz",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on An Effective Implementation for the Generalized Input-Output Construct of CSP Authors: G. N. Buckley Department of Computer Sciences, College of Natural Sciences, The University of Texas at Austin, Austin, TX Department of Computer Sciences, College of Natural Sciences, The University of Texas at Austin, Austin, TXView Profile , Abraham Silberschatz Department of Computer Sciences, College of Natural Sciences, The University of Texas at Austin, Austin, TX Department of Computer Sciences, College of Natural Sciences, The University of Texas at Austin, Austin, TXView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 2April 1983 pp 223–235https://doi.org/10.1145/69624.357208Published:01 April 1983Publication History 92citation420DownloadsMetricsTotal Citations92Total Downloads420Last 12 Months30Last 6 weeks8 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W2059563369",
    "type": "article"
  },
  {
    "title": "Deriving Target Code as a Representation of Continuation Semantics",
    "doi": "https://doi.org/10.1145/357172.357179",
    "publication_date": "1982-07-01",
    "publication_year": 1982,
    "authors": "Mitchell Wand",
    "corresponding_authors": "Mitchell Wand",
    "abstract": "article Free Access Share on Deriving Target Code as a Representation of Continuation Semantics Author: Mitchell Wand Computer Science Department, Indiana University, Bloomington, IN Computer Science Department, Indiana University, Bloomington, INView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 3pp 496–517https://doi.org/10.1145/357172.357179Published:01 July 1982Publication History 82citation561DownloadsMetricsTotal Citations82Total Downloads561Last 12 Months76Last 6 weeks8 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W1987031623",
    "type": "article"
  },
  {
    "title": "The ``Hoare Logic'' of CSP, and All That",
    "doi": "https://doi.org/10.1145/2993.357247",
    "publication_date": "1984-04-01",
    "publication_year": 1984,
    "authors": "Leslie Lamport; Fred B. Schneider",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on The ``Hoare Logic'' of CSP, and All That Authors: Leslie Lamport Computer Science Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA Computer Science Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CAView Profile , Fred B. Schneider Department of Computer Science, Cornell University, Ithaca, NY Department of Computer Science, Cornell University, Ithaca, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 2pp 281–296https://doi.org/10.1145/2993.357247Published:01 April 1984Publication History 79citation1,128DownloadsMetricsTotal Citations79Total Downloads1,128Last 12 Months64Last 6 weeks6 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W2123464354",
    "type": "article"
  },
  {
    "title": "A System for Assisting Program Transformation",
    "doi": "https://doi.org/10.1145/357153.357154",
    "publication_date": "1982-01-01",
    "publication_year": 1982,
    "authors": "Martin S. Feather",
    "corresponding_authors": "Martin S. Feather",
    "abstract": "article Free Access Share on A System for Assisting Program Transformation Author: Martin S. Feather USC/Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA USC/Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 1Jan. 1982 pp 1–20https://doi.org/10.1145/357153.357154Published:01 January 1982Publication History 99citation430DownloadsMetricsTotal Citations99Total Downloads430Last 12 Months17Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 117,
    "openalex_id": "https://openalex.org/W1988901815",
    "type": "article"
  },
  {
    "title": "An Automatic Technique for Selection of Data Representations in SETL Programs",
    "doi": "https://doi.org/10.1145/357133.357135",
    "publication_date": "1981-04-01",
    "publication_year": 1981,
    "authors": "Edmond Schonberg; Jacob T. Schwartz; Micha Sharir",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on An Automatic Technique for Selection of Data Representations in SETL Programs Authors: Edmond Schonberg Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NY Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NYView Profile , Jacob T. Schwartz Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NY Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NYView Profile , Micha Sharir Department of Mathematics, Tel Aviv University, Tel Aviv, Israel Department of Mathematics, Tel Aviv University, Tel Aviv, IsraelView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 2April 1981 pp 126–143https://doi.org/10.1145/357133.357135Published:01 April 1981Publication History 87citation522DownloadsMetricsTotal Citations87Total Downloads522Last 12 Months39Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 116,
    "openalex_id": "https://openalex.org/W2006875594",
    "type": "article"
  },
  {
    "title": "Hybrid type checking",
    "doi": "https://doi.org/10.1145/1667048.1667051",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Kenneth Knowles; Cormac Flanagan",
    "corresponding_authors": "",
    "abstract": "Traditional static type systems are effective for verifying basic interface specifications. Dynamically checked contracts support more precise specifications, but these are not checked until runtime, resulting in incomplete detection of defects. Hybrid type checking is a synthesis of these two approaches that enforces precise interface specifications, via static analysis where possible, but also via dynamic checks where necessary. This article explores the key ideas and implications of hybrid type checking, in the context of the λ-calculus extended with contract types , that is, with dependent function types and with arbitrary refinements of base types.",
    "cited_by_count": 116,
    "openalex_id": "https://openalex.org/W2144160229",
    "type": "article"
  },
  {
    "title": "Efficient constraint propagation engines",
    "doi": "https://doi.org/10.1145/1452044.1452046",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "Christian Schulte; Peter J. Stuckey",
    "corresponding_authors": "",
    "abstract": "This article presents a model and implementation techniques for speeding up constraint propagation. Three fundamental approaches to improving constraint propagation based on propagators as implementations of constraints are explored: keeping track of which propagators are at fixpoint, choosing which propagator to apply next, and how to combine several propagators for the same constraint. We show how idempotence reasoning and events help track fixpoints more accurately. We improve these methods by using them dynamically (taking into account current variable domains to improve accuracy). We define priority-based approaches to choosing a next propagator and show that dynamic priorities can improve propagation. We illustrate that the use of multiple propagators for the same constraint can be advantageous with priorities, and introduce staged propagators that combine the effects of multiple propagators with priorities for greater efficiency.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W2035811095",
    "type": "article"
  },
  {
    "title": "Revisiting coroutines",
    "doi": "https://doi.org/10.1145/1462166.1462167",
    "publication_date": "2009-02-01",
    "publication_year": 2009,
    "authors": "Ana Lúcia de Moura; Roberto Ierusalimschy",
    "corresponding_authors": "",
    "abstract": "This article advocates the revival of coroutines as a convenient general control abstraction. After proposing a new classification of coroutines, we introduce the concept of full asymmetric coroutines and provide a precise definition for it through an operational semantics. We then demonstrate that full coroutines have an expressive power equivalent to one-shot continuations and one-shot delimited continuations. We also show that full asymmetric coroutines and one-shot delimited continuations have many similarities, and therefore present comparable benefits. Nevertheless, coroutines are easier implemented and understood, especially in the realm of procedural languages.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W2295885203",
    "type": "article"
  },
  {
    "title": "Procedures as persistent data objects",
    "doi": "https://doi.org/10.1145/4472.4477",
    "publication_date": "1985-10-01",
    "publication_year": 1985,
    "authors": "Malcolm Atkinson; Ronald Morrison",
    "corresponding_authors": "",
    "abstract": "A persistent programming environment, together with a language that supports first class procedures, may be used to provide the semantic features of other object modeling languages. In particular, the two concepts may be combined to implement abstract data types, modules, separate compilation, views, and data protection. Furthermore, the ideas may be used in system construction and version control, as demonstrated here.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2145764100",
    "type": "article"
  },
  {
    "title": "Programming by Refinement, as Exemplified by the SETL Representation Sublanguage",
    "doi": "https://doi.org/10.1145/357062.357064",
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "Robert Dewar; Arthur Grand; Ssu-Cheng Liu; Jacob T. Schwartz; Edmond Schonberg",
    "corresponding_authors": "",
    "abstract": "“Pure” SETL is a language of very high level allowing algorithms to be programmed rapidly and succintly. SETL's representation sublanguage adds a system of declarations which allow the user of the language to control the data structures that will be used to implement an algorithm which has already been written in pure SETL, so as to improve its efficiency. Ideally no rewriting of the algorithm should be necessary. The facilities provided by the representation sublanguage and the run-time data structures that it can generate are described; based on this a heuristic which uses some of the methods of global program analysis and which should be capable of selecting an acceptably efficient representation automatically is given.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2040073555",
    "type": "article"
  },
  {
    "title": "Efficient field-sensitive pointer analysis of C",
    "doi": "https://doi.org/10.1145/1290520.1290524",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "David J. Pearce; Paul H. J. Kelly; Chris Hankin",
    "corresponding_authors": "",
    "abstract": "The subject of this article is flow- and context-insensitive pointer analysis. We present a novel approach for precisely modelling struct variables and indirect function calls. Our method emphasises efficiency and simplicity and is based on a simple language of set constraints. We obtain an O ( v 4 ) bound on the time needed to solve a set of constraints from this language, where v is the number of constraint variables. This gives, for the first time, some insight into the hardness of performing field-sensitive pointer analysis of C. Furthermore, we experimentally evaluate the time versus precision trade-off for our method by comparing against the field-insensitive equivalent. Our benchmark suite consists of 11 common C programs ranging in size from 15,000 to 200,000 lines of code. Our results indicate the field-sensitive analysis is more expensive to compute, but yields significantly better precision. In addition, our technique has been integrated into the latest release (version 4.1) of the GNU Compiler GCC. Finally, we identify several previously unknown issues with an alternative and less precise approach to modelling struct variables, known as field-based analysis.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2148755014",
    "type": "article"
  },
  {
    "title": "LOCKSMITH",
    "doi": "https://doi.org/10.1145/1889997.1890000",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Polyvios Pratikakis; Jeffrey S. Foster; Michael Hicks",
    "corresponding_authors": "",
    "abstract": "Locksmith is a static analysis tool for automatically detecting data races in C programs. In this article, we describe each of Locksmith's component analyses precisely, and present systematic measurements that isolate interesting trade-offs between precision and efficiency in each analysis. Using a benchmark suite comprising stand-alone applications and Linux device drivers totaling more than 200,000 lines of code, we found that a simple no-worklist strategy yielded the most efficient interprocedural dataflow analysis; that our sharing analysis was able to determine that most locations are thread-local, and therefore need not be protected by locks; that modeling C structs and void pointers precisely is key to both precision and efficiency; and that context sensitivity yields a much more precise analysis, though with decreased scalability. Put together, our results illuminate some of the key engineering challenges in building Locksmith and data race detection analyses in particular, and constraint-based program analyses in general.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2045238089",
    "type": "article"
  },
  {
    "title": "The impact of interprocedural analysis and optimization in the R <sup>n</sup> programming environment",
    "doi": "https://doi.org/10.1145/6465.6489",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Keith D. Cooper; Ken Kennedy; Linda Torczon",
    "corresponding_authors": "",
    "abstract": "In spite of substantial progress in the theory of interprocedural data flow analysis, few practical compiling systems can afford to apply it to produce more efficient object programs. To perform interprocedural analysis, a compiler needs not only the source code of the module being compiled, but also information about the side effects of every procedure in the program containing that module, even separately compiled procedures. In a conventional batch compiler system, the increase in compilation time required to gather this information would make the whole process impractical. In an integrated programming environment, however, other tools can cooperate with the compiler to compute the necessary interprocedural information incrementally . as the program is being developed, decreasing both the overall cost of the analysis and the cost of individual compilations. A central goal of the R n project at Rice University is to construct a prototype software development environment that is designed to build whole programs, rather than just individual modules. It employs interprocedural analysis and optimization to produce high-quality machine code for whole programs. This paper presents an overview of the methods used by the environment to accomplish this task and discusses the impact of these methods on the various environment components. The responsibilities of each component of the environment for the preparation and use of interprocedural information are presented in detail.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2060630116",
    "type": "article"
  },
  {
    "title": "A new foundation for control dependence and slicing for modern program structures",
    "doi": "https://doi.org/10.1145/1275497.1275502",
    "publication_date": "2007-08-02",
    "publication_year": 2007,
    "authors": "Venkatesh-Prasad Ranganath; Torben Amtoft; Anindya Banerjee; John Hatcliff; Matthew B. Dwyer",
    "corresponding_authors": "",
    "abstract": "The notion of control dependence underlies many program analysis and transformation techniques. Despite being widely used, existing definitions and approaches to calculating control dependence are difficult to apply directly to modern program structures because these make substantial use of exception processing and increasingly support reactive systems designed to run indefinitely. This article revisits foundational issues surrounding control dependence, and develops definitions and algorithms for computing several variations of control dependence that can be directly applied to modern program structures. To provide a foundation for slicing reactive systems, the article proposes a notion of slicing correctness based on weak bisimulation, and proves that some of these new definitions of control dependence generate slices that conform to this notion of correctness. This new framework of control dependence definitions, with corresponding correctness results, is even able to support programs with irreducible control flow graphs. Finally, a variety of properties show that the new definitions conservatively extend classic definitions. These new definitions and algorithms form the basis of the Indus Java slicer, a publicly available program slicer that has been implemented for full Java.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2076807040",
    "type": "article"
  },
  {
    "title": "A programming model for concurrent object-oriented programs",
    "doi": "https://doi.org/10.1145/1452044.1452045",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "Bart Jacobs; Frank Piessens; Jan Smans; K. Rustan M. Leino; Wolfram Schulte",
    "corresponding_authors": "",
    "abstract": "Reasoning about multithreaded object-oriented programs is difficult, due to the nonlocal nature of object aliasing and data races. We propose a programming regime (or programming model ) that rules out data races, and enables local reasoning in the presence of object aliasing and concurrency. Our programming model builds on the multithreading and synchronization primitives as they are present in current mainstream programming languages. Java or C# programs developed according to our model can be annotated by means of stylized comments to make the use of the model explicit. We show that such annotated programs can be formally verified to comply with the programming model. If the annotated program verifies, the underlying Java or C# program is guaranteed to be free from data races, and it is sound to reason locally about program behavior. Verification is modular: a program is valid if all methods are valid, and validity of a method does not depend on program elements that are not visible to the method. We have implemented a verifier for programs developed according to our model in a custom build of the Spec# programming system, and we have validated our approach on a case study.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W2113058702",
    "type": "article"
  },
  {
    "title": "Efficient Computation of LALR(1) Look-Ahead Sets",
    "doi": "https://doi.org/10.1145/69622.357187",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "Frank DeRemer; Thomas J. Pennello",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Efficient Computation of LALR(1) Look-Ahead Sets Authors: Frank DeRemer Computer and Information Sciences, University of California, Santa Cruz, CA Computer and Information Sciences, University of California, Santa Cruz, CAView Profile , Thomas Pennello Computer and Information Sciences, University of California, Santa Cruz, CA Computer and Information Sciences, University of California, Santa Cruz, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 615–649https://doi.org/10.1145/69622.357187Published:01 October 1982Publication History 83citation2,258DownloadsMetricsTotal Citations83Total Downloads2,258Last 12 Months471Last 6 weeks49 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W1966686491",
    "type": "article"
  },
  {
    "title": "Algorithms for on-the-fly garbage collection",
    "doi": "https://doi.org/10.1145/579.587",
    "publication_date": "1984-07-01",
    "publication_year": 1984,
    "authors": "Mordechai Ben‐Ari",
    "corresponding_authors": "Mordechai Ben‐Ari",
    "abstract": "article Free Access Share on Algorithms for on-the-fly garbage collection Author: Mordechai Ben-Ari Tel Aviv University Tel Aviv UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 3pp 333–344https://doi.org/10.1145/579.587Published:01 July 1984Publication History 91citation781DownloadsMetricsTotal Citations91Total Downloads781Last 12 Months94Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2112630372",
    "type": "article"
  },
  {
    "title": "Algorithmic verification of asynchronous programs",
    "doi": "https://doi.org/10.1145/2160910.2160915",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "Pierre Ganty; Rupak Majumdar",
    "corresponding_authors": "",
    "abstract": "Asynchronous programming is a ubiquitous systems programming idiom for managing concurrent interactions with the environment. In this style, instead of waiting for time-consuming operations to complete, the programmer makes a non-blocking call to the operation and posts a callback task to a task buffer that is executed later when the time-consuming operation completes. A cooperative scheduler mediates the interaction by picking and executing callback tasks from the task buffer to completion (and these callbacks can post further callbacks to be executed later). Writing correct asynchronous programs is hard because the use of callbacks, while efficient, obscures program control flow. We provide a formal model underlying asynchronous programs and study verification problems for this model. We show that the safety verification problem for finite-data asynchronous programs is expspace-complete. We show that liveness verification for finite-data asynchronous programs is decidable and polynomial-time equivalent to Petri net reachability. Decidability is not obvious, since even if the data is finite-state, asynchronous programs constitute infinite-state transition systems: both the program stack for an executing task and the task buffer of pending calls to tasks can be potentially unbounded. Our main technical constructions are polynomial-time, semantics-preserving reductions from asynchronous programs to Petri nets and back. The first reduction allows the use of algorithmic techniques on Petri nets for the verification of asynchronous programs, and the second allows lower bounds on Petri nets to apply also to asynchronous programs. We also study several extensions to the basic models of asynchronous programs that are inspired by additional capabilities provided by implementations of asynchronous libraries and classify the decidability and undecidability of verification questions on these extensions.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W2031575609",
    "type": "article"
  },
  {
    "title": "Secure Compilation to Protected Module Architectures",
    "doi": "https://doi.org/10.1145/2699503",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "Marco Patrignani; Pieter Agten; Raoul Strackx; Bart Jacobs; Dave Clarke; Frank Piessens",
    "corresponding_authors": "",
    "abstract": "A fully abstract compiler prevents security features of the source language from being bypassed by an attacker operating at the target language level. Unfortunately, developing fully abstract compilers is very complex, and it is even more so when the target language is an untyped assembly language. To provide a fully abstract compiler that targets untyped assembly, it has been suggested to extend the target language with a protected module architecture—an assembly-level isolation mechanism which can be found in next-generation processors. This article provides a fully abstract compilation scheme whose source language is an object-oriented, high-level language and whose target language is such an extended assembly language. The source language enjoys features such as dynamic memory allocation and exceptions. Secure compilation of first-order method references, cross-package inheritance, and inner classes is also presented. Moreover, this article contains the formal proof of full abstraction of the compilation scheme. Measurements of the overhead introduced by the compilation scheme indicate that it is negligible.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W2046697462",
    "type": "article"
  },
  {
    "title": "Towards a Compiler for Reals",
    "doi": "https://doi.org/10.1145/3014426",
    "publication_date": "2017-03-10",
    "publication_year": 2017,
    "authors": "Eva Darulová; Viktor Kunčak",
    "corresponding_authors": "",
    "abstract": "Numerical software, common in scientific computing or embedded systems, inevitably uses a finite-precision approximation of the real arithmetic in which most algorithms are designed. In many applications, the roundoff errors introduced by finite-precision arithmetic are not the only source of inaccuracy, and measurement and other input errors further increase the uncertainty of the computed results. Adequate tools are needed to help users select suitable data types and evaluate the provided accuracy, especially for safety-critical applications. We present a source-to-source compiler called Rosa that takes as input a real-valued program with error specifications and synthesizes code over an appropriate floating-point or fixed-point data type. The main challenge of such a compiler is a fully automated, sound, and yet accurate-enough numerical error estimation. We introduce a unified technique for bounding roundoff errors from floating-point and fixed-point arithmetic of various precisions. The technique can handle nonlinear arithmetic, determine closed-form symbolic invariants for unbounded loops, and quantify the effects of discontinuities on numerical errors. We evaluate Rosa on a number of benchmarks from scientific computing and embedded systems and, comparing it to the state of the art in automated error estimation, show that it presents an interesting tradeoff between accuracy and performance.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2299696796",
    "type": "article"
  },
  {
    "title": "Environmental bisimulations for higher-order languages",
    "doi": "https://doi.org/10.1145/1889997.1890002",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Davide Sangiorgi; Naoki Kobayashi; Eijiro Sumii",
    "corresponding_authors": "",
    "abstract": "Developing a theory of bisimulation in higher-order languages can be hard. Particularly challenging can be: (1) the proof of congruence, as well as enhancements of the bisimulation proof method with “up-to context” techniques, and (2) obtaining definitions and results that scale to languages with different features. To meet these challenges, we present environment{} bisimulations , a form of bisimulation for higher-order languages, and its basic theory. We consider four representative calculi: pure λ-calculi (call-by-name and call-by-value), call-by-value λ-calculus with higher-order store, and then Higher-Order π-calculus. In each case: we present the basic properties of environment bisimilarity, including congruence; we show that it coincides with contextual equivalence; we develop some up-to techniques, including up-to context, as examples of possible enhancements of the associated bisimulation method. Unlike previous approaches (such as applicative bisimulations, logical relations, Sumii-Pierce-Koutavas-Wand), our method does not require induction/indices on evaluation derivation/steps (which may complicate the proofs of congruence, transitivity, and the combination with up-to techniques), or sophisticated methods such as Howe's for proving congruence. It also scales from the pure λ-calculi to the richer calculi with simple congruence proofs.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2023149698",
    "type": "article"
  },
  {
    "title": "Probabilistic Relational Reasoning for Differential Privacy",
    "doi": "https://doi.org/10.1145/2492061",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Gilles Barthe; Boris Köpf; Federico Olmedo; Santiago Zanella-Béguelin",
    "corresponding_authors": "",
    "abstract": "Differential privacy is a notion of confidentiality that allows useful computations on sensible data while protecting the privacy of individuals. Proving differential privacy is a difficult and error-prone task that calls for principled approaches and tool support. Approaches based on linear types and static analysis have recently emerged; however, an increasing number of programs achieve privacy using techniques that fall out of their scope. Examples include programs that aim for weaker, approximate differential privacy guarantees and programs that achieve differential privacy without using any standard mechanisms. Providing support for reasoning about the privacy of such programs has been an open problem. We report on CertiPriv, a machine-checked framework for reasoning about differential privacy built on top of the Coq proof assistant. The central component of CertiPriv is a quantitative extension of probabilistic relational Hoare logic that enables one to derive differential privacy guarantees for programs from first principles. We demonstrate the applicability of CertiPriv on a number of examples whose formal analysis is out of the reach of previous techniques. In particular, we provide the first machine-checked proofs of correctness of the Laplacian, Gaussian, and exponential mechanisms and of the privacy of randomized and streaming algorithms from the literature.",
    "cited_by_count": 83,
    "openalex_id": "https://openalex.org/W2091015169",
    "type": "article"
  },
  {
    "title": "Foundations of Typestate-Oriented Programming",
    "doi": "https://doi.org/10.1145/2629609",
    "publication_date": "2014-10-28",
    "publication_year": 2014,
    "authors": "Ronald Garcia; Éric Tanter; Roger Wolff; Jonathan Aldrich",
    "corresponding_authors": "",
    "abstract": "Typestate reflects how the legal operations on imperative objects can change at runtime as their internal state changes. A typestate checker can statically ensure, for instance, that an object method is only called when the object is in a state for which the operation is well defined. Prior work has shown how modular typestate checking can be achieved thanks to access permissions and state guarantees. However, typestate was not treated as a primitive language concept: typestate checkers are an additional verification layer on top of an existing language. In contrast, a typestate-oriented programming (TSOP) language directly supports expressing typestates. For example, in the Plaid programming language, the typestate of an object directly corresponds to its class, and that class can change dynamically. Plaid objects have not only typestate-dependent interfaces but also typestate-dependent behaviors and runtime representations. This article lays foundations for TSOP by formalizing a nominal object-oriented language with mutable state that integrates typestate change and typestate checking as primitive concepts. We first describe a statically typed language—Featherweight Typestate (FT)—where the types of object references are augmented with access permissions and state guarantees. We describe a novel flow-sensitive permission-based type system for FT. Because static typestate checking is still too rigid for some applications, we then extend this language into a gradually typed language—Gradual Featherweight Typestate (GFT). This language extends the notion of gradual typing to account for typestate: gradual typestate checking seamlessly combines static and dynamic checking by automatically inserting runtime checks into programs. The gradual type system of GFT allows programmers to write dynamically safe code even when the static type checker can only partly verify it.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W2123691799",
    "type": "article"
  },
  {
    "title": "Analyzing Runtime and Size Complexity of Integer Programs",
    "doi": "https://doi.org/10.1145/2866575",
    "publication_date": "2016-08-02",
    "publication_year": 2016,
    "authors": "Marc Brockschmidt; Fabian Emmes; Stephan Falke; Carsten Fuhs; Jürgen Giesl",
    "corresponding_authors": "",
    "abstract": "We present a modular approach to automatic complexity analysis of integer programs. Based on a novel alternation between finding symbolic time bounds for program parts and using these to infer bounds on the absolute values of program variables, we can restrict each analysis step to a small part of the program while maintaining a high level of precision. The bounds computed by our method are polynomial or exponential expressions that depend on the absolute values of input parameters. We show how to extend our approach to arbitrary cost measures, allowing the use of our technique to find upper bounds for other expended resources, such as network requests or memory consumption. Our contributions are implemented in the open-source tool KoAT, and extensive experiments show the performance and power of our implementation in comparison with other tools.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2461846980",
    "type": "article"
  },
  {
    "title": "RustHorn: CHC-based Verification for Rust Programs",
    "doi": "https://doi.org/10.1145/3462205",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Yusuke Matsushita; Takeshi Tsukada; Naoki Kobayashi",
    "corresponding_authors": "",
    "abstract": "Reduction to satisfiability of constrained Horn clauses (CHCs) is a widely studied approach to automated program verification. Current CHC-based methods, however, do not work very well for pointer-manipulating programs, especially those with dynamic memory allocation. This article presents a novel reduction of pointer-manipulating Rust programs into CHCs, which clears away pointers and memory states by leveraging Rust’s guarantees on permission. We formalize our reduction for a simplified core of Rust and prove its soundness and completeness. We have implemented a prototype verifier for a subset of Rust and confirmed the effectiveness of our method.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W3209913027",
    "type": "article"
  },
  {
    "title": "Automatic data layout for distributed-memory machines",
    "doi": "https://doi.org/10.1145/291891.291901",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Ken Kennedy; Ulrich Kremer",
    "corresponding_authors": "",
    "abstract": "The goal of languages like Fortran D or High Performance Fortran (HPF) is to provide a simple yet efficient machine-independent parallel programming model. After the algorithm selection, the data layout choice is the key intellectual challenge in writing an efficient program in such languages. The performance of a data layout depends on the target compilation system, the target machine, the problem size, and the number of available processors. This makes the choice of a good layout extremely difficult for most users of such languages. If languages such as HPF are to find general acceptance, the need for data layout selection support has to be addressed. We beleive that the appropriate way to provide the needed support is through a tool that generates data layout specifications automatically. This article discusses the design and implementation of a data layout selection tool that generates HPF-style data layout specifications automatically. Because layout is done in a tool that is not embedded in the target compiler and hence will be run only a few times during the tuning phase of an application, it can use techniques such as integer programming that may be considered too computationally expensive for inclusion in production compilers. The proposed framework for automatic data layout selection builds and examines search spaces of candidate data layouts. A candidate layout is an efficient layout for some part of the program. After the generation of search spaces, a single candidate layout is selected for each program part, resulting in a data layout for the entire program. A good overall data layout may require the remapping of arrays between program parts. A performance estimator based on a compiler model, an execution model, and a machine model are needed to predict the execution time of each candidate layout and the costs of possible remappings between candidate data layouts. In the proposed framework, instances of NP-complete problems are solved during the construction of candidate layout search spaces and the final selection of candidate layouts from each search space. Rather than resorting to heuristics, the framework capitalizes on state-of-the-art 0-1 integer programming technology to compute optimal solutions of these NP-complete problems. A prototype data layout assistant tool based on our framework has been implemented as part of the D system currently under development at Rice University. The article reports preliminary experimental results. The results indicate that the framework is efficient and allows the generation of data layouts of high quality.",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W2008117760",
    "type": "article"
  },
  {
    "title": "Model-checking concurrent systems with unbounded integer variables",
    "doi": "https://doi.org/10.1145/325478.325480",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "Tevfik Bultan; R. Gerber; William Pugh",
    "corresponding_authors": "",
    "abstract": "Model checking is a powerful technique for analyzing large, finite-state systems. In an infinite state system, however, many basic properties are undecidable. In this article, we present a new symbolic model checker which conservatively evaluates safety and liveness properties on programs with unbounded integer variables. We use Presburger formulas to symbolically encode a program's transition system, as well as its model-checking computations. All fixpoint calculations are executed symbolically, and their convergence is guaranteed by using approximation techniques. We demonstrate the promise of this technology on some well-known infinite-state concurrency problems.",
    "cited_by_count": 125,
    "openalex_id": "https://openalex.org/W1995151557",
    "type": "article"
  },
  {
    "title": "Evidence-based static branch prediction using machine learning",
    "doi": "https://doi.org/10.1145/239912.239923",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Brad Calder; Dirk Grunwald; Michael S. Jones; Donald Lindsay; James Martin; Michael C. Mozer; Benjamin G. Zorn",
    "corresponding_authors": "",
    "abstract": "Correctly predicting the direction that branches will take is increasingly important in today's wide-issue computer architectures. The name program-based branch prediction is given to static branch prediction techniques that base their prediction on a program's structure. In this article, we investigate a new approach to program-based branch prediction that uses a body of existing programs to predict the branch behavior in a new program. We call this approach to program-based branch prediction evidence-based static prediction , or ESP. The main idea of ESP is that the behavior of a corpus of programs can be used to infer the behavior of new programs. In this article, we use neural networks and decision trees to map static features associated with each branch to a prediction that the branch will be taken. ESP shows significant advantages over other prediction mechanisms. Specifically, it is a program-based technique; it is effective across a range of programming languages and programming styles; and it does not rely on the use of expert-defined heuristics. In this article, we describe the application of ESP to the problem of static branch prediction and compare our results to existing program-based branch predictors. We also investigate the applicability of ESP across computer architectures, programming languages, compilers, and run-time systems. We provide results showing how sensitive ESP is to the number and type of static features and programs included in the ESP training sets, and we compare the efficacy of static branch prediction for subroutine libraries. Averaging over a body of 43 C and Fortran programs, ESP branch prediction results in a miss rate of 20%, as compared with the 25% miss rate obtained using the best existing program-based heuristics.",
    "cited_by_count": 125,
    "openalex_id": "https://openalex.org/W2081211681",
    "type": "article"
  },
  {
    "title": "Typed memory management via static capabilities",
    "doi": "https://doi.org/10.1145/363911.363923",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "David Walker; Karl Crary; Greg Morrisett",
    "corresponding_authors": "",
    "abstract": "Region-based memory management is an alternative to standard tracing garbage collection that makes operation such as memory deallocation explicit but verifiably safe. In this article, we present a new compiler intermediate language, called the Capability Language (CL), that supports region-based memory management and enjoys a provably safe type systems. Unlike previous region-based type system, region lifetimes need not be lexically scoped, and yet the language may be checked for safety without complex analyses. Therefore, our type system may be deployed in settings such as extensible operating systems where both the performance and safety of untrusted code is important. The central novelty of the language is the use of static capabilities to specify the permissibility of various operations, such as memory access and deallocation. In order to ensure capabilities are relinquished properly, the type system tracks aliasing information using a form of bounded quantification. Moreover, unlike previous work on region-based type systems, the proof of soundness of our type system is relatively simple, employing only standard syntactic techniques. In order to show how our language may be used in practice, we show how to translate a variant of Tofte and Talpin's high-level type-and-effects system for region-based memory management into our language. When combined with known region inference algorithms, this translation provides a way to compile source-level languages to CL.",
    "cited_by_count": 124,
    "openalex_id": "https://openalex.org/W2070111977",
    "type": "article"
  },
  {
    "title": "An abstract machine for tabled execution of fixed-order stratified logic programs",
    "doi": "https://doi.org/10.1145/291889.291897",
    "publication_date": "1998-05-01",
    "publication_year": 1998,
    "authors": "Konstantinos Sagonas; Terrance Swift",
    "corresponding_authors": "",
    "abstract": "SLG resolution uses tabling to evaluate nonfloundering normal logic pr ograms according to the well-founded semantics. The SLG-WAM, which forms the engine of the XSB system, can compute in-memory recursive queries an order of magnitute faster than current deductive databases. At the same time, the SLG-WAM tightly intergrates Prolog code with tabled SLG code, and executes Prolog code with minimal overhead compared to the WAM. As a result, the SLG-WAM brings to logic programming important termination and complexity properties of deductive databases. This article describes the architecture of the SLG-WAM for a powerful class of programs, the class of fixed-order dynamically stratified programs . We offer a detailed description of the algorithms, data structures, and instructions that the SLG-WAM adds to the WAM, and a performance analysis of engine overhead due to the extensions.",
    "cited_by_count": 122,
    "openalex_id": "https://openalex.org/W1997210046",
    "type": "article"
  },
  {
    "title": "Nesting of reducible and irreducible loops",
    "doi": "https://doi.org/10.1145/262004.262005",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Paul Havlak",
    "corresponding_authors": "Paul Havlak",
    "abstract": "Recognizing and transforming loops are essential steps in any attempt to improve the running time of a program. Aggressive restructuring techniques have been developed for single-entry (reducible) loops, but restructurers and the dataflow and dependence analysis they rely on often give up in the presence of multientry (irreducible) loops. Thus one irreducible loop can prevent the improvement of all loops in a procedure. This article give an algorithm to build a loop nesting tree for a procedure with arbitrary control flow. The algorithm uses definitions of reducible and irreducible loops which allow either kind of loop to be nested in the other. The tree construction algorithm, an extension of Tarjan's algorithm for testing reducibility, runs in almost linear time. In the presence of irreducible loops, the loop nesting tree can depend on the depth-first spanning tree used to build it. In particular, the header node representing a reducible loop in one version of the loop nesting tree can be the representative of an irreducible loop in another. We give a normalization method that maximizes the set of reducible loops discovered, independent of the depth-first spanning tree used. The normalization require the insertion of at most one node and one edge per reducible loop.",
    "cited_by_count": 121,
    "openalex_id": "https://openalex.org/W2082318969",
    "type": "article"
  },
  {
    "title": "Proofs about a folklore let-polymorphic type inference algorithm",
    "doi": "https://doi.org/10.1145/291891.291892",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Oukseh Lee; Kwangkeun Yi",
    "corresponding_authors": "",
    "abstract": "The Hindley/Milner let-polymorphic type inference system has two different algorithms: one is the de facto standard Algorithm 𝒲 that is bottom-up (or context-insensitive), and the other is a “folklore” algorithm that is top-down (or context-sensitive). Because the latter algorithm has not been formally presented with its soundness and completeness proofs, and its relation with the 𝒲 algorithm has not been rigorously investigated, its use in place of (or in combination with) 𝒲 is not well founded. In this article, we formally define the context-sensitive, top-down type inference algorithm (named “M”), prove its soundness and completeness, and show a distinguishing property that M always stops earlier than 𝒲 if the input program is ill typed. Our proofs can be seen as theoretical justifications for various type-checking strategies being used in practice.",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W2168280960",
    "type": "article"
  },
  {
    "title": "Precise flow-insensitive may-alias analysis is NP-hard",
    "doi": "https://doi.org/10.1145/239912.239913",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Susan Horwitz",
    "corresponding_authors": "Susan Horwitz",
    "abstract": "Determining aliases is one of the foundamental static analysis problems, in part because the precision with which this problem is solved can affect the precision of other analyses such as live variables, available expressions, and constant propagation. Previous work has investigated the complexity of flow-sensitive alias analysis. In this article we show that precise flow- insensitive may-alias analysis is NP-hard given arbitrary levels of pointers and arbitrary pointer dereferencing.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W1990053053",
    "type": "article"
  },
  {
    "title": "Undecidability of context-sensitive data-dependence analysis",
    "doi": "https://doi.org/10.1145/345099.345137",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Thomas Reps",
    "corresponding_authors": "Thomas Reps",
    "abstract": "A number of program-analysis problems can be tackled by transforming them into certain kinds of graph-reachability problems in labeled directed graphs. The edge labels can be used to filter out paths that are not interest: a path P from vertex s to vertex t only counts as a“valid connection” between s and t if the word spelled out by P is in a certain language. Often the languages used for such filtering purposes are languages of matching parantheses. In some cases, the matched-parenthesis condition is used to filter out paths with mismatched calls and returns. This leads to so-called “context- sensitive ” program analyses, such as context- sensitive interprocedural slicing and context- sensitive interprocedural dataflow analysis. In other cases, the matched-parenthesis condition is used to capture a graph-theoretic analog of McCarthy's rules: “car (cons(x,y)) = x” and “cdr(cons(x,y)) =y”. That is, in the code fragment c = cons(a,b); d = car(c); the fact that there is a “structure-transmitted data-dependence” from a to d, but not from b to d, is captured in a graph by (1) using a vertex for each variable, (2)an edge from vertex i to vertex j when i is used on the right-hand side of an assignment to j , (3) parentheses that match as the labels on the edges that run from a to c and c to d, and (4) parentheses that do not match as the labels on the edges that run from a to c and c to d. However, structure-transmitted data-dependence analysis is context- insensitive , because there are no constraints that filter out paths with mismatched calls and returns. Thus, a natural question is whether these two kinds of uses of parentheses can be combined to create a context- sensitive analysis for structure-transmitted data-dependences. This article answers the question in the negative: in general, the problem of context sensitive , structure-transmitted data-dependence analysis is undecidable. The results imply that in general, both context- sensitive set-based analysis and ∞-CFA (when data constructors and selectors and selectors are taken into account) are also undecidable.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W1997981086",
    "type": "article"
  },
  {
    "title": "Sets and constraint logic programming",
    "doi": "https://doi.org/10.1145/365151.365169",
    "publication_date": "2000-09-01",
    "publication_year": 2000,
    "authors": "Agostino Dovier; Carla Piazza; Enrico Pontelli; Gianfranco Rossi",
    "corresponding_authors": "",
    "abstract": "In this paper we present a study of the problem of handling constraints made by conjunctions of positive and negative literals based on the predicate symbols =, ∈,∪ and || (i.e., disjointness of two sets) in a (hybrid) universe of finite sets . We also review and compare the main techniques considered to represent finite sets in the context of logic languages. The resulting contraint algorithms are embedded in a Constraint Logic Programming (CLP) language which provides finite sets—along with basic set-theoretic operations—as first-class objects of the language. The language—called CLP( SET )—is an instance of the general CLP framework, and as such it inherits all the general features and theoretical results of this scheme. We provide, through programming examples, a taste of the expressive power offered by programming in CLP( SET ).",
    "cited_by_count": 117,
    "openalex_id": "https://openalex.org/W2027319813",
    "type": "article"
  },
  {
    "title": "A partially deadlock-free typed process calculus",
    "doi": "https://doi.org/10.1145/276393.278524",
    "publication_date": "1998-03-01",
    "publication_year": 1998,
    "authors": "Naoki Kobayashi",
    "corresponding_authors": "Naoki Kobayashi",
    "abstract": "article Free Access Share on A partially deadlock-free typed process calculus Author: Naoki Kobayashi University of Tokyo University of TokyoView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 20Issue 2March 1998 pp 436–482https://doi.org/10.1145/276393.278524Published:01 March 1998Publication History 79citation587DownloadsMetricsTotal Citations79Total Downloads587Last 12 Months28Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 116,
    "openalex_id": "https://openalex.org/W2148901852",
    "type": "article"
  },
  {
    "title": "Efficient Java RMI for parallel programming",
    "doi": "https://doi.org/10.1145/506315.506317",
    "publication_date": "2001-11-01",
    "publication_year": 2001,
    "authors": "Jason Maassen; Rob V. van Nieuwpoort; Ronald Veldema; Henri E. Bal; Thilo Kielmann; Ceriel J. H. Jacobs; Rutger F. H. Hofman",
    "corresponding_authors": "",
    "abstract": "Java offers interesting opportunities for parallel computing. In particular, Java Remote Method Invocation (RMI) provides a flexible kind of remote procedure call (RPC) that supports polymorphism. Sun's RMI implementation achieves this kind of flexibility at the cost of a major runtime overhead. The goal of this article is to show that RMI can be implemented efficiently, while still supporting polymorphism and allowing interoperability with Java Virtual Machines (JVMs). We study a new approach for implementing RMI, using a compiler-based Java system called Manta. Manta uses a native (static) compiler instead of a just-in-time compiler. To implement RMI efficiently, Manta exploits compile-time type information for generating specialized serializers. Also, it uses an efficient RMI protocol and fast low-level communication protocols.A difficult problem with this approach is how to support polymorphism and interoperability. One of the consequences of polymorphism is that an RMI implementation must be able to download remote classes into an application during runtime. Manta solves this problem by using a dynamic bytecode compiler, which is capable of compiling and linking bytecode into a running application. To allow interoperability with JVMs, Manta also implements the Sun RMI protocol (i.e., the standard RMI protocol), in addition to its own protocol.We evaluate the performance of Manta using benchmarks and applications that run on a 32-node Myrinet cluster. The time for a null-RMI (without parameters or a return value) of Manta is 35 times lower than for the Sun JDK 1.2, and only slightly higher than for a C-based RPC protocol. This high performance is accomplished by pushing almost all of the runtime overhead of RMI to compile time. We study the performance differences between the Manta and the Sun RMI protocols in detail. The poor performance of the Sun RMI protocol is in part due to an inefficient implementation of the protocol. To allow a fair comparison, we compiled the applications and the Sun RMI protocol with the native Manta compiler. The results show that Manta's null-RMI latency is still eight times lower than for the compiled Sun RMI protocol and that Manta's efficient RMI protocol results in 1.8 to 3.4 times higher speedups for four out of six applications.",
    "cited_by_count": 115,
    "openalex_id": "https://openalex.org/W2067748597",
    "type": "article"
  },
  {
    "title": "Parallelism for free",
    "doi": "https://doi.org/10.1145/229542.229545",
    "publication_date": "1996-05-01",
    "publication_year": 1996,
    "authors": "Jens Knoop; Bernhard Steffen; Jürgen Vollmer",
    "corresponding_authors": "",
    "abstract": "We consider parallel programs with shared memory and interleaving semantics, for which we show how to construct for unidirectional bitvector problems optimal analysis algorithms that are as efficient as their purely sequential counterparts and that can easily be implemented. Whereas the complexity result is rather obvious, our optimality result is a consequence of a new Kam/Ullman-style Coincidence Theorem. Thus using our method, the standard algorithms for sequential programs computing liveness, availability, very busyness, reaching definitions, definition-use chains, or the analyses for performing code motion, assignment motion, partial dead-code elimination or strength reduction, can straightforward be transferred to the parallel setting at almost no cost.",
    "cited_by_count": 113,
    "openalex_id": "https://openalex.org/W2065818693",
    "type": "article"
  },
  {
    "title": "Stack allocation and synchronization optimizations for Java using escape analysis",
    "doi": "https://doi.org/10.1145/945885.945892",
    "publication_date": "2003-11-01",
    "publication_year": 2003,
    "authors": "Jong-Deok Choi; Manish Gupta; Maurício Serrano; Vugranam C. Sreedhar; Samuel P. Midkiff",
    "corresponding_authors": "",
    "abstract": "This article presents an escape analysis framework for Java to determine (1) if an object is not reachable after its method of creation returns, allowing the object to be allocated on the stack, and (2) if an object is reachable only from a single thread during its lifetime, allowing unnecessary synchronization operations on that object to be removed. We introduce a new program abstraction for escape analysis, the connection graph , that is used to establish reachability relationships between objects and object references. We show that the connection graph can be succinctly summarized for each method such that the same summary information may be used in different calling contexts without introducing imprecision into the analysis. We present an interprocedural algorithm that uses the above property to efficiently compute the connection graph and identify the nonescaping objects for methods and threads. The experimental results, from a prototype implementation of our framework in the IBM High Performance Compiler for Java, are very promising. The percentage of objects that may be allocated on the stack exceeds 70% of all dynamically created objects in the user code in three out of the ten benchmarks (with a median of 19%); 11% to 92% of all mutex lock operations are eliminated in those 10 programs (with a median of 51%), and the overall execution time reduction ranges from 2% to 23% (with a median of 7%) on a 333-MHz PowerPC workstation with 512 MB memory.",
    "cited_by_count": 112,
    "openalex_id": "https://openalex.org/W2004420781",
    "type": "article"
  },
  {
    "title": "Automatic program specialization for Java",
    "doi": "https://doi.org/10.1145/778559.778561",
    "publication_date": "2003-07-01",
    "publication_year": 2003,
    "authors": "Ulrik Pagh Schultz; Julia L. Lawall; Charles Consel",
    "corresponding_authors": "",
    "abstract": "The object-oriented style of programming facilitates program adaptation and enhances program genericness, but at the expense of efficiency. We demonstrate experimentally that state-of-the-art Java compilers fail to compensate for the use of object-oriented abstractions in the implementation of generic programs, and that program specialization can eliminate a significant portion of these overheads. We present an automatic program specializer for Java, illustrate its use through detailed case studies, and demonstrate experimentally that it can significantly reduce program execution time. Although automatic program specialization could be seen as being subsumed by existing optimizing compiler technology, we show that specialization and compiler optimization are in fact complementary.",
    "cited_by_count": 112,
    "openalex_id": "https://openalex.org/W2116981323",
    "type": "article"
  },
  {
    "title": "On the type structure of standard ML",
    "doi": "https://doi.org/10.1145/169701.169696",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Robert Harper; John C. Mitchell",
    "corresponding_authors": "",
    "abstract": "article Free AccessOn the type structure of standard ML Authors: Robert Harper Carnegie Mellon Univ., Pittsburgh, PA Carnegie Mellon Univ., Pittsburgh, PAView Profile , John C. Mitchell Stanford Univ., Stanford, CA Stanford Univ., Stanford, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 2pp 211–252https://doi.org/10.1145/169701.169696Published:01 April 1993Publication History 83citation805DownloadsMetricsTotal Citations83Total Downloads805Last 12 Months40Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 111,
    "openalex_id": "https://openalex.org/W2014889169",
    "type": "article"
  },
  {
    "title": "Constraint-based array dependence analysis",
    "doi": "https://doi.org/10.1145/291889.291900",
    "publication_date": "1998-05-01",
    "publication_year": 1998,
    "authors": "William Pugh; David Wonnacott",
    "corresponding_authors": "",
    "abstract": "Traditional array dependence analysis, which detects potential memory aliasing of array references is a key analysis technique for automatic parallelization. Recent studies of benchmark codes indicate that limitations of analysis cause many compilers to overlook large amounts of potential parallelism, and that exploiting this parallelism requires algorithms to answer new question about array references, not just get better answers to the old questions of aliasing. We need to ask about the flow of values in arrays, to check the legality of array privatization, and about the conditions under which a dependence exists, to obtain information about conditional parallelism. In some cases, we must answer these questions about code containing nonlinear terms in loop bounds or subscripts. This article describes techniques for phrasing these questions in terms of systems of contstraints. Conditional dependence analysis can be performed with a constraint operation we call the \"gist\" operation. When subscripts and loop bounds are affine, questions about the flow of values in array variables can be phrased in terms of Presburger Arithmetic. When the constraints describing a dependence are not affine, we introduce uninterpreted function symbols to represent the nonaffine terms. Our constraint language also provides a rich language for communication with the dependence analyzer, by either the programmer or other phases of the compiler. This article also documents our investigations of the praticality of our approach. The worst-case complexity of Presburger Arithmetic indicates that it might be unsuitable for any practical application. However, we have found that analysis of benchmark programs does not cause the exponential growth in the number of constraints that could occur in the worst case. We have studied the constraints produced during our aanalysis, and identified characteristics that keep our algorithms free of exponential behavior in practice.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2034591560",
    "type": "article"
  },
  {
    "title": "Mobile safe ambients",
    "doi": "https://doi.org/10.1145/596980.596981",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "Francesca Levi; Davide Sangiorgi",
    "corresponding_authors": "",
    "abstract": "Two forms of interferences are individuated in Cardelli and Gordon's Mobile Ambients (MA): plain interferences , which are similar to the interferences one finds in CCS and π-calculus; and grave interferences , which are more dangerous and may be regarded as programming errors. To control interferences, the MA movement primitives are modified; the resulting calculus is called Mobile Safe Ambients (SA).The modification also has computational significance. In the MA interaction rules, an ambient may enter, exit, or open another ambient. The second ambient undergoes the action; it has no control on when the action takes place. In SA this is rectified: any movement takes place only if both participants agree.Existing type systems for MA can be easily adapted to SA. The type systems for controlling mobility, however, appear to be more powerful in SA, in that (i) type systems for MA may give more precise information when transplanted onto SA , and (ii) new type systems may be defined. Two type systems are presented that remove all grave interferences.Other advantages of SA are: a useful algebraic theory; programs sometimes more robust (they require milder conditions for correctness) and/or simpler. All these points are illustrated in several examples.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2087663894",
    "type": "article"
  },
  {
    "title": "Reconciling responsiveness with performance in pure object-oriented languages",
    "doi": "https://doi.org/10.1145/233561.233562",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "Urs Hölzle; David Ungar",
    "corresponding_authors": "",
    "abstract": "Dynamically dispatched calls often limit the performance of object-oriented programs, since opject-oriented programming encourages factoring code into small, reusable units, thereby increasing the frequency of these expensive operations. Frequent calls not only slow down execution with the dispatch overhead per se, but more importantly they hinder optimization by limiting the range and effectiveness of standard global optimizations. In particular, dynamically dispatched calles prevent standard interprocedual optimizations that depend on the availability of a static call graph. The SELF implementation described here offers tow novel approaches to optimization. Type feedback speculatively inlines dynamically dispatched calls based on profile information that predicts likely receiver classes. Adaptive optimization reconciles optimizing compilation with interactive performance by incrementally optimizing only the frequently executed parts of a program. When combined, these two techniques result in a system that can execute programs significantly faster than previous systems while retaining much of the interactiveness of an interpreted system.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W1979823255",
    "type": "article"
  },
  {
    "title": "Extending Java for high-level Web service construction",
    "doi": "https://doi.org/10.1145/945885.945890",
    "publication_date": "2003-11-01",
    "publication_year": 2003,
    "authors": "Aske Simon Christensen; Anders Møller; Michael I. Schwartzbach",
    "corresponding_authors": "",
    "abstract": "We incorporate innovations from the &lt;bigwig&gt; project into the Java language to provide high-level features for Web service programming. The resulting language, JWIG, contains an advanced session model and a flexible mechanism for dynamic construction of XML documents, in particular XHTML. To support program development we provide a suite of program analyses that at compile time verify for a given program that no runtime errors can occur while building documents or receiving form input, and that all documents being shown are valid according to the document type definition for XHTML 1.0.We compare JWIG with Servlets and JSP which are widely used Web service development platforms. Our implementation and evaluation of JWIG indicate that the language extensions can simplify the program structure and that the analyses are sufficiently fast and precise to be practically useful.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W1993550470",
    "type": "article"
  },
  {
    "title": "Escape analysis for Java <sup>TM</sup>",
    "doi": "https://doi.org/10.1145/945885.945886",
    "publication_date": "2003-11-01",
    "publication_year": 2003,
    "authors": "Bruno Blanchet",
    "corresponding_authors": "Bruno Blanchet",
    "abstract": "Escape analysis is a static analysis that determines whether the lifetime of data may exceed its static scope.This paper first presents the design and correctness proof of an escape analysis for Java TM . This analysis is interprocedural, context sensitive, and as flow-sensitive as the static single assignment form. So, assignments to object fields are analyzed in a flow-insensitive manner. Since Java is an imperative language, the effect of assignments must be precisely determined. This goal is achieved thanks to our technique using two interdependent analyses, one forward, one backward. We introduce a new method to prove the correctness of this analysis, using aliases as an intermediate step. We use integers to represent the escaping parts of values, which leads to a fast and precise analysis.Our implementation [Blanchet 1999], which applies to the whole Java language, is then presented. Escape analysis is applied to stack allocation and synchronization elimination. In our benchmarks, we stack allocate 13% to 95% of data, eliminate more than 20% of synchronizations on most programs (94% and 99% on two examples) and get up to 43% runtime decrease (21% on average). Our detailed experimental study on large programs shows that the improvement comes more from the decrease of the garbage collection and allocation times than from improvements on data locality, contrary to what happened for ML. This comes from the difference in the garbage collectors.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2095558030",
    "type": "article"
  },
  {
    "title": "MultiJava",
    "doi": "https://doi.org/10.1145/1133651.1133655",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Curtis Clifton; Todd Millstein; Gary T. Leavens; Craig Chambers",
    "corresponding_authors": "",
    "abstract": "MultiJava is a conservative extension of the Java programming language that adds symmetric multiple dispatch and open classes. Among other benefits, multiple dispatch provides a solution to the binary method problem. Open classes provide a solution to the extensibility problem of object-oriented programming languages, allowing the modular addition of both new types and new operations to an existing type hierarchy. This article illustrates and motivates the design of MultiJava and describes its modular static typechecking and modular compilation strategies. Although MultiJava extends Java, the key ideas of the language design are applicable to other object-oriented languages, such as C# and C++, and even, with some modifications, to functional languages such as ML.This article also discusses the variety of application domains in which MultiJava has been successfully used by others, including pervasive computing, graphical user interfaces, and compilers. MultiJava allows users to express desired programming idioms in a way that is declarative and supports static typechecking, in contrast to the tedious and type-unsafe workarounds required in Java. MultiJava also provides opportunities for new kinds of extensibility that are not easily available in Java.",
    "cited_by_count": 108,
    "openalex_id": "https://openalex.org/W2098884586",
    "type": "article"
  },
  {
    "title": "How to securely replicate services",
    "doi": "https://doi.org/10.1145/177492.177745",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Michael K. Reiter; Kenneth P. Birman",
    "corresponding_authors": "",
    "abstract": "We present a method for constructing replicated services that retain their availability and integrity despite several servers and clients being corrupted by an intruder, in addition to others failing benignly. We also address the issue of maintaining a causal order among client requests. We illustrate a security breach resulting from an intruder's ability to effect a violation of causality in the sequence of requests processed by the service and propose an approach to counter this attack. An important and novel feature of our techniques is that the client need not be able to identify or authenticate even a single server. Instead, the client is required to possess only a single public key for the service. We demonstrate the performance of our techniques with a service we have implemented using one of our protocols.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W2023930035",
    "type": "article"
  },
  {
    "title": "A practical soft type system for scheme",
    "doi": "https://doi.org/10.1145/239912.239917",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Andrew K. Wright; Robert Cartwright",
    "corresponding_authors": "",
    "abstract": "A soft type system infers types for the procedures and data structures of dynamically typed programs. Like conventional static types, soft types express program invariants and thereby provide valuable information for program optimization and debugging. A soft type checker uses the types inferred by a soft type system to eliminate run-time checks that are provably unnecessary; any remaining run-time checks are flagged as potential program errors. Soft Scheme is a practical soft type checker for R4RS Scheme. Its underlying type system generalizes conventional Hindley-Milner type inference by incorporating recursive types and a limited form of union type. Soft Scheme accommodates all of R4RS Scheme including uncurried procedures of fixed and variable arity, assignment, and continuations.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W2055886480",
    "type": "article"
  },
  {
    "title": "Concurrency in heavily loaded neighborhood-constrained systems",
    "doi": "https://doi.org/10.1145/69558.69560",
    "publication_date": "1989-10-01",
    "publication_year": 1989,
    "authors": "Valmir C. Barbosa; Eli Gafni",
    "corresponding_authors": "",
    "abstract": "Let G be a connected undirected graph in which each node corresponds to a process and two nodes are connected by an edge if the corresponding processes share a resource. We consider distributed computations in which processes are constantly demanding all of their resources in order to operate, and in which neighboring processes may not operate concurrently. We advocate that such a system is general enough for representing a large class of resource-sharing systems under heavy load. We employ a distributed scheduling mechanism based on acyclic orientations of G and investigate the amount of concurrency that it provides. We show that this concurrency is given by a number akin to G 's chromatic and multichromatic numbers, and that, among scheduling schemes which require neighbors in G to alternate in their turns to operate, ours is the one that potentially provides the greatest concurrency. However, we also show that the decision problem corresponding to optimizing concurrency is NP -complete.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W2072754757",
    "type": "article"
  },
  {
    "title": "A polymorphic record calculus and its compilation",
    "doi": "https://doi.org/10.1145/218570.218572",
    "publication_date": "1995-11-30",
    "publication_year": 1995,
    "authors": "Atsushi Ohori",
    "corresponding_authors": "Atsushi Ohori",
    "abstract": "article Free Access Share on A polymorphic record calculus and its compilation Author: Atsushi Ohori Kyoto University Kyoto UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 17Issue 630 November 1995pp 844–895https://doi.org/10.1145/218570.218572Published:30 November 1995Publication History 74citation673DownloadsMetricsTotal Citations74Total Downloads673Last 12 Months68Last 6 weeks27 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W2092632814",
    "type": "article"
  },
  {
    "title": "A reflection on call-by-value",
    "doi": "https://doi.org/10.1145/267959.269968",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "Amr Sabry; Philip Wadler",
    "corresponding_authors": "",
    "abstract": "One way to model a sound and complete translation from a source calculus into a target calculus is with an adjoint or a Galois connection . In the special case of a reflection , one also has that the target calculus is isomorphic to a subset of the source. We show that three widely studied translations form reflections. We use as our source language Moggi's computational lambda calculus, which is an extension of Plotkin's call-by-value calculus. We show that Plotkin's CPS translation, Moggi's monad translation, and Girard's translation to linear logic can all be regarded as reflections form this source language, and we put forward the computational lambda calculus as a model of call-by-value computation that improves on the traditional call-by-value calculus. Our work strengthens Plotkin's and Moggi's original results and improves on recent work based on equational correspondence , which uses equations rather than reductions.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W1969661839",
    "type": "article"
  },
  {
    "title": "Should your specification language be typed",
    "doi": "https://doi.org/10.1145/319301.319317",
    "publication_date": "1999-05-01",
    "publication_year": 1999,
    "authors": "Leslie Lamport; Lawrence C. Paulson",
    "corresponding_authors": "",
    "abstract": "Most specification languages have a type system. Type systems are hard to get right, and getting them wrong can lead to inconsistencies. Set theory can serve as the basis for a specification language without types. This possibility, which has been widely overlooked, offers many advantages. Untyped set theory is simple and is more flexible than any simple typed formalism. Polymorphism, overloading, and subtyping can make a type system more powerful, but at the cost of increased somplexity, and such refinements can never attain the flexibility of having no types at all. Typed formalisms have advantages, too, stemming from the power of mechanical type checking. While types serve little purpose in hand proofs, they do help with mechanized proofs. In the absence of verificaiton, type checking can catch errors in specifications. It may be possible to have the best of both worlds by adding typing annotations to an untyped specification language. We consider only specification languages, not programming languages.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W2140595120",
    "type": "article"
  },
  {
    "title": "Static slicing in the presence of goto statements",
    "doi": "https://doi.org/10.1145/183432.183438",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Jong-Deok Choi; Jeanne Ferrante",
    "corresponding_authors": "",
    "abstract": "A static program slice is an extract of a program which can help our understanding of the behavior of the program; it has been proposed for use in debugging, optimization, parallelization, and integration of programs. This article considers two types of static slices: executable and nonexecutable. Efficient and well-founded methods have been developed to construct executable slices for programs without goto statements; it would be tempting to assume these methods would apply as well in programs with arbitrary goto statements. We show why previous methods do not work in this more general setting, and describe our solutions that correctly and efficiently compute executable slices for programs even with arbitrary goto statements. Our conclusion is that goto statements can be accommodated in generating executable static slices.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2073971651",
    "type": "article"
  },
  {
    "title": "Storage assignment to decrease code size",
    "doi": "https://doi.org/10.1145/229542.229543",
    "publication_date": "1996-05-01",
    "publication_year": 1996,
    "authors": "Stan Liao; Srinivas Devadas; Kurt Keutzer; Steven Tjiang; Albert Wang",
    "corresponding_authors": "",
    "abstract": "DSP architectures typically provide indirect addressing modes with autoincrement and decrement. In addition, indexing mode is generally not available, and there are usually few, if any, general-purpose registers. Hence, it is necessary to use address registers and perform address arithmetic to access automatic variables. Subsuming the address arithmetic into autoincrement and decrement modes improves the size of the generated code. In this article we present a formulation of the problem of optimal storage assignment such that explicit instructions for address arithmetic are minimized. We prove that for the case of a single address register the decision problem is NP-complete, even for a single basic block. We then generalize the problem to multiple address registers. For both cases heuristic algorithms are given, and experimental results are presented.",
    "cited_by_count": 102,
    "openalex_id": "https://openalex.org/W1998044834",
    "type": "article"
  },
  {
    "title": "Editing by example",
    "doi": "https://doi.org/10.1145/4472.4476",
    "publication_date": "1985-10-01",
    "publication_year": 1985,
    "authors": "Robert Nix",
    "corresponding_authors": "Robert Nix",
    "abstract": "An editing by example system is an automatic program synthesis facility embedded in a text editor that can be used to solve repetitive text editing problems. The user provides the editor with a few examples of a text transformation. The system analyzes the examples and generalizes them into a program that can perform the transformation to the rest of the user's text. This paper presents an overview of the design, analysis, and implementation of a practical editing by example system. It studies the problem of synthesizing a text processing program that generalizes the transformation implicitly described by a small number of input/output examples. A class of text processing programs called gap programs is defined and the problems associated with synthesizing them from examples are examined, leading to an efficient heuristic that provably synthesizes a gap program from examples of its input/output behavior. The editing by example system derived from this analysis has been embedded in a production text editor. By developing an editing by example system that solves a useful class of text processing problems, this work demonstrates that program synthesis is feasible in the domain of text editing.",
    "cited_by_count": 102,
    "openalex_id": "https://openalex.org/W2018365488",
    "type": "article"
  },
  {
    "title": "Distributed algorithms for finding centers and medians in networks",
    "doi": "https://doi.org/10.1145/579.585",
    "publication_date": "1984-07-01",
    "publication_year": 1984,
    "authors": "Ephraim Korach; Doron Rotem; Nicola Santoro",
    "corresponding_authors": "",
    "abstract": "article Free AccessDistributed algorithms for finding centers and medians in networks Authors: E Korach IBM Israel IBM IsraelView Profile , D Rotem University of Waterloo University of WaterlooView Profile , N Santoro Carleton University Carleton UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 3pp 380–401https://doi.org/10.1145/579.585Published:01 July 1984Publication History 64citation895DownloadsMetricsTotal Citations64Total Downloads895Last 12 Months63Last 6 weeks39 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 101,
    "openalex_id": "https://openalex.org/W2029934607",
    "type": "article"
  },
  {
    "title": "Information flow vs. resource access in the asynchronous pi-calculus",
    "doi": "https://doi.org/10.1145/570886.570890",
    "publication_date": "2002-09-01",
    "publication_year": 2002,
    "authors": "Matthew Hennessy; James Riely",
    "corresponding_authors": "",
    "abstract": "We propose an extension of the asynchronous π-calculus in which a variety of security properties may be captured using types. These are an extension of the input/output types for the π-calculus in which I/O capabilities are assigned specific security levels. The main innovation is a uniform typing system that, by varying slightly the allowed set of types, captures different notions of security.We first define a typing system that ensures that processes running at security level σ cannot access resources with a security level higher than σ. The notion of access control guaranteed by this system is formalized in terms of a Type Safety Theorem.We then show that, by restricting the allowed types, our system prohibits implicit information flow from high-level to low-level processes. We prove that low-level behavior can not be influenced by changes to high-level behavior. This is formalized as a noninterference theorem with respect to may testing.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W2074877666",
    "type": "article"
  },
  {
    "title": "<i>Mutatis Mutandis</i>",
    "doi": "https://doi.org/10.1145/1255450.1255455",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Gareth Stoyle; Michael Hicks; Gavin Bierman; Peter Sewell; Iulian Neamtiu",
    "corresponding_authors": "",
    "abstract": "This article presents Proteus, a core calculus that models dynamic software updating, a service for fixing bugs and adding features to a running program. Proteus permits a program's type structure to change dynamically but guarantees the updated program remains type-correct by ensuring a property we call con-freeness. We show how con-freeness can be enforced dynamically, and how it can be approximated via a novel static analysis. This analysis can be used to assess the implications of a program's structure on future updates in order to make update success more predictable. We have implemented Proteus for C, and briefly discuss our implementation which we have tested on several well-known programs.",
    "cited_by_count": 99,
    "openalex_id": "https://openalex.org/W2068937020",
    "type": "article"
  },
  {
    "title": "Type extensions",
    "doi": "https://doi.org/10.1145/42190.46167",
    "publication_date": "1988-04-01",
    "publication_year": 1988,
    "authors": "Niklaus Wirth",
    "corresponding_authors": "Niklaus Wirth",
    "abstract": "Software systems represent a hierarchy of modules. Client modules contain sets of procedures that extend the capabilities of imported modules. This concept of extension is here applied to data types. Extended types are related to their ancestor in terms of a hierarchy. Variables of an extended type are compatible with variables of the ancestor type. This scheme is expressed by three language constructs only: the declaration of extended record types, the type test, and the type guard. The facility of extended types, which closely resembles the class concept, is defined in rigorous and concise terms, and an efficient implementation is presented.",
    "cited_by_count": 99,
    "openalex_id": "https://openalex.org/W2295616626",
    "type": "article"
  },
  {
    "title": "Domain specific language implementation via compile-time meta-programming",
    "doi": "https://doi.org/10.1145/1391956.1391958",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Laurence Tratt",
    "corresponding_authors": "Laurence Tratt",
    "abstract": "Domain specific languages (DSLs) are mini-languages that are increasingly seen as being a valuable tool for software developers and non-developers alike. DSLs must currently be created in an ad-hoc fashion, often leading to high development costs and implementations of variable quality. In this article, I show how expressive DSLs can be hygienically embedded in the Converge programming language using its compile-time meta-programming facility, the concept of DSL blocks, and specialised error reporting techniques. By making use of pre-existing facilities, and following a simple methodology, DSL implementation costs can be significantly reduced whilst leading to higher quality DSL implementations.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W1982534263",
    "type": "article"
  },
  {
    "title": "Detecting global variables in denotational specifications",
    "doi": "https://doi.org/10.1145/3318.3323",
    "publication_date": "1985-04-01",
    "publication_year": 1985,
    "authors": "David A. Schmidt",
    "corresponding_authors": "David A. Schmidt",
    "abstract": "Sufficient criteria are given for replacing all occurrences of the store argument in a Scott-Strachey denotational definition of a programming language by a single global variable. The criteria and transformation are useful for transforming denotational definitions into compilers and interpreters for imperative machines, for optimizing applicative programs, and for judging the suitability of semantic notations for describing imperative languages. An example transformation of a semantics of a repeat-loop language to one which uses a global store variable is given to illustrate the technique.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W2082433897",
    "type": "article"
  },
  {
    "title": "Send-receive considered harmful",
    "doi": "https://doi.org/10.1145/963778.963780",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Sergei Gorlatch",
    "corresponding_authors": "Sergei Gorlatch",
    "abstract": "During the software crisis of the 1960s, Dijkstra's famous thesis \"goto considered harmful\" paved the way for structured programming. This short communication suggests that many current difficulties of parallel programming based on message passing are caused by poorly structured communication, which is a consequence of using low-level send-receive primitives. We argue that, like goto in sequential programs, send-receive should be avoided as far as possible and replaced by collective operations in the setting of message passing. We dispute some widely held opinions about the apparent superiority of pairwise communication over collective communication and present substantial theoretical and empirical evidence to the contrary in the context of MPI (Message Passing Interface).",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W2079579585",
    "type": "article"
  },
  {
    "title": "Incremental data-flow analysis algorithms",
    "doi": "https://doi.org/10.1145/42192.42193",
    "publication_date": "1988-01-01",
    "publication_year": 1988,
    "authors": "Barbara G. Ryder; Marvin C. Paull",
    "corresponding_authors": "",
    "abstract": "An incremental update algorithm modifies the solution of a problem that has been changed, rather than re-solving the entire problem. ACINCF and ACINCB are incremental update algorithms for forward and backward data-flow analysis, respectively, based on our equations model of Allen-Cocke interval analysis. In addition, we have studied their performance on a “nontoy” structured programming language L. Given a set of localized program changes in a program written in L, we identify a priori the nodes in its flow graph whose corresponding data-flow equations may be affected by the changes. We characterize these possibly affected nodes by their corresponding program structures and their relation to the original change sites, and do so without actually performing the incremental updates . Our results can be refined to characterize the reduced equations possibly affected if structured loop exit mechanisms are used, either singly or together, thereby relating richness of programming language usage to the ease of incremental updating.",
    "cited_by_count": 95,
    "openalex_id": "https://openalex.org/W2083000170",
    "type": "article"
  },
  {
    "title": "Verifying temporal properties without temporal logic",
    "doi": "https://doi.org/10.1145/59287.62028",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "Bowen Alpern; Fred B. Schneider",
    "corresponding_authors": "",
    "abstract": "An approach to proving temporal properties of concurrent programs that does not use temporal logic as an inference system is presented. The approach is based on using Buchi automata to specify properties. To show that a program satisfies a given property, proof obligations are derived from the Buchi automata specifying that property. These obligations are discharged by devising suitable invariant assertions and variant functions for the program. The approach is shown to be sound and relatively complete. A mutual exclusion protocol illustrates its application.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W1986033227",
    "type": "article"
  },
  {
    "title": "A constraint-based approach to guarded algebraic data types",
    "doi": "https://doi.org/10.1145/1180475.1180476",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Vincent Simonet; François Pottier",
    "corresponding_authors": "",
    "abstract": "We study HMG( X ), an extension of the constraint-based type system HM( X ) with deep pattern matching, polymorphic recursion, and guarded algebraic data types . Guarded algebraic data types subsume the concepts known in the literature as indexed types , guarded recursive datatype constructors , (first-class) phantom types , and equality qualified types , and are closely related to inductive types . Their characteristic property is to allow every branch of a case construct to be typechecked under different assumptions about the type variables in scope. We prove that HMG( X ) is sound and that, provided recursive definitions carry a type annotation, type inference can be reduced to constraint solving. Constraint solving is decidable, at least for some instances of X , but prohibitively expensive. Effective type inference for guarded algebraic data types is left as an issue for future research.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2093737074",
    "type": "article"
  },
  {
    "title": "Assessing Test Data Adequacy through Program Inference",
    "doi": "https://doi.org/10.1145/69575.357231",
    "publication_date": "1983-10-01",
    "publication_year": 1983,
    "authors": "Elaine J. Weyuker",
    "corresponding_authors": "Elaine J. Weyuker",
    "abstract": "article Free Access Share on Assessing Test Data Adequacy through Program Inference Author: Elaine J. Weyuker Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NY Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 4pp 641–655https://doi.org/10.1145/69575.357231Published:01 October 1983Publication History 70citation489DownloadsMetricsTotal Citations70Total Downloads489Last 12 Months57Last 6 weeks6 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2045671792",
    "type": "article"
  },
  {
    "title": "Reverse-mode AD in a functional framework",
    "doi": "https://doi.org/10.1145/1330017.1330018",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Barak A. Pearlmutter; Jeffrey Mark Siskind",
    "corresponding_authors": "",
    "abstract": "We show that reverse-mode AD (Automatic Differentiation)—a generalized gradient-calculation operator—can be incorporated as a first-class function in an augmented lambda calculus, and therefore into a functional-programming language. Closure is achieved, in that the new operator can be applied to any expression in the augmented language, yielding an expression in that language. This requires the resolution of two major technical issues: (a) how to transform nested lambda expressions, including those with free-variable references, and (b) how to support self application of the AD machinery. AD transformations preserve certain complexity properties, among them that the reverse phase of the reverse-mode AD transformation of a function have the same temporal complexity as the original untransformed function. First-class unrestricted AD operators increase the expressive power available to the numeric programmer, and may have significant practical implications for the construction of numeric software that is robust, modular, concise, correct, and efficient.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W1965256357",
    "type": "article"
  },
  {
    "title": "Using Peephole Optimization on Intermediate Code",
    "doi": "https://doi.org/10.1145/357153.357155",
    "publication_date": "1982-01-01",
    "publication_year": 1982,
    "authors": "Andrew S. Tanenbaum; Hans van Staveren; Johan W. Stevenson",
    "corresponding_authors": "",
    "abstract": "article Open Access Share on Using Peephole Optimization on Intermediate Code Authors: Andrew S. Tanenbaum Department of Mathematics, Vrije Universiteit, Postbus 7161, 1007 MC Amsterdam, The Netherlands Department of Mathematics, Vrije Universiteit, Postbus 7161, 1007 MC Amsterdam, The NetherlandsView Profile , Hans van Staveren Department of Mathematics, Vrije Universiteit, Postbus 7161, 1007 MC Amsterdam, The Netherlands Department of Mathematics, Vrije Universiteit, Postbus 7161, 1007 MC Amsterdam, The NetherlandsView Profile , Johan W. Stevenson Arsycom BV, Kabelweg 43, Amsterdam, The Netherlands Arsycom BV, Kabelweg 43, Amsterdam, The NetherlandsView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 1pp 21–36https://doi.org/10.1145/357153.357155Published:01 January 1982Publication History 82citation1,257DownloadsMetricsTotal Citations82Total Downloads1,257Last 12 Months120Last 6 weeks17 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Publisher SiteeReaderPDF",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W2100087302",
    "type": "article"
  },
  {
    "title": "Incremental Parsing",
    "doi": "https://doi.org/10.1145/357062.357066",
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "Carlo Ghezzi; Dino Mandrioli",
    "corresponding_authors": "",
    "abstract": "An incremental parser is a device which is able to perform syntax analysis in an incremental way, avoiding complete reparsing of a program after each modification. The incremental parser presented extends the conventional LR parsing algorithm and its performance is compared with that of a conventional parser. Suggestions for an implementation and possible extensions to other parsing methods are also discussed.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W2296332834",
    "type": "article"
  },
  {
    "title": "A modular verifiable exception handling mechanism",
    "doi": "https://doi.org/10.1145/3318.3320",
    "publication_date": "1985-04-01",
    "publication_year": 1985,
    "authors": "Shaula Yemini; Daniel M. Berry",
    "corresponding_authors": "",
    "abstract": "This paper presents a new model for exception handling, called the replacement model. The replacement model, in contrast to other exception-handling proposals, supports all the handler responses of resumption, termination, retry, and exception propagation, within both statements and expressions, in a modular, simple, and uniform fashion. The model can be embedded in any expression-oriented language and can also be adapted to languages which are not expression oriented with almost all the above advantages. This paper presents the syntactic extensions for embedding the replacement model into Algol 68 and its operational semantics. An axiomatic semantic definition for the model can be found in [27].",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W1965105464",
    "type": "article"
  },
  {
    "title": "Output Guards and Nondeterminism in “Communicating Sequential Processes”",
    "doi": "https://doi.org/10.1145/357094.357101",
    "publication_date": "1980-04-01",
    "publication_year": 1980,
    "authors": "Arthur Bernstein",
    "corresponding_authors": "Arthur Bernstein",
    "abstract": "In a recent paper C.A.R. Hoare outlined a language for concurrent programming. Guarded commands and nondeterminism are two features of the language. This paper points out two problems that arise in connection with these features and addresses one of them.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W1980529102",
    "type": "article"
  },
  {
    "title": "Generating editing environments based on relations and attributes",
    "doi": "https://doi.org/10.1145/6465.6512",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Susan Horwitz; Tim Teitelbaum",
    "corresponding_authors": "",
    "abstract": "The ability to generate language-based editors depends on the existence of a powerful, language-independent model of editing. A model is proposed in which programs are represented as attributed abstract-syntax trees with an associated relational database. Relations can depend on the state of the attributed tree, and attributes can depend on the values in relations, provided there are no circular dependencies. The power and the limitations of relational operations are demonstrated with respect to the support of static-semantic checking, anomaly detection, an interrogation facility, and the ability to define alternative program displays. The advantages of the hybrid system over both the purely relational and purely attribute-based systems are presented, and new algorithms are given for query evaluation and incremental view updating motivated by the efficiency requirements of interactive editing under the defined model. A prototype implementation of an editor generator is described, and suggestions for future research are made.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2013041359",
    "type": "article"
  },
  {
    "title": "IDL: sharing intermediate representations",
    "doi": "https://doi.org/10.1145/24039.24040",
    "publication_date": "1987-07-01",
    "publication_year": 1987,
    "authors": "David Alex Lamb",
    "corresponding_authors": "David Alex Lamb",
    "abstract": "IDL (Interface Description Language) is a practical and useful tool for controlling the exchange of structured data between different components of a large system. IDL is a notation for describing collections of programs and the data structures through which they communicate. Using IDL, a designer gives abstract descriptions of data structures, together with representation specifications that specialize the abstract structures for particular programs. A tool, the IDL translator, generates readers and writers that map between concrete internal representations and abstract exchange representations.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W2010252954",
    "type": "article"
  },
  {
    "title": "Distributed FIFO allocation of identical resources using small shared space",
    "doi": "https://doi.org/10.1145/59287.59292",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "Michael J. Fischer; Nancy Lynch; James E. Burns; Allan Borodin",
    "corresponding_authors": "",
    "abstract": "We present a simple and efficient algorithm for the FIFO allocation of k identical resources among asynchronous processes that communicate via shared memory. The algorithm simulates a shared queue but uses exponentially fewer shared memory values, resulting in practical savings of time and space as well as program complexity. The algorithm is robust against process failure through unannounced stopping, making it attractive also for use in an environment of processes of widely differing speeds. In addition to its practical advantages, we show that for fixed k , the shared space complexity of the algorithm as a function of the number N of processes is optimal to within a constant factor.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2095244541",
    "type": "article"
  },
  {
    "title": "BI-hyperdoctrines, higher-order separation logic, and abstraction",
    "doi": "https://doi.org/10.1145/1275497.1275499",
    "publication_date": "2007-08-02",
    "publication_year": 2007,
    "authors": "Bodil Biering; Lars Birkedal; Noah Torp-Smith",
    "corresponding_authors": "",
    "abstract": "We present a precise correspondence between separation logic and a simple notion of predicate BI, extending the earlier correspondence given between part of separation logic and propositional BI. Moreover, we introduce the notion of a BI hyperdoctrine, show that it soundly models classical and intuitionistic first- and higher-order predicate BI, and use it to show that we may easily extend separation logic to higher-order . We also demonstrate that this extension is important for program proving, since it provides sound reasoning principles for data abstraction in the presence of aliasing.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2125505801",
    "type": "article"
  },
  {
    "title": "A New Approach to Proving the Correctness of Multiprocess Programs",
    "doi": "https://doi.org/10.1145/357062.357068",
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "Leslie Lamport",
    "corresponding_authors": "Leslie Lamport",
    "abstract": "A new, nonassertional approach to proving multiprocess program correctness is described by proving the correctness of a new algorithm to solve the mutual exclusion problem. The algorithm is an improved version of the bakery algorithm. It is specified and proved correct without being decomposed into indivisible, atomic operations. This allows two different implementations for a conventional, nondistributed system. Moreover, the approach provides a sufficiently general specification of the algorithm to allow nontrivial implementations for a distributed system as well.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2040068897",
    "type": "article"
  },
  {
    "title": "A semantics-based approach to malware detection",
    "doi": "https://doi.org/10.1145/1387673.1387674",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Mila Dalla Preda; Mihai Christodorescu; Somesh Jha; Saumya Debray",
    "corresponding_authors": "",
    "abstract": "Malware detection is a crucial aspect of software security. Current malware detectors work by checking for signatures , which attempt to capture the syntactic characteristics of the machine-level byte sequence of the malware. This reliance on a syntactic approach makes current detectors vulnerable to code obfuscations, increasingly used by malware writers, that alter the syntactic properties of the malware byte sequence without significantly affecting their execution behavior. This paper takes the position that the key to malware identification lies in their semantics. It proposes a semantics-based framework for reasoning about malware detectors and proving properties such as soundness and completeness of these detectors. Our approach uses a trace semantics to characterize the behavior of malware as well as that of the program being checked for infection, and uses abstract interpretation to “hide” irrelevant aspects of these behaviors. As a concrete application of our approach, we show that (1) standard signature matching detection schemes are generally sound but not complete, (2) the semantics-aware malware detector proposed by Christodorescu et al. is complete with respect to a number of common obfuscations used by malware writers and (3) the malware detection scheme proposed by Kinder et al. and based on standard model-checking techniques is sound in general and complete on some, but not all, obfuscations handled by the semantics-aware malware detector.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2143807210",
    "type": "article"
  },
  {
    "title": "A Language for Array and Vector Processors",
    "doi": "https://doi.org/10.1145/357073.357075",
    "publication_date": "1979-10-01",
    "publication_year": 1979,
    "authors": "R. H. Perrott",
    "corresponding_authors": "R. H. Perrott",
    "abstract": "The scientific community has consistently demanded from computing machines an increase in the number of instructions executed per second. The latest increase has been achieved by duplication of arithmetic units for an array processor and the pipelining of functional units for vector processors. The high level programming languages for such machines have not benefited from the advances which have been made in programming language design and implementation techniques. A high level language is described in this paper which is appropriate for both array and vector processors and is defined without reference to the hardware of either type of machine. The syntax enables the parallel nature of a problem to be expressed in a form which can be readily exploited by these machines. This is achieved by using the data declarations to indicate the maximum extent of parallel processing and then to manipulate this, or a lesser extent, in the course of program execution. It was found to be possible to modify many of the structured programming and data structuring concepts for this type of parallel environment and to maintain the benefits of compile time and run time checking. Several special constructs and operators are also defined. The language offers to the large scale scientific computing community many of the advances which have been made in software engineering techniques while it exploits the architectural advances which have been made.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2051249047",
    "type": "article"
  },
  {
    "title": "Type-based publish/subscribe",
    "doi": "https://doi.org/10.1145/1180475.1180481",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Patrick Eugster",
    "corresponding_authors": "Patrick Eugster",
    "abstract": "A continuously increasing number of interconnected computer devices makes the requirement for programming abstractions for remote one-to-many interaction yet more stringent. The publish/subscribe paradigm has been advocated as a candidate abstraction for such one-to-many interaction at large scale. Common practices in publish/subscribe, however, include low-level abstractions which hardly leverage type safety, and provide only poor support for object encapsulation. This tends to put additional burden on software developers; guarantees such as the aforementioned type safety and object encapsulation become of increasing importance with an accrued number of software components, which modern applications also involve, besides an increasing number of hardware components. Type-based publish/subscribe (TPS) is a high-level variant of the publish/subscribe paradigm which aims precisely at providing guarantees such as type safety and encapsulation. We present the rationale and principles underlying TPS, as well as two implementations in Java: the first based on a specific extension of the Java language, and a second novel implementation making use of recent general-purpose features of Java, such as generics and behavioral reflection. We compare the two approaches, thereby evaluating the aforementioned features---as well as additional features which have been included in the most recent Java 1.5 release---in the context of distributed and concurrent programming. We discuss the benefits of alternative programming languages and features for implementing TPS. By revisiting alternative abstractions for distributed programming, including “classic” and recent ones, we extend our investigations to programming language support for distributed programming in general, pointing out that overall, the support in current mainstream programming languages is still insufficient.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2046997852",
    "type": "article"
  },
  {
    "title": "Final Data Types and Their Specification",
    "doi": "https://doi.org/10.1145/357195.357202",
    "publication_date": "1983-01-01",
    "publication_year": 1983,
    "authors": "Samuel Kamin",
    "corresponding_authors": "Samuel Kamin",
    "abstract": "article Free AccessFinal Data Types and Their Specification Author: Samuel Kamin Computer Science Department, University of Illinois at Urbana-Champaign, 1304 W. Springfield, Urbana, IL Computer Science Department, University of Illinois at Urbana-Champaign, 1304 W. Springfield, Urbana, ILView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 1Jan. 1983 pp 97–121https://doi.org/10.1145/357195.357202Published:01 January 1983Publication History 60citation414DownloadsMetricsTotal Citations60Total Downloads414Last 12 Months18Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W1989177109",
    "type": "article"
  },
  {
    "title": "Assignment and Procedure Call Proof Rules",
    "doi": "https://doi.org/10.1145/357114.357119",
    "publication_date": "1980-10-01",
    "publication_year": 1980,
    "authors": "David Gries; Gary Levin",
    "corresponding_authors": "",
    "abstract": "The multiple assignment statement is defined in full generality—including assignment to subscripted variables and record fields—using the “axiomatic” approach of Hoare. Proof rules are developed for calls of procedures using global variables, var parameters, result parameters, and value parameters, using the idea of multiple assignment to provide understanding. An attempt is made to clarify some issues that have arisen concerning the use of rules of inference to aid in generating “verification conditions” in mechanical verifiers and the use of logical variables to denote initial values of program variables.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2084291524",
    "type": "article"
  },
  {
    "title": "Termination Detection of Diffusing Computations in Communicating Sequential Processes",
    "doi": "https://doi.org/10.1145/357153.357156",
    "publication_date": "1982-01-01",
    "publication_year": 1982,
    "authors": "Jayadev Misra; K. Mani Chandy",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Termination Detection of Diffusing Computations in Communicating Sequential Processes Authors: Jayadev Misra Department of Computer Sciences, College of Natural Sciences, The University of Texas at Austin, Austin, TX Department of Computer Sciences, College of Natural Sciences, The University of Texas at Austin, Austin, TXView Profile , K. M. Chandy Department of Computer Sciences, College of Natural Sciences, The University of Texas at Austin, Austin, TX Department of Computer Sciences, College of Natural Sciences, The University of Texas at Austin, Austin, TXView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 1Jan. 1982 pp 37–43https://doi.org/10.1145/357153.357156Published:01 January 1982Publication History 56citation836DownloadsMetricsTotal Citations56Total Downloads836Last 12 Months66Last 6 weeks23 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W2095359054",
    "type": "article"
  },
  {
    "title": "Verification of Array, Record, and Pointer Operations in Pascal",
    "doi": "https://doi.org/10.1145/357073.357078",
    "publication_date": "1979-10-01",
    "publication_year": 1979,
    "authors": "D Luckham; Norihisa Suzuki",
    "corresponding_authors": "",
    "abstract": "A practical method is presented for automating in a uniform way the verification of Pascal programs that operate on the standard Pascal data structures Array, Record, and Pointer. New assertion language primitives are introduced for describing computational effects of operations on these data structures. Axioms defining the semantics of the new primitives are given. Proof rules for standard Pascal operations on data structures are then defined using the extended assertion language. An axiomatic rule for the Pascal storage allocation operation, NEW, is also given. These rulers have been implemented in the Stanford Pascal program verifier. Examples illustrating the verification of programs which operate on list structures implemented with pointers and records are discussed. These include programs with side effects.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2048168398",
    "type": "article"
  },
  {
    "title": "Automated termination proofs for haskell by term rewriting",
    "doi": "https://doi.org/10.1145/1890028.1890030",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Jürgen Giesl; Matthias Raffelsieper; Peter Schneider–Kamp; Stephan Swiderski; René Thiemann",
    "corresponding_authors": "",
    "abstract": "There are many powerful techniques for automated termination analysis of term rewriting. However, up to now they have hardly been used for real programming languages. We present a new approach which permits the application of existing techniques from term rewriting to prove termination of most functions defined in Haskell programs. In particular, we show how termination techniques for ordinary rewriting can be used to handle those features of Haskell which are missing in term rewriting (e.g., lazy evaluation, polymorphic types, and higher-order functions). We implemented our results in the termination prover AProVE and successfully evaluated them on existing Haskell libraries.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2099473324",
    "type": "article"
  },
  {
    "title": "Refactoring using type constraints",
    "doi": "https://doi.org/10.1145/1961204.1961205",
    "publication_date": "2011-04-01",
    "publication_year": 2011,
    "authors": "Frank Tip; Robert M. Fuhrer; Adam Kieżun; Michael D. Ernst; Ittai Balaban; Bjorn De Sutter",
    "corresponding_authors": "",
    "abstract": "Type constraints express subtype relationships between the types of program expressions, for example, those relationships that are required for type correctness. Type constraints were originally proposed as a convenient framework for solving type checking and type inference problems. This paper shows how type constraints can be used as the basis for practical refactoring tools. In our approach, a set of type constraints is derived from a type-correct program P . The main insight behind our work is the fact that P constitutes just one solution to this constraint system, and that alternative solutions may exist that correspond to refactored versions of P . We show how a number of refactorings for manipulating types and class hierarchies can be expressed naturally using type constraints. Several refactorings in the standard distribution of Eclipse are based on our work.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2000952465",
    "type": "article"
  },
  {
    "title": "Operational semantics for multi-language programs",
    "doi": "https://doi.org/10.1145/1498926.1498930",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Jacob Matthews; Robert Bruce Findler",
    "corresponding_authors": "",
    "abstract": "Interoperability is big business, a fact to which .NET, the JVM, and COM can attest. Language designers are well aware of this, and they are designing programming languages that reflect it—for instance, SML.NET, F#, Mondrian, and Scala all treat interoperability as a central design feature. Still, current multi-language research tends not to focus on the semantics of these features, but only on how to implement them efficiently. In this article, we attempt to rectify that by giving a technique for specifying the operational semantics of a multi-language system as a composition of the models of its constituent languages. Our technique abstracts away the low-level details of interoperability like garbage collection and representation coherence, and lets us focus on semantic properties like type-safety, equivalence, and termination behavior. In doing so it allows us to adapt standard theoretical techniques such as subject-reduction, logical relations, and operational equivalence for use on multi-language systems. Generally speaking, our proofs of properties in a multi-language context are mutually referential versions of their single language counterparts. We demonstrate our technique with a series of strategies for embedding a Scheme-like language into an ML-like language. We start by connecting very simple languages with a very simple strategy, and work our way up to languages that interact in sophisticated ways and have sophisticated features such as polymorphism and effects. Along the way, we prove relevant results such as type-soundness and termination for each system we present using adaptations of standard techniques. Beyond giving simple expressive models, our studies have uncovered several interesting facts about interoperability. For example, higher-order function contracts naturally emerge as the glue to ensure that interoperating languages respect each other's type systems. Our models also predict that the embedding strategy where foreign values are opaque is as expressive as the embedding strategy where foreign values are translated to corresponding values in the other language, and we were able to experimentally verify this behavior using PLT Scheme's foreign function interface.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2020899082",
    "type": "article"
  },
  {
    "title": "Dependence clusters in source code",
    "doi": "https://doi.org/10.1145/1596527.1596528",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Mark Harman; David Binkley; Keith Gallagher; Nicolas Gold; Jens Krinke",
    "corresponding_authors": "",
    "abstract": "A dependence cluster is a set of program statements, all of which are mutually inter-dependent. This article reports a large scale empirical study of dependence clusters in C program source code. The study reveals that large dependence clusters are surprisingly commonplace. Most of the 45 programs studied have clusters of dependence that consume more than 10% of the whole program. Some even have clusters consuming 80% or more. The widespread existence of clusters has implications for source code analyses such as program comprehension, software maintenance, software testing, reverse engineering, reuse, and parallelization.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2061972420",
    "type": "article"
  },
  {
    "title": "A Theory of Synchronous Relational Interfaces",
    "doi": "https://doi.org/10.1145/1985342.1985345",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Stavros Tripakis; Ben Lickly; Thomas A. Henzinger; Edward A. Lee",
    "corresponding_authors": "",
    "abstract": "Compositional theories are crucial when designing large and complex systems from smaller components. In this work we propose such a theory for synchronous concurrent systems. Our approach follows so-called interface theories, which use game-theoretic interpretations of composition and refinement. These are appropriate for systems with distinct inputs and outputs, and explicit conditions on inputs that must be enforced during composition. Our interfaces model systems that execute in an infinite sequence of synchronous rounds. At each round, a contract must be satisfied. The contract is simply a relation specifying the set of valid input/output pairs. Interfaces can be composed by parallel, serial or feedback composition. A refinement relation between interfaces is defined, and shown to have two main properties: (1) it is preserved by composition, and (2) it is equivalent to substitutability, namely, the ability to replace an interface by another one in any context. Shared refinement and abstraction operators, corresponding to greatest lower and least upper bounds with respect to refinement, are also defined. Input-complete interfaces, that impose no restrictions on inputs, and deterministic interfaces, that produce a unique output for any legal input, are discussed as special cases, and an interesting duality between the two classes is exposed. A number of illustrative examples are provided, as well as algorithms to compute compositions, check refinement, and so on, for finite-state interfaces.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2074119802",
    "type": "article"
  },
  {
    "title": "Separation and information hiding",
    "doi": "https://doi.org/10.1145/1498926.1498929",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Peter W. O’Hearn; Hongseok Yang; John Reynolds",
    "corresponding_authors": "",
    "abstract": "We investigate proof rules for information hiding, using the formalism of separation logic. In essence, we use the separating conjunction to partition the internal resources of a module from those accessed by the module's clients. The use of a logical connective gives rise to a form of dynamic partitioning, where we track the transfer of ownership of portions of heap storage between program components. It also enables us to enforce separation in the presence of mutable data structures with embedded addresses that may be aliased.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2130111506",
    "type": "article"
  },
  {
    "title": "Implicit dynamic frames",
    "doi": "https://doi.org/10.1145/2160910.2160911",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "Jan Smans; Bart Jacobs; Frank Piessens",
    "corresponding_authors": "Jan Smans",
    "abstract": "An important, challenging problem in the verification of imperative programs with shared, mutable state is the frame problem in the presence of data abstraction. That is, one must be able to specify and verify upper bounds on the set of memory locations a method can read and write without exposing that method's implementation. Separation logic is now widely considered the most promising solution to this problem. However, unlike conventional verification approaches, separation logic assertions cannot mention heap-dependent expressions from the host programming language, such as method calls familiar to many developers. Moreover, separation logic-based verifiers are often based on symbolic execution. These symbolic execution-based verifiers typically do not support non-separating conjunction, and some of them rely on the developer to explicitly fold and unfold predicate definitions. Furthermore, several researchers have wondered whether it is possible to use verification condition generation and standard first-order provers instead of symbolic execution to automatically verify conformance with a separation logic specification. In this article, we propose a variant of separation logic called implicit dynamic frames that supports heap-dependent expressions inside assertions. Conformance with an implicit dynamic frames specification can be checked by proving the validity of a number of first-order verification conditions. To show that these verification conditions can be discharged automatically by standard first-order provers, we have implemented our approach in a verifier prototype and have used this prototype to verify several challenging examples from related work. Our prototype automatically folds and unfolds predicate definitions, as required, during the proof and can reason about non-separating conjunction which is used in the specifications of some of these examples. Finally, we prove the soundness of the approach.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2074285438",
    "type": "article"
  },
  {
    "title": "Polyhedral AST Generation Is More Than Scanning Polyhedra",
    "doi": "https://doi.org/10.1145/2743016",
    "publication_date": "2015-07-15",
    "publication_year": 2015,
    "authors": "Tobias Grosser; Sven Verdoolaege; Albert Cohen",
    "corresponding_authors": "",
    "abstract": "Abstract mathematical representations such as integer polyhedra have been shown to be useful to precisely analyze computational kernels and to express complex loop transformations. Such transformations rely on abstract syntax tree (AST) generators to convert the mathematical representation back to an imperative program. Such generic AST generators avoid the need to resort to transformation-specific code generators, which may be very costly or technically difficult to develop as transformations become more complex. Existing AST generators have proven their effectiveness, but they hit limitations in more complex scenarios. Specifically, (1) they do not support or may fail to generate control flow for complex transformations using piecewise schedules or mappings involving modulo arithmetic; (2) they offer limited support for the specialization of the generated code exposing compact, straightline, vectorizable kernels with high arithmetic intensity necessary to exploit the peak performance of modern hardware; (3) they offer no support for memory layout transformations; and (4) they provide insufficient control over the AST generation strategy, preventing their application to complex domain-specific optimizations. We present a new AST generation approach that extends classical polyhedral scanning to the full generality of Presburger arithmetic, including existentially quantified variables and piecewise schedules, and introduce new optimizations for the detection of components and shifted strides. Not limiting ourselves to control flow generation, we expose functionality to generate AST expressions from arbitrary piecewise quasi-affine expressions, which enables the use of our AST generator for data-layout transformations. We complement this with support for specialization by polyhedral unrolling, user-directed versioning, and specialization of AST expressions according to the location at which they are generated, and we complete this work with fine-grained user control over the AST generation strategies used. Using this generalized idea of AST generation, we present how to implement complex domain-specific transformations without the need to write specialized code generators, but instead relying on a generic AST generator parametrized to a specific problem domain.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W1191262899",
    "type": "article"
  },
  {
    "title": "Extending Type Inference to Variational Programs",
    "doi": "https://doi.org/10.1145/2518190",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Sheng Chen; Martin Erwig; Eric Walkingshaw",
    "corresponding_authors": "",
    "abstract": "Through the use of conditional compilation and related tools, many software projects can be used to generate a huge number of related programs. The problem of typing such variational software is difficult. The brute-force strategy of generating all variants and typing each one individually is: (1) usually infeasible for efficiency reasons and (2) produces results that do not map well to the underlying variational program. Recent research has focused mainly on efficiency and addressed only the problem of type checking. In this work we tackle the more general problem of variational type inference and introduce variational types to represent the result of typing a variational program. We introduce the variational lambda calculus (VLC) as a formal foundation for research on typing variational programs. We define a type system for VLC in which VLC expressions are mapped to correspondingly variational types. We show that the type system is correct by proving that the typing of expressions is preserved over the process of variation elimination, which eventually results in a plain lambda calculus expression and its corresponding type. We identify a set of equivalence rules for variational types and prove that the type unification problem modulo these equivalence rules is unitary and decidable; we also present a sound and complete unification algorithm. Based on the unification algorithm, the variational type inference algorithm is an extension of algorithm W . We show that it is sound and complete and computes principal types. We also consider the extension of VLC with sum types, a necessary feature for supporting variational data types, and demonstrate that the previous theoretical results also hold under this extension. Finally, we characterize the complexity of variational type inference and demonstrate the efficiency gains over the brute-force strategy.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2106799975",
    "type": "article"
  },
  {
    "title": "Conditioning in Probabilistic Programming",
    "doi": "https://doi.org/10.1145/3156018",
    "publication_date": "2018-01-03",
    "publication_year": 2018,
    "authors": "Federico Olmedo; Friedrich Gretz; Nils Jansen; Benjamin Lucien Kaminski; Joost-Pieter Katoen; Annabelle McIver",
    "corresponding_authors": "",
    "abstract": "This article investigates the semantic intricacies of conditioning, a main feature in probabilistic programming. Our study is based on an extension of the imperative probabilistic guarded command language pGCL with conditioning. We provide a weakest precondition (wp) semantics and an operational semantics. To deal with possibly diverging program behavior, we consider liberal preconditions. We show that diverging program behavior plays a key role when defining conditioning. We establish that weakest preconditions coincide with conditional expected rewards in Markov chains—the operational semantics—and that the wp-semantics conservatively extends the existing semantics of pGCL (without conditioning). An extension of these results with nondeterminism turns out to be problematic: although an operational semantics using Markov decision processes is rather straightforward, we show that providing an inductive wp-semantics in this setting is impossible. Finally, we present two program transformations that eliminate conditioning from any program. The first transformation hoists conditioning while updating the probabilistic choices in the program, while the second transformation replaces conditioning—in the same vein as rejection sampling—by a program with loops. In addition, we present a last program transformation that replaces an independent identically distributed loop with conditioning.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2782235253",
    "type": "article"
  },
  {
    "title": "The Design and Implementation of a Verification Technique for GPU Kernels",
    "doi": "https://doi.org/10.1145/2743017",
    "publication_date": "2015-05-22",
    "publication_year": 2015,
    "authors": "Adam Betts; Nathan Chong; Alastair F. Donaldson; Jeroen Ketema; Shaz Qadeer; Paul Thomson; John Wickerson",
    "corresponding_authors": "",
    "abstract": "We present a technique for the formal verification of GPU kernels, addressing two classes of correctness properties: data races and barrier divergence. Our approach is founded on a novel formal operational semantics for GPU kernels termed &lt;i&gt;synchronous, delayed visibility (SDV)&lt;/i&gt; semantics, which captures the execution of a GPU kernel by multiple groups of threads. The SDV semantics provides operational definitions for barrier divergence and for both inter- and intra-group data races. We build on the semantics to develop a method for reducing the task of verifying a massively parallel GPU kernel to that of verifying a sequential program. This completely avoids the need to reason about thread interleavings, and allows existing techniques for sequential program verification to be leveraged. We describe an efficient encoding of data race detection and propose a method for automatically inferring the loop invariants that are required for verification. We have implemented these techniques as a practical verification tool, GPUVerify, that can be applied directly to OpenCL and CUDA source code. We evaluate GPUVerify with respect to a set of 162 kernels drawn from public and commercial sources. Our evaluation demonstrates that GPUVerify is capable of efficient, automatic verification of a large number of real-world kernels.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2234512370",
    "type": "article"
  },
  {
    "title": "Behavioral Subtyping, Specification Inheritance, and Modular Reasoning",
    "doi": "https://doi.org/10.1145/2766446",
    "publication_date": "2015-08-13",
    "publication_year": 2015,
    "authors": "Gary T. Leavens; David A. Naumann",
    "corresponding_authors": "",
    "abstract": "Verification of a dynamically dispatched method call, E . m (), seems to depend on E ’s dynamic type. To avoid case analysis and allow incremental development, object-oriented program verification uses supertype abstraction. In other words, one reasons about E . m () using m ’s specification for E ’s static type. Supertype abstraction is valid when each subtype in the program is a behavioral subtype. This article semantically formalizes supertype abstraction and behavioral subtyping for a Java-like sequential language with mutation and proves that behavioral subtyping is both necessary and sufficient for the validity of supertype abstraction. Specification inheritance, as in JML, is also formalized and proved to entail behavioral subtyping.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W1480522639",
    "type": "article"
  },
  {
    "title": "On the Impact of Programming Languages on Code Quality",
    "doi": "https://doi.org/10.1145/3340571",
    "publication_date": "2019-10-12",
    "publication_year": 2019,
    "authors": "Emery D. Berger; Celeste Hollenbeck; Petr Maj; Olga Vitek; Jan Vítek",
    "corresponding_authors": "",
    "abstract": "This paper is a reproduction of work by Ray et al. which claimed to have uncovered a statistically significant association between eleven programming languages and software defects in projects hosted on GitHub. First we conduct an experimental repetition, repetition is only partially successful, but it does validate one of the key claims of the original work about the association of ten programming languages with defects. Next, we conduct a complete, independent reanalysis of the data and statistical modeling steps of the original study. We uncover a number of flaws that undermine the conclusions of the original study as only four languages are found to have a statistically significant association with defects, and even for those the effect size is exceedingly small. We conclude with some additional sources of bias that should be investigated in follow up work and a few best practice recommendations for similar efforts.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W3121939465",
    "type": "article"
  },
  {
    "title": "The Pluto+ Algorithm",
    "doi": "https://doi.org/10.1145/2896389",
    "publication_date": "2016-04-08",
    "publication_year": 2016,
    "authors": "Uday Bondhugula; Aravind Acharya; Albert Cohen",
    "corresponding_authors": "",
    "abstract": "Affine transformations have proven to be powerful for loop restructuring due to their ability to model a very wide range of transformations. A single multidimensional affine function can represent a long and complex sequence of simpler transformations. Existing affine transformation frameworks such as the Pluto algorithm, which include a cost function for modern multicore architectures for which coarse-grained parallelism and locality are crucial, consider only a subspace of transformations to avoid a combinatorial explosion in finding transformations. The ensuing practical trade-offs lead to the exclusion of certain useful transformations: in particular, transformation compositions involving loop reversals and loop skewing by negative factors. In addition, there is currently no proof that the algorithm successfully finds a tree of permutable loop bands for all affine loop nests. In this article, we propose an approach to address these two issues (1) by modeling a much larger space of practically useful affine transformations in conjunction with the existing cost function of Pluto, and (2) by extending the Pluto algorithm in a way that allows a proof for its soundness and completeness for all affine loop nests. We perform an experimental evaluation of both, the effect on compilation time, and performance of generated codes. The evaluation shows that our new framework, Pluto+, provides no degradation in performance for any benchmark from Polybench. For the Lattice Boltzmann Method (LBM) simulations with periodic boundary conditions, it provides a mean speedup of 1.33 × over Pluto. We also show that Pluto+ does not increase compilation time significantly. Experimental results on Polybench show that Pluto+ increases overall polyhedral source-to-source optimization time by only 15%. In cases in which it improves execution time significantly, it increased polyhedral optimization time by only 2.04 × .",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2318529993",
    "type": "article"
  },
  {
    "title": "A Principled Approach to Selective Context Sensitivity for Pointer Analysis",
    "doi": "https://doi.org/10.1145/3381915",
    "publication_date": "2020-05-19",
    "publication_year": 2020,
    "authors": "Yue Li; Tian Tan; Anders Møller; Yannis Smaragdakis",
    "corresponding_authors": "",
    "abstract": "Context sensitivity is an essential technique for ensuring high precision in static analyses. It has been observed that applying context sensitivity partially, only on a select subset of the methods, can improve the balance between analysis precision and speed. However, existing techniques are based on heuristics that do not provide much insight into what characterizes this method subset. In this work, we present a more principled approach for identifying precision-critical methods, based on general patterns of value flows that explain where most of the imprecision arises in context-insensitive pointer analysis. Using this theoretical foundation, we present an efficient algorithm, ZIPPER, to recognize these flow patterns in a given program and employ context sensitivity accordingly. We also present a variant, ZIPPERe, that additionally takes into account which methods are disproportionally costly to analyze with context sensitivity. Our experimental results on standard benchmark and real-world Java programs show that ZIPPER preserves effectively all of the precision (98.8%) of a highly precise conventional context-sensitive pointer analysis (2-object-sensitive with a context-sensitive heap, 2obj for short), with a substantial speedup (on average, 3.4× and up to 9.4×), and that ZIPPERe preserves 94.7% of the precision of 2obj, with an order-of-magnitude speedup (on average, 25.5× and up to 88×). In addition, for 10 programs that cannot be analyzed by 2obj within a three-hour time limit, on average ZIPPERe can guide 2obj to finish analyzing them in less than 11 minutes with high precision compared to context-insensitive and introspective context-sensitive analyses.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W3030148664",
    "type": "article"
  },
  {
    "title": "Observational Equality Meets CIC",
    "doi": "https://doi.org/10.1145/3719342",
    "publication_date": "2025-02-25",
    "publication_year": 2025,
    "authors": "Loïc Pujet; Yann Leray; Nicolas Tabareau",
    "corresponding_authors": "",
    "abstract": "The notion of equality is at the heart of dependent type theory, as it plays a fundamental role in program specifications and mathematical reasoning. In mainstream proof assistants such as Agda , Lean and Coq , equality is usually defined using Martin-Löf’s identity type, an elegant and simple approach that has stood the test of time since the 1970s. However, this definition also comes with serious downsides: the intensional nature of Martin-Löf’s identity type means that it is impractical for reasoning about functions and predicates, and it is impossible to define quotient types. Recently, observational equality has garnered attention as an alternative method for encoding equality, particularly in proof assistants supporting definitionally proof-irrelevant propositions. However, it has yet to be integrated in any of the three proof assistants mentioned above, as it is not fully compatible with another important feature of type theory: indexed inductive types. In this paper, we propose a systematic approach to reconcile observational equality with indexed inductive types, using a type coercion operator that computes on reflexive identity proofs. The second contribution of this paper is a formal proof that this additional computation rule can be integrated to the system without compromising the decidability of conversion. Finally, we provide an implementation of our observational equality in an extension of Coq . This extension is based on the recently introduced rewrite rules, and provides new extensionality principles while remaining fully backward-compatible.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4407910644",
    "type": "article"
  },
  {
    "title": "Understanding class hierarchies using concept analysis",
    "doi": "https://doi.org/10.1145/353926.353940",
    "publication_date": "2000-05-01",
    "publication_year": 2000,
    "authors": "Gregor Snelting; Frank Tip",
    "corresponding_authors": "",
    "abstract": "A new method is presented for analyzing and reengineering class hierarchies. In our approach, a class hierarchy is processed along with a set of applications that use it, and a fine-grained analysis of the access and subtype relationships between objects, variables, and class members is performed. The result of this analysis is again a class hierarchy, which is guaranteed to be behaviorally equivalent to the original hierarchy, but in which each object only contains the members that are required. Our method is semantically well-founded in concept analysis : the new class hierarchy is a minimal and maximally factorized concept lattice that reflects the access and subtype relationships between variables, objects and class members. The method is primarily intended as a tool for finding imperfections in the design of class hierarchies, and can be used as the basis for tools that largely automate the process of reengineering such hierachies. The method can also be used as a space-optimizing source-to-source transformation that removes redundant fields from objects. A prototype implementation for Java has been constructed, and used to conduct several case studies. Our results demonstrate that the method can provide valuable insights into the usage of a class hierarchy in a specific context, and lead to useful restructuring proposals.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2008059648",
    "type": "article"
  },
  {
    "title": "Techniques for the translation of MATLAB programs into Fortran 90",
    "doi": "https://doi.org/10.1145/316686.316693",
    "publication_date": "1999-03-01",
    "publication_year": 1999,
    "authors": "Luiz De Rose; David Padua",
    "corresponding_authors": "",
    "abstract": "This article describes the main techiques developed for FALCON's MATLAB-to-Fortran 90 compiler. FALCON is a programming environment for the development of high-performance scientific programs. It combines static and dynamic inference methods to translate MATLAB programs into Fortran 90. The static inference is supported with advanced value propagation techniques and symbolic algorithms for subscript analysis. Experiments show that FALCON's MATLAB translator can generate code that performs more than 1000 times faster than the interpreted version of MATLAB and substantially faster than commercially available MATLAB compilers on one processor of an SGI Power Challenge. Futhermore, in most cases we have tested, the compiler-generated code is as fast as corresponding hand-written programs.",
    "cited_by_count": 101,
    "openalex_id": "https://openalex.org/W2122878128",
    "type": "article"
  },
  {
    "title": "Partial evaluation of functional logic programs",
    "doi": "https://doi.org/10.1145/291891.291896",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Marı́a Alpuente; Moreno Falaschi; Germán Vidal",
    "corresponding_authors": "",
    "abstract": "Languages that integrate functional and logic programming with a complete operational semantics are based on narrowing, a unification-based goal-solving mechanism which subsumes the reduction principle of functional languages and the resolution principle of logic languages. In this article, we present a partial evaluation scheme for functional logic languages based on an automatic unfolding algorithm which builds narrowing trees. The method is formalized within the theoretical framework established by Lloyd and Shepherdson for the partial deduction of logic programs, which we have generalized for dealing with functional computations. A generic specialization algorithm is proposed which does not depend on the eager or lazy nature of the narrower being used. To the best of our knowledge, this is the first generic algorithm for the specialization of functional logic programs. We also discuss the relation to work on partial evaluation in functional programming, term-rewriting systems, and logic programming. Finally, we present some experimental results with an implementation of the algorithm which show in practice that the narrowing-driven partial evaluator effectively combines the propagation of partial data structures (by means of logical variables and unification) with better opportunities for optimization (thanks to the functional dimension).",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W2116776985",
    "type": "article"
  },
  {
    "title": "Static caching for incremental computation",
    "doi": "https://doi.org/10.1145/291889.291895",
    "publication_date": "1998-05-01",
    "publication_year": 1998,
    "authors": "Yanhong A. Liu; Scott D. Stoller; Tim Teitelbaum",
    "corresponding_authors": "",
    "abstract": "A systematic approach is given for deriving incremental programs that exploit caching. The cache-and-prune method presented in the article consists of three stages: (I) the original program is extended to cache the results of all its intermediate subcomputations as well as the final result, (II)) the extended program is incrementalized so that computation on a new input can use all intermediate results on an old input, and (III) unused results cached by the extended program and maintained by the incremental program are pruned away, leaving a pruned extended program that caches only useful intermediate results and a pruned incremental program that uses and maintains only useful results. All three stages utilize static analyses and semantics-preserving transformations. Stages I and III are simple, clean, and fully automatable. The overall method has a kind of optimality with respect to the techniques used in Stage II. The method can be applied straightfowardly to provide a systematic approach to program improvement via caching.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W1967890297",
    "type": "article"
  },
  {
    "title": "A schema for interprocedural modification side-effect analysis with pointer aliasing",
    "doi": "https://doi.org/10.1145/383043.381532",
    "publication_date": "2001-03-01",
    "publication_year": 2001,
    "authors": "Barbara G. Ryder; William Landi; P. Stocks; Sean X. Zhang; Rita Z. Altucher",
    "corresponding_authors": "",
    "abstract": "The first interprocedural modification side-effects analysis for C (MOD C ) that obtains better than worst-case precision on programs with general-purpose pointer usage is presented with empirical results. The analysis consists of an algorithm schema corresponding to a family of MOD C algorithms with two independent phases: one for determining pointer-induced aliases and a subsequent one for propagating interprocedural side effects. These MOD C algorithms are parameterized by the aliasing method used. The empirical results compare the performance of two dissimilar MOD C algorithms: MOD C ( FSAlias ) uses a flow-sensitive, calling-context-sensitive interprocedural alias analysis; MOD C ( FIAlias uses a flow-insensitive, calling-context-insensitive alias analysis which is much faster, but less accurate. These two algorithms were profiled on 45 programs ranging in size from 250 to 30,000 lines of C code, and the results demonstrate dramatically the possible cost-precision trade-offs. This first comparative implementation of MOD C analyses offers insight into the differences between flow-/context-sensitive and flow-/context-insensitive analyses. The analysis cost versus precision trade-offs in side-effect information obtained are reported. The results show surprisingly that the precision of flow-sensitive side-effect analysis is not always prohibitive in cost, and that the precision of flow-insensitive analysis is substantially better than worst-case estimates and seems sufficient for certain applications. On average MOD C ( FSAlias ) for procedures and calls is in the range of 20% more precise than MOD C ( FIAlias ); however, the performance was found to be at least an order of magnitude slower than MOD C ( FIAlias ).",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W1972427803",
    "type": "article"
  },
  {
    "title": "Optimizing memory usage in the polyhedral model",
    "doi": "https://doi.org/10.1145/365151.365152",
    "publication_date": "2000-09-01",
    "publication_year": 2000,
    "authors": "Fabien Quilleré; Sanjay Rajopadhye",
    "corresponding_authors": "",
    "abstract": "The polyhedral model provides a single unified foundation for systolic array synthesis and automatic parallelization of loop programs. We investigate the problem of memory reuse when compiling Alpha (a functional language based on this model). Direct compilation would require unacceptably large memory (for example O(n 3 ) for matrix multiplication). Researchers have previously addressed the problem of memory reuse, and the analysis that this entails for projective memory allocations. This paper addresses, for a given schedule, the choice of the projections so as to minimize the volume of the residual memory. We prove tight bounds on the number of linearly independent projection vectors. Our method is constructive, yielding an optimal memory allocation. We extend the method to modular functions, and deal with the subsequent problems of code generation. Our ideas are illustrated on a number of examples generated by the current version of the Alpha compiler.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2010196134",
    "type": "article"
  },
  {
    "title": "A practical framework for demand-driven interprocedural data flow analysis",
    "doi": "https://doi.org/10.1145/267959.269970",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "Evelyn Duesterwald; Rajiv Gupta; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "The high cost and growing importance of interprocedural data flow analysis have led to an increased interest in demand-driven algorithms. In this article, we present a general framework for developing demand-driven interprocedural data flow analyzers and report our experience in evaluating the performance of this approach. A demand for data flow information is modeled as a set of queries. The framework includes a generic demand-driven algorithm that determines the response to query by iteratively applying a system of query propagation rules. The propagation rules yield precise responses for the class of distributive finite data flow problems. We also describe a two-phase framework variation to accurately handle nondistributive problems. A performance evaluation of our demand-driven approach is presented for two data flow problems, namely, reaching-definitions and copy constant propagation. Our experiments show that demand-driven analysis performs well in practice, reducing both time and space requirements when compared with exhaustive analysis.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2164638440",
    "type": "article"
  },
  {
    "title": "Total correctness by local improvement in the transformation of functional programs",
    "doi": "https://doi.org/10.1145/227699.227716",
    "publication_date": "1996-03-01",
    "publication_year": 1996,
    "authors": "David Sands",
    "corresponding_authors": "David Sands",
    "abstract": "The goal of program transformation is to improve efficiency while preserving meaning. One of the best-known transformation techniques is Burstall and Darlington's unfold-fold method. Unfortunately the unfold-fold method itself guarantees neither improvement in efficiency nor total correctness. The correctness problem for unfold-fold is an instance of a strictly more general problem: transformation by locally equivalence-preserving steps does not necessarily preserve (global) equivalence. This article presents a condition for the total correctness of transformations on recursive programs, which, for the first time, deals with higher-order functional languages (both strict and nonstrict) including lazy data structures. The main technical result is an improvement theorem which says that if the local transformation steps are guided by certain optimization concerns (a fairly natural condition for a transformation), then correctness of the transformation follows. The improvement theorem makes essential use of a formalized improvement theory; as a rather pleasing corollary it also guarantees that the transformed program is a formal improvement over the original. The theorem has immediate practical consequences: it is a powerful tool for proving the correctness of existing transformation methods for higher-order functional programs, without having to ignore crucial factors such as memoization or folding , and it yields a simple syntactic method for guiding and constraining the unfold-fold method in the general case so that total correctness (and improvement) is always guaranteed.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2009332552",
    "type": "article"
  },
  {
    "title": "Partial redundancy elimination in SSA form",
    "doi": "https://doi.org/10.1145/319301.319348",
    "publication_date": "1999-05-01",
    "publication_year": 1999,
    "authors": "Robert E. Kennedy; Sun Chan; Shin-Ming Liu; Raymond Lo; Peng Tu; Fred Chow",
    "corresponding_authors": "",
    "abstract": "The SSAPRE algorithm for performing partial redundancy elimination based entirely on SSA form is presented. The algorithm is formulated based on a new conceptual framework, the factored redundancy graph, for analyzing redundancy, and representes the first sparse approach to the classical problem and on methods for its solution. With the algorithm description, theorems and their proofs are given showing that the algorithm produces the best possible code by the criteria of computational optimality and lifetime optimality of the introduced temporaries. In addition to the base algorithm, a practical implementation of SSAPRE that exhibits additional compile-time efficiencies is described. In closing, measurement statistics are provided that characterize the instances of the partial redundancy problem from a set of benchmark programs and compare optimization time spent by an implementation of SSAPRE aganist a classical partial redundancy elimination implementation. The data lend insight into the nature of partial redundancy elimination and demonstrate the expediency of this new approach.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2000722076",
    "type": "article"
  },
  {
    "title": "Semiring-based constraint logic programming",
    "doi": "https://doi.org/10.1145/383721.383725",
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Stefano Bistarelli; Ugo Montanari; Francesca Rossi",
    "corresponding_authors": "",
    "abstract": "We extend the Constraint Logic Programming (CLP) formalism in order to handle semiring-based constraints. This allows us to perform in the same language both constraint solving and optimization. In fact, constraints based on semirings are able to model both classical constraint solving and more sophisticated features like uncertainty, probability, fuzziness, and optimization. We then provide this class of languages with three equivalent semantics: model-theoretic, fix-point, and proof-theoretic, in the style of classical CLP programs.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2005863443",
    "type": "article"
  },
  {
    "title": "Polymorphic type inference and abstract data types",
    "doi": "https://doi.org/10.1145/186025.186031",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Konstantin Läufer; Martin Odersky",
    "corresponding_authors": "",
    "abstract": "Many statically typed programming languages provide an abstract data type construct, such as the module in Modula-2. However, in most of these languages, implementations of abstract data types are not first-class values. Thus, they cannot be assigned to variables, passed as function parameters, or returned as function results. Several higher-order functional languages feature strong and static type systems, parametric polymorphism, algebraic data types, and explicit type variables. Most of them rely on Hindley-Milner type inference instead of requiring explicit type declarations for identifiers. Although some of these languages support abstract data types, it appears that none of them directly provides light-weight abstract data types whose implementations are first-class values. We show how to add significant expressive power to statically typed functional languages with explicit type variables by incorporating first-class abstract types as an extension of algebraic data types. Furthermore, we extend record types to allow abstract components. The components of such abstract records are selected using the dot notation. Following Mitchell and Plotkin, we formalize abstract types in terms of existentially quantified types. We give a syntactically sound and complete type inference algorithm and prove that our type system is semantically sound with respect to standard denotational semantics.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2112150205",
    "type": "article"
  },
  {
    "title": "Controlling generalization and polyvariance in partial deduction of normal logic programs",
    "doi": "https://doi.org/10.1145/271510.271525",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Michaël Leuschel; Bern Martens; Danny De Schreye",
    "corresponding_authors": "",
    "abstract": "Given a program and some input data, partial deduction computes a specialized program handling any remaining input more efficiently. However, controlling the process well is a rather difficult problem. In this article, we elaborate global control for partial deduction: for which atoms, among possibly infinitely many, should specialized relations be produced, meanwhile guaranteeing correctness as well as termination? Our work is based on two ingredients. First, we use the concept of a characteristic tree, encapsulating specialization behavior rather than syntactic structure, to guide generalization and polyvariance, and we show how this can be done in a correct and elegant way. Second, we structure combinations of atoms and associated characteristic trees in global trees registering “causal” relationships among such pairs. This allows us to spot looming nontermination and perform proper generalization in order to avert the danger, without having to impose a depth bound on characteristic trees. The practical relevance and benefits of the work are illustrated through extensive experiments. Finally, a similar approach may improve upon current (on-line) control strategies for program transformation in general such as (positive) supercompilation of functional programs. It also seems valuable in the context of abstract interpretation to handle infinite domains of infinite height with more precision.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2137818097",
    "type": "article"
  },
  {
    "title": "The design of the E programming language",
    "doi": "https://doi.org/10.1145/169683.174157",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Joel E. Richardson; Michael J. Carey; Daniel T. Schuh",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on The design of the E programming language Authors: Joel E. Richardson IBM Almaden Research Center, San Jose, CA IBM Almaden Research Center, San Jose, CAView Profile , Michael J. Carey Univ. of Wisconsin–Madison, Madison Univ. of Wisconsin–Madison, MadisonView Profile , Daniel T. Schuh Univ. of Wisconsin–Madison, Madison Univ. of Wisconsin–Madison, MadisonView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 3July 1993 pp 494–534https://doi.org/10.1145/169683.174157Published:01 July 1993Publication History 43citation1,143DownloadsMetricsTotal Citations43Total Downloads1,143Last 12 Months411Last 6 weeks35 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2164306675",
    "type": "article"
  },
  {
    "title": "Continuous program optimization",
    "doi": "https://doi.org/10.1145/778559.778562",
    "publication_date": "2003-07-01",
    "publication_year": 2003,
    "authors": "Thomas Kistler; Michael Franz",
    "corresponding_authors": "",
    "abstract": "Much of the software in everyday operation is not making optimal use of the hardware on which it actually runs. Among the reasons for this discrepancy are hardware/software mismatches, modularization overheads introduced by software engineering considerations, and the inability of systems to adapt to users' behaviors.A solution to these problems is to delay code generation until load time. This is the earliest point at which a piece of software can be fine-tuned to the actual capabilities of the hardware on which it is about to be executed, and also the earliest point at wich modularization overheads can be overcome by global optimization.A still better match between software and hardware can be achieved by replacing the already executing software at regular intervals by new versions constructed on-the-fly using a background code re-optimizer. This not only enables the use of live profiling data to guide optimization decisions, but also facilitates adaptation to changing usage patterns and the late addition of dynamic link libraries.This paper presents a system that provides code generation at load-time and continuous program optimization at run-time. First, the architecture of the system is presented. Then, two optimization techniques are discussed that were developed specifically in the context of continuous optimization. The first of these optimizations continually adjusts the storage layouts of dynamic data structures to maximize data cache locality, while the second performs profile-driven instruction re-scheduling to increase instruction-level parallelism. These two optimizations have very different cost/benefit ratios, presented in a series of benchmarks. The paper concludes with an outlook to future research directions and an enumeration of some remaining research problems.The empirical results presented in this paper make a case in favor of continuous optimization, but indicate that it needs to be applied judiciously. In many situations, the costs of dynamic optimizations outweigh their benefit, so that no break-even point is ever reached. In favorable circumstances, on the other hand, speed-ups of over 120% have been observed. It appears as if the main beneficiaries of continuous optimization are shared libraries, which at different times can be optimized in the context of the currently dominant client application.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W2058719553",
    "type": "article"
  },
  {
    "title": "Mobile objects in distributed Oz",
    "doi": "https://doi.org/10.1145/265943.265972",
    "publication_date": "1997-09-01",
    "publication_year": 1997,
    "authors": "Peter Van Roy; Seif Haridi; Per Brand; Gert Smolka; Michael J. Mehl; Ralf Scheidhauer",
    "corresponding_authors": "",
    "abstract": "Some of the most difficult questions to answer when designing a distributed application are related to mobility: what information to transfer between sites and when and how to transfer it. Network-transparent distribution, the property that a program's behavior is independent of how it is partitioned among sites, does not directly address these questions. Therefore we propose to extend all language entities with a network behavior that enables efficient distributed programming by giving the programmer a simple and predictable control over network communication patterns. In particular, we show how to give objects an arbitrary mobility behavior that is independent of the objects definition. In this way, the syntax and semantics of objects are the same regardless of whether they are used as stationary servers, mobile agents, or simply as caches. These ideas have been implemented in Distributed Oz, a concurrent object-oriented language that is state aware and has dataflow synchronization. We prove that the implementation of objects in Distributed Oz is network transparent. To satisfy the predictability condition, the implementation avoids forwarding chains through intermediate sites. The implementation is an extension to the publicly available DFKI Oz 2.0 system.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W1997910015",
    "type": "article"
  },
  {
    "title": "A general framework for semantics-based bottom-up abstract interpretation of logic programs",
    "doi": "https://doi.org/10.1145/151646.151650",
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "Roberto Barbuti; Roberto Giacobazzi; Giorgio Levi",
    "corresponding_authors": "",
    "abstract": "The theory of abstract interpretation provides a formal framework to develop advanced dataflow analysis tools. The idea is to define a nonstandard semantics which is able to compute, in finite time, an approximated model of the program. In this paper, we define an abstract interpretation framework based on a fixpoint approach to the semantics. This leads to the definition, by means of a suitable set of operators, of an abstract fixpoint characterization of a model associated with the program. Thus, we obtain a specializable abstract framework for bottom-up abstract interpretations of definite logic programs. The specialization of the framework is shown on two examples, namely, gound-dependence analysis and depth- k analysis.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2043827880",
    "type": "article"
  },
  {
    "title": "Powerlist: a structure for parallel recursion",
    "doi": "https://doi.org/10.1145/197320.197356",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Jayadev Misra",
    "corresponding_authors": "Jayadev Misra",
    "abstract": "Many data-parallel algorithms—Fast Fourier Transform, Batcher's sorting schemes, and the prefix-sum—exhibit recursive structure. We propose a data structure called powerlist that permits succinct descriptions of such algorithms, highlighting the roles of both parallelism and recursion. Simple algebraic properties of this data structure can be explotied to derive properties of these algorithms and to establish equivalence of different algorithms that solve the same problem.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2055893308",
    "type": "article"
  },
  {
    "title": "Practical adaption of the global optimization algorithm of Morel and Renvoise",
    "doi": "https://doi.org/10.1145/103135.214520",
    "publication_date": "1991-04-01",
    "publication_year": 1991,
    "authors": "Dhananjay M. Dhamdhere",
    "corresponding_authors": "Dhananjay M. Dhamdhere",
    "abstract": "article Free AccessPractical adaption of the global optimization algorithm of Morel and Renvoise Author: D. M. Dhamdhere Computer Science and Engineering Department, Indian Institute of Technology, Bombay-400 076, India Computer Science and Engineering Department, Indian Institute of Technology, Bombay-400 076, IndiaView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 2pp 291–294https://doi.org/10.1145/103135.214520Published:01 April 1991Publication History 81citation518DownloadsMetricsTotal Citations81Total Downloads518Last 12 Months25Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W2163466224",
    "type": "article"
  },
  {
    "title": "Identifying loops using DJ graphs",
    "doi": "https://doi.org/10.1145/236114.236115",
    "publication_date": "1996-11-01",
    "publication_year": 1996,
    "authors": "Vugranam C. Sreedhar; Guang R. Gao; Yong-Fong Lee",
    "corresponding_authors": "",
    "abstract": "Loop identification is a necessary step in loop transformations for high-performance architectures. One classical technique for detecting loops is Tarjan's interval-finding algorithm. The intervals identified by Tarjan's method are single-entry, strongly connected subgraphs that closely reflect a program's loop structure. We present a simple algorithm for identifying both reducible and irreducible loops using DJ graphs. Our method is a generalization of Tarjan's method, as it identifies nested intervals (or loops) even in the presence of irreducibility.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2027991598",
    "type": "article"
  },
  {
    "title": "Access control for mobile agents",
    "doi": "https://doi.org/10.1145/963778.963781",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Michele Bugliesi; Giuseppe Castagna; Silvia Crafà",
    "corresponding_authors": "",
    "abstract": "Boxed Ambients are a variant of Mobile Ambients that result from dropping the open capability and introducing new primitives for ambient communication. The new model of communication is faithful to the principles of distribution and location-awareness of Mobile Ambients, and complements the constructs in and out for mobility with finer-grained mechanisms for ambient interaction. We introduce the new calculus, study the impact of the new mechanisms for communication of typing and mobility, and show that they yield an effective framework for resource protection and access control in distributed systems.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2031844066",
    "type": "article"
  },
  {
    "title": "Closure analysis in constraint form",
    "doi": "https://doi.org/10.1145/200994.201001",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Jens Palsberg",
    "corresponding_authors": "Jens Palsberg",
    "abstract": "Flow analyses of untyped higher-order functional programs have in the past decade been presented by Ayers, Bondorf, Consel, Jones, Heintze, Sestoft, Shivers, Steckler, Wand, and others. The analyses are usually defined as abstract interpretations and are used for rather different tasks such as type recovery, globalization, and binding-time analysis. The analyses all contain a global closure analysis that computes information about higher-order control-flow. Sestoft proved in 1989 and 1991 that closure analysis is correct with respect to call-by-name and call-by-value semantics, but it remained open if correctness holds for arbitrary beta-reduction. This article answers the question; both closure analysis and others are correct with respect to arbitrary beta-reduction. We also prove a subject-reduction result: closure information is still valid after beta-reduction. The core of our proof technique is to define closure analysis using a constraint system. The constraint system is equivalent to the closure analysis of Bondorf, which in turn is based on Sestoft's.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2065403304",
    "type": "article"
  },
  {
    "title": "Handling floating-point exceptions in numeric programs",
    "doi": "https://doi.org/10.1145/227699.227701",
    "publication_date": "1996-03-01",
    "publication_year": 1996,
    "authors": "John R. Hauser",
    "corresponding_authors": "John R. Hauser",
    "abstract": "There are a number of schemes for handling arithmetic exceptions that can be used to improve the speed (or alternatively the reliability) of numeric code. Overflow and underflow are the most troublesome exceptions, and depending on the context in which the exception can occur, they may be addressed either: (1) through a “brute force” reevaluation with extended range, (2) by reevaluating using a technique known as scaling , (3) by substituting an infinity or zero, or (4) in the case of underflow, with gradual underflow. In the first two of these cases, the offending computation is simply reevaluated using a safer but slower method. The latter two cases are cheaper, more automated schemes that ideally are built in as options within the computer system. Other arithmetic exceptions can be handled with similar methods. These and some other techniques are examined with an eye toward determining the support programming languages and computer systems ought to provide for floating-point exception handling. It is argued that the cheapest short-term solution would be to give full support to most of the required (as opposed to recommended) special features of the IEC/IEEE Standard for Binary Floating-Point Arithmetic. An essential part of this support would include standardized access from high-level languages to the exception flags defined by the standard. Some possibilities outside the IEEE Standard are also considered, and a few thought on possible better-structured support within programming languages are discussed.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W2028591972",
    "type": "article"
  },
  {
    "title": "Typechecking and modules for multimethods",
    "doi": "https://doi.org/10.1145/218570.218571",
    "publication_date": "1995-11-30",
    "publication_year": 1995,
    "authors": "Craig Chambers; Gary T. Leavens",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Typechecking and modules for multimethods Authors: Craig Chambers University of Washington University of WashingtonView Profile , Gary T. Leavens lowa State University lowa State UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 17Issue 6Nov. 1995 pp 805–843https://doi.org/10.1145/218570.218571Published:30 November 1995Publication History 64citation384DownloadsMetricsTotal Citations64Total Downloads384Last 12 Months23Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W2054373994",
    "type": "article"
  },
  {
    "title": "Java bytecode compression for low-end embedded systems",
    "doi": "https://doi.org/10.1145/353926.353933",
    "publication_date": "2000-05-01",
    "publication_year": 2000,
    "authors": "Lars Clausen; Ulrik Pagh Schultz; Charles Consel; Gilles Muller",
    "corresponding_authors": "",
    "abstract": "A program executing on a low-end embedded system, such as a smart-card, faces scarce memory resources and fixed execution time constraints. We demonstrate that factorization of common instruction sequences in Java bytecode allows the memory footprint to be reduced, on average, to 85% of its original size, with a minimal execution time penalty. While preserving Java compatibility, our solution requires only a few modifications which are straightforward to implement in any JVM used in a low-end embedded system.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2000832133",
    "type": "article"
  },
  {
    "title": "Procedure placement using temporal-ordering information",
    "doi": "https://doi.org/10.1145/330249.330254",
    "publication_date": "1999-09-01",
    "publication_year": 1999,
    "authors": "Nikolas Gloy; Michael D. Smith",
    "corresponding_authors": "",
    "abstract": "Instruction cache performance is important to instruction fetch efficiency and overall processor performance. The layout of an executable has a substantial effect on the cache miss rate and the instruction working set size during execution. This means that the performance of an executable can be improved by applying a code-placement algorithm that minimizes instruction cache conflicts and improves spatial locality. We describe an algorithm for procedure placement, one type of code placement, that signicantly differs from previous approaches in the type of information used to drive the placement algorithm. In particular, we gather temporal-ordering information that summarizes the interleaving of procedures in a program trace. Our algorithm uses this information along with cache configuration and procedure size information to better estimate the conflict cost of a potential procedure ordering. It optimizes the procedure placement for single level and multilevel caches. In addition to reducing instruction cache conflicts, the algorithm simultaneously minimizes the instruction working set size of the program. We compare the performance of our algorithm with a particularly successful procedure-placement algorithm and show noticeable improvements in the instruction cache behavior, while maintaining the same instruction working set size.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2004976090",
    "type": "article"
  },
  {
    "title": "Leader election in uniform rings",
    "doi": "https://doi.org/10.1145/169683.174161",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Shing‐Tsaan Huang",
    "corresponding_authors": "Shing‐Tsaan Huang",
    "abstract": "article Free Access Share on Leader election in uniform rings Author: Shing-Tsaan Huang National Tsing-Hua University, Taiwan National Tsing-Hua University, TaiwanView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 301 July 1993pp 563–573https://doi.org/10.1145/169683.174161Published:01 July 1993Publication History 57citation803DownloadsMetricsTotal Citations57Total Downloads803Last 12 Months27Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2044696477",
    "type": "article"
  },
  {
    "title": "Type reconstruction in the presence of polymorphic recursion",
    "doi": "https://doi.org/10.1145/169701.169687",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "A. J. Kfoury; Jerzy Tiuryn; Paweł Urzyczyn",
    "corresponding_authors": "",
    "abstract": "article Free AccessType reconstruction in the presence of polymorphic recursion Authors: A. J. Kfoury Boston University Boston UniversityView Profile , J. Tiuryn University of Warsaw University of WarsawView Profile , P. Urzyczyn University of Warsaw University of WarsawView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 2pp 290–311https://doi.org/10.1145/169701.169687Published:01 April 1993Publication History 50citation541DownloadsMetricsTotal Citations50Total Downloads541Last 12 Months46Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2055065270",
    "type": "article"
  },
  {
    "title": "Type-based analysis of uncaught exceptions",
    "doi": "https://doi.org/10.1145/349214.349230",
    "publication_date": "2000-03-01",
    "publication_year": 2000,
    "authors": "Xavier Leroy; François Pessaux",
    "corresponding_authors": "",
    "abstract": "This article presents a program analysis to estimate uncaught exceptions in ML programs. This analysis relies on unification-based type inference in a nonstandard type system, using rows to approximate both the flow of escaping exceptions (a la effect systems) and the flow of result values (a la control-flow analyses). The resulting analysis is efficient and precise; in particular, arguments carried by exceptions are accurately handled.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2072162091",
    "type": "article"
  },
  {
    "title": "An incremental algorithm for satisfying hierarchies of multiway dataflow constraints",
    "doi": "https://doi.org/10.1145/225540.225543",
    "publication_date": "1996-01-01",
    "publication_year": 1996,
    "authors": "Brad Vander Zanden",
    "corresponding_authors": "Brad Vander Zanden",
    "abstract": "One-way dataflow constraints have gained popularity in many types of interactive systems because of their simplicity, efficiency, and manageability. Although it is widely acknowledged that multiway dataflow constraint could make it easier to specify certain relationships in these applications, concerns about their predictability and efficiency have impeded their acceptance. Constraint hierarchies have been developed to address the predictability problem, and incremental algorithms have been developed to address the efficiency problem. However, existing incremental alogrithms for satisfying constraint hierarchies encounter two difficulties : (1) they are incapable of guaranteeing an acylic solution if a constraint hierarchy has one or more cyclic solutions and (2) they require worst-case exponential time to satisfy systems of multioutput constriants. This article surmounts these difficulties by presenting an incremental algorithm called QuickPlan that satisfies in worst-case O(N 2 ) time any hierarchy of multiway, multiout-put dataflow constraint that has at least one acyclic solution, where N is the number of constraints. With benchmarks and real problems that can be solved efficiently using exisitng algorithms, its performance is competitive or superior. With benchmarks and real problems that cannot be solved using exisitng algorithms or that cannot be solved efficiently, QuickPlan finds solutions and does so efficiently, typically in O(N) time or less. QuickPlan is based on the strategy of propagation of degrees of freedom. The only restriction it imposes is that every constraint method must use all of the variables in the constraint as either an input or an output variable. This requirement is met in every constraint-based, interactive application that we have developed or seen.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2078328646",
    "type": "article"
  },
  {
    "title": "Incremental analysis of constraint logic programs",
    "doi": "https://doi.org/10.1145/349214.349216",
    "publication_date": "2000-03-01",
    "publication_year": 2000,
    "authors": "Manuel V. Hermenegildo; Germán Puebla; Kim Marriott; Peter J. Stuckey",
    "corresponding_authors": "",
    "abstract": "Global analyzers traditionally read and analyze the entire program at once, in a nonincremental way. However, there are many situations which are not well suited to this simple model and which instead require reanalysis of certain parts of a program which has already been analyzed. In these cases, it appears inefficient to perform the analysis of the program again from scratch, as needs to be done with current systems. We describe how the fixed-point algorithms used in current generic analysis engines for (constraint) logic programming languages can be extended to support incremental analysis. The possible changes to a program are classified into three types: addition, deletion, and arbitrary change. For each one of these, we provide one or more algorithms for identifying the parts of the analysis that must be recomputed and for performing the actual recomputation. The potential benefits and drawbacks of these algorithms are discussed. Finally, we present some experimental results obtained with an implementation of the algorithms in the PLAI generic abstract interpretation framework. The results show significant benefits when using the proposed incremental analysis algorithms.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2163660741",
    "type": "article"
  },
  {
    "title": "A modular technique for the design of efficient distributed leader finding algorithms",
    "doi": "https://doi.org/10.1145/77606.77610",
    "publication_date": "1990-01-03",
    "publication_year": 1990,
    "authors": "Ephraim Korach; Shay Kutten; Shlomo Moran",
    "corresponding_authors": "",
    "abstract": "A general, modular technique for designing efficient leader finding algorithms in distributed, asynchronous networks is developed. This technique reduces the problem of efficient leader finding to a simpler problem of efficient serial traversing of the corresponding network. The message complexity of the resulting leader finding algorithms is bounded by [ f ( n ) + n )(log 2 k + 1) (or ( f ( m ) + n )(log 2 k + 1)], where n is the number of nodes in the network [ m is the number of edges in the network], k is the number of nodes that start the algorithm, and f ( n ) [ f ( m )] is the message complexity of traversing the nodes [edges] of the network. The time complexity of these algorithms may be as large as their message complexity. This technique does not require that the FIFO discipline is obeyed by the links. The local memory needed for each node, besides the memory needed for the traversal algorithm, is logarithmic in the maximal identity of a node in the network. This result achieves in a unified way the best known upper bounds on the message complexity of leader finding algorithms for circular, complete, and general networks. It is also shown to be applicable to other classes of networks, and in some cases the message complexity of the resulting algorithms is better by a constant factor than that of previously known algorithms.",
    "cited_by_count": 82,
    "openalex_id": "https://openalex.org/W2024606983",
    "type": "article"
  },
  {
    "title": "An adaptive tenuring policy for generation scavengers",
    "doi": "https://doi.org/10.1145/111186.116734",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "David Ungar; Frank Jackson",
    "corresponding_authors": "",
    "abstract": "One of the more promising automatic storage reclamation techniques, generation scavenging, suffers poor performance if many objects live for a fairly long time and then die. We have investigated the severity of this problem by simulating a two-generation scavenger using traces taken from actual 4-h sessions. There was a wide variation in the sample runs, with garbage-collection overhead ranging from insignificant, during three of the runs, to severe, during a single run. All runs demonstrated that performance could be improved with two techniques: segregating large bitmaps and strings, and adapting the scavenger's tenuring policy according to demographic feedback. We therefore incorporated these ideas into a commercial Smalltalk implementation. These two improvements deserve consideration for any storage reclamation strategy that utilizes a generation scavenger.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2005330614",
    "type": "article"
  },
  {
    "title": "Type-extension type test can be performed in constant time",
    "doi": "https://doi.org/10.1145/115372.115297",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Norman Cohen",
    "corresponding_authors": "Norman Cohen",
    "abstract": "article Free AccessType-extension type test can be performed in constant time Author: Norman H. Cohen T. J. Watson Research Center, Yorktown Heights, NY T. J. Watson Research Center, Yorktown Heights, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 4Oct. 1991 pp 626–629https://doi.org/10.1145/115372.115297Published:01 October 1991Publication History 63citation439DownloadsMetricsTotal Citations63Total Downloads439Last 12 Months29Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2044439539",
    "type": "article"
  },
  {
    "title": "An example of stepwise refinement of distributed programs: quiescence detection",
    "doi": "https://doi.org/10.1145/5956.5958",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Mani Chandy; Jayadev Misra",
    "corresponding_authors": "",
    "abstract": "We propose a methodology for the development of concurrent programs and apply it to an important class of problems: quiescence detection. The methodology is based on a novel view of programs. A key feature of the methodology is the separation of concerns between the core problem to be solved and details of the forms of concurrency employed in the target architecture and programming language. We begin development of concurrent programs by ignoring issues dealing with concurrency and introduce such concerns in manageable doses. The class of problems solved includes termination and deadlock detection.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2154004999",
    "type": "article"
  },
  {
    "title": "Single-pass generation of static single-assignment form for structured languages",
    "doi": "https://doi.org/10.1145/197320.197331",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Marc Michael Brandis; Hanspeter Mössenböck",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Single-pass generation of static single-assignment form for structured languages Authors: Marc M. Brandis ETH Zu¨rich, Zu¨rich, Switzerland ETH Zu¨rich, Zu¨rich, SwitzerlandView Profile , Hanspeter Mössenböck Univ. of Linz, Linz, Austria Univ. of Linz, Linz, AustriaView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 16Issue 6Nov. 1994 pp 1684–1698https://doi.org/10.1145/197320.197331Online:01 November 1994Publication History 55citation1,674DownloadsMetricsTotal Citations55Total Downloads1,674Last 12 Months152Last 6 weeks19 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2100221082",
    "type": "article"
  },
  {
    "title": "Resource usage analysis",
    "doi": "https://doi.org/10.1145/1057387.1057390",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Atsushi Igarashi; Naoki Kobayashi",
    "corresponding_authors": "",
    "abstract": "It is an important criterion of program correctness that a program accesses resources in a valid manner. For example, a memory region that has been allocated should eventually be deallocated, and after the deallocation, the region should no longer be accessed. A file that has been opened should be eventually closed. So far, most of the methods to analyze this kind of property have been proposed in rather specific contexts (like studies of memory management and verification of usage of lock primitives), and it was not clear what the essence of those methods was or how methods proposed for individual problems are related. To remedy this situation, we formalize a general problem of analyzing resource usage as a resource usage analysis problem , and propose a type-based method as a solution to the problem.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W1981638222",
    "type": "article"
  },
  {
    "title": "Modular logic programming",
    "doi": "https://doi.org/10.1145/183432.183528",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Antonio Brogi; Paolo Mancarella; Dino Pedreschi; Franco Turini",
    "corresponding_authors": "",
    "abstract": "Modularity is a key issue in the design of modern programming languages. When designing modular features for declarative languages in general, and for logic programming languages in particular, the challenge lies in avoiding the superimposition of a complex syntactic and semantic structure over the simple structure of the basic language. The modular framework defined here for logic programming consists of a small number of operations over modules which are (meta-) logically defined and semantically justified in terms of the basic logic programming semantics. The operations enjoy a number of algebraic properties, thus yielding an algebra of modules. Despite its simplicity, the suite of operations is shown capable of capturing the core features of modularization: information hiding, import/export relationships, and construction of module hierarchies. A metalevel implementation and a compilation-oriented implementation of the operations are provided and proved sound with respect to the semantics. The compilation-oriented implementation is based on manipulation of name spaces and provides the basis for an efficient implementation.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2017160649",
    "type": "article"
  },
  {
    "title": "INC",
    "doi": "https://doi.org/10.1145/103135.103137",
    "publication_date": "1991-04-01",
    "publication_year": 1991,
    "authors": "Daniel M. Yellin; Robert E. Strom",
    "corresponding_authors": "",
    "abstract": "An incremental computation is one that is performed repeatedly on nearly identical inputs. Incremental computations occur naturally in many environments, such as compilers, language-based editors, spreadsheets, and formatters. This article describes a proposed tool for making it easy to write incremental programs. The tool consists of a programming language, INC, and a set of compile-time transformations for the primitive elements of INC. A programmer defines an algorithm in INC without regard to efficient incremental execution. The transformations automatically convert this algorithm into an efficient incremental algorithm. INC is a functional language. The implementation of an INC program is a network of processes. Each INC function is transformed into a process that receives and transmits messages describing changes to its inputs and outputs. We give an overview to the language and illustrate the incremental techniques employed by INC. We present the static and incremental complexity bounds for the primitive INC functions. We also present some example programs illustrating INC's flexibility.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2112648314",
    "type": "article"
  },
  {
    "title": "Optimal parallel generation of a computation tree form",
    "doi": "https://doi.org/10.1145/3318.3478",
    "publication_date": "1985-04-01",
    "publication_year": 1985,
    "authors": "Ilan Bar‐On; Uzi Vishkin",
    "corresponding_authors": "",
    "abstract": "Given a general arithmetic expression, we find a computation binary tree representation in O (log n ) time using n /log n processors on a concurrent-read, exclusive-write, parallel random-access machine. A new algorithm is introduced for this purpose. Unlike previous serial and parallel solutions, it is not based on using a stack.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W1981796679",
    "type": "article"
  },
  {
    "title": "A solution to a problem with Morel and Renvoise's “Global optimization by suppression of partial redundancies”",
    "doi": "https://doi.org/10.1145/48022.214509",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "Karl-Heinz Drechsler; Manfred Stadel",
    "corresponding_authors": "",
    "abstract": "Morel and Renvoise have previously described a method for global optimization and code motion by suppression of partial redundancies [l]. Morel and Renvoise use data flow analysis to determine expression computations that should be inserted at the end of certain basic blocks and to determine redundant computations that can be eliminated. The execution of these techniques results in the movement of loop invariant expressions out of the loop. In addition to [l] Morel and Renvoise's techniques can also be applied to subexpressions of larger expressions. Then, however, in certain special cases these optimization techniques move expressions to places where some of its subexpressions are neither available nor moved together with the expression. In this paper we present a modification of Morel and Renvoise's algorithm that avoids the above described situations.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W1994334933",
    "type": "article"
  },
  {
    "title": "Incremental attribute evaluation",
    "doi": "https://doi.org/10.1145/117009.117012",
    "publication_date": "1991-07-01",
    "publication_year": 1991,
    "authors": "Scott E. Hudson",
    "corresponding_authors": "Scott E. Hudson",
    "abstract": "article Free Access Share on Incremental attribute evaluation: a flexible algorithm for lazy update Author: Scott E. Hudson Univ. of Arizona, Tucson Univ. of Arizona, TucsonView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 3pp 315–341https://doi.org/10.1145/117009.117012Published:01 July 1991Publication History 56citation720DownloadsMetricsTotal Citations56Total Downloads720Last 12 Months63Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W1990597096",
    "type": "article"
  },
  {
    "title": "Axiomatic semantics of communicating sequential processes",
    "doi": "https://doi.org/10.1145/1780.1805",
    "publication_date": "1984-10-01",
    "publication_year": 1984,
    "authors": "Ν. Soundararajan",
    "corresponding_authors": "Ν. Soundararajan",
    "abstract": "article Free AccessAxiomatic semantics of communicating sequential processes Author: N. Soundararajan The Ohio State Univ. The Ohio State Univ.View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 4pp 647–662https://doi.org/10.1145/1780.1805Published:01 October 1984Publication History 56citation602DownloadsMetricsTotal Citations56Total Downloads602Last 12 Months20Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2030590001",
    "type": "article"
  },
  {
    "title": "An on-the-fly reference-counting garbage collector for java",
    "doi": "https://doi.org/10.1145/1111596.1111597",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Yossi Levanoni; Erez Petrank",
    "corresponding_authors": "",
    "abstract": "Reference-counting is traditionally considered unsuitable for multiprocessor systems. According to conventional wisdom, the update of reference slots and reference-counts requires atomic or synchronized operations. In this work we demonstrate this is not the case by presenting a novel reference-counting algorithm suitable for a multiprocessor system that does not require any synchronized operation in its write barrier (not even a compare-and-swap type of synchronization). A second novelty of this algorithm is that it allows eliminating a large fraction of the reference-count updates, thus, drastically reducing the reference-counting traditional overhead. This article includes a full proof of the algorithm showing that it is safe (does not reclaim live objects) and live (eventually reclaims all unreachable objects).We have implemented our algorithm on Sun Microsystems' Java Virtual Machine (JVM) 1.2.2 and ran it on a four-way IBM Netfinity 8500R server with 550-MHz Intel Pentium III Xeon and 2 GB of physical memory. Our results show that the algorithm has an extremely low latency and throughput that is comparable to the stop-the-world mark and sweep algorithm used in the original JVM.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2079043385",
    "type": "article"
  },
  {
    "title": "A theory of overloading",
    "doi": "https://doi.org/10.1145/1108970.1108974",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Peter J. Stuckey; Martin Sulzmann",
    "corresponding_authors": "",
    "abstract": "We present a novel approach to allow for overloading of identifiers in the spirit of type classes. Our approach relies on a combination of the HM(X) type system framework with Constraint Handling Rules (CHRs). CHRs are a declarative language for writing incremental constraint solvers, that provide our scheme with a form of programmable type language. CHRs allow us to precisely describe the relationships among overloaded identifiers. Under some sufficient conditions on the CHRs we achieve decidable type inference and the semantic meaning of programs is unambiguous. Our approach provides a common formal basis for many type class extensions such as multiparameter type classes and functional dependencies.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2091033850",
    "type": "article"
  },
  {
    "title": "Nominal logic programming",
    "doi": "https://doi.org/10.1145/1387673.1387675",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "James Cheney; Christian Urban",
    "corresponding_authors": "",
    "abstract": "Nominal logic is an extension of first-order logic which provides a simple foundation for formalizing and reasoning about abstract syntax modulo consistent renaming of bound names (that is, α-equivalence). This article investigates logic programming based on nominal logic. We describe some typical nominal logic programs, and develop the model-theoretic, proof-theoretic, and operational semantics of such programs. Besides being of interest for ensuring the correct behavior of implementations, these results provide a rigorous foundation for techniques for analysis and reasoning about nominal logic programs, as we illustrate via examples.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2172176924",
    "type": "article"
  },
  {
    "title": "Analysis of modular arithmetic",
    "doi": "https://doi.org/10.1145/1275497.1275504",
    "publication_date": "2007-08-02",
    "publication_year": 2007,
    "authors": "Markus Müller-Olm; Helmut Seidl",
    "corresponding_authors": "",
    "abstract": "We consider integer arithmetic modulo a power of 2 as provided by mainstream programming languages like Java or standard implementations of C. The difficulty here is that, for w &gt; 1, the ring Z m of integers modulo m = 2 w has zero divisors and thus cannot be embedded into a field. Not withstanding that, we present intra- and interprocedural algorithms for inferring for every program point u affine relations between program variables valid at u . If conditional branching is replaced with nondeterministic branching, our algorithms are not only sound but also complete in that they detect all valid affine relations in a natural class of programs. Moreover, they run in time linear in the program size and polynomial in the number of program variables and can be implemented by using the same modular integer arithmetic as the target language to be analyzed. We also indicate how our analysis can be extended to deal with equality guards, even in an interprocedural setting.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W1991504773",
    "type": "article"
  },
  {
    "title": "A formal approach to undo operations in programming languages",
    "doi": "https://doi.org/10.1145/5001.5005",
    "publication_date": "1986-01-02",
    "publication_year": 1986,
    "authors": "George B. Leeman",
    "corresponding_authors": "George B. Leeman",
    "abstract": "A framework is presented for adding a general Undo facility to programming languages. A discussion of relevant literature is provided to show that the idea of Undoing pervades several areas in computer science, and even other disciplines. A simple model of computation is introduced, and it is augmented with a minimal amount of additional structure needed for recovery and reversal. Two different interpretations of Undo are motivated with examples. Then, four primitives are defined in a language-independent manner; they are sufficient to support a wide range of Undo capability. Two of these primitives carry out state saving, and the others mirror the two versions of the Undo operation. Properties of and relationships between these primitives are explored, and there are some preliminary remarks on how one could implement a system based on this formalism. The main conclusion is that the notions of recovery and reversal of actions can become part of the programming process.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W1987371135",
    "type": "article"
  },
  {
    "title": "Functional computations in logic programs",
    "doi": "https://doi.org/10.1145/65979.65984",
    "publication_date": "1989-07-01",
    "publication_year": 1989,
    "authors": "Saumya Debray; David S. Warren",
    "corresponding_authors": "",
    "abstract": "Although the ability to simulate nondeterminism and to compute multiple solutions for a single query is a powerful and attractive feature of logic programming languages, it is expensive in both time and space. Since programs in such languages are very often functional, that is, they do not produce more than one distinct solution for a single input, this overhead is especially undesirable. This paper describes how programs may be analyzed statically to determine which literals and predicates are functional, and how the program may then be optimized using this information. Our notion of “functionality” subsumes the notion of “determinacy” that has been considered by various researchers. Our algorithm is less reliant on language features such as the cut, and thus extends more easily to parallel execution strategies, than others that have been proposed.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2092371432",
    "type": "article"
  },
  {
    "title": "Exceptional situations and program reliability",
    "doi": "https://doi.org/10.1145/1330017.1330019",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Westley Weimer; George C. Necula",
    "corresponding_authors": "",
    "abstract": "It is difficult to write programs that behave correctly in the presence of run-time errors. Proper behavior in the face of exceptional situations is important to the reliability of long-running programs. Existing programming language features often provide poor support for executing clean-up code and for restoring invariants. We present a data-flow analysis for finding a certain class of exception-handling defects: those related to a failure to release resources or to clean up properly along all paths. Many real-world programs violate such resource usage rules because of incorrect exception handling. Our flow-sensitive analysis keeps track of outstanding obligations along program paths and does a precise modeling of control flow in the presence of exceptions. Using it, we have found over 1,300 exception handling defects in over 5 million lines of Java code. Based on those defects we propose a programming language feature, the compensation stack, that keeps track of obligations at run time and ensures that they are discharged. We present a type system for compensation stacks that tracks collections of obligations. Finally, we present case studies to demonstrate that this feature is natural, efficient, and can improve reliability.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2141670850",
    "type": "article"
  },
  {
    "title": "<i>win</i> and <i>sin</i> : predicate transformers for concurrency",
    "doi": "https://doi.org/10.1145/78969.78970",
    "publication_date": "1990-07-01",
    "publication_year": 1990,
    "authors": "Leslie Lamport",
    "corresponding_authors": "Leslie Lamport",
    "abstract": "The weakest liberal precondition and strongest postcondition predicate transformers are generalized to the weakest invariant and strongest invariant . These new predicate transformers are useful for reasoning about concurrent programs containing operations in which the grain of atomicity is unspecified. They can also be used to replace behavioral arguments with more rigorous assertional ones.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2105673930",
    "type": "article"
  },
  {
    "title": "An experimental analysis of self-adjusting computation",
    "doi": "https://doi.org/10.1145/1596527.1596530",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Umut A. Acar; Guy E. Blelloch; M. Blume; Robert Harper; Kanat Tangwongsan",
    "corresponding_authors": "",
    "abstract": "Recent work on adaptive functional programming (AFP) developed techniques for writing programs that can respond to modifications to their data by performing change propagation . To achieve this, executions of programs are represented with dynamic dependence graphs (DDGs) that record data dependences and control dependences in a way that a change-propagation algorithm can update the computation as if the program were from scratch, by re-executing only the parts of the computation affected by the changes. Since change-propagation only re-executes parts of the computation, it can respond to certain incremental modifications asymptotically faster than recomputing from scratch, potentially offering significant speedups. Such asymptotic speedups, however, are rare: for many computations and modifications, change propagation is no faster than recomputing from scratch. In this article, we realize a duality between dynamic dependence graphs and memoization, and combine them to give a change-propagation algorithm that can dramatically increase computation reuse. The key idea is to use DDGs to identify and re-execute the parts of the computation that are affected by modifications, while using memoization to identify the parts of the computation that remain unaffected by the changes. We refer to this approach as self-adjusting computation. Since DDGs are imperative, but (traditional) memoization requires purely functional computation, reusing computation correctly via memoization becomes a challenge. We overcome this challenge with a technique for remembering and reusing not just the results of function calls (as in conventional memoization), but their executions represented with DDGs. We show that the proposed approach is realistic by describing a library for self-adjusting computation, presenting efficient algorithms for realizing the library, and describing and evaluating an implementation. Our experimental evaluation with a variety of applications, ranging from simple list primitives to more sophisticated computational geometry algorithms, shows that the approach is effective in practice: compared to recomputing from-scratch; self-adjusting programs respond to small modifications to their data orders of magnitude faster.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2023614281",
    "type": "article"
  },
  {
    "title": "Termination analysis of logic programs through combination of type-based norms",
    "doi": "https://doi.org/10.1145/1216374.1216378",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Maurice Bruynooghe; Michael Codish; John P. Gallagher; Samir Genaim; Wim Vanhoof",
    "corresponding_authors": "",
    "abstract": "This article makes two contributions to the work on semantics-based termination analysis for logic programs. The first involves a novel notion of type - based norm where for a given type, a corresponding norm is defined to count in a term the number of subterms of that type. This provides a collection of candidate norms, one for each type defined in the program. The second enables an analyzer to base termination proofs on the combination of several different norms. This is useful when different norms are better suited to justify the termination of different parts of the program. Application of the two contributions together consists in considering the combination of the type-based candidate norms for a given program. This results in a powerful and practical technique. Both contributions have been introduced into a working termination analyzer. Experimentation indicates that they yield state-of-the-art results in a fully automatic analysis tool, improving with respect to methods that do not use both types and combined norms.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2070546615",
    "type": "article"
  },
  {
    "title": "Ada exception handling: an axiomatic approach",
    "doi": "https://doi.org/10.1145/357094.357100",
    "publication_date": "1980-04-01",
    "publication_year": 1980,
    "authors": "D Luckham; Wolfgang Polak",
    "corresponding_authors": "",
    "abstract": "A method of documenting exception propagation and handling in Ada programs is proposed. Exception propagation declarations are introduced as a new component of Ada specifications, permitting documentation of those exceptions that can be propagated by a subprogram. Exception handlers are documented by entry assertions. Axioms and proof rules for Ada exceptions given. These rules are simple extensions of previous rules for Pascal and define an axiomatic semantics of Ada exceptions. As a result, Ada programs specified according to the method can be analyzed by formal proof techniques for consistency with their specifications, even if they employ exception propagation and handling to achieve required results (i.e., nonerror situations). Example verifications are given.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2081379485",
    "type": "article"
  },
  {
    "title": "Verified interoperable implementations of security protocols",
    "doi": "https://doi.org/10.1145/1452044.1452049",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "Karthikeyan Bhargavan; Cédric Fournet; Andrew D. Gordon; Stephen Tse",
    "corresponding_authors": "",
    "abstract": "We present an architecture and tools for verifying implementations of security protocols. Our implementations can run with both concrete and symbolic implementations of cryptographic algorithms. The concrete implementation is for production and interoperability testing. The symbolic implementation is for debugging and formal verification. We develop our approach for protocols written in F#, a dialect of ML, and verify them by compilation to ProVerif, a resolution-based theorem prover for cryptographic protocols. We establish the correctness of this compilation scheme, and we illustrate our approach with protocols for Web Services security.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W1991234099",
    "type": "article"
  },
  {
    "title": "Flow-insensitive type qualifiers",
    "doi": "https://doi.org/10.1145/1186632.1186635",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Jeffrey S. Foster; Robert T. Johnson; John Kodumal; Alex Aiken",
    "corresponding_authors": "",
    "abstract": "We describe flow-insensitive type qualifiers, a lightweight, practical mechanism for specifying and checking properties not captured by traditional type systems. We present a framework for adding new, user-specified type qualifiers to programming languages with static type systems, such as C and Java. In our system, programmers add a few type qualifier annotations to their program, and automatic type qualifier inference determines the remaining qualifiers and checks the annotations for consistency. We describe a tool CQual for adding type qualifiers to the C programming language. Our tool CQual includes a visualization component for displaying browsable inference results to the programmer. Finally, we present several experiments using our tool, including inferring const qualifiers, finding security vulnerabilities in several popular C programs, and checking initialization data usage in the Linux kernel. Our results suggest that inference and visualization make type qualifiers lightweight, that type qualifier inference scales to large programs, and that type qualifiers are applicable to a wide variety of problems.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2157859774",
    "type": "article"
  },
  {
    "title": "Prettyprinting",
    "doi": "https://doi.org/10.1145/357114.357115",
    "publication_date": "1980-10-01",
    "publication_year": 1980,
    "authors": "Dereck C. Oppen",
    "corresponding_authors": "Dereck C. Oppen",
    "abstract": "An algorithm for prettyprinting is given. For an input stream of length n and an output device with linewidth m , the algorithm requires time O ( n ) and space O ( m ). The algorithm is described in terms of two parallel processes: the first scans the input stream to determine the space required to print logical blocks of tokens; the second uses this information to decide where to break lines of text; the two processes communicate by means of a buffer of size O ( m ). The algorithm does not wait for the entire stream to be input, but begins printing as soon as it has received a full line of input. The algorithm is easily implemented.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2912911385",
    "type": "article"
  },
  {
    "title": "A Distributed Graph Algorithm: Knot Detection",
    "doi": "https://doi.org/10.1145/69622.357190",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "Jayadev Misra; K. Mani Chandy",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A Distributed Graph Algorithm: Knot Detection Authors: Jayadev Misra Department of Computer Sciences, University of Texas, Austin, TX Department of Computer Sciences, University of Texas, Austin, TXView Profile , K. M. Chandy Department of Computer Sciences, University of Texas, Austin, TX Department of Computer Sciences, University of Texas, Austin, TXView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 678–686https://doi.org/10.1145/69622.357190Published:01 October 1982Publication History 56citation886DownloadsMetricsTotal Citations56Total Downloads886Last 12 Months98Last 6 weeks24 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2142302732",
    "type": "article"
  },
  {
    "title": "Types for atomicity",
    "doi": "https://doi.org/10.1145/1377492.1377495",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Cormac Flanagan; Stephen N. Freund; Marina Lifshin; Shaz Qadeer",
    "corresponding_authors": "",
    "abstract": "Atomicity is a fundamental correctness property in multithreaded programs. A method is atomic if, for every execution, there is an equivalent serial execution in which the actions of the method are not interleaved with actions of other threads. Atomic methods are amenable to sequential reasoning, which significantly facilitates subsequent analysis and verification. This article presents a type system for specifying and verifying the atomicity of methods in multithreaded Java programs using a synthesis of Lipton's theory of reduction and type systems for race detection. The type system supports guarded, write-guarded, and unguarded fields, as well as thread-local data, parameterized classes and methods, and protected locks. We also present an algorithm for verifying atomicity via type inference. We have applied our type checker and type inference tools to a number of commonly used Java library classes and programs. These tools were able to verify the vast majority of methods in these benchmarks as atomic, indicating that atomicity is a widespread methodology for multithreaded programming. In addition, reported atomicity violations revealed some subtle errors in the synchronization disciplines of these programs.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2171537091",
    "type": "article"
  },
  {
    "title": "Fast online pointer analysis",
    "doi": "https://doi.org/10.1145/1216374.1216379",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Martin Hirzel; Daniel von Dincklage; Amer Diwan; Michael Hind",
    "corresponding_authors": "",
    "abstract": "Pointer analysis benefits many useful clients, such as compiler optimizations and bug finding tools. Unfortunately, common programming language features such as dynamic loading, reflection, and foreign language interfaces, make pointer analysis difficult. This article describes how to deal with these features by performing pointer analysis online during program execution. For example, dynamic loading may load code that is not available for analysis before the program starts. Only an online analysis can analyze such code, and thus support clients that optimize or find bugs in it. This article identifies all problems in performing Andersen's pointer analysis for the full Java language, presents solutions to these problems, and uses a full implementation of the solutions in a Java virtual machine for validation and performance evaluation. Our analysis is fast: On average over our benchmark suite, if the analysis recomputes points-to results upon each program change, most analysis pauses take under 0.1 seconds, and add up to 64.5 seconds.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2139980638",
    "type": "article"
  },
  {
    "title": "Equivalence checking of static affine programs using widening to handle recurrences",
    "doi": "https://doi.org/10.1145/2362389.2362390",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Sven Verdoolaege; Gerda Janssens; Maurice Bruynooghe",
    "corresponding_authors": "",
    "abstract": "Designers often apply manual or semi-automatic loop and data transformations on array- and loop-intensive programs to improve performance. It is crucial that such transformations preserve the functionality of the program. This article presents an automatic method for constructing equivalence proofs for the class of static affine programs. The equivalence checking is performed on a dependence graph abstraction and uses a new approach based on widening to find the proper induction hypotheses for reasoning about recurrences. Unlike transitive-closure-based approaches, this widening approach can also handle nonuniform recurrences. The implementation is publicly available and is the first of its kind to fully support commutative operations.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2046877387",
    "type": "article"
  },
  {
    "title": "Mechanically verified proof obligations for linearizability",
    "doi": "https://doi.org/10.1145/1889997.1890001",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "John Derrick; Gerhard Schellhorn; Heike Wehrheim",
    "corresponding_authors": "",
    "abstract": "Concurrent objects are inherently complex to verify. In the late 80s and early 90s, Herlihy and Wing proposed linearizability as a correctness condition for concurrent objects, which, once proven, allows us to reason about concurrent objects using pre- and postconditions only. A concurrent object is linearizable if all of its operations appear to take effect instantaneously some time between their invocation and return. In this article we define simulation-based proof conditions for linearizability and apply them to two concurrent implementations, a lock-free stack and a set with lock-coupling. Similar to other approaches, we employ a theorem prover (here, KIV) to mechanize our proofs. Contrary to other approaches, we also use the prover to mechanically check that our proof obligations actually guarantee linearizability. This check employs the original ideas of Herlihy and Wing of verifying linearizability via possibilities .",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2089064888",
    "type": "article"
  },
  {
    "title": "Dependent Type Theory for Verification of Information Flow and Access Control Policies",
    "doi": "https://doi.org/10.1145/2491522.2491523",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Aleksandar Nanevski; Anindya Banerjee; Deepak Garg",
    "corresponding_authors": "",
    "abstract": "Dedicated to the memory of John C. Reynolds (1935--2013). We present Relational Hoare Type Theory (RHTT), a novel language and verification system capable of expressing and verifying rich information flow and access control policies via dependent types. We show that a number of security policies which have been formalized separately in the literature can all be expressed in RHTT using only standard type-theoretic constructions such as monads, higher-order functions, abstract types, abstract predicates, and modules. Example security policies include conditional declassification, information erasure, and state-dependent information flow and access control. RHTT can reason about such policies in the presence of dynamic memory allocation, deallocation, pointer aliasing and arithmetic.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2162846161",
    "type": "article"
  },
  {
    "title": "Algorithmic Analysis of Qualitative and Quantitative Termination Problems for Affine Probabilistic Programs",
    "doi": "https://doi.org/10.1145/3174800",
    "publication_date": "2018-05-28",
    "publication_year": 2018,
    "authors": "Krishnendu Chatterjee; Hongfei Fu; Petr Novotný; Rouzbeh Hasheminezhad",
    "corresponding_authors": "",
    "abstract": "In this article, we consider the termination problem of probabilistic programs with real-valued variables. The questions concerned are: qualitative ones that ask (i) whether the program terminates with probability 1 (almost-sure termination) and (ii) whether the expected termination time is finite (finite termination); and quantitative ones that ask (i) to approximate the expected termination time (expectation problem) and (ii) to compute a bound B such that the probability not to terminate after B steps decreases exponentially (concentration problem). To solve these questions, we utilize the notion of ranking supermartingales, which is a powerful approach for proving termination of probabilistic programs. In detail, we focus on algorithmic synthesis of linear ranking-supermartingales over affine probabilistic programs (A pps ) with both angelic and demonic non-determinism. An important subclass of A pps is LRA pp which is defined as the class of all A pps over which a linear ranking-supermartingale exists. Our main contributions are as follows. Firstly, we show that the membership problem of LRA pp (i) can be decided in polynomial time for A pps with at most demonic non-determinism, and (ii) is NP-hard and in PSPACE for A pps with angelic non-determinism. Moreover, the NP-hardness result holds already for A pps without probability and demonic non-determinism. Secondly, we show that the concentration problem over LRA pp can be solved in the same complexity as for the membership problem of LRA pp . Finally, we show that the expectation problem over LRA pp can be solved in 2EXPTIME and is PSPACE-hard even for A pps without probability and non-determinism (i.e., deterministic programs). Our experimental results demonstrate the effectiveness of our approach to answer the qualitative and quantitative questions over A pps with at most demonic non-determinism.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2807469900",
    "type": "article"
  },
  {
    "title": "A Widening Approach to Multithreaded Program Verification",
    "doi": "https://doi.org/10.1145/2629608",
    "publication_date": "2014-10-28",
    "publication_year": 2014,
    "authors": "Alexander Kaiser; Daniel Kroening; Thomas Wahl",
    "corresponding_authors": "",
    "abstract": "Pthread-style multithreaded programs feature rich thread communication mechanisms, such as shared variables, signals, and broadcasts. In this article, we consider the automated verification of such programs where an unknown number of threads execute a given finite-data procedure in parallel. Such procedures are typically obtained as predicate abstractions of recursion-free source code written in C or Java. Many safety problems over finite-data replicated multithreaded programs are decidable via a reduction to the coverability problem in certain types of well-ordered infinite-state transition systems. On the other hand, in full generality, this problem is Ackermann-hard, which seems to rule out efficient algorithmic treatment. We present a novel, sound, and complete yet empirically efficient solution. Our approach is to judiciously widen the original set of coverability targets by configurations that involve fewer threads and are thus easier to decide, and whose exploration may well be sufficient: if they turn out uncoverable, so are the original targets. To soften the impact of “bad guesses”—configurations that turn out coverable—the exploration is accompanied by a parallel engine that generates coverable configurations; none of these is ever selected for widening. Its job being merely to prevent bad widening choices, such an engine need not be complete for coverability analysis, which enables a range of existing partial (e.g., nonterminating) techniques. We present extensive experiments on multithreaded C programs, including device driver code from FreeBSD, Solaris, and Linux distributions. Our approach outperforms existing coverability methods by orders of magnitude.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2078846861",
    "type": "article"
  },
  {
    "title": "Type-Driven Gradual Security with References",
    "doi": "https://doi.org/10.1145/3229061",
    "publication_date": "2018-12-13",
    "publication_year": 2018,
    "authors": "Matías Toro; Ronald Garcia; Éric Tanter",
    "corresponding_authors": "",
    "abstract": "In security-typed programming languages, types statically enforce noninterference between potentially conspiring values, such as the arguments and results of functions. But to adopt static security types, like other advanced type disciplines, programmers face a steep wholesale transition, often forcing them to refactor working code just to satisfy their type checker. To provide a gentler path to security typing that supports safe and stylish but hard-to-verify programming idioms, researchers have designed languages that blend static and dynamic checking of security types. Unfortunately, most of the resulting languages only support static, type-based reasoning about noninterference if a program is entirely statically secured. This limitation substantially weakens the benefits that dynamic enforcement brings to static security typing. Additionally, current proposals are focused on languages with explicit casts and therefore do not fulfill the vision of gradual typing, according to which the boundaries between static and dynamic checking only arise from the (im)precision of type annotations and are transparently mediated by implicit checks. In this article, we present GSL Ref , a gradual security-typed higher-order language with references. As a gradual language, GSL Ref supports the range of static-to-dynamic security checking exclusively driven by type annotations, without resorting to explicit casts. Additionally, GSL Ref lets programmers use types to reason statically about termination-insensitive noninterference in all programs, even those that enforce security dynamically. We prove that GSL Ref satisfies all but one of Siek et al.’s criteria for gradually-typed languages, which ensure that programs can seamlessly transition between simple typing and security typing. A notable exception regards the dynamic gradual guarantee, which some specific programs must violate if they are to satisfy noninterference; it remains an open question whether such a language could fully satisfy the dynamic gradual guarantee. To realize this design, we were led to draw a sharp distinction between syntactic type safety and semantic type soundness , each of which constrains the design of the gradual language.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2904756193",
    "type": "article"
  },
  {
    "title": "A Practical Approach for Model Checking C/C++11 Code",
    "doi": "https://doi.org/10.1145/2806886",
    "publication_date": "2016-05-02",
    "publication_year": 2016,
    "authors": "Brian Norris; Brian Demsky",
    "corresponding_authors": "",
    "abstract": "Writing low-level concurrent software has traditionally required intimate knowledge of the entire toolchain and often has involved coding in assembly. New language standards have extended C and C++ with support for low-level atomic operations and a weak memory model, enabling developers to write portable and efficient multithreaded code. In this article, we present CDSC hecker , a tool for exhaustively exploring the behaviors of concurrent code under the C/C++ memory model. We have used CDSC hecker to exhaustively unit test concurrent data structure implementations and have discovered errors in a published implementation of a work-stealing queue and a single producer, single consumer queue.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2347106652",
    "type": "article"
  },
  {
    "title": "Armed Cats",
    "doi": "https://doi.org/10.1145/3458926",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Jade Alglave; Will Deacon; Richard Grisenthwaite; Antoine Hacquard; Luc Maranget",
    "corresponding_authors": "",
    "abstract": "We report on the process for formal concurrency modelling at Arm. An initial formal consistency model of the Arm achitecture, written in the cat language, was published and upstreamed to the herd+diy tool suite in 2017. Since then, we have extended the original model with extra features, for example, mixed-size accesses, and produced two provably equivalent alternative formulations. In this article, we present a comprehensive review of work done at Arm on the consistency model. Along the way, we also show that our principle for handling mixed-size accesses applies to x86: We confirm this via vast experimental campaigns. We also show that our alternative formulations are applicable to any model phrased in a style similar to the one chosen by Arm.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W3186023305",
    "type": "article"
  },
  {
    "title": "Capturing Types",
    "doi": "https://doi.org/10.1145/3618003",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Aleksander Boruch-Gruszecki; Martin Odersky; Edward Lee; Ondřej Lhoták; Jonathan Immanuel Brachthäuser",
    "corresponding_authors": "",
    "abstract": "Type systems usually characterize the shape of values but not their free variables. However, many desirable safety properties could be guaranteed if one knew the free variables captured by values. We describe CC &lt; :◻ , a calculus where such captured variables are succinctly represented in types, and show it can be used to safely implement effects and effect polymorphism via scoped capabilities. We discuss how the decision to track captured variables guides key aspects of the calculus, and show that CC &lt; :◻ admits simple and intuitive types for common data structures and their typical usage patterns. We demonstrate how these ideas can be used to guide the implementation of capture checking in a practical programming language.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4386711821",
    "type": "article"
  },
  {
    "title": "Utilizing symmetry when model-checking under fairness assumptions",
    "doi": "https://doi.org/10.1145/262004.262008",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "E. Allen Emerson; A. Prasad Sistla",
    "corresponding_authors": "",
    "abstract": "One useful technique for combating the state explosion problem is to exploit symmetry when performing temporal logic model checking. In previous work it is shown how, using some basic notions of group theory, symmetry may be exploited for the full range of correctness properties expressible in the very expressive temporal logic CTL*. Surprisingly, while fairness properties are readily expressible in CTL*, these methods are not powerful enough to admit any amelioration of state explosion, when fairness assumptions are involved. We show that it is nonetheless possible to handle fairness efficiently by trading some group theory for automata theory. Our automata-theoretic approach depends on detecting fair paths subtly encoded in a quotient structure whose arcs are annotated with permutations, by using a threaded structure that reflects coordinate shifts caused by the permutations.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2055558998",
    "type": "article"
  },
  {
    "title": "More dynamic object reclassification",
    "doi": "https://doi.org/10.1145/514952.514955",
    "publication_date": "2002-03-01",
    "publication_year": 2002,
    "authors": "Sophia Drossopoulou; Ferruccio Damiani; Mariangiola Dezani-Ciancaglini; Paola Giannini",
    "corresponding_authors": "",
    "abstract": "Reclassification changes the class membership of an object at run-time while retaining its identity. We suggest language features for object reclassification, which extend an imperative, typed, class-based, object-oriented language.We present our proposal through the language Fickle ⋄⋄ . The imperative features, combined with the requirement for a static and safe type system, provided the main challenges. We develop a type and effect system for Fickle ⋄⋄ and prove its soundness with respect to the operational semantics. In particular, even though objects may be reclassified across classes with different members, there will never be an attempt to access nonexisting members.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2084315690",
    "type": "article"
  },
  {
    "title": "A type system for Java bytecode subroutines",
    "doi": "https://doi.org/10.1145/314602.314606",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Raymie Stata; Martı́n Abadi",
    "corresponding_authors": "",
    "abstract": "Java is typically compiled into an intermediate language, JVML, that is interpreted by the Java Virtual Machine. Because mobile JVML code is not always trusted, a bytecode verifier enforces static constraints that prevent various dynamic errors. Given the importance of the bytecode verifier for security, its current descriptions are inadequate. This article proposes using typing rules to describe the bytecode verifier because they are more precise than prose, clearer than code, and easier to reason about than either. JVML has a subroutine construct which is used for the compilation of Java's try-finally statement. Subroutines are a major source of complexity for the bytecode verifier because they are not obviously last-in/first-out and because they require a kind of polymorphism. Focusing on subroutines, we isolate an interesting, small subset of JVML. We give typing rules for this subset and prove their correctness. Our type system constitutes a sound basis for bytecode verification and a rational reconstruction of a delicate part of Sun's bytecode verifier.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2031833469",
    "type": "article"
  },
  {
    "title": "Efficient and precise array access analysis",
    "doi": "https://doi.org/10.1145/509705.509708",
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "Yunheung Paek; Jay Hoeflinger; David Padua",
    "corresponding_authors": "",
    "abstract": "A number of existing compiler techniques hinge on the analysis of array accesses in a program. The most important task in array access analysis is to collect the information about array accesses of interest and summarize it in some standard form. Traditional forms used in array access analysis are sensitive to the complexity of array subscripts; that is, they are usually quite accurate and efficient for simple array subscripting expressions, but lose accuracy or require potentially expensive algorithms for complex subscripts. Our study has revealed that in many programs, particularly numerical applications, many access patterns are simple in nature even when the subscripting expressions are complex. Based on this analysis, we have developed a new, general array region representational form, called the linear memory access descriptor (LMAD). The key idea of the LMAD is to relate all memory accesses to the linear machine memory rather than to the shape of the logical data structures of a programming language. This form helps us expose the simplicity of the actual patterns of array accesses in memory, which may be hidden by complex array subscript expressions. Our recent experimental studies show that our new representation simplifies array access analysis and, thus, enables efficient and accurate compiler analysis.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W1980390491",
    "type": "article"
  },
  {
    "title": "Symbolic analysis for parallelizing compilers",
    "doi": "https://doi.org/10.1145/233561.233568",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "Mohammad R. Haghighat; Constantine D. Polychronopoulos",
    "corresponding_authors": "",
    "abstract": "The notion of dependence captures that most important properties of a program for efficient execution on parallel computers. The dependence structure of a program defines that necessary constraints of the order of execution of the program components and provides sufficient information for the exploitation of the available parallelism. Static discovery and management of the dependence structure of programs save a tremendous amount of execution time, and dynamic utilization of dependence information results in a significant performance gain on parallel computers. However, experiments with parallel computers indicate that existing multiprocessing environments are unable to deliver the desired performance over a wide range of real applications, mainly due to lack of precision of their dependence information. This calls for an effective compilation scheme capable of understanding the dependence structure of complicated application programs. This article describes a methodology for capturing analyzing program properties that are essential in the effective detection and efficient exploitation of parallelism on parallel computers. Based on this methodology, a symbolic analysis framework is developed for the Parafrase-2 parallelizing compiler. This framework extends the scope of a variety of important program analysis problems and solves them in a unified way. The attained solution space of these problems is much larger than that handled by existing compiler technology. Such a powerful approach is required for the effective compilation of a large class of application programs.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W4300721217",
    "type": "article"
  },
  {
    "title": "Operator strength reduction",
    "doi": "https://doi.org/10.1145/504709.504710",
    "publication_date": "2001-09-01",
    "publication_year": 2001,
    "authors": "Keith D. Cooper; L. Taylor Simpson; Christopher A. Vick",
    "corresponding_authors": "",
    "abstract": "Operator strength reduction is a technique that improves compiler-generated code by reformulating certain costly computations in terms of less expensive ones. A common case arises in array addressing expressions used in loops. The compiler can replace the sequence of multiplies generated by a direct translation of the address expression with an equivalent sequence of additions. When combined with linear function test replacement, strength reduction can speed up the execution of loops containing array references. The improvement comes from two sources: a reduction in the number of operations needed to implement the loop and the use of less costly operations.This paper presents a new algorithm for operator strength reduction, called OSR. OSR improves upon an earlier algorithm of Allen, Cocke, and Kennedy [Allen et al. 1981]. OSR operates on the static single assignment (SSA) form of a procedure [Cytron et al. 1991]. By taking advantage of the properties of SSA form, we have derived an algorithm that is simple to understand, quick to implement, and, in practice, fast to run. Its asymptotic complexity is, in the worst case, the same as the Allen, Cocke,and Kennedy algorithm (ACK). OSR achieves optimization results that are equivalent to those obtained with the ACK algorithm. OSR has been implemented in several research and production compilers.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2069996154",
    "type": "article"
  },
  {
    "title": "A type system for object initialization in the Java bytecode language",
    "doi": "https://doi.org/10.1145/330643.330646",
    "publication_date": "1999-11-01",
    "publication_year": 1999,
    "authors": "Stephen N. Freund; John C. Mitchell",
    "corresponding_authors": "",
    "abstract": "In the standard Java implementation, a Java language program is compiled to Java bytecode. This bytecode may be sent across the network to another site, where it is then executed by the Java Virtual Machine. Since bytecode may be written by hand, or corrupted during network transmission, the Java Virtual Machine contains a bytecode verifier that performs a number of consistency checks before code is run. These checks include type correctness and, as illus-trated by previous attacks on the Java Virtual Machine, are critical for system security. In order to analyze existing bytecode verifiers and to understand the properties that should be verified, we develop a precise specification of statically correct Java bytecode, in the form of a type system. Our focus in this article is a subset of the bytecode language dealing with object creation and initialization. For this subset, we prove, that, for every Java bytecode program that satisfies our typing constraints, every object is initialized before it is used. The type system is easily combined with a previous system developed by Stata and Abadi for bytecode subroutines. Our analysis of subroutines and object initialization reveals a previously unpub-lished bug in the Sun JDK bytecode verifier.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2010608535",
    "type": "article"
  },
  {
    "title": "A type system equivalent to flow analysis",
    "doi": "https://doi.org/10.1145/210184.210187",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "Jens Palsberg; Patrick O'Keefe",
    "corresponding_authors": "",
    "abstract": "Flow-based safety analysis of higher-order languages has been studied by Shivers, and Palsberg and Schwartzbach. Open until now is the problem of finding a type system that accepts exactly the same programs as safety analysis. In this article we prove that Amadio and Cardelli's type system with subtyping and recursive types accepts the same programs as a certain safety analysis. The proof involves mappings from types to flow information and back. As a result, we obtain an inference algorithm for the type system, thereby solving an open problem.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2052440299",
    "type": "article"
  },
  {
    "title": "Componential set-based analysis",
    "doi": "https://doi.org/10.1145/316686.316703",
    "publication_date": "1999-03-01",
    "publication_year": 1999,
    "authors": "Cormac Flanagan; Matthias Felleisen",
    "corresponding_authors": "",
    "abstract": "Set-based analysis (SBA) produces good predictions about the behavior of functional and object-oriented programs. The analysis proceeds by inferring constraints that characterize the data flow relationships of the analyzed program. Experiences with MrSpidey, a static debugger based on SBA, indicate that SBA can adequately deal with programs of up to a couple of thousand lines of code. SBA fails, however, to cope with larger programs because it generates systems of constraints that are at least linear, and possibility quadratic, in the size of the analyzed program. This article presents theoretical and practical results concerning methods for reducing the size of constraint systems. The theoretical results include of proof-theoretic characterization of the observable behavior of constraint systems for program components, and a complete algorithm for deciding the observable equivalence of constraint systems. In the course of this development we establish a close connection between the observable equivalence of constraint systems and the equivalence of regular-tree grammars. We then exploit this connection to adapt a variety of algoirthms for simplifying grammars to the problem of simplifying constraint systems. Based on the resulting algorithms, we have developed componential set-based analysis , a modular and polymorphic variant of SBA. Experimental results verify the effectiveness of the simplification algorithms and the componential analysis. The simplified constraint systems are typically an order of magnitude smaller than the original systems. These reductions in size produce significant gains in the speed of the analysis.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2034379323",
    "type": "article"
  },
  {
    "title": "A model parametric real-time logic",
    "doi": "https://doi.org/10.1145/133233.129397",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "Angelo Morzenti; Dino Mandrioli; Carlo Ghezzi",
    "corresponding_authors": "",
    "abstract": "TRIO is a formal notation for the logic-based specification of real-time systems. In this paper the language and its straightforward model-theoretic semantics are briefly summarized. Then the need for assigning a consistent meaning to TRIO specifications is discussed, with reference to a variety of underlying time structures such as infinite-time structures (both dense and discrete) and finite-time structures. The main motivation is the ability to validate formal specifications. A solution to this problem is presented, which gives a new, model-parametric semantics to the language. An algorithm for constructively verifying the satisfiability of formulas in the decidable cases is defined, and several important temporal properties of specifications are characterized.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2062663486",
    "type": "article"
  },
  {
    "title": "Composing first-class transactions",
    "doi": "https://doi.org/10.1145/197320.197346",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Nicholas Haines; Darrell Kindred; J. Gregory Morrisett; Scott Nettles; Jeannette M. Wing",
    "corresponding_authors": "",
    "abstract": "article Free AccessComposing first-class transactions Authors: Nicholas Haines Carnegie Mellon Univ., Pittsburgh, PA Carnegie Mellon Univ., Pittsburgh, PAView Profile , Darrell Kindred Carnegie Mellon Univ., Pittsburgh, PA Carnegie Mellon Univ., Pittsburgh, PAView Profile , J. Gregory Morrisett Carnegie Mellon Univ., Pittsburgh, PA Carnegie Mellon Univ., Pittsburgh, PAView Profile , Scott M. Nettles Carnegie Mellon Univ., Pittsburgh, PA Carnegie Mellon Univ., Pittsburgh, PAView Profile , Jeannette M. Wing Carnegie Mellon Univ., Pittsburgh, PA Carnegie Mellon Univ., Pittsburgh, PAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 16Issue 6Nov. 1994 pp 1719–1736https://doi.org/10.1145/197320.197346Published:01 November 1994Publication History 48citation387DownloadsMetricsTotal Citations48Total Downloads387Last 12 Months12Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2092311766",
    "type": "article"
  },
  {
    "title": "The derivation of distributed termination detection algorithms from garbage collection schemes",
    "doi": "https://doi.org/10.1145/151646.151647",
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "Gérard Tel; Friedemann Mattern",
    "corresponding_authors": "",
    "abstract": "It is shown that the termination detection problem for distributed computations can be modeled as an instance of the garbage collection problem. Consequently, algorithms for the termination detection problem are obtained by applying transformations to garbage collection algorithms. The transformation can be applied to collectors of the “mark-and-sweep” type as well as to reference-counting protocol of Lermen and Maurer, the weighted-reference-counting protocol, the local-reference-counting protocol, and Ben-Ari's mark-and-sweep collector into termination detection algorithms. Known termination detection algorithms as well as new variants are obtained.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2170471564",
    "type": "article"
  },
  {
    "title": "Higher-order distributed objects",
    "doi": "https://doi.org/10.1145/213978.213986",
    "publication_date": "1995-09-01",
    "publication_year": 1995,
    "authors": "Henry Cejtin; Suresh Jagannathan; Richard Kelsey",
    "corresponding_authors": "",
    "abstract": "We describe a distributed implementation of Scheme that permits efficient transmission of higher-order objects such as closures and continuations. The integration of distributed communication facilities within a higher-order programming language engenders a number of new abstractions and paradigms for distributed computing. Among these are user-specified load-balancing and migration policies for threads, incrementally linked distributed computations, and parameterized client-server applications. To our knowledge, this is the first distributed dialect of Scheme (or a related language) that addresses lightweight communication abstractions for higher-order objects.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W1971810486",
    "type": "article"
  },
  {
    "title": "Denotational abstract interpretation of logic programs",
    "doi": "https://doi.org/10.1145/177492.177650",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Kim Marriott; Harald Søndergaard; Neil D. Jones",
    "corresponding_authors": "",
    "abstract": "Logic-programming languages are based on a principle of separation “logic” and “control.”. This means that they can be given simple model-theoretic semantics without regard to any particular execution mechanism (or proof procedure, viewing execution as theorem proving). Although the separation is desirable from a semantical point of view, it makes sound, efficient implementation of logic-programming languages difficult. The lack of “control information” in programs calls for complex data-flow analysis techniques to guide execution. Since data-flow analysis furthermore finds extensive use in error-finding and transformation tools, there is a need for a simple and powerful theory of data-flow analysis of logic programs. This paper offers such a theory, based on F. Nielson's extension of P. Cousot and R. Cousot's abstract interpretation . We present a denotational definition of the semantics of definite logic programs. This definition is of interest in its own right because of its compactness. Stepwise we develop the definition into a generic data-flow analysis that encompasses a large class of data-flow analyses based on the SLD execution model. We exemplify one instance of the definition by developing a provably correct groundness analysis to predict how variables may be bound to ground terms during execution. We also discuss implementation issues and related work.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2008289497",
    "type": "article"
  },
  {
    "title": "Jam---designing a Java extension with mixins",
    "doi": "https://doi.org/10.1145/937563.937567",
    "publication_date": "2003-09-01",
    "publication_year": 2003,
    "authors": "Davide Ancona; Giovanni Lagorio; Elena Zucca",
    "corresponding_authors": "",
    "abstract": "In this paper we present Jam, an extension of the Java language supporting mixins , that is, parametric heir classes. A mixin declaration in Jam is similar to a Java heir class declaration, except that it does not extend a fixed parent class, but simply specifies the set of fields and methods a generic parent should provide. In this way, the same mixin can be instantiated on many parent classes, producing different heirs, thus avoiding code duplication and largely improving modularity and reuse. Moreover, as happens for classes and interfaces, mixin names are reference types, and all the classes obtained by instantiating the same mixin are considered subtypes of the corresponding type, and hence can be handled in a uniform way through the common interface. This possibility allows a programming style where different ingredients are \"mixed\" together in defining a class; this paradigm is somewhat similar to that based on multiple inheritance, but avoids its complication.The language has been designed with the main objective in mind to obtain, rather than a new theoretical language, a working and smooth extension of Java. That means, on the design side, that we have faced the challenging problem of integrating the Java overall principles and complex type system with this new notion; on the implementation side, it means that we have developed a Jam-to-Java translator which makes Jam sources executable on every Java Virtual Machine.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2086957250",
    "type": "article"
  },
  {
    "title": "Verifying parameterized networks",
    "doi": "https://doi.org/10.1145/265943.265960",
    "publication_date": "1997-09-01",
    "publication_year": 1997,
    "authors": "E. M. Clarke; Orna Grümberg; Sumit Kumar Jha",
    "corresponding_authors": "",
    "abstract": "This article describes a technique based on network grammars and abstraction to verify families of state-transition systems. The family of state-transition systems is represented by a context-free network grammar. Using the structure of the network grammar our technique constructs a process invariant that simulates all the state-transition systems in the family. A novel idea introduced in this article is the use of regular languages to express state properties. We have implemented our techniques and verified two nontrivial examples.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W1975425602",
    "type": "article"
  },
  {
    "title": "Data flow analysis of communicating finite state machines",
    "doi": "https://doi.org/10.1145/117009.117015",
    "publication_date": "1991-07-01",
    "publication_year": 1991,
    "authors": "Wuxu Peng; S. Puroshothaman",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Data flow analysis of communicating finite state machines Authors: Wuxu Peng Southwest Texas State Univ., San Marcos Southwest Texas State Univ., San MarcosView Profile , S. Puroshothaman The Pennsylvania State Univ, University Park The Pennsylvania State Univ, University ParkView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 3pp 399–442https://doi.org/10.1145/117009.117015Published:01 July 1991Publication History 60citation895DownloadsMetricsTotal Citations60Total Downloads895Last 12 Months34Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W1994564174",
    "type": "article"
  },
  {
    "title": "Improving abstract interpretations by combining domains",
    "doi": "https://doi.org/10.1145/200994.200998",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Michael Codish; Anne Mulkers; Maurice Bruynooghe; María García de la Banda; Manuel V. Hermenegildo",
    "corresponding_authors": "",
    "abstract": "This article considers static analysis based on abstract interpretation of logic programs over combined domains. It is known that analyses over combined domains provide more information potentially than obtained by the independent analyses. However, the construction of a combined analysis often requires redefining the basic operations for the combined domain. A practical approach to maintain precision in combined analyses of logic programs which reuses the individual analyses and does not redefine the basic operations is illustrated. The advantages of the approach are that (1) proofs of correctness for the new domains are not required and (2) implementations can be reused. The approach is demonstrated by showing that a combined sharing analysis—constructed from “old” proposals—compares well with other “new” proposals suggested in recent literature both from the point of view of efficiency and accuracy.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2053608039",
    "type": "article"
  },
  {
    "title": "Symbolic bounds analysis of pointers, array indices, and accessed memory regions",
    "doi": "https://doi.org/10.1145/1057387.1057388",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Radu Rugina; Martin Rinard",
    "corresponding_authors": "",
    "abstract": "This article presents a novel framework for the symbolic bounds analysis of pointers, array indices, and accessed memory regions. Our framework formulates each analysis problem as a system of inequality constraints between symbolic bound polynomials. It then reduces the constraint system to a linear program. The solution to the linear program provides symbolic lower and upper bounds for the values of pointer and array index variables and for the regions of memory that each statement and procedure accesses. This approach eliminates fundamental problems associated with applying standard fixed-point approaches to symbolic analysis problems. Experimental results from our implemented compiler show that the analysis can solve several important problems, including static race detection, automatic parallelization, static detection of array bounds violations, elimination of array bounds checks, and reduction of the number of bits used to store computed values.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2107742417",
    "type": "article"
  },
  {
    "title": "Practical extraction techniques for Java",
    "doi": "https://doi.org/10.1145/586088.586090",
    "publication_date": "2002-11-01",
    "publication_year": 2002,
    "authors": "Frank Tip; Peter F. Sweeney; Chris Laffra; Aldo Eisma; David Streeter",
    "corresponding_authors": "",
    "abstract": "Reducing application size is important for software that is distributed via the internet, in order to keep download times manageable, and in the domain of embedded systems, where applications are often stored in (Read-Only or Flash) memory. This paper explores extraction techniques such as the removal of unreachable methods and redundant fields, inlining of method calls, and transformation of the class hierarchy for reducing application size. We implemented a number of extraction techniques in Jax , an application extractor for Java, and evaluated their effectiveness on a set of large Java applications. We found that, on average, the class file archives for these benchmarks were reduced to 37.5% of their original size. Modeling dynamic language features such as reflection, and extracting software distributions other than complete applications requires additional user input. We present a uniform approach for supplying this input that relies on MEL, a modular specification language. We also discuss a number of issues and challenges associated with the extraction of embedded systems applications.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2162762034",
    "type": "article"
  },
  {
    "title": "On the adequacy of graph rewriting for simulating term rewriting",
    "doi": "https://doi.org/10.1145/177492.177577",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Richard Kennaway; Jan Willem Klop; M. R. Sleep; F. J. de Vries",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on On the adequacy of graph rewriting for simulating term rewriting Authors: J. R. Kennaway Univ. of East Anglia, Norwich, UK Univ. of East Anglia, Norwich, UKView Profile , J. W. Klop CWI, Vrije Univ., Amsterdam, The Netherlands CWI, Vrije Univ., Amsterdam, The NetherlandsView Profile , M. R. Sleep Univ. of East Anglia, Norwich, UK Univ. of East Anglia, Norwich, UKView Profile , F. J. de Vries CWI, Amsterdam, The Netherlands CWI, Amsterdam, The NetherlandsView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 16Issue 301 May 1994pp 493–523https://doi.org/10.1145/177492.177577Published:01 May 1994Publication History 45citation347DownloadsMetricsTotal Citations45Total Downloads347Last 12 Months8Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2006468185",
    "type": "article"
  },
  {
    "title": "Synthesis of concurrent systems with many similar processes",
    "doi": "https://doi.org/10.1145/271510.271519",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Paul C. Attie; E. Allen Emerson",
    "corresponding_authors": "",
    "abstract": "Methods for synthesizing concurrent programs from temporal logic specifications based on the use of a decision procedure for testing temporal satisfiability have been proposed by Emerson and Clarke and by Manna and Wolper. An important advantage of these synthesis methods is that they obviate the need to manually compose a program and manually construct a proof of its correctness. One only has to formulate a precise problem specification; the synthesis method then mechanically constructs a correct solution. A serious drawback of these methods in practice, however, is that they suffer from the state explosion problem. To synthesize a concurrent system consisting of K sequential processes, each having N states in its local transition diagram, requires construction of the global product-machine having about N K global states in general. This exponential growth in K makes it infeasible to synthesize systems composed of more than 2 or 3 processes. In this article, we show how to synthesize concurrent systems consisting of many (i.e., a finite but arbitrarily large number K of) similar sequential processes. Our approach avoids construction of the global product-machine for K processes; instead, it constructs a two-process product-machine for a single pair of generic sequential processes. The method is uniform in K , providing a simple template that can be instantiated for each process to yield a solution for any fixed K . The method is also illustrated on synchronization problems from the literature.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2012351729",
    "type": "article"
  },
  {
    "title": "Automated data-member layout of heap objects to improve memory-hierarchy performance",
    "doi": "https://doi.org/10.1145/353926.353937",
    "publication_date": "2000-05-01",
    "publication_year": 2000,
    "authors": "Thomas Kistler; Michael Franz",
    "corresponding_authors": "",
    "abstract": "We present and evaluate a simple, yet efficient optimization technique that improves memory-hierarchy performance for pointer-centric applications by up to 24% and reduces cache misses by up to 35%. This is achieved by selecting an improved ordering for the data members of pointer-based data structures. Our optimization is applicable to all type-safe programming languages that completely abstract from physical storage layout; examples of such languages are Java and Oberon. Our technique does not involve programmers in the optimization process, but runs fully automatically, guided by dynamic profiling information that captures which paths through the program are taken with that frequencey. The algorithm first strives to cluster data members that are accessed closely after one another onto the same cache line, increasing spatial locality. Then, the data members that have been mapped to a particular cache line are ordered to minimize load latency in case of a cache miss.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2033303189",
    "type": "article"
  },
  {
    "title": "Compositional specification and verification of distributed systems",
    "doi": "https://doi.org/10.1145/174662.174665",
    "publication_date": "1994-03-01",
    "publication_year": 1994,
    "authors": "Bengt Jönsson",
    "corresponding_authors": "Bengt Jönsson",
    "abstract": "We present a method for specification and verification of distributed systems that communicate via asynchronous message passing. The method handles both safety and liveness properties. It is compositional, i.e., a specification of a composite system can be obtained from specifications of its components. Specifications are given as labeled transition systems with fairness properties, using a program-like notation with guarded multiple assignments. Compositionality is attained by partitioning the labels of a transition system into input events, which intuitively denote message receptions, and output events, which intuitively denote message transmissions. A specification denotes a set of allowed sequences of message transmissions and receptions, in analogy with the way finite automata are used as acceptors of finite strings. A lower-level specification implements a higher-level one. We present a verification technique which reduces the problem of verifying the correctness of an implementation to classical verification conditions. Safety properties are verified by establishing a simulation relation between transition systems. Liveness properties are verified using methods for proving termination under fairness assumptions. Since specifications can be given at various levels of abstraction, the method is suitable in a development process where a detailed implementation is developed from an abstract specification through a sequence of refinement steps. As an application of the method, an algorithm by Thomas for updating a distributed database is specified and verified.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2039019493",
    "type": "article"
  },
  {
    "title": "Interprocedural optimization",
    "doi": "https://doi.org/10.1145/169683.169678",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Michael Burke; Linda Torczon",
    "corresponding_authors": "",
    "abstract": "article Free AccessInterprocedural optimization: eliminating unnecessary recompilation Authors: Michael Burke IBM Research IBM ResearchView Profile , Linda Torczon Rice University Rice UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 3pp 367–399https://doi.org/10.1145/169683.169678Published:01 July 1993Publication History 50citation633DownloadsMetricsTotal Citations50Total Downloads633Last 12 Months87Last 6 weeks36 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2064311741",
    "type": "article"
  },
  {
    "title": "Lightweight closure conversion",
    "doi": "https://doi.org/10.1145/239912.239915",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Paul Steckler; Mitchell Wand",
    "corresponding_authors": "",
    "abstract": "We consider the problem of lightweight closure conversion, in which multiple procedure call protocols may coexist in the same code. A lightweight closure omits bindings for some of the free variables of the procedure that is represents. Flow analysis is used to match the protocol expected by each procedure and the protocol used at its possible call sites. We formulate the flow analysis as a deductive system that generates a labeled transition system and a set of constraints. We show that any solution to the constraints justifies the resulting transformation. Some of the techniques used are similar to those of abstract interpretation, but others appear to be novel.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W1985450085",
    "type": "article"
  },
  {
    "title": "Interprocedural parallelization analysis in SUIF",
    "doi": "https://doi.org/10.1145/1075382.1075385",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Mary Hall; Saman Amarasinghe; Brian R. Murphy; Shih-Wei Liao; Monica S. Lam",
    "corresponding_authors": "",
    "abstract": "As shared-memory multiprocessor systems become widely available, there is an increasing need for tools to simplify the task of developing parallel programs. This paper describes one such tool, the automatic parallelization system in the Stanford SUIF compiler. This article represents a culmination of a several-year research effort aimed at making parallelizing compilers significantly more effective. We have developed a system that performs full interprocedural parallelization analyses, including array privatization analysis, array reduction recognition, and a suite of scalar data-flow analyses including symbolic analysis. These analyses collaborate in an integrated fashion to exploit coarse-grain parallel loops, computationally intensive loops that can execute on multiple processors independently with no cross-processor synchronization or communication. The system has successfully parallelized large interprocedural loops over a thousand lines of code completely automatically from sequential applications.This article provides a comprehensive description of the analyses in the SUIF system. We also present extensive empirical results on four benchmark suites, showing the contribution of individual analysis techniques both in executing more of the computation in parallel, and in increasing the granularity of the parallel computations. These results demonstrate the importance of interprocedural array data-flow analysis, array privatization and array reduction recognition; a third of the programs spend more than 50% of their execution time in computations that are parallelized with these techniques. Overall, these results indicate that automatic parallelization can be effective on sequential scientific computations, but only if the compiler incorporates all of these analyses.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W1984303163",
    "type": "article"
  },
  {
    "title": "A systematic approach to static access control",
    "doi": "https://doi.org/10.1145/1057387.1057392",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "François Pottier; Christian Skalka; Scott F. Smith",
    "corresponding_authors": "",
    "abstract": "The Java Security Architecture includes a dynamic mechanism for enforcing access control checks, the so-called stack inspection process. While the architecture has several appealing features, access control checks are all implemented via dynamic method calls. This is a highly nondeclarative form of specification that is hard to read, and that leads to additional run-time overhead. This article develops type systems that can statically guarantee the success of these checks. Our systems allow security properties of programs to be clearly expressed within the types themselves, which thus serve as static declarations of the security policy. We develop these systems using a systematic methodology: we show that the security-passing style translation, proposed by Wallach et al. [2000] as a dynamic implementation technique, also gives rise to static security-aware type systems, by composition with conventional type systems. To define the latter, we use the general HM( X ) framework, and easily construct several constraint- and unification-based type systems.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2002829132",
    "type": "article"
  },
  {
    "title": "Automatic tiling of iterative stencil loops",
    "doi": "https://doi.org/10.1145/1034774.1034777",
    "publication_date": "2004-11-01",
    "publication_year": 2004,
    "authors": "Zhiyuan Li; Yonghong Song",
    "corresponding_authors": "",
    "abstract": "Iterative stencil loops are used in scientific programs to implement relaxation methods for numerical simulation and signal processing. Such loops iteratively modify the same array elements over different time steps, which presents opportunities for the compiler to improve the temporal data locality through loop tiling. This article presents a compiler framework for automatic tiling of iterative stencil loops, with the objective of improving the cache performance. The article first presents a technique which allows loop tiling to satisfy data dependences in spite of the difficulty created by imperfectly nested inner loops. It does so by skewing the inner loops over the time steps and by applying a uniform skew factor to all loops at the same nesting level. Based on a memory cost analysis, the article shows that the skew factor must be minimized at every loop level in order to minimize cache misses. A graph-theoretical algorithm, which takes polynomial time, is presented to determine the minimum skew factor. Furthermore, the memory-cost analysis derives the tile size which minimizes capacity misses. Given the tile size, an efficient and general &lt;i&gt;array-padding&lt;/i&gt; scheme is applied to remove conflict misses. Experiments were conducted on 16 test programs and preliminary results showed an average speedup of 1.58 and a maximum speedup of 5.06 across those test programs.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W1967264375",
    "type": "article"
  },
  {
    "title": "Global analysis of constraint logic programs",
    "doi": "https://doi.org/10.1145/232706.232734",
    "publication_date": "1996-09-01",
    "publication_year": 1996,
    "authors": "María García de la Banda; Manuel V. Hermenegildo; Maurice Bruynooghe; V. Dumortier; Gerda Janssens; Wim Simoens",
    "corresponding_authors": "",
    "abstract": "This article presents and illustrates a practical approach to the dataflow analysis of constraint logic programming languages using abstract interpretation. It is first argued that, from the framework point of view, it suffices to propose relatively simple extensions of traditional analysis methods which have already been proved useful and practical and for which efficient fixpoint algorithms exist. This is shown by proposing a simple extension of Bruynooghe's traditional framework which allows it to analyze constraint logic programs. Then, and using this generalized framework, two abstract domains and their required abstract functions are presented: the first abstract domain approximates definiteness information and the second one freeness. Finally, an approach for combining those domains is proposed. The two domains and their combination have been implemented and used in the analysis of CLP( R ) and Prolog-III applications. Results form this implementation showing its performance and accuracy are also presented.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2033130678",
    "type": "article"
  },
  {
    "title": "An ad hoc approach to the implementation of polymorphism",
    "doi": "https://doi.org/10.1145/117009.117017",
    "publication_date": "1991-07-01",
    "publication_year": 1991,
    "authors": "Ron Morrison; Alan Dearle; Richard Connor; A. L. Brown",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on An ad hoc approach to the implementation of polymorphism Authors: R. Morrison Univ. of St. Andrews, Fife, Scotland, UK Univ. of St. Andrews, Fife, Scotland, UKView Profile , A. Dearle Univ. of St. Andrews, Fife, Scotland, UK Univ. of St. Andrews, Fife, Scotland, UKView Profile , R. C. H. Connor Univ. of St. Andrews, Fife, Scotland, UK Univ. of St. Andrews, Fife, Scotland, UKView Profile , A. L. Brown Univ. of St. Andrews, Fife, Scotland, UK Univ. of St. Andrews, Fife, Scotland, UKView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 3July 1991 pp 342–371https://doi.org/10.1145/117009.117017Published:01 July 1991Publication History 45citation656DownloadsMetricsTotal Citations45Total Downloads656Last 12 Months46Last 6 weeks12 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2048132113",
    "type": "article"
  },
  {
    "title": "Synthesis of fault-tolerant concurrent programs",
    "doi": "https://doi.org/10.1145/963778.963782",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Paul C. Attie; Anish Arora; E. Allen Emerson",
    "corresponding_authors": "",
    "abstract": "Methods for mechanically synthesizing concurrent programs from temporal logic specifications obviate the need to manually construct a program and compose a proof of its correctness. A serious drawback of extant synthesis methods, however, is that they produce concurrent programs for models of computation that are often unrealistic. In particular, these methods assume completely fault-free operation, that is, the programs they produce are fault-intolerant. In this paper, we show how to mechanically synthesize fault-tolerant concurrent programs for various fault classes. We illustrate our method by synthesizing fault-tolerant solutions to the mutual exclusion and barrier synchronization problems.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2039065451",
    "type": "article"
  },
  {
    "title": "Scheduling expressions on a pipelined processor with a maximal delay of one cycle",
    "doi": "https://doi.org/10.1145/59287.59291",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "David Bernstein; Izidor Gertner",
    "corresponding_authors": "",
    "abstract": "Consider a pipelined machine that can issue instructions every machine cycle. Sometimes, an instruction that uses the result of the instruction preceding it in a pipe must be delayed to ensure that a program computes a right value. We assume that issuing of such instructions is delayed by at most one machine cycle. For such a machine model, given an unbounded number of machine registers and memory locations, an algorithm to find a shortest schedule of the given expression is presented and analyzed. The proposed algorithm is a modification of Coffman-Graham's algorithm [7], which provides an optimal solution to the problem of scheduling tasks on two parallel processors.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2044304745",
    "type": "article"
  },
  {
    "title": "The multiway rendezvous",
    "doi": "https://doi.org/10.1145/24039.24050",
    "publication_date": "1987-07-01",
    "publication_year": 1987,
    "authors": "Arthur Charlesworth",
    "corresponding_authors": "Arthur Charlesworth",
    "abstract": "The multiway rendezvous is a natural generalization of the rendezvous in which more than two processes may participate. The utility of the multiway rendezvous is illustrated by solutions to a variety of problems. To make their simplicity apparent, these solutions are written using a construct tailor-made to support the multiway rendezvous. The degree of support for multiway rendezvous applications by several well-known languages that support the two-way rendezvous is examined. Since such support for the multiway rendezvous is found to be inadequate, well-integrated extensions to these languages are considered that would help provide such support.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2046042302",
    "type": "article"
  },
  {
    "title": "On the algebraic definition of programming languages",
    "doi": "https://doi.org/10.1145/9758.10501",
    "publication_date": "1987-01-01",
    "publication_year": 1987,
    "authors": "Manfred Broy; Martin Wirsing; Peter Pepper",
    "corresponding_authors": "",
    "abstract": "The algebraic specification of the semantics of programming languages is outlined. Particular emphasis is given to the problem of specifying least-fixed points by first-order conditional equations. To cover this issue, the theory of specifying partial heterogeneous algebras by abstract data types is slightly extended by a more general notion of homomorphism. In this framework the semantics of programming languages can be uniquely specified in a purely algebraic way, using particular models of a hierarchy of abstract types. This approach is demonstrated for a simple procedural programming language. Several increasingly complex versions of iterations are treated and analyzed with respect to their theoretical consequences. Finally, as a complementary algebraic technique, transformational semantics is explained and applied to our examples.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W1963842623",
    "type": "article"
  },
  {
    "title": "Data types are values",
    "doi": "https://doi.org/10.1145/3916.3987",
    "publication_date": "1985-07-01",
    "publication_year": 1985,
    "authors": "James Donahue; Alan Demers",
    "corresponding_authors": "",
    "abstract": "An important goal of programming language research is to isolate the fundamenal concepts of languages, those basic ideas that allow us to understand the relationships among various language features. This paper examines one of these underlying notions, that of data type , with particular attention to the treatment of generic or polymorphic procedures and static type-checking.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2084792162",
    "type": "article"
  },
  {
    "title": "On the productivity of recursive list definitions",
    "doi": "https://doi.org/10.1145/69558.69563",
    "publication_date": "1989-10-01",
    "publication_year": 1989,
    "authors": "Ben A. Sijtsma",
    "corresponding_authors": "Ben A. Sijtsma",
    "abstract": "Several related notions of the productivity are presented for functional languages with lazy evaluation. The notion of productivity captures the idea of computability, of progress of infinite-list programs. If an infinite-list program is productive , then every element of the list can be computed in finite “time.” These notions are used to study recursive list definitions, that is, lists defined by l where l = fl . Sufficient conditions are given in terms of the function f that either guarantee the productivity of the list or its unproductivity. Furthermore, a calculus is developed that can be used in verifying that lists defined by l where l = f I are productive. The power and the usefulness of our theory are demonstrated by several nontrivial examples. Several observations are given in conclusion.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2107474854",
    "type": "article"
  },
  {
    "title": "Analysis of functional programs to detect run-time garbage cells",
    "doi": "https://doi.org/10.1145/48022.48025",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "Katsuro Inoue; Hiroyuki Seki; Hikaru Yagi",
    "corresponding_authors": "",
    "abstract": "We propose a method for detecting the generation of garbage cells by analyzing a source text written in a functional programming language which uses ordinary linked lists to implement list-type values. For a subexpression such as F ( G ( . . . )) in a program where the function values of F and G are of list type, if a cell c is created during the computation of G and if c does not appear in a list-type value of F , then c becomes a garbage cell at the end of the computation of F . We discuss this problem on the basis of formal languages derived from the functional program text and show some sufficient conditions that predict the generation of garbage cells. Also, we give an efficient algorithm to detect at compile time the generation of garbage cells which are linearly linked. We have implemented these algorithms in an experimental LISP system. By executing several sample programs on the system, we conclude that our method is effective in detecting the generation of garbage cells.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2047793578",
    "type": "article"
  },
  {
    "title": "Link-time binary rewriting techniques for program compaction",
    "doi": "https://doi.org/10.1145/1086642.1086645",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Bjorn De Sutter; Bruno De Bus; Koen De Bosschere",
    "corresponding_authors": "",
    "abstract": "Small program size is an important requirement for embedded systems with limited amounts of memory. We describe how link-time compaction through binary rewriting can achieve code size reductions of up to 62% for statically bound languages such as C, C++, and Fortran, without compromising on performance. We demonstrate how the limited amount of information about a program at link time can be exploited to overcome overhead resulting from separate compilation. This is done with scalable, cost-effective, whole-program analyses, optimizations, and duplicate code and data elimination techniques. The discussed techniques are evaluated and their cost-effectiveness is quantified with Squeeze++, a prototype link-time compactor.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2056608406",
    "type": "article"
  },
  {
    "title": "Fairness in parallel programs: the transformational approach",
    "doi": "https://doi.org/10.1145/44501.44504",
    "publication_date": "1988-07-01",
    "publication_year": 1988,
    "authors": "Ernst-Rüdiger Olderog; Krzysztof R. Apt",
    "corresponding_authors": "",
    "abstract": "Program transformations are proposed as a means of providing fair parallelism semantics for parallel programs with shared variables. The transformations are developed in two steps. First, abstract schedulers that implement the various fairness policies are introduced. These schedulers use random assignments z := ? to represent the unbounded nondeterminism induced by fairness. Concrete schedulers are derived by suitably refining the ?. The transformations are then obtained by embedding the abstract schedulers into the parallel programs. This embedding is proved correct on the basis of a simple transition semantics. Since the parallel structure of the original program is preserved, the transformations also provide a basis for syntax-directed proofs of total correctness under the fairness assumption. These proofs make use of infinite ordinals.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2000589007",
    "type": "article"
  },
  {
    "title": "Comparison of Compacting Algorithms for Garbage Collection",
    "doi": "https://doi.org/10.1145/69575.357226",
    "publication_date": "1983-10-01",
    "publication_year": 1983,
    "authors": "Jacques Cohen; Alexandru Nicolau",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Comparison of Compacting Algorithms for Garbage Collection Authors: Jacques Cohen Computer Science Program, Ford Hall, Brandeis University, Waltham, MA Computer Science Program, Ford Hall, Brandeis University, Waltham, MAView Profile , Alexandru Nicolau Computer Science Department, Yale University, New Haven, CT Computer Science Department, Yale University, New Haven, CTView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 4Oct. 1983 pp 532–553https://doi.org/10.1145/69575.357226Published:01 October 1983Publication History 52citation899DownloadsMetricsTotal Citations52Total Downloads899Last 12 Months28Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2061248378",
    "type": "article"
  },
  {
    "title": "Parameterized Specifications: Parameter Passing and Implementation with Respect to Observability",
    "doi": "https://doi.org/10.1145/2166.357212",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Harald Ganzinger",
    "corresponding_authors": "Harald Ganzinger",
    "abstract": "article Free Access Share on Parameterized Specifications: Parameter Passing and Implementation with Respect to Observability Author: Harald Ganzinger Institut für Informatik, Technische Universität München, Postfach 202420, D-8000 München 2, W. Germany Institut für Informatik, Technische Universität München, Postfach 202420, D-8000 München 2, W. GermanyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3pp 318–354https://doi.org/10.1145/2166.357212Published:01 July 1983Publication History 54citation375DownloadsMetricsTotal Citations54Total Downloads375Last 12 Months47Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W1969438769",
    "type": "article"
  },
  {
    "title": "Optimization of parser tables for portable compilers",
    "doi": "https://doi.org/10.1145/1780.1802",
    "publication_date": "1984-10-01",
    "publication_year": 1984,
    "authors": "Peter Dencker; Karl Dürre; Johannes Heuft",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Optimization of parser tables for portable compilers Authors: Peter Dencker Univ. Karlsruhe, Karlsruhe, West Germany Univ. Karlsruhe, Karlsruhe, West GermanyView Profile , Karl Dürre Univ. Karlsruhe, Karlsruhe, West Germany Univ. Karlsruhe, Karlsruhe, West GermanyView Profile , Johannes Heuft Univ. Karlsruhe, Karlsruhe, West Germany Univ. Karlsruhe, Karlsruhe, West GermanyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 4Oct. 1984 pp 546–572https://doi.org/10.1145/1780.1802Published:01 October 1984Publication History 50citation868DownloadsMetricsTotal Citations50Total Downloads868Last 12 Months72Last 6 weeks20 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W1989482425",
    "type": "article"
  },
  {
    "title": "Experience with the SETL Optimizer",
    "doi": "https://doi.org/10.1145/357195.357197",
    "publication_date": "1983-01-01",
    "publication_year": 1983,
    "authors": "Stefan M. Freudenberger; Jacob T. Schwartz; Micha Sharir",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Experience with the SETL Optimizer Authors: Stefan M. Freudenberger Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NY Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NYView Profile , Jacob T. Schwartz Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NY Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NYView Profile , Micha Sharir Department of Mathematics, Tel-Aviv University, Ramat-Aviv, Tel Aviv 69978, Israel Department of Mathematics, Tel-Aviv University, Ramat-Aviv, Tel Aviv 69978, IsraelView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 1Jan. 1983 pp 26–45https://doi.org/10.1145/357195.357197Published:01 January 1983Publication History 43citation480DownloadsMetricsTotal Citations43Total Downloads480Last 12 Months16Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2066575220",
    "type": "article"
  },
  {
    "title": "Generating object lifetime traces with Merlin",
    "doi": "https://doi.org/10.1145/1133651.1133654",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Matthew Hertz; Stephen M. Blackburn; J. Eliot B. Moss; Kathryn S. McKinley; Darko Stefanović",
    "corresponding_authors": "",
    "abstract": "Programmers are writing a rapidly growing number of programs in object-oriented languages, such as Java and C#, that require garbage collection. Garbage collection traces and simulation speed up research by enabling deeper understandings of object lifetime behavior and quick exploration and design of new garbage collection algorithms. When generating perfect traces, the brute-force method of computing object lifetimes requires a whole-heap garbage collection at every potential collection point in the program. Because this process is prohibitively expensive, researchers often use granulated traces by collecting only periodically, for example, every 32 KB of allocation.We extend the state of the art for simulating garbage collection algorithms in two ways. First, we develop a systematic methodology for simulation studies of copying garbage collection and present results showing the effects of trace granularity on these simulations. We show that trace granularity often distorts simulated garbage collection results compared with perfect traces. Second, we present and measure the performance of a new algorithm called Merlin for computing object lifetimes. Merlin timestamps objects and later uses the timestamps of dead objects to reconstruct when they died. The Merlin algorithm piggybacks on garbage collections performed by the base system. Experimental results show that Merlin can generate traces over two orders of magnitude faster than the brute-force method which collects after every object allocation. We also use Merlin to produce visualizations of heap behavior that expose new object lifetime behaviors.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2165497190",
    "type": "article"
  },
  {
    "title": "Variant parametric types",
    "doi": "https://doi.org/10.1145/1152649.1152650",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Atsushi Igarashi; Mirko Viroli",
    "corresponding_authors": "",
    "abstract": "We develop the mechanism of variant parametric types as a means to enhance synergy between parametric and inclusion polymorphism in object-oriented programming languages. Variant parametric types are used to control both the subtyping between different instantiations of one generic class and the accessibility of their fields and methods. On one hand, one parametric class can be used to derive covariant types, contravariant types, and bivariant types (generally called variant parametric types) by attaching a variance annotation to a type argument. On the other hand, the type system prohibits certain method/field accesses, according to variance annotations, when these accesses may otherwise make the program unsafe. By exploiting variant parametric types, a programmer can write generic code abstractions that work on a wide range of parametric types in a safe manner. For instance, a method that only reads the elements of a container of numbers can be easily modified so as to accept containers of integers, floating-point numbers, or any subtype of the number type.Technical subtleties in typing for the proposed mechanism are addressed in terms of an intuitive correspondence between variant parametric and bounded existential types. Then, for a rigorous argument of correctness of the proposed typing rules, we extend Featherweight GJ---an existing formal core calculus for Java with generics---with variant parametric types and prove type soundness.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W1993140586",
    "type": "article"
  },
  {
    "title": "Parsing and compiling using Prolog",
    "doi": "https://doi.org/10.1145/22719.22946",
    "publication_date": "1987-03-20",
    "publication_year": 1987,
    "authors": "Jacques Cohen; Timothy J. Hickey",
    "corresponding_authors": "",
    "abstract": "This paper presents the material needed for exposing the reader to the advantages of using Prolog as a language for describing succinctly most of the algorithms needed in prototyping and implementing compilers or producing tools that facilitate this task. The available published material on the subject describes one particular approach in implementing compilers using Prolog. It consists of coupling actions to recursive descent parsers to produce syntax-trees which are subsequently utilized in guiding the generation of assembly language code. Although this remains a worthwhile approach, there is a host of possibilities for Prolog usage in compiler construction. The primary aim of this paper is to demonstrate the use of Prolog in parsing and compiling. A second, but equally important, goal of this paper is to show that Prolog is a labor-saving tool in prototyping and implementing many non-numerical algorithms which arise in compiling, and whose description using Prolog is not available in the literature. The paper discusses the use of unification and nondeterminism in compiler writing as well as means to bypass these (costly) features when they are deemed unnecessary. Topics covered include bottom-up and top-down parsers, syntax-directed translation, grammar properties, parser generation, code generation, and optimizations. Newly proposed features that are useful in compiler construction are also discussed. A knowledge of Prolog is assumed.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2037452894",
    "type": "article"
  },
  {
    "title": "Translation of attribute grammars into procedures",
    "doi": "https://doi.org/10.1145/579.586",
    "publication_date": "1984-07-01",
    "publication_year": 1984,
    "authors": "Takuya Katayama",
    "corresponding_authors": "Takuya Katayama",
    "abstract": "article Open Access Share on Translation of attribute grammars into procedures Author: Takuya Katayama Tokyo Institute of Technology, Tokyo, Japan Tokyo Institute of Technology, Tokyo, JapanView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 3pp 345–369https://doi.org/10.1145/579.586Published:01 July 1984Publication History 84citation592DownloadsMetricsTotal Citations84Total Downloads592Last 12 Months80Last 6 weeks15 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W1980217707",
    "type": "article"
  },
  {
    "title": "Program transformations in a denotational setting",
    "doi": "https://doi.org/10.1145/3916.3917",
    "publication_date": "1985-07-01",
    "publication_year": 1985,
    "authors": "Flemming Nielson",
    "corresponding_authors": "Flemming Nielson",
    "abstract": "Program transformations are frequently performed by optimizing compilers, and the correctness of applying them usually depends on data flow information. For language-to-same-language transformations, it is shown how a denotational setting can be useful for validating such program transformations. Strong equivalence is obtained for transformations that exploit information from a class of forward data flow analyses, whereas only weak equivalence is obtained for transformations that exploit information from a class of backward data flow analyses. To obtain strong equivalence, both the original and the transformed program must be data flow analysed, but consideration of a transformation-exploiting liveness of variables indicates that a more satisfactory approach may be possible.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2032277118",
    "type": "article"
  },
  {
    "title": "Dynamic graph-based software fingerprinting",
    "doi": "https://doi.org/10.1145/1286821.1286826",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Christian Collberg; Clark Thomborson; Gregg M. Townsend",
    "corresponding_authors": "",
    "abstract": "Fingerprinting embeds a secret message into a cover message. In media fingerprinting, the secret is usually a copyright notice and the cover a digital image. Fingerprinting an object discourages intellectual property theft, or when such theft has occurred, allows us to prove ownership. The Software Fingerprinting problem can be described as follows. Embed a structure W into a program P such that: W can be reliably located and extracted from P even after P has been subjected to code transformations such as translation, optimization and obfuscation; W is stealthy; W has a high data rate; embedding W into P does not adversely affect the performance of P ; and W has a mathematical property that allows us to argue that its presence in P is the result of deliberate actions. In this article, we describe a software fingerprinting technique in which a dynamic graph fingerprint is stored in the execution state of a program. Because of the hardness of pointer alias analysis such fingerprints are difficult to attack automatically.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2102748361",
    "type": "article"
  },
  {
    "title": "FeatherTrait",
    "doi": "https://doi.org/10.1145/1330017.1330022",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Luigi Liquori; Arnaud Spiwack",
    "corresponding_authors": "",
    "abstract": "In the context of statically typed, class-based languages , we investigate classes that can be extended with trait composition. A trait is a collection of methods without state; it can be viewed as an incomplete stateless class . Traits can be composed in any order, but only make sense when imported by a class that provides state variables and additional methods to disambiguate conflicting names arising between the imported traits. We introduce FeatherTrait Java (FTJ), a conservative extension of the simple lightweight class-based calculus Featherweight Java (FJ) with statically typed traits . In FTJ, classes can be built using traits as basic behavioral bricks; method conflicts between imported traits must be resolved explicitly by the user either by (i) aliasing or excluding method names in traits, or by (ii) overriding explicitly the conflicting methods in the class or in the trait itself. We present an operational semantics with a lookup algorithm, and a sound type system that guarantees that evaluating a well-typed expression never yields a message-not-understood run-time error nor gets the interpreter stuck. We give examples of the increased expressive power of the trait-based inheritance model. The resulting calculus appears to be a good starting point for a rigorous mathematical analysis of typed class-based languages featuring trait-based inheritance.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2114980545",
    "type": "article"
  },
  {
    "title": "The embedded machine",
    "doi": "https://doi.org/10.1145/1286821.1286824",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Thomas A. Henzinger; Christoph Kirsch",
    "corresponding_authors": "",
    "abstract": "The Embedded Machine is a virtual machine that mediates in real time the interaction between software processes and physical processes. It separates the compilation of embedded programs into two phases. The first phase, the platform-independent compiler phase, generates E code (code executed by the Embedded Machine), which supervises the timing, not the scheduling of, application tasks relative to external events such as clock ticks and sensor interrupts. E code is portable and, given an input behavior, exhibits predictable (i.e., deterministic) timing and output behavior. The second phase, the platform-dependent compiler phase, checks the time safety of the E code, that is, whether platform performance (determined by the hardware) and platform utilization (determined by the scheduler of the operating system) enable its timely execution. We have used the Embedded Machine to compile and execute high-performance control applications written in Giotto, such as the flight control system of an autonomous model helicopter.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2140274690",
    "type": "article"
  },
  {
    "title": "Eliminating Redundant Recursive Calls.",
    "doi": "https://doi.org/10.1145/2166.2167",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Norman Cohen",
    "corresponding_authors": "Norman Cohen",
    "abstract": "article Free Access Share on Eliminating Redundant Recursive Calls. Author: Norman H. Cohen Harvard University Harvard UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3pp 265–299https://doi.org/10.1145/2166.2167Published:01 July 1983Publication History 50citation593DownloadsMetricsTotal Citations50Total Downloads593Last 12 Months91Last 6 weeks18 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2023647809",
    "type": "article"
  },
  {
    "title": "The computational power and complexity of constraint handling rules",
    "doi": "https://doi.org/10.1145/1462166.1462169",
    "publication_date": "2009-02-01",
    "publication_year": 2009,
    "authors": "Jon Sneyers; Tom Schrijvers; Bart Demoen",
    "corresponding_authors": "",
    "abstract": "Constraint Handling Rules (CHR) is a high-level rule-based programming language which is increasingly used for general-purpose programming. We introduce the CHR machine, a model of computation based on the operational semantics of CHR. Its computational power and time complexity properties are compared to those of the well-understood Turing machine and Random Access Memory machine. This allows us to prove the interesting result that every algorithm can be implemented in CHR with the best known time and space complexity. We also investigate the practical relevance of this result and the constant factors involved. Finally we expand the scope of the discussion to other (declarative) programming languages.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2144960539",
    "type": "article"
  },
  {
    "title": "Managing Reentrant Structures Using Reference Counts",
    "doi": "https://doi.org/10.1145/357103.357104",
    "publication_date": "1980-07-01",
    "publication_year": 1980,
    "authors": "Daniel G. Bobrow",
    "corresponding_authors": "Daniel G. Bobrow",
    "abstract": "Automatic storage management requires that one identify storage unreachable by a user's program and return it to free status. One technique maintains a count of the references from user's programs to each cell, since a count of zero implies the storage is unreachable. Reentrant structures are self-referencing; hence no cell in them will have a count of zero, even though the entire structure is unreachable. A modification of standard reference counting can be used to manaage the deallocation of a large class of frequently used reentrant structures, including two-way and circularly linked lists. All the cells of a potentially reentrant structure are considered as part of a single group for deallocation purposes. Information associated with each cell specifies its group membership. Internal references (pointers from one cell of the group to another) are not reference counted. External references to any cell of this group are counted as references to the group as a whole. When the external reference count goes to zero, all the cells of the group can be deallocated. This paper describes several ways of specifying group membership, properties of each implementation, and properties of mutable and immutable group membership.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2046866979",
    "type": "article"
  },
  {
    "title": "Affix grammar driven code generation",
    "doi": "https://doi.org/10.1145/4472.4486",
    "publication_date": "1985-10-01",
    "publication_year": 1985,
    "authors": "Mahadevan Ganapathi; Charles N. Fischer",
    "corresponding_authors": "",
    "abstract": "Affix grammars are used to describe the instruction set of a target architecture for purposes of compiler code generation. A code generator is obtained automatically for a compiler using attributed parsing techniques. A compiler built on this model can automatically perform most popular machine-dependent optimizations, including peephole optimizations. Code generators based on this model demonstrate retargetability for the VAX 1 -11, iAPX 2 -86, Z-8000 3 , PDP 4 -11, MC-68000, NS32032, FOM, and IBM-370 architectures.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W1985608943",
    "type": "article"
  },
  {
    "title": "On the Development of the Algebra of Functional Programs",
    "doi": "https://doi.org/10.1145/69622.357193",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "John H. Williams",
    "corresponding_authors": "John H. Williams",
    "abstract": "article Free AccessOn the Development of the Algebra of Functional Programs Author: John H. Williams K51-282, IBM Research, 5600 Cottle Road, San Jose, CA K51-282, IBM Research, 5600 Cottle Road, San Jose, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 733–757https://doi.org/10.1145/69622.357193Published:01 October 1982Publication History 32citation445DownloadsMetricsTotal Citations32Total Downloads445Last 12 Months35Last 6 weeks16 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2015477722",
    "type": "article"
  },
  {
    "title": "AspectML",
    "doi": "https://doi.org/10.1145/1353445.1353448",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Daniel S. Dantas; David Walker; Geoffrey Washburn; Stephanie Weirich",
    "corresponding_authors": "",
    "abstract": "This article defines AspectML, a typed functional, aspect-oriented programming language. The main contribution of AspectML is the seamless integration of polymorphism, run-time type analysis and aspect-oriented programming language features. In particular, AspectML allows programmers to define type-safe polymorphic advice using pointcuts constructed from a collection of polymorphic join points. AspectML also comes equipped with a type inference algorithm that conservatively extends Hindley--Milner type inference. To support first-class polymorphic point-cut designators, a crucial feature for developing aspect-oriented profiling or logging libraries, the algorithm blends the conventional Hindley--Milner type inference algorithm with a simple form of local type inference. We give our language operational meaning via a type-directed translation into an expressive type-safe intermediate language. Many complexities of the source language are eliminated in this translation, leading to a modular specification of its semantics. One of the novelties of the intermediate language is the definition of polymorphic labels for marking control-flow points. When a set of labels is assembled as a pointcut, the type of each label is an instance of the type of the pointcut.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2068706008",
    "type": "article"
  },
  {
    "title": "Satin",
    "doi": "https://doi.org/10.1145/1709093.1709096",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Rob V. van Nieuwpoort; Gosia Wrzesiñska; Ceriel J. H. Jacobs; Henri E. Bal",
    "corresponding_authors": "",
    "abstract": "Computational grids have an enormous potential to provide compute power. However, this power remains largely unexploited today for most applications, except trivially parallel programs. Developing parallel grid applications simply is too difficult. Grids introduce several problems not encountered before, mainly due to the highly heterogeneous and dynamic computing and networking environment. Furthermore, failures occur frequently, and resources may be claimed by higher-priority jobs at any time. In this article, we solve these problems for an important class of applications: divide-and-conquer. We introduce a system called Satin that simplifies the development of parallel grid applications by providing a rich high-level programming model that completely hides communication. All grid issues are transparently handled in the runtime system, not by the programmer. Satin's programming model is based on Java, features spawn-sync primitives and shared objects, and uses asynchronous exceptions and an abort mechanism to support speculative parallelism. To allow an efficient implementation, Satin consistently exploits the idea that grids are hierarchically structured. Dynamic load-balancing is done with a novel cluster-aware scheduling algorithm that hides the long wide-area latencies by overlapping them with useful local work. Satin's shared object model lets the application define the consistency model it needs. If an application needs only loose consistency, it does not have to pay high performance penalties for wide-area communication and synchronization. We demonstrate how grid problems such as resource changes and failures can be handled transparently and efficiently. Finally, we show that adaptivity is important in grids. Satin can increase performance considerably by adding and removing compute resources automatically, based on the application's requirements and the utilization of the machines and networks in the grid. Using an extensive evaluation on real grids with up to 960 cores, we demonstrate that it is possible to provide a simple high-level programming model for divide-and-conquer applications, while achieving excellent performance on grids. At the same time, we show that the divide-and-conquer model scales better on large systems than the master-worker approach, since it has no single central bottleneck.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2012251549",
    "type": "article"
  },
  {
    "title": "A probabilistic language based on sampling functions",
    "doi": "https://doi.org/10.1145/1452044.1452048",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "Sungwoo Park; Frank Pfenning; Sebastian Thrun",
    "corresponding_authors": "",
    "abstract": "As probabilistic computations play an increasing role in solving various problems, researchers have designed probabilistic languages which treat probability distributions as primitive datatypes. Most probabilistic languages, however, focus only on discrete distributions and have limited expressive power. This article presents a probabilistic language, called λ ○ , whose expressive power is beyond discrete distributions. Rich expressiveness of λ ○ is due to its use of sampling functions , that is, mappings from the unit interval (0.0,1.0] to probability domains, in specifying probability distributions. As such, λ ○ enables programmers to formally express and reason about sampling methods developed in simulation theory. The use of λ ○ is demonstrated with three applications in robotics: robot localization, people tracking, and robotic mapping. All experiments have been carried out with real robots.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2110839939",
    "type": "article"
  },
  {
    "title": "On contract satisfaction in a higher-order world",
    "doi": "https://doi.org/10.1145/2039346.2039348",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "Christos Dimoulas; Matthias Felleisen",
    "corresponding_authors": "",
    "abstract": "Behavioral software contracts have become a popular mechanism for specifying and ensuring logical claims about a program's flow of values. While contracts for first-order functions come with a natural interpretation and are well understood, the various incarnations of higher-order contracts adopt, implicitly or explicitly, different views concerning the meaning of contract satisfaction. In this article, we define various notions of contract satisfaction in terms of observational equivalence and compare them with each other and notions in the literature. Specifically, we introduce a small model language with higher-order contracts and use it to formalize different notions of contract satisfaction. Each of them demands that the contract parties satisfy certain observational equivalences.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W1998223027",
    "type": "article"
  },
  {
    "title": "Semantics of fractional permissions with nesting",
    "doi": "https://doi.org/10.1145/1749608.1749611",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "John Boyland",
    "corresponding_authors": "John Boyland",
    "abstract": "Permissions specify mutable state that can be accessed by a program. Fractions distinguish write access (1) from read access (any smaller fraction). Nesting can model object invariants and ownership. Fractional permissions provides a foundation the meaning of many of access-based annotations: uniqueness, read-only, immutability, method effects, guarded state, etc. The semantics of fractional permissions with nesting is given in terms of “fractional heaps.” We show that the fraction law Π ≡ 1/2 Π + 1/2 Π permits sound reasoning and that nesting can be carried out safely using only local reasoning.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W1974806232",
    "type": "article"
  },
  {
    "title": "From datalog rules to efficient programs with time and space guarantees",
    "doi": "https://doi.org/10.1145/1552309.1552311",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Yanhong A. Liu; Scott D. Stoller",
    "corresponding_authors": "",
    "abstract": "This article describes a method for transforming any given set of Datalog rules into an efficient specialized implementation with guaranteed worst-case time and space complexities, and for computing the complexities from the rules. The running time is optimal in the sense that only useful combinations of facts that lead to all hypotheses of a rule being simultaneously true are considered, and each such combination is considered exactly once in constant time. The associated space usage may sometimes be reduced using scheduling optimizations to eliminate some summands in the space usage formula. The transformation is based on a general method for algorithm design that exploits fixed-point computation, incremental maintenance of invariants, and combinations of indexed and linked data structures. We apply the method to a number of analysis problems, some with improved algorithm complexities and all with greatly improved algorithm understanding and greatly simplified complexity analysis.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2107489588",
    "type": "article"
  },
  {
    "title": "TSL",
    "doi": "https://doi.org/10.1145/2450136.2450139",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Junghee Lim; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "This article describes the design and implementation of a system, called T SL (for Transformer Specification Language), that provides a systematic solution to the problem of creating retargetable tools for analyzing machine code. T SL is a tool generator---that is, a metatool---that automatically creates different abstract interpreters for machine-code instruction sets. The most challenging technical issue that we faced in designing T SL was how to automate the generation of the set of abstract transformers for a given abstract interpretation of a given instruction set. From a description of the concrete operational semantics of an instruction set, together with the datatypes and operations that define an abstract domain, T SL automatically creates the set of abstract transformers for the instructions of the instruction set. T SL advances the state-of-the-art in program analysis because it provides two dimensions of parameterizability: (i) a given analysis component can be retargeted to different instruction sets; (ii) multiple analysis components can be created automatically from a single specification of the concrete operational semantics of the language to be analyzed. T SL is an abstract-transformer-generator generator . The article describes the principles behind T SL , and discusses how one uses T SL to develop different abstract interpreters.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2170371367",
    "type": "article"
  },
  {
    "title": "TF-Coder: Program Synthesis for Tensor Manipulations",
    "doi": "https://doi.org/10.1145/3517034",
    "publication_date": "2022-03-29",
    "publication_year": 2022,
    "authors": "Kensen Shi; David Bieber; Rishabh Singh",
    "corresponding_authors": "",
    "abstract": "The success and popularity of deep learning is on the rise, partially due to powerful deep learning frameworks such as TensorFlow and PyTorch that make it easier to develop deep learning models. However, these libraries also come with steep learning curves, since programming in these frameworks is quite different from traditional imperative programming with explicit loops and conditionals. In this work, we present a tool called TF-Coder for programming by example in TensorFlow. TF-Coder uses a bottom-up weighted enumerative search, with value-based pruning of equivalent expressions and flexible type- and value-based filtering to ensure that expressions adhere to various requirements imposed by the TensorFlow library. We train models to predict TensorFlow operations from features of the input and output tensors and natural language descriptions of tasks, to prioritize relevant operations during search. TF-Coder solves 63 of 70 real-world tasks within 5 minutes, sometimes finding simpler solutions in less time compared to experienced human programmers.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4220929067",
    "type": "article"
  },
  {
    "title": "Passport: Improving Automated Formal Verification Using Identifiers",
    "doi": "https://doi.org/10.1145/3593374",
    "publication_date": "2023-04-24",
    "publication_year": 2023,
    "authors": "Alex Sanchez-Stern; Emily First; Timothy Zhou; Zhanna Kaufman; Yuriy Brun; Talia Ringer",
    "corresponding_authors": "",
    "abstract": "Formally verifying system properties is one of the most effective ways of improving system quality, but its high manual effort requirements often render it prohibitively expensive. Tools that automate formal verification by learning from proof corpora to synthesize proofs have just begun to show their promise. These tools are effective because of the richness of the data the proof corpora contain. This richness comes from the stylistic conventions followed by communities of proof developers, together with the powerful logical systems beneath proof assistants. However, this richness remains underexploited, with most work thus far focusing on architecture rather than on how to make the most of the proof data. This article systematically explores how to most effectively exploit one aspect of that proof data: identifiers. We develop the Passport approach, a method for enriching the predictive Coq model used by an existing proof-synthesis tool with three new encoding mechanisms for identifiers: category vocabulary indexing, subword sequence modeling, and path elaboration. We evaluate our approach’s enrichment effect on three existing base tools: ASTactic, Tac, and Tok. In head-to-head comparisons, Passport automatically proves 29% more theorems than the best-performing of these base tools. Combining the three tools enhanced by the Passport approach automatically proves 38% more theorems than combining the three base tools. Finally, together, these base tools and their enhanced versions prove 45% more theorems than the combined base tools. Overall, our findings suggest that modeling identifiers can play a significant role in improving proof synthesis, leading to higher-quality software.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4366825838",
    "type": "article"
  },
  {
    "title": "Choral: Object-oriented Choreographic Programming",
    "doi": "https://doi.org/10.1145/3632398",
    "publication_date": "2023-11-22",
    "publication_year": 2023,
    "authors": "Saverio Giallorenzo; Fabrizio Montesi; Marco Peressotti",
    "corresponding_authors": "",
    "abstract": "Choreographies are coordination plans for concurrent and distributed systems, which define the roles of the involved participants and how they are supposed to work together. In the paradigm of choreographic programming, choreographies are programs that can be compiled into executable implementations. In this article, we present Choral, the first choreographic programming language based on mainstream abstractions. The key idea in Choral is a new notion of data type, which allows for expressing that data is distributed over different roles.We use this idea to reconstruct the paradigm of choreographic programming through object-oriented abstractions. Choreographies are classes, and instances of choreographies are objects with states and behaviours implemented collaboratively by roles. Choral comes with a compiler that, given a choreography, generates an implementation for each of its roles. These implementations are libraries in pure Java, whose types are under the control of the Choral programmer. Developers can then modularly compose these libraries in their programs, to participate correctly in choreographies. Choral is the first incarnation of choreographic programming offering such modularity, which finally connects more than a decade of research on the paradigm to practical software development. The integration of choreographic and object-oriented programming yields other powerful advantages, where the features of one paradigm benefit the other in ways that go beyond the sum of the parts. On the one hand, the high-level abstractions and static checks from the world of choreographies can be used to write concurrent and distributed object-oriented software more concisely and correctly. On the other hand, we obtain a much more expressive choreographic language from object-oriented abstractions than in previous work. This expressivity allows for writing more reusable and flexible choreographies. For example, object passing makes Choral the first higher-order choreographic programming language, whereby choreographies can be parameterised over other choreographies without any need for central coordination. We also extend method overloading to a new dimension: specialisation based on data location. Together with subtyping and generics, this allows Choral to elegantly support user-defined communication mechanisms and middleware.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4388903845",
    "type": "article"
  },
  {
    "title": "A Brookes-Style Denotational Semantics for Release/Acquire Concurrency",
    "doi": "https://doi.org/10.1145/3715096",
    "publication_date": "2025-01-30",
    "publication_year": 2025,
    "authors": "Yotam Dvir; Ohad Kammar; Ori Lahav",
    "corresponding_authors": "",
    "abstract": "We present a compositional denotational semantics for a functional language with first-class parallel composition and sharedmemory operations whose operational semantics follows the Release/Acquire weak memory model (RA). The semantics is formulated in Moggi’s monadic approach, and is based on Brookes-style traces. To do so we adapt Brookes’s traces to Kang et al.’s view-based machine for RA, and supplement Brookes’s mumble and stutter closure operations with additional operations, specific to RA. The latter provides a more nuanced understanding of traces that uncouples them from operational interrupted executions. We show that our denotational semantics is adequate and use it to validate various program transformations of interest. This is the first work to put weak memory models on the same footing as many other programming effects in Moggi’s standard monadic approach.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406967674",
    "type": "article"
  },
  {
    "title": "Specification and verification of fault-tolerance, timing, and scheduling",
    "doi": "https://doi.org/10.1145/314602.314605",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Zhiming Liu; Mathai Joseph",
    "corresponding_authors": "",
    "abstract": "Fault-tolerance and timing have often been considered to be implementation issues of a program, quite distinct from the functional safety and liveness properties. Recent work has shown how these non-functional and functional properties can be verified in a similar way. However, the more practical question of determining whether a real-time program will meet its deadlines, i.e., showing that there is a feasible schedule, is usually done using scheduling theory, quite separately from the verification of other properties of the program. This makes it hard to use the results of scheduling analysis in the design, or redesign, of fault-tolerant and real-time programs. This article shows how fault-tolerance, timing, and schedulability can be specified and verified using a single notation and model. This allows a unified view to be taken of the functional and nonfunctional properties of programs and a simple transformational method to be usedto combine these properties. It also permits results from scheduling theory to be interpreted and used within a formal proof framework. The notation and model are illustrated using a simple example.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2063163522",
    "type": "article"
  },
  {
    "title": "Syntactic type abstraction",
    "doi": "https://doi.org/10.1145/371880.371887",
    "publication_date": "2000-11-01",
    "publication_year": 2000,
    "authors": "Dan Grossman; Greg Morrisett; Steve Zdancewic",
    "corresponding_authors": "",
    "abstract": "Software developers often structure programs in such a way that different pieces of code constitute distinct principals . Types help define the protocol by which these principals interact. In particular, abstract types allow a principal to make strong assumptions about how well-typed clients use the facilities that it provides. We show how the notions of principals and type abstraction can be formalized within a language. Different principals can know the implementation of different abstract types. We use additional syntax to track the flow of values with abstract types during the evaluation of a program and demonstrate how this framework supports syntactic proofs (in the sytle of subject reduction) for type-abstraction properties. Such properties have traditionally required semantic arguments; using syntax aboids the need to build a model and recursive typesfor the language. We present various typed lambda calculi with principals, including versions that have mutable state and recursive types.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2060029146",
    "type": "article"
  },
  {
    "title": "A new, simpler linear-time dominators algorithm",
    "doi": "https://doi.org/10.1145/295656.295663",
    "publication_date": "1998-11-01",
    "publication_year": 1998,
    "authors": "Adam L. Buchsbaum; Haim Kaplan; Anne Rogers; Jeffery Westbrook",
    "corresponding_authors": "",
    "abstract": "We present a new linear-time algorithm to find the immediate dominators of all vertices in a flowgraph. Our algorithm is simpler than previous linear-time algorithms: rather than employ complicated data structures, we combine the use of microtrees and memoization with new observations on a restricted class of path compressions. We have implemented our algorithm, and we report experimental results that show that the constant factors are low. Compared to the standard, slightly superlinear algorithm of Lengauer and Tarjan, which has much less overhead, our algorithm runs 10-20% slower on real flowgraphs of reasonable size and only a few percent slower on very large flowgraphs.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2124339980",
    "type": "article"
  },
  {
    "title": "Stack inspection",
    "doi": "https://doi.org/10.1145/641909.641912",
    "publication_date": "2003-05-01",
    "publication_year": 2003,
    "authors": "Cédric Fournet; Andrew D. Gordon",
    "corresponding_authors": "",
    "abstract": "Stack inspection is a security mechanism implemented in runtimes such as the JVM and the CLR to accommodate components with diverse levels of trust. Although stack inspection enables the fine-grained expression of access control policies, it has rather a complex and subtle semantics. We present a formal semantics and an equational theory to explain how stack inspection affects program behavior and code optimisations. We discuss the security properties enforced by stack inspection, and also consider variants with stronger, simpler properties.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2046970568",
    "type": "article"
  },
  {
    "title": "Formally based profiling for higher-order functional languages",
    "doi": "https://doi.org/10.1145/244795.244802",
    "publication_date": "1997-03-03",
    "publication_year": 1997,
    "authors": "Patrick M. Sansom; Simon Jones",
    "corresponding_authors": "",
    "abstract": "We present the first source-level profiler for a compiled, nonstrict, higher-order, purely functional language capable of measuring time as well as space usage. Our profiler is implemented in a production-quality optimizing compiler for Haskell and can successfully profile large applications. A unique feature of our approach is that we give a formal specification of the attribution of execution costs to cost centers. This specification enables us to discuss our design decisions in a precise framework, prove properties about the attribution of costs, and examine to effects of different program transformations on the attribution of costs. Since it is not obvious how to map this specification onto a particular implementation, we also present an implementation-oriented operational semantics, and prove it equivalent to the specification.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2092408876",
    "type": "article"
  },
  {
    "title": "Specification and dialogue control of visual interaction through visual rewriting systems",
    "doi": "https://doi.org/10.1145/330643.330644",
    "publication_date": "1999-11-01",
    "publication_year": 1999,
    "authors": "Paolo Bottoni; Maria Francesca Costabile; P. Mussio",
    "corresponding_authors": "",
    "abstract": "Computers are increasingly being seen not only as computing tools but more so as communication tools, thus placing special emphasis on human-computer interaction (HCI). In this article, the focus is on visual HCI, where the messages exchanged between human and computer are images appearing on the computer screen, as usual in current popular user interfaces. We formalize interactive sessions of a human-computer dialogue as a structured set of legal visual sentences, i.e., as a visual language, and show how rewriting systems can be generalized to specify both the pictorial and the computational aspects of visual languages. To this end, Visual Conditional Attributed Rewriting (VCARW) systems are introduced, and use for specification of visual languages. These specifications are given as inputs to a procedure illustrated in the article as a system of algorithms, which automatically generates control mechanisms of the interaction, thus favoring the design of more reliable and usable systems.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2094853090",
    "type": "article"
  },
  {
    "title": "Proving concurrent constraint programs correct",
    "doi": "https://doi.org/10.1145/265943.265954",
    "publication_date": "1997-09-01",
    "publication_year": 1997,
    "authors": "Frank S. de Boer; Maurizio Gabbrielli; Elena Marchiori; Catuscia Palamidessi",
    "corresponding_authors": "",
    "abstract": "We introduce a simple compositional proof system for proving (partial) correctness of concurrent constraint programs (CCP). The proof system is based on a denotational approximation of the strongest postcondition semantics of CCP programs. The proof system is proved to be correct for full CCP and complete for the class of programs in which the denotational semantics characterizes exactly the strongest postcondition. This class includes the so-called confluent CCP, a special case of which is constraint logic programming with dynamic scheduling.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W1975923086",
    "type": "article"
  },
  {
    "title": "Constraint-based termination analysis of logic programs",
    "doi": "https://doi.org/10.1145/330643.330645",
    "publication_date": "1999-11-01",
    "publication_year": 1999,
    "authors": "Stefaan Decorte; Danny De Schreye; Henk Vandecasteele",
    "corresponding_authors": "",
    "abstract": "Current norm-based automatic termination analysis techniques for logic programs can be split up into different components: inference of mode or type information, derivation of models, generation of well-founded orders, and verification of the termination conditions themselves. Although providing high-precision results, these techniques suffer from an efficiency point of view, as several of these analyses are often performed through abstract interpretation. We present a new termination analysis which integrates the various components and produces a set of constraints that, when solvable, identifies successful termination proofs. The proposed method is both efficient and precise. The use of constraint sets enables the propagation on information over all different phases while the need for multiple analyses is considerably reduced.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2011889853",
    "type": "article"
  },
  {
    "title": "Formal derivation of efficient parallel programs by construction of list homomorphisms",
    "doi": "https://doi.org/10.1145/256167.256201",
    "publication_date": "1997-05-01",
    "publication_year": 1997,
    "authors": "Zhenjiang Hu; Hideya Iwasaki; Masato Takechi",
    "corresponding_authors": "",
    "abstract": "It has been attracting much attention to make use of list homomorphisms in parallel programming because they ideally suit the divide-and-conquer parallel paradigm. However, they have been usually treated rather informally and ad hoc in the development of efficient parallel programs. What is worse is that some interesting functions, e.g., the maximum segment sum problem, are basically not list homomorphisms. In this article, we propose a systematic and formal way for the construction of a list homomorphism for a given problem so that an efficient parallel program is derived. We show, with several well-known but nontrivial problems, how a straightforward, and “obviously” correct, but quite inefficient solution to the problem can be successfully turned into a semantically equivalent “almost list homomorphism.” The derivation is based on two transformations, namely tupling and fusion, which are defined according to the specific recursive structures of list homomorphisms.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W1963634389",
    "type": "article"
  },
  {
    "title": "On the use of regular expressions for searching text",
    "doi": "https://doi.org/10.1145/256167.256174",
    "publication_date": "1997-05-01",
    "publication_year": 1997,
    "authors": "Charles L. A. Clarke; Gordon V. Cormack",
    "corresponding_authors": "",
    "abstract": "The use of regular expressions for text search is widely known and well understood. It is then surprising that the standard techniques and tools prove to be of limited use for searching structured text formatted with SGML or similar markup languages. Our experience with structured text search has caused us to reexamine the current practice. The generally accepted rule of “leftmost longest match” is an unfortunate choice and is at the root of the difficulties. We instead propose a rule which is semantically cleaner. This rule is generally applicable to a variety of text search applications, including source code analysis, and has interesting properties in its own right. We have written a publicly available search tool implementing the theory in the article, which has proved valuable in a variety of circumstances.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2013120826",
    "type": "article"
  },
  {
    "title": "Parallelizing nonnumerical code with selective scheduling and software pipelining",
    "doi": "https://doi.org/10.1145/267959.269966",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "Soo‐Mook Moon; Kemal Ebci̇oğlu",
    "corresponding_authors": "",
    "abstract": "Instruction-level parallelism (ILP) in nonnumerical code is regarded as scarce and hard to exploit due to its irregularity. In this article, we introduce a new code-scheduling technique for irregular ILP called “selective scheduling” which can be used as a component for superscalar and VLIW compilers. Selective scheduling can compute a wide set of independent operations across all execution paths based on renaming and forward-substitution and can compute available operations across loop iterations if combined with software pipelining. This scheduling approach has better heuristics for determining the usefulness of moving one operation versus moving another and can successfully find useful code motions without resorting to branch profiling. The compile-time overhead of selective scheduling is low due to its incremental computation technique and its controlled code duplication. We parallelized the SPEC integer benchmarks and five AIX utilities without using branch probabilities. The experiments indicate that a fivefold speedup is achievable on realistic resources with a reasonable overhead in compilation time and code expansion and that a solid speedup increase is also obtainable on machines with fewer resources. These results improve previously known characteristics of irregular ILP.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2034393996",
    "type": "article"
  },
  {
    "title": "Complementation in abstract interpretation",
    "doi": "https://doi.org/10.1145/239912.239914",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Agostino Cortesi; Gilberto Filé; Francesco Ranzato; Roberto Giacobazzi; Catuscia Palamidessi",
    "corresponding_authors": "",
    "abstract": "Reduced product of abstract domains is a rather well-known operation for domain composition in abstract interpretation. In this article, we study its inverse operation, introducing a notion of domain complementation in abstract interpretation. Complementation provides as systematic way to design new abstract domains, and it allows to systematically decompose domains. Also, such an operation allows to simplify domain verification problems, and it yields space-saving representations for complex domains. We show that the complement exists in most coses, and we apply complementation to three well-know abstract domains, notably to Cousot and Cousot's interval domain for integer variable analysis, to Cousot and Cousot's domain for comportment analysis of functional languages, and to the domain Sharing for aliasing analysis of logic languages.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2002387648",
    "type": "article"
  },
  {
    "title": "Space-efficient scheduling of nested parallelism",
    "doi": "https://doi.org/10.1145/314602.314607",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Girija Narlikar; Guy E. Blelloch",
    "corresponding_authors": "",
    "abstract": "Many of today's high-level parallel languages support dynamic, fine-grained parallelism. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. Hence an efficient scheduling algorithm is required to assign computations to processors at runtime. Besides having low overheads and good load balancing, it is important for the scheduling algorithm to minimize the space usage of the parallel program. This article presents an on-line scheduling algorithm that is provably space efficient and time efficient for nested-parallel languages. For a computation with depth D and serial space requirement S 1 , the algorithm generates a schedule that requires at most S 1 + O (K•D•p ) space (including scheduler space) on p processors. Here, K is a user-adjustable runtime parameter specifying the net amount of memory that a thread may allocate before it is preempted by the scheduler. Adjusting the value of K provides a trade-off between the running time and the memory requirement of a parallel computation. To allow the scheduler to scale with the number of processors we also parallelize the scheduler and analyze the space and time bounds of the computation to include scheduling costs. In addition to showing that the scheduling algorithm is space and time efficient in theory, we demonstrate that it is effective in practice. We have implemented a runtime system that uses our algorithm to schedule lightweight parallel threads. The results of executing parallel programs on this system show that our scheduling algorithm significantly reduces memory usage compared to previous techniques, without compromising performance.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2087610851",
    "type": "article"
  },
  {
    "title": "Polymorphic splitting",
    "doi": "https://doi.org/10.1145/271510.271523",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Andrew K. Wright; Suresh Jagannathan",
    "corresponding_authors": "",
    "abstract": "This article describes a general-purpose program analysis that computes global control-flow and data-flow information for higher-order, call-by-value languages. The analysis employs a novel form of polyvariance called polymorhic splitting that uses let-expressions as syntactic clues to gain precision. The information derived from the analysis is used both to eliminate run-time checks and to inline procedure. The analysis and optimizations have been applied to a suite of Scheme programs. Experimental results obtained from the prototype implementation indicate that the analysis is extremely precise and has reasonable cost. Compared to monovariant flow analyses such as 0CFA, or analyses based on type inference such as soft typing, the analysis eliminates significantly more run-time checks. Run-time check elimination and inlining together typically yield a 20 to 40% performance improvement for the benchmark suite, with some programs running four times as fast.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2003786919",
    "type": "article"
  },
  {
    "title": "On loops, dominators, and dominance frontiers",
    "doi": "https://doi.org/10.1145/570886.570887",
    "publication_date": "2002-09-01",
    "publication_year": 2002,
    "authors": "G. Ramalingam",
    "corresponding_authors": "G. Ramalingam",
    "abstract": "This article explores the concept of loops and loop nesting forests of control-flow graphs, using the problem of constructing the dominator tree of a graph and the problem of computing the iterated dominance frontier of a set of vertices in a graph as guiding applications. The contributions of this article include: (1) An axiomatic characterization, as well as a constructive characterization, of a family of loop nesting forests that includes various specific loop nesting forests that have been previously defined. (2) The definition of a new loop nesting forest, as well as an efficient, almost linear-time, algorithm for constructing this forest. (3) An illustration of how loop nesting forests can be used to transform arbitrary (potentially irreducible) problem instances into equivalent acylic graph problem instances in the case of the two problems of (a) constructing the dominator tree of a graph, and (b) computing the iterated dominance frontier of a set of vertices in a graph, leading to new, almost linear-time, algorithms for these problems.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2054253211",
    "type": "article"
  },
  {
    "title": "From flop to megaflops",
    "doi": "https://doi.org/10.1145/349214.349222",
    "publication_date": "2000-03-01",
    "publication_year": 2000,
    "authors": "José E. Moreira; Samuel P. Midkiff; Manish Gupta",
    "corresponding_authors": "",
    "abstract": "Although there has been some experimentation with Java as a language for numerically intensive computing, there is a perception by many that the language is unsuited for such work because of performance deficiencies. In this article we show how optimizing array bounds checks and null pointer checks creates loop nests on which aggressive optimizations can be used. Applying these optimizations by hand to a simple matrix-multiply test case leads to Java-compliant programs whose performance is in excess of 500 Mflops on a four-processor 332MHz RS/6000 model F50 computer. We also report in this article the effect that various optimizations have on the performance of six floating-point-intensive benchmarks. Through these optimizations we have been able to achieve with Java at least 80% of the peak Fortran performance on the same benchmarks. Since all of these optimizations can be automated, we conclude that Java will soon be a serious contender for numerically intensive computing.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2063612156",
    "type": "article"
  },
  {
    "title": "Traversals of object structures",
    "doi": "https://doi.org/10.1145/973097.973102",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Karl Lieberherr; Boaz Patt-Shamir; Doug Orleans",
    "corresponding_authors": "",
    "abstract": "Separation of concerns and loose coupling of concerns are important issues in software enginnering. In this paper we show how to separate traversal-related concerns from other concerns, how to loosely couple traversal-related concerns to the structural concern, and how to efficiently implement traversal-related concerns. The stress is on the detailed description of our algorithms and the traversal specifications they operate on.Traversal of object structures is a ubiquitous routine in most types of information processing. Ad hoc implementations of traversals lead to scattered and tangled code and in this paper we present a new approach, called traversal strategies, to succinctly modularize traversals. In our approach traversals are defined using a high-level directed graph description, which is compiled into a dynamic road map to assist run-time traversals. The complexity of the compilation algorithm is polynomial in the size of the traversal strategy graph and the class graph of the given application. Prototypes of the system have been developed and are being successfully used to implement traversals for Java and AspectJ [Kiczales et al. 2001] and for generating adapters for software components. Our previous approach, called traversal specifications [Lieberherr 1992; Palsberg et al. 1995], was less general and less succinct, and its compilation algorithm was of exponential complexity in some cases. In an additional result we show that this bad behavior is inherent to the static traversal code generated by previous implementations, where traversals are carried out by invoking methods without parameters.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2102217676",
    "type": "article"
  },
  {
    "title": "Experimental results from dynamic slicing of C programs",
    "doi": "https://doi.org/10.1145/201059.201062",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "G. A. Venkatesh",
    "corresponding_authors": "G. A. Venkatesh",
    "abstract": "Program slicing is a program analysis technique that has been studied in the context of several different applications in the construction, optimization, maintenance, testing, and debugging of programs. Algorithms are available for constructing slices for a particular execution of a program (dynamic slices), as well as to approximate a subset of the behavior over all possible executions of a program (static slices). However, these algorithms have been studied only in the context of small abstract languages. Program slicing is bound to remain an academic exercise unless one can not only demonstrate the feasibility of building a slicer for nontrivial programs written in a real programming language, but also verify that a type of slice is sufficiently thin, on the average, for the application for which it is chosen. In this article, we present results from using SLICE , a dynamic program slicer for C programs, designed and implemented to experiment with several different kinds of program slices and to study them both qualitatively and quantitatively. Several application programs, ranging in size (i.e., number of lines of code) over two orders of magnitude, were sliced exhaustively to obtain average worst-case metrics for the size of program slices.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2061608960",
    "type": "article"
  },
  {
    "title": "Scheduling time-critical instructions on RISC machines",
    "doi": "https://doi.org/10.1145/155183.155190",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Krishna V. Palem; Barbara Sımons",
    "corresponding_authors": "",
    "abstract": "We present a polynomial time algorithm for constructing a minimum completion time schedule of instructions from a basic block on RISC machines such as the Sun SPARC, the IBM 801, the Berkeley RISC machine, and the HP Precision Architecture. Our algorithm can be used as a heuristic for RISC processors with longer pipelines, for which there is no known optimal algorithm. Our algorithm can also handle time-critical instructions, which are instructions that have to be completed by a specific time. Time-critical instructions occur in some real-time computations, and can also be used to make shared resources such as registers quickly available for reuse. We also prove that in the absence of time-critical constraints, a greedy scheduling algorithm always produces a schedule for a target machine with multiple identical pipelines that has a length less than twice that of an optimal schedule. The behavior of the heuristic is of interest because, as we show, the instruction scheduling problem becomes NP-hard for arbitrary length pipelines, even when the basic block of code being input consists of only several independent streams of straightline code, and there are no time-critical constraints. Finally, we prove that the problem becomes NP-hard even for small pipelines, no time-critical constraints, and input of several independent streams of straightline code if either there is only a single register or if no two instructions are allowed to complete simultaneously because of some shared resource such as a bus.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2117720131",
    "type": "article"
  },
  {
    "title": "Optimal evaluation of array expressions on massively parallel machines",
    "doi": "https://doi.org/10.1145/200994.201004",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Siddhartha Chatterjee; John R. Gilbert; Robert Schreiber; Shang‐Hua Teng",
    "corresponding_authors": "",
    "abstract": "We investigate the problem of evaluating Fortran 90-style array expressions on massively parallel distributed-memory machines. On such a machine, an elementwise operation can be performed in constant time for arrays whose corresponding elements are in the same processor. If the arrays are not aligned in this manner, the cost of aligning them is part of the cost of evaluating the expression tree. The choice of where to perform the operation then affects this cost. We describe the communication cost of the parallel machine theoretically as a metric space; we model the alignment problem as that of finding a minimum-cost embedding of the expression tree into this space. We present algorithms based on dynamic programming that solve the embedding problem optimally for several communication cost metrics: multidimensional grids and rings, hypercubes, fat-trees, and the discrete metric. We also extend our approach to handle operations that change the shape of the arrays.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W1977256916",
    "type": "article"
  },
  {
    "title": "PolyTOIL",
    "doi": "https://doi.org/10.1145/641888.641891",
    "publication_date": "2003-03-01",
    "publication_year": 2003,
    "authors": "Kim B. Bruce; Angela Schuett; R. H. van Gent; Adrian Fiech",
    "corresponding_authors": "",
    "abstract": "PolyTOIL is a new statically typed polymorphic object-oriented programming language that is provably typesafe. By separating the definitions of subtyping and inheritance, providing a name for the type of self, and carefully defining the type-checking rules, we have obtained a language that is very expressive while supporting modular type-checking of classes. The matching relation on types, which is related to F-bounded quantification, is used both in stating type-checking rules and expressing the bounds on type parameters for polymorphism. The design of PolyTOIL is based on a careful formal definition of type-checking rules and semantics. A proof of type safety is obtained with the aid of a subject reduction theorem.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2163420823",
    "type": "article"
  },
  {
    "title": "A parallel, incremental, mostly concurrent garbage collector for servers",
    "doi": "https://doi.org/10.1145/1108970.1108972",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Katherine Barabash; Ori Ben-Yitzhak; Irit Goft; Elliot K. Kolodner; Victor Leikehman; Yoav Ossia; Avi Owshanko; Erez Petrank",
    "corresponding_authors": "",
    "abstract": "Multithreaded applications with multigigabyte heaps running on modern servers provide new challenges for garbage collection (GC). The challenges for “server-oriented” GC include: ensuring short pause times on a multigigabyte heap while minimizing throughput penalty, good scaling on multiprocessor hardware, and keeping the number of expensive multicycle fence instructions required by weak ordering to a minimum.We designed and implemented a collector facing these demands building on the mostly concurrent garbage collector proposed by Boehm et al. [1991]. Our collector incorporates new ideas into the original collector. We make it parallel and incremental; we employ concurrent low-priority background GC threads to take advantage of processor idle time; we propose novel algorithmic improvements to the basic mostly concurrent algorithm improving its efficiency and shortening its pause times; and finally, we use advanced techniques, such as a low-overhead work packet mechanism to enable full parallelism among the incremental and concurrent collecting threads and ensure load balancing.We compared the new collector to the mature, well-optimized, parallel, stop-the-world mark-sweep collector already in the IBM JVM. When allowed to run aggressively, using 72% of the CPU utilization during a short concurrent phase, our collector prototype reduces the maximum pause time from 161 ms to 46 ms while only losing 11.5% throughput when running the SPECjbb2000 benchmark on a 600-MB heap on an 8-way PowerPC 1.1-GHz processors. When the collector is limited to a nonintrusive operation using only 29% of the CPU utilization, the maximum pause time obtained is 79 ms and the loss in throughput is 15.4%.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2053026915",
    "type": "article"
  },
  {
    "title": "A mechanism for environment integration",
    "doi": "https://doi.org/10.1145/77606.77607",
    "publication_date": "1990-01-03",
    "publication_year": 1990,
    "authors": "Geoffrey Clemm; Leon J. Osterweil",
    "corresponding_authors": "",
    "abstract": "This paper describes research associated with the development and evaluation of Odin-an environment integration system based on the idea that tools should be integrated around a centralized store of persistent software objects. The paper describes this idea in detail and then presents the Odin architecture, which features such notions as the typing of software objects, composing tools out of modular tool fragments, optimizing the storage and rederivation of software objects, and isolating tool interconnectivity information in a single centralized object. The paper then describes some projects that have used Odin to integrate tools on a large scale. Finally, it discusses the significance of this work and the conclusions that can be drawn about superior software environment architectures.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2054813151",
    "type": "article"
  },
  {
    "title": "Design and evaluation of dynamic optimizations for a Java just-in-time compiler",
    "doi": "https://doi.org/10.1145/1075382.1075386",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Toshio Suganuma; Toshiaki Yasue; Motohiro Kawahito; Hideaki Komatsu; Toshio Nakatani",
    "corresponding_authors": "",
    "abstract": "The high performance implementation of Java Virtual Machines (JVM) and Just-In-Time (JIT) compilers is directed toward employing a dynamic compilation system on the basis of online runtime profile information. The trade-off between the compilation overhead and performance benefit is a crucial issue for such a system. This article describes the design and implementation of a dynamic optimization framework in a production-level Java JIT compiler, together with two techniques for profile-directed optimizations: method inlining and code specialization. Our approach is to employ a mixed mode interpreter and a three-level optimizing compiler, supporting level-1 to level-3 optimizations, each of which has a different set of trade-offs between compilation overhead and execution speed. A lightweight sampling profiler operates continuously during the entire period while applications are running to monitor the programs' hot spots. Detailed information on runtime behavior can be collected by dynamically generating instrumentation code that is installed to and uninstalled from the specified recompilation target code. Value profiling with this instrumentation mechanism allows fully automatic profile-directed method inlining and code specialization to be performed on the basis of call site information or specific parameter values at the higher optimization levels. The experimental results show that our approach offers high performance and low compilation overhead in both program startup and steady state measurements in comparison to the previous systems. The two profile-directed optimization techniques contribute significant portions of the improvements.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2061483729",
    "type": "article"
  },
  {
    "title": "Embedding continuations in procedural objects",
    "doi": "https://doi.org/10.1145/29873.30392",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Christopher T. Haynes; Daniel P. Friedman",
    "corresponding_authors": "",
    "abstract": "Continuations, when available as first-class objects, provide a general control abstraction in programming languages. They liberate the programmer from specific control structures, increasing programming language extensibility. Such continuations may be extended by embedding them in procedural objects. This technique is first used to restore a fluid environment when a continuation object is invoked. We then consider techniques for constraining the power of continuations in the interest of security and efficiency. Domain mechanisms, which create dynamic barriers for enclosing control, are implemented using fluids. Domains are then used to implement an unwind-protect facility in the presence of first-class continuations. Finally, we present two mechanisms, wind-unwind and dynamic-wind, that generalize unwind-protect.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2063622838",
    "type": "article"
  },
  {
    "title": "Compilation of functional languages by program transformation",
    "doi": "https://doi.org/10.1145/114005.102805",
    "publication_date": "1991-01-01",
    "publication_year": 1991,
    "authors": "Pascal Fradet; Daniel Le Métayer",
    "corresponding_authors": "",
    "abstract": "One of the most important issues concerning functional languages is the efficiency and the correctness of their implementation. We focus on sequential implementations for conventional von Neumann computers. The compilation process is described in terms of program transformations in the functional framework. The original functional expression is transformed into a functional term that can be seen as a traditional machine code. The two main steps are the compilation of the computation rule by the introduction of continuation functions and the compilation of the environment management using combinators. The advantage of this approach is that we do not have to introduce an abstract machine, which makes correctness proofs much simpler. As far as efficiency is concerned, this approach is promising since many optimizations can be described and formally justified in the functional framework.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2073771426",
    "type": "article"
  },
  {
    "title": "A practical method for LR and LL syntactic error diagnosis and recovery",
    "doi": "https://doi.org/10.1145/22719.22720",
    "publication_date": "1987-03-20",
    "publication_year": 1987,
    "authors": "Michael Burke; Gerald A. Fisher",
    "corresponding_authors": "",
    "abstract": "This paper presents a powerful, practical, and essentially language-independent syntactic error diagnosis and recovery method that is applicable within the frameworks of LR and LL parsing. The method generally issues accurate diagnoses even where multiple errors occur within close proximity, yet seldom issues spurious error messages. It employs a new technique, parse action deferral, that allows the most appropriate recovery in cases where this would ordinarily be precluded by late detection of the error. The method is practical in that it does not impose substantial space or time overhead on the parsing of correct programs, and in that its time efficiency in processing an error allows for its incorporation in a production compiler. The method is language independent, but it does allow for tuning with respect to particular languages and implementations through the setting of language-specific parameters.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2004071957",
    "type": "article"
  },
  {
    "title": "An Ada package for dimensional analysis",
    "doi": "https://doi.org/10.1145/42190.42346",
    "publication_date": "1988-04-01",
    "publication_year": 1988,
    "authors": "Paul Hilfinger",
    "corresponding_authors": "Paul Hilfinger",
    "abstract": "This paper illustrates the use of Ada's abstraction facilities—notably, operator overloading and type parameterization—to define an oft-requested feature: a way to attribute units of measure to variables and values. The definition given allows the programmer to specify units of measure for variables, constants, and parameters; checks uses of these entities for dimensional consistency; allows arithmetic between them, where legal; and provides scale conversions between commensurate units. It is not constrained to a particular system of measurement (such as the metric or English systems). Although the definition is in standard Ada and requires nothing special of the compiler, certain reasonable design choices in the compiler, discussed here at some length, can make its implementation particularly efficient.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2016043411",
    "type": "article"
  },
  {
    "title": "Grammar-Based Definition of Metaprogramming Systems",
    "doi": "https://doi.org/10.1145/357233.357235",
    "publication_date": "1984-01-01",
    "publication_year": 1984,
    "authors": "Robert D. Cameron; M.R. Ito",
    "corresponding_authors": "",
    "abstract": "article Free AccessGrammar-Based Definition of Metaprogramming Systems Authors: Robert D. Cameron Department of Computing Science, Simon Fraser University, Burnaby, British Columbia, V5A 1S6, Canada Department of Computing Science, Simon Fraser University, Burnaby, British Columbia, V5A 1S6, CanadaView Profile , M. Robert Ito Department of Electrical Engineering, 2356 Main Mall, University of British Columbia, Vancouver, British Columbia V6T 1W5, Canada Department of Electrical Engineering, 2356 Main Mall, University of British Columbia, Vancouver, British Columbia V6T 1W5, CanadaView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 1Jan. 1984 pp 20–54https://doi.org/10.1145/357233.357235Published:01 January 1984Publication History 33citation810DownloadsMetricsTotal Citations33Total Downloads810Last 12 Months59Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2082975296",
    "type": "article"
  },
  {
    "title": "A distributed alternative to finite-state-machine specifications",
    "doi": "https://doi.org/10.1145/2363.2365",
    "publication_date": "1985-01-02",
    "publication_year": 1985,
    "authors": "Pamela Zave",
    "corresponding_authors": "Pamela Zave",
    "abstract": "A specification technique, formally equivalent to finite-state machines, is offered as an alternative because it is inherently distributed and more comprehensible. When applied to modules whose complexity is dominated by control, the technique guides the analyst to an effective decomposition of complexity, encourages well-structured error handling, and offers an opportunity for parallel computation. When applied to distributed protocols, the technique provides a unique perspective and facilitates automatic detection of some classes of error. These applications are illustrated by a controller for a distributed telephone system and the full-duplex alternating-bit protocol for data communication. Several schemes are presented for executing the resulting specifications.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2050340037",
    "type": "article"
  },
  {
    "title": "Right nulled GLR parsers",
    "doi": "https://doi.org/10.1145/1146809.1146810",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Elizabeth Scott; Adrian Johnstone",
    "corresponding_authors": "",
    "abstract": "The right nulled generalized LR parsing algorithm is a new generalization of LR parsing which provides an elegant correction to, and extension of, Tomita's GLR methods whereby we extend the notion of a reduction in a shift-reduce parser to include right nulled items. The result is a parsing technique which runs in linear time on LR(1) grammars and whose performance degrades gracefully to a polynomial bound in the presence of nonLR(1) rules. Compared to other GLR-based techniques, our algorithm is simpler and faster.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W1969357165",
    "type": "article"
  },
  {
    "title": "A New Solution to Lamport's Concurrent Programming Problem Using Small Shared Variables",
    "doi": "https://doi.org/10.1145/357195.357199",
    "publication_date": "1983-01-01",
    "publication_year": 1983,
    "authors": "Gary L. Peterson",
    "corresponding_authors": "Gary L. Peterson",
    "abstract": "article Free Access Share on A New Solution to Lamport's Concurrent Programming Problem Using Small Shared Variables Author: Gary L. Peterson Department of Computer Science, The University of Rochester, Rochester, NY Department of Computer Science, The University of Rochester, Rochester, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 1pp 56–65https://doi.org/10.1145/357195.357199Published:01 January 1983Publication History 40citation773DownloadsMetricsTotal Citations40Total Downloads773Last 12 Months34Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2023292202",
    "type": "article"
  },
  {
    "title": "Generators in Icon",
    "doi": "https://doi.org/10.1145/357133.357136",
    "publication_date": "1981-04-01",
    "publication_year": 1981,
    "authors": "Ralph E. Griswold; David R. Hanson; John T. Korb",
    "corresponding_authors": "",
    "abstract": "article Free AccessGenerators in Icon Authors: Ralph E. Griswold Department of Computer Science, The University of Arizona, Tucson, AZ Department of Computer Science, The University of Arizona, Tucson, AZView Profile , David R. Hanson Department of Computer Science, The University of Arizona, Tucson, AZ Department of Computer Science, The University of Arizona, Tucson, AZView Profile , John T. Korb Xerox Corporation, 3333 Coyote Hill Road, Palo Alto, CA Xerox Corporation, 3333 Coyote Hill Road, Palo Alto, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 2pp 144–161https://doi.org/10.1145/357133.357136Published:01 April 1981Publication History 53citation475DownloadsMetricsTotal Citations53Total Downloads475Last 12 Months31Last 6 weeks6 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2070482555",
    "type": "article"
  },
  {
    "title": "Efficient implementation of the first-fit strategy for dynamic storage allocation",
    "doi": "https://doi.org/10.1145/65979.65981",
    "publication_date": "1989-07-01",
    "publication_year": 1989,
    "authors": "Richard P. Brent",
    "corresponding_authors": "Richard P. Brent",
    "abstract": "We describe an algorithm that efficiently implements the first-fit strategy for dynamic storage allocation. The algorithm imposes a storage overhead of only one word per allocated block (plus a few percent of the total space used for dynamic storage), and the time required to allocate or free a block is O (log W ), where W is the maximum number of words allocated dynamically. The algorithm is faster than many commonly used algorithms, especially when many small blocks are allocated, and has good worst-case behavior. It is relatively easy to implement and could be used internally by an operating system or to provide run-time support for high-level languages such as Pascal and Ada. A Pascal implementation is given in the Appendix.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2106331723",
    "type": "article"
  },
  {
    "title": "Dynamic slicing on Java bytecode traces",
    "doi": "https://doi.org/10.1145/1330017.1330021",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Tao Wang; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Dynamic slicing is a well-known technique for program analysis, debugging and understanding. Given a program P and input I, it finds all program statements which directly/indirectly affect the values of some variables' occurrences when P is executed with I. In this article, we develop a dynamic slicing method for Java programs. Our technique proceeds by backwards traversal of the bytecode trace produced by an input I in a given program P . Since such traces can be huge, we use results from data compression to compactly represent bytecode traces. The major space savings in our method come from the optimized representation of (a) data addresses used as operands by memory reference bytecodes, and (b) instruction addresses used as operands by control transfer bytecodes. We show how dynamic slicing algorithms can directly traverse our compact bytecode traces without resorting to costly decompression. We also extend our dynamic slicing algorithm to perform “relevant slicing”. The resultant slices can be used to explain omission errors that is, why some events did not happen during program execution. Detailed experimental results on space/time overheads of tracing and slicing are reported in the article. The slices computed at the bytecode level are translated back by our tool to the source code level with the help of information available in Java class files. Our JSlice dynamic slicing tool has been integrated with the Eclipse platform and is available for usage in research and development.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2116570626",
    "type": "article"
  },
  {
    "title": "Smarter recompilation",
    "doi": "https://doi.org/10.1145/48022.214505",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "Robert W. Schwanke; Gail E. Kaiser",
    "corresponding_authors": "",
    "abstract": "Tichy's Smart Recompilation method can be made smarter by permitting benign type inconsistencies between separately compiled modules. This enhanced method helps the programmer to make far-reaching changes in small, manageable steps.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2296144649",
    "type": "article"
  },
  {
    "title": "Using message passing for distributed programming: proof rules and disciplines",
    "doi": "https://doi.org/10.1145/579.583",
    "publication_date": "1984-07-01",
    "publication_year": 1984,
    "authors": "Richard D. Schlichting; Fred B. Schneider",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Using message passing for distributed programming: proof rules and disciplines Authors: Richard D. Schlichting University of Arizona University of ArizonaView Profile , Fred B. Schneider Cornell University, Ithaca, NY Cornell University, Ithaca, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 3pp 402–431https://doi.org/10.1145/579.583Published:01 July 1984Publication History 30citation450DownloadsMetricsTotal Citations30Total Downloads450Last 12 Months16Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2160098227",
    "type": "article"
  },
  {
    "title": "Some Techniques for Recursion Removal from Recursive Functions",
    "doi": "https://doi.org/10.1145/357162.357171",
    "publication_date": "1982-04-01",
    "publication_year": 1982,
    "authors": "Jacques Arsac; Yves Kodratoff",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Some Techniques for Recursion Removal from Recursive Functions Authors: J. Arsac L.A. 248 du CNRS, Institut de programmation, Tour 55-65, 4, place Jussieu, 75230 Paris Cedex 05, France L.A. 248 du CNRS, Institut de programmation, Tour 55-65, 4, place Jussieu, 75230 Paris Cedex 05, FranceView Profile , Y. Kodratoff L.R.I., Bat. 490, F-91405, Orsay, France L.R.I., Bat. 490, F-91405, Orsay, FranceView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 201 April 1982pp 295–322https://doi.org/10.1145/357162.357171Published:01 April 1982Publication History 40citation858DownloadsMetricsTotal Citations40Total Downloads858Last 12 Months212Last 6 weeks25 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W1968602872",
    "type": "article"
  },
  {
    "title": "Proving liveness for networks of communicating finite state machines",
    "doi": "https://doi.org/10.1145/5001.5002",
    "publication_date": "1986-01-02",
    "publication_year": 1986,
    "authors": "Mohammed G. Gouda; Chung-Kuo Chang",
    "corresponding_authors": "",
    "abstract": "Consider a network of communicating finite state machines that exchange messages over unbounded FIFO channels. Each machine in the network can be defined by a directed graph whose nodes represent the machine states and whose edges represent its transitions. In general, for a node in one of the machines to be live (i.e., encountered infinitely often during the course of communication), each machine in the network should progress in some fair fashion. We define three graduated notions of fair progress (namely, node fairness, edge fairness, and network fairness), and on this basis we define three corresponding degrees of node liveness. We discuss techniques to verify that a given node is live under each of these fairness assumptions. These techniques can be automated; and they are effective even if the network under consideration has an infinite number of reachable states. We use our techniques to establish the liveness of some practical communication protocols; these include an unbounded start-stop protocol, an unbounded alternating bit protocol, and a simplified version of the CSMA/CD protocol for local area networks.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2011723719",
    "type": "article"
  },
  {
    "title": "An APL Compiler for a Vector Processor",
    "doi": "https://doi.org/10.1145/579.357248",
    "publication_date": "1984-07-01",
    "publication_year": 1984,
    "authors": "Timothy A. Budd",
    "corresponding_authors": "Timothy A. Budd",
    "abstract": "article Free Access Share on An APL Compiler for a Vector Processor Author: Timothy A. Budd Department of Computer Science, University of Arizona, Tucson, AZ Department of Computer Science, University of Arizona, Tucson, AZView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 3pp 297–313https://doi.org/10.1145/579.357248Published:01 July 1984Publication History 36citation566DownloadsMetricsTotal Citations36Total Downloads566Last 12 Months40Last 6 weeks8 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2017164080",
    "type": "article"
  },
  {
    "title": "A Formal System for Reasoning about Programs Accessing a Relational Database",
    "doi": "https://doi.org/10.1145/357103.357111",
    "publication_date": "1980-07-01",
    "publication_year": 1980,
    "authors": "Marco R. Casanova; P. A. Bernstein",
    "corresponding_authors": "",
    "abstract": "A formal system for proving properties of programs accessing a database is introduced. Proving that a program preserves consistency of the database is one of the possible applications of the system. The formal system is a variant of dynamic logic and incorporates a data definition language (DDL) for describing relational databases and a data manipulation language (DML) whose programs access data in a database. The DDL is a many-sorted first-order language that accounts for data aggregations. The DML features a many-sorted assignment in place of the usual data manipulation statements, in addition to the normal programming language constructs.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2081251433",
    "type": "article"
  },
  {
    "title": "Towards monolingual programming environments",
    "doi": "https://doi.org/10.1145/3318.3321",
    "publication_date": "1985-04-01",
    "publication_year": 1985,
    "authors": "Jan Heering; Paul Klint",
    "corresponding_authors": "",
    "abstract": "Most programming environments are much too complex. One way of simplifying them is to reduce the number of mode-dependent languages the user has to be familiar with. As a first step towards this end, the feasibility of unified command/programming/debugging languages, and the concepts on which such languages have to be based, are investigated. The unification process is accomplished in two phases. First, a unified command/programming framework is defined and, second, this framework is extended by adding an integrated debugging capability to it. Strict rules are laid down by which to judge language concepts presenting themselves as candidates for inclusion in the framework during each phase. On the basis of these rules many of the language design questions that have hitherto been resolved this way or that, depending on the taste of the designer, lose their vagueness and can be decided in an unambiguous manner.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W1977997607",
    "type": "article"
  },
  {
    "title": "Control Flow Aspects of Semantics-Directed Compiling",
    "doi": "https://doi.org/10.1145/69575.357227",
    "publication_date": "1983-10-01",
    "publication_year": 1983,
    "authors": "Ravi Sethi",
    "corresponding_authors": "Ravi Sethi",
    "abstract": "article Free Access Share on Control Flow Aspects of Semantics-Directed Compiling Author: Ravi Sethi Bell Laboratories, Murray Hill, NJ Bell Laboratories, Murray Hill, NJView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 4Oct. 1983 pp 554–595https://doi.org/10.1145/69575.357227Published:01 October 1983Publication History 29citation412DownloadsMetricsTotal Citations29Total Downloads412Last 12 Months23Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2071836561",
    "type": "article"
  },
  {
    "title": "JavaCOP",
    "doi": "https://doi.org/10.1145/1667048.1667049",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Shane Markstrum; Daniel Marino; Matthew Esquivel; Todd Millstein; Chris Andreae; James Noble",
    "corresponding_authors": "",
    "abstract": "Pluggable types enable users to enforce multiple type systems in one programming language. We have developed a suite of tools, called the JavaCOP framework, that allows developers to create pluggable type systems for Java. JavaCOP provides a simple declarative language in which program constraints are defined over a program's abstract syntax tree. The JavaCOP compiler automatically enforces these constraints on programs during compilation. The JavaCOP framework also includes a dataflow analysis API in order to support type systems which depend on flow-sensitive information. Finally, JavaCOP includes a novel test framework which helps users gain confidence in the correctness of their pluggable type systems. We demonstrate the framework by discussing a number of pluggable type systems which have been implemented in JavaCOP in order to detect errors and enforce strong invariants in programs. These type systems range from general-purpose checkers, such as a type system for nonnull references, to domain-specific ones, such as a checker for conformance to a library's usage rules.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2153559293",
    "type": "article"
  },
  {
    "title": "A calculus for uniform feature composition",
    "doi": "https://doi.org/10.1145/1745312.1745316",
    "publication_date": "2008-05-24",
    "publication_year": 2008,
    "authors": "Sven Apel; DeLesley Hutchins",
    "corresponding_authors": "",
    "abstract": "The goal of feature-oriented programming (FOP) is to modularize software systems in terms of features. A feature refines the content of a base program. Both base programs and features may contain various kinds of software artifacts, for example, source code in different languages, models, build scripts, and documentation. We and others have noticed that when composing features, different kinds of software artifacts can be refined in a uniform way, regardless of what they represent. We present gDeep, a core calculus for feature composition, which captures the language independence of FOP; it can be used to compose features containing many different kinds of artifact in a type-safe way. The calculus allows us to gain insight into the principles of FOP and to define general algorithms for feature composition and validation. We provide the formal syntax, operational semantics, and type system of gDeep and outline how languages like Java, Haskell, Bali, and XML can be plugged in.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W1991911581",
    "type": "article"
  },
  {
    "title": "Semantic foundations for typed assembly languages",
    "doi": "https://doi.org/10.1145/1709093.1709094",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Amal Ahmed; Andrew W. Appel; Christina D. Richards; Kedar N. Swadi; Gang Tan; Daniel C. Wang",
    "corresponding_authors": "",
    "abstract": "Typed Assembly Languages (TALs) are used to validate the safety of machine-language programs. The Foundational Proof-Carrying Code project seeks to verify the soundness of TALs using the smallest possible set of axioms: the axioms of a suitably expressive logic plus a specification of machine semantics. This article proposes general semantic foundations that permit modular proofs of the soundness of TALs. These semantic foundations include Typed Machine Language (TML), a type theory for specifying properties of low-level data with powerful and orthogonal type constructors, and L c , a compositional logic for specifying properties of machine instructions with simplified reasoning about unstructured control flow. Both of these components, whose semantics we specify using higher-order logic, are useful for proving the soundness of TALs. We demonstrate this by using TML and L c to verify the soundness of a low-level, typed assembly language, LTAL, which is the target of our core-ML-to-sparc compiler. To prove the soundness of the TML type system we have successfully applied a new approach, that of step-indexed logical relations . This approach provides the first semantic model for a type system with updatable references to values of impredicative quantified types. Both impredicative polymorphism and mutable references are essential when representing function closures in compilers with typed closure conversion, or when compiling objects to simpler typed primitives.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W1993030244",
    "type": "article"
  },
  {
    "title": "Local policies for resource usage analysis",
    "doi": "https://doi.org/10.1145/1552309.1552313",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Massimo Bartoletti; Pierpaolo Degano; Gian-Luigi Ferrari; Roberto Zunino",
    "corresponding_authors": "",
    "abstract": "An extension of the λ-calculus is proposed, to study resource usage analysis and verification. It features usage policies with a possibly nested, local scope, and dynamic creation of resources. We define a type and effect system that, given a program, extracts a history expression, that is, a sound overapproximation to the set of histories obtainable at runtime. After a suitable transformation, history expressions are model-checked for validity. A program is resource-safe if its history expression is verified valid: If such, no runtime monitor is needed to safely drive its executions.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2073585991",
    "type": "article"
  },
  {
    "title": "Formal Verification of an SSA-Based Middle-End for CompCert",
    "doi": "https://doi.org/10.1145/2579080",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Gilles Barthe; Delphine Demange; David Pichardie",
    "corresponding_authors": "",
    "abstract": "CompCert is a formally verified compiler that generates compact and efficient code for a large subset of the C language. However, CompCert foregoes using SSA, an intermediate representation employed by many compilers that enables writing simpler, faster optimizers. In fact, it has remained an open problem to verify formally an SSA-based compiler. We report on a formally verified, SSA-based middle-end for CompCert. In addition to providing a formally verified SSA-based middle-end, we address two problems raised by Leroy in [2009]: giving an intuitive formal semantics to SSA, and leveraging its global properties to reason locally about program optimizations.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2034340985",
    "type": "article"
  },
  {
    "title": "Rely-Guarantee-Based Simulation for Compositional Verification of Concurrent Program Transformations",
    "doi": "https://doi.org/10.1145/2576235",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Hongjin Liang; Xinyu Feng; Ming Fu",
    "corresponding_authors": "",
    "abstract": "Verifying program transformations usually requires proving that the resulting program (the target) refines or is equivalent to the original one (the source). However, the refinement relation between individual sequential threads cannot be preserved in general with the presence of parallel compositions, due to instruction reordering and the different granularities of atomic operations at the source and the target. On the other hand, the refinement relation defined based on fully abstract semantics of concurrent programs assumes arbitrary parallel environments, which is too strong and cannot be satisfied by many well-known transformations. In this article, we propose a R ely- G uarantee-based Sim ulation (RGSim) to verify concurrent program transformations. The relation is parametrized with constraints of the environments that the source and the target programs may compose with. It considers the interference between threads and their environments, thus is less permissive than relations over sequential programs. It is compositional with respect to parallel compositions as long as the constraints are satisfied. Also, RGSim does not require semantics preservation under all environments, and can incorporate the assumptions about environments made by specific program transformations in the form of rely/guarantee conditions. We use RGSim to reason about optimizations and prove atomicity of concurrent objects. We also propose a general garbage collector verification framework based on RGSim, and verify the Boehm et al. concurrent mark-sweep GC.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2045183833",
    "type": "article"
  },
  {
    "title": "A Transformation Framework for Optimizing Task-Parallel Programs",
    "doi": "https://doi.org/10.1145/2450136.2450138",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "V. Krishna Nandivada; Jun Shirako; Jisheng Zhao; Vivek Sarkar",
    "corresponding_authors": "",
    "abstract": "Task parallelism has increasingly become a trend with programming models such as OpenMP 3.0, Cilk, Java Concurrency, X10, Chapel and Habanero-Java (HJ) to address the requirements of multicore programmers. While task parallelism increases productivity by allowing the programmer to express multiple levels of parallelism, it can also lead to performance degradation due to increased overheads. In this article, we introduce a transformation framework for optimizing task-parallel programs with a focus on task creation and task termination operations. These operations can appear explicitly in constructs such as async, finish in X10 and HJ, task, taskwait in OpenMP 3.0, and spawn, sync in Cilk, or implicitly in composite code statements such as foreach and ateach loops in X10, forall and foreach loops in HJ, and parallel loop in OpenMP. Our framework includes a definition of data dependence in task-parallel programs, a happens-before analysis algorithm, and a range of program transformations for optimizing task parallelism. Broadly, our transformations cover three different but interrelated optimizations: (1) finish-elimination , (2) forall-coarsening , and (3) loop-chunking . Finish-elimination removes redundant task termination operations, forall-coarsening replaces expensive task creation and termination operations with more efficient synchronization operations, and loop-chunking extracts useful parallelism from ideal parallelism. All three optimizations are specified in an iterative transformation framework that applies a sequence of relevant transformations until a fixed point is reached. Further, we discuss the impact of exception semantics on the specified transformations, and extend them to handle task-parallel programs with precise exception semantics. Experimental results were obtained for a collection of task-parallel benchmarks on three multicore platforms: a dual-socket 128-thread (16-core) Niagara T2 system, a quad-socket 16-core Intel Xeon SMP, and a quad-socket 32-core Power7 SMP. We have observed that the proposed optimizations interact with each other in a synergistic way, and result in an overall geometric average performance improvement between 6.28× and 10.30×, measured across all three platforms for the benchmarks studied.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2013433526",
    "type": "article"
  },
  {
    "title": "Precise Predictive Analysis for Discovering Communication Deadlocks in MPI Programs",
    "doi": "https://doi.org/10.1145/3095075",
    "publication_date": "2017-08-17",
    "publication_year": 2017,
    "authors": "Vojtǎch Forejt; Saurabh Joshi; Daniel Kroening; Ganesh Narayanaswamy; Subodh Sharma",
    "corresponding_authors": "",
    "abstract": "The Message Passing Interface (MPI) is the standard API for parallelization in high-performance and scientific computing. Communication deadlocks are a frequent problem in MPI programs, and this article addresses the problem of discovering such deadlocks. We begin by showing that if an MPI program is single path, the problem of discovering communication deadlocks is NP-complete. We then present a novel propositional encoding scheme that captures the existence of communication deadlocks. The encoding is based on modeling executions with partial orders and implemented in a tool called MOPPER . The tool executes an MPI program, collects the trace, builds a formula from the trace using the propositional encoding scheme, and checks its satisfiability. Finally, we present experimental results that quantify the benefit of the approach in comparison to other analyzers and demonstrate that it offers a scalable solution for single-path programs.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2749582297",
    "type": "article"
  },
  {
    "title": "Abstract Domains of Affine Relations",
    "doi": "https://doi.org/10.1145/2651361",
    "publication_date": "2014-10-28",
    "publication_year": 2014,
    "authors": "Matt Elder; Junghee Lim; Tushar Sharma; Tycho Andersen; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "This article considers some known abstract domains for affine-relation analysis (ARA), along with several variants, and studies how they relate to each other. The various domains represent sets of points that satisfy affine relations over variables that hold machine integers and are based on an extension of linear algebra to modules over a ring (in particular, arithmetic performed modulo 2 w , for some machine-integer width w ). We show that the abstract domains of Müller-Olm/Seidl (MOS) and King/Søndergaard (KS) are, in general, incomparable. However, we give sound interconversion methods. In other words, we give an algorithm to convert a KS element v KS to an overapproximating MOS element v MOS —that is, γ ( v KS ) ⊆ γ ( v MOS —as well as an algorithm to convert an MOS element w MOS to an overapproximating KS element w KS —that is, γ ( w MOS ) ⊆ γ ( w KS ). The article provides insight on the range of options that one has for performing ARA in a program analyzer: —We describe how to perform a greedy, operator-by-operator abstraction method to obtain KS abstract transformers. —We also describe a more global approach to obtaining KS abstract transformers that considers the semantics of an entire instruction, basic block, or other loop-free program fragment. The latter method can yield best abstract transformers, and hence can be more precise than the former method. However, the latter method is more expensive. We also explain how to use the KS domain for interprocedural program analysis using a bit-precise concrete semantics, but without bit blasting.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W1973680906",
    "type": "article"
  },
  {
    "title": "Lazy Scheduling",
    "doi": "https://doi.org/10.1145/2629643",
    "publication_date": "2014-09-15",
    "publication_year": 2014,
    "authors": "Alexandros Tzannes; George C. Caragea; Uzi Vishkin; Rajeev Barua",
    "corresponding_authors": "",
    "abstract": "Lazy scheduling is a runtime scheduler for task-parallel codes that effectively coarsens parallelism on load conditions in order to significantly reduce its overheads compared to existing approaches, thus enabling the efficient execution of more fine-grained tasks. Unlike other adaptive dynamic schedulers, lazy scheduling does not maintain any additional state to infer system load and does not make irrevocable serialization decisions. These two features allow it to scale well and to provide excellent load balancing in practice but at a much lower overhead cost compared to work stealing, the golden standard of dynamic schedulers. We evaluate three variants of lazy scheduling on a set of benchmarks on three different platforms and find it to substantially outperform popular work stealing implementations on fine-grained codes. Furthermore, we show that the vast performance gap between manually coarsened and fully parallel code is greatly reduced by lazy scheduling, and that, with minimal static coarsening, lazy scheduling delivers performance very close to that of fully tuned code. The tedious manual coarsening required by the best existing work stealing schedulers and its damaging effect on performance portability have kept novice and general-purpose programmers from parallelizing their codes. Lazy scheduling offers the foundation for a declarative parallel programming methodology that should attract those programmers by minimizing the need for manual coarsening and by greatly enhancing the performance portability of parallel code.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2062273873",
    "type": "article"
  },
  {
    "title": "Cross-Language Interoperability in a Multi-Language Runtime",
    "doi": "https://doi.org/10.1145/3201898",
    "publication_date": "2018-05-28",
    "publication_year": 2018,
    "authors": "Matthias Grimmer; Roland Schatz; Chris Seaton; Thomas Würthinger; Mikel Luján; Hanspeter Mössenböck",
    "corresponding_authors": "",
    "abstract": "In large-scale software applications, programmers often combine different programming languages because this allows them to use the most suitable language for a given problem, to gradually migrate existing projects from one language to another, or to reuse existing source code. However, different programming languages have fundamentally different implementations, which are hard to combine. The composition of language implementations often results in complex interfaces between languages, insufficient flexibility, or poor performance. We propose TruffleVM, a virtual machine (VM) that can execute different programming languages and is able to compose them in a seamless way. TruffleVM supports dynamically-typed languages (e.g., JavaScript and Ruby) as well as statically typed low-level languages (e.g., C). It consists of individual language implementations, which translate source code to an intermediate representation that is executed by a shared VM. TruffleVM composes these different language implementations via generic access . Generic access is a language-agnostic mechanism that language implementations use to access foreign data or call foreign functions. It features language-agnostic messages that the TruffleVM resolves to efficient foreign-language-specific operations at runtime. Generic access supports multiple languages, enables an efficient multi-language development, and ensures high performance. We evaluate generic access with two case studies. The first one explains the transparent composition of JavaScript, Ruby, and C. The second one shows an implementation of the C extensions application programming interface (API) for Ruby. We show that generic access guarantees good runtime performance. It avoids conversion or marshalling of foreign objects at the language boundary and allows the dynamic compiler to perform its optimizations across language boundaries.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2806285345",
    "type": "article"
  },
  {
    "title": "Static Identification of Injection Attacks in Java",
    "doi": "https://doi.org/10.1145/3332371",
    "publication_date": "2019-07-02",
    "publication_year": 2019,
    "authors": "Fausto Spoto; Elisa Burato; Michael D. Ernst; Pietro Ferrara; Alberto Lovato; Damiano Macedonio; Ciprian Spiridon",
    "corresponding_authors": "",
    "abstract": "The most dangerous security-related software errors, according to the OWASP Top Ten 2017 list, affect web applications. They are potential injection attacks that exploit user-provided data to execute undesired operations: database access and updates ( SQL injection ); generation of malicious web pages ( cross-site scripting injection ); redirection to user-specified web pages ( redirect injection ); execution of OS commands and arbitrary scripts ( command injection ); loading of user-specified, possibly heavy or dangerous classes at run time ( reflection injection ); access to arbitrary files on the file system ( path-traversal ); and storing user-provided data into heap regions normally assumed to be shielded from the outside world ( trust boundary violation ). All these attacks exploit the same weakness: unconstrained propagation of data from sources that the user of a web application controls into sinks whose activation might trigger dangerous operations. Although web applications are written in a variety of languages, Java remains a frequent choice, in particular for banking applications, where security has tangible relevance. This article defines a unified, sound protection mechanism against such attacks, based on the identification of all possible explicit flows of tainted data in Java code. Such flows can be arbitrarily complex, passing through dynamically allocated data structures in the heap. The analysis is based on abstract interpretation and is interprocedural, flow-sensitive, and context-sensitive. Its notion of taint applies to reference (non-primitive) types dynamically allocated in the heap and is object-sensitive and field-sensitive. The analysis works by translating the program into Boolean formulas that model all possible data flows. Its implementation, within the Julia analyzer for Java and Android, found injection security vulnerabilities in the Internet banking service and in the customer relationship management of large Italian banks, as well as in a set of open-source third-party applications. It found the command injection, which is at the origin of the 2017 Equifax data breach, one of the worst data breaches ever. For objective, repeatable results, this article also evaluates the implementation on two open-source security benchmarks: the Juliet Suite and the OWASP Benchmark for the automatic comparison of static analyzers for cybersecurity. We compared this technique against more than 10 other static analyzers, both free and commercial. The result of these experiments is that ours is the only analysis for injection that is sound (up to well-stated limitations such as multithreading and native code) and works on industrial code, and it is also much more precise than other tools.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2955471678",
    "type": "article"
  },
  {
    "title": "The Truth, The Whole Truth, and Nothing But the Truth",
    "doi": "https://doi.org/10.1145/2983574",
    "publication_date": "2016-10-13",
    "publication_year": 2016,
    "authors": "Stephen M. Blackburn; Amer Diwan; Matthias Hauswirth; Peter F. Sweeney; José Nelson Amaral; Tim Brecht; Lubomír Bulej; Cliff Click; Lieven Eeckhout; Sebastian Fischmeister; Daniel Frampton; Laurie Hendren; Michael Hind; Antony L. Hosking; Richard Jones; Tomáš Kalibera; Nathan Keynes; Nathaniel Nystrom; Andreas Zeller",
    "corresponding_authors": "",
    "abstract": "An unsound claim can misdirect a field, encouraging the pursuit of unworthy ideas and the abandonment of promising ideas. An inadequate description of a claim can make it difficult to reason about the claim, for example, to determine whether the claim is sound. Many practitioners will acknowledge the threat of unsound claims or inadequate descriptions of claims to their field. We believe that this situation is exacerbated, and even encouraged, by the lack of a systematic approach to exploring, exposing, and addressing the source of unsound claims and poor exposition. This article proposes a framework that identifies three sins of reasoning that lead to unsound claims and two sins of exposition that lead to poorly described claims and evaluations. Sins of exposition obfuscate the objective of determining whether or not a claim is sound, while sins of reasoning lead directly to unsound claims. Our framework provides practitioners with a principled way of critiquing the integrity of their own work and the work of others. We hope that this will help individuals conduct better science and encourage a cultural shift in our research community to identify and promulgate sound claims.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2465422497",
    "type": "article"
  },
  {
    "title": "Rethinking Incremental and Parallel Pointer Analysis",
    "doi": "https://doi.org/10.1145/3293606",
    "publication_date": "2019-03-01",
    "publication_year": 2019,
    "authors": "Bozhen Liu; Jeff Huang; Lawrence Rauchwerger",
    "corresponding_authors": "",
    "abstract": "Pointer analysis is at the heart of most interprocedural program analyses. However, scaling pointer analysis to large programs is extremely challenging. In this article, we study incremental pointer analysis and present a new algorithm for computing the points-to information incrementally (i.e., upon code insertion, deletion, and modification). Underpinned by new observations of incremental pointer analysis, our algorithm significantly advances the state of the art in that it avoids redundant computations and the expensive graph reachability analysis, and preserves precision as the corresponding whole program exhaustive analysis. Moreover, it is parallel within each iteration of fixed-point computation. We have implemented our algorithm, IPA, for Java based on the WALA framework and evaluated its performance extensively on real-world large, complex applications. Experimental results show that IPA achieves more than 200X speedups over existing incremental algorithms, two to five orders of magnitude faster than whole program pointer analysis, and also improves the performance of an incremental data race detector by orders of magnitude. Our IPA implementation is open source and has been adopted by WALA.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2920740222",
    "type": "article"
  },
  {
    "title": "Modular Product Programs",
    "doi": "https://doi.org/10.1145/3324783",
    "publication_date": "2019-11-21",
    "publication_year": 2019,
    "authors": "Marco Eilers; Péter Müller; Samuel Hitz",
    "corresponding_authors": "",
    "abstract": "Many interesting program properties like determinism or information flow security are hyperproperties, that is, they relate multiple executions of the same program. Hyperproperties can be verified using relational logics, but these logics require dedicated tool support and are difficult to automate. Alternatively, constructions such as self-composition represent multiple executions of a program by one product program, thereby reducing hyperproperties of the original program to trace properties of the product. However, existing constructions do not fully support procedure specifications, for instance, to derive the determinism of a caller from the determinism of a callee, making verification non-modular. We present modular product programs, a novel kind of product program that permits hyperproperties in procedure specifications and, thus, can reason about calls modularly. We provide a general formalization of our product construction and prove it sound and complete. We demonstrate its expressiveness by applying it to information flow security with advanced features such as declassification and termination-sensitivity. Modular product programs can be verified using off-the-shelf verifiers; we have implemented our approach for both secure information flow and general hyperproperties using the Viper verification infrastructure. Our evaluation demonstrates that modular product programs can be used to prove hyperproperties for challenging examples in reasonable time.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2797766813",
    "type": "article"
  },
  {
    "title": "Ranking and Repulsing Supermartingales for Reachability in Randomized Programs",
    "doi": "https://doi.org/10.1145/3450967",
    "publication_date": "2021-06-08",
    "publication_year": 2021,
    "authors": "Toru Takisaka; Yuichiro Oyabu; Natsuki Urabe; Ichiro Hasuo",
    "corresponding_authors": "",
    "abstract": "Computing reachability probabilities is a fundamental problem in the analysis of randomized programs. This article aims at a comprehensive and comparative account of various martingale-based methods for over- and under-approximating reachability probabilities. Based on the existing works that stretch across different communities (formal verification, control theory, etc.), we offer a unifying account. In particular, we emphasize the role of order-theoretic fixed points—a classic topic in computer science—in the analysis of randomized programs. This leads us to two new martingale-based techniques, too. We also make an experimental comparison using our implementation of template-based synthesis algorithms for those martingales.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3171209508",
    "type": "article"
  },
  {
    "title": "What’s Decidable About Causally Consistent Shared Memory?",
    "doi": "https://doi.org/10.1145/3505273",
    "publication_date": "2022-04-06",
    "publication_year": 2022,
    "authors": "Ori Lahav; Udi Boker",
    "corresponding_authors": "",
    "abstract": "While causal consistency is one of the most fundamental consistency models weaker than sequential consistency, the decidability of safety verification for (finite-state) concurrent programs running under causally consistent shared memories is still unclear. In this article, we establish the decidability of this problem for two standard and well-studied variants of causal consistency. To do so, for each variant, we develop an equivalent “lossy” operational semantics, whose states track possible futures, rather than more standard semantics that record the history of the execution. We show that these semantics constitute well-structured transition systems, thus enabling decidable verification. Based on a key observation, which we call the “shared-memory causality principle,” the two novel semantics may also be of independent use in the investigation of weakly consistent models and their verification. Interestingly, our results are in contrast to the undecidability of this problem under the Release/Acquire fragment of the C/C++11 memory model, which forms another variant of causally consistent memory that, in terms of allowed outcomes, lies strictly between the two models studied here. Nevertheless, we show that all these three variants coincide for write/write-race-free programs, which implies the decidability of verification for such programs under Release/Acquire.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4226367879",
    "type": "article"
  },
  {
    "title": "Locally Abstract, Globally Concrete Semantics of Concurrent Programming Languages",
    "doi": "https://doi.org/10.1145/3648439",
    "publication_date": "2024-02-16",
    "publication_year": 2024,
    "authors": "Crystal Chang Din; Reiner Hähnle; Ludovic Henrio; Einar Broch Johnsen; Violet Ka I Pun; Silvia Lizeth Tapia Tarifa",
    "corresponding_authors": "",
    "abstract": "Formal, mathematically rigorous programming language semantics are the essential prerequisite for the design of logics and calculi that permit automated reasoning about concurrent programs. We propose a novel modular semantics designed to align smoothly with program logics used in deductive verification and formal specification of concurrent programs. Our semantics separates local evaluation of expressions and statements performed in an abstract, symbolic environment from their composition into global computations, at which point they are concretised. This makes incremental addition of new language concepts possible, without the need to revise the framework. The basis is a generalisation of the notion of a program trace as a sequence of evolving states that we enrich with event descriptors and trailing continuation markers. This allows to postpone scheduling constraints from the level of local evaluation to the global composition stage, where well-formedness predicates over the event structure declaratively characterise a wide range of concurrency models. We also illustrate how a sound program logic and calculus can be defined for this semantics.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391879488",
    "type": "article"
  },
  {
    "title": "Hierarchical modularity",
    "doi": "https://doi.org/10.1145/325478.325518",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "M. Blume; Andrew W. Appel",
    "corresponding_authors": "",
    "abstract": "To cope with the complexity of very large systems, it is not sufficient to divide them into simple pieces because the pieces themselves will either be too numerous or too large. A hierarchical modular structure is the natural solution. In this article we explain how that approach can be applied to software. Our compilation manager provides a language for specifying where individual modules fit into a hierarchy and how they are related semantically. We pay particular attention to the structure of the global name space of program identifiers that are used for module linkage because any potential for name clashes between otherwise unrelated parts of a program can negatively affect modularity. We discuss the theoretical issues in building software hierarchically, and we describe our implementation of CM, the compilation manager for Standard ML of New Jersey.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2295347924",
    "type": "article"
  },
  {
    "title": "An automata-theoretic approach to modular model checking",
    "doi": "https://doi.org/10.1145/345099.345104",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Orna Kupferman; Moshe Y. Vardi",
    "corresponding_authors": "",
    "abstract": "In modular verification the specification of a module consists of two part. One part describes the guaranteed behavior of the module. The other part describes the assumed behavior of the system in which the module is interacting. This is called the assume-guarantee paradigm. In this paper we consider assume-guarantee specifications in which the guarantee is specified by branching temporal formulas. We distinguish between two approaches. In the first approach, the assumption is specified by branching temporal formulas too. In the second approach, the assumption is specified by linear temporal logic. We consider guarantees in ∀CTL, and ∀CTL*. We develop two fundamental techniques: building maximal models for ∀CTL and ∀CTL* formulas and using alternating automata to obtain space-efficient algorithms for fair model checking. Using these techniques we classify the complexity of satisfiability, validity, implication, and modular verification for ∀CTL and ∀CTL*. We show that modular verification is PSPACE-complete for ∀CTL and is EXSPACE-complete for ∀CTL*. We prove that when the assumption is linear, these bounds hold also for guarantees in CTL and CTL*. On the other hand, the problem remains EXSPACE-hard even when we restrict the assumptions to LTL and take the guarantees as a fixed ∀CTL formula.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2037416037",
    "type": "article"
  },
  {
    "title": "Identifying loops in almost linear time",
    "doi": "https://doi.org/10.1145/316686.316687",
    "publication_date": "1999-03-01",
    "publication_year": 1999,
    "authors": "G. Ramalingam",
    "corresponding_authors": "G. Ramalingam",
    "abstract": "Loop identification is an essential step in performing various loop optimizations and transformations. The classical algorithm for identifying loops is Tarjan's interval-finding algorithm, which is restricted to reducible graphs. More recently, serveral people have proposed extensions to Tarjan's algortihm to deal with irreducible graphs. Havlak presents one such extension, which constructs a loop-nesting forest for an arbitrary flow graph. We show that the running time of this algorithm is quadratic in the worst-case, and not almost linear as claimed. We then show how to modify the algorithm to make it run in almost linear time. We next consider the quadratic algorithm presented by Sreedhar et al. which constructs a loop-nesting forest different from the one constructed by Havlak algorithm. We show that this algorithm too can be adapted to run in almost linear time. We finally consider an algorithm due to Steensgaard, which constructs yet antoher loop-nesting forest. We show how this algorithm can be made more efficient by borrowing ideas from the other algorithms discussed earlier.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W1990854931",
    "type": "article"
  },
  {
    "title": "Alma-O",
    "doi": "https://doi.org/10.1145/293677.293679",
    "publication_date": "1998-09-01",
    "publication_year": 1998,
    "authors": "Krzysztof R. Apt; Jacob Brunekreef; Vincent Partington; Andrea Schaerf",
    "corresponding_authors": "",
    "abstract": "We describe here an implemented small programming language, called Alma-O, that augments the expressive power of imperative programming by a limited number of features inspired by the logic programming paradigm. These additions encourage declarative programming and make it a more attractive vehicle for problems that involve search. We illustrate the use of Alma-O by presenting solutions to a number of classical problems, including α-β search, STRIPS planning, knapsack, and Eight Queens. These solutions are substantially simpler than their counterparts written in the imperative or in the logic programming style and can be used for different purposes without any modification. We also discuss here the implementation of Alma-O and an operational, executable, semantics of a large subset of the language.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W1968265180",
    "type": "article"
  },
  {
    "title": "Eta-expansion does The Trick",
    "doi": "https://doi.org/10.1145/236114.236119",
    "publication_date": "1996-11-01",
    "publication_year": 1996,
    "authors": "Olivier Danvy; Karoline Malmkjær; Jens Palsberg",
    "corresponding_authors": "",
    "abstract": "Partial-evaluation folklore has it that massaging one's source programs can make them specialize better. In Jones, Gomard, and Sestoft's recent textbook, a whole chapter is dedicated to listing such “binding-time improvements”: nonstandard use of continuation-passing style, eta-expansion, and a popular transformation called “The Trick.” We provide a unified view of these binding-time improvements, from a typing perspective. Just as a proper treatment of product values in partial evaluation requires partially static values, a proper treatment of disjoint sums requires moving static contexts across dynamic case expressions. This requirement precisely accounts for the nonstandard use of continuation-passing style encountered in partial evaluation. Eta-expansion thus acts as a uniform binding-time coercion between values and contexts, be they of function type, product type, or disjoint-sum type. For the latter case, it enables “The Trick.” In this article, we extend Gomard and Jones' partial evaluator for the λ-calculus, λ-Mix, with products and disjoint sums; we point out how eta-expansion for (finite) disjoint sums enable The Trick; we generalize our earlier work by identifying the eta-expansion can be obtained in the binding-time analysis simple by adding two coercion rules; and we specify and prove the correctness of our extension to λ-Mix.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2160249850",
    "type": "article"
  },
  {
    "title": "A logical model for relational abstract domains",
    "doi": "https://doi.org/10.1145/293677.293680",
    "publication_date": "1998-09-01",
    "publication_year": 1998,
    "authors": "Roberto Giacobazzi; Francesca Scozzari",
    "corresponding_authors": "",
    "abstract": "In this article we introduce the notion of Heyting completion in abstract interpretation. We prove that Heyting completion provides a model for Cousot's reduced cardinal power of abstract domains and that it supplies a logical basis to specify relational domains for program analysis and abstract interpretation. We study the algebraic properties of Heyting completion in relation with other well-known domain transformers, like reduced product and disjunctive completion. This provides a uniform algebraic setting where complex abstract domains can be specified by simple logic formulas, or as solutions of recursive abstract domain equations, involving few basic operations for domain construction, all characterized by a clean logical interpretation. We apply our framework to characterize directionality and condensing and in downward closed analysis of (constraint) logic programs.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W1968297379",
    "type": "article"
  },
  {
    "title": "Optimistic register coalescing",
    "doi": "https://doi.org/10.1145/1011508.1011512",
    "publication_date": "2004-07-01",
    "publication_year": 2004,
    "authors": "Jinpyo Park; Soo‐Mook Moon",
    "corresponding_authors": "",
    "abstract": "Graph-coloring register allocators eliminate copies by coalescing the source and target nodes of a copy if they do not interfere in the interference graph. Coalescing, however, can be harmful to the colorability of the graph because it tends to yield a graph with nodes of higher degrees. Unlike aggressive coalescing , which coalesces any pair of noninterfering copy-related nodes, conservative coalescing or iterated coalescing perform safe coalescing that preserves the colorability. Unfortunately, these heuristics give up coalescing too early, losing many opportunities for coalescing that would turn out to be safe. Moreover, they ignore the fact that coalescing may even improve the colorability of the graph by reducing the degree of neighbor nodes that are interfering with both the source and target nodes being coalesced. This article proposes a new heuristic called optimistic coalescing which optimistically performs aggressive coalescing, thus exploiting the positive impact of coalescing aggressively, but when a coalesced node is to be spilled, it is split back into separate nodes. Since there is a better chance of coloring one of those splits, we can reduce the overall spill amount.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2003672926",
    "type": "article"
  },
  {
    "title": "Graph rewrite systems for program optimization",
    "doi": "https://doi.org/10.1145/363911.363914",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "Uwe Aßmann",
    "corresponding_authors": "Uwe Aßmann",
    "abstract": "Graph rewrite systems can be used to specify and generate program optimizations. For termination of the systems several rule-based criteria are developed, defining exhaustive graph rewrite systems . For nondeterministic systems stratification is introduced which automatically selects single normal forms. To illustrate how far the methodology reaches, parts of the lazy code motion optimization are specified. The resulting graph rewrite system classes can be evaluated by a uniform algorithm, which forms the basis for the optimizer generator OPTIMIX. With this tool several optimizer components have been generated, and some numbers on their speed are presented.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2139212997",
    "type": "article"
  },
  {
    "title": "A generalized theory of bit vector data flow analysis",
    "doi": "https://doi.org/10.1145/186025.186043",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Uday P. Khedker; Dhananjay M. Dhamdhere",
    "corresponding_authors": "",
    "abstract": "The classical theory of data flow analysis, which has its roots in unidirectional flows, is inadequate to characterize bidirectional data flow problems. We present a generalized theory of bit vector data flow analysis which explains the known results in unidirectional and bidirectional data flows and provides a deeper insight into the process of data flow analysis. Based on the theory, we develop a worklist-based generic algorithm which is uniformly applicable to unidirectional and bidirectional data flow problems. It is simple, versatile, and easy to adapt for a specific problem. We show that the theory and the algorithm are applicable to all bounded monotone data flow problems which possess the property of the separability of solution. The theory yields valuable information about the complexity of data flow analysis. We show that the complexity of worklist-based iterative analysis is the same for unidirectional and bidirectional problems. We also define a measure of the complexity of round-robin iterative analysis. This measure, called width , is uniformly applicable to unidirectional and bidirectional problems and provides a tighter bound for unidirectional problems than the traditional measure of depth . Other applications include explanation of isolated results in efficient solution techniques and motivation of new techniques for bidirectional flows. In particular, we discuss edge splitting and edge placement and develop a feasibility criterion for decomposition of a bidirectional flow into a sequence of unidirectional flows.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W1991158186",
    "type": "article"
  },
  {
    "title": "Debugging optimized code without being misled",
    "doi": "https://doi.org/10.1145/177492.177517",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Max Copperman",
    "corresponding_authors": "Max Copperman",
    "abstract": "Correct optimization can change the behavior of an incorrect program; therefore at times it is necessary to debug optimized code. However, optimizing compilers produce code that impedes source-level debugging. Optimization can cause an inconsistency between where the user expects a breakpoint to be located and the breakpoint's actual location. This article describes a mapping between statements and breakpoint locations that ameliorates this problem. The mapping enables debugger behavior on optimized code that approximates debugger behavior on unoptimized code sufficiently closely for the user to use traditional debugging strategies. Optimization can also cause the value of a variable to be noncurrent —to differ from the value that would be predicted by a close reading of the source code. This article presents a method of determining when this has occurred, and shows how a debugger can describe the relevant effects of optimization. The determination method is more general than previously published methods; it handles global optimization and many flow graph transformations, and it is not tightly coupled to optimizations performed by a particular compiler. Necessary compiler support is also described.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2017616301",
    "type": "article"
  },
  {
    "title": "Efficient and flexible incremental parsing",
    "doi": "https://doi.org/10.1145/293677.293678",
    "publication_date": "1998-09-01",
    "publication_year": 1998,
    "authors": "Tim A. Wagner; Susan L. Graham",
    "corresponding_authors": "",
    "abstract": "Previously published algorithms for LR ( k ) incremental parsing are inefficient, unnecessarily restrictive, and in some cases incorrect. We present a simple algorithm based on parsing LR( k ) sentential forms that can incrementally parse an arbitrary number of textual and/or structural modifications in optimal time and with no storage overhead. The central role of balanced sequences in achieving truly incremental behavior from analysis algorithms is described, along with automated methods to support balancing during parse table generation and parsing. Our approach extends the theory of sentential-form parsing to allow for ambiguity in the grammar, exploiting it for notational convenience, to denote sequences, and to construct compact (“abstract”) syntax trees directly. Combined, these techniques make the use of automatically generated incremental parsers in interactive software development environments both practical and effective. In addition, we address information preservation in these environments: Optimal node reuse is defined; previous definitions are shown to be insufficient; and a method for detecting node reuse is provided that is both simpler and faster than existing techniques. A program representation based on self-versioning documents is used to detect changes in the program, generate efficient change reports for subsequent analyses, and allow the parsing transformation itself to be treated as a reversible modification in the edit log.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2020749411",
    "type": "article"
  },
  {
    "title": "Parameterized partial evaluation",
    "doi": "https://doi.org/10.1145/169683.174155",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Charles Consel; Siau Cheng Khoo",
    "corresponding_authors": "",
    "abstract": "article Free AccessParameterized partial evaluation Authors: Charles Consel Yale University Yale UniversityView Profile , Siau Cheng Khoo Yale University Yale UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 3pp 463–493https://doi.org/10.1145/169683.174155Published:01 July 1993Publication History 47citation352DownloadsMetricsTotal Citations47Total Downloads352Last 12 Months11Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2041655838",
    "type": "article"
  },
  {
    "title": "Fusion-based register allocation",
    "doi": "https://doi.org/10.1145/353926.353929",
    "publication_date": "2000-05-01",
    "publication_year": 2000,
    "authors": "Guei-Yuan Lueh; Thomas R. Gross; Ali-Reza Adl-Tabatabai",
    "corresponding_authors": "",
    "abstract": "The register allocation phase of a compiler maps live ranges of a program to registers. If there are more candidates than there are physical registers, the register allocator must spill a live range (the home location is in memory) or split a live range (the live range occupies multiple locations). One of the challenges for a register allocator is to deal with spilling and splitting together. Fusion-based register allocation uses the structure of the program to make splitting and spilling decisions, with the goal to move overhead operations to infrequently executed parts of a program. The basic idea of fusion-based register allocation is to build up the interference graph. Starting with some base region (e.g., a basic block, a loop), the register allocator adds basic blocks to the region and incrementallly builds the interference graph. When there are more live ranges than registers, the register allocator selects live ranges to split; these live ranges are split along the edge that was most recently added to the region. This article describes fusion-based register allocation in detail and compares it with other approaches to register allocation. For programs from the SPEC92 suite, fusion-based register allocation can improve the execution time (of optimized programs, for the MIPS architecture) by up to 8.4% over Chaitin-style register allocation.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2065941861",
    "type": "article"
  },
  {
    "title": "A self-applicable partial evaluator for the lambda calculus",
    "doi": "https://doi.org/10.1145/128861.128864",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "Carsten K. Gomard",
    "corresponding_authors": "Carsten K. Gomard",
    "abstract": "We describe theoretical and a few practical aspects of an implemented self-applicable partial evaluator for the untyped lambda calculus with constants, conditionals, and a fixed point operator. The purpose of this paper is first to announce the existence of (and to describe) a partial evaluator that is both higher-order and self-applicable; second to describe a surprisingly simple solution to the central problem of binding time analysis, and third to prove that the partial evaluator yields correct answers. While λ-mix (the name of our system) seems to have been the first higher-order self-applicable partial evaluator to run on a computer, it was developed mainly for research purposes. Two recently developed systems are much more powerful for practical use, but also much more complex: Similix[3,5] and Schism[7]. Our partial evaluator is surprisingly simple, completely automatic, and has been implemented in a side effect-free subset of Scheme. It has been used to compile, generate compilers and generate a compiler generator.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2110656424",
    "type": "article"
  },
  {
    "title": "On the usefulness of type and liveness accuracy for garbage collection and leak detection",
    "doi": "https://doi.org/10.1145/586088.586089",
    "publication_date": "2002-11-01",
    "publication_year": 2002,
    "authors": "Martin Hirzel; Amer Diwan; Johannes Henkel",
    "corresponding_authors": "",
    "abstract": "The effectiveness of garbage collectors and leak detectors in identifying dead objects depends on the accuracy of their reachability traversal. Accuracy has two orthogonal dimensions: (i) whether the reachability traversal can distinguish between pointers and nonpointers ( type accuracy ), and (ii) whether the reachability traversal can identify memory locations that will be dereferenced in the future ( liveness accuracy ). This article presents an experimental study of the importance of type and liveness accuracy for reachability traversals. We show that liveness accuracy reduces the reachable heap size by up to 62% for our benchmark programs. However, the simpler liveness schemes (e.g., intraprocedural analysis of local variables) are largely ineffective for our benchmark runs: one must analyze global variables using interprocedural analysis to obtain significant benefits. Type accuracy has an insignificant impact on a garbage collector's ability to find unreachable objects in our benchmark runs. We report results for programs written in C, C++, and Eiffel.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2169275155",
    "type": "article"
  },
  {
    "title": "The pattern calculus",
    "doi": "https://doi.org/10.1145/1034774.1034775",
    "publication_date": "2004-11-01",
    "publication_year": 2004,
    "authors": "C. Barry Jay",
    "corresponding_authors": "C. Barry Jay",
    "abstract": "There is a significant class of operations such as mapping that are common to all data structures. The goal of generic programming is to support these operations on arbitrary data types without having to recode for each new type. The pattern calculus and combinatory type system reach this goal by representing each data structure as a combination of names and a finite set of constructors. These can be used to define generic functions by pattern-matching programs in which each pattern has a different type. Evaluation is type-free. Polymorphism is captured by quantifying over type variables that represent unknown structures. A practical type inference algorithm is provided.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W1987975642",
    "type": "article"
  },
  {
    "title": "A bounded first-in, first-enabled solution to the <i>l</i> -exclusion problem",
    "doi": "https://doi.org/10.1145/177492.177731",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Yehuda Afek; Danny Dolev; Eli Gafni; Michael Merritt; Nir Shavit",
    "corresponding_authors": "",
    "abstract": "This article presents a solution to the first-come, first-enabled ℓ-exclusion problem of Fischer et al. [1979]. Unlike their solution, this solution does not use powerful read-modify-write synchronization primitives and requires only bounded shared memory. Use of the concurrent timestamp system of Dolev and Shavir [1989] is key in solving the problem within bounded shared memory.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2022086264",
    "type": "article"
  },
  {
    "title": "Making graphs reducible with controlled node splitting",
    "doi": "https://doi.org/10.1145/267959.269971",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "J. P. F. M. Janssen; Henk Corporaal",
    "corresponding_authors": "",
    "abstract": "Several compiler optimizations, such as data flow analysis, the exploitation of instruction-level parallelism (ILP), loop transformations, and memory disambiguation, require programs with reducible control flow graphs. However, not all programs satisfy this property. A new method for transforming irreducible control flow graphs to reducible control flow graphs, called Controlled Node Splitting (CNS), is presented. CNS duplicates nodes of the control flow graph to obtain reducible control flow graphs. CNS results in a minimum number of splits and a minimum number of duplicates. Since the computation time to find the optimal split sequence is large, a heuristic has been developed. The results of this heuristic are close to the optimum. Straightforward application of node splitting resulted in an average code size increase of 235% per procedure of our benchmark programs. CNS with the heuristic limits this increase to only 3%. The impact on the total code size of the complete programs is 13.6% for a straightforward application of node splitting. However, when CNS is used, with the heuristic the average growth in code size of a complete program dramatically reduces to 0.2%",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2078508340",
    "type": "article"
  },
  {
    "title": "A method for specializing logic programs",
    "doi": "https://doi.org/10.1145/78942.78947",
    "publication_date": "1990-04-01",
    "publication_year": 1990,
    "authors": "Annalisa Bossi; Nicoletta Cocco; Susi Dulli",
    "corresponding_authors": "",
    "abstract": "A specialization method for logic programs that allows one to restrict a general program to special cases by means of constraint predicates is presented. A set of basic transformation operations, which are shown to produce equivalent programs, is defined. The method uses these operations for propagating the constraint information through the program and for consequently simplifying it whenever possible. Some examples of specializations are given, and some improvements and developments of the method are discussed.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2081484419",
    "type": "article"
  },
  {
    "title": "On the occur-check-free PROLOG programs",
    "doi": "https://doi.org/10.1145/177492.177673",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Krzysztof R. Apt; Alessandro Pellegrini",
    "corresponding_authors": "",
    "abstract": "In most PROLOG implementations, for efficiency occur-check is omitted from the unification algorithm. This paper provides natural syntactic conditions that allow the occur-check to be safely omitted. The established results apply to most well-known PROLOG programs, including those that use difference lists, and seem to explain why this omission does not lead in practice to any complications. When applying these results to general programs, we show their usefulness for proving absence of floundering. Finally, we propose a program transformation that transforms every program into a program for which only the calls to the built-in unification predicate need to be resolved by a unification algorithm with the occur-check.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2152685301",
    "type": "article"
  },
  {
    "title": "The KaffeOS Java runtime system",
    "doi": "https://doi.org/10.1145/1075382.1075383",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Godmar Back; Wilson C. Hsieh",
    "corresponding_authors": "",
    "abstract": "Single-language runtime systems, in the form of Java virtual machines, are widely deployed platforms for executing untrusted mobile code. These runtimes provide some of the features that operating systems provide: interapplication memory protection and basic system services. They do not, however, provide the ability to isolate applications from each other. Neither do they provide the ability to limit the resource consumption of applications. Consequently, the performance of current systems degrades severely in the presence of malicious or buggy code that exhibits ill-behaved resource usage. We show that Java runtime systems can be extended to support processes , and that processes can provide robust and efficient support for untrusted applications.We have designed and built KaffeOS, a Java runtime system that provides support for processes. KaffeOS isolates processes and manages the physical resources available to them: CPU and memory. Unlike existing Java virtual machines, KaffeOS can safely terminate processes without adversely affecting the integrity of the system, and it can fully reclaim a terminated process's resources. Finally, KaffeOS requires no changes to the Java language. The novel aspects of the KaffeOS architecture include the application of a user/kernel boundary as a structuring principle for runtime systems, the employment of garbage collection techniques for resource management and isolation, and a model for direct sharing of objects between untrusted applications. The difficulty in designing KaffeOS lay in balancing the goals of isolation and resource management against the goal of allowing direct sharing of objects.For the SpecJVM benchmarks, the overhead that our KaffeOS prototype incurs ranges from 0% to 25%, when compared to the open-source JVM on which it is based. We consider this overhead acceptable for the safety that KaffeOS provides. In addition, our KaffeOS prototype can scale to run more applications than running multiple JVMs. Finally, in the presence of malicious or buggy code that engages in a denial-of-service attack, KaffeOS can contain the attack, remove resources from the attacked applications, and continue to provide robust service to other clients.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W1979651969",
    "type": "article"
  },
  {
    "title": "The Euclidean definition of the functions div and mod",
    "doi": "https://doi.org/10.1145/128861.128862",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "Raymond Boute",
    "corresponding_authors": "Raymond Boute",
    "abstract": "The definitions of the functions div and mod in the computer science literature and in programming languages are either similar to the Algol of Pascal definition (which is shown to be an unfortunate choice) or based on division by truncation (T-definition) or division by flooring as defined by Knuth (F-definition). The differences between various definitions that are in common usage are discussed, and an additional one is proposed, which is based on Euclid's theorem and therefore is called the Euclidean definition (E-definition). Its distinguishing feature is that 0 ≤ D mod d &lt; | d | irrespective of the signs of D and d . It is argued that the E- and F-definitions are superior to all other ones in regularity and useful mathematical properties and hence deserve serious consideration as the standard convention at the applications and language level. It is also shown that these definitions are the most suitable ones for describing number representation systems and the realization of arithmetic operations at the architecture and hardware level.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2083861585",
    "type": "article"
  },
  {
    "title": "Software merge",
    "doi": "https://doi.org/10.1145/197320.197403",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Valdis Bērziņš",
    "corresponding_authors": "Valdis Bērziņš",
    "abstract": "We present a language-independent semantic model of the process of combining changes to programs. This model extends the domains used in denotational semantics (complete partial orders) to Boolean algebras, and represents incompatible modifications as well as compatible extensions. The model is used to define the intended semantics of change-merging operations on programs and to establish some general properties of software merging. We determine conditions under which changes to subprograms of a software system can be merged independently and illustrate cases where this is not possible.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2151803774",
    "type": "article"
  },
  {
    "title": "A simple interprocedural register allocation algorithm and its effectiveness for LISP",
    "doi": "https://doi.org/10.1145/59287.59289",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "Peter Steenkiste; John L. Hennessy",
    "corresponding_authors": "",
    "abstract": "Register allocation is an important optimization in many compilers, but with per-procedure register allocation, it is often not possible to make good use of a large register set. Procedure calls limit the improvement from global register allocation, since they force variables allocated to registers to be saved and restored. This limitation is more pronounced in LISP programs due to the higher frequency of procedure calls. An interprocedural register allocation algorithm is developed by simplifying a version of interprocedural graph coloring. The simplification corresponds to a bottom-up coloring of the interference graph. The scheme is evaluated using a number of LISP programs. The evaluation considers the scheme's limitations and compares these “software register windows” against the hardware register windows used in the Berkeley RISC and SPUR processors.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W1976147675",
    "type": "article"
  },
  {
    "title": "A region-based compilation technique for dynamic compilers",
    "doi": "https://doi.org/10.1145/1111596.1111600",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Toshio Suganuma; Toshiaki Yasue; Toshio Nakatani",
    "corresponding_authors": "",
    "abstract": "Method inlining and data flow analysis are two major optimization components for effective program transformations, but they often suffer from the existence of rarely or never executed code contained in the target method. One major problem lies in the assumption that the compilation unit is partitioned at method boundaries. This article describes the design and implementation of a region-based compilation technique in our dynamic optimization framework, in which the compiled regions are selected as code portions without rarely executed code. The key parts of this technique are the region selection, partial inlining, and region exit handling. For region selection, we employ both static heuristics and dynamic profiles to identify and eliminate rare sections of code. The region selection process and method inlining decisions are interwoven, so that method inlining exposes other targets for region selection, while the region selection in the inline target conserves the inlining budget, allowing more method inlining to be performed. The inlining process can be performed for parts of a method, not just for the entire body of the method. When the program attempts to exit from a region boundary, we trigger recompilation and then use on-stack replacement to continue the execution from the corresponding entry point in the recompiled code. We have implemented these techniques in our Java JIT compiler, and conducted a comprehensive evaluation. The experimental results show that our region-based compilation approach achieves approximately 4% performance improvement on average, while reducing the compilation overhead by 10% to 30%, in comparison to the traditional method-based compilation techniques.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2037823608",
    "type": "article"
  },
  {
    "title": "A tail-recursive machine with stack inspection",
    "doi": "https://doi.org/10.1145/1034774.1034778",
    "publication_date": "2004-11-01",
    "publication_year": 2004,
    "authors": "John Clements; Matthias Felleisen",
    "corresponding_authors": "",
    "abstract": "Security folklore holds that a security mechanism based on stack inspection is incompatible with a global tail call optimization policy; that an implementation of such a language must allocate memory for a source-code tail call, and a program that uses only tail calls (and no other memory-allocating construct) may nevertheless exhaust the available memory. In this article, we prove this widely held belief wrong. We exhibit an abstract machine for a language with security stack inspection whose space consumption function is equivalent to that of the canonical tail call optimizing abstract machine. Our machine is surprisingly simple and suggests that tail calls are as easy to implement in a security setting as they are in a conventional one.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2116918412",
    "type": "article"
  },
  {
    "title": "Adding liveness properties to coupled finite-state machines",
    "doi": "https://doi.org/10.1145/78942.78948",
    "publication_date": "1990-04-01",
    "publication_year": 1990,
    "authors": "Shubhani Aggarwal; Costas Courcoubetis; Pierre Wolper",
    "corresponding_authors": "",
    "abstract": "Informal specifications of protocols are often imprecise and incomplete and are usually not sufficient to ensure the correctness of even very simple protocols. Consequently, formal specification methods, such as finite-state models, are increasingly being used. The selection/resolution (S/R) model is a finite-state model with a powerful communication mechanism that makes it easy to describe complex protocols as a collection of simple finite-state machines. A software environment, called SPANNER, has been developed to specify and analyze protocols specified with the S/R model. SPANNER provides the facility to compute the joint behavior of a number of finite-state machines and to check if the “product” machine has inaccessible states, states corresponding to deadlocks, and loops corresponding to livelocks. So far, however, SPANNER has had no facility to systematically deal with liveness conditions. For example, one might wish to specify that, although a communication channel is unreliable, a message will get through if it is sent infinitely often, and to check that the infinite behavior of the protocol viewed as an infinite sequence will always be in some ω-regular set (possibly specified in terms of a formula in temporal logic or as an ω-automata). In this paper we show that with very minor modifications to the implemented system it is possible to substantially extend the type of properties that can be specified and checked by SPANNER. This is done by extending the S/R model to include acceptance conditions found in automatons on infinite words, which permits the incorporation of arbitrary liveness conditions into the model. We show how these extensions can be easily incorporated into SPANNER (and into essentially any finite-state verification system) and how the resulting system is used to automatically verify the correctness of protocols.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W1991956281",
    "type": "article"
  },
  {
    "title": "On Lamport's interprocessor communication model",
    "doi": "https://doi.org/10.1145/65979.65982",
    "publication_date": "1989-07-01",
    "publication_year": 1989,
    "authors": "Frank D. Anger",
    "corresponding_authors": "Frank D. Anger",
    "abstract": "Leslie Lamport presented a set of axioms in 1979 that capture the essential properties of the temporal relationships between complex and perhaps unspecified activities within any system, and proceeded to use this axiom system to prove the correctness of sophisticated algorithms for reliable communication and mutual exclusion in systems without shared memory. As a step toward a more complete metatheory of Lamport's axiom system, this paper determines the extent to which that system differs from systems based on “atomic,” or indivisible, actions. Theorem 1 shows that only very weak conditions need be satisfied in addition to the given axioms to guarantee the existence of an atomic “model,” while Proposition 1 gives sufficient conditions under which any such model must be a “faithful” representation. Finally, Theorem 2 restates a result of Lamport showing exactly when a system can be thought of as made up of a set of atomic events that can be totally ordered temporally. A new constructive proof is offered for this result.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W1970655487",
    "type": "article"
  },
  {
    "title": "When do bounds and domain propagation lead to the same search space?",
    "doi": "https://doi.org/10.1145/1065887.1065889",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Christian Schulte; Peter J. Stuckey",
    "corresponding_authors": "",
    "abstract": "This article explores the question of when two propagation-based constraint systems have the same behavior, in terms of search space. We categorize the behavior of domain and bounds propagators for primitive constraints, and provide theorems that allow us to determine propagation behaviors for conjunctions of constraints. We then show how we can use this to analyze CLP(FD) programs to determine when we can safely replace domain propagators by more efficient bounds propagators without increasing search space. Empirical evaluation shows that programs optimized by the analysis' results are considerably more efficient.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2000188963",
    "type": "article"
  },
  {
    "title": "The geometry of semaphore programs",
    "doi": "https://doi.org/10.1145/9758.9759",
    "publication_date": "1987-01-01",
    "publication_year": 1987,
    "authors": "Scott D. Carson; Paul F. Reynolds",
    "corresponding_authors": "",
    "abstract": "Synchronization errors in concurrent programs are notoriously difficult to find and correct. Deadlock, partial deadlock, and unsafeness are conditions that constitute such errors. A model of concurrent semaphore programs based on multidimensional, solid geometry is presented. While previously reported geometric models are restricted to two-process mutual exclusion problems, the model described here applies to a broader class of synchronization problems. The model is shown to be exact for systems composed of an arbitrary, yet fixed number of concurrent processes, each consisting of a straight line sequence of arbitrarily ordered semaphore operations.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2064785956",
    "type": "article"
  },
  {
    "title": "A type system for certified binaries",
    "doi": "https://doi.org/10.1145/1053468.1053469",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Zhong Shao; Valery Trifonov; Bratin Saha; Nikolaos Papaspyrou",
    "corresponding_authors": "",
    "abstract": "A certified binary is a value together with a proof that the value satisfies a given specification. Existing compilers that generate certified code have focused on simple memory and control-flow safety rather than more advanced properties. In this article, we present a general framework for explicitly representing complex propositions and proofs in typed intermediate and assembly languages. The new framework allows us to reason about certified programs that involve effects while still maintaining decidable typechecking. We show how to integrate an entire proof system (the calculus of inductive constructions) into a compiler intermediate language and how the intermediate language can undergo complex transformations (CPS and closure conversion) while preserving proofs represented in the type system. Our work provides a foundation for the process of automatically generating certified binaries in a type-theoretic framework.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2135248810",
    "type": "article"
  },
  {
    "title": "The Type Theory of PL/CV3",
    "doi": "https://doi.org/10.1145/357233.357238",
    "publication_date": "1984-01-01",
    "publication_year": 1984,
    "authors": "Robert L. Constable; Daniel R. Zlatin",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on The Type Theory of PL/CV3 Authors: Robert L. Constable Department of Computer Science, 405 Upson Hall, Cornell University, Ithaca, N.Y. Department of Computer Science, 405 Upson Hall, Cornell University, Ithaca, N.Y.View Profile , Daniel R. Zlatin Department of Computer Science, 405 Upson Hall, Cornell University, Ithaca, N.Y. Department of Computer Science, 405 Upson Hall, Cornell University, Ithaca, N.Y.View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 1Jan. 1984 pp 94–117https://doi.org/10.1145/357233.357238Online:01 January 1984Publication History 25citation305DownloadsMetricsTotal Citations25Total Downloads305Last 12 Months12Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W1986382835",
    "type": "article"
  },
  {
    "title": "Slicing as a program transformation",
    "doi": "https://doi.org/10.1145/1216374.1216375",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Martin Ward; Hussein Zedan",
    "corresponding_authors": "",
    "abstract": "The aim of this article is to provide a unified mathematical framework for program slicing which places all slicing work for sequential programs on a sound theoretical foundation. The main advantage to a mathematical approach is that it is not tied to a particular representation. In fact the mathematics provides a sound basis for any particular representation. We use the WSL (wide-spectrum language) program transformation theory as our framework. Within this framework we define a new semantic relation, semirefinement , which lies between semantic equivalence and semantic refinement. Combining this semantic relation, a syntactic relation (called reduction ), and WSL's remove statement, we can give mathematical definitions for backwards slicing, conditioned slicing, static and dynamic slicing, and semantic slicing as program transformations in the WSL transformation theory. A novel technique of “encoding” operational semantics within a denotational semantics allows the framework to handle “operational slicing”. The theory also enables the concept of slicing to be applied to nondeterministic programs. These transformations are implemented in the industry-strength FermaT transformation system.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2030309884",
    "type": "article"
  },
  {
    "title": "Designing families of data types using exemplars",
    "doi": "https://doi.org/10.1145/63264.63265",
    "publication_date": "1989-04-01",
    "publication_year": 1989,
    "authors": "Wilf R. LaLonde",
    "corresponding_authors": "Wilf R. LaLonde",
    "abstract": "Designing data types in isolation is fundamentally different from designing them for integration into communities of data types, especially when inheritance is a fundamental issue. Moreover, we can distinguish between the design of families—integrated types that are variations of each other—and more general communities where totally different but cohesive collections of types support specific applications (e.g., a compiler). We are concerned with the design of integrated families of data types as opposed to individual data types; that is, on the issues that arise when the focus is intermediate between the design of individual data types and more general communities of data types. We argue that design at this level is not adequately served by systems providing only class-based inheritance hierarchies and that systems which additionally provide a coupled subtype specification hierarchy are still not adequate. We propose a system that provides an unlimited number of uncoupled specification hi erarchies and illustrate it with three: a subtype hierarchy, a specialization/generalization hierarchy, and a like hierarchy. We also resurrect a relatively unknown Smalltalk design methodology that we call programming-by-exemplars and argue that it is an important addition to a designer's grab bag of techniques. The methodology is used to show that the subtype hierarchy must be decoupled from the inheritance hierarchy, something that other researchers have also suggested. However, we do so in the context of exemplar-based systems to additionally show that they can already support the extensions required without modification and that they lead to a better separation between users and implementers, since classes and exemplars can be related in more flexible ways. We also suggest that class-based systems need the notion of private types if they are to surmount their current limitations. Our points are made in the guise of designing a family of List data types. Among these is a new variety of lists that have never been previously published: prefix-sharing lists. We also argue that there is a need for familial classes to serve as an intermediary between users and the members of a family.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2036169321",
    "type": "article"
  },
  {
    "title": "Program termination analysis in polynomial time",
    "doi": "https://doi.org/10.1145/1180475.1180480",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Amir M. Ben-Amram; Chin Soon Lee",
    "corresponding_authors": "",
    "abstract": "A size-change termination algorithm takes as input abstract information about a program in the form of size-change graphs and uses it to determine whether any infinite computation would imply that some data decrease in size infinitely. Since such an infinite descent is presumed impossible, this proves program termination. The property of the graphs that implies program termination is called SCT. There are many examples of practical programs whose termination can be verified by creating size-change graphs and testing them for SCT.The size-change graph abstraction is useful because the graphs often carry sufficient information to deduce termination, and at the same time are simple enough to be analyzed automatically. However, there is a tradeoff between the completeness and efficiency of this analysis, and complete algorithms in the literature can easily be pushed to an exponential combinatorial search by certain patterns in the graph structures.We therefore propose a novel algorithm to detect common forms of parameter-descent behavior efficiently. Specifically, we target lexicographic descent, multiset descent, and min- and max-descent. Our algorithm makes it possible to verify practical instances of SCT while guarding against unwarranted combinatorial search. It has worst-case time complexity cubic in the input size, and its effectiveness is demonstrated empirically using a test suite of over 90 programs.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2097944031",
    "type": "article"
  },
  {
    "title": "Synchronization of asynchronous processes in CSP",
    "doi": "https://doi.org/10.1145/69558.69561",
    "publication_date": "1989-10-01",
    "publication_year": 1989,
    "authors": "Rajive Bagrodia",
    "corresponding_authors": "Rajive Bagrodia",
    "abstract": "Many concurrent programming languages including CSP and Ada use synchronous message-passing to define communication between a pair of asynchronous processes. Suggested primitives like the generalized alternative command for CSP and the symmetric select statement for Ada allow a process to nondeterministically select one of several communication statements for execution. The communication statement may be either an input or an output statement. We propose a simple algorithm to implement the generalized alternative command and show that it uses fewer messages than existing algorithms.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2116081767",
    "type": "article"
  },
  {
    "title": "An editor for revision control",
    "doi": "https://doi.org/10.1145/22719.22948",
    "publication_date": "1987-03-20",
    "publication_year": 1987,
    "authors": "Christipher W. Fraser; Eugene W. Myers",
    "corresponding_authors": "",
    "abstract": "Programming environments support revision control in several guises. Explicitly, revision control software manages the trees of revisions that grow as software is modified. Implicitly, editors retain past versions by automatically saving backup copies and by allowing users to undo commands. This paper describes an editor that offers a uniform solution to these problems by never destroying the old version of the file being edited. It represents files using a generalization of AVL trees called “AVL dags,” which makes it affordable to automatically retain past versions of files. Automatic retention makes revision maintenance transparent to users. The editor also uses the same command language to edit both text and revision trees.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2055954649",
    "type": "article"
  },
  {
    "title": "Optimizing indirect branch prediction accuracy in virtual machine interpreters",
    "doi": "https://doi.org/10.1145/1286821.1286828",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Kevin Casey; M. Anton Ertl; David Gregg",
    "corresponding_authors": "",
    "abstract": "Interpreters designed for efficiency execute a huge number of indirect branches and can spend more than half of the execution time in indirect branch mispredictions. Branch target buffers (BTBs) are the most widely available form of indirect branch prediction; however, their prediction accuracy for existing interpreters is only 2%--50%. In this article we investigate two methods for improving the prediction accuracy of BTBs for interpreters: replicating virtual machine (VM) instructions and combining sequences of VM instructions into superinstructions. We investigate static (interpreter build-time) and dynamic (interpreter runtime) variants of these techniques and compare them and several combinations of these techniques. To show their generality, we have implemented these optimizations in VMs for both Java and Forth. These techniques can eliminate nearly all of the dispatch branch mispredictions, and have other benefits, resulting in speedups by a factor of up to 4.55 over efficient threaded-code interpreters, and speedups by a factor of up to 1.34 over techniques relying on dynamic superinstructions alone.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2149827666",
    "type": "article"
  },
  {
    "title": "Formal Specification of Graphic Data Types",
    "doi": "https://doi.org/10.1145/69622.357191",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "William R. Mallgren",
    "corresponding_authors": "William R. Mallgren",
    "abstract": "article Free Access Share on Formal Specification of Graphic Data Types Author: William R. Mallgren Systems Development Department, Xerox Corporation, 3450 Hillview Avenue, Palo Alto, CA Systems Development Department, Xerox Corporation, 3450 Hillview Avenue, Palo Alto, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4pp 687–710https://doi.org/10.1145/69622.357191Published:01 October 1982Publication History 34citation500DownloadsMetricsTotal Citations34Total Downloads500Last 12 Months36Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2067237100",
    "type": "article"
  },
  {
    "title": "Applicability of Software Validation Techniques to Scientific Programs",
    "doi": "https://doi.org/10.1145/357103.357107",
    "publication_date": "1980-07-01",
    "publication_year": 1980,
    "authors": "William E. Howden",
    "corresponding_authors": "William E. Howden",
    "abstract": "Error analysis involves the examination of a collection of programs whose errors are known. Each error is analyzed and validation techniques which would discover the error are identified. The errors that were present in version five of a package of Fortran scientific subroutines and then later corrected in version six were analyzed. An integrated collection of static and dynamic analysis methods would have discovered the errors in version five before its release. An integrated approach to validation and the effectiveness of individual methods are discussed.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2082617035",
    "type": "article"
  },
  {
    "title": "Run-time principals in information-flow type systems",
    "doi": "https://doi.org/10.1145/1290520.1290526",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Stephen Tse; Steve Zdancewic",
    "corresponding_authors": "",
    "abstract": "Information-flow type systems are a promising approach for enforcing strong end-to-end confidentiality and integrity policies. Such policies, however, are usually specified in terms of static information—data is labeled high or low security at compile time. In practice, the confidentiality of data may depend on information available only while the system is running. This article studies language support for run-time principals , a mechanism for specifying security policies that depend on which principals interact with the system. We establish the basic property of noninterference for programs written in such language, and use run-time principals for specifying run-time authority in downgrading mechanisms such as declassification. In addition to allowing more expressive security policies, run-time principals enable the integration of language-based security mechanisms with other existing approaches such as Java stack inspection and public key infrastructures. We sketch an implementation of run-time principals via public keys such that principal delegation is verified by certificate chains.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W1998003182",
    "type": "article"
  },
  {
    "title": "Empirical study of optimization techniques for massive slicing",
    "doi": "https://doi.org/10.1145/1290520.1290523",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "David Binkley; Mark Harman; Jens Krinke",
    "corresponding_authors": "",
    "abstract": "This article presents results from a study of techniques that improve the performance of graph-based interprocedural slicing of the System Dependence Graph (SDG). This is useful in “massive slicing” where slices are required for many or all of the possible set of slicing criteria. Several different techniques are considered, including forming strongly connected components, topological sorting, and removing transitive edges. Data collected from a test bed of just over 1,000,000 lines of code are presented. This data illustrates the impact on computation time of the techniques. Together, the best combination produces a 71% reduction in run-time (and a 64% reduction in memory usage). The complete set of techniques also illustrates the point at which faster computation is not viable due to prohibitive preprocessing costs.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2069501081",
    "type": "article"
  },
  {
    "title": "Uniform Random Generation of Balanced Parenthesis Strings",
    "doi": "https://doi.org/10.1145/357084.357091",
    "publication_date": "1980-01-01",
    "publication_year": 1980,
    "authors": "David Arnold; M. R. Sleep",
    "corresponding_authors": "",
    "abstract": "The empirical testing of error repair schemes for skeletons of source programs in a block-structured language leads to the problem of generating balanced parenthesis strings in a uniform random manner. An efficient generator which works from left to right must compute the correct probability for the next symbol at each stage of the generation. The associated enumeration problem may be solved by adopting a geometric interpretation usually associated with random walk problems. This solution leads immediately to an O ( n ) algorithm for the generator.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2075367417",
    "type": "article"
  },
  {
    "title": "Compact Encodings of List Structure",
    "doi": "https://doi.org/10.1145/357073.357081",
    "publication_date": "1979-10-01",
    "publication_year": 1979,
    "authors": "Daniel G. Bobrow; Douglas W. Clark",
    "corresponding_authors": "",
    "abstract": "List structures provide a general mechanism for representing easily changed structured data, but can introduce inefficiencies in the use of space when fields of uniform size are used to contain pointers to data and to link the structure. Empirically determined regularity can be exploited to provide more space-efficient encodings without losing the flexibility inherent in list structures. The basic scheme is to provide compact pointer fields big enough to accommodate most values that occur in them and to provide “escape” mechanisms for exceptional cases. Several examples of encoding designs are presented and evaluated, including two designs currently used in Lisp machines. Alternative escape mechanisms are described, and various questions of cost and implementation are discussed. In order to extrapolate our results to larger systems than those measured, we propose a model for the generation of list pointers and we test the model against data from two programs. We show that according to our model, list structures with compact cdr fields will, as address space grows, continue to be compacted well with a fixed-width small field. Our conclusion is that with a microcodable processor, about a factor of two gain in space efficiency for list structure can be had for little or no cost in processing time.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2043201813",
    "type": "article"
  },
  {
    "title": "Comments on “Communicating Sequential Processes”",
    "doi": "https://doi.org/10.1145/357073.357077",
    "publication_date": "1979-10-01",
    "publication_year": 1979,
    "authors": "Richard B. Kieburtz; Abraham Silberschatz",
    "corresponding_authors": "",
    "abstract": "In his recent paper, “Communicating Sequential Processes” ( Comm. ACM 21, 8 (Aug. 1978), 666-677), C.A.R. Hoare outlines a programming language notation for interprocess communication in which processes are synchronized by the messages they exchange. The notation carries with it certain implications for the synchronization protocols required in a message transfer. These are not at all obvious and are made explicit here. An alternative convention is suggested in which communication and synchronization are partially uncoupled from one another.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2126643714",
    "type": "article"
  },
  {
    "title": "And/Or Programs: A New Approach to Structured Programming",
    "doi": "https://doi.org/10.1145/357084.357085",
    "publication_date": "1980-01-01",
    "publication_year": 1980,
    "authors": "David Harel",
    "corresponding_authors": "David Harel",
    "abstract": "A simple tree-like programming/specification language is presented. The central idea is the dividing of conventional programming constructs into the two classes of and and or subgoaling, the subgoal tree itself constituting the program. Programs written in the language can, in general, be both nondeterministic and parallel. The syntax and semantics of the language are defined, a method for verifying programs written in it is described, and the practical significance of programming in the language assessed. Finally, some directions for further research are indicated.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1966535250",
    "type": "article"
  },
  {
    "title": "A hybrid type system for lock-freedom of mobile processes",
    "doi": "https://doi.org/10.1145/1745312.1745313",
    "publication_date": "2008-05-24",
    "publication_year": 2008,
    "authors": "Naoki Kobayashi; Davide Sangiorgi",
    "corresponding_authors": "",
    "abstract": "We propose a type system for lock-freedom in the π-calculus, which guarantees that certain communications will eventually succeed. Distinguishing features of our type system are: it can verify lock-freedom of concurrent programs that have sophisticated recursive communication structures; it can be fully automated; it is hybrid, in that it combines a type system for lock-freedom with local reasoning about deadlock-freedom, termination, and confluence analyses. Moreover, the type system is parameterized by deadlock-freedom/termination/confluence analyses, so that any methods (e.g. type systems and model checking) can be used for those analyses. A lock-freedom analysis tool has been implemented based on the proposed type system, and tested for nontrivial programs.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2042668508",
    "type": "article"
  },
  {
    "title": "A type discipline for authorization policies",
    "doi": "https://doi.org/10.1145/1275497.1275500",
    "publication_date": "2007-08-02",
    "publication_year": 2007,
    "authors": "Cédric Fournet; Andrew D. Gordon; Sergio Maffeis",
    "corresponding_authors": "",
    "abstract": "Distributed systems and applications are often expected to enforce high-level authorization policies. To this end, the code for these systems relies on lower-level security mechanisms such as digital signatures, local ACLs, and encrypted communications. In principle, authorization specifications can be separated from code and carefully audited. Logic programs in particular can express policies in a simple, abstract manner. We consider the problem of checking whether a distributed implementation based on communication channels and cryptography complies with a logical authorization policy. We formalize authorization policies and their connection to code by embedding logical predicates and claims within a process calculus. We formulate policy compliance operationally by composing a process model of the distributed system with an arbitrary opponent process. Moreover, we propose a dependent type system for verifying policy compliance of implementation code. Using Datalog as an authorization logic, we show how to type several examples using policies and present a general schema for compiling policies.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2117403024",
    "type": "article"
  },
  {
    "title": "Synthesis of Resource Invariants for Concurrent Programs",
    "doi": "https://doi.org/10.1145/357103.357109",
    "publication_date": "1980-07-01",
    "publication_year": 1980,
    "authors": "Edmund M. Clarke",
    "corresponding_authors": "Edmund M. Clarke",
    "abstract": "Owicki and Gries have developed a proof system for conditional critical regions . In their system, logically related variables accessed by more than one process are grouped together as resources , and processes are allowed access to a resource only in a critical region for that resource. Proofs of synchronization properties are constructed by devising predicates called resource invariants which describe relationships among the variables of a resource when no process is in a critical region for the resource. In constructing proofs using the system of Owicki and Gries, the programmer is required to supply the resource invariants. Methods are developed in this paper for automatically synthesizing resource invariants. Specifically, the resource invariants of a concurrent program are characterized as least fixpoints of a functional which can be obtained from the text of the program. By the use of this fixpoint characterization and a widening operator based on convex closure, good approximations may be obtained for the resource invariants of many concurrent programs.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2047051012",
    "type": "article"
  },
  {
    "title": "A capability calculus for concurrency and determinism",
    "doi": "https://doi.org/10.1145/1387673.1387676",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Tachio Terauchi; Alex Aiken",
    "corresponding_authors": "",
    "abstract": "This article presents a static system for checking determinism (technically, partial confluence) of communicating concurrent processes. Our approach automatically detects partial confluence in programs communicating via a mix of different kinds of communication methods: rendezvous channels, buffered channels, broadcast channels, and reference cells. Our system reduces the partial confluence checking problem in polynomial time (in the size of the program) to the problem of solving a system of rational linear inequalities, and is thus efficient.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2053267751",
    "type": "article"
  },
  {
    "title": "A relational approach to interprocedural shape analysis",
    "doi": "https://doi.org/10.1145/1667048.1667050",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Bertrand Jeannet; А. А. Логинов; Thomas Reps; Mooly Sagiv",
    "corresponding_authors": "",
    "abstract": "This article addresses the verification of properties of imperative programs with recursive procedure calls, heap-allocated storage, and destructive updating of pointer-valued fields, that is, interprocedural shape analysis . The article makes three contributions. — It introduces a new method for abstracting relations over memory configurations for use in abstract interpretation. — It shows how this method furnishes the elements needed for a compositional approach to shape analysis. In particular, abstracted relations are used to represent the shape transformation performed by a sequence of operations, and an overapproximation to relational composition can be performed using the meet operation of the domain of abstracted relations. — It applies these ideas in a new algorithm for context-sensitive interprocedural shape analysis. The algorithm creates procedure summaries using abstracted relations over memory configurations, and the meet-based composition operation provides a way to apply the summary transformer for a procedure P at each call site from which P is called. The algorithm has been applied successfully to establish properties of both (i) recursive programs that manipulate lists and (ii) recursive programs that manipulate binary trees.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2049524532",
    "type": "article"
  },
  {
    "title": "Scratchpad allocation for concurrent embedded software",
    "doi": "https://doi.org/10.1145/1734206.1734210",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Vivy Suhendra; Abhik Roychoudhury; Tulika Mitra",
    "corresponding_authors": "",
    "abstract": "Software-controlled scratchpad memory is increasingly employed in embedded systems as it offers better timing predictability compared to caches. Previous scratchpad allocation algorithms typically consider single-process applications. But embedded applications are mostly multitasking with real-time constraints, where the scratchpad memory space has to be shared among interacting processes that may preempt each other. In this work, we develop a novel dynamic scratchpad allocation technique that takes these process interferences into account to improve the performance and predictability of the memory system. We model the application as a Message Sequence Chart (MSC) to best capture the interprocess interactions. Our goal is to optimize the Worst-Case Response Time (WCRT) of the application through runtime reloading of the scratchpad memory content at appropriate execution points. We propose an iterative allocation algorithm that consists of two critical steps: (1) analyze the MSC along with the existing allocation to determine potential interference patterns, and (2) exploit this interference information to tune the scratchpad reloading points and content so as to best improve the WCRT. We present various alternative scratchpad allocation heuristics and evaluate their effectiveness in reducing the WCRT. The scheme is also extended to work on Message Sequence Graph models. We evaluate our memory allocation scheme on two real-world embedded applications controlling an Unmanned Aerial Vehicle (UAV) and an in-orbit monitoring instrument, respectively.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2128337123",
    "type": "article"
  },
  {
    "title": "Parameterized loop tiling",
    "doi": "https://doi.org/10.1145/2160910.2160912",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "Lakshminarayanan Renganarayanan; DaeGon Kim; Michelle Mills Strout; Sanjay Rajopadhye",
    "corresponding_authors": "",
    "abstract": "Loop tiling is a widely used program optimization that improves data locality and enables coarse-grained parallelism. Parameterized tiled loops, where the tile sizes remain symbolic parameters until runtime, are quite useful for iterative compilers and autotuners that produce highly optimized libraries and codes. Although it is easy to generate such loops for (hyper-) rectangular iteration spaces tiled with (hyper-) rectangular tiles, many important computations do not fall into this restricted domain. In the past, parameterized tiled code generation for the general case of convex iteration spaces being tiled by (hyper-) rectangular tiles has been solved with bounding box approaches or with sophisticated and expensive machinery. We present a novel formulation of the parameterized tiled loop generation problem using a polyhedral set called the outset . By reducing the problem of parameterized tiled code generation to that of generating standard loops and simple postprocessing of these loops, the outset method achieves a code generation efficiency that is comparable to existing code generation techniques, including those for fixed tile sizes. We compare the performance of our technique with several other tiled loop generation methods on kernels from BLAS3 and scientific computations. The simplicity of our solution makes it well suited for use in production compilers—in particular, the IBM XL compiler uses the inset-based technique introduced in this article for register tiling. We also provide a complete coverage of parameterized tiling of perfect loop nests by describing three related techniques: (i) a scheme for separating full and partial tiles; (ii) a scheme for generating tiled loops directly from the abstract syntax tree representation of loops; (iii) a formal characterization of parameterized loop tiling using bilinear forms and a Symbolic Fourier-Motzkin Elimination (SFME)-based parameterized tiled loop generation method.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1967414258",
    "type": "article"
  },
  {
    "title": "Making the java memory model safe",
    "doi": "https://doi.org/10.1145/2518191",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Andreas Lochbihler",
    "corresponding_authors": "Andreas Lochbihler",
    "abstract": "This work presents a machine-checked formalisation of the Java memory model and connects it to an operational semantics for Java and Java bytecode. For the whole model, I prove the data race freedom guarantee and type safety. The model extends previous formalisations by dynamic memory allocation, thread spawns and joins, infinite executions, the wait-notify mechanism, and thread interruption, all of which interact in subtle ways with the memory model. The formalisation resulted in numerous clarifications of and fixes to the existing JMM specification.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2008197313",
    "type": "article"
  },
  {
    "title": "Semantics of transactional memory and automatic mutual exclusion",
    "doi": "https://doi.org/10.1145/1889997.1889999",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Martı́n Abadi; Andrew Birrell; Tim Harris; Michael Isard",
    "corresponding_authors": "",
    "abstract": "Software Transactional Memory (STM) is an attractive basis for the development of language features for concurrent programming. However, the semantics of these features can be delicate and problematic. In this article we explore the trade-offs semantic simplicity, the viability of efficient implementation strategies, and the flexibility of language constructs. Specifically, we develop semantics and type systems for the constructs of the Automatic Mutual Exclusion (AME) programming model; our results apply also to other constructs, such as atomic blocks. With this semantics as a point of reference, we study several implementation strategies. We model STM systems that use in-place update, optimistic concurrency, lazy conflict detection, and rollback. These strategies are correct only under nontrivial assumptions that we identify and analyze. One important source of errors is that some efficient implementations create dangerous “zombie” computations where a transaction keeps running after experiencing a conflict; the assumptions confine the effects of these computations.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2126484338",
    "type": "article"
  },
  {
    "title": "Kitsune",
    "doi": "https://doi.org/10.1145/2629460",
    "publication_date": "2014-10-28",
    "publication_year": 2014,
    "authors": "Christopher M. Hayden; Karla Saur; Edward K. Smith; Michael Hicks; Jeffrey S. Foster",
    "corresponding_authors": "",
    "abstract": "Dynamic software updating (DSU) systems facilitate software updates to running programs, thereby permitting developers to add features and fix bugs without downtime. This article introduces Kitsune, a DSU system for C. Kitsune’s design has three notable features. First, Kitsune updates the whole program, rather than individual functions, using a mechanism that places no restrictions on data representations or allowed compiler optimizations. Second, Kitsune makes the important aspects of updating explicit in the program text, making the program’s semantics easy to understand while minimizing programmer effort. Finally, the programmer can write simple specifications to direct Kitsune to generate code that traverses and transforms old-version state for use by new code; such state transformation is often necessary and is significantly more difficult in prior DSU systems. We have used Kitsune to update six popular, open-source, single- and multithreaded programs and find that few program changes are required to use Kitsune, that it incurs essentially no performance overhead, and that update times are fast.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2049659774",
    "type": "article"
  },
  {
    "title": "A data-centric approach to synchronization",
    "doi": "https://doi.org/10.1145/2160910.2160913",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "Julian Dolby; Christian Hammer; Daniel Marino; Frank Tip; Mandana Vaziri; Jan Vítek",
    "corresponding_authors": "",
    "abstract": "Concurrency-related errors, such as data races, are frustratingly difficult to track down and eliminate in large object-oriented programs. Traditional approaches to preventing data races rely on protecting instruction sequences with synchronization operations. Such control-centric approaches are inherently brittle, as the burden is on the programmer to ensure that all concurrently accessed memory locations are consistently protected. Data-centric synchronization is an alternative approach that offloads some of the work on the language implementation. Data-centric synchronization groups fields of objects into atomic sets to indicate that these fields must always be updated atomically. Each atomic set has associated units of work , that is, code fragments that preserve the consistency of that atomic set. Synchronization operations are added automatically by the compiler. We present an extension to the Java programming language that integrates annotations for data-centric concurrency control. The resulting language, called AJ, relies on a type system that enables separate compilation and supports atomic sets that span multiple objects and that also supports full encapsulation for more efficient code generation. We evaluate our proposal by refactoring classes from standard libraries, as well as a number of multithreaded benchmarks, to use atomic sets. Our results suggest that data-centric synchronization is easy to use and enjoys low annotation overhead, while successfully preventing data races. Moreover, experiments on the SPECjbb benchmark suggest that acceptable performance can be achieved with a modest amount of tuning.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2060234806",
    "type": "article"
  },
  {
    "title": "Bisimulation for Quantum Processes",
    "doi": "https://doi.org/10.1145/2400676.2400680",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Yuan Feng; Runyao Duan; Mingsheng Ying",
    "corresponding_authors": "",
    "abstract": "Quantum cryptographic systems have been commercially available, with a striking advantage over classical systems that their security and ability to detect the presence of eavesdropping are provable based on the principles of quantum mechanics. On the other hand, quantum protocol designers may commit more faults than classical protocol designers since human intuition is poorly adapted to the quantum world. To offer formal techniques for modeling and verification of quantum protocols, several quantum extensions of process algebra have been proposed. An important issue in quantum process algebra is to discover a quantum generalization of bisimulation preserved by various process constructs, in particular, parallel composition, where one of the major differences between classical and quantum systems, namely quantum entanglement, is present. Quite a few versions of bisimulation have been defined for quantum processes in the literature, but in the best case they are only proved to be preserved by parallel composition of purely quantum processes where no classical communication is involved. Many quantum cryptographic protocols, however, employ the LOCC (Local Operations and Classical Communication) scheme, where classical communication must be explicitly specified. So, a notion of bisimulation preserved by parallel composition in the circumstance of both classical and quantum communication is crucial for process algebra approach to verification of quantum cryptographic protocols. In this article we introduce novel notions of strong bisimulation and weak bisimulation for quantum processes, and prove that they are congruent with respect to various process algebra combinators including parallel composition even when both classical and quantum communication are present. We also establish some basic algebraic laws for these bisimulations. In particular, we show the uniqueness of the solutions to recursive equations of quantum processes, which proves useful in verifying complex quantum protocols. To capture the idea that a quantum process approximately implements its specification, and provide techniques and tools for approximate reasoning, a quantified version of strong bisimulation, which defines for each pair of quantum processes a bisimulation-based distance characterizing the extent to which they are strongly bisimilar, is also introduced.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2077160660",
    "type": "article"
  },
  {
    "title": "Don’t Sit on the Fence",
    "doi": "https://doi.org/10.1145/2994593",
    "publication_date": "2017-05-29",
    "publication_year": 2017,
    "authors": "Jade Alglave; Daniel Kroening; Vincent Nimal; Daniel Poetzl",
    "corresponding_authors": "",
    "abstract": "Modern architectures rely on memory fences to prevent undesired weakenings of memory consistency. As the fences’ semantics may be subtle, the automation of their placement is highly desirable. But precise methods for restoring consistency do not scale to deployed systems’ code. We choose to trade some precision for genuine scalability: our technique is suitable for large code bases. We implement it in our new musketeer tool and report experiments on more than 700 executables from packages found in Debian GNU/Linux 7.1, including memcached with about 10,000 LoC.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1588615233",
    "type": "article"
  },
  {
    "title": "Æminium",
    "doi": "https://doi.org/10.1145/2543920",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Sven Stork; Karl Naden; Joshua Sunshine; Manuel Mohr; Alcides Fonseca; Paulo Marques; Jonathan Aldrich",
    "corresponding_authors": "",
    "abstract": "Writing concurrent applications is extremely challenging, not only in terms of producing bug-free and maintainable software, but also for enabling developer productivity. In this article we present the Æminium concurrent-by-default programming language. Using Æminium programmers express data dependencies rather than control flow between instructions. Dependencies are expressed using permissions, which are used by the type system to automatically parallelize the application. The Æminium approach provides a modular and composable mechanism for writing concurrent applications, preventing data races in a provable way. This allows programmers to shift their attention from low-level, error-prone reasoning about thread interleaving and synchronization to focus on the core functionality of their applications. We study the semantics of Æminium through μ Æminium, a sound core calculus that leverages permission flow to enable concurrent-by-default execution. After discussing our prototype implementation we present several case studies of our system. Our case studies show up to 6.5X speedup on an eight-core machine when leveraging data group permissions to manage access to shared state, and more than 70% higher throughput in a Web server application.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2015979616",
    "type": "article"
  },
  {
    "title": "Combinatorial Register Allocation and Instruction Scheduling",
    "doi": "https://doi.org/10.1145/3332373",
    "publication_date": "2019-07-02",
    "publication_year": 2019,
    "authors": "Roberto Castañeda Lozano; Mats Carlsson; Gabriel Hjort Blindell; Christian Schulte",
    "corresponding_authors": "",
    "abstract": "This article introduces a combinatorial optimization approach to register allocation and instruction scheduling, two central compiler problems. Combinatorial optimization has the potential to solve these problems optimally and to exploit processor-specific features readily. Our approach is the first to leverage this potential in practice : it captures the complete set of program transformations used in state-of-the-art compilers, scales to medium-sized functions of up to 1,000 instructions, and generates executable code. This level of practicality is reached by using constraint programming, a particularly suitable combinatorial optimization technique. Unison, the implementation of our approach, is open source, used in industry, and integrated with the LLVM toolchain. An extensive evaluation confirms that Unison generates better code than LLVM while scaling to medium-sized functions. The evaluation uses systematically selected benchmarks from MediaBench and SPEC CPU2006 and different processor architectures (Hexagon, ARM, MIPS). Mean estimated speedup ranges from 1.1% to 10% and mean code size reduction ranges from 1.3% to 3.8% for the different architectures. A significant part of this improvement is due to the integrated nature of the approach. Executing the generated code on Hexagon confirms that the estimated speedup results in actual speedup. Given a fixed time limit, Unison solves optimally functions of up to 946 instructions, nearly an order of magnitude larger than previous approaches. The results show that our combinatorial approach can be applied in practice to trade compilation time for code quality beyond the usual compiler optimization levels, identify improvement opportunities in heuristic algorithms, and fully exploit processor-specific features.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2962770763",
    "type": "article"
  },
  {
    "title": "Non-polynomial Worst-Case Analysis of Recursive Programs",
    "doi": "https://doi.org/10.1145/3339984",
    "publication_date": "2019-10-12",
    "publication_year": 2019,
    "authors": "Krishnendu Chatterjee; Hongfei Fu; Amir Kafshdar Goharshady",
    "corresponding_authors": "",
    "abstract": "We study the problem of developing efficient approaches for proving worst-case bounds of non-deterministic recursive programs. Ranking functions are sound and complete for proving termination and worst-case bounds of non-recursive programs. First, we apply ranking functions to recursion, resulting in measure functions. We show that measure functions provide a sound and complete approach to prove worst-case bounds of non-deterministic recursive programs. Our second contribution is the synthesis of measure functions in non-polynomial forms. We show that non-polynomial measure functions with logarithm and exponentiation can be synthesized through abstraction of logarithmic or exponentiation terms, Farkas Lemma, and Handelman’s Theorem using linear programming. While previous methods obtain polynomial worst-case bounds, our approach can synthesize bounds of various forms including O( n log n ) and O( n r ), where r is not an integer. We present experimental results to demonstrate that our approach can efficiently obtain worst-case bounds of classical recursive algorithms such as (i) Merge sort, Heap sort, and the divide-and-conquer algorithm for the Closest Pair problem, where we obtain O( n log n ) worst-case bound, and (ii) Karatsuba’s algorithm for polynomial multiplication and Strassen’s algorithm for matrix multiplication, for which we obtain O( n r ) bounds such that r is not an integer and is close to the best-known bound for the respective algorithm. Besides the ability to synthesize non-polynomial bounds, we also show that our approach is equally capable of obtaining polynomial worst-case bounds for classical programs such as Quick sort and the dynamic programming algorithm for computing Fibonacci numbers.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2980978064",
    "type": "article"
  },
  {
    "title": "The Design and Formalization of Mezzo, a Permission-Based Programming Language",
    "doi": "https://doi.org/10.1145/2837022",
    "publication_date": "2016-08-02",
    "publication_year": 2016,
    "authors": "Thibaut Balabonski; François Pottier; Jonathan Protzenko",
    "corresponding_authors": "",
    "abstract": "The programming language Mezzo is equipped with a rich type system that controls aliasing and access to mutable memory. We give a comprehensive tutorial overview of the language. Then we present a modular formalization of Mezzo’s core type system, in the form of a concurrent λ-calculus, which we successively extend with references, locks, and adoption and abandon, a novel mechanism that marries Mezzo’s static ownership discipline with dynamic ownership tests. We prove that well-typed programs do not go wrong and are data-race free. Our definitions and proofs are machine checked.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2476818328",
    "type": "article"
  },
  {
    "title": "A Lightweight Formalism for Reference Lifetimes and Borrowing in Rust",
    "doi": "https://doi.org/10.1145/3443420",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "David J. Pearce",
    "corresponding_authors": "David J. Pearce",
    "abstract": "Rust is a relatively new programming language that has gained significant traction since its v1.0 release in 2015. Rust aims to be a systems language that competes with C/C++. A claimed advantage of Rust is a strong focus on memory safety without garbage collection. This is primarily achieved through two concepts, namely, reference lifetimes and borrowing . Both of these are well-known ideas stemming from the literature on region-based memory management and linearity / uniqueness . Rust brings both of these ideas together to form a coherent programming model. Furthermore, Rust has a strong focus on stack-allocated data and, like C/C++ but unlike Java, permits references to local variables. Type checking in Rust can be viewed as a two-phase process: First, a traditional type checker operates in a flow-insensitive fashion; second, a borrow checker enforces an ownership invariant using a flow-sensitive analysis. In this article, we present a lightweight formalism that captures these two phases using a flow-sensitive type system that enforces “ type and borrow safety .” In particular, programs that are type and borrow safe will not attempt to dereference dangling pointers. Our calculus core captures many aspects of Rust, including copy- and move-semantics, mutable borrowing, reborrowing, partial moves, and lifetimes. In particular, it remains sufficiently lightweight to be easily digested and understood and, we argue, still captures the salient aspects of reference lifetimes and borrowing. Furthermore, extensions to the core can easily add more complex features (e.g., control-flow, tuples, method invocation). We provide a soundness proof to verify our key claims of the calculus. We also provide a reference implementation in Java with which we have model checked our calculus using over 500B input programs. We have also fuzz tested the Rust compiler using our calculus against 2B programs and, to date, found one confirmed compiler bug and several other possible issues.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3152550240",
    "type": "article"
  },
  {
    "title": "Compositional Programming",
    "doi": "https://doi.org/10.1145/3460228",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Weixin Zhang; Yaozhu Sun; Bruno C. d. S. Oliveira",
    "corresponding_authors": "",
    "abstract": "Modularity is a key concern in programming. However, programming languages remain limited in terms of modularity and extensibility. Small canonical problems, such as the Expression Problem (EP), illustrate some of the basic issues: the dilemma between choosing one kind of extensibility over another one in most programming languages. Other problems, such as how to express dependencies in a modular way, add up to the basic issues and remain a significant challenge. This article presents a new statically typed modular programming style called Compositional Programming . In Compositional Programming, there is no EP: It is easy to get extensibility in multiple dimensions (i.e., it is easy to add new variants as well as new operations). Compositional Programming offers an alternative way to model data structures that differs from both algebraic datatypes in functional programming and conventional OOP class hierarchies. We introduce four key concepts for Compositional Programming: compositional interfaces , compositional traits , method patterns , and nested trait composition . Altogether, these concepts allow us to naturally solve challenges such as the Expression Problem, model attribute-grammar-like programs, and generally deal with modular programs with complex dependencies . We present a language design, called CP , which is proved to be type-safe, together with several examples and three case studies.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4206708621",
    "type": "article"
  },
  {
    "title": "Gradualizing the Calculus of Inductive Constructions",
    "doi": "https://doi.org/10.1145/3495528",
    "publication_date": "2022-04-06",
    "publication_year": 2022,
    "authors": "Meven Lennon-Bertrand; Kenji Maillard; Nicolas Tabareau; Éric Tanter",
    "corresponding_authors": "",
    "abstract": "We investigate gradual variations on the Calculus of Inductive Construction (CIC) for swifter prototyping with imprecise types and terms. We observe, with a no-go theorem, a crucial tradeoff between graduality and the key properties of normalization and closure of universes under dependent product that CIC enjoys. Beyond this Fire Triangle of Graduality, we explore the gradualization of CIC with three different compromises, each relaxing one edge of the Fire Triangle. We develop a parametrized presentation of Gradual CIC (GCIC) that encompasses all three variations, and develop their metatheory. We first present a bidirectional elaboration of GCIC to a dependently-typed cast calculus, CastCIC, which elucidates the interrelation between typing, conversion, and the gradual guarantees. We use a syntactic model of CastCIC to inform the design of a safe, confluent reduction, and establish, when applicable, normalization. We study the static and dynamic gradual guarantees as well as the stronger notion of graduality with embedding-projection pairs formulated by New and Ahmed, using appropriate semantic model constructions. This work informs and paves the way towards the development of malleable proof assistants and dependently-typed programming languages.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4205625158",
    "type": "article"
  },
  {
    "title": "SSProve: A Foundational Framework for Modular Cryptographic Proofs in Coq",
    "doi": "https://doi.org/10.1145/3594735",
    "publication_date": "2023-05-04",
    "publication_year": 2023,
    "authors": "Philipp G. Haselwarter; Exequiel Rivas; Antoine Van Muylder; Théo Winterhalter; Carmine Abate; Nikolaj Sidorenco; Cătălin Hriţcu; Kenji Maillard; Bas Spitters",
    "corresponding_authors": "",
    "abstract": "State-separating proofs (SSP) is a recent methodology for structuring game-based cryptographic proofs in a modular way, by using algebraic laws to exploit the modular structure of composed protocols. While promising, this methodology was previously not fully formalized and came with little tool support. We address this by introducing SSProve, the first general verification framework for machine-checked state-separating proofs. SSProve combines high-level modular proofs about composed protocols, as proposed in SSP, with a probabilistic relational program logic for formalizing the lower-level details, which together enable constructing machine-checked cryptographic proofs in the Coq proof assistant. Moreover, SSProve is itself fully formalized in Coq, including the algebraic laws of SSP, the soundness of the program logic, and the connection between these two verification styles.To illustrate SSProve, we use it to mechanize the simple security proofs of ElGamal and pseudo-random-function-based encryption. We also validate the SSProve approach by conducting two more substantial case studies: First, we mechanize an SSP security proof of the key encapsulation mechanism-data encryption mechanism (KEM-DEM) public key encryption scheme, which led to the discovery of an error in the original paper proof that has since been fixed. Second, we use SSProve to formally prove security of the sigma-protocol zero-knowledge construction, and we moreover construct a commitment scheme from a sigma-protocol to compare with a similar development in CryptHOL. We instantiate the security proof for sigma-protocols to give concrete security bounds for Schnorr's sigma-protocol.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3187205106",
    "type": "article"
  },
  {
    "title": "A practical and flexible flow analysis for higher-order languages",
    "doi": "https://doi.org/10.1145/291891.291898",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "John M. Ashley; R. Kent Dybvig",
    "corresponding_authors": "",
    "abstract": "A flow analysis collects data-flow and control-flow information about programs. A compiler can use this information to enable optimizations. The analysis described in this article unifies and extends previous work on flow analysis for higher-order languages supporting assignment and control operators. The analysis is abstract interpretation based and is parameterized over two polyvariance operators and a projection operator. These operators are used to regulate the speed and accuracy of the analysis. An implementation of the analysis is incorporated into and used in a production Scheme compiler. The analysis can process any legal Scheme program without modification. Others have demonstrated that a 0CFA analysis can enables the optimizations, but a 0CFA analysis is O ( n ) 3 ). An O ( n ) instantiation of our analysis successfully enables the optimization of closure representations and procedure calls. Experiments with the cheaper instantiation show that it is as effective as 0CFA for these optimizations.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W1986693746",
    "type": "article"
  },
  {
    "title": "Efficient implementation of adaptive software",
    "doi": "https://doi.org/10.1145/201059.201066",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Jens Palsberg; Cun Xiao; Karl Lieberherr",
    "corresponding_authors": "",
    "abstract": "Adaptive programs compute with objects, just like object-oriented programs. Each task to be accomplished is specified by a so-called propagation pattern which traverses the receiver object. The object traversal is a recursive descent via the instance variables where information is collected or propagated along the way. A propagation pattern consists of (1) a name for the task, (2) a succinct specification of the parts of the receiver object that should be traversed, and (3) code fragments to be executed when specific object types are encountered. The propagation patterns need to be complemented by a class graph which defines the detailed object structure. The separation of structure and behavior yields a degree of flexibility and understandability not present in traditional object-oriented languages. For example, the class graph can be changed without changing the adaptive program at all. We present an efficient implementation of adaptive programs. Given an adaptive program and a class graph, we generate an efficient object-oriented program, for example, in C++. Moreover, we prove the correctness of the core of this translation. A key assumption in the theorem is that the traversal specifications are consistent with the class graph. We prove the soundness of a proof system for conservatively checking consistency, and we show how to implement it efficiently.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2055027013",
    "type": "article"
  },
  {
    "title": "Finitary fairness",
    "doi": "https://doi.org/10.1145/295656.295659",
    "publication_date": "1998-11-01",
    "publication_year": 1998,
    "authors": "Rajeev Alur; Thomas A. Henzinger",
    "corresponding_authors": "",
    "abstract": "Fairness is a mathematical abstraction: in a multiprogramming environment, fairness abstracts the details of admissible (“fair”) schedulers; in a distributed environment, fairness abstracts the relative speeds of processors. We argue that the standard definition of fairness often is unnecessarily weak and can be replaced by the stronger, yet still abstract, notion of finitary fairness. While standard weak fairness requires that no enabled transition is postponed forever, finitary weak fairness requires that for every computation of a system there is an unknown bound k such that no enabled transition is postponed more than k consecutive times. In general, the finitary restriction fin ( F ) of any given fairness requirement F is the union of all ω-regular safety properties contained in F . The adequacy of the proposed abstraction is shown in two ways. Suppose we prove a program property under the assumption of finitary fairness. In a multiprogramming environment, the program then satisfies the property for all fair finite-state schedulers. In a distributed environment, the program then satisfies the property for all choices of lower and upper bounds on the speeds (or timings) of processors. The benefits of finitary fairness are twofold. First, the proof rules for verifying liveness properties of concurrent programs are simplified: well-founded induction over the natural numbers is adequate to prove termination under finitary fairness. Second, the fundamental problem of consensus in a faulty asynchronous distributed environment can be solved assuming finitary fairness.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2296540112",
    "type": "article"
  },
  {
    "title": "Sharing and groundness dependencies in logic programs",
    "doi": "https://doi.org/10.1145/330249.330252",
    "publication_date": "1999-09-01",
    "publication_year": 1999,
    "authors": "Michael Codish; Harald Søndergaard; Peter J. Stuckey",
    "corresponding_authors": "",
    "abstract": "We investigate Jacobs and Langen's Sharing domain, introduced for the analysis of variable sharing in logic programs, and show that it is isomorphic to Marriott and Søndergaard's Pos domain, introduced for the analysis of groundness dependencies. Our key idea is to view the sets of variables in a Sharing domain element as the models of a corresponding Boolean function. This leads to a recasting of sharing analysis in terms of the property of “not being affected by the binding of a single variable.” Such an “unaffectedness dependency” analysis has close connections with groundness dependency analysis using positive Boolean functions. This new view improves our understanding of sharing analysis, and leads to an elegant expression of its combination with groundness dependency analysis based on the reduced product of Sharing and Pos. It also opens up new avenues for the efficient implementation of sharing analysis, for example using reduced order binary decision diagrams, as well as efficient implementation of the reduced product, using domain factorizations.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W1969900041",
    "type": "article"
  },
  {
    "title": "Iteration abstraction in Sather",
    "doi": "https://doi.org/10.1145/225540.225541",
    "publication_date": "1996-01-01",
    "publication_year": 1996,
    "authors": "Stephan Murer; Stephen M. Omohundro; David Stoutamire; Clemens Szyperski",
    "corresponding_authors": "",
    "abstract": "Sather extends the notion of an iterator in a powerful new way. We argue that iteration abstractions belong in class interfaces on an equal footing with routines. Sather iterators were derived from CLU iterators but are much more flexible and better suited for object-oriented programming. We retain the property that iterators are structured , i.e., strictly bound to a controlling structured statement. We motivate and describe the construct along with several simple examples. We compare it with iteration based on CLU iterators, cursors, riders, streams, series, generators, coroutines, blocks, closures, and lambda expressions. Finally, we describe experiences with iterators in the Sather compiler and libraries.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W1975442062",
    "type": "article"
  },
  {
    "title": "Compositional verification of concurrent systems using Petri-net-based condensation rules",
    "doi": "https://doi.org/10.1145/293677.293681",
    "publication_date": "1998-09-01",
    "publication_year": 1998,
    "authors": "Eric Y. T. Juan; Jeffrey J. P. Tsai; T. Murata",
    "corresponding_authors": "",
    "abstract": "The state-explosion problem of formal verification has obstructed its application to large-scale software systems. In this article, we introduce a set of new condensation theories: IOT-failure equivalence, IOT-state equivalence, and firing-dependence theory to cope with this problem. Our condensation theories are much weaker than current theories used for the compositional verification of Petri nets. More significantly, our new condensation theories can eliminate the interleaved behaviors caused by asynchronously sending actions. Therefore, our technique provides a much more powerful means for the compositional verification of asynchronous processes. Our technique can efficiently analyze several state-based properties: boundedness, reachable markings, reachable submarkings, and deadlock states. Based on the notion of our new theories, we develop a set of condensation rules for efficient verification of large-scale software systems. The experimental results show a significant improvement in the analysis large-scale concurrent systems.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2001243851",
    "type": "article"
  },
  {
    "title": "Efficient logic variables for distributed computing",
    "doi": "https://doi.org/10.1145/319301.319347",
    "publication_date": "1999-05-01",
    "publication_year": 1999,
    "authors": "Seif Haridi; Peter Van Roy; Per Brand; Michael J. Mehl; Ralf Scheidhauer; Gert Smolka",
    "corresponding_authors": "",
    "abstract": "We define a practical algorithm for distrubuted rational tree unification and prove its correctness in both the off-line and on-line cases. We derive the distributed algorithm from a centralized one, showing clearly the trade-offs between local and distributed execution. The algorithm is used to realize logic variables in the Mozart Programming System, which implements the Oz language (see http://www/mozart-oz.org). Oz appears to the programmer as a concurrent object-oriented language with dataflow synchronization. Logic variables implement the dataflow behavior. We show that lohgic variables can easily be added to the more restricted models of Java and ML, thus providing an alternative way to do concurent programming in these languages. We present common distributed programming idioms in a network-transparent way using logic variables. We show that in common cases the algorithm maintains the same message latency as explicit message passing. In addition, it is able to handle uncommon cases that arise from the properties of latency tolerance and third-party independence. This is evidence that using logic variables in distributed computing is beneficial at both the system and language levels. At the system level, they improve latency tolerance and third-party independence. At the language level, they help make network-transparent distribution practical.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2002157613",
    "type": "article"
  },
  {
    "title": "Local and temporal predicates in distributed systems",
    "doi": "https://doi.org/10.1145/200994.201005",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Bernadette Charron-Bost; Carole Delporte-Gallet; Hugues Fauconnier",
    "corresponding_authors": "",
    "abstract": "The definitions of the predicates Possibly φ and Definitely φ, where φ is a global predicate of a distributed computation, lead to the definitions of two predicate transformers P and D . We show that P plays the same role with respect to time as the predicate transformers K i in knowledge theory play with respect to space . Pursuing this analogy, we prove that local predicates are exactly the fixed points of the K i 's while the stable predicates are the fixed points of P . In terms of the predicate transformers P and D , we define a new class of predicates that we call observer-independent predicates and for which the detection of Possibly φ and Definitely φ is quite easy. Finally, we establish a temporal counterpart to the knowledge change theorem of Chandy and Misra which formally proves that the global view of a distributed system provided by its various observations does not differ too much from its truth behavior.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2065156609",
    "type": "article"
  },
  {
    "title": "Optimal control dependence computation and the Roman chariots problem",
    "doi": "https://doi.org/10.1145/256167.256217",
    "publication_date": "1997-05-01",
    "publication_year": 1997,
    "authors": "Keshav Pingali; Gianfranco Bilardi",
    "corresponding_authors": "",
    "abstract": "The control dependence relation plays a fundamental role in program restructuring and optimization. The usual representation of this relation is the control dependence graph (CDG), but the size of the CDG can grow quadratically with the input programs, even for structured programs. In this article, we introduce the augmented postdominator tree (APT) , a data structure which can be constructed in space and time proportional to the size of the program and which supports enumeration of a number of useful control dependence sets in time proportional to their size. Therefore, APT provides an optimal representation of control dependence. Specifically, the APT data structure supports enumeration of the set cd(e), which is the set of statements control dependent on control-flow edge e, of the set conds (w), which is the set of edges on which statement w is dependent, and of the set cdequiv ( w ), which is the set of statements having the same control dependences as w . Technically, APT can be viewed as a factored representation of the CDG where queries are processed using an approach known as filtering search.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2083582440",
    "type": "article"
  },
  {
    "title": "Compositional parallel programming languages",
    "doi": "https://doi.org/10.1145/233561.233565",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "Ian Foster",
    "corresponding_authors": "Ian Foster",
    "abstract": "In task-parallel programs, diversee activities can take place concurrently, and communication and synchronization patterns are complex and not easily predictable. Previous work has identified compositionality as an important design principle for task-parallel programs. In this article, we discuss alternative approaches to the realization of this principle, which holds that properties of program components should be preserved when those co ponents are composed in parallel with other program components. We review two programming languages, Strand and Program Composition Notation, that support compositionality via a small number of simple concepts, namely, monotone operations on shared opbects, a uniform addressing mechanism, and parallel composition. Both languages have been used extensively for large-scale application development, allowing us to provide an informed assessment of both their strengths and their weaknesses. We observe that while compositionality simplifies development of complex applications, the use of specialized languages hinders reuse of existing code and tools and the specification of domain decomposition strategies. This suggests an alternative approach based on small extensions to existing sequential languages. We conclude the article with a discussion of two languages that realized this strategy.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W1996360642",
    "type": "article"
  },
  {
    "title": "Pointer analysis for structured parallel programs",
    "doi": "https://doi.org/10.1145/596980.596982",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "Radu Rugina; Martin Rinard",
    "corresponding_authors": "",
    "abstract": "This paper presents a novel interprocedural, flow-sensitive, and context-sensitive pointer analysis algorithm for multithreaded programs that may concurrently update shared pointers. The algorithm is designed to handle programs with structured parallel constructs, including fork-join constructs, parallel loops, and conditionally spawned threads. For each pointer and each program point, the algorithm computes a conservative approximation of the memory locations to which that pointer may point. The algorithm correctly handles a wide range of programming language constructs, including recursive functions, recursively generated parallelism, function pointers, structures, arrays, nested structures and arrays, pointer arithmetic, casts between different pointer types, heap and stack allocated memory, shared global variables, and thread-private global variables. We have implemented the algorithm in the SUIF compiler system and used the implementation to analyze a set of multithreaded programs written in the Cilk programming language. Our experimental results show that the analysis has good precision and converges quickly for our set of Cilk programs.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2013578854",
    "type": "article"
  },
  {
    "title": "Suspension analyses for concurrent logic programs",
    "doi": "https://doi.org/10.1145/177492.177656",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Michael Codish; Moreno Falaschi; Kim Marriott",
    "corresponding_authors": "",
    "abstract": "Concurrent logic languages specify reactive systems which consist of collections of communicating processes. The presence of unintended suspended computations is a common programming error which is difficult to detect using standard debugging and testing techniques. We develop a number of analyses, based on abstract interpretation, which succeed if a program is definitely suspension free. If an analysis fails, the program may, or may not, be suspension free. Examples demonstrate that the analyses are practically useful. They are conceptually simple and easy to justify because they are based directly on the transition system semantics of concurrent logic programs. A naive analysis must consider all scheduling policies . However, it is proven that for our analyses it suffices to consider only one scheduling policy, allowing for efficient implementation.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2059656088",
    "type": "article"
  },
  {
    "title": "Static analysis of upper and lower bounds on dependences and parallelism",
    "doi": "https://doi.org/10.1145/183432.183525",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "William Pugh; David Wonnacott",
    "corresponding_authors": "",
    "abstract": "Existing compilers often fail to parallelize sequential code, even when a program can be manually transformed into parallel form by a sequence of well-understood transformations (as in the case for many of the Perfect Club Benchmark programs). These failures can occur for several reasons: the code transformations implemented in the compiler may not be sufficient to produce parallel code, the compiler may not find the proper sequence of transformations, or the compiler may not be able to prove that one of the necessary transformations is legal. When a compiler fails to extract sufficient parallelism from a program, the programmer may try to extract additional parallelism. Unfortunately, the programmer is typically left to search for parallelism without significant assistance. The compiler generally does not give feedback about which parts of the program might contain additional parallelism, or about the types of transformations that might be needed to realize this parallelism. Standard program transformations and dependence abstractions cannot be used to provide this feedback. In this paper, we propose a two-step approach to the search for parallelism in sequential programs. In the first step, we construct several sets of constraints that describe, for each statement, which iterations of that statement can be executed concurrently. By constructing constraints that correspond to different assumptions about which dependences might be eliminated through additional analysis, transformations, and user assertions, we can determine whether we can expose parallelism by eliminating dependences. In the second step of our search for parallelism, we examine these constraint sets to identify the kinds of transformations needed to exploit scalable parallelism. Our tests will identify conditional parallelism and parallelism that can be exposed by combinations of transformations that reorder the iteration space (such as loop interchange and loop peeling). This approach lets us distinguish inherently sequential code from code that contains unexploited parallelism. It also produces information about the kinds of transformations needed to parallelize the code, without worrying about the order of application of the transformations. Furthermore, when our dependence test is inexact we can identify which unresolved dependences inhibit parallelism by comparing the effects of assuming dependence or independence. We are currently exploring the use of this information in programmer-assisted parallelization.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2060221201",
    "type": "article"
  },
  {
    "title": "Effectivness of abstract interpretation in automatic parallelization",
    "doi": "https://doi.org/10.1145/316686.316688",
    "publication_date": "1999-03-01",
    "publication_year": 1999,
    "authors": "Francisco Bueno; María García de la Banda; Manuel V. Hermenegildo",
    "corresponding_authors": "",
    "abstract": "We report on a detailed study of the application and effectiveness of program analysis based on abstract interpretation of automatic program parallelization. We study the case of parallelizing logic programs using the notion of strict independence. We first propose and prove correct a methodology for the application in the parallelization task of the information inferred by abstract interpretation, using a parametric domain. The methodology is generic in the sense of allowing the use of different analysis domains. A number of well-known approximation domains are then studied and the transformation into the parametric domain defined. The transformation directly illustrates the revelance and applicability of each abstract domain for the application. Both local and global analyzers are then built using these domains and embedded in a complete parallelizing compiler. Then, the performance of the domains in this context is assessed through a number of experiments. A comparatively wide range of aspects is studied, from the resources needed by the analyzers in terms of time and memory to the actual benefits obtained from the information inferred. Such benefits are evaluated both in terms of the characteristics of the parallelized code and of the actual speedups obtained from it. The results show that data flow analysis plays an important role in achieving efficient parallelizations, and that the cost of such analysis con be reasonable even for quite sophisticated abstract domains. Furthermore, the results also offer significant insight into the characteristics of the domains, the demands of the application, and the trade-offs involved.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2065590295",
    "type": "article"
  },
  {
    "title": "Class analyses as abstract interpretations of trace semantics",
    "doi": "https://doi.org/10.1145/937563.937565",
    "publication_date": "2003-09-01",
    "publication_year": 2003,
    "authors": "Fausto Spoto; Thomas Jensen",
    "corresponding_authors": "",
    "abstract": "We use abstract interpretation to abstract a compositional trace semantics for a simple imperative object-oriented language into its projection over a set of program points called watchpoints . We say that the resulting watchpoint semantics is focused on the watchpoints. Every abstraction of the computational domain of this semantics induces an abstract, still compositional, and focused watchpoint semantics. This establishes a basis for developing static analyses obtaining information pertaining only to the watchpoints. As an example, we consider three domains for class analysis of object-oriented programs derived from three techniques present in the literature, namely, rapid type analysis, a simple dataflow analysis, and a constraint-based analysis. We obtain three static analyses which are provably correct and whose abstract operations are provably optimal. Moreover, we prove that our formalization of the constraint-based analysis is more precise than that of the other two analyses. We have implemented our watchpoint semantics and our three domains for class analysis. This implementation shows that the time and space costs of the analysis are actually proportional to the number of watchpoints, as a consequence of the focused nature of the watchpoint semantics.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2075091218",
    "type": "article"
  },
  {
    "title": "Automatic isolation of compiler errors",
    "doi": "https://doi.org/10.1145/186025.186103",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "David Whalley",
    "corresponding_authors": "David Whalley",
    "abstract": "This paper describes a tool called vpoiso that was developed to isolate errors automatically in the vpo compiler system. The two general types of compiler errors isolated by this tool are optimization and nonoptimization errors. When isolating optimization errors, vpoiso relies on the vpo optimizer to identify sequences of changes, referred to as transformations, that result in semantically equivalent code and to provide the ability to stop performing improving (or unnecessary) transformations after a specified number have been performed. A compilation of a typical program by vpo often results in thousands of improving transformations being performed. The vpoiso tool can automatically isolate the first improving transformation that causes incorrect output of the execution of the compiled programs by using a binary search that varies the number of improving transformation performed. Not only is the illegal transformation automatically isolated, but vpoiso also identifies the location and instant the transformation is performed in vpo . Nonoptimization errors occur from problems in the front end, code generator, and necessary transformations in the optimizer. If another compiler is available that can produce correct (but perhaps more inefficient) code, then vpoiso can isolate nonoptimization errors to a single function. Automatic isolation of compiler errors facilitates retargeting a compiler to a new machine, maintenance of the compiler, and supporting experimentation with new optimizations.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2095064458",
    "type": "article"
  },
  {
    "title": "Automatic transformation of series expressions into loops",
    "doi": "https://doi.org/10.1145/114005.102806",
    "publication_date": "1991-01-01",
    "publication_year": 1991,
    "authors": "Richard C. Waters",
    "corresponding_authors": "Richard C. Waters",
    "abstract": "The benefits of programming in a functional style are well known. In particular, algorithms that are expressed as compositions of functions operating on sequences/vectors/streams of data elements are easier to understand and modify than equivalent algorithms expressed as loops. Unfortunately, this kind of expression is not used anywhere near as often as it could be, for at least three reasons: (1) most programmers are less familiar with this kind of expression than with loops; (2) most programming languages provide poor support for this kind of expression; and (3) when support is provided, it is seldom effcient. In any programming language, the second and third problems can be largely solved by introducing a data type called series , a comprehensive set of procedures operating on series, and a preprocessor (or compiler extension) that automatically converts most series expressions into efficient loops. A set of restrictions specifies which series expressions can be optimized. If programmers stay within the limits imposed, they are guaranteed of high efficiency at all times. A common Lisp macro package supporting series has been in use for some time. A prototype demonnstrates that series can be straightforwardly supported in Pascal.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W1968942384",
    "type": "article"
  },
  {
    "title": "Efficient fault-tolerant algorithms for distributed resource allocation",
    "doi": "https://doi.org/10.1145/203095.203101",
    "publication_date": "1995-05-01",
    "publication_year": 1995,
    "authors": "Manhoi Choy; Ambuj K. Singh",
    "corresponding_authors": "",
    "abstract": "Solutions to resource allocation problems and other related synchronization problems in distributed systems are examined with respect to the measures of response time, message complexity, and failure locality . Response time measures the time it takes for an algorithm to respond to the requests of a process; message complexity measures the number of messages sent and received by a process; and failure locality characterizes the size of the network that is affected by the failure of a single process. An algorithm for the resource allocation problem that achieves a constant failure locality of four along with a quadratic response time and a quadratic message complexity is presented. Applications of the algorithm to other process synchronization problems in distributed systems are also demonstrated.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2032721352",
    "type": "article"
  },
  {
    "title": "Using types to analyze and optimize object-oriented programs",
    "doi": "https://doi.org/10.1145/383721.383732",
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Amer Diwan; Kathryn S. McKinley; J. Eliot B. Moss",
    "corresponding_authors": "",
    "abstract": "Object-oriented programming languages provide many software engineering benefits, but these often come at a performance cost. Object-oriented programs make extensive use of method invocations and pointer dereferences, both of which are potentially costly on modern machines. We show how to use types to produce effective, yet simple, techniques that reduce the costs of these features in Modula-3, a statically typed, object-oriented language. Our compiler performs type-based alias analysis to disambiguate memory references. It uses the results of the type-based alias analysis to eliminate redundant memory references and to replace monomorphic method invocation sites with direct calls. Using limit, static, and running time evaluation, we demonstrate that these techniques are effective, and sometimes perfect for a set of Modula-3 benchmarks.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2045903735",
    "type": "article"
  },
  {
    "title": "BURS automata generation",
    "doi": "https://doi.org/10.1145/203095.203098",
    "publication_date": "1995-05-01",
    "publication_year": 1995,
    "authors": "Todd A. Proebsting",
    "corresponding_authors": "Todd A. Proebsting",
    "abstract": "A simple and efficient algorithm for generating bottom-up rewrite system (BURS) tables is described. A small code-generator generator implementation produces BURS tables efficiently, even for complex instruction set descriptions. The algorithm does not require novel data structures or complicated algorithmic techniques. Previously published methods for on-the-fly elimination of states are generalized and simplified to create a new method, triangle trimming, that is employed in the algorithm. A prototype implementation, burg, generates BURS tables very efficiently.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2056609175",
    "type": "article"
  },
  {
    "title": "Polymorphic predicate abstraction",
    "doi": "https://doi.org/10.1145/1057387.1057391",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Thomas Ball; Todd Millstein; Sriram K. Rajamani",
    "corresponding_authors": "",
    "abstract": "Predicate abstraction is a technique for creating abstract models of software that are amenable to model checking algorithms. We show how polymorphism, a well-known concept in programming languages and program analysis, can be incorporated in a predicate abstraction algorithm for C programs. The use of polymorphism in predicates, via the introduction of symbolic names for values, allows us to model the effect of a procedure independent of its calling contexts. Therefore, we can safely and precisely abstract a procedure once and then reuse this abstraction across multiple calls and multiple applications containing the procedure. Polymorphism also enables us to handle programs that need to be analyzed in an open environment, for all possible callers. We have proved that our algorithm is sound and have implemented it in the C2BP tool as part of the SLAM software model checking toolkit.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2086991228",
    "type": "article"
  },
  {
    "title": "A fast and accurate framework to analyze and optimize cache memory behavior",
    "doi": "https://doi.org/10.1145/973097.973099",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Xavier Vera; Nerina Bermudo; Josep Llosa; Antonio González",
    "corresponding_authors": "",
    "abstract": "The gap between processor and main memory performance increases every year. In order to overcome this problem, cache memories are widely used. However, they are only effective when programs exhibit sufficient data locality. Compile-time program transformations can significantly improve the performance of the cache. To apply most of these transformations, the compiler requires a precise knowledge of the locality of the different sections of the code, both before and after being transformed.Cache miss equations (CMEs) allow us to obtain an analytical and precise description of the cache memory behavior for loop-oriented codes. Unfortunately, a direct solution of the CMEs is computationally intractable due to its NP-complete nature.This article proposes a fast and accurate approach to estimate the solution of the CMEs. We use sampling techniques to approximate the absolute miss ratio of each reference by analyzing a small subset of the iteration space. The size of the subset, and therefore the analysis time, is determined by the accuracy selected by the user. In order to reduce the complexity of the algorithm to solve CMEs, effective mathematical techniques have been developed to analyze the subset of the iteration space that is being considered. These techniques exploit some properties of the particular polyhedra represented by CMEs.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2103388991",
    "type": "article"
  },
  {
    "title": "Cost and precision tradeoffs of dynamic data slicing algorithms",
    "doi": "https://doi.org/10.1145/1075382.1075384",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Xiangyu Zhang; Rajiv Gupta; Youtao Zhang",
    "corresponding_authors": "",
    "abstract": "Dynamic slicing algorithms are used to narrow the attention of the user or an algorithm to a relevant subset of executed program statements. Although dynamic slicing was first introduced to aid in user level debugging, increasingly applications aimed at improving software quality, reliability, security, and performance are finding opportunities to make automated use of dynamic slicing. In this paper we present the design and evaluation of three precise dynamic data slicing algorithms called the full preprocessing (FP), no preprocessing (NP) and limited preprocessing (LP) algorithms. The algorithms differ in the relative timing of constructing the dynamic data dependence graph and its traversal for computing requested dynamic data slices. Our experiments show that the LP algorithm is a fast and practical precise data slicing algorithm. In fact we show that while precise data slices can be orders of magnitude smaller than imprecise dynamic data slices, for small number of data slicing requests, the LP algorithm is faster than an imprecise dynamic data slicing algorithm proposed by Agrawal and Horgan.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2134936699",
    "type": "article"
  },
  {
    "title": "Efficient evaluation of circular attribute grammars",
    "doi": "https://doi.org/10.1145/78969.78971",
    "publication_date": "1990-07-01",
    "publication_year": 1990,
    "authors": "Larry G. Jones",
    "corresponding_authors": "Larry G. Jones",
    "abstract": "We present efficient algorithms for exhaustive and incremental evaluation of circular attributes under any conditions that guarantee finite convergence. The algorithms are derived from those for noncircular attribute grammars by partitioning the underlying attribute dependency graph into its strongly connected components and by ordering the evaluations to follow a topological sort of the resulting directed acyclic graph. The algorithms are efficient in the sense that their worst-case running time is proportional to the cost of computing the fixed points of only those strongly connected components containing affected attributes or attributes directly dependent on affected attributes. When the attribute grammar is noncircular or the specific dependency graph under consideration is acyclic, both algorithms reduce to the standard optimal algorithms for noncircular attribute evaluation.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2158747086",
    "type": "article"
  },
  {
    "title": "Symmetry and reduced symmetry in model checking",
    "doi": "https://doi.org/10.1145/1011508.1011511",
    "publication_date": "2004-07-01",
    "publication_year": 2004,
    "authors": "A. Prasad Sistla; Patrice Godefroid",
    "corresponding_authors": "",
    "abstract": "Symmetry reduction methods exploit symmetry in a system in order to efficiently verify its temporal properties. Two problems may prevent the use of symmetry reduction in practice: (1) the property to be checked may distinguish symmetric states and hence not be preserved by the symmetry, and (2) the system may exhibit little or no symmetry. In this article, we present a general framework that addresses both of these problems. We introduce \"Guarded Annotated Quotient Structures\" for compactly representing the state space of systems even when those are asymmetric. We then present algorithms for checking any temporal property on such representations, including non-symmetric properties.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2107675344",
    "type": "article"
  },
  {
    "title": "Director strings as combinators",
    "doi": "https://doi.org/10.1145/48022.48026",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "Richard Kennaway; Ronan Sleep",
    "corresponding_authors": "",
    "abstract": "A simple calculus (the Director String Calculus-DSC) for expressing abstractions is introduced, which captures the essence of the “long reach” combinators introduced by Turner. We present abstraction rules that preserve the applicative structure of the original lambda term, and that cannot increase the number of subterms in the translation. A translated lambda term can be reduced according to the evaluation rules of DSC. If this terminates with a DSC normal form, this can be translated into a lambda term using rules presented below. We call this process of abstracting a lambda term, reducing to normal form in the space of DSC terms, and translating back to a lambda term an implementation . We show that our implementation of the lambda calculus is correct: For lambda terms with a normal form that contains no lambdas ( ground terms ), the implementation is shown to yield a lambda calculus normal form. For lambda terms whose normal forms represent functions, it is shown that the implementation yields lambda terms that are beta-convertible in zero or more steps to the normal form of the original lambda term. In this sense, our implementation involves weak reduction according to Hindley et al. [9].",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2160667911",
    "type": "article"
  },
  {
    "title": "Interprocedural slicing of multithreaded programs with applications to Java",
    "doi": "https://doi.org/10.1145/1186632.1186636",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Mangala Gowri Nanda; S. Ramesh",
    "corresponding_authors": "",
    "abstract": "Slicing is a well-known program reduction technique where for a given program P and a variable of interest v at some statement P in the program, a program slice contains those set of statements belonging to P that affect v . This article presents two algorithms for interprocedural slicing of concurrent programs--a context-insensitive algorithm and a context-sensitive algorithm. The context-insensitive algorithm is efficient and correct (it includes every statement that may affect the slicing criterion) but is imprecise since it may include certain extra statements that are unnecessary. Precise slicing has been shown to be undecidable for concurrent programs. However, the context-sensitive algorithm computes correct and reasonably precise slices, but has a worst-case exponential-time complexity. Our context-sensitive algorithm computes a closure of dependencies while ensuring that statements sliced in each thread belong to a realizable path in that thread.A realizable path in a thread with procedure calls is one that reflects the fact that when a procedure finishes, execution returns to the site of the most recently executed call in that thread. One of the novelties of this article is a practical solution to determine whether a given set of statements in a thread may belong to a realizable path. This solution is precise even in the presence of recursion and long call chains in the flow graph.The slicing algorithms are applicable to concurrent programs with shared memory, interleaving semantics, explicit wait/notify synchronization and monitors. We first give a solution for a simple model of concurrency and later show how to extend the solution to the Java concurrency model. We have implemented the algorithms for Java bytecode and give experimental results.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2046065453",
    "type": "article"
  },
  {
    "title": "A denotational semantics for Prolog",
    "doi": "https://doi.org/10.1145/69558.69564",
    "publication_date": "1989-10-01",
    "publication_year": 1989,
    "authors": "Tim Nicholson; Norman Foo",
    "corresponding_authors": "",
    "abstract": "A denotational semantics is presented for the language Pro.og. Metapredicates are not considered. Conventional control sequencing is assumed for Prolog's execution. The semantics is nonstandard, and goal continuations are used to explicate the sequencing.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W1985058553",
    "type": "article"
  },
  {
    "title": "Correctness proofs of distributed termination algorithms",
    "doi": "https://doi.org/10.1145/5956.6000",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Krzysztof R. Apt",
    "corresponding_authors": "Krzysztof R. Apt",
    "abstract": "The problem of correctness of the solutions to the distributed termination problem of Francez [7] is addressed. Correctness criteria are formalized in the customary framework for program correctness. A very simple proof method is proposed and applied to show correctness of a solution to the problem. It allows us to reason about liveness properties of temporal logic (see, e.g., Manna and Pnueli [12]) using a new notion of weak total correctness .",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2059229220",
    "type": "article"
  },
  {
    "title": "Efficient demand-driven evaluation. Part 1",
    "doi": "https://doi.org/10.1145/3318.3480",
    "publication_date": "1985-04-01",
    "publication_year": 1985,
    "authors": "Keshav Pingali; Arvind Arvind",
    "corresponding_authors": "",
    "abstract": "We describe a program transformation technique for programs in a general stream language L whereby a data-driven evaluation of the transformed program performs exactly the same computation as a demand-driven evaluation of the original program. The transformational technique suggests a simple denotational characterization of demand-driven evaluation.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2093716079",
    "type": "article"
  },
  {
    "title": "Denali",
    "doi": "https://doi.org/10.1145/1186632.1186633",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Rajeev Joshi; Greg Nelson; Y. Zhou",
    "corresponding_authors": "",
    "abstract": "This article presents a design for the Denali-2 superoptimizer, which will generate minimum-instruction-length machine code for realistic machine architectures using automatic theorem-proving technology: specifically, using E-graph matching (a technique for pattern matching in the presence of equality information) and Boolean satisfiability solving.This article presents a precise definition of the underlying automatic programming problem solved by the Denali-2 superoptimizer. It sketches the E-graph matching phase and presents a detailed exposition and proof of soundness of the reduction of the automatic programming problem to the Boolean satisfiability problem.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1977279860",
    "type": "article"
  },
  {
    "title": "Methods for Computing LALR( <i>k</i> ) Lookahead",
    "doi": "https://doi.org/10.1145/357121.357126",
    "publication_date": "1981-01-01",
    "publication_year": 1981,
    "authors": "Bent Bruun Kristensen; Ole Lehrmann Madsen",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Methods for Computing LALR(k) Lookahead Authors: Bent Bruun Kristensen Aalborg University Center, Aalborg, Denmark Aalborg University Center, Aalborg, DenmarkView Profile , Ole Lehrmann Madsen Computer Science Department, Aarhus University, Ny Munkegade, DK-8000, Aarhus C, Denmark Computer Science Department, Aarhus University, Ny Munkegade, DK-8000, Aarhus C, DenmarkView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 1Jan. 1981 pp 60–82https://doi.org/10.1145/357121.357126Published:01 January 1981Publication History 28citation683DownloadsMetricsTotal Citations28Total Downloads683Last 12 Months100Last 6 weeks50 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2034782178",
    "type": "article"
  },
  {
    "title": "Annotations to Control Parallelism and Reduction Order in the Distributed Evaluation of Functional Programs",
    "doi": "https://doi.org/10.1145/2993.357241",
    "publication_date": "1984-04-01",
    "publication_year": 1984,
    "authors": "F. Warren Burton",
    "corresponding_authors": "F. Warren Burton",
    "abstract": "article Free Access Share on Annotations to Control Parallelism and Reduction Order in the Distributed Evaluation of Functional Programs Author: F. Warren Burton Department of Electrical Engineering and Computer Science, University of Colorado at Denver, Denver, CO Department of Electrical Engineering and Computer Science, University of Colorado at Denver, Denver, COView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 2April 1984 pp 159–174https://doi.org/10.1145/2993.357241Published:01 April 1984Publication History 29citation292DownloadsMetricsTotal Citations29Total Downloads292Last 12 Months6Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2042566745",
    "type": "article"
  },
  {
    "title": "Encapsulating objects with confined types",
    "doi": "https://doi.org/10.1145/1286821.1286823",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Christian Grothoff; Jens Palsberg; Jan Vítek",
    "corresponding_authors": "",
    "abstract": "Object-oriented languages provide little support for encapsulating objects. Reference semantics allows objects to escape their defining scope, and the pervasive aliasing that ensues remains a major source of software defects. This paper presents Kacheck/J, a tool for inferring object encapsulation properties of large Java programs. Our goal is to develop practical tools to assist software engineers, thus we focus on simple and scalable techniques. Kacheck/J is able to infer confinement —the property that all instances of a given type are encapsulated in their defining package. This simple property can be used to identify accidental leaks of sensitive objects, as well as for compiler optimizations. We report on the analysis of a large body of code and discuss language support and refactoring for confinement.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2027222319",
    "type": "article"
  },
  {
    "title": "Parallel Generation of Postfix and Tree Forms",
    "doi": "https://doi.org/10.1145/2166.357211",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Eliezer Dekel; Sartaj Sahni",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Parallel Generation of Postfix and Tree Forms Authors: Eliezer Dekel Mathematical Sciences Program, University of Texas at Dallas, Richardson, TX Mathematical Sciences Program, University of Texas at Dallas, Richardson, TXView Profile , Sartaj Sahni Department of Computer Science, University of Minnesota Twin Cities, 136 Lind Hall, 207 Church Street S.E., Minneapolis, MN Department of Computer Science, University of Minnesota Twin Cities, 136 Lind Hall, 207 Church Street S.E., Minneapolis, MNView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 301 July 1983pp 300–317https://doi.org/10.1145/2166.357211Published:01 July 1983Publication History 32citation423DownloadsMetricsTotal Citations32Total Downloads423Last 12 Months18Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2065890574",
    "type": "article"
  },
  {
    "title": "Profile-based pretenuring",
    "doi": "https://doi.org/10.1145/1180475.1180477",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Stephen M. Blackburn; Matthew Hertz; Kathryn S. McKinley; J. Eliot B. Moss; Ting Yang",
    "corresponding_authors": "",
    "abstract": "Pretenuring can reduce copying costs in garbage collectors by allocating long-lived objects into regions that the garbage collector will rarely, if ever, collect. We extend previous work on pretenuring as follows: (1) We produce pretenuring advice that is neutral with respect to the garbage collector algorithm and configuration. We thus can and do combine advice from different applications. We find for our benchmarks that predictions using object lifetimes at each allocation site in Java programs are accurate, which simplifies the pretenuring implementation. (2) We gather and apply advice to both applications and Jikes RVM, a compiler and runtime system for Java written in Java. Our results demonstrate that building combined advice into Jikes RVM from different application executions improves performance, regardless of the application Jikes RVM is compiling and executing. This build-time advice thus gives user applications some benefits of pretenuring, without any application profiling. No previous work uses profile feedback to pretenure in the runtime system. (3) We find that application-only advice also consistently improves performance, but that the combination of build-time and application-specific advice is almost always noticeably better. (4) Our same advice improves the performance of generational, Older First, and Beltway collectors, illustrating that it is collector neutral . (5) We include an immortal allocation space in addition to a nursery and older generation, and show that pretenuring to immortal space has substantial benefit.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2078973953",
    "type": "article"
  },
  {
    "title": "A Weaker Precondition for Loops",
    "doi": "https://doi.org/10.1145/69622.357189",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "H.J. Boom",
    "corresponding_authors": "H.J. Boom",
    "abstract": "article Free Access Share on A Weaker Precondition for Loops Author: H. J. Boom Computer Science Department, Concordia University, 1455 de Maisonneuve Boulevard West, Montreal, P.Q., Canada H3G 1M8 Computer Science Department, Concordia University, 1455 de Maisonneuve Boulevard West, Montreal, P.Q., Canada H3G 1M8View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 668–677https://doi.org/10.1145/69622.357189Published:01 October 1982Publication History 26citation454DownloadsMetricsTotal Citations26Total Downloads454Last 12 Months33Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2102484310",
    "type": "article"
  },
  {
    "title": "XARK",
    "doi": "https://doi.org/10.1145/1391956.1391959",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Manuel Arenaz; Juan Touriño; Ramón Doallo",
    "corresponding_authors": "",
    "abstract": "The recognition of program constructs that are frequently used by software developers is a powerful mechanism for optimizing and parallelizing compilers to improve the performance of the object code. The development of techniques for automatic recognition of computational kernels such as inductions, reductions and array recurrences has been an intensive research area in the scope of compiler technology during the 90's. This article presents a new compiler framework that, unlike previous techniques that focus on specific and isolated kernels, recognizes a comprehensive collection of computational kernels that appear frequently in full-scale real applications. The XARK compiler operates on top of the Gated Single Assignment (GSA) form of a high-level intermediate representation (IR) of the source code. Recognition is carried out through a demand-driven analysis of this high-level IR at two different levels. First, the dependences between the statements that compose the strongly connected components (SCCs) of the data-dependence graph of the GSA form are analyzed. As a result of this intra-SCC analysis, the computational kernels corresponding to the execution of the statements of the SCCs are recognized. Second, the dependences between statements of different SCCs are examined in order to recognize more complex kernels that result from combining simpler kernels in the same code. Overall, the XARK compiler builds a hierarchical representation of the source code as kernels and dependence relationships between those kernels. This article describes in detail the collection of computational kernels recognized by the XARK compiler. Besides, the internals of the recognition algorithms are presented. The design of the algorithms enables to extend the recognition capabilities of XARK to cope with new kernels, and provides an advanced symbolic analysis framework to run other compiler techniques on demand. Finally, extensive experiments showing the effectiveness of XARK for a collection of benchmarks from different application domains are presented. In particular, the SparsKit-II library for the manipulation of sparse matrices, the Perfect benchmarks, the SPEC CPU2000 collection and the PLTMG package for solving elliptic partial differential equations are analyzed in detail.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2010714123",
    "type": "article"
  },
  {
    "title": "Use of a Nonprocedural Specification Language and Associated Program Generator in Software Development",
    "doi": "https://doi.org/10.1145/357073.357076",
    "publication_date": "1979-10-01",
    "publication_year": 1979,
    "authors": "Noah S. Prywes; Amir and S. Shastry",
    "corresponding_authors": "",
    "abstract": "The Model II language and the associated program generator are used to explain and illustrate the use of very high level nonprocedural languages for computer programming. The effect of a very high level language is obtained in Model II through the elimination of procedural and control facilities that exist in high level programming languages such as PL/I or Cobol. In particular, the statements may be given in any order and there are no control constructs such as input/output, iterations, and memory allocation. The task of ordering the statements for execution and providing control statements is performed by the automatic program generator. The specification of a program is therefore much shorter (approximately one-fifth) than the equivalent high level procedural language program. Most important, a user need not regard the task of specifying a program as defining a process but rather as describing data and relations. This point of view greatly reduces the computer programming proficiency required of a user. The paper focuses on an example of the use of the language in business data processing, its advantages, and its novelty. It only briefly reviews the methodology incorporated in the existing program generator, a detailed description of which may be found in the references.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W1982188882",
    "type": "article"
  },
  {
    "title": "Generation of Compiler Symbol Processing Mechanisms from Specifications",
    "doi": "https://doi.org/10.1145/69624.69625",
    "publication_date": "1983-04-01",
    "publication_year": 1983,
    "authors": "Stephen P. Reiss",
    "corresponding_authors": "Stephen P. Reiss",
    "abstract": "article Free Access Share on Generation of Compiler Symbol Processing Mechanisms from Specifications Author: Stephen P. Reiss Brown Univ., Providence, RI Brown Univ., Providence, RIView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 2April 1983 pp 127–163https://doi.org/10.1145/69624.69625Published:01 April 1983Publication History 31citation423DownloadsMetricsTotal Citations31Total Downloads423Last 12 Months22Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2047404945",
    "type": "article"
  },
  {
    "title": "R for Semantics",
    "doi": "https://doi.org/10.1145/357162.357170",
    "publication_date": "1982-04-01",
    "publication_year": 1982,
    "authors": "E. A. Ashcroft; William W. Wadge",
    "corresponding_authors": "",
    "abstract": "article Free AccessR for Semantics Authors: E. A. Ashcroft Department of Computer Science, University of Waterloo, Waterloo, Ontario N2L 3G1, Canada Department of Computer Science, University of Waterloo, Waterloo, Ontario N2L 3G1, CanadaView Profile , W. W. Wadge Department of Computer Science, University of Warwick, Coventry, England Department of Computer Science, University of Warwick, Coventry, EnglandView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 2April 1982 pp 283–294https://doi.org/10.1145/357162.357170Published:01 April 1982Publication History 27citation352DownloadsMetricsTotal Citations27Total Downloads352Last 12 Months17Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2085859074",
    "type": "article"
  },
  {
    "title": "PEAK—a fast and effective performance tuning system via compiler optimization orchestration",
    "doi": "https://doi.org/10.1145/1353445.1353451",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Zhelong Pan; Rudolf Eigenmann",
    "corresponding_authors": "",
    "abstract": "Compile-time optimizations generally improve program performance. Nevertheless, degradations caused by individual compiler optimization techniques are to be expected. Feedback-directed optimization orchestration systems generate optimized code versions under a series of optimization combinations, evaluate their performance, and search for the best version. One challenge to such systems is to tune program performance quickly in an exponential search space. Another challenge is to achieve high program performance, considering that optimizations interact. Aiming at these two goals, this article presents an automated performance tuning system, PEAK , which searches for the best compiler optimization combinations for the important code sections in a program. The major contributions made in this work are as follows: (1) An algorithm called Combined Elimination (CE) is developed to explore the optimization space quickly and effectively; (2) Three fast and accurate rating methods are designed to evaluate the performance of an optimized code section based on a partial execution of the program; (3) An algorithm is developed to identify important code sections as candidates for performance tuning by trading off tuning speed and tuned program performance; and (4) A set of compiler tools are implemented to automate optimization orchestration. Orchestrating optimization options in SUN Forte compilers at the whole-program level, our CE algorithm improves performance by 10.8% over the SPEC CPU2000 FP baseline setting, compared to 5.6% improved by manual tuning. Orchestrating GCC O3 optimizations, CE improves performance by 12% over O3, the highest optimization level. Applying the rating methods, PEAK reduces tuning time from 2.19 hours to 5.85 minutes on average, while achieving equal or better program performance.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2058021293",
    "type": "article"
  },
  {
    "title": "An Alternative to the Use of Patterns in String Processing",
    "doi": "https://doi.org/10.1145/357094.357096",
    "publication_date": "1980-04-01",
    "publication_year": 1980,
    "authors": "Ralph E. Griswold; David R. Hanson",
    "corresponding_authors": "",
    "abstract": "SNOBOL4 is best known for its string processing facilities, which are based on patterns as data objects. Despite the demonstrated success of patterns, there are many shortcomings associated with their use. The concept of patterns in SNOBOL4 is examined and problem areas are discussed. An alternative method for high-level string processing is described. This method, implemented in the programming language Icon, employs generators , which are capable of producing alternative values. Generators, coupled with a goal-driven method of expression evaluation, provide the string processing facilities of SNOBOL4 without the disadvantages associated with patterns. Comparisons between SNOBOL4 and Icon are included and the broader implications of the new approach are discussed.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2023675244",
    "type": "article"
  },
  {
    "title": "Global Context Recovery: A New Strategy for Syntactic Error Recovery by Table-Drive Parsers",
    "doi": "https://doi.org/10.1145/357084.357086",
    "publication_date": "1980-01-01",
    "publication_year": 1980,
    "authors": "Ajit B. Pai; Richard B. Kieburtz",
    "corresponding_authors": "",
    "abstract": "Described is a method for syntactic error recovery that is compatible with deterministic parsing methods and that is able to recover from many errors more quickly than do other schemes because it performs global context recovery. The method relies on fiducial symbols, which are typically reserved key words of a language, to provide mileposts for error recovery. The method has been applied to LL(1) parsers, for which a detailed algorithm is given, and informally proved correct. The algorithm will always recover and return control to the parser if the text being analyzed satisfies only minimal requirements: that it contains one or more occurrences of fiducial symbols following the point at which an error is detected. Tables needed for error recovery have been automatically generated, along with parsing tables, by a parser constructor for the LL(1) grammars. A theoretical characterization of fiducial symbols is given, and the utility of this characterization in practice is discussed. It has been applied to a grammar for the programming language Pascal to aid in selection of a set of fiducial symbols. The error recovery scheme has been tested on a set of student-written Pascal program texts and is compared with other error recovery strategies.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2037338965",
    "type": "article"
  },
  {
    "title": "Operators",
    "doi": "https://doi.org/10.1145/357073.357074",
    "publication_date": "1979-10-01",
    "publication_year": 1979,
    "authors": "Kenneth E. Iverson",
    "corresponding_authors": "Kenneth E. Iverson",
    "abstract": "Although operators, which apply to functions to produce functions, prove very useful in mathematics, they are absent from most programming languages. This paper illustrates their simplicity and power in terms of the operators of APL, and examines related constructs in other programming languages.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W4244391805",
    "type": "article"
  },
  {
    "title": "Expressive and modular predicate dispatch for Java",
    "doi": "https://doi.org/10.1145/1462166.1462168",
    "publication_date": "2009-02-01",
    "publication_year": 2009,
    "authors": "Todd Millstein; Christopher Frost; Jason Ryder; Alessandro Warth",
    "corresponding_authors": "",
    "abstract": "Predicate dispatch is an object-oriented (OO) language mechanism for determining the method implementation to be invoked upon a message send. With predicate dispatch, each method implementation includes a predicate guard specifying the conditions under which the method should be invoked, and logical implication of predicates determines the method overriding relation. Predicate dispatch naturally unifies and generalizes several common forms of dynamic dispatch, including traditional OO dispatch, multimethod dispatch, and functional-style pattern matching. Unfortunately, prior languages supporting predicate dispatch have had several deficiencies that limit the practical utility of this language feature. We describe JPred, a backward-compatible extension to Java supporting predicate dispatch. While prior languages with predicate dispatch have been extensions to toy or nonmainstream languages, we show how predicate dispatch can be naturally added to a traditional OO language. While prior languages with predicate dispatch have required the whole program to be available for typechecking and compilation, JPred retains Java's modular typechecking and compilation strategies. While prior languages with predicate dispatch have included special-purpose algorithms for reasoning about predicates, JPred employs general-purpose, off-the-shelf decision procedures. As a result, JPred's type system is more flexible, allowing several useful programming idioms that are spuriously rejected by those other languages. After describing the JPred language informally, we present an extension to Featherweight Java that formalizes the language and its modular type system, which we have proven sound. Finally, we discuss two case studies that illustrate the practical utility of JPred, including its use in the detection of several errors.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2155266165",
    "type": "article"
  },
  {
    "title": "Analysis of Recursively Parallel Programs",
    "doi": "https://doi.org/10.1145/2518188",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Ahmed Bouajjani; Michael Emmi",
    "corresponding_authors": "",
    "abstract": "We propose a general formal model of isolated hierarchical parallel computations, and identify several fragments to match the concurrency constructs present in real-world programming languages such as Cilk and X10. By associating fundamental formal models (vector addition systems with recursive transitions) to each fragment, we provide a common platform for exposing the relative difficulties of algorithmic reasoning. For each case we measure the complexity of deciding state reachability for finite-data recursive programs, and propose algorithms for the decidable cases. The complexities which include PTIME, NP, EXPSPACE, and 2EXPTIME contrast with undecidable state reachability for recursive multithreaded programs.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2000116086",
    "type": "article"
  },
  {
    "title": "Solving systems of rational equations through strategy iteration",
    "doi": "https://doi.org/10.1145/1961204.1961207",
    "publication_date": "2011-04-01",
    "publication_year": 2011,
    "authors": "Thomas Martin Gawlitza; Helmut Seidl",
    "corresponding_authors": "",
    "abstract": "We present practical algorithms for computing exact least solutions of equation systems over the reals with addition, multiplication by positive constants, minimum and maximum. The algorithms are based on strategy iteration. Our algorithms can, for instance, be used for the analysis of recursive stochastic games. In the present article we apply our techniques for computing abstract least fixpoint semantics of affine programs over the relational template polyhedra domain. In particular, we thus obtain practical algorithms for computing abstract least fixpoint semantics over the abstract domains of intervals, zones, and octagons.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2055372726",
    "type": "article"
  },
  {
    "title": "On the Termination of Integer Loops",
    "doi": "https://doi.org/10.1145/2400676.2400679",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Amir M. Ben-Amram; Samir Genaim; Abu Naser Masud",
    "corresponding_authors": "",
    "abstract": "In this article we study the decidability of termination of several variants of simple integer loops, without branching in the loop body and with affine constraints as the loop guard (and possibly a precondition). We show that termination of such loops is undecidable in some cases, in particular, when the body of the loop is expressed by a set of linear inequalities where the coefficients are from Z ∪ { r } with r an arbitrary irrational; when the loop is a sequence of instructions, that compute either linear expressions or the step function; and when the loop body is a piecewise linear deterministic update with two pieces. The undecidability result is proven by a reduction from counter programs, whose termination is known to be undecidable. For the common case of integer linear-constraint loops with rational coefficients we have not succeeded in proving either decidability or undecidability of termination, but we show that a Petri net can be simulated with such a loop; this implies some interesting lower bounds. For example, termination for a partially specified input is at least EXPSPACE-hard.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1971872488",
    "type": "article"
  },
  {
    "title": "SPL",
    "doi": "https://doi.org/10.1145/3039207",
    "publication_date": "2017-03-06",
    "publication_year": 2017,
    "authors": "Martin Hirzel; Scott Schneider; Buğra Gedik",
    "corresponding_authors": "",
    "abstract": "Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing . Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2591953675",
    "type": "article"
  },
  {
    "title": "Interval Analysis and Machine Arithmetic",
    "doi": "https://doi.org/10.1145/2651360",
    "publication_date": "2015-01-20",
    "publication_year": 2015,
    "authors": "Graeme Gange; Jorge A. Navas; Peter Schachte; Harald Søndergaard; Peter J. Stuckey",
    "corresponding_authors": "",
    "abstract": "The most commonly used integer types have fixed bit-width, making it possible for computations to “wrap around,” and many programs depend on this behaviour. Yet much work to date on program analysis and verification of integer computations treats integers as having infinite precision, and most analyses that do respect fixed width lose precision when overflow is possible. We present a novel integer interval abstract domain that correctly handles wrap-around. The analysis is signedness agnostic. By treating integers as strings of bits, only considering signedness for operations that treat them differently, we produce precise, correct results at a modest cost in execution time.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1996543446",
    "type": "article"
  },
  {
    "title": "Global Sparse Analysis Framework",
    "doi": "https://doi.org/10.1145/2590811",
    "publication_date": "2014-09-25",
    "publication_year": 2014,
    "authors": "Hakjoo Oh; Kihong Heo; Wonchan Lee; Woosuk Lee; Daejun Park; Jeehoon Kang; Kwangkeun Yi",
    "corresponding_authors": "",
    "abstract": "In this article, we present a general method for achieving global static analyzers that are precise and sound, yet also scalable. Our method, on top of the abstract interpretation framework, is a general sparse analysis technique that supports relational as well as nonrelational semantics properties for various programming languages. Analysis designers first use the abstract interpretation framework to have a global and correct static analyzer whose scalability is unattended. Upon this underlying sound static analyzer, analysis designers add our generalized sparse analysis techniques to improve its scalability while preserving the precision of the underlying analysis. Our method prescribes what to prove to guarantee that the resulting sparse version should preserve the precision of the underlying analyzer. We formally present our framework and show that existing sparse analyses are all restricted instances of our framework. In addition, we show more semantically elaborate design examples of sparse nonrelational and relational static analyses. We then present their implementation results that scale to globally analyze up to one million lines of C programs. We also show a set of implementation techniques that turn out to be critical to economically support the sparse analysis process.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2049311281",
    "type": "article"
  },
  {
    "title": "Context-Free Session Type Inference",
    "doi": "https://doi.org/10.1145/3229062",
    "publication_date": "2019-03-15",
    "publication_year": 2019,
    "authors": "Luca Padovani",
    "corresponding_authors": "Luca Padovani",
    "abstract": "Some interesting communication protocols can be precisely described only by context-free session types, an extension of conventional session types supporting a general form of sequential composition. The complex metatheory of context-free session types, however, hinders the definition of corresponding checking and inference algorithms. In this work, we study a new syntax-directed type system for context-free session types that is easy to embed into a host programming language. We also detail 2 OCaml embeddings that allow us to piggyback on OCaml’s type system to check and infer context-free session types.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2922398162",
    "type": "article"
  },
  {
    "title": "Obsidian",
    "doi": "https://doi.org/10.1145/3417516",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Michael Coblenz; Reed Oei; Tyler Etzel; Paulette Koronkevich; Miles Baker; Yannick Bloem; Brad A. Myers; Joshua Sunshine; Jonathan Aldrich",
    "corresponding_authors": "",
    "abstract": "Blockchain platforms are coming into use for processing critical transactions among participants who have not established mutual trust. Many blockchains are programmable, supporting smart contracts , which maintain persistent state and support transactions that transform the state. Unfortunately, bugs in many smart contracts have been exploited by hackers. Obsidian is a novel programming language with a type system that enables static detection of bugs that are common in smart contracts today. Obsidian is based on a core calculus, Silica, for which we proved type soundness. Obsidian uses typestate to detect improper state manipulation and uses linear types to detect abuse of assets. We integrated a permissions system that encodes a notion of ownership to allow for safe, flexible aliasing. We describe two case studies that evaluate Obsidian’s applicability to the domains of parametric insurance and supply chain management, finding that Obsidian’s type system facilitates reasoning about high-level states and ownership of resources. We compared our Obsidian implementation to a Solidity implementation, observing that the Solidity implementation requires much boilerplate checking and tracking of state, whereas Obsidian does this work statically.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3106677187",
    "type": "article"
  },
  {
    "title": "Deep Dive into ZGC: A Modern Garbage Collector in OpenJDK",
    "doi": "https://doi.org/10.1145/3538532",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Albert Mingkun Yang; Tobias Wrigstad",
    "corresponding_authors": "",
    "abstract": "ZGC is a modern, non-generational, region-based, mostly concurrent, parallel, mark-evacuate collector recently added to OpenJDK. It aims at having GC pauses that do not grow as the heap size increases, offering low latency even with large heap sizes. The ZGC C++ source code is readily accessible in the OpenJDK repository, but reading it (25 KLOC) can be very intimidating, and one might easily get lost in low-level implementation details, obscuring the key concepts. To make the ZGC algorithm more approachable, this work provides a thorough description on a high-level, focusing on the overall design with moderate implementation details. To explain the concurrency aspects, we provide a SPIN model that allows studying races between mutators and GC threads, and how they are resolved in ZGC. Such a model is not only useful for learning the current design (offering a deterministic and interactive experience) but also beneficial for prototyping new ideas and extensions. Our hope is that our detailed description and the SPIN model will enable the use of ZGC as a building block for future GC research, and research ideas implemented on top of it could even be adopted in the industry more readily, bridging the gap between academia and industry in the context of GC research.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4281383262",
    "type": "article"
  },
  {
    "title": "Contextual Linear Types for Differential Privacy",
    "doi": "https://doi.org/10.1145/3589207",
    "publication_date": "2023-04-06",
    "publication_year": 2023,
    "authors": "Matías Toro; David Darais; Chiké Abuah; Joseph P. Near; Damián Árquez; Federico Olmedo; Éric Tanter",
    "corresponding_authors": "",
    "abstract": "Language support for differentially private programming is both crucial and delicate. While elaborate program logics can be very expressive, type-system-based approaches using linear types tend to be more lightweight and amenable to automatic checking and inference, and in particular in the presence of higher-order programming. Since the seminal design of Fuzz , which is restricted to ϵ-differential privacy in its original design, significant progress has been made to support more advanced variants of differential privacy, like (ϵ, δ )-differential privacy. However, supporting these advanced privacy variants while also supporting higher-order programming in full has proven to be challenging. We present Jazz , a language and type system that uses linear types and latent contextual effects to support both advanced variants of differential privacy and higher-order programming. Latent contextual effects allow delaying the payment of effects for connectives such as products, sums, and functions, yielding advantages in terms of precision of the analysis and annotation burden upon elimination, as well as modularity. We formalize the core of Jazz , prove it sound for privacy via a logical relation for metric preservation, and illustrate its expressive power through a number of case studies drawn from the recent differential privacy literature.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3093630031",
    "type": "article"
  },
  {
    "title": "A Verified Optimizer for Quantum Circuits",
    "doi": "https://doi.org/10.1145/3604630",
    "publication_date": "2023-07-12",
    "publication_year": 2023,
    "authors": "Kesha Hietala; Robert W. Rand; Liyi Li; Shih-Han Hung; Xiaodi Wu; Michael Hicks",
    "corresponding_authors": "",
    "abstract": "We present voqc , the first verified optimizer for quantum circuits, written using the Coq proof assistant. Quantum circuits are expressed as programs in a simple, low-level language called sqir , a small quantum intermediate representation, which is deeply embedded in Coq. Optimizations and other transformations are expressed as Coq functions, which are proved correct with respect to a semantics of sqir programs. sqir programs denote complex-valued matrices, as is standard in quantum computation, but we treat matrices symbolically to reason about programs that use an arbitrary number of quantum bits. sqir ’s careful design and our provided automation make it possible to write and verify a broad range of optimizations in voqc , including full-circuit transformations from cutting-edge optimizers.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4384030121",
    "type": "article"
  },
  {
    "title": "Adversities in Abstract Interpretation - Accommodating Robustness by Abstract Interpretation",
    "doi": "https://doi.org/10.1145/3649309",
    "publication_date": "2024-02-24",
    "publication_year": 2024,
    "authors": "Roberto Giacobazzi; Isabella Mastroeni; Elia Perantoni",
    "corresponding_authors": "",
    "abstract": "Robustness is a key and desirable property of any classifying system, in particular, to avoid the ever-rising threat of adversarial attacks. Informally, a classification system is robust when the result is not affected by the perturbation of the input. This notion has been extensively studied, but little attention has been dedicated to how the perturbation affects the classification. The interference between perturbation and classification can manifest in many different ways, and its understanding is the main contribution of the present article. Starting from a rigorous definition of a standard notion of robustness, we build a formal method for accommodating the required degree of robustness—depending on the amount of error the analyst may accept on the classification result. Our idea is to precisely model this error as an abstraction . This leads us to define weakened forms of robustness also in the context of programming languages, particularly in language-based security, e.g., information-flow policies, and in program verification. The latter is possible by moving from a quantitative (standard) model of perturbation to a novel qualitative model, given by means of the notion of abstraction. As in language-based security, we show that it is possible to confine adversities, which means to characterize the degree of perturbation (and/or the degree of class generalization) for which the classifier may be deemed adequately robust. We conclude with an experimental evaluation of our ideas, showing how weakened forms of robustness apply to state-of-the-art image classifiers.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4392131517",
    "type": "article"
  },
  {
    "title": "A study of the applicability of existing exception-handling techniques to component-based real-time software technology",
    "doi": "https://doi.org/10.1145/276393.276395",
    "publication_date": "1998-03-01",
    "publication_year": 1998,
    "authors": "Jun Lang; David B. Stewart",
    "corresponding_authors": "",
    "abstract": "This study focuses on the current state of error-handling technology and concludes with recommendations for further research in error handling for component-based real-time software. With real-time programs growing in size and complexity, the quality and cost of developing and maintaining them are still deep concerns to embedded software industries. Component-based software is a promising approach in reducing development cost while increasing quality and reliability. As with any other real-time software, component-based software needs exception detection and handling mechanisms to satisfy reliability requirements. The current lack of suitable error-handling techniques can make an application composed of reusable software nondeterministic and difficult to understand in the presence of errors.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2000083322",
    "type": "article"
  },
  {
    "title": "A global communication optimization technique based on data-flow analysis and linear algebra",
    "doi": "https://doi.org/10.1145/330643.330647",
    "publication_date": "1999-11-01",
    "publication_year": 1999,
    "authors": "Mahmut Kandemir; P. Banerjee; Alok Choudhary; J. Ramanujam; Niraj Shenoy",
    "corresponding_authors": "",
    "abstract": "Reducing communication overhead is extremely important in distributed-memory message-passing architectures. In this article, we present a technique to improve communication that considers data access patterns of the entire program. Our approach is based on a combination of traditional data-flow analysis and a linear algebra framework, and it works on structured programs with conditional statements and nested loops but without arbitrary goto statements.The distinctive features of the solution are the accuracy in keeping communication set information, support for general alignments and distributions including block-cyclic distribu-tions, and the ability to simulate some of the previous approaches with suitable modifications. We also show how optimizations such as message vectorization, message coalescing, and redundancy elimination are supported by our framework. Experimental results on several benchmarks show that our technique is effective in reducing the number of messages (anaverage of 32% reduction), the volume of the data communicated (an average of 37%reduction), and the execution time (an average of 26% reduction).",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2069224457",
    "type": "article"
  },
  {
    "title": "Synthesis of concurrent programs for an atomic read/write model of computation",
    "doi": "https://doi.org/10.1145/383043.383044",
    "publication_date": "2001-03-01",
    "publication_year": 2001,
    "authors": "Paul C. Attie; E. Allen Emerson",
    "corresponding_authors": "",
    "abstract": "Methods for mechanically synthesizing concurrent programs for temporal logic specifications have been proposed by Emerson and Clarke and by Manna and Wolper. An important advantage of these synthesis methods is that they obviate the need to manually compose a program and manually construct a proof of its correctness. A serious drawback of these methods in practice, however, is that they produce concurrent programs for models of computation that are often unrealistic, involving highly centralized system architecture (Manna and Wolper), processes with global information about the system state (Emerson and Clarke), or reactive modules that can read all of their inputs in one atomic step (Anuchitanukul and Manna, and Pnueli and Rosner). Even simple synchronization protocols based on atomic read/write primitives such as Peterson's solution to the mutual exclusion problem have remained outside the scope of practical mechanical synthesis methods. In this paper, we show how to mechanically synthesize in more realistic computational models solutions to synchronization problems. We illustrate the method by synthesizing Peterson's solution to the mutual exclusion problem.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2070369873",
    "type": "article"
  },
  {
    "title": "Standard fixpoint iteration for Java bytecode verification",
    "doi": "https://doi.org/10.1145/363911.363915",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "Zhenyu Qian",
    "corresponding_authors": "Zhenyu Qian",
    "abstract": "Java bytecode verification forms the basis for Java-based Internet security and needs a rigorous description. One important aspect of bytecode verification is to check if a Java Virtual Machine (JVM) program is statically well-typed. So far, several formal specifications have been proposed to define what the static well-typedness means. This paper takes a step further and presents a chaotic fixpoint iteration, which represents a family of fixpoint computation strategies to compute a least type for each JVM program within a finite number of iteration steps. Since a transfer function in the iteration is not monotone, we choose to follow the example of a nonstandard fixpoint theorem, which requires that all transfer functions are increasing, and monotone in case the bigger element is already a fixpoint. The resulting least type is the artificial top element if and only if he JVM program is not statically well-typed. The iteration is standard and close to Sun's informal specification and most commercial bytecode verifiers.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W1996469056",
    "type": "article"
  },
  {
    "title": "Type-preserving compilation of Featherweight Java",
    "doi": "https://doi.org/10.1145/514952.514954",
    "publication_date": "2002-03-01",
    "publication_year": 2002,
    "authors": "Christopher League; Zhong Shao; Valery Trifonov",
    "corresponding_authors": "",
    "abstract": "We present an efficient encoding of core Java constructs in a simple, implementable typed intermediate language. The encoding, after type erasure, has the same operational behavior as a standard implementation using vtables and self-application for method invocation. Classes inherit super-class methods with no overhead. We support mutually recursive classes while preserving separate compilation. Our strategy extends naturally to a significant subset of Java, including interfaces and privacy. The formal translation using Featherweight Java allows comprehensible type-preservation proofs and serves as a starting point for extending the translation to new features. Our work provides a foundation for supporting certifying compilation of Java-like class-based languages in a type-theoretic framework.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2081596204",
    "type": "article"
  },
  {
    "title": "Lessons learned about one-way, dataflow constraints in the Garnet and Amulet graphical toolkits",
    "doi": "https://doi.org/10.1145/506315.506318",
    "publication_date": "2001-11-01",
    "publication_year": 2001,
    "authors": "Bradley T. Vander Zanden; Richard Halterman; Brad A. Myers; Rich McDaniel; Rob Miller; Pedro Szekely; Dario A. Giuse; David S. Kosbie",
    "corresponding_authors": "",
    "abstract": "One-way, dataflow constraints are commonly used in graphical interface toolkits, programming environments, and circuit applications. Previous papers on dataflow constraints have focused on the design and implementation of individual algorithms. In contrast, this article focuses on the lessons we have learned from a decade of implementing competing algorithms in the Garnet and Amulet graphical interface toolkits. These lessons reveal the design and implementation tradeoffs for different one-way, constraint satisfaction algorithms. The most important lessons we have learned are that (1) mark-sweep algorithms are more efficient than topological ordering algorithms; (2) lazy and eager evaluators deliver roughly comparable performance for most applications; and (3) constraint satisfaction algorithms have more than adequate speed, except that the storage required by these algorithms can be problematic.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2102400845",
    "type": "article"
  },
  {
    "title": "Register tiling in nonrectangular iteration spaces",
    "doi": "https://doi.org/10.1145/567097.567101",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Marta M. Jimenez; J.M. Llaberia; A. Fernández",
    "corresponding_authors": "",
    "abstract": "Loop tiling is a well-known loop transformation generally used to expose coarse-grain parallelism and to exploit data reuse at the cache level. Tiling can also be used to exploit data reuse at the register level and to improve a program's ILP. However, previous proposals in the literature (as well as commercial compilers) are only able to perform multidimensional tiling for the register level when the iteration space is rectangular. In this article we present a new general algorithm to perform multidimensional tiling for the register level in both rectangular and nonrectangular iteration spaces. We also propose a simple heuristic to determine the tiling parameters at this level. Finally, we evaluate our method using as benchmarks typical linear algebra algorithms having nonrectangular iteration spaces and compare our proposal against hand-optimized vendor-supplied numerical libraries and against commercial compilers able to perform optimizing code transformations such as inner unrolling, unroll-and-jam, and software pipelining. Measurements were taken on three different superscalar microprocessors. Results will show that our method outperforms the native compilers (showing speedups of 2.5 in average) and matches the performance of vendor-supplied numerical libraries. The general conclusion is that compiler technology can make it possible for nonrectangular loop nests to achieve as high performance as hand-optimized codes.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1973359136",
    "type": "article"
  },
  {
    "title": "JR",
    "doi": "https://doi.org/10.1145/982158.982162",
    "publication_date": "2004-05-01",
    "publication_year": 2004,
    "authors": "Aaron W. Keen; Tingjian Ge; Justin T. Maris; Ronald A. Olsson",
    "corresponding_authors": "",
    "abstract": "Java provides a clean object-oriented programming model and allows for inherently system-independent programs. Unfortunately, Java has a limited concurrency model, providing only threads and remote method invocation (RMI).The JR programming language extends Java to provide a rich concurrency model, based on that of SR. JR provides dynamic remote virtual machine creation, dynamic remote object creation, remote method invocation, asynchronous communication, rendezvous, and dynamic process creation. JR's concurrency model stems from the addition of operations (a generalization of procedures) and JR supports the redefinition of operations through inheritance. JR programs are written in an extended Java and then translated into standard Java programs. The JR run-time support system is also written in standard Java.This paper describes the JR programming language and its implementation. Some initial measurements of the performance of the implementation are also included.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2018863170",
    "type": "article"
  },
  {
    "title": "A first-come-first-served mutual-exclusion algorithm with small communication variables",
    "doi": "https://doi.org/10.1145/115372.115370",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Edward A. Lycklama; Vassos Hadzilacos",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A first-come-first-served mutual-exclusion algorithm with small communication variables Authors: Edward A. Lycklama Univ. of Toronto, Toronto, Ont., Canada Univ. of Toronto, Toronto, Ont., CanadaView Profile , Vassos Hadzilacos Univ. of Toronto, Toronto, Ont., Canada Univ. of Toronto, Toronto, Ont., CanadaView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 401 October 1991pp 558–576https://doi.org/10.1145/115372.115370Published:01 October 1991Publication History 30citation647DownloadsMetricsTotal Citations30Total Downloads647Last 12 Months32Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2094104092",
    "type": "article"
  },
  {
    "title": "A foundation for embedded languages",
    "doi": "https://doi.org/10.1145/641909.641910",
    "publication_date": "2003-05-01",
    "publication_year": 2003,
    "authors": "Morten Rhiger",
    "corresponding_authors": "Morten Rhiger",
    "abstract": "Recent work on embedding object languages into Haskell use \"phantom types\" (i.e., parameterized types whose parameter does not occur on the right-hand side of the type definition) to ensure that the embedded object-language terms are simply typed. But is it a safe assumption that only simply-typed terms can be represented in Haskell using phantom types? And conversely, can all simply-typed terms be represented in Haskell under the restrictions imposed by phantom types? In this article we investigate the conditions under which these assumptions are true: We show that these questions can be answered affirmatively for an idealized Haskell-like language and discuss to which extent Haskell can be used as a meta-language.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1968470225",
    "type": "article"
  },
  {
    "title": "Experience with a software-defined machine architecture",
    "doi": "https://doi.org/10.1145/129393.129395",
    "publication_date": "1992-05-01",
    "publication_year": 1992,
    "authors": "David W. Wall",
    "corresponding_authors": "David W. Wall",
    "abstract": "We have built a system in which the compiler back end and the linker work together to present an abstract machine at a considerably higher level than the actual machine. The intermediate language translated by the back end is the target language of all high-level compilers and is also the only assembly language generally available. This lets us do intermodule register allocation, which would be harder if some of the code in the program had come from a traditional assembler, out of sight of the optimizer. We do intermodule register allocation and pipeline instruction scheduling at link time, using information gathered by the compiler back end. The mechanism for analyzing and modifying the program at link time is also useful in a wide array of instrumentation tools.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2012182306",
    "type": "article"
  },
  {
    "title": "Toward compiler implementation correctness proofs",
    "doi": "https://doi.org/10.1145/5397.30847",
    "publication_date": "1986-04-01",
    "publication_year": 1986,
    "authors": "Laurian M. Chirica; David F. Martin",
    "corresponding_authors": "",
    "abstract": "Aspect of the interaction between compiler theory and practice is addressed. Presented is a technique for the syntax-directed specification of compilers together with a method for proving the correctness of their parse-driven implementations. The subject matter is presented in an order-algebraic framework; while not strictly necessary, this approach imposes beneficial structure and modularity on the resulting specifications and implementation correctness proofs. Compilers are specified using an order-algebraic definition of attribute grammars. A practical class of compiler implementations is considered, consisting of those driven by LR( k ) or LL( k ) parsers which cause a sequence of translation routine activations to modify a suitably initialized collection of data structures (called a translation environment). The implementation correctness criterion consists of appropriately comparing, for each source program, the corresponding object program (contained in the final translation environment) produced by the compiler implementation to the object program dictated by the compiler specification. Provided that suitable intermediate assertions (called translation invariants ) are supplied, the program consisting of the (parse-induced) sequence of translation routine activations can be proven partially correct via standard inductive assertion methods.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2058125478",
    "type": "article"
  },
  {
    "title": "Constrained expressions: toward broad applicability of analysis methods for distributed software systems",
    "doi": "https://doi.org/10.1145/44501.44502",
    "publication_date": "1988-07-01",
    "publication_year": 1988,
    "authors": "Laura K. Dillon; George S. Avrunin; Jack C. Wileden",
    "corresponding_authors": "",
    "abstract": "It is extremely difficult to characterize the possible behaviors of a distributed software system through informal reasoning. Developers of distributed systems require tools that support formal reasoning about properties of the behaviors of their systems. These tools should be applicable to designs and other preimplementation descriptions of a system, as well as to completed programs. Furthermore, they should not limit a developer's choice of development languages. In this paper we present a basis for broadly applicable analysis methods for distributed software systems. The constrained expression formalism can be used with a wide variety of distributed system development notations to give a uniform closed-form representation of a system's behavior. A collection of formal analysis techniques can then be applied with this representation to establish properties of the system. Examples of these formal analysis techniques appear elsewhere. Here we illustrate the broad applicability of the constrained expression formalism by showing how constrained expression representations are obtained from descriptions of systems in three different notations: SDYMOL, CSP, and Petri nets. Features of these three notations span most of the significant alternatives for describing distributed software systems. Our examples thus offer persuasive evidence for the broad applicability of the constrained expression approach.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W1984852853",
    "type": "article"
  },
  {
    "title": "Applicative caching",
    "doi": "https://doi.org/10.1145/5001.5004",
    "publication_date": "1986-01-02",
    "publication_year": 1986,
    "authors": "Robert Keller; M. R. Sleep",
    "corresponding_authors": "",
    "abstract": "The “referential transparency” principle of applicative language expressions stipulates that a single value exists for all occurrences of an expression in a given context (where a context is a set of bindings of variables to values). In principle, each such value therefore need to be computed only once. However, in applicative language systems supporting recursive programming or tasking notions, the bindings are not all precomputed and explicit. As a result, textual recognition of all multipleoccurrences is precluded, with the unfortunate consequence that such occurrences are recomputed. We elaborate upon the early notion of “memo function” for solving this problem. We suggest syntactic and semantic constructs providing programmer control for avoiding recomputation, which is incorporated into a “building-block” approach.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2294416791",
    "type": "article"
  },
  {
    "title": "A bisimulation-based semantic theory of Safe Ambients",
    "doi": "https://doi.org/10.1145/1119479.1119482",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Massimo Merro; Matthew Hennessy",
    "corresponding_authors": "",
    "abstract": "We develop a semantics theory for SAP, a variant of Levi and Sangiorgi's Safe Ambients, SA.The dynamics of SA relies upon capabilities (and co-capabilities ) exercised by mobile agents , called ambients , to interact with each other. These capabilities contain references, the names of ambients with which they wish to interact. In SAP we generalize the notion of capability: in order to interact with an ambient n , an ambient m must exercise a capability indicating both n and a password h to access n ; the interaction between n and m takes place only if n is willing to perform a corresponding co-capability with the same password h . The name h can also be looked upon as a port to access ambient n via port h .In SAP, by managing passwords/ports, for example generating new ones and distributing them selectively, an ambient may now program who may migrate into its computation space, and when. Moreover in SAP, an ambient may provide different services/resources depending on the port accessed by the incoming clients. Then we give an lts -based operational semantics for SAP and a labelled bisimulation equivalence, which is proved to coincide with reduction barbed congruence .We use our notion of bisimulation to prove a set of algebraic laws that are subsequently exploited to prove more significant examples.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1990041920",
    "type": "article"
  },
  {
    "title": "Nontermination inference of logic programs",
    "doi": "https://doi.org/10.1145/1119479.1119481",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Étienne Payet; Fred Mesnard",
    "corresponding_authors": "",
    "abstract": "We present a static analysis technique for nontermination inference of logic programs. Our framework relies on an extension of the subsumption test, where some specific argument positions can be instantiated while others are generalized. We give syntactic criteria to statically identify such argument positions from the text of a program. Atomic left looping queries are generated bottom-up from selected subsets of the binary unfoldings of the program of interest. We propose a set of correct algorithms for automating the approach. Then, nontermination inference is tailored to attempt proofs of optimality of left termination conditions computed by a termination inference tool. An experimental evaluation is reported and the analyzers can be tried online at http://www.univ-reunion.fr/~gcc. When termination and nontermination analysis produce complementary results for a logic procedure, then with respect to the leftmost selection rule and the language used to describe sets of atomic queries, each analysis is optimal and together, they induce a characterization of the operational behavior of the logic procedure.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2014410720",
    "type": "article"
  },
  {
    "title": "An algebraic array shape inference system for MATLAB®",
    "doi": "https://doi.org/10.1145/1152649.1152651",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Pramod G. Joisha; Prithviraj Banerjee",
    "corresponding_authors": "",
    "abstract": "The problem of inferring array shapes ahead of time in languages that exhibit both implicit and dynamic typing is a critical one because the ramifications of its solution are the better organization of array storage through compaction and reuse, and the generation of high-performance code through specialization by shape. This article addresses the problem in a prototypical implicitly and dynamically typed array language called MATLAB. The approach involves modeling the language's shape semantics using an algebraic system, and applying term rewriting techniques to evaluate expressions under this algebra. Unlike prior efforts at array shape determination, this enables the deduction of valuable shape information even when array extents are compile-time unknowns. Furthermore, unlike some previous methods, our approach doesn't impose monotonicity requirements on an operator's shape semantics. The work also describes an inference methodology and reports measurements from a type inference engine called MAGICA. In a benchmark suite of 17 programs, the shape inference subsystem in MAGICA detected the equivalence of over 61% of the symbolic shapes in six programs, and over 57% and 37% of the symbolic shapes in two others. In the remaining nine programs, all array shapes were inferred to be compile-time constants.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2014954069",
    "type": "article"
  },
  {
    "title": "Some Observations Concerning Formal Differentiation of Set Theoretic Expressions",
    "doi": "https://doi.org/10.1145/357162.357166",
    "publication_date": "1982-04-01",
    "publication_year": 1982,
    "authors": "Micha Sharir",
    "corresponding_authors": "Micha Sharir",
    "abstract": "article Free Access Share on Some Observations Concerning Formal Differentiation of Set Theoretic Expressions Author: Micha Sharir School of Mathematical Sciences, Tel-Aviv University, Ramat-Aviv, Tel Aviv 69978, Israel School of Mathematical Sciences, Tel-Aviv University, Ramat-Aviv, Tel Aviv 69978, IsraelView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 2pp 196–225https://doi.org/10.1145/357162.357166Published:01 April 1982Publication History 23citation297DownloadsMetricsTotal Citations23Total Downloads297Last 12 Months17Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2046831192",
    "type": "article"
  },
  {
    "title": "Noncorrecting syntax error recovery",
    "doi": "https://doi.org/10.1145/3916.4019",
    "publication_date": "1985-07-01",
    "publication_year": 1985,
    "authors": "Helmut Richter",
    "corresponding_authors": "Helmut Richter",
    "abstract": "A parser must be able to continue parsing after encountering a syntactic error to check the remainder of the input. To achieve this, it is not necessary to perform corrections on either the input text or the stack contents. A formal framework is provided in which noncorrecting syntax error recovery concepts are defined and investigated. The simplicity of these concepts allows the statement of provable properties, such as the absence of spurious error messages or the avoidance of skipping input text. These properties are due to the fact that no assumptions about the nature of the errors need be made to continue parsing.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W1991654060",
    "type": "article"
  },
  {
    "title": "Program abstraction and instantiation",
    "doi": "https://doi.org/10.1145/3916.3986",
    "publication_date": "1985-07-01",
    "publication_year": 1985,
    "authors": "Nachum Dershowitz",
    "corresponding_authors": "Nachum Dershowitz",
    "abstract": "Our goal is to develop formal methods for abstracting a given set of programs into a program schema and for instantiating a given schema to satisfy concrete specifications. Abstraction and instantiation are two important phases in software development which allow programmers to apply knowledge learned in the solutions of past problems when faced with new situations. For example, from two programs using a linear (or binary) search technique, an abstract schema can be derived that embodies the shared idea and that can be instantiated to solve similar new problems. Along similar lines, the development and application of program transformations are considered. We suggest the formulation of analogies as a basic tool in program abstraction. An analogy is first sought between the specifications of the given programs; this yields an abstract specification that may be instantiated to any of the given concrete specifications. The analogy is then used as a basis for transforming the existing programs into an abstract schema that represents the embedded technique, with the invariant assertions and correctness proofs of the given programs helping to verify and complete the analogy. A given concrete specification of a new problem may then be compared with the abstract specification of the schema to suggest an instantiation of the schema that yields a correct program.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2046666535",
    "type": "article"
  },
  {
    "title": "<i>Forma</i>",
    "doi": "https://doi.org/10.1145/1290520.1290522",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Peng Zhao; Shimin Cui; Yaoqing Gao; Raúl Silvera; José Nelson Amaral",
    "corresponding_authors": "",
    "abstract": "This article presents Forma , a practical, safe, and automatic data reshaping framework that reorganizes arrays to improve data locality. Forma splits large aggregated data-types into smaller ones to improve data locality. Arrays of these large data types are then replaced by multiple arrays of the smaller types. These new arrays form natural data streams that have smaller memory footprints, better locality, and are more suitable for hardware stream prefetching. Forma consists of a field-sensitive alias analyzer, a data type checker, a portable structure reshaping planner, and an array reshaper. An extensive experimental study compares different data reshaping strategies in two dimensions: (1) how the data structure is split into smaller ones ( maximal partition × frequency-based partition × affinity-based partition ); and (2) how partitioned arrays are linked to preserve program semantics ( address arithmetic-based reshaping × pointer-based reshaping ). This study exposes important characteristics of array reshaping. First, a practical data reshaper needs not only an inter-procedural analysis but also a data-type checker to make sure that array reshaping is safe. Second, the performance improvement due to array reshaping can be dramatic: standard benchmarks can run up to 2.1 times faster after array reshaping. Array reshaping may also result in some performance degradation for certain benchmarks. An extensive micro-architecture-level performance study identifies the causes for this degradation. Third, the seemingly naive maximal partition achieves best or close-to-best performance in the benchmarks studied. This article presents an analysis that explains this surprising result. Finally, address-arithmetic-based reshaping always performs better than its pointer-based counterpart.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W1984005572",
    "type": "article"
  },
  {
    "title": "Associons: A Program Notation with Tuples Instead of Variables",
    "doi": "https://doi.org/10.1145/357139.357142",
    "publication_date": "1981-07-01",
    "publication_year": 1981,
    "authors": "M. Rem",
    "corresponding_authors": "M. Rem",
    "abstract": "article Free AccessAssocions: A Program Notation with Tuples Instead of Variables Author: Martin Rem Department of Mathematics, Eindhoven University of Technology, P. O. Box 513, 5600 MB Eindhoven, The Netherlands Department of Mathematics, Eindhoven University of Technology, P. O. Box 513, 5600 MB Eindhoven, The NetherlandsView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 3pp 251–262https://doi.org/10.1145/357139.357142Published:01 July 1981Publication History 26citation290DownloadsMetricsTotal Citations26Total Downloads290Last 12 Months13Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2065221338",
    "type": "article"
  },
  {
    "title": "A Deterministic Attribute Grammar Evaluator Based on Dynamic Scheduling",
    "doi": "https://doi.org/10.1145/357062.357072",
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "Ken Kennedy; Jayashree Ramanathan",
    "corresponding_authors": "",
    "abstract": "The problem of semantic evaluation in a compiler-generating system can be addressed by specifying language semantics in an attribute grammar [19], a context-free grammar augmented with “attributes” for the nonterminals and “semantic functions” to compute the attributes. A deterministic method for evaluating all attributes in a “semantic” parse tree is derived and shown to have time and space complexities which are essentially linear in the size of the tree. In a prepass through the parse tree, the method determines an evaluation sequence for the attributes; thus it is somewhat analogous to dynamic programming. The constructor-evaluator system described should be suitable for inclusion in a general translator-writing system.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2087852575",
    "type": "article"
  },
  {
    "title": "Size-change termination with difference constraints",
    "doi": "https://doi.org/10.1145/1353445.1353450",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Amir M. Ben-Amram",
    "corresponding_authors": "Amir M. Ben-Amram",
    "abstract": "This article considers an algorithmic problem related to the termination analysis of programs. More specifically, we are given bounds on differences in sizes of data values before and after every transition in the program's control-flow graph. Our goal is to infer program termination via the following reasoning (“the size-change principle”): if in any infinite (hypothetic) execution of the program, some size must descend unboundedly, the program must always terminate, since infinite descent of a natural number is impossible. The problem of inferring termination from such abstract information is not the halting problem for programs and may well be decidable. If this is the case, the decision algorithm forms a “back end” of a termination verifier, and it is interesting to find out the computational complexity of the problem. A restriction of the problem described above, which only uses monotonicity information (but not difference bounds), is already known to be decidable. We prove that the unrestricted problem is undecidable, which gives a theoretical argument for studying restricted cases. We consider a case where the termination proof is allowed to make use of at most one bound per target variable in each transition. For this special case, which we claim is practically significant, we give (for the first time) an algorithm and show that the problem is in PSPACE, in fact that it is PSPACE-complete. The algorithm is based on combinatorial arguments and results from the theory of integer programming not previously used for similar problems. The algorithm has interesting connections to other work in termination, in particular to methods for generating linear ranking functions or invariants.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2029579251",
    "type": "article"
  },
  {
    "title": "A uniform type structure for secure information flow",
    "doi": "https://doi.org/10.1145/1286821.1286822",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Kohei Honda; Nobuko Yoshida",
    "corresponding_authors": "",
    "abstract": "The π-calculus, a calculus of mobile processes, can compositionally represent dynamics of major programming constructs by decomposing them into name passing. The present work reports our experience in using a linear/affine typed π-calculus for the analysis and development of type-based analyses for programming languages, focussing on secure information flow analysis. After presenting a basic typed calculus for secrecy, we demonstrate its usage by a sound embedding of the dependency core calculus (DCC) and the development of the call-by-value version of DCC. The secrecy analysis is then extended to stateful computation, for which we develop a novel type discipline for imperative programming language that extends a secure multi-threaded imperative language by Smith and Volpano with general references and higher-order procedures. In each analysis, the embedding gives a simple proof of noninterference.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W1994158024",
    "type": "article"
  },
  {
    "title": "Ranking functions for size-change termination",
    "doi": "https://doi.org/10.1145/1498926.1498928",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Chin Soon Lee",
    "corresponding_authors": "Chin Soon Lee",
    "abstract": "This article explains how to construct a ranking function for any program that is proved terminating by size-change analysis . The “principle of size-change termination” for a first-order functional language with well-ordered data is intuitive: A program terminates on all inputs, if every infinite call sequence (following program control flow) would imply an infinite descent in some data values. Size-change analysis is based on information associated with the subject program's call-sites. This information indicates, for each call-site, strict or weak data decreases observed as a computation traverses the call-site. The set DESC of call-site sequences for which the size-changes imply infinite descent is ω-regular, as is the set FLOW of infinite call-site sequences following the program flowchart. If FLOW ⊆ DESC (a decidable problem), every infinite call sequence would imply infinite descent in a well-ordering—an impossibility—so the program must terminate. This analysis accounts for termination arguments applicable to different call-site sequences, without indicating a ranking function for the program's termination. In this article, it is explained how one can be constructed whenever size-change analysis succeeds. The constructed function has an unexpectedly simple form; it is expressed using only min, max, and lexicographic tuples of parameters and constants. In principle, such functions can be tested to determine whether size-change analysis will be successful. As a corollary, if a program verified as terminating performs only multiply recursive operations, the function that it computes is multiply recursive. The ranking function construction is connected with the determinization of the Büchi automaton for DESC . While the result is not practical, it is of value in addressing the scope of size-change reasoning. This reasoning has been applied broadly, in the analysis of functional and logic programs, as well as term rewrite systems.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2135021832",
    "type": "article"
  },
  {
    "title": "Checking type safety of foreign function calls",
    "doi": "https://doi.org/10.1145/1377492.1377493",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Michael Furr; Jeffrey S. Foster",
    "corresponding_authors": "",
    "abstract": "Foreign function interfaces (FFIs) allow components in different languages to communicate directly with each other. While FFIs are useful, they often require writing tricky low-level code and include little or no static safety checking, thus providing a rich source of hard-to-find programming errors. In this article, we study the problem of enforcing type safety across the OCaml-to-C FFI and the Java Native Interface (JNI). We present O-Saffire and J-Saffire, a pair of multilingual type inference systems that ensure C code that uses these FFIs accesses high-level data safely. Our inference systems use representational types to model C's low-level view of OCaml and Java values, and singleton types to track integers, strings, memory offsets, and type tags through C. J-Saffire, our Java system, uses a polymorphic flow-insensitive, unification-based analysis. Polymorphism is important because it allows us to precisely model user-defined wrapper functions and the more than 200 JNI functions. O-Saffire, our OCaml system, uses a monomorphic flow-sensitive analysis because, while polymorphism is much less important for the OCaml FFI flow-sensitivity is critical to track conditional branches, which are used when pattern matching OCaml data in C. O-Saffire also tracks garbage collection information to ensure that local C pointers to the OCaml heap are registered properly, which is not necessary for the JNI. We have applied O-Saffire and J-Saffire to a set of benchmarks and found many bugs and questionable coding practices. These results suggest that static checking of FFIs can be a valuable tool in writing correct multilingual software.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1973020382",
    "type": "article"
  },
  {
    "title": "Software model checking using languages of nested trees",
    "doi": "https://doi.org/10.1145/2039346.2039347",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "Rajeev Alur; Swarat Chaudhuri; P. Madhusudan",
    "corresponding_authors": "",
    "abstract": "While model checking of pushdown systems is by now an established technique in software verification, temporal logics and automata traditionally used in this area are unattractive on two counts. First, logics and automata traditionally used in model checking cannot express requirements such as pre/post-conditions that are basic to analysis of software. Second, unlike in the finite-state world, where the μ-calculus has a symbolic model-checking algorithm and serves as an “assembly language” to which temporal logics can be compiled, there is no common formalism—either fixpoint-based or automata-theoretic—to model-check requirements on pushdown models. In this article, we introduce a new theory of temporal logics and automata that addresses the above issues, and provides a unified foundation for the verification of pushdown systems. The key idea here is to view a program as a generator of structures known as nested trees as opposed to trees. A fixpoint logic (called N T -μ) and a class of automata (called nested tree automata ) interpreted on languages of these structures are now defined, and branching-time model-checking is phrased as language inclusion and membership problems for these languages. We show that N T -μ and nested tree automata allow the specification of a new frontier of requirements usable in software verification. At the same time, their model checking problem has the same worst-case complexity as their traditional analogs, and can be solved symbolically using a fixpoint computation that generalizes, and includes as a special case, “summary”-based computations traditionally used in interprocedural program analysis. We also show that our logics and automata define a robust class of languages—in particular, just as the μ-calculus is equivalent to alternating parity automata on trees, NT-μ is equivalent to alternating parity automata on nested trees.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2075969214",
    "type": "article"
  },
  {
    "title": "On Subtyping-Relation Completeness, with an Application to Iso-Recursive Types",
    "doi": "https://doi.org/10.1145/2994596",
    "publication_date": "2017-03-06",
    "publication_year": 2017,
    "authors": "Jay Ligatti; Jeremy Blackburn; Michael Nachtigal",
    "corresponding_authors": "",
    "abstract": "Well-known techniques exist for proving the soundness of subtyping relations with respect to type safety. However, completeness has not been treated with widely applicable techniques, as far as we’re aware. This article develops techniques for stating and proving that a subtyping relation is complete with respect to type safety and applies the techniques to the study of iso-recursive subtyping. A new proof technique, induction on failing derivations, is provided that may be useful in other domains as well. The common subtyping rules for iso-recursive types—the “Amber rules”—are shown to be incomplete with respect to type safety. That is, there exist iso-recursive types τ 1 and τ 2 such that τ 1 can safely be considered a subtype of τ 2 , but τ 1 ⩽ τ 2 is not derivable with the Amber rules. New, algorithmic rules are defined for subtyping iso-recursive types, and the rules are proved sound and complete with respect to type safety. The fully implemented subtyping algorithm is optimized to run in O ( mn ) time, where m is the number of μ-terms in the types being considered and n is the size of the types being considered.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2593125519",
    "type": "article"
  },
  {
    "title": "Multiple Facets for Dynamic Information Flow with Exceptions",
    "doi": "https://doi.org/10.1145/3024086",
    "publication_date": "2017-05-10",
    "publication_year": 2017,
    "authors": "Thomas H. Austin; Tommy Schmitz; Cormac Flanagan",
    "corresponding_authors": "",
    "abstract": "JavaScript is the source of many security problems, including cross-site scripting attacks and malicious advertising code. Central to these problems is the fact that code from untrusted sources runs with full privileges. Information flow controls help prevent violations of data confidentiality and integrity. This article explores faceted values , a mechanism for providing information flow security in a dynamic manner that avoids the stuck executions of some prior approaches, such as the no-sensitive-upgrade technique. Faceted values simultaneously simulate multiple executions for different security levels to guarantee termination-insensitive noninterference. We also explore the interaction of faceted values with exceptions, declassification, and clearance.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2612745372",
    "type": "article"
  },
  {
    "title": "From Clarity to Efficiency for Distributed Algorithms",
    "doi": "https://doi.org/10.1145/2994595",
    "publication_date": "2017-05-26",
    "publication_year": 2017,
    "authors": "Yanhong A. Liu; Scott D. Stoller; Bo Lin",
    "corresponding_authors": "",
    "abstract": "This article describes a very high-level language for clear description of distributed algorithms and optimizations necessary for generating efficient implementations. The language supports high-level control flows in which complex synchronization conditions can be expressed using high-level queries, especially logic quantifications, over message history sequences. Unfortunately, the programs would be extremely inefficient, including consuming unbounded memory, if executed straightforwardly. We present new optimizations that automatically transform complex synchronization conditions into incremental updates of necessary auxiliary values as messages are sent and received. The core of the optimizations is the first general method for efficient implementation of logic quantifications. We have developed an operational semantics of the language, implemented a prototype of the compiler and the optimizations, and successfully used the language and implementation on a variety of important distributed algorithms.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2950923141",
    "type": "article"
  },
  {
    "title": "Divergence analysis",
    "doi": "https://doi.org/10.1145/2523815",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Diogo Sampaio; Rafael Martins de Souza; Caroline Collange; Fernando Magno Quintão Pereira",
    "corresponding_authors": "",
    "abstract": "Growing interest in graphics processing units has brought renewed attention to the Single Instruction Multiple Data (SIMD) execution model. SIMD machines give application developers tremendous computational power; however, programming them is still challenging. In particular, developers must deal with memory and control-flow divergences. These phenomena stem from a condition that we call data divergence, which occurs whenever two processing elements (PEs) see the same variable name holding different values. This article introduces divergence analysis, a static analysis that discovers data divergences. This analysis, currently deployed in an industrial quality compiler, is useful in several ways: it improves the translation of SIMD code to non-SIMD CPUs, it helps developers to manually improve their SIMD applications, and it also guides the automatic optimization of SIMD programs. We demonstrate this last point by introducing the notion of a divergence-aware register spiller. This spiller uses information from our analysis to either rematerialize or share common data between PEs. As a testimony of its effectiveness, we have tested it on a suite of 395 CUDA kernels from well-known benchmarks. The divergence-aware spiller produces GPU code that is 26.21% faster than the code produced by the register allocator used in the baseline compiler.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2295329047",
    "type": "article"
  },
  {
    "title": "Automated Classification of Data Races Under Both Strong and Weak Memory Models",
    "doi": "https://doi.org/10.1145/2734118",
    "publication_date": "2015-05-22",
    "publication_year": 2015,
    "authors": "Baris Kasikci; Cristian Zamfir; George Candea",
    "corresponding_authors": "",
    "abstract": "Data races are one of the main causes of concurrency problems in multithreaded programs. Whether all data races are bad, or some are harmful and others are harmless, is still the subject of vigorous scientific debate [Narayanasamy et al. 2007; Boehm 2012]. What is clear, however, is that today's code has many data races [Kasikci et al. 2012; Jin et al. 2012; Erickson et al. 2010], and fixing data races without introducing bugs is time consuming [Godefroid and Nagappan 2008]. Therefore, it is important to efficiently identify data races in code and understand their consequences to prioritize their resolution. We present Portend + , a tool that not only detects races but also automatically classifies them based on their potential consequences: Could they lead to crashes or hangs? Could their effects be visible outside the program? Do they appear to be harmless? How do their effects change under weak memory models? Our proposed technique achieves high accuracy by efficiently analyzing multiple paths and multiple thread schedules in combination, and by performing symbolic comparison between program outputs. We ran Portend + on seven real-world applications: it detected 93 true data races and correctly classified 92 of them, with no human effort. Six of them were harmful races. Portend + 's classification accuracy is up to 89% higher than that of existing tools, and it produces easy-to-understand evidence of the consequences of “harmful” races, thus both proving their harmfulness and making debugging easier. We envision Portend + being used for testing and debugging, as well as for automatically triaging bug reports.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1876811830",
    "type": "article"
  },
  {
    "title": "Probabilistic Termination by Monadic Affine Sized Typing",
    "doi": "https://doi.org/10.1145/3293605",
    "publication_date": "2019-03-15",
    "publication_year": 2019,
    "authors": "Ugo Dal Lago; Charles Grellois",
    "corresponding_authors": "",
    "abstract": "We introduce a system of monadic affine sized types, which substantially generalizes usual sized types and allows in this way to capture probabilistic higher-order programs that terminate almost surely. Going beyond plain, strong normalization without losing soundness turns out to be a hard task, which cannot be accomplished without a richer, quantitative notion of types, but also without imposing some affinity constraints. The proposed type system is powerful enough to type classic examples of probabilistically terminating programs such as random walks. The way typable programs are proved to be almost surely terminating is based on reducibility but requires a substantial adaptation of the technique.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2921909284",
    "type": "article"
  },
  {
    "title": "A Machine-Learning Algorithm with Disjunctive Model for Data-Driven Program Analysis",
    "doi": "https://doi.org/10.1145/3293607",
    "publication_date": "2019-06-19",
    "publication_year": 2019,
    "authors": "Minseok Jeon; Sehun Jeong; Sungdeok Cha; Hakjoo Oh",
    "corresponding_authors": "",
    "abstract": "We present a new machine-learning algorithm with disjunctive model for data-driven program analysis. One major challenge in static program analysis is a substantial amount of manual effort required for tuning the analysis performance. Recently, data-driven program analysis has emerged to address this challenge by automatically adjusting the analysis based on data through a learning algorithm. Although this new approach has proven promising for various program analysis tasks, its effectiveness has been limited due to simple-minded learning models and algorithms that are unable to capture sophisticated, in particular disjunctive, program properties. To overcome this shortcoming, this article presents a new disjunctive model for data-driven program analysis as well as a learning algorithm to find the model parameters. Our model uses Boolean formulas over atomic features and therefore is able to express nonlinear combinations of program properties. A key technical challenge is to efficiently determine a set of good Boolean formulas, as brute-force search would simply be impractical. We present a stepwise and greedy algorithm that efficiently learns Boolean formulas. We show the effectiveness and generality of our algorithm with two static analyzers: context-sensitive points-to analysis for Java and flow-sensitive interval analysis for C. Experimental results show that our automated technique significantly improves the performance of the state-of-the-art techniques including ones hand-crafted by human experts.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2950121745",
    "type": "article"
  },
  {
    "title": "Debugging Large-scale Datalog",
    "doi": "https://doi.org/10.1145/3379446",
    "publication_date": "2020-04-17",
    "publication_year": 2020,
    "authors": "David Zhao; Pavle Subotić; Bernhard Scholz",
    "corresponding_authors": "",
    "abstract": "Logic programming languages such as Datalog have become popular as Domain Specific Languages (DSLs) for solving large-scale, real-world problems, in particular, static program analysis and network analysis. The logic specifications that model analysis problems process millions of tuples of data and contain hundreds of highly recursive rules. As a result, they are notoriously difficult to debug. While the database community has proposed several data provenance techniques that address the Declarative Debugging Challenge for Databases, in the cases of analysis problems, these state-of-the-art techniques do not scale. In this article, we introduce a novel bottom-up Datalog evaluation strategy for debugging: Our provenance evaluation strategy relies on a new provenance lattice that includes proof annotations and a new fixed-point semantics for semi-naïve evaluation. A debugging query mechanism allows arbitrary provenance queries, constructing partial proof trees of tuples with minimal height. We integrate our technique into Soufflé, a Datalog engine that synthesizes C++ code, and achieve high performance by using specialized parallel data structures. Experiments are conducted with D OOP /DaCapo, producing proof annotations for tens of millions of output tuples. We show that our method has a runtime overhead of 1.31× on average while being more flexible than existing state-of-the-art techniques.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3021830120",
    "type": "article"
  },
  {
    "title": "Static correlated branch prediction",
    "doi": "https://doi.org/10.1145/330249.330255",
    "publication_date": "1999-09-01",
    "publication_year": 1999,
    "authors": "Cliff Young; Michael D. Smith",
    "corresponding_authors": "",
    "abstract": "Recent work in history-based branch prediction uses novel hardware structures to capture branch correlation and increase branch prediction accuracy. Branch correlation occurs when the outcome of a conditional branch can be accurately predicted by observing the outcomes of previously executed branches in the dynamic instruction stream. In this article, we show how to instrument a program so that it is practical to collect run-time statistics that indicate where branch correlation occurs, and we then show how to use these statistics to transform the program so that its static branch prediction accuracy is improved. The run-time information that we gather is called a path profile , and it summarizes how often each executed sequence of program points occurs in the program trace. Our path proles are more general than those previously proposed. The code transformation that we present is called static correlated branch prediction (SCBP). It exhibits better branch prediction accuracy than previously thought possible for static prediction techniques. Furthermore, through the use of an overpruning heuristic, we show that it is possible to determine automatically an appropriate trade-off between code expansion and branch predictability so that our transformation improves the performance of multiple-issue, deeply pipelined microprocessors like those being built today.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1970348747",
    "type": "article"
  },
  {
    "title": "A unified model of pointwise equivalence of procedural computations",
    "doi": "https://doi.org/10.1145/197320.197402",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "David G. von Bank; Charles M. Shub; Robert W. Sebesta",
    "corresponding_authors": "",
    "abstract": "The execution of a program on a processor is viewed as a representation of that program going through a sequence of states. Each state change is manifested by the execution of a single instruction. Models that depend on this perspective are presented. The first is a static model of a description of a procedural computation. This model formalizes the description of the information in an executable module. Following this dynamic model of a procedural computation is given. This second model describes how a computation transitions from state to state and how the states of a computation are represented. Next, the state of a procedural computation is defined at certain well-defined points in its progression. These points represent potential points of correspondence to another instance of the computation. Then, the equivalence of these well-defined computation states is described. This refinement eliminates the nonmatching potential correspondences. The remaining points describe where the two computations are in the same state. These are precisely the points of equivalence of procedural computations. This final model of pointwise equivalence can be applied to the problem of migrating a computation from one processor to another (possibly architecturally dissimilar) processor.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1974264097",
    "type": "article"
  },
  {
    "title": "Analysis of Or-parallel execution models",
    "doi": "https://doi.org/10.1145/155183.155220",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Gopal Gupta; Bharat Jayaraman",
    "corresponding_authors": "",
    "abstract": "We discuss fundamental limitations of or-parallel execution models of nondeterministic programming languages. Or-parallelism corresponds to the execution of different nondeterministic computational paths in parallel. A natural way to represent the state of (parallel) execution of a nondeterministic program is by means of an or-parallel tree. We identify three important criteria that underlie the design of or-parallel implementations based on the or-parallel tree: constant-time access to variables, constant-time task creation, and constant-time task switching, where the term constant-time means that the time for these operations is independent of the number of nodes in the or-parallel tree, as well as the size of each node. We prove that all three criteria cannot be simultaneously satisfied by any or-parallel execution model based on a finite number of processors but unbounded memory. We discuss in detail the application of our result to the class of logic programming languages and show how our result can serve as a useful way to categorize the various or-parallel methods proposed in this field. We also discuss the suitability of different or-parallel implemenation strategies for different parallel architectures.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W1991003782",
    "type": "article"
  },
  {
    "title": "An assume-guarantee rule for checking simulation",
    "doi": "https://doi.org/10.1145/509705.509707",
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "Thomas A. Henzinger; Shaz Qadeer; Sriram K. Rajamani; Serdar Taşiran",
    "corresponding_authors": "",
    "abstract": "The simulation preorder on state transition systems is widely accepted as a useful notion of refinement, both in its own right and as an efficiently checkable sufficient condition for trace containment. For composite systems, due to the exponential explosion of the state space, there is a need for decomposing a simulation check of the form P ≤ s Q , denoting \" P is simulated by Q ,\" into simpler simulation checks on the components of P and Q . We present an assume-guarantee rule that enables such a decomposition. To the best of our knowledge, this is the first assume-guarantee rule that applies to a refinement relation different from trace containment. Our rule is circular, and its soundness proof requires induction on trace trees. The proof is constructive: given simulation relations that witness the simulation preorder between corresponding components of P and Q , we provide a procedure for constructing a witness relation for P ≤ s Q . We also extend our assume-guarantee rule to account for fairness constraints on transition systems.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2007378539",
    "type": "article"
  },
  {
    "title": "Automatic data and computation decomposition on distributed memory parallel computers",
    "doi": "https://doi.org/10.1145/509705.509706",
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "PeiZong Lee; Zvi M. Kedem",
    "corresponding_authors": "",
    "abstract": "To exploit parallelism on shared memory parallel computers (SMPCs), it is natural to focus on decomposing the computation (mainly by distributing the iterations of the nested Do-Loops). In contrast, on distributed memory parallel computers (DMPCs), the decomposition of computation and the distribution of data must both be handled---in order to balance the computation load and to minimize the migration of data. We propose and validate experimentally a method for handling computations and data synergistically to minimize the overall execution time on DMPCs. The method is based on a number of novel techniques, also presented in this article. The core idea is to rank the \"importance\" of data arrays in a program and specify some of the dominant. The intuition is that the dominant arrays are the ones whose migration would be the most expensive. Using the correspondence between iteration space mapping vectors and distributed dimensions of the dominant data array in each nested Do-loop, allows us to design algorithms for determining data and computation decompositions at the same time. Based on data distribution, computation decomposition for each nested Do-loop is determined based on either the \"owner computes\" rule or the \"owner stores\" rule with respect to the dominant data array. If all temporal dependence relations across iteration partitions are regular, we use tiling to allow pipelining and the overlapping of computation and communication. However, in order to use tiling on DMPCs, we needed to extend the existing techniques for determining tiling vectors and tile sizes, as they were originally suited for SMPCs only. The overall method is illustrated on programs for the 2D heat equation, for the Gaussian elimination with pivoting, and for the 2D fast Fourier transform on a linear processor array and on a 2D processor grid.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2081344748",
    "type": "article"
  },
  {
    "title": "Generating LR syntax error messages from examples",
    "doi": "https://doi.org/10.1145/937563.937566",
    "publication_date": "2003-09-01",
    "publication_year": 2003,
    "authors": "Clinton Jeffery",
    "corresponding_authors": "Clinton Jeffery",
    "abstract": "LR parser generators are powerful and well-understood, but the parsers they generate are not suited to provide good error messages. Many compilers incur extensive modifications to the source grammar to produce useful syntax error messages. Interpreting the parse state (and input token) at the time of error is a nonintrusive alternative that does not entangle the error recovery mechanism in error message production. Unfortunately, every change to the grammar may significantly alter the mapping from parse states to diagnostic messages, creating a maintenance problem. Merr is a tool that allows a compiler writer to associate diagnostic messages with syntax errors by example, avoiding the need to add error productions to the grammar or interpret integer parse states. From a specification of errors and messages, Merr runs the compiler on each example error to obtain the relevant parse state and input token, and generates a yyerror() function that maps parse states and input tokens to diagnostic messages. Merr enables useful syntax error messages in LR-based compilers in a manner that is robust in the presence of grammar changes.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2154464718",
    "type": "article"
  },
  {
    "title": "A task- and data-parallel programming language based on shared objects",
    "doi": "https://doi.org/10.1145/295656.295658",
    "publication_date": "1998-11-01",
    "publication_year": 1998,
    "authors": "Saniya Ben Hassen; Henri E. Bal; Ceriel J. H. Jacobs",
    "corresponding_authors": "",
    "abstract": "Many programming languages support either task parallelism, but few languages provide a uniform framework for writing applications that need both types of parallelism or data parallelism. We present a programming language and system that integrates task and data parallelism using shared objects. Shared objects may be stored on one processor or may be replicated. Objects may also be partitioned and distributed on several processors.Task parallelism is achieved by forking processes remotely and have them communicate and synchronize through objects. Data parallelism is achieved by executing operations on partitioned objects in parallel. Writing task-and data-parallel applications with shared objects has several advantages. Programmers use the objects as if they were stored in a memory common to all processors. On distributed-memory machines, if objects are remote, replicated, or partitioned, the system takes care of many low-level details such as data transfers and consistency semantics. In this article, we show how to write task-and data-parallel programs with our shared object model. We also desribe a portable implementation of the model. To assess the performance of the system, we wrote several applications that use task and data parallelism and excuted them on a collection of Pentium Pros connected by Myrinet. The performance of these applications is also discussed in this article.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W1979263490",
    "type": "article"
  },
  {
    "title": "Compiling nested data-parallel programs for shared-memory multiprocessors",
    "doi": "https://doi.org/10.1145/169683.174152",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Siddhartha Chatterjee",
    "corresponding_authors": "Siddhartha Chatterjee",
    "abstract": "article Free Access Share on Compiling nested data-parallel programs for shared-memory multiprocessors Author: Siddhartha Chatterjee Carnegie Mellon Univ., Pittsburgh, PA Carnegie Mellon Univ., Pittsburgh, PAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 3July 1993 pp 400–462https://doi.org/10.1145/169683.174152Published:01 July 1993Publication History 27citation568DownloadsMetricsTotal Citations27Total Downloads568Last 12 Months16Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2006208699",
    "type": "article"
  },
  {
    "title": "Extracting task-level parallelism",
    "doi": "https://doi.org/10.1145/210184.210189",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "Milind Girkar; Constantine D. Polychronopoulos",
    "corresponding_authors": "",
    "abstract": "Automatic detection of task-level parallelism (also referred to as functional, DAG, unstructured, or thread parallelism) at various levels of program granularity is becoming increasingly important for parallelizing and back-end compilers. Parallelizing compilers detect iteration-level or coarser granularity parallelism which is suitable for parallel computers; detection of parallelism at the statement-or operation-level is essential for most modern microprocessors, including superscalar and VLIW architectures. In this article we study the problem of detecting, expressing, and optimizing task-level parallelism, where “task” refers to a program statement of arbitrary granularity. Optimizing the amount of functional parallelism (by allowing synchronization between arbitrary nodes) in sequential programs requires the notion of precedence in terms of paths in graphs which incorporate control and data dependences. Precedences have been defined before in a different context; however, the definition was dependent on the ideas of parallel execution and time. We show that the problem of determining precedences statically is NP-complete. Determining precedence relationships is useful in finding the essential data dependences. We show that there exists a unique minimum set of essential data dependences; finding this minimum set is NP-hard and NP-easy. We also propose a heuristic algorithm for finding the set of essential data dependences. Static analysis of a program in the Perfect Benchmarks was done, and we present some experimental results.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2047301304",
    "type": "article"
  },
  {
    "title": "Efficient register allocation via coloring using clique separators",
    "doi": "https://doi.org/10.1145/177492.177499",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Rajiv Gupta; Mary Lou Soffa; Denise Ombres",
    "corresponding_authors": "",
    "abstract": "Although graph coloring is widely recognized as an effective technique for register allocation, memory demands can become quite high for large interference graphs that are needed in coloring. In this paper we present an algorithm that uses the notion of clique separators to improve the space overhead of coloring. The algorithm, based on a result by R. Tarjan regarding the colorability of graphs, partitions program code into code segments using clique separators. The interference graphs for the code partitions are constructed one at a time and colored independently. The colorings for the partitions are combined to obtain a register allocation for the entire program. This approach can be used to perform register allocation in a space-efficient manner. For straight-line code (e.g., local register allocation), an optimal allocation can be obtained from optimal allocations for individual code partitions. Experimental results are presented demonstrating memory demand reductions for interference graphs when allocating registers using clique separators.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2080554063",
    "type": "article"
  },
  {
    "title": "An elimination algorithm for bidirectional data flow problems using edge placement",
    "doi": "https://doi.org/10.1145/169701.169684",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Dhananjay M. Dhamdhere; Harish Patil",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on An elimination algorithm for bidirectional data flow problems using edge placement Authors: D. M. Dhamdhere Indian Institute of Technology Indian Institute of TechnologyView Profile , Harish Patil Indian Institute of Technology Indian Institute of TechnologyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 2pp 312–336https://doi.org/10.1145/169701.169684Published:01 April 1993Publication History 32citation424DownloadsMetricsTotal Citations32Total Downloads424Last 12 Months31Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2022607081",
    "type": "article"
  },
  {
    "title": "Lazy rewriting on eager machinery",
    "doi": "https://doi.org/10.1145/345099.345102",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Wan Fokkink; Jasper F.T Kamperman; Pum Walters",
    "corresponding_authors": "",
    "abstract": "The article introduces a novel notion of lazy rewriting. By annotating argument positions as lazy, redundant rewrite steps are avoided, and the termination behavior of a term-rewriting system can be improved. Some transformations of rewrite rules enable an implementation using the same primitives as an implementation of eager rewriting.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2081230256",
    "type": "article"
  },
  {
    "title": "Program transformation and runtime support for threaded MPI execution on shared-memory machines",
    "doi": "https://doi.org/10.1145/363911.363920",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "Hong Tang; Kai Shen; Tao Yang",
    "corresponding_authors": "",
    "abstract": "Parallel programs written in MPI have been widely used for developing high-performance applications on various platforms. Because of a restriction of the MPI computation model, conventional MPI implementations on shared-memory machines map each MPI node to an OS process, which can suffer serious performance degradation in the presence of multiprogramming. This paper studies compile-time and runtime techniques for enhancing performance portability of MPI code running on multiprogrammed shared-memory machines. The proposed techniques allow MPI nodes to be executed safety and efficiently as threads. Compile-time transformation eliminates global and static variables in C code using node-specific data. The runtime support includes an efficient and provably correct communication protocol that uses lock-free data structure and takes advantage of address space sharing among threads. The experiments on SGI Origin 2000 show that our MPI prototype called TMPI using the proposed techniques is competitive with SGI's native MPI implementation in a dedicated environment, and that it has significant performance advantages in a multiprogrammed environment.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2119266178",
    "type": "article"
  },
  {
    "title": "Fractal symbolic analysis",
    "doi": "https://doi.org/10.1145/945885.945888",
    "publication_date": "2003-11-01",
    "publication_year": 2003,
    "authors": "Vijay Menon; Keshav Pingali; Nikolay Mateev",
    "corresponding_authors": "",
    "abstract": "Modern compilers restructure programs to improve their efficiency. Dependence analysis is the most widely used technique for proving the correctness of such transformations, but it suffers from the limitation that it considers only the memory locations read and written by a statement without considering what is being computed by that statement. Exploiting the semantics of program statements permits more transformations to be proved correct, and is critical for automatic restructuring of codes such as LU with partial pivoting.One approach to exploiting the semantics of program statements is symbolic analysis and comparison of programs.In principle, this technique is very powerful, but in practice, it is intractable for all but the simplest programs.In this paper, we propose a new form of symbolic analysis and comparison of programs which is appropriate for use in restructuring compilers. Fractal symbolic analysis is an approximate symbolic analysis that compares a program and its transformed version by repeatedly simplifying these programs until symbolic analysis becomes tractable while ensuring that equality of the simplified programs is sufficient to guarantee equality of the original programs.Fractal symbolic analysis combines some of the power of symbolic analysis with the tractability of dependence analysis. We discuss a prototype implementation of fractal symbolic analysis, and show how it can be used to solve the long-open problem of verifying the correctness of transformations required to improve the cache performance of LU factorization with partial pivoting.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1963571656",
    "type": "article"
  },
  {
    "title": "Using symbolic execution for verification of Ada tasking programs",
    "doi": "https://doi.org/10.1145/88616.96551",
    "publication_date": "1990-10-01",
    "publication_year": 1990,
    "authors": "Laura K. Dillon",
    "corresponding_authors": "Laura K. Dillon",
    "abstract": "A method is presented for using symbolic execution to generate the verification conditions required for proving correctness of programs written in a tasking subset of Ada. The symbolic execution rules are derived from proof systems that allow tasks to be verified independently in local proofs, which are then checked for cooperation. The isolation nature of this approach to symbolic execution of concurrent programs makes it better suited to formal verification than the more traditional interleaving approach, which suffers from combinatorial problems. The criteria for correct operation of a concurrent program include partial correctness, as well as more general safety properties, such as mutual exclusion and freedom from deadlock.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1983705309",
    "type": "article"
  },
  {
    "title": "Resource aware programming",
    "doi": "https://doi.org/10.1145/1065887.1065891",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Luc Moreau; Christian Queinnec",
    "corresponding_authors": "",
    "abstract": "We introduce the Resource Aware Programming framework, which allows users to monitor the resources used by their programs and to programmatically express policies for the management of such resources. The framework is based on a notion of hierarchical groups, which act as resource containers for the computations they sponsor. Asynchronous notifications for resource exhaustion and for computation termination can be handled by arbitrary user code, which is also executed under the control of this hierarchical group structure. Resources are manipulated by the programmer using resource descriptors, whose operations are specified by a resource algebra. In this article, we overview the Resource Aware Programming framework and describe its semantics in the form of a language-independent abstract machine able to model both shared and distributed memory environments. Finally, we discuss a prototype implementation of the Resource Aware Programming framework in Java.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2075313561",
    "type": "article"
  },
  {
    "title": "A mathematical approach to nondeterminism in data types",
    "doi": "https://doi.org/10.1145/42192.42194",
    "publication_date": "1988-01-01",
    "publication_year": 1988,
    "authors": "Wim H. Hesselink",
    "corresponding_authors": "Wim H. Hesselink",
    "abstract": "The theory of abstract data types is generalized to the case of nondeterministic operations (set-valuedfunctions). Since the nondeterminism of operations may be coupled, signatures are extended so that operations can have results in Cartesian products. Input/output behavior is used to characterize implementation of one model by another. It is described by means of accumulated arrows, which form a generalization of the term algebra. Morphisms of nondeterministic models are introduced. Both innovations prove to be powerful tools in the analysis of input/output behavior. Extraction equivalence and observable equivalence of values are investigated. Quotient models for such equivalence relations are constructed. The equivalence relations are compared with each other, with separation of values by means of experiments, and with the separation property that characterizes a terminal model. Examples are given to show that the four concepts are different. In deterministic models the concepts coincide.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2049498746",
    "type": "article"
  },
  {
    "title": "Abstract interaction tools: a language for user interface management systems",
    "doi": "https://doi.org/10.1145/42190.42191",
    "publication_date": "1988-04-01",
    "publication_year": 1988,
    "authors": "Jan van den Bos",
    "corresponding_authors": "Jan van den Bos",
    "abstract": "A language model is presented for the specification of User Interface Management Systems. The model, called the Abstract Interaction Tool (AIT) model, offers a tree-like hierarchy of interaction objects. Each object represents a subtree and can be considered as an abstract input device containing a syntax-like specification of the required input pattern. The hierarchy of specifications amounts to a system of syntactical productions with multiple control. Terminal nodes of the AIT tree represent the interface to the physical interaction devices. The AIT model features hierarchical output resource management. At the higher, more abstract, level the input-output is loosely coupled. At lower levels the coupling becomes increasingly tight. At the upper levels, AITs model the functions (what) required by the user, whereas at the lower levels the way to accomplish them (how) is stressed. The AIT model has modes for multithread and multiple-device user interaction. There are facilities for context-dependent prompting, echoing, feedback, error correction, and expertise levels. A special section in the AIT provides for links to application modules. As a model for general interactive systems, AITs can be applied to graphics, process control, dialogue, and real-time systems. AITs can also be used to define controlled production rules in knowledge-based systems. In addition the model can provide tools for the software engineering phases specification and prototyping.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2082207996",
    "type": "article"
  },
  {
    "title": "Algorithmic specifications: a constructive specification method for abstract data types",
    "doi": "https://doi.org/10.1145/29873.30399",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Jacques Loeckx",
    "corresponding_authors": "Jacques Loeckx",
    "abstract": "This paper presents a new specification method for abstract data types and a pertaining logic. The specification method proposed differs from the classical algebraic one by its constructive, yet abstract nature. Although it leads to a different style in specification, the method avoids some fundamental problems inherent in the algebraic specification method. The logic proposed is essentially a first-order logic for strict (partial) functions. It allows in particular the expression of the semantic conditions guaranteeing the consistency of a specification.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W1983043931",
    "type": "article"
  },
  {
    "title": "Real-Time Synchronization of Interprocess Communications",
    "doi": "https://doi.org/10.1145/2993.357244",
    "publication_date": "1984-04-01",
    "publication_year": 1984,
    "authors": "John H. Reif; Paul G. Spirakis",
    "corresponding_authors": "",
    "abstract": "article Free AccessReal-Time Synchronization of Interprocess Communications Authors: John H. Reif Aiken Computation Laboratory, Harvard University, Cambridge, MA Aiken Computation Laboratory, Harvard University, Cambridge, MAView Profile , Paul G. Spirakis Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NY Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer Street, New York, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 2April 1984 pp 215–238https://doi.org/10.1145/2993.357244Published:01 April 1984Publication History 24citation553DownloadsMetricsTotal Citations24Total Downloads553Last 12 Months22Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1965790513",
    "type": "article"
  },
  {
    "title": "Side effects and aliasing can have simple axiomatic descriptions",
    "doi": "https://doi.org/10.1145/4472.4474",
    "publication_date": "1985-10-01",
    "publication_year": 1985,
    "authors": "Hans‐Juergen Boehm",
    "corresponding_authors": "Hans‐Juergen Boehm",
    "abstract": "We present a different style of axiomatic definition for programming languages. It is oriented toward imperative languages, such as Algol 68, that do not distinguish between statements and expressions. Rather than basing the logic on a notion of pre- or postcondition, we use the value of a programming language expression as the underlying primitive. A number of language constructs are examined in this framework. We argue that this style of definition gives us a significantly different view of the notion of “easy axiomatixability.” Side effects in expressions as well as aliasing between variables are shown to be “easily axiomatizable” in our system.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2032396207",
    "type": "article"
  },
  {
    "title": "Some comments on “A solution to a problem with Morel and Renvoise's 'Global optimization by suppression of partial redundancies'”",
    "doi": "https://doi.org/10.1145/69558.214513",
    "publication_date": "1989-10-01",
    "publication_year": 1989,
    "authors": "A. Sorkin",
    "corresponding_authors": "A. Sorkin",
    "abstract": "Abstract: Drechsler and Stadel presented a solution to a problem with Morel and Renvoise's “Global Optimization by Suppression of Partial Redundancies.” We cite some earlier generalizations of Morel and Renvoise's algorithm that solve the same problem, and we comment on their applicability.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2042468601",
    "type": "article"
  },
  {
    "title": "Controlling garbage collection and heap growth to reduce the execution time of Java applications",
    "doi": "https://doi.org/10.1145/1152649.1152652",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Tim Brecht; Eshrat Arjomandi; Chang Li; Hang Pham",
    "corresponding_authors": "",
    "abstract": "In systems that support garbage collection, a tension exists between collecting garbage too frequently and not collecting it frequently enough. Garbage collection that occurs too frequently may introduce unnecessary overheads at the risk of not collecting much garbage during each cycle. On the other hand, collecting garbage too infrequently can result in applications that execute with a large amount of virtual memory (i.e., with a large footprint) and suffer from increased execution times due to paging.In this article, we use a large set of Java applications and the highly tuned and widely used Boehm-Demers-Weiser (BDW) conservative mark-and-sweep garbage collector to experimentally examine the extent to which the frequency of garbage collection impacts an application's execution time, footprint, and pause times. We use these results to devise some guidelines for controlling garbage collection and heap growth in a conservative garbage collector in order to minimize application execution times. Then we describe new strategies for controlling garbage collection and heap growth that impact not only the frequency with which garbage collection occurs but also the points at which it occurs. Experimental results demonstrate that when compared with the existing approach used in the standard BDW collector, our new strategy can significantly reduce application execution times.Our goal is to obtain a better understanding of how to control garbage collection and heap growth for an individual application executing in isolation. These results can be applied in a number of high-performance computing and server environments, in addition to some single-user environments. This work should also provide insights into how to make better decisions that impact garbage collection in multiprogrammed environments.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2048907983",
    "type": "article"
  },
  {
    "title": "Systems semantics: principles, applications, and implementation",
    "doi": "https://doi.org/10.1145/42192.45067",
    "publication_date": "1988-01-01",
    "publication_year": 1988,
    "authors": "R. Boute",
    "corresponding_authors": "R. Boute",
    "abstract": "Systems semantics extends the denotational semantics of programming languages to a semantics for the description of arbitrary systems, including objects that are not computations in any sense. By defining different meaning functions, the same formal description may be used to denote different system properties, such as structure, behavior, component cost, and performance aspects (e.g., timing). The definition of these semantic functions also provides guidance in language design, in particular for the match between language constructs and the system concepts to be expressed. Aiming at compositionality ensures useful properties for formal manipulation. In this fashion, the meaning functions can be made sufficiently simple to serve not only as a direct implementation on a machine but also as rules for reasoning about systems in a transformational manner. As the applications show, however, compositionality can be ensured only through careful consideration of the characteristics of the flow of information inside the system. Two classes of application are discussed: Unidirectional systems, in particular digital systems without feedback (combinational) and with feedback (sequential), and a certain class of analog systems. Nonunidirectional systems, in particular two-port analog networks. The emphasis will be on the functional style of description and on formal reasoning (theorem proving, derivation of properties). Implementation and rapid prototyping strategies in various system description environments are also briefly discussed. These would permit the concepts of system semantics to be explored without the need for a complete implementation.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2053312945",
    "type": "article"
  },
  {
    "title": "A Formal Framework for the Derivation of Machine-Specific Optimizers",
    "doi": "https://doi.org/10.1145/2166.357219",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Robert Giegerich",
    "corresponding_authors": "Robert Giegerich",
    "abstract": "article Free Access Share on A Formal Framework for the Derivation of Machine-Specific Optimizers Author: Robert Giegerich Institut für Informatik, Technische Universität München, Postfach 202420, D-8000 München 2, W. Germany Institut für Informatik, Technische Universität München, Postfach 202420, D-8000 München 2, W. GermanyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3pp 478–498https://doi.org/10.1145/2166.357219Published:01 July 1983Publication History 22citation293DownloadsMetricsTotal Citations22Total Downloads293Last 12 Months14Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2086643496",
    "type": "article"
  },
  {
    "title": "Selective and locally controlled transport of privileges",
    "doi": "https://doi.org/10.1145/1780.1786",
    "publication_date": "1984-10-01",
    "publication_year": 1984,
    "authors": "Naftaly H. Minsky",
    "corresponding_authors": "Naftaly H. Minsky",
    "abstract": "article Free Access Share on Selective and locally controlled transport of privileges Author: Naftaly H. Minsky Rutgers University Rutgers UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 4Oct. 1984 pp 573–602https://doi.org/10.1145/1780.1786Online:01 October 1984Publication History 21citation303DownloadsMetricsTotal Citations21Total Downloads303Last 12 Months7Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2091769087",
    "type": "article"
  },
  {
    "title": "Optimizing aggregate array computations in loops",
    "doi": "https://doi.org/10.1145/1053468.1053471",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Yanhong A. Liu; Scott D. Stoller; Ning Li; Tom Rothamel",
    "corresponding_authors": "",
    "abstract": "An aggregate array computation is a loop that computes accumulated quantities over array elements. Such computations are common in programs that use arrays, and the array elements involved in such computations often overlap, especially across iterations of loops, resulting in significant redundancy in the overall computations. This article presents a method and algorithms that eliminate such overlapping aggregate array redundancies and shows analytical and experimental performance improvements. The method is based on incrementalization, that is, updating the values of aggregate array computations from iteration to iteration rather than computing them from scratch in each iteration. This involves maintaining additional values not maintained in the original program. We reduce various analysis problems to solving inequality constraints on loop variables and array subscripts, and we apply results from work on array data dependence analysis. For aggregate array computations that have significant redundancy, incrementalization produces drastic speedup compared to previous optimizations; when there is little redundancy, the benefit might be offset by cache effects and other factors. Previous methods for loop optimizations of arrays do not perform incrementalization, and previous techniques for loop incrementalization do not handle arrays.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2127256702",
    "type": "article"
  },
  {
    "title": "Proving systolic systems correct",
    "doi": "https://doi.org/10.1145/5956.5999",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Matthew Hennessy",
    "corresponding_authors": "Matthew Hennessy",
    "abstract": "A language for describing communicating systems is described. It is sufficiently expressive to describe both the desired behavior of systems, their specifications , and their actual implementations in terms of simpler components. We say I satisfies S if 1 is a correct implementation of the specification S . We briefly discuss a semantic treatment of this notion of satisfaction, but the main emphasis of the paper is on a proof technique for proving statements of the form I satisfies S , based on syntactic transformations and induction. Two examples are given, both systolic systems from the literature.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2011523523",
    "type": "article"
  },
  {
    "title": "Efficient and effective array bound checking",
    "doi": "https://doi.org/10.1145/1065887.1065893",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Nga Nguyen; François Irigoin",
    "corresponding_authors": "",
    "abstract": "Array bound checking refers to determining whether all array references in a program are within their declared ranges. This checking is critical for software verification and validation because subscripting arrays beyond their declared sizes may produce unexpected results, security holes, or failures. It is available in most commercial compilers but current implementations are not as efficient and effective as one may have hoped: (1) the execution times of array bound checked programs are increased by a factor of up to 5, (2) the compilation times are increased, which is detrimental to development and debugging, (3) the related error messages do not usually carry information to locate the faulty references, and (4) the consistency between actual array sizes and formal array declarations is not often checked.This article presents two optimization techniques that deal with Points 1, 2, and 3, and a new algorithm to tackle Point 4, which is not addressed by the current literature. The first optimization technique is based on the elimination of redundant tests, to provide very accurate information about faulty references during development and testing phases. The second one is based on the insertion of unavoidable tests to provide the smallest possible slowdown during the production phase. The new algorithm ensures the absence of bound violations in every array access in the called procedure with respect to the array declarations in the calling procedure. Our experiments suggest that the optimization of array bound checking depends on several factors, not only the percentage of removed checks, usually considered as the best improvement measuring metrics. The debugging capability and compile-time and run-time performances of our techniques are better than current implementations. The execution times of SPEC95 CFP benchmarks with range checking added by PIPS, our Fortran research compiler, are slightly longer, less than 20%, than that of unchecked programs. More problems due to functional and data recursion would have to be solved to extend these results from Fortran to other languages such as C, C++, or Java, but the issues addressed in this article are nevertheless relevant.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2027055567",
    "type": "article"
  },
  {
    "title": "A new analysis of LALR formalisms",
    "doi": "https://doi.org/10.1145/2363.2527",
    "publication_date": "1985-01-02",
    "publication_year": 1985,
    "authors": "Joseph C.H. Park; K. CHOE; C. H. Chang",
    "corresponding_authors": "",
    "abstract": "The traditional LALR analysis is reexamined using a new operator and an associated graph. An improved method that allows factoring out a crucial part of the computation for defining states of LR(0) canonical collection and for computing LALR(1) lookahead sets is presented. This factorization leads to significantly improved algorithms with respect to execution time as well as storage requirements. Experimental results including comparison with other known methods are presented.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2049727710",
    "type": "article"
  },
  {
    "title": "Deleting Irrelevant Tasks in an Expression-Oriented Multiprocessor System",
    "doi": "https://doi.org/10.1145/357121.357125",
    "publication_date": "1981-01-01",
    "publication_year": 1981,
    "authors": "Dale H. Grit; Rex Page",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Deleting Irrelevant Tasks in an Expression-Oriented Multiprocessor System Authors: Dale H. Grit Computer Science Department, Colorado State University, Fort Collins, CO Computer Science Department, Colorado State University, Fort Collins, COView Profile , Rex L. Page Computer Science Department, Colorado State University, Fort Collins, CO Computer Science Department, Colorado State University, Fort Collins, COView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 1Jan. 1981 pp 49–59https://doi.org/10.1145/357121.357125Online:01 January 1981Publication History 17citation211DownloadsMetricsTotal Citations17Total Downloads211Last 12 Months6Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2071077931",
    "type": "article"
  },
  {
    "title": "Type inference for unique pattern matching",
    "doi": "https://doi.org/10.1145/1133651.1133652",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Stijn Vansummeren",
    "corresponding_authors": "Stijn Vansummeren",
    "abstract": "Regular expression patterns provide a natural, declarative way to express constraints on semistructured data and to extract relevant information from it. Indeed, it is a core feature of the programming language Perl, surfaces in various UNIX tools such as sed and awk, and has recently been proposed in the context of the XML programming language XDuce. Since regular expressions can be ambiguous in general, different disambiguation policies have been proposed to get a unique matching strategy. We formally define the matching semantics under both (1) the POSIX, and (2) the first and longest match disambiguation strategies. We show that the generally accepted method of defining the longest match in terms of the first match and recursion does not conform to the natural notion of longest match. We continue by solving the type inference problem for both disambiguation strategies, which consists of calculating the set of all subparts of input values a subexpression can match under the given policy.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2080480757",
    "type": "article"
  },
  {
    "title": "The Activity of a Variable and Its Relation to Decision Trees",
    "doi": "https://doi.org/10.1145/357114.357120",
    "publication_date": "1980-10-01",
    "publication_year": 1980,
    "authors": "B. E. Moret; Michael G. Thomason; Rafael C. González",
    "corresponding_authors": "",
    "abstract": "The construction of sequential testing procedures from functions of discrete arguments is a common problem in switching theory, software engineering, pattern recognition, and management. The concept of the activity of an argument is introduced, and a theorem is proved which relates it to the expected testing cost of the most general type of decision trees. This result is then extended to trees constructed from relations on finite sets and to decision procedures with cycles. These results are used, in turn, as the basis for a fast heuristic selection rule for constructing testing procedures. Finally, some bounds on the performance of the selection rule are developed.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1966978162",
    "type": "article"
  },
  {
    "title": "METRIC",
    "doi": "https://doi.org/10.1145/1216374.1216380",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Jaydeep Marathe; Frank Mueller; Tushar Mohan; Sally A. McKee; Bronis R. de Supinski; Andy Yoo",
    "corresponding_authors": "",
    "abstract": "With the diverging improvements in CPU speeds and memory access latencies, detecting and removing memory access bottlenecks becomes increasingly important. In this work we present METRIC, a software framework for isolating and understanding such bottlenecks using partial access traces. METRIC extracts access traces from executing programs without special compiler or linker support. We make four primary contributions. First, we present a framework for extracting partial access traces based on dynamic binary rewriting of the executing application. Second, we introduce a novel algorithm for compressing these traces. The algorithm generates constant space representations for regular accesses occurring in nested loop structures. Third, we use these traces for offline incremental memory hierarchy simulation. We extract symbolic information from the application executable and use this to generate detailed source-code correlated statistics including per-reference metrics, cache evictor information, and stream metrics. Finally, we demonstrate how this information can be used to isolate and understand memory access inefficiencies. This illustrates a potential advantage of METRIC over compile-time analysis for sample codes, particularly when interprocedural analysis is required.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1988502524",
    "type": "article"
  },
  {
    "title": "Perfect hashing as an almost perfect subtype test",
    "doi": "https://doi.org/10.1145/1391956.1391960",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Roland Ducournau",
    "corresponding_authors": "Roland Ducournau",
    "abstract": "Subtype tests are an important issue in the implementation of object-oriented programming languages. Many techniques have been proposed, but none of them perfectly fulfills the five requirements that we have identified: constant-time, linear-space, multiple inheritance, dynamic loading and inlining. In this article, we propose a subtyping test implementation that involves a combination of usual hashtables and Cohen's display, which is a well-known technique for single inheritance hierarchies. This novel approach is based on perfect hashing , that is, an optimized and truly constant-time variant of hashing that applies to immutable hashtables. We show that the resulting technique closely meets all five requirements. Furthermore, in the framework of Java-like languages—characterized by single inheritance of classes and multiple subtyping of interfaces—perfect hashing also applies to method invocation when the receiver is typed by an interface. The proposed technique is compared to some alternatives, including the proposal by Palacz and Vitek [2003]. Time-efficiency is assessed at the cycle level in the framework of Driesen's pseudo-code and the linear-space criterion is validated by statistical simulation on benchmarks consisting of large-scale class hierarchies.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2060789440",
    "type": "article"
  },
  {
    "title": "Sequent calculi and abstract machines",
    "doi": "https://doi.org/10.1145/1516507.1516508",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Zena M. Ariola; Aaron Bohannon; Amr Sabry",
    "corresponding_authors": "",
    "abstract": "We propose a sequent calculus derived from the λ―μμ˜-calculus of Curien and Herbelin that is expressive enough to directly represent the fine details of program evaluation using typical abstract machines. Not only does the calculus easily encode the usual components of abstract machines such as environments and stacks, but it can also simulate the transition steps of the abstract machine with just a constant overhead. Technically this is achieved by ensuring that reduction in the calculus always happens at a bounded depth from the root of the term. We illustrate these properties by providing shallow encodings of the Krivine (call-by-name) and the CEK (call-by-value) abstract machines in the calculus.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2077444071",
    "type": "article"
  },
  {
    "title": "Program transformations using temporal logic side conditions",
    "doi": "https://doi.org/10.1145/1516507.1516509",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Sara Kalvala; Richard Warburton; David Lacey",
    "corresponding_authors": "",
    "abstract": "This article describes an approach to program optimization based on transformations, where temporal logic is used to specify side conditions, and strategies are created which expand the repertoire of transformations and provide a suitable level of abstraction. We demonstrate the power of this approach by developing a set of optimizations using our transformation language and showing how the transformations can be converted into a form which makes it easier to apply them, while maintaining trust in the resulting optimizing steps. The approach is illustrated through a transformational case study where we apply several optimizations to a small program.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2029383818",
    "type": "article"
  },
  {
    "title": "Nomadic pict",
    "doi": "https://doi.org/10.1145/1734206.1734209",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Peter Sewell; Paweł T. Wojciechowski; Asis Unyapoth",
    "corresponding_authors": "",
    "abstract": "Mobile computation, in which executing computations can move from one physical computing device to another, is a recurring theme: from OS process migration, to language-level mobility, to virtual machine migration. This article reports on the design, implementation, and verification of overlay networks to support reliable communication between migrating computations, in the Nomadic Pict project. We define two levels of abstraction as calculi with precise semantics: a low-level Nomadic π calculus with migration and location-dependent communication, and a high-level calculus that adds location-independent communication. Implementations of location-independent communication, as overlay networks that track migrations and forward messages, can be expressed as translations of the high-level calculus into the low. We discuss the design space of such overlay network algorithms and define three precisely, as such translations. Based on the calculi, we design and implement the Nomadic Pict distributed programming language, to let such algorithms (and simple applications above them) to be quickly prototyped. We go on to develop the semantic theory of the Nomadic π calculi, proving correctness of one example overlay network. This requires novel equivalences and congruence results that take migration into account, and reasoning principles for agents that are temporarily immobile (e.g., waiting on a lock elsewhere in the system). The whole stands as a demonstration of the use of principled semantics to address challenging system design problems.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2012223141",
    "type": "article"
  },
  {
    "title": "Certificate translation for optimizing compilers",
    "doi": "https://doi.org/10.1145/1538917.1538919",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Gilles Barthe; Benjamin Grégoire; César Kunz; Tamara Rezk",
    "corresponding_authors": "",
    "abstract": "Proof Carrying Code provides trust in mobile code by requiring certificates that ensure the code adherence to specific conditions. The prominent approach to generate certificates for compiled code is Certifying Compilation, that automatically generates certificates for simple safety properties. In this work, we present Certificate Translation, a novel extension for standard compilers that automatically transforms formal proofs for more expressive and complex properties of the source program to certificates for the compiled code. The article outlines the principles of certificate translation, instantiated for a nonoptimizing compiler and for standard compiler optimizations in the context of an intermediate RTL Language.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2084416080",
    "type": "article"
  },
  {
    "title": "Morphing",
    "doi": "https://doi.org/10.1145/1890028.1890029",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Shan Huang; Yannis Smaragdakis",
    "corresponding_authors": "",
    "abstract": "We present MorphJ: a language for specifying general classes whose members are produced by iterating over members of other classes. We call this technique “class morphing” or just “morphing.” Morphing extends the notion of genericity so that not only types of methods and fields, but also the structure of a class can vary according to type variables. This adds a disciplined form of metaprogramming to mainstream languages and allows expressing common programming patterns in a highly generic way that is otherwise not supported by conventional techniques. For instance, morphing lets us write generic proxies (i.e., classes that can be parameterized with another class and export the same public methods as that class); default implementations (e.g., a generic do-nothing type, configurable for any interface); semantic extensions (e.g., specialized behavior for methods that declare a certain annotation); and more. MorphJ's hallmark feature is that, despite its emphasis on generality, it allows modular type-checking: a MorphJ class can be checked independently of its uses. Thus, the possibility of supplying a type parameter that will lead to invalid code is detected early, an invaluable feature for highly general components that will be statically instantiated by other programmers. We demonstrate the benefits of morphing with several examples, including a MorphJ reimplementation of DSTM2, a software transactional memory library which reduces 1,484 lines of Java reflection and bytecode engineering library calls to just 586 lines of MorphJ code.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2072956890",
    "type": "article"
  },
  {
    "title": "Natural and Flexible Error Recovery for Generated Modular Language Environments",
    "doi": "https://doi.org/10.1145/2400676.2400678",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "M. de Jonge; Lennart C.L. Kats; Eelco Visser; Emma Söderberg",
    "corresponding_authors": "",
    "abstract": "Integrated Development Environments (IDEs) increase programmer productivity, providing rapid, interactive feedback based on the syntax and semantics of a language. Unlike conventional parsing algorithms, scannerless generalized-LR parsing supports the full set of context-free grammars, which is closed under composition, and hence can parse languages composed from separate grammar modules. To apply this algorithm in an interactive environment, this article introduces a novel error recovery mechanism. Our approach is language independent, and relies on automatic derivation of recovery rules from grammars. By taking layout information into consideration it can efficiently suggest natural recovery suggestions.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2113112210",
    "type": "article"
  },
  {
    "title": "Mixin’ Up the ML Module System",
    "doi": "https://doi.org/10.1145/2450136.2450137",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Andreas Rossberg; Derek Dreyer",
    "corresponding_authors": "",
    "abstract": "ML modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. Mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. We synthesize the complementary advantages of these two mechanisms in a novel module system design we call MixML. A MixML module is like an ML structure in which some of the components are specified but not defined. In other words, it unifies the ML structure and signature languages into one. MixML seamlessly integrates hierarchical composition, translucent ML-style data abstraction, and mixin-style recursive linking. Moreover, the design of MixML is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ML module system (and several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role. We provide a declarative type system for MixML, including two important extensions: higher-order modules, and modules as first-class values. We also present a sound and complete, three-pass type-checking algorithm for this system. The operational semantics of MixML is defined by an elaboration translation into an internal core language called LTG---namely, a polymorphic lambda calculus with single-assignment references and recursive type generativity---which employs a linear type and kind system to track definedness of term and type imports.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1989760056",
    "type": "article"
  },
  {
    "title": "Partially Evaluating Finite-State Runtime Monitors Ahead of Time",
    "doi": "https://doi.org/10.1145/2220365.2220366",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Eric Bodden; Patrick Lam; Laurie Hendren",
    "corresponding_authors": "",
    "abstract": "Finite-state properties account for an important class of program properties, typically related to the order of operations invoked on objects. Many library implementations therefore include manually written finite-state monitors to detect violations of finite-state properties at runtime. Researchers have recently proposed the explicit specification of finite-state properties and automatic generation of monitors from the specification. However, runtime monitoring only shows the presence of violations, and typically cannot prove their absence. Moreover, inserting a runtime monitor into a program under test can slow down the program by several orders of magnitude. In this work, we therefore present a set of four static whole-program analyses that partially evaluate runtime monitors at compile time, with increasing cost and precision. As we show, ahead-of-time evaluation can often evaluate the monitor completely statically. This may prove that the program cannot violate the property on any execution or may prove that violations do exist. In the remaining cases, the partial evaluation converts the runtime monitor into a residual monitor. This monitor only receives events from program locations that the analyses failed to prove irrelevant. This makes the residual monitor much more efficient than a full monitor, while still capturing all property violations at runtime. We implemented the analyses in Clara, a novel framework for the partial evaluation of AspectJ-based runtime monitors, and validated our approach by applying Clara to finite-state properties over several large-scale Java programs. Clara proved that most of the programs never violate our example properties. Some programs required monitoring, but in those cases Clara could often reduce the monitoring overhead to below 10%. We observed that several programs did violate the stated properties.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2170181173",
    "type": "article"
  },
  {
    "title": "Pattern-Based Verification for Multithreaded Programs",
    "doi": "https://doi.org/10.1145/2629644",
    "publication_date": "2014-09-15",
    "publication_year": 2014,
    "authors": "Javier Esparza; Pierre Ganty; Tomáš Poch",
    "corresponding_authors": "",
    "abstract": "Pattern-based verification checks the correctness of program executions that follow a given pattern , a regular expression over the alphabet of program transitions of the form w 1 * … w n * . For multithreaded programs, the alphabet of the pattern is given by the reads and writes to the shared storage. We study the complexity of pattern-based verification for multithreaded programs with shared counters and finite variables. While unrestricted verification is undecidable for abstracted multithreaded programs with recursive procedures and PSPACE-complete for abstracted multithreaded while-programs (even without counters), we show that pattern-based verification is NP-complete for both classes, even in the presence of counters. We then conduct a multiparameter analysis to study the complexity of the problem on its three natural parameters (number of threads+counters+variables, maximal size of a thread, size of the pattern) and on two parameters related to thread structure (maximal number of procedures per thread and longest simple path of procedure calls). We present an algorithm that for a fixed number of threads, counters, variables, and pattern size solves the verification problem in st O ( lsp + ⌈ log ( pr +1) ⌉) time, where st is the maximal size of a thread, pr is the maximal number of procedures per thread, and lsp is the longest simple path of procedure calls.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1986359097",
    "type": "article"
  },
  {
    "title": "Practical Fine-Grained Information Flow Control Using Laminar",
    "doi": "https://doi.org/10.1145/2638548",
    "publication_date": "2014-11-17",
    "publication_year": 2014,
    "authors": "Donald E. Porter; Michael D. Bond; Indrajit Roy; Kathryn S. McKinley; Emmett Witchel",
    "corresponding_authors": "",
    "abstract": "Decentralized Information Flow Control (DIFC) is a promising model for writing programs with powerful, end-to-end security guarantees. Current DIFC systems that run on commodity hardware can be broadly categorized into two types: language-level and operating system-level DIFC. Language solutions provide no guarantees against security violations on system resources such as files and sockets. Operating system solutions mediate accesses to system resources but are either inefficient or imprecise at monitoring the flow of information through fine-grained program data structures. This article describes Laminar, the first system to implement DIFC using a unified set of abstractions for OS resources and heap-allocated objects. Programmers express security policies by labeling data with secrecy and integrity labels and access the labeled data in security methods . Laminar enforces the security policies specified by the labels at runtime. Laminar is implemented using a modified Java virtual machine and a new Linux security module. This article shows that security methods ease incremental deployment and limit dynamic security checks by retrofitting DIFC policies on four application case studies. Replacing the applications' ad hoc security policies changes less than 10% of the code and incurs performance overheads from 5% to 56%. Compared to prior DIFC systems, Laminar supports a more general class of multithreaded DIFC programs efficiently and integrates language and OS abstractions.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2168245916",
    "type": "article"
  },
  {
    "title": "Algorithms for Algebraic Path Properties in Concurrent Systems of Constant Treewidth Components",
    "doi": "https://doi.org/10.1145/3210257",
    "publication_date": "2018-07-05",
    "publication_year": 2018,
    "authors": "Krishnendu Chatterjee; Rasmus Ibsen-Jensen; Amir Kafshdar Goharshady; Andreas Pavlogiannis",
    "corresponding_authors": "",
    "abstract": "We study algorithmic questions wrt algebraic path properties in concurrent systems, where the transitions of the system are labeled from a complete, closed semiring. The algebraic path properties can model dataflow analysis problems, the shortest path problem, and many other natural problems that arise in program analysis. We consider that each component of the concurrent system is a graph with constant treewidth, a property satisfied by the controlflow graphs of most programs. We allow for multiple possible queries, which arise naturally in demand driven dataflow analysis. The study of multiple queries allows us to consider the tradeoff between the resource usage of the one-time preprocessing and for each individual query. The traditional approach constructs the product graph of all components and applies the best-known graph algorithm on the product. In this approach, even the answer to a single query requires the transitive closure (i.e., the results of all possible queries), which provides no room for tradeoff between preprocessing and query time. Our main contributions are algorithms that significantly improve the worst-case running time of the traditional approach, and provide various tradeoffs depending on the number of queries. For example, in a concurrent system of two components, the traditional approach requires hexic time in the worst case for answering one query as well as computing the transitive closure, whereas we show that with one-time preprocessing in almost cubic time, each subsequent query can be answered in at most linear time, and even the transitive closure can be computed in almost quartic time. Furthermore, we establish conditional optimality results showing that the worst-case running time of our algorithms cannot be improved without achieving major breakthroughs in graph algorithms (i.e., improving the worst-case bound for the shortest path problem in general graphs). Preliminary experimental results show that our algorithms perform favorably on several benchmarks.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2880875791",
    "type": "article"
  },
  {
    "title": "A Theoretical Foundation of Sensitivity in an Abstract Interpretation Framework",
    "doi": "https://doi.org/10.1145/3230624",
    "publication_date": "2018-08-09",
    "publication_year": 2018,
    "authors": "Sewon Kim; Xavier Rival; Sukyoung Ryu",
    "corresponding_authors": "",
    "abstract": "Program analyses often utilize various forms of sensitivity such as context sensitivity, call-site sensitivity, and object sensitivity. These techniques all allow for more precise program analyses, that are able to compute more precise program invariants, and to verify stronger properties. Despite the fact that sensitivity techniques are now part of the standard toolkit of static analyses designers and implementers, no comprehensive frameworks allow the description of all common forms of sensitivity. As a consequence, the soundness proofs of static analysis tools involving sensitivity often rely on ad hoc formalization, which are not always carried out in an abstract interpretation framework. Moreover, this also means that opportunities to identify similarities between analysis techniques to better improve abstractions or to tune static analysis tools can easily be missed. In this article, we present and formalize a framework for the description of sensitivity in static analysis . Our framework is based on a powerful abstract domain construction, and utilizes reduced cardinal power to tie basic abstract predicates to the properties analyses are sensitive to. We formalize this abstraction, and the main abstract operations that are needed to turn it into a generic abstract domain construction. We demonstrate that our approach can allow for a more precise description of program states, and that it can also describe a large set of sensitivity techniques, both when sensitivity criteria are static (known before the analysis) or dynamic (inferred as part of the analysis), and sensitive analysis tuning parameters. Last, we show that sensitivity techniques used in state-of-the-art static analysis tools can be described in our framework.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2886900155",
    "type": "article"
  },
  {
    "title": "Fast",
    "doi": "https://doi.org/10.1145/2791292",
    "publication_date": "2015-10-16",
    "publication_year": 2015,
    "authors": "Loris D’Antoni; Margus Veanes; Benjamin Livshits; Dávid Molnár",
    "corresponding_authors": "",
    "abstract": "Tree automata and transducers are used in a wide range of applications in software engineering. While these formalisms are of immense practical use, they can only model finite alphabets. To overcome this problem we augment tree automata and transducers with symbolic alphabets represented as parametric theories. Admitting infinite alphabets makes these models more general and succinct than their classic counterparts. Despite this, we show how the main operations, such as composition and language equivalence, remain computable given a decision procedure for the alphabet theory. We introduce a high-level language called F ast that acts as a front-end for the preceding formalisms.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2000356331",
    "type": "article"
  },
  {
    "title": "Selective X-Sensitive Analysis Guided by Impact Pre-Analysis",
    "doi": "https://doi.org/10.1145/2821504",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Hakjoo Oh; Wonchan Lee; Kihong Heo; Hongseok Yang; Kwangkeun Yi",
    "corresponding_authors": "",
    "abstract": "We present a method for selectively applying context-sensitivity during interprocedural program analysis. Our method applies context-sensitivity only when and where doing so is likely to improve the precision that matters for resolving given queries. The idea is to use a pre-analysis to estimate the impact of context-sensitivity on the main analysis’s precision, and to use this information to find out when and where the main analysis should turn on or off its context-sensitivity. We formalize this approach and prove that the analysis always benefits from the pre-analysis--guided context-sensitivity. We implemented this selective method for an existing industrial-strength interval analyzer for full C. The method reduced the number of (false) alarms by 24.4% while increasing the analysis cost by 27.8% on average. The use of the selective method is not limited to context-sensitivity. We demonstrate this generality by following the same principle and developing a selective relational analysis and a selective flow-sensitive analysis. Our experiments show that the method cost-effectively improves the precision in the these analyses as well.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2204346447",
    "type": "article"
  },
  {
    "title": "Verifying Custom Synchronization Constructs Using Higher-Order Separation Logic",
    "doi": "https://doi.org/10.1145/2818638",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Mike Dodds; Suresh Jagannathan; Matthew Parkinson; Kasper Svendsen; Lars Birkedal",
    "corresponding_authors": "",
    "abstract": "Synchronization constructs lie at the heart of any reliable concurrent program. Many such constructs are standard (e.g., locks, queues, stacks, and hash-tables). However, many concurrent applications require custom synchronization constructs with special-purpose behavior. These constructs present a significant challenge for verification. Like standard constructs, they rely on subtle racy behavior, but unlike standard constructs, they may not have well-understood abstract interfaces. As they are custom built, such constructs are also far more likely to be unreliable. This article examines the formal specification and verification of custom synchronization constructs. Our target is a library of channels used in automated parallelization to enforce sequential behavior between program statements. Our high-level specification captures the conditions necessary for correct execution; these conditions reflect program dependencies necessary to ensure sequential behavior. We connect the high-level specification with the low-level library implementation to prove that a client's requirements are satisfied. Significantly, we can reason about program and library correctness without breaking abstraction boundaries. To achieve this, we use a program logic called iCAP (impredicative Concurrent Abstract Predicates) based on separation logic. iCAP supports both high-level abstraction and low-level reasoning about races. We use this to show that our high-level channel specification abstracts three different, increasingly complex lowlevel implementations of the library. iCAP's support for higher-order reasoning lets us prove that sequential dependencies are respected, while iCAP's next-generation semantic model lets us avoid ugly problems with cyclic dependencies.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2231621909",
    "type": "article"
  },
  {
    "title": "Polymorphic Manifest Contracts, Revised and Resolved",
    "doi": "https://doi.org/10.1145/2994594",
    "publication_date": "2017-02-06",
    "publication_year": 2017,
    "authors": "Taro Sekiyama; Atsushi Igarashi; Michael Greenberg",
    "corresponding_authors": "",
    "abstract": "Manifest contracts track precise program properties by refining types with predicates—for example, { x :Int∣ x &gt; 0} denotes the positive integers. Contracts and polymorphism make a natural combination: programmers can give strong contracts to abstract types, precisely stating pre- and post conditions while hiding implementation details— for instance, an abstract type of stacks might specify that the pop operation has input type { x :α Stack ∣ not (empty x )}. This article studies a polymorphic calculus with manifest contracts and establishes fundamental properties including type soundness and relational parametricity. Indeed, this is not the first work on polymorphic manifest contracts, but existing calculi are not very satisfactory. Gronski et al. developed the S age language, which introduces polymorphism through the Type:Type discipline, but they do not study parametricity. Some authors of this article have produced two separate works: Belo et al. [2011] and Greenberg [2013] studied polymorphic manifest contracts and parametricity, but their calculi have metatheoretical problems in the type conversion relations. Indeed, they depend on a few conjectures, which turn out to be false. Our calculus is the first polymorphic manifest calculus with parametricity, depending on no conjectures—it resolves the issues in prior calculi with delayed substitution on casts.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2586829215",
    "type": "article"
  },
  {
    "title": "Faster Algorithms for Dynamic Algebraic Queries in Basic RSMs with Constant Treewidth",
    "doi": "https://doi.org/10.1145/3363525",
    "publication_date": "2019-11-13",
    "publication_year": 2019,
    "authors": "Krishnendu Chatterjee; Amir Kafshdar Goharshady; Prateesh Goyal; Rasmus Ibsen-Jensen; Andreas Pavlogiannis",
    "corresponding_authors": "",
    "abstract": "Interprocedural analysis is at the heart of numerous applications in programming languages, such as alias analysis, constant propagation, and so on. Recursive state machines (RSMs) are standard models for interprocedural analysis. We consider a general framework with RSMs where the transitions are labeled from a semiring and path properties are algebraic with semiring operations. RSMs with algebraic path properties can model interprocedural dataflow analysis problems, the shortest path problem, the most probable path problem, and so on. The traditional algorithms for interprocedural analysis focus on path properties where the starting point is fixed as the entry point of a specific method. In this work, we consider possible multiple queries as required in many applications such as in alias analysis. The study of multiple queries allows us to bring in an important algorithmic distinction between the resource usage of the one-time preprocessing vs for each individual query. The second aspect we consider is that the control flow graphs for most programs have constant treewidth. Our main contributions are simple and implementable algorithms that support multiple queries for algebraic path properties for RSMs that have constant treewidth. Our theoretical results show that our algorithms have small additional one-time preprocessing but can answer subsequent queries significantly faster as compared to the current algorithmic solutions for interprocedural dataflow analysis. We have also implemented our algorithms and evaluated their performance for performing on-demand interprocedural dataflow analysis on various domains, such as for live variable analysis and reaching definitions, on a standard benchmark set. Our experimental results align with our theoretical statements and show that after a lightweight preprocessing, on-demand queries are answered much faster than the standard existing algorithmic approaches.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2989189770",
    "type": "article"
  },
  {
    "title": "Omnisemantics: Smooth Handling of Nondeterminism",
    "doi": "https://doi.org/10.1145/3579834",
    "publication_date": "2023-01-24",
    "publication_year": 2023,
    "authors": "Arthur Charguéraud; Adam Chlipala; Andres Erbsen; Samuel Gruetter",
    "corresponding_authors": "",
    "abstract": "This article gives an in-depth presentation of the omni-big-step and omni-small-step styles of semantic judgments. These styles describe operational semantics by relating starting states to sets of outcomes rather than to individual outcomes. A single derivation of these semantics for a particular starting state and program describes all possible nondeterministic executions (hence the name omni ), whereas in traditional small-step and big-step semantics, each derivation only talks about one single execution. This restructuring allows for straightforward modeling of both nondeterminism and undefined behavior as commonly encountered in sequential functional and imperative programs. Specifically, omnisemantics inherently assert safety (i.e., they guarantee that none of the execution branches gets stuck), while traditional semantics need either a separate judgment or additional error markers to specify safety in the presence of nondeterminism. Omnisemantics can be understood as an inductively defined weakest-precondition semantics (or more generally, predicate-transformer semantics) that does not involve invariants for loops and recursion but instead uses unrolling rules like in traditional small-step and big-step semantics. Omnisemantics were previously described in association with several projects, but we believe the technique has been underappreciated and deserves a well-motivated, extensive, and pedagogical presentation of its benefits. We also explore several novel aspects associated with these semantics, in particular, their use in type-safety proofs for lambda calculi, partial-correctness reasoning, and forward proofs of compiler correctness for terminating but potentially nondeterministic programs being compiled to nondeterministic target languages. All results in this article are formalized in Coq.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4297927502",
    "type": "article"
  },
  {
    "title": "Interactive Abstract Interpretation with Demanded Summarization",
    "doi": "https://doi.org/10.1145/3648441",
    "publication_date": "2024-02-15",
    "publication_year": 2024,
    "authors": "B. Stein; Bor-Yuh Evan Chang; Manu Sridharan",
    "corresponding_authors": "",
    "abstract": "We consider the problem of making expressive, interactive static analyzers compositional . Such a technique could help bring the power of server-based static analyses to integrated development environments (IDEs), updating their results live as the code is modified. Compositionality is key for this scenario, as it enables reuse of already-computed analysis results for unmodified code. Previous techniques for interactive static analysis either lack compositionality, cannot express arbitrary abstract domains, or are not from-scratch consistent. We present demanded summarization, the first algorithm for incremental compositional analysis in arbitrary abstract domains that guarantees from-scratch consistency. Our approach analyzes individual procedures using a recent technique for demanded analysis, computing summaries on demand for procedure calls. A dynamically updated summary dependency graph enables precise result invalidation after program edits, and the algorithm is carefully designed to guarantee from-scratch-consistent results after edits, even in the presence of recursion and in arbitrary abstract domains. We formalize our technique and prove soundness, termination, and from-scratch consistency. An experimental evaluation of a prototype implementation on synthetic and real-world program edits provides evidence for the feasibility of this theoretical framework, showing potential for major performance benefits over non-demanded compositional analyses.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391843668",
    "type": "article"
  },
  {
    "title": "Isomorph-free model enumeration",
    "doi": "https://doi.org/10.1145/276393.276396",
    "publication_date": "1998-03-01",
    "publication_year": 1998,
    "authors": "Daniel Jackson; Somesh Jha; Craig A. Damon",
    "corresponding_authors": "",
    "abstract": "Software specifications often involve data structures with huge numbers of value, and consequently they cannot be checked using standard state exploration or model-checking techniques. Data structures can be expressed with binary relations, and operations over such structures can be expressed as formulae involving relational variables. Checking properties such as preservation of an invariant thus reduces to determining the validity of a formula or, equivalently, finding a model (of the formula's negation). A new method for finding relational models is presented. It exploits the permutation invariance of models—if two interpretations are isomorphic, then neither is a model, or both are—by partitioning the space into equivalence classes of symmetrical interpretations. Representatives of these classes are constructed incrementally by using the symmetry of the partial interpretation to limit the enumeration of new relation values. The notion of symmetry depends on the type structure of the formula; by picking the weakest typing, larger equivalence classes (and thus fewer representatives) are obtained. A more refined notion of symmetry that exploits the meaning of the relational operators is also described. The method typically leads to exponential reductions; in combination with other, simpler, reductions it makes automatic analysis of relational specifications possible for the first time.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2167030131",
    "type": "article"
  },
  {
    "title": "Decompilation: the enumeration of types and grammars",
    "doi": "https://doi.org/10.1145/186025.186093",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Péter Breuer; Jonathan P. Bowen",
    "corresponding_authors": "",
    "abstract": "While a compiler produces low-level object code from high-level source code, a decompiler produces high-level code from low-level code and has applications in the testing and validation of safety-critical software. The decompilation of an object code provides an independent demonstration of correctness that is hard to better for industrial purposes (an alternative is to prove the compiler correct). But, although compiler compilers are in common use in the software industry, a decompiler compiler is much more unusual. It turns out that a data type specification for a programming-language grammar can be remolded into a functional program that enumerates all of the abstract syntax trees of the grammar. This observation is the springboard for a general method for compiling decompilers from the specifications of (nonoptimizing) compilers. This paper deals with methods and theory, together with an application of the technique. The correctness of a decompiler generated from a simple occam-like compiler specification is demonstrated. The basic problem of enumerating the syntax trees of grammars, and then stopping, is shown to have no recursive solution, but methods of abstract interpretation can be used to guarantee the adequacy and completeness of our technique in practical instances, including the decompiler for the language presented here.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2070251230",
    "type": "article"
  },
  {
    "title": "An alternative solution to a problem on self-stabilization",
    "doi": "https://doi.org/10.1145/155183.155228",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Sukumar Ghosh",
    "corresponding_authors": "Sukumar Ghosh",
    "abstract": "Dijkstra [4, 5] introduced the problem of self-stabilization in distributed systems as an interesting exercise for achieving global convergence through local actions. In [4] he presented three solutions to a specific version of the self-stabilization problem, one of which was proved in [6]. This paper presents an alternative solution of his self-stabilization problem with four-state machines.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2091403433",
    "type": "article"
  },
  {
    "title": "Time-constrained buffer specifications in CSP + T and timed CSP",
    "doi": "https://doi.org/10.1145/197320.197322",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "John Žic",
    "corresponding_authors": "John Žic",
    "abstract": "A finite buffer with time constraints on the rate of accepting inputs, producing outputs, and message latency is specified using both Timed CSP and a new real-time specification language, CSP + T, which adds expressive power to some of the sequential aspects of CSP and allows the description of complex event timings from within a single sequential process. On the other hand, Timed CSP encourages event-timing descriptions to be built up in a constraint-oriented manner with the parallel composition of several processes. Although these represent two complementary specification styles, both provide valuable insights into the specification of complex event timings.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2118471548",
    "type": "article"
  },
  {
    "title": "Fast algorithms for compressed multimethod dispatch table generation",
    "doi": "https://doi.org/10.1145/271510.271521",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Éric Dujardin; Eric Amiel; Eric Simon",
    "corresponding_authors": "",
    "abstract": "The efficiency of dynamic dispatch is a major impediment to the adoption of multimethods in object-oriented languages. In this article, we propose a simple multimethod dispatch scheme based on compressed dispatch tables. This scheme is applicable to any object-oriented language using a method precedence order that satisfies a specific monotonous property (e.g., as Cecil and Dylan) and guarantees that dynamic dispatch is performed in constant time, the latter being a major requirement for some languages and applications. We provide efficient algorithms to build the dispatch tables, provide their worst-case complexity, and demonstrate the effectiveness of our scheme by real measurements performed on two large object-oriented applications. Finally, we provide a detailed comparison of our technique with other existing techniques.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W1966217640",
    "type": "article"
  },
  {
    "title": "Slicing real-time programs for enhanced schedulability",
    "doi": "https://doi.org/10.1145/256167.256394",
    "publication_date": "1997-05-01",
    "publication_year": 1997,
    "authors": "R. Gerber; Seongsoo Hong",
    "corresponding_authors": "",
    "abstract": "In this article we present a compiler-based technique to help develop correct real-time systems. The domain we consider is that of multiprogrammed real-time applications, in which periodic tasks control physical systems via interacting with external sensors and actuators. While a system is up and running, these operations must be performed as specified—otherwise the system may fail. Correctness depends not only on each program individually, but also on the time-multiplexed behavior of all of the programs running together. Errors due to overloaded resources are exposed very late in a development process, and often at runtime. They are usually remedied by human-intensive activities such as instrumentation, measurement, code tuning and redesign. We describe a static alternative to this process, which relies on well-accepted technologies from optimizing compilers and fixed-priority scheduling. Specifically, when a set of tasks are found to be overloaded, a scheduling analyzer determines candidate tasks to be transformed via program slicing. The slicing engine decomposes each of the selected tasks into two fragments: one that is “time critical” and the other “unobservable.” The unobservable part is then spliced to the end of the time-critical code, with the external semantics being maintained. The benefit is that the scheduler may postpone the unobservable code beyond its original deadline, which can enhance overall schedulability. While the optimization is completely local, the improvement is realized globally, for the entire task set.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2002524149",
    "type": "article"
  },
  {
    "title": "A stepwise refinement heuristic for protocol construction",
    "doi": "https://doi.org/10.1145/129393.129394",
    "publication_date": "1992-05-01",
    "publication_year": 1992,
    "authors": "A. Udaya Shankar; Simon S. Lam",
    "corresponding_authors": "",
    "abstract": "A stepwise refinement heuristic to construct distributed systems is presented. The heuristic is based on a conditional refinement relation between system specifications, and a “Marking”. It is applied to construct four sliding window protocols that provide reliable data transfer over unreliable communication channels. The protocols use modulo- N sequence numbers. The first protocol is for channels that can only lose messages in transit. By refining this protocol, we obtain three protocols for channels that can lose, reorder, and duplicate messages in transit. The protocols herein are less restrictive and easier to implement than sliding window protocols previously studied in the protocol verification literature.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2026356804",
    "type": "article"
  },
  {
    "title": "Efficient and safe-for-space closure conversion",
    "doi": "https://doi.org/10.1145/345099.345125",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Zhong Shao; Andrew W. Appel",
    "corresponding_authors": "",
    "abstract": "Modern compilers often implement function calls (or returns) in two steps: first, a “closure” environment is properly installed to provide access for free variables in the target program fragment; second, the control is transferred to the target by a “jump with arguments (for results).” Closure conversion—which decides where and how to represent closures at runtime—is a crucial step in the compilation of functional languages. This paper presents a new algorithm that exploits the use of compile-time control and data-flow information to optimize funtion calls. By extensive closure sharing and allocation by 36% and memory fetches for local and global variables by 43%; and improves the already efficient code generated by an earlier version of the Standard ML of New Jersey compiler by about 17% on a DECstation 5000. Moreover, unlike most other approaches, our new closure-allocation scheme the strong safe-for-space-complexity rule, thus achieving good asymptotic space usage.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2059416532",
    "type": "article"
  },
  {
    "title": "Symbolic model checking for event-driven real-time systems",
    "doi": "https://doi.org/10.1145/244795.244803",
    "publication_date": "1997-03-03",
    "publication_year": 1997,
    "authors": "Jin Yang; Aloysius K. Mok; Farn Wang",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Symbolic model checking for event-driven real-time systems Authors: Jin Yang Univ. of Texas at Austin, Austin Univ. of Texas at Austin, AustinView Profile , Aloysius K. Mok Univ. of Texas at Austin, Austin Univ. of Texas at Austin, AustinView Profile , Farn Wang Academia Sinica, Taiwan, Republic of China Academia Sinica, Taiwan, Republic of ChinaView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 19Issue 2March 1997 pp 386–412https://doi.org/10.1145/244795.244803Published:03 March 1997Publication History 23citation706DownloadsMetricsTotal Citations23Total Downloads706Last 12 Months17Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2124214533",
    "type": "article"
  },
  {
    "title": "Program optimization and parallelization using idioms",
    "doi": "https://doi.org/10.1145/177492.177494",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Shlomit S. Pinter; Ron Y. Pinter",
    "corresponding_authors": "",
    "abstract": "Programs in languages such as Fortran, Pascal, and C were designed and written for a sequential machine model. During the last decade, several methods to vectorize such programs and recover other forms of parallelism that apply to more advanced machine architectures have been developed (particularly for Fortran, due to its pointer-free semantics). We propose and demonstrate a more powerful translation technique for making such programs run efficiently on parallel machines which support facilities such as parallel prefix operations as well as parallel and vector capabilities. This technique, which is global in nature and involves a modification of the traditional definition of the program dependence graph (PDG), is based on the extraction of parallelizable program structures (“idioms”) from the given (sequential) program. The benefits of our technique extend beyond the above-mentioned architectures and can be viewed as a general program optimization method, applicable in many other situations. We show a few examples in which our method indeed outperforms existing analysis techniques.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1997186449",
    "type": "article"
  },
  {
    "title": "The role of commutativity in constraint propagation algorithms",
    "doi": "https://doi.org/10.1145/371880.371884",
    "publication_date": "2000-11-01",
    "publication_year": 2000,
    "authors": "Krzysztof R. Apt",
    "corresponding_authors": "Krzysztof R. Apt",
    "abstract": "Constraing propagation algorithms form an important part of most of the constraint programming systems. We provide here a simple, yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way. In this framework we proceed in two steps. First, we introduce a generic iteration algorithm on partial orderings and prove its correctness in an abstract setting. Then we instantiate this algorithm with specific partial orderings and functions to obtain specific constraint propagation algorithms. In particular, using the notions commutativity and semi-commutativity, we show that the AC-3, PC-2, DAC, and DPC algorithms for achieving (directional) arc consistency and (directional) path consistency are instances of a single generic algorithm. The work reported here extends and simplifies that of Apt [1999a].",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2022854328",
    "type": "article"
  },
  {
    "title": "Hancock",
    "doi": "https://doi.org/10.1145/973097.973100",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Corinna Cortes; Kathleen Fisher; Daryl Pregibon; Anne Rogers; Frederick Smith",
    "corresponding_authors": "",
    "abstract": "Massive transaction streams present a number of opportunities for data mining techniques. The transactions in such streams might represent calls on a telephone network, commercial credit card purchases, stock market trades, or HTTP requests to a web server. While historically such data have been collected for billing or security purposes, they are now being used to discover how the transactors, for example, credit-card numbers or IP addresses, use the associated services.Over the past 5 years, we have computed evolving profiles (called signatures ) of transactors in several very large data streams. The signature for each transactor captures the salient features of his or her behavior through time. Programs for processing signatures must be highly optimized because of the size of the data stream (several gigabytes per day) and the number of signatures to maintain (hundreds of millions). Originally, we wrote such programs directly in C, but because these programs often sacrificed readability for performance, they were difficult to verify and maintain.Hancock is a domain-specific language we created to express computationally efficient signature programs cleanly. In this paper, we describe the obstacles to computing signatures from massive streams and explain how Hancock addresses these problems. For expository purposes, we present Hancock using a running example from the telecommunications industry; however, the language itself is general and applies equally well to other data sources.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W1977489852",
    "type": "article"
  },
  {
    "title": "Automatic generation and use of abstract structure operators",
    "doi": "https://doi.org/10.1145/115372.115369",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Tim Sheard",
    "corresponding_authors": "Tim Sheard",
    "abstract": "article Free Access Share on Automatic generation and use of abstract structure operators Author: Tim Sheard Amherst College, Amherst, MA Amherst College, Amherst, MAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 4pp 531–557https://doi.org/10.1145/115372.115369Published:01 October 1991Publication History 16citation300DownloadsMetricsTotal Citations16Total Downloads300Last 12 Months16Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2061891986",
    "type": "article"
  },
  {
    "title": "Modular typechecking for hierarchically extensible datatypes and functions",
    "doi": "https://doi.org/10.1145/1018203.1018207",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Todd Millstein; Colin Bleckner; Craig Chambers",
    "corresponding_authors": "",
    "abstract": "One promising approach for adding object-oriented (OO) facilities to functional languages like ML is to generalize the existing datatype and function constructs to be hierarchical and extensible, so that datatype variants simulate classes and function cases simulate methods. This approach allows existing datatypes to be easily extended with both new operations and new variants, resolving a longstanding conflict between the functional and OO styles. However, previous designs based on this approach have been forced to give up modular typechecking, requiring whole-program checks to ensure type safety. We describe Extensible ML (EML), an ML-like language that supports hierarchical, extensible datatypes and functions while preserving purely modular typechecking. To achieve this result,EML's type system imposes a few requirements on datatype and function extensibility, but EML is still able to express both traditional functional and OO idioms. We have formalized a core version of EML and proven the associated type system sound, and we have developed a prototype interpreter for the language.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2126234902",
    "type": "article"
  },
  {
    "title": "A framework for the integration of partial evaluation and abstract interpretation of logic programs",
    "doi": "https://doi.org/10.1145/982158.982159",
    "publication_date": "2004-05-01",
    "publication_year": 2004,
    "authors": "Michaël Leuschel",
    "corresponding_authors": "Michaël Leuschel",
    "abstract": "Recently the relationship between abstract interpretation and program specialization has received a lot of scrutiny, and the need has been identified to extend program specialization techniques so as to make use of more refined abstract domains and operators. This article clarifies this relationship in the context of logic programming, by expressing program specialization in terms of abstract interpretation. Based on this, a novel specialization framework, along with generic correctness results for computed answers and finite failure under SLD-resolution, is developed.This framework can be used to extend existing logic program specialization methods, such as partial deduction and conjunctive partial deduction, to make use of more refined abstract domains. It is also shown how this opens up the way for new optimizations. Finally, as shown in the paper, the framework also enables one to prove correctness of new or existing specialization techniques in a simpler manner.The framework has already been applied in the literature to develop and prove correct specialization algorithms using regular types, which in turn have been applied to the verification of infinite state process algebras.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2151036935",
    "type": "article"
  },
  {
    "title": "An approach to support automatic generation of user interfaces",
    "doi": "https://doi.org/10.1145/88616.214518",
    "publication_date": "1990-10-01",
    "publication_year": 1990,
    "authors": "Prasun Dewan; Marvin Solomon",
    "corresponding_authors": "",
    "abstract": "In traditional interactive programming environments, each application individually manages its interaction with the human user. The result is duplication of effort in implementing user interface code and nonuniform—hence confusing—input conventions. This paper presents an approach to support automatic generation of user interfaces in environments based on algebraic languages. The approach supports the editing model of interaction, which allows a user to view all applications as data that can be edited. An application interacts with a user by submitting variables (of arbitrary types) to a dialogue manager , which displays their presentations to the user and offers type-directed editing of these presentations. Applications and dialogue managers communicate through a protocol that allows a presentation to be kept consistent with the variable it displays. A particular implementation of the approach, called Dost, has been constructed for the Xerox development environment and the Mesa programming language. Dost is used as a concrete example to describe the editing model, the primitives to support it, and our preliminary experience with these primitives. The approach is compared with related work, its shortcomings are discussed, and suggestions for future work are made.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1969849199",
    "type": "article"
  },
  {
    "title": "A distributed deadlock detection algorithm for CSP-like communication",
    "doi": "https://doi.org/10.1145/77606.77611",
    "publication_date": "1990-01-03",
    "publication_year": 1990,
    "authors": "S.-T. Huang",
    "corresponding_authors": "S.-T. Huang",
    "abstract": "An algorithm for detecting deadlocks in distributed systems with CSP-like communication is proposed. Unlike previous work, the proposed algorithm avoids periodically sending deadlock-detecting messages by the processes and requires no local storage for the processes with size predetermined by the number of processes in the system. The algorithm is proven to have the following properties: (0) it never detects false deadlocks; (1) it has only one process in a knot report the deadlock; and (2) it detects every true deadlock in finite time.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1999484846",
    "type": "article"
  },
  {
    "title": "Modeling the distributed termination convention of CSP",
    "doi": "https://doi.org/10.1145/579.584",
    "publication_date": "1984-07-01",
    "publication_year": 1984,
    "authors": "Krzysztof R. Apt; Nissim Francez",
    "corresponding_authors": "",
    "abstract": "article Free AccessModeling the distributed termination convention of CSP Authors: Krzysztof R. Apt Universite Paris 7 Universite Paris 7View Profile , Nissim Francez Technion-Israel Institute of Technology Technion-Israel Institute of TechnologyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 3July 1984 pp 370–379https://doi.org/10.1145/579.584Published:01 July 1984Publication History 21citation265DownloadsMetricsTotal Citations21Total Downloads265Last 12 Months6Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2070373441",
    "type": "article"
  },
  {
    "title": "Correctness Proofs of Communicating Processes: Three Illustrative Examples From the Literature",
    "doi": "https://doi.org/10.1145/69575.2083",
    "publication_date": "1983-10-01",
    "publication_year": 1983,
    "authors": "Marty Ossefort",
    "corresponding_authors": "Marty Ossefort",
    "abstract": "article Free Access Share on Correctness Proofs of Communicating Processes: Three Illustrative Examples From the Literature Author: Marty Ossefort The University of Texas at Austin The University of Texas at AustinView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 4Oct. 1983 pp 620–640https://doi.org/10.1145/69575.2083Online:01 October 1983Publication History 15citation297DownloadsMetricsTotal Citations15Total Downloads297Last 12 Months5Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2003164038",
    "type": "article"
  },
  {
    "title": "Space-Efficient Storage Management in an Attribute Grammar Evaluator",
    "doi": "https://doi.org/10.1145/357146.357148",
    "publication_date": "1981-10-01",
    "publication_year": 1981,
    "authors": "Mehdi Jazayeri; Diane Pozefsky",
    "corresponding_authors": "",
    "abstract": "article Free AccessSpace-Efficient Storage Management in an Attribute Grammar Evaluator Authors: Mehdi Jazayeri Synapse Computer Corporation, 801 Buckeye Court, Milpitas, CA Synapse Computer Corporation, 801 Buckeye Court, Milpitas, CAView Profile , Diane Pozefsky IBM Corporation, P. O. Box 12195, Research Triangle Park, NC IBM Corporation, P. O. Box 12195, Research Triangle Park, NCView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 4Oct. 1981 pp 388–404https://doi.org/10.1145/357146.357148Published:01 October 1981Publication History 20citation311DownloadsMetricsTotal Citations20Total Downloads311Last 12 Months44Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2018115058",
    "type": "article"
  },
  {
    "title": "Noncanonical SLR(1) Grammars",
    "doi": "https://doi.org/10.1145/357073.357083",
    "publication_date": "1979-10-01",
    "publication_year": 1979,
    "authors": "Kou-Chung Tai",
    "corresponding_authors": "Kou-Chung Tai",
    "abstract": "Two noncanonical extensions of the simple LR(1) (SLR(1)) method are presented, which reduce not only handles but also other phrases of sentential forms. A class of context-free grammars called leftmost SLR(1) (LSLR(1)) is defined by using lookahead symbols which appear in leftmost derivations. This class includes the SLR(1), reflected SMSP, and total precedence grammars as proper subclasses. The class of LSLR(1) languages properly includes the deterministic context-free languages, their reflections, and total precedence languages. By requiring that phrases which have been scanned be reduced as early as possible, a larger class of context-free grammars called noncanonical SLR(1) (NSLR(1)) is defined. The NSLR(1) languages can be recognized deterministically in linear time using a two-stack pushdown automaton. An NSLR(1) parser generator has been implemented. Empirical results show that efficient NSLR(1) parsers can be constructed for some non-LR grammars which generate nondeterministic languages. Applications of the NSLR(1) method to improve the parsing and translation of programming languages are discussed.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2019794278",
    "type": "article"
  },
  {
    "title": "An efficient on-the-fly cycle collection",
    "doi": "https://doi.org/10.1145/1255450.1255453",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Harel Paz; David F. Bacon; Elliot K. Kolodner; Erez Petrank; V. T. Rajan",
    "corresponding_authors": "",
    "abstract": "A reference-counting garbage collector cannot reclaim unreachable cyclic structures of objects. Therefore, reference-counting collectors either use a backup tracing collector infrequently, or employ a cycle collector to reclaim cyclic structures. We propose a new concurrent cycle collector, one that runs concurrently with the program threads, imposing negligible pauses (of around 1ms) on a multiprocessor. Our new collector combines a state-of-the-art cycle collector [Bacon and Rajan 2001] with sliding-views collectors [Levanoni and Petrank 2001, 2006; Azatchi et al. 2003]. The use of sliding views for cycle collection yields two advantages. First, it drastically reduces the number of cycle candidates, which in turn drastically reduces the work required to record and trace these candidates. Consequentially, a large improvement in cycle collection efficiency is achieved. Second, it eliminates the theoretical termination problem that appeared in the earlier concurrent cycle collector. There, a rare race may delay the reclamation of an unreachable cyclic structure forever. The sliding-views cycle collector guarantees reclamation of all unreachable cyclic structures. The proposed collector was implemented on the Jikes RVM and we provide measurements including a comparison between the use of backup tracing and the use of cycle collection with reference counting. To the best of our knowledge, such a comparison has not been reported before.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2020431536",
    "type": "article"
  },
  {
    "title": "Local reasoning about a copying garbage collector",
    "doi": "https://doi.org/10.1145/1377492.1377499",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Noah Torp-Smith; Lars Birkedal; John Reynolds",
    "corresponding_authors": "",
    "abstract": "We present a programming language, model, and logic appropriate for implementing and reasoning about a memory management system. We state semantically what is meant by correctness of a copying garbage collector, and employ a variant of the novel separation logics to formally specify partial correctness of Cheney's copying garbage collector in our program logic. Finally, we prove that our implementation of Cheney's algorithm meets its specification using the logic we have given and auxiliary variables.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1966891975",
    "type": "article"
  },
  {
    "title": "A type system equivalent to a model checker",
    "doi": "https://doi.org/10.1145/1387673.1387678",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Mayur Naik; Jens Palsberg",
    "corresponding_authors": "",
    "abstract": "Type systems and model checking are two prevalent approaches to program verification. A prominent difference between them is that type systems are typically defined in a syntactic and modular style whereas model checking is usually performed in a semantic and whole-program style. This difference between the two approaches makes them complementary to each other: type systems are good at explaining why a program was accepted while model checkers are good at explaining why a program was rejected. We present a type system that is equivalent to a model checker for verifying temporal safety properties of imperative programs. The model checker is natural and may be instantiated with any finite-state abstraction scheme such as predicate abstraction. The type system, which is also parametric, type checks exactly those programs that are accepted by the model checker. It uses a variant of function types to capture flow sensitivity and intersection and union types to capture context sensitivity. Our result sheds light on the relationship between type systems and model checking, provides a methodology for studying their relative expressiveness, is a step towards sharing results between the two approaches, and motivates synergistic program analyses involving interplay between them.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2053822998",
    "type": "article"
  },
  {
    "title": "Parametric polymorphism for XML",
    "doi": "https://doi.org/10.1145/1596527.1596529",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Haruo Hosoya; Alain Frisch; Giuseppe Castagna",
    "corresponding_authors": "",
    "abstract": "Despite the extensiveness of recent investigations on static typing for XML, parametric polymorphism has rarely been treated. This well-established typing discipline can also be useful in XML processing in particular for programs involving “parametric schemas,” that is, schemas parameterized over other schemas (e.g., SOAP). The difficulty in treating polymorphism for XML lies in how to extend the “semantic” approach used in the mainstream (monomorphic) XML type systems. A naive extension would be “semantic” quantification over all substitutions for type variables. However, this approach reduces to an NEXPTIME-complete problem for which no practical algorithm is known and induces a subtyping relation that may not always match the programmer's intuition. In this article, we propose a different method that smoothly extends the semantic approach yet is algorithmically easier. The key idea here is to devise a novel and simple marking technique, where we interpret a polymorphic type as a set of values with annotations of which subparts are parameterized. We exploit this interpretation in every ingredient of our polymorphic type system such as subtyping, inference of type arguments, etc. As a result, we achieve a sensible system that directly represents a usual expected behavior of polymorphic type systems—“values of abstract types are never reconstructed”—in a reminiscence of Reynold's parametricity theory. Also, we obtain a set of practical algorithms for typechecking by local modifications to existing ones for a monomorphic system.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2039018550",
    "type": "article"
  },
  {
    "title": "Space overhead bounds for dynamic memory management with partial compaction",
    "doi": "https://doi.org/10.1145/2362389.2362392",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Anna Bendersky; Erez Petrank",
    "corresponding_authors": "",
    "abstract": "Dynamic memory allocation is ubiquitous in today's runtime environments. Allocation and deallocation of objects during program execution may cause fragmentation and foil the program's ability to allocate objects. Robson [1971] has shown that a worst-case scenario can create a space overhead within a factor of log n of the space that is actually required by the program, where n is the size of the largest possible object. Compaction can eliminate fragmentation, but is too costly to be run frequently. Many runtime systems employ partial compaction, in which only a small fraction of the allocated objects are moved. Partial compaction reduces some of the existing fragmentation at an acceptable cost. In this article we study the effectiveness of partial compaction and provide the first rigorous lower and upper bounds on its effectiveness in reducing fragmentation at a low cost.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2000746586",
    "type": "article"
  },
  {
    "title": "Reachability analysis of program variables",
    "doi": "https://doi.org/10.1145/2529990",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Đurica Nikolić; Fausto Spoto",
    "corresponding_authors": "",
    "abstract": "Reachability from a program variable v to a program variable w states that from v , it is possible to follow a path of memory locations that leads to the object bound to w . We present a new abstract domain for the static analysis of possible reachability between program variables or, equivalently, definite unreachability between them. This information is important for improving the precision of other static analyses, such as side-effects, field initialization, cyclicity and path-length analysis, as well as more complex analyses built upon them, such as nullness and termination analysis. We define and prove correct our reachability analysis for Java bytecode, defined as a constraint-based analysis, where the constraint is a graph whose nodes are the program points and whose arcs propagate reachability information in accordance to the abstract semantics of each bytecode instruction. For each program point p , our reachability analysis produces an overapproximation of the ordered pairs of variables 〈 v , w 〉 such that v might reach w at p . Seen the other way around, if a pair 〈 v , w 〉 is not present in the overapproximation at p , then v definitely does not reach w at p . We have implemented the analysis inside the Julia static analyzer. Our experiments of analysis of nontrivial Java and Android programs show the improvement of precision due to the presence of reachability information. Moreover, reachability analysis actually reduces the overall cost of nullness and termination analysis.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1995268059",
    "type": "article"
  },
  {
    "title": "S <scp>cala</scp> E <scp>xtrap</scp>",
    "doi": "https://doi.org/10.1145/2160910.2160914",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "Xing Wu; Frank Mueller",
    "corresponding_authors": "",
    "abstract": "Performance modeling for scientific applications is important for assessing potential application performance and systems procurement in high-performance computing (HPC). Recent progress on communication tracing opens up novel opportunities for communication modeling due to its lossless yet scalable trace collection. Estimating the impact of scaling on communication efficiency still remains nontrivial due to execution-time variations and exposure to hardware and software artifacts. This work contributes a fundamentally novel modeling scheme. We synthetically generate the application trace for large numbers of nodes via extrapolation from a set of smaller traces. We devise an innovative approach for topology extrapolation of single program, multiple data (SPMD) codes with stencil or mesh communication. Experimental results show that the extrapolated traces precisely reflect the communication behavior and the performance characteristics at the target scale for both strong and weak scaling applications. The extrapolated trace can subsequently be (a) replayed to assess communication requirements before porting an application, (b) transformed to autogenerate communication benchmarks for various target platforms, and (c) analyzed to detect communication inefficiencies and scalability limitations. To the best of our knowledge, rapidly obtaining the communication behavior of parallel applications at arbitrary scale with the availability of timed replay, yet without actual execution of the application, at this scale, is without precedence and has the potential to enable otherwise infeasible system simulation at the exascale level.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2127314823",
    "type": "article"
  },
  {
    "title": "Sound Non-Statistical Clustering of Static Analysis Alarms",
    "doi": "https://doi.org/10.1145/3095021",
    "publication_date": "2017-08-17",
    "publication_year": 2017,
    "authors": "Woosuk Lee; Wonchan Lee; Dongok Kang; Kihong Heo; Hakjoo Oh; Kwangkeun Yi",
    "corresponding_authors": "",
    "abstract": "We present a sound method for clustering alarms from static analyzers. Our method clusters alarms by discovering sound dependencies between them such that if the dominant alarms of a cluster turns out to be false, all the other alarms in the same cluster are guaranteed to be false. We have implemented our clustering algorithm on top of a realistic buffer-overflow analyzer and proved that our method reduces 45% of alarm reports. Our framework is applicable to any abstract interpretation-based static analysis and orthogonal to abstraction refinements and statistical ranking schemes.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2749587018",
    "type": "article"
  },
  {
    "title": "Bit-Precise Procedure-Modular Termination Analysis",
    "doi": "https://doi.org/10.1145/3121136",
    "publication_date": "2017-12-10",
    "publication_year": 2017,
    "authors": "Hongyi Chen; Cristina David; Daniel Kroening; Peter Schrammel; Björn Wachter",
    "corresponding_authors": "",
    "abstract": "Non-termination is the root cause of a variety of program bugs, such as hanging programs and denial-of-service vulnerabilities. This makes an automated analysis that can prove the absence of such bugs highly desirable. To scale termination checks to large systems, an interprocedural termination analysis seems essential. This is a largely unexplored area of research in termination analysis, where most effort has focussed on small but difficult single-procedure problems. We present a modular termination analysis for C programs using template-based interprocedural summarisation. Our analysis combines a context-sensitive, over-approximating forward analysis with the inference of under-approximating preconditions for termination. Bit-precise termination arguments are synthesised over lexicographic linear ranking function templates. Our experimental results show the advantage of interprocedural reasoning over monolithic analysis in terms of efficiency, while retaining comparable precision.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2752631201",
    "type": "article"
  },
  {
    "title": "Behavioural Equivalence via Modalities for Algebraic Effects",
    "doi": "https://doi.org/10.1145/3363518",
    "publication_date": "2019-11-21",
    "publication_year": 2019,
    "authors": "Alex Simpson; Niels Voorneveld",
    "corresponding_authors": "",
    "abstract": "The article investigates behavioural equivalence between programs in a call-by-value functional language extended with a signature of (algebraic) effect-triggering operations. Two programs are considered as being behaviourally equivalent if they enjoy the same behavioural properties. To formulate this, we define a logic whose formulas specify behavioural properties. A crucial ingredient is a collection of modalities expressing effect-specific aspects of behaviour. We give a general theory of such modalities. If two conditions, openness and decomposability , are satisfied by the modalities, then the logically specified behavioural equivalence coincides with a modality-defined notion of applicative bisimilarity, which can be proven to be a congruence by a generalisation of Howe’s method. We show that the openness and decomposability conditions hold for several examples of algebraic effects: nondeterminism, probabilistic choice, global store, and input/output.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2989806547",
    "type": "article"
  },
  {
    "title": "Conditional Independence by Typing",
    "doi": "https://doi.org/10.1145/3490421",
    "publication_date": "2021-12-09",
    "publication_year": 2021,
    "authors": "Maria I. Gorinova; Andrew D. Gordon; Charles Sutton; Matthijs Vákár",
    "corresponding_authors": "",
    "abstract": "A central goal of probabilistic programming languages (PPLs) is to separate modelling from inference. However, this goal is hard to achieve in practice. Users are often forced to re-write their models to improve efficiency of inference or meet restrictions imposed by the PPL. Conditional independence (CI) relationships among parameters are a crucial aspect of probabilistic models that capture a qualitative summary of the specified model and can facilitate more efficient inference. We present an information flow type system for probabilistic programming that captures conditional independence (CI) relationships and show that, for a well-typed program in our system, the distribution it implements is guaranteed to have certain CI-relationships. Further, by using type inference, we can statically deduce which CI-properties are present in a specified model. As a practical application, we consider the problem of how to perform inference on models with mixed discrete and continuous parameters. Inference on such models is challenging in many existing PPLs, but can be improved through a workaround, where the discrete parameters are used implicitly , at the expense of manual model re-writing. We present a source-to-source semantics-preserving transformation, which uses our CI-type system to automate this workaround by eliminating the discrete parameters from a probabilistic program. The resulting program can be seen as a hybrid inference algorithm on the original program, where continuous parameters can be drawn using efficient gradient-based inference methods, while the discrete parameters are inferred using variable elimination. We implement our CI-type system and its example application in SlicStan: a compositional variant of Stan. 1",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4206292736",
    "type": "article"
  },
  {
    "title": "CHAD: Combinatory Homomorphic Automatic Differentiation",
    "doi": "https://doi.org/10.1145/3527634",
    "publication_date": "2022-08-17",
    "publication_year": 2022,
    "authors": "Matthijs Vákár; Tom Smeding",
    "corresponding_authors": "",
    "abstract": "We introduce Combinatory Homomorphic Automatic Differentiation (CHAD), a principled, pure, provably correct define-then-run method for performing forward and reverse mode automatic differentiation (AD) on programming languages with expressive features. It implements AD as a compositional, type-respecting source-code transformation that generates purely functional code. This code transformation is principled in the sense that it is the unique homomorphic (structure preserving) extension to expressive languages of Elliott’s well-known and unambiguous definitions of AD for a first-order functional language. Correctness of the method follows by a (compositional) logical relations argument that shows that the semantics of the syntactic derivative is the usual calculus derivative of the semantics of the original program. In their most elegant formulation, the transformations generate code with linear types. However, the code transformations can be implemented in a standard functional language lacking linear types: While the correctness proof requires tracking of linearity, the actual transformations do not. In fact, even in a standard functional language, we can get all of the type-safety that linear types give us: We can implement all linear types used to type the transformations as abstract types by using a basic module system. In this article, we detail the method when applied to a simple higher-order language for manipulating statically sized arrays. However, we explain how the methodology applies, more generally, to functional languages with other expressive features. Finally, we discuss how the scope of CHAD extends beyond applications in AD to other dynamic program analyses that accumulate data in a commutative monoid.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3147570207",
    "type": "article"
  },
  {
    "title": "Fast Graph Simplification for Interleaved-Dyck Reachability",
    "doi": "https://doi.org/10.1145/3492428",
    "publication_date": "2022-02-24",
    "publication_year": 2022,
    "authors": "Yuanbo Li; Qirun Zhang; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "Many program-analysis problems can be formulated as graph-reachability problems. Interleaved Dyck language reachability ( InterDyck -reachability) is a fundamental framework to express a wide variety of program-analysis problems over edge-labeled graphs. The InterDyck language represents an intersection of multiple matched-parenthesis languages (i.e., Dyck languages). In practice, program analyses typically leverage one Dyck language to achieve context-sensitivity, and other Dyck languages to model data dependencies, such as field-sensitivity and pointer references/dereferences. In the ideal case, an InterDyck -reachability framework should model multiple Dyck languages simultaneously . Unfortunately, precise InterDyck -reachability is undecidable. Any practical solution must over-approximate the exact answer. In the literature, a lot of work has been proposed to over-approximate the InterDyck -reachability formulation. This article offers a new perspective on improving both the precision and the scalability of InterDyck -reachability: we aim at simplifying the underlying input graph G . Our key insight is based on the observation that if an edge is not contributing to any InterDyck -paths, we can safely eliminate it from G . Our technique is orthogonal to the InterDyck -reachability formulation and can serve as a pre-processing step with any over-approximating approach for InterDyck -reachability. We have applied our graph simplification algorithm to pre-processing the graphs from a recent InterDyck -reachability-based taint analysis for Android. Our evaluation of three popular InterDyck -reachability algorithms yields promising results. In particular, our graph-simplification method improves both the scalability and precision of all three InterDyck -reachability algorithms, sometimes dramatically.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4225793633",
    "type": "article"
  },
  {
    "title": "Side-channel Elimination via Partial Control-flow Linearization",
    "doi": "https://doi.org/10.1145/3594736",
    "publication_date": "2023-05-03",
    "publication_year": 2023,
    "authors": "Luigi Soares; Michael Canesche; Fernando Magno Quintão Pereira",
    "corresponding_authors": "",
    "abstract": "Partial control-flow linearization is a code transformation conceived to maximize work performed in vectorized programs. In this article, we find a new service for it. We show that partial control-flow linearization protects programs against timing attacks. This transformation is sound: Given an instance of its public inputs, the partially linearized program always runs the same sequence of instructions, regardless of secret inputs. Incidentally, if the original program is publicly safe, then accesses to the data cache will be data oblivious in the transformed code. The transformation is optimal: Every branch that depends on some secret data is linearized; no branch that depends on only public data is linearized. Therefore, the transformation preserves loops that depend exclusively on public information. If every branch that leaves a loop depends on secret data, then the transformed program will not terminate. Our transformation extends previous work in non-trivial ways. It handles C constructs such as “goto,” “break,” “switch,” and “continue,” which are absent in the FaCT domain-specific language (2018). Like Constantine (2021), our transformation ensures operation invariance but without requiring profiling information. Additionally, in contrast to SC-Eliminator (2018) and Lif (2021), it handles programs containing loops whose trip count is not known at compilation time.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4367847590",
    "type": "article"
  },
  {
    "title": "Dynamic typing for distributed programming in polymorphic languages",
    "doi": "https://doi.org/10.1145/314602.314604",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Dominic Duggan",
    "corresponding_authors": "Dominic Duggan",
    "abstract": "While static typing is widely accepted as being necessary for secure program execution, dynamic typing is also viewed as being essential in some applications, particularly for distributed programming environments. Dynamics have been proposed as a language construct for dynamic typing, based on experience with languages such as CLU, Cedar/Mesa, and Modula-3. However proposals for incorporating dynamic typing into languages with parametric polymorphism have serious shortcomings. A new approach is presented to extending polymorphic lnanguages with dynamic typing. At the heart of the approach is the use of dynamic type dispatch, where polymorphic functions may analyze the structure of their type arguments. This approach solves several open problems with the traditional approach to adding dynamic typing to polymorphic languages. An explicity typed language XML dyn is presented; this language uses refinement kinds to ensure that dynamic type dispatch does not fail at run-time. Safe dynamics are a new form of dynamics that use refinement kinds to statically check the use of run-time dynamic typing. Run-time errors are isolated to a separate construct for performing run-time type checks",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2076212434",
    "type": "article"
  },
  {
    "title": "A provably time-efficient parallel implementation of full speculation",
    "doi": "https://doi.org/10.1145/316686.316690",
    "publication_date": "1999-03-01",
    "publication_year": 1999,
    "authors": "John Greiner; Guy E. Blelloch",
    "corresponding_authors": "",
    "abstract": "Speculative evaluation, including leniency and futures, is often used to produce high degrees of parallelism. Understanding the performance characteristics of such evaluation, however, requires having a detailed understanding of the implementation. For example, the particular implementaion technique used to suspend and reactivate threads can have an asymptotic effect on performance. With the goal of giving the users some understanding of performance without requiring them to understand the implementation, we present a provable implementation bound for a language based on speculative evaluation. The idea is (1) to supply the users with a semantics for a language that defines abstract costs for measuring or analyzing the performance of computations, (2) to supply the users with a mapping of these costs onto runtimes on various machine models, and (3) to describe an implementation strategy of the language and prove that it meets these mappings. For this purpose we consider a simple language based on speculative evaluation. For every computation, the semantics of the language returns a directed acyclic graph (DAG) in which each node represents a unit of computation, and each edge represents a dependence. We then describe an implementation strategy of the language and show that any computation with w work (the number of nodes in the DAG) and d depth (the length of the longest path in the DAG) will run on a p -processor PRAM in O ( w / p + d log p ) time. The bounds are work efficient (within a constant factor of linear speedup) when there is sufficient parallelism, w / d ≥ p log p . These are the first time bounds we know of for languages with speculative evaluation. The main challenge is in parallelizing the necessary queuing operations on suspended threads.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1973739282",
    "type": "article"
  },
  {
    "title": "Efficiently computing Φ-nodes on-the-fly",
    "doi": "https://doi.org/10.1145/203095.203099",
    "publication_date": "1995-05-01",
    "publication_year": 1995,
    "authors": "Ron K. Cytron; Jeanne Ferrante",
    "corresponding_authors": "",
    "abstract": "Recently, Static Single-Assignment Form and Sparse Evaluation Graphs have been advanced for the efficient solution of program optimization problems. Each method is provided with an initial set of flow graph nodes that inherently affect a problem's solution. Other relevant nodes are those where potentially disparate solutions must combine. Previously, these so-called φ-nodes were found by computing the iterated dominance frontiers of the initial set of nodes, a process that could take worst-case quadratic time with respect to the input flow graph. In this article we present an almost-linear algorithm for determining exactly the same set of φ-nodes.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1974662185",
    "type": "article"
  },
  {
    "title": "On subtyping and matching",
    "doi": "https://doi.org/10.1145/233561.233563",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "Martı́n Abadi; Luca Cardelli",
    "corresponding_authors": "",
    "abstract": "A relation between recursive object types, called matching , has been proposed as a generalization of subtyping. Unlike subtyping, matching does not support subsumption, but it does support inheritance of binary methods. We argue that matching is a good idea, but that it should not be regarded as a form of F-bounded subtyping (as was originally intended). We show that a new interpretation of matching as higher-order subtyping has better properties. Matching turns out to be a third-order construction, possibly the only one to have been proposed for general use in programming.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2023878569",
    "type": "article"
  },
  {
    "title": "Optimal incremental parsing",
    "doi": "https://doi.org/10.1145/200994.200996",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "J. M. Larchevêque",
    "corresponding_authors": "J. M. Larchevêque",
    "abstract": "This communication sets the problem of incremental parsing in the context of a complete incremental compiling system. It turns out that, according to the incrementally paradigm of the attribute evaluator and data-flow analyzer to be used, two definitions of optimal incrementality in a parser are possible. Algorithms for achieving both forms of optimality are given, both of them based on ordinary LALR(1) parse tables. Optimality and correctness proofs, which are merely outlined in this communication, are made intuitive thanks to the concept of a well-formed list of threaded trees , a natural extension of the concept of threaded tree found in earlier works on incremental parsing.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2083717890",
    "type": "article"
  },
  {
    "title": "Pure versus impure Lisp",
    "doi": "https://doi.org/10.1145/244795.244798",
    "publication_date": "1997-03-03",
    "publication_year": 1997,
    "authors": "Nicholas Pippenger",
    "corresponding_authors": "Nicholas Pippenger",
    "abstract": "article Free AccessPure versus impure Lisp Author: Nicholas Pippenger Univ. of British Columbia, Vancouver, B.C., Canada Univ. of British Columbia, Vancouver, B.C., CanadaView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 19Issue 2pp 223–238https://doi.org/10.1145/244795.244798Published:03 March 1997Publication History 16citation753DownloadsMetricsTotal Citations16Total Downloads753Last 12 Months39Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2010526744",
    "type": "article"
  },
  {
    "title": "Lattice frameworks for multisource and bidirectional data flow problems",
    "doi": "https://doi.org/10.1145/213978.213989",
    "publication_date": "1995-09-01",
    "publication_year": 1995,
    "authors": "Stephen P. Masticola; Thomas J. Marlowe; Barbara G. Ryder",
    "corresponding_authors": "",
    "abstract": "Multisource data flow problems involve information which may enter nodes independently through different classes of edges. In some cases, dissimilar meet operations appear to be used for different types of nodes. These problems include bidirectional and flow-sensitive problems as well as many static analyses of concurrent programs with synchronization. K-tuple frameworks , a type of standard data flow framework, provide a natural encoding for multisource problems using a single meet operator. Previously, the solution of these problems has been described as the fixed point of a set of data flow equations. Using our k -tuple representation, we can access the general results of standard data flow frameworks concerning convergence time and solution precision for these problems. We demonstrate this for the bidirectional component of partial redundancy suppression and two problems on the program summary graph. An interesting subclass of k -tuple frameworks, the join-of-meets frameworks, is useful for reachability problems, especially those stemming from analyses of explicitly parallel programs. We give results on function space properties for join-of-meets frameworks that indicate precise solutions for most of them will be difficult to obtain.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2054068074",
    "type": "article"
  },
  {
    "title": "M-LISP: a representation-independent dialect of LISP with reduction semantics",
    "doi": "https://doi.org/10.1145/133233.133254",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "Robert Muller",
    "corresponding_authors": "Robert Muller",
    "abstract": "In this paper we introduce M-LISP, a dialect of LISP designed with an eye toward reconciling LISP's metalinguistic power with the structural style of operational semantics advocated by Plotkin [28]. We begin by reviewing the original definition of LISP [20] in an attempt to clarify the source of its metalinguistic power. We find that it arises from a problematic clause in this definition. We then define the abstract syntax and operational semantics of M-LISP, essentially a hybrid of M-expression LISP and Scheme. Next, we tie the operational semantics to the corresponding equational logic. As usual, provable equality in the logic implies operational equality. Having established this framework we then extend M-LISP with the metalinguistic eval and reify operators (the latter is a nonstrict operator that converts its argument to its metalanguage representation). These operators encapsulate the matalinguistic representation conversions that occur globally in S-expression LISP. We show that the naive versions of these operators render LISP's equational logic inconsistent. On the positive side, we show that a naturally restricted form of the eval operator is confluent and therefore a conservative extension of M-LISP. Unfortunately, we must weaken the logic considerably to obtain a consistent theory of reification.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2074423628",
    "type": "article"
  },
  {
    "title": "Avoidance and suppression of compensation code in a trace scheduling compiler",
    "doi": "https://doi.org/10.1145/183432.183446",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Stefan M. Freudenberger; Thomas R. Gross; P. Geoffrey Lowney",
    "corresponding_authors": "",
    "abstract": "Trace scheduling is an optimization technique that selects a sequence of basic blocks as a trace and schedules the operations from the trace together. If an operation is moved across basic block boundaries, one or more compensation copies may be required in the off-trace code. This article discusses the generation of compensation code in a trace scheduling compiler and presents techniques for limiting the amount of compensation code: avoidance (restricting code motion so that no compensation code is required) and suppression (analyzing the global flow of the program to detect when a copy is redundant). We evaluate the effectiveness of these techniques based on measurements for the SPEC89 suite and the Livermore Fortran Kernels, using our implementation of trace scheduling for a Multiflow Trace 7/300. The article compares different compiler models contrasting the performance of trace scheduling with the performance obtained from typical RISC compilation techniques. There are two key results of this study. First, the amount of compensation code generated is not large. For the SPEC89 suite, the average code size increase due to trace scheduling is 6%. Avoidance is more important than suppression, although there are some kernels that benefit significantly from compensation code suppression. Since compensation code is not a major issue, a compiler can be more aggressive in code motion and loop unrolling. Second, compensation code is not critical to obtain the benefits of trace scheduling. Our implementation of trace scheduling improves the SPEC mark rating by 30% over basic block scheduling, but restricting trace scheduling so that no compensation code is required improves the rating by 25%. This indicates that most basic block scheduling techniques can be extended to trace scheduling without requiring any complicated compensation code bookkeeping.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2138133941",
    "type": "article"
  },
  {
    "title": "The apprentice challenge",
    "doi": "https://doi.org/10.1145/514188.514189",
    "publication_date": "2002-05-01",
    "publication_year": 2002,
    "authors": "J Strother Moore; George Porter",
    "corresponding_authors": "",
    "abstract": "We describe a mechanically checked proof of a property of a small system of Java programs involving an unbounded number of threads and synchronization, via monitors. We adopt the output of the javac compiler as the semantics and verify the system at the bytecode level under an operational semantics for the JVM. We assume a sequentially consistent memory model and atomicity at the bytecode level. Our operational semantics is expressed in ACL2, a Lisp-based logic of recursive functions. Our proofs are checked with the ACL2 theorem prover. The proof involves reasoning about arithmetic; infinite loops; the creation and modification of instance objects in the heap, including threads; the inheritance of fields from superclasses; pointer chasing and smashing; the invocation of instance methods (and the concomitant dynamic method resolution); use of the start method on thread objects; the use of monitors to attain synchronization between threads; and consideration of all possible interleavings (at the bytecode level) over an unbounded number of threads. Readers familiar with monitor-based proofs of mutual exclusion will recognize our proof as fairly classical. The novelty here comes from (i) the complexity of the individual operations on the abstract machine; (ii) the dependencies between Java threads, heap objects, and synchronization; (iii) the bytecode-level interleaving; (iv) the unbounded number of threads; (v) the presence in the heap of incompletely initialized threads and other objects; and (vi) the proof engineering permitting automatic mechanical verification of code-level theorems. We discuss these issues. The problem posed here is also put forth as a benchmark against which to measure other approaches to formally proving properties of multithreaded Java programs.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2140200969",
    "type": "article"
  },
  {
    "title": "Incremental global reoptimization of programs",
    "doi": "https://doi.org/10.1145/128861.128865",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "Lori Pollock; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "Although optimizing compilers have been quite successful in producing excellent code, two factors that limit their usefulness are the accompanying long compilation times and the lack of good symbolic debuggers for optimized code. One approach to attaining faster recompilations is to reduce the redundant analysis that is performed for optimization in response to edits, and in particulars, small maintenance changes, without affecting the quality of the generated code. Although modular programming with separate compilation aids in eliminating unnecessary recompilation and reoptimization, recent studies have discovered that more efficient code can be generated by collapsing a modular program through procedure inlining. To avoid having to reoptimize the resultant large procedures, this paper presents techniques for incrementally incorporating changes into globally optimized code. An algorithm is given for determining which optimizations are no longer safe after a program change, and for discovering which new optimizations can be performed in order to maintain a high level of optimization. An intermediate representation is incrementally updated to reflect the current optimizations in the program. Analysis is performed in response to changes rather than in preparation for possible changes, so analysis is not wasted if an edit has no far-reaching effects. The techniques developed in this paper have also been exploited to improve on the current techniques for symbolic debugging of optimized code.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2011749833",
    "type": "article"
  },
  {
    "title": "Efficiently counting program events with support for on-line queries",
    "doi": "https://doi.org/10.1145/186025.186027",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Thomas Ball",
    "corresponding_authors": "Thomas Ball",
    "abstract": "The ability to count events in a program's execution is required by many program analysis applications. We represent an instrumentation method for efficiently counting events in a program's execution, with support for on-line queries of the event count. Event counting differs from basic block profiling in that an aggregate count of events is kept rather than a set of counters. Due to this difference, solutions to basic block profiling are not well suited to event counting. Our algorithm finds a subset of points in a program to instrument, while guaranteeing that accurate event counts can be obtained efficiently at every point in the execution.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2046523746",
    "type": "article"
  },
  {
    "title": "Type-Safe linking with recursive DLLs and shared libraries",
    "doi": "https://doi.org/10.1145/586088.586093",
    "publication_date": "2002-11-01",
    "publication_year": 2002,
    "authors": "Dominic Duggan",
    "corresponding_authors": "Dominic Duggan",
    "abstract": "Component-based programming is an increasingly prevalent theme in software development, motivating the need for expressive and safe module interconnection languages. Dynamic linking is an important requirement for module interconnection languages, as exemplified by dynamic link libraries (DLLs) and class loaders in operating systems and Java, respectively. A semantics is given for a type-safe module interconnection language that supports shared libraries and dynamic linking, as well as circular import dependencies (recursive modules). The core language requirements of the module interconnection language are compatible with programming languages such as Java and C#.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2011010004",
    "type": "article"
  },
  {
    "title": "Repairing syntax errors in LR parsers",
    "doi": "https://doi.org/10.1145/586088.586092",
    "publication_date": "2002-11-01",
    "publication_year": 2002,
    "authors": "Rafael Corchuelo; José Antonio Pérez; Antonio Ruiz–Cortés; Miguel Toro",
    "corresponding_authors": "",
    "abstract": "This article reports on an error-repair algorithm for LR parsers. It locally inserts, deletes or shifts symbols at the positions where errors are detected, thus modifying the right context in order to resume parsing on a valid piece of input. This method improves on others in that it does not require the user to provide additional information about the repair process, it does not require precalculation of auxiliary tables, and it can be easily integrated into existing LR parser generators. A Yacc-based implementation is presented along with some experimental results and comparisons with other well-known methods.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2079991543",
    "type": "article"
  },
  {
    "title": "Termination analysis and specialization-point insertion in offline partial evaluation",
    "doi": "https://doi.org/10.1145/1108970.1108973",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Arne John Glenstrup; Neil D. Jones",
    "corresponding_authors": "",
    "abstract": "Recent research suggests that the goal of fully automatic and reliable program generation for a broad range of applications is coming nearer to feasibility. However, several interesting and challenging problems remain to be solved before it becomes a reality. Solving them is also necessary, if we hope ever to elevate software engineering from its current state (a highly developed handiwork) into a successful branch of engineering, capable of solving a wide range of new problems by systematic, well-automated and well-founded methods.A key problem in all program generation is termination of the generation process. This article focuses on off-line partial evaluation and describes recent progress towards automatically solving the termination problem, first for individual programs, and then for specializers and “generating extensions,” the program generators that most offline partial evaluators produce.The technique is based on size-change graphs that approximate the changes in parameter sizes at function calls. We formulate a criterion, bounded anchoring, for detecting parameters known to be bounded during specialization: a bounded parameter can act as an anchor for other parameters. Specialization points necessary for termination are computed by adding a parameter that tracks call depth, and then selecting a specialization point in every call loop where it is unanchored. By generalizing all unbounded parameters, we compute a binding-time division which together with the set of specialization points guarantees termination.Contributions of this article include a proof, based on the operational semantics of partial evaluation with memoization, that the analysis guarantees termination; and an in-depth description of safety of the increasing size approximation operator required for termination analysis in partial evaluation.Initial experiments with a prototype shows that the analysis overall yields binding-time divisions that can achieve a high degree of specialization, while still guaranteeing termination.The article ends with a list of challenging problems whose solution would bring the community closer to the goal of broad-spectrum, fully automatic and reliable program generation.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2018231677",
    "type": "article"
  },
  {
    "title": "Control predicates are better than dummy variables for reasoning about program control",
    "doi": "https://doi.org/10.1145/42190.42348",
    "publication_date": "1988-04-01",
    "publication_year": 1988,
    "authors": "Leslie Lamport",
    "corresponding_authors": "Leslie Lamport",
    "abstract": "When explicit control predicates rather than dummy variables are used, the Owicki-Gries method for proving safety properties of concurrent programs can be strengthened, making it easier to construct the required program annotations.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2169333969",
    "type": "article"
  },
  {
    "title": "VLSI Layout as Programming",
    "doi": "https://doi.org/10.1145/2166.357216",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Richard J. Lipton; Jacobo Valdes; Gopalakrishnan Vijayan; Stephen C. North; Robert Sedgewick",
    "corresponding_authors": "",
    "abstract": "article Free AccessVLSI Layout as Programming Authors: Richard J. Lipton Department of Electrical Engineering and Computer Science, School of Engineering/Applied Science, Engineering Quadrangle, Princeton University, Princeton, NJ Department of Electrical Engineering and Computer Science, School of Engineering/Applied Science, Engineering Quadrangle, Princeton University, Princeton, NJView Profile , Jacobo Valdes Department of Electrical Engineering and Computer Science, School of Engineering/Applied Science, Engineering Quadrangle, Princeton University, Princeton, NJ Department of Electrical Engineering and Computer Science, School of Engineering/Applied Science, Engineering Quadrangle, Princeton University, Princeton, NJView Profile , Gopalakrishnan Vijayan Department of Electrical Engineering and Computer Science, School of Engineering/Applied Science, Engineering Quadrangle, Princeton University, Princeton, NJ Department of Electrical Engineering and Computer Science, School of Engineering/Applied Science, Engineering Quadrangle, Princeton University, Princeton, NJView Profile , Stephen C. North Department of Electrical Engineering and Computer Science, School of Engineering/Applied Science, Engineering Quadrangle, Princeton University, Princeton, NJ and Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ Department of Electrical Engineering and Computer Science, School of Engineering/Applied Science, Engineering Quadrangle, Princeton University, Princeton, NJ and Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJView Profile , Robert Sedgewick Computer Science Department, Brown University, Providence, RI Computer Science Department, Brown University, Providence, RIView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3July 1983 pp 405–421https://doi.org/10.1145/2166.357216Published:01 July 1983Publication History 20citation378DownloadsMetricsTotal Citations20Total Downloads378Last 12 Months6Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2037310207",
    "type": "article"
  },
  {
    "title": "Offline partial evaluation can be as accurate as online partial evaluation",
    "doi": "https://doi.org/10.1145/963778.963784",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Niels H. Christensen; Robert Glück",
    "corresponding_authors": "",
    "abstract": "We show that the accuracy of online partial evaluation, or polyvariant specialization based on constant propagation, can be simulated by offline partial evaluation using a maximally polyvariant binding-time analysis. We point out that, while their accuracy is the same, online partial evaluation offers better opportunities for powerful generalization strategies. Our results are presented using a flowchart language with recursive procedures.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2046571941",
    "type": "article"
  },
  {
    "title": "Combining Algebraic and Algorithmic Reasoning: An Approach to the Schorr-Waite Algorithm",
    "doi": "https://doi.org/10.1145/357172.357175",
    "publication_date": "1982-07-01",
    "publication_year": 1982,
    "authors": "Manfred Broy; Peter Pepper",
    "corresponding_authors": "",
    "abstract": "article Free AccessCombining Algebraic and Algorithmic Reasoning: An Approach to the Schorr-Waite Algorithm Authors: Manfred Broy Institut für Informatik, Technische Universität München, Postfach 202420, D-8000 München 2, W. Germany and Technical University Munich Institut für Informatik, Technische Universität München, Postfach 202420, D-8000 München 2, W. Germany and Technical University MunichView Profile , Peter Pepper Institut für Informatik, Technische Universität München, Postfach 202420, D-8000 München 2, W. Germany and Technical University Munich Institut für Informatik, Technische Universität München, Postfach 202420, D-8000 München 2, W. Germany and Technical University MunichView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 3July 1982 pp 362–381https://doi.org/10.1145/357172.357175Published:01 July 1982Publication History 18citation349DownloadsMetricsTotal Citations18Total Downloads349Last 12 Months21Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2062275091",
    "type": "article"
  },
  {
    "title": "An exercise in the formal derivation of parallel programs: maximum flows in graphs",
    "doi": "https://doi.org/10.1145/78942.78945",
    "publication_date": "1990-04-01",
    "publication_year": 1990,
    "authors": "Edgar Knapp",
    "corresponding_authors": "Edgar Knapp",
    "abstract": "We apply a new method for the development of parallel programs to the problem of finding maximum flows in graphs. The method facilitates concurrent program design by separating the concerns of correctness from those of hardware and implementation. It uses predicate transformer semantics to define a set of basic operators for the specification and verification of programs. From an initial specification program development proceeds by a series of refinement steps, each of which constitutes a strengthening of the specification of the previous refinement. The method is completely formal in the sense that all reasoning steps are performed within predicate calculus. A program is viewed as a mathematical object enjoying certain properties, rather than in terms of its possible executions. We demonstrate the usefulness of the approach by deriving an efficient algorithm for the Maximum Flow Problem in a top-down manner.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2107372211",
    "type": "article"
  },
  {
    "title": "Locally Least-Cost Error Recovery in Earley's Algorithm",
    "doi": "https://doi.org/10.1145/357139.357145",
    "publication_date": "1981-07-01",
    "publication_year": 1981,
    "authors": "Stuart Anderson; Roland Backhouse",
    "corresponding_authors": "",
    "abstract": "article Free AccessLocally Least-Cost Error Recovery in Earley's Algorithm Authors: S. O. Anderson Department of Computer Science, Heriot-Watt University, 79 Grassmarket, Edinburgh EH1 2HJ, Scotland Department of Computer Science, Heriot-Watt University, 79 Grassmarket, Edinburgh EH1 2HJ, ScotlandView Profile , R. C. Backhouse Department of Computer Science, Heriot-Watt University, 79 Grassmarket, Edinburgh EH1 2HJ, Scotland Department of Computer Science, Heriot-Watt University, 79 Grassmarket, Edinburgh EH1 2HJ, ScotlandView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 3pp 318–347https://doi.org/10.1145/357139.357145Published:01 July 1981Publication History 17citation474DownloadsMetricsTotal Citations17Total Downloads474Last 12 Months20Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1972907726",
    "type": "article"
  },
  {
    "title": "Fast partial evaluation of pattern matching in strings",
    "doi": "https://doi.org/10.1145/1146809.1146812",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Mads Sig Ager; Olivier Danvy; Henning Korsholm Rohde",
    "corresponding_authors": "",
    "abstract": "We show how to obtain all of Knuth, Morris, and Pratt's linear-time string matcher by specializing a quadratic-time string matcher with respect to a pattern string. Although it has been known for fifteen years how to obtain this linear matcher by partial evaluation of a quadratic one, how to obtain it in linear time has remained an open problem.Obtaining a linear matcher by the partial evaluation of a quadratic one is achieved by performing its backtracking at specialization time and memoizing its results. We show (1) how to rewrite the source matcher such that its static intermediate computations can be shared at specialization time and (2) how to extend the memoization capabilities of a partial evaluator to static functions. Such an extended partial evaluator, if its memoization is implemented efficiently, specializes the rewritten source matcher in linear time. Finally, we show that the method also applies to a variant of Boyer and Moore's string matcher.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2115841603",
    "type": "article"
  },
  {
    "title": "Exploiting reference idempotency to reduce speculative storage overflow",
    "doi": "https://doi.org/10.1145/1152649.1152653",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Seon Wook Kim; Chong-liang Ooi; Rudolf Eigenmann; Babak Falsafi; T. N. Vijaykumar",
    "corresponding_authors": "",
    "abstract": "Recent proposals for multithreaded architectures employ speculative execution to allow threads with unknown dependences to execute speculatively in parallel. The architectures use hardware speculative storage to buffer speculative data, track data dependences and correct incorrect executions through roll-backs. Because all memory references access the speculative storage, current proposals implement speculative storage using small memory structures to achieve fast access. The limited capacity of the speculative storage causes considerable performance loss due to speculative storage overflow whenever a thread's speculative state exceeds the speculative storage capacity. Larger threads exacerbate the overflow problem but are preferable to smaller threads, as larger threads uncover more parallelism.In this article, we discover a new program property called memory reference idempotency . Idempotent references are guaranteed to be eventually corrected, though the references may be temporarily incorrect in the process of speculation. Therefore, idempotent references, even from nonparallelizable program sections, need not be tracked in the speculative storage, and instead can directly access nonspeculative storage (i.e., conventional memory hierarchy). Thus, we reduce the demand for speculative storage space in large threads. We define a formal framework for reference idempotency and present a novel compiler-assisted speculative execution model. We prove the necessary and sufficient conditions for reference idempotency using our model. We present a compiler algorithm to label idempotent memory references for the hardware. Experimental results show that for our benchmarks, over 60% of the references in nonparallelizable program sections are idempotent.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2078958381",
    "type": "article"
  },
  {
    "title": "A deterministic logical semantics for pure Esterel",
    "doi": "https://doi.org/10.1145/1216374.1216376",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Olivier Tardieu",
    "corresponding_authors": "Olivier Tardieu",
    "abstract": "Esterel is a synchronous design language for the specification of reactive systems. There exist two main semantics for Esterel. On the one hand, the logical behavioral semantics provides a simple and compact formalization of the behavior of programs using SOS rules. But it does not ensure deterministic deadlock-free executions, as it may define zero, one, or many possible behaviors for a given program and input sequence. Since nondeterministic programs have to be rejected by compilers, this means that it defines behaviors for incorrect programs, which is awkward. On the other hand, the constructive semantics is deterministic (amongst other properties) but at the expense of a much more complex formalism. In this work, we build and thoroughly analyze a new deterministic semantics for Esterel that retains the simplicity of the logical behavioral semantics from which it derives. It defines, at most, one behavior per program and input sequence. We further extend this semantics with the ability to deal with errors so that incorrect programs are no longer (negatively) characterized by a lack of behavior, but (positively) by the existence of an incorrect behavior. In our view, this new semantics, with or without explicit errors, provides a better framework for formal and automated reasoning about Esterel programs.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2152010088",
    "type": "article"
  },
  {
    "title": "Static validation of XSL transformations",
    "doi": "https://doi.org/10.1145/1255450.1255454",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Anders Møller; Mads Østerby Olesen; Michael I. Schwartzbach",
    "corresponding_authors": "",
    "abstract": "XSL Transformations (XSLT) is a programming language for defining transformations among XML languages. The structure of these languages is formally described by schemas, for example using DTD or XML Schema, which allows individual documents to be validated. However, existing XSLT tools offer no static guarantees that, under the assumption that the input is valid relative to the input schema, the output of the transformation is valid relative to the output schema. We present a validation technique for XSLT based on the XML graph formalism introduced in the static analysis of JWIG Web services and X ACT XML transformations. Being able to provide static guarantees, we can detect a large class of errors in an XSLT stylesheet at the time it is written instead of later when it has been deployed, and thereby provide benefits similar to those of static type checkers for modern programming languages. Our analysis takes a pragmatic approach that focuses its precision on the essential language features but still handles the entire XSLT language. We evaluate the analysis precision on a range of real stylesheets and demonstrate how it may be useful in practice.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1972036697",
    "type": "article"
  },
  {
    "title": "Verifying policy-based web services security",
    "doi": "https://doi.org/10.1145/1391956.1391957",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Karthikeyan Bhargavan; Cédric Fournet; Andrew D. Gordon",
    "corresponding_authors": "",
    "abstract": "WS-SecurityPolicy is a declarative language for configuring web services security mechanisms. We describe a formal semantics for WS-SecurityPolicy and propose a more abstract language for specifying secure links between web services and their clients. We present the architecture and implementation of tools that (1) compile policy files from link specifications, and (2) verify by invoking a theorem prover whether a set of policy files run by any number of senders and receivers correctly implements the goals of a link specification, in spite of active attackers. Policy-driven web services implementations are prone to the usual subtle vulnerabilities associated with cryptographic protocols; our tools help prevent such vulnerabilities. We can verify policies when first compiled from link specifications, and also re-verify policies against their original goals after any modifications during deployment. Moreover, we present general security theorems for all configurations that rely on compiled policies.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1992555946",
    "type": "article"
  },
  {
    "title": "Transition predicate abstraction and fair termination",
    "doi": "https://doi.org/10.1145/1232420.1232422",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Andreas Podelski; Andrey Rybalchenko",
    "corresponding_authors": "",
    "abstract": "Predicate abstraction is the basis of many program verification tools. Until now, the only known way to overcome the inherent limitation of predicate abstraction to safety properties was to manually annotate the finite-state abstraction of a program. We extend predicate abstraction to transition predicate abstraction. Transition predicate abstraction goes beyond the idea of finite abstract-state programs (and checking the absence of loops). Instead, our abstraction algorithm transforms a program into a finite abstract-transition program. Then a second algorithm checks fair termination. The two algorithms together yield an automated method for the verification of liveness properties under full fairness assumptions (impartiality, justice, and compassion). In summary, we exhibit principles that extend the applicability of predicate abstraction-based program verification to the full set of temporal properties.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2086026530",
    "type": "article"
  },
  {
    "title": "Erratum to: Efficient constraint propagation engines",
    "doi": "https://doi.org/10.1145/1462166.1462170",
    "publication_date": "2009-02-01",
    "publication_year": 2009,
    "authors": "Christian Schulte; Peter J. Stuckey",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W1984817799",
    "type": "erratum"
  },
  {
    "title": "JavaGI",
    "doi": "https://doi.org/10.1145/1985342.1985343",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Stefan Wehr; Peter Thiemann",
    "corresponding_authors": "",
    "abstract": "The language JavaGI extends Java 1.5 conservatively by a generalized interface mechanism. The generalization subsumes retroactive and type-conditional interface implementations, binary methods, symmetric multiple dispatch, interfaces over families of types, and static interface methods. These features make certain coding patterns redundant, increase the expressiveness of the type system, and permit solutions to extension and integration problems with components in binary form, for which previously several unrelated extensions had been suggested. This article explains JavaGI and motivates its design. Moreover, it formalizes a core calculus for JavaGI and proves type soundness, decidability of typechecking, and determinacy of evaluation. The article also presents the implementation of a JavaGI compiler and an accompanying run-time system. The compiler, based on the Eclipse Compiler for Java, offers mostly modular static typechecking and fully modular code generation. It defers certain well-formedness checks until load time to increase flexibility and to enable full support for dynamic loading. Benchmarks show that the code generated by the compiler offers good performance. Several case studies demonstrate the practical utility of the language and its implementation.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2033843600",
    "type": "article"
  },
  {
    "title": "Reasoning about Web Applications",
    "doi": "https://doi.org/10.1145/2220365.2220369",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Gérard Boudol; Zhengqin Luo; Tamara Rezk; Manuel Serrano",
    "corresponding_authors": "",
    "abstract": "We propose a small-step operational semantics to support reasoning about Web applications written in the multitier language HOP. The semantics covers both server side and client side computations, as well as their interactions, and includes creation of Web services, distributed client-server communications, concurrent evaluation of service requests at server side, elaboration of HTML documents, DOM operations, evaluation of script nodes in HTML documents and actions from HTML pages at client side. We also model the browser same origin policy (SOP) in the semantics. We propose a safety property by which programs do not get stuck due to a violation of the SOP and a type system to enforce it.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2048152615",
    "type": "article"
  },
  {
    "title": "Separating ownership topology and encapsulation with generic universe types",
    "doi": "https://doi.org/10.1145/2049706.2049709",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Werner Dietl; Sophia Drossopoulou; Péter Müller",
    "corresponding_authors": "",
    "abstract": "Ownership is a powerful concept to structure the object store and to control aliasing and modifications of objects. This article presents an ownership type system for a Java-like programming language with generic types. Like our earlier Universe type system, Generic Universe Types structure the heap hierarchically. In contrast to earlier work, we separate the enforcement of an ownership topology from an encapsulation system. The topological system uses an existential modifier to express that no ownership information is available statically. On top of the topological system, we build an encapsulation system that enforces the owner-as-modifier discipline. This discipline does not restrict aliasing, but requires modifications of an object to be initiated by its owner. This allows owner objects to control state changes of owned objects—for instance, to maintain invariants. Separating the topological system from the encapsulation system allows for a cleaner formalization, separation of concerns, and simpler reuse of the individual systems in different contexts.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2068984860",
    "type": "article"
  },
  {
    "title": "Constraint-Based Refactoring",
    "doi": "https://doi.org/10.1145/3156016",
    "publication_date": "2018-01-03",
    "publication_year": 2018,
    "authors": "Friedrich Steimann",
    "corresponding_authors": "Friedrich Steimann",
    "abstract": "Constraint-based refactoring generalizes constraint-based type refactoring as introduced by Tip et al. [61] by extending the coverage of change from types to names, locations, accessibilities, and other properties of program elements. Starting with a generic specification of refactoring tools, we systematically develop constraint-based refactoring as a generic solution to a certain class of refactoring problems and provide a condition under which constraint-based refactoring tools are proven correct for any given target language. Although compliance with this correctness condition is hard to prove for target languages whose semantics is not formally defined, we show how the condition gives rise to automated testing procedures. We present a novel algorithm based on constraint-logic programming for the generation of constraints from a program to be refactored, and demonstrate its time and space requirements by using it in the application of refactorings to open source programs. Summarizing earlier work, we show how the principles underlying constraint-based refactoring tools extend to ad hoc refactoring, cross-language refactoring, and model/code co-refactoring.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2781881785",
    "type": "article"
  },
  {
    "title": "Failure Recovery in Resilient X10",
    "doi": "https://doi.org/10.1145/3332372",
    "publication_date": "2019-07-02",
    "publication_year": 2019,
    "authors": "David Grove; Sara S. Hamouda; Benjamin Herta; Arun Iyengar; Kiyokuni Kawachiya; Josh Milthorpe; Vijay Saraswat; Avraham Shinnar; Mikio Takeuchi; Olivier Tardieu",
    "corresponding_authors": "",
    "abstract": "Cloud computing has made the resources needed to execute large-scale in-memory distributed computations widely available. Specialized programming models, e.g., MapReduce, have emerged to offer transparent fault tolerance and fault recovery for specific computational patterns, but they sacrifice generality. In contrast, the Resilient X10 programming language adds failure containment and failure awareness to a general purpose, distributed programming language. A Resilient X10 application spans over a number of places. Its formal semantics precisely specify how it continues executing after a place failure. Thanks to failure awareness, the X10 programmer can in principle build redundancy into an application to recover from failures. In practice, however, correctness is elusive, as redundancy and recovery are often complex programming tasks. This article further develops Resilient X10 to shift the focus from failure awareness to failure recovery, from both a theoretical and a practical standpoint. We rigorously define the distinction between recoverable and catastrophic failures. We revisit the happens-before invariance principle and its implementation. We shift most of the burden of redundancy and recovery from the programmer to the runtime system and standard library. We make it easy to protect critical data from failure using resilient stores and harness elasticity—dynamic place creation—to persist not just the data but also its spatial distribution. We demonstrate the flexibility and practical usefulness of Resilient X10 by building several representative high-performance in-memory parallel application kernels and frameworks. These codes are 10× to 25× larger than previous Resilient X10 benchmarks. For each application kernel, the average runtime overhead of resiliency is less than 7%. By comparing application kernels written in the Resilient X10 and Spark programming models, we demonstrate that Resilient X10’s more general programming model can enable significantly better application performance for resilient in-memory distributed computations.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2953520583",
    "type": "article"
  },
  {
    "title": "Proof-Directed Parallelization Synthesis by Separation Logic",
    "doi": "https://doi.org/10.1145/2491522.2491525",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Matko Botinčan; Mike Dodds; Suresh Jagannathan",
    "corresponding_authors": "",
    "abstract": "We present an analysis which takes as its input a sequential program, augmented with annotations indicating potential parallelization opportunities, and a sequential proof, written in separation logic, and produces a correctly synchronized parallelized program and proof of that program. Unlike previous work, ours is not a simple independence analysis that admits parallelization only when threads do not interfere; rather, we insert synchronization to preserve dependencies in the sequential program that might be violated by a naïve translation. Separation logic allows us to parallelize fine-grained patterns of resource usage, moving beyond straightforward points-to analysis. The sequential proof need only represent shape properties, meaning we can handle complex algorithms without verifying every aspect of their behavior. Our analysis works by using the sequential proof to discover dependencies between different parts of the program. It leverages these discovered dependencies to guide the insertion of synchronization primitives into the parallelized program, and to ensure that the resulting parallelized program satisfies the same specification as the original sequential program, and exhibits the same sequential behavior. Our analysis is built using frame inference and abduction, two techniques supported by an increasing number of separation logic tools.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2074144668",
    "type": "article"
  },
  {
    "title": "MCALIB",
    "doi": "https://doi.org/10.1145/2665073",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "Michael Frechtling; Philip H. W. Leong",
    "corresponding_authors": "",
    "abstract": "Runtime analysis provides an effective method for measuring the sensitivity of programs to rounding errors. To date, implementations have required significant changes to source code, detracting from their widespread application. In this work, we present an open source system that automates the quantitative analysis of floating point rounding errors through the use of C-based source-to-source compilation and a Monte Carlo arithmetic library. We demonstrate its application to the comparison of algorithms, detection of catastrophic cancellation, and determination of whether single precision floating point provides sufficient accuracy for a given application. Methods for obtaining quantifiable measurements of sensitivity to rounding error are also detailed.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2020213615",
    "type": "article"
  },
  {
    "title": "Specialization Slicing",
    "doi": "https://doi.org/10.1145/2566620",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Min Aung; Susan Horwitz; Rich Joiner; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "This paper defines a new variant of program slicing, called specialization slicing , and presents an algorithm for the specialization-slicing problem that creates an optimal output slice. An algorithm for specialization slicing is polyvariant : for a given procedure р, the algorithm may create multiple specialized copies of р. In creating specialized procedures, the algorithm must decide for which patterns of formal parameters a given procedure should be specialized and which program elements should be included in each specialized procedure. We formalize the specialization-slicing problem as a partitioning problem on the elements of the possibly infinite unrolled program. To manipulate possibly infinite sets of program elements, the algorithm makes use of automata-theoretic techniques originally developed in the model-checking community. The algorithm returns a finite answer that is optimal (with respect to a criterion defined in the article). In particular, (i) each element replicated by the specialization-slicing algorithm provides information about specialized patterns of program behavior that are intrinsic to the program, and (ii) the answer is of minimal size (i.e., among all possible answers with property (i), there is no smaller one). The specialization-slicing algorithm provides a new way to create executable slices. Moreover, by combining specialization slicing with forward slicing, we obtain a method for removing unwanted features from a program. While it was previously known how to solve the feature-removal problem for single-procedure programs, it was not known how to solve it for programs with procedure calls.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2293040969",
    "type": "article"
  },
  {
    "title": "An Extended Account of Trace-relating Compiler Correctness and Secure Compilation",
    "doi": "https://doi.org/10.1145/3460860",
    "publication_date": "2021-11-10",
    "publication_year": 2021,
    "authors": "Carmine Abate; Roberto Blanco; Ştefan Ciobâcă; Adrien Durier; Deepak Garg; Cătălin Hriţcu; Marco Patrignani; Éric Tanter; Jérémy Thibault",
    "corresponding_authors": "",
    "abstract": "Compiler correctness, in its simplest form, is defined as the inclusion of the set of traces of the compiled program in the set of traces of the original program. This is equivalent to the preservation of all trace properties. Here, traces collect, for instance, the externally observable events of each execution. However, this definition requires the set of traces of the source and target languages to be the same, which is not the case when the languages are far apart or when observations are fine-grained. To overcome this issue, we study a generalized compiler correctness definition, which uses source and target traces drawn from potentially different sets and connected by an arbitrary relation. We set out to understand what guarantees this generalized compiler correctness definition gives us when instantiated with a non-trivial relation on traces. When this trace relation is not equality, it is no longer possible to preserve the trace properties of the source program unchanged. Instead, we provide a generic characterization of the target trace property ensured by correctly compiling a program that satisfies a given source property, and dually, of the source trace property one is required to show to obtain a certain target property for the compiled code. We show that this view on compiler correctness can naturally account for undefined behavior, resource exhaustion, different source and target values, side channels, and various abstraction mismatches. Finally, we show that the same generalization also applies to many definitions of secure compilation, which characterize the protection of a compiled program linked against adversarial code.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3212177851",
    "type": "article"
  },
  {
    "title": "Bounded Verification of Multi-threaded Programs via Lazy Sequentialization",
    "doi": "https://doi.org/10.1145/3478536",
    "publication_date": "2021-12-09",
    "publication_year": 2021,
    "authors": "Omar Inverso; Ermenegildo Tomasco; Bernd Fischer; Salvatore La Torre; Gennaro Parlato",
    "corresponding_authors": "",
    "abstract": "Bounded verification techniques such as bounded model checking (BMC) have successfully been used for many practical program analysis problems, but concurrency still poses a challenge. Here, we describe a new approach to BMC of sequentially consistent imperative programs that use POSIX threads. We first translate the multi-threaded program into a nondeterministic sequential program that preserves reachability for all round-robin schedules with a given bound on the number of rounds. We then reuse existing high-performance BMC tools as backends for the sequential verification problem. Our translation is carefully designed to introduce very small memory overheads and very few sources of nondeterminism, so it produces tight SAT/SMT formulae, and is thus very effective in practice: Our Lazy-CSeq tool implementing this translation for the C programming language won several gold and silver medals in the concurrency category of the Software Verification Competitions (SV-COMP) 2014–2021 and was able to find errors in programs where all other techniques (including testing) failed. In this article, we give a detailed description of our translation and prove its correctness, sketch its implementation using the CSeq framework, and report on a detailed evaluation and comparison of our approach.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4200340358",
    "type": "article"
  },
  {
    "title": "Revisiting Iso-Recursive Subtyping",
    "doi": "https://doi.org/10.1145/3549537",
    "publication_date": "2022-07-25",
    "publication_year": 2022,
    "authors": "Yaoda Zhou; Jinxu Zhao; Bruno C. d. S. Oliveira",
    "corresponding_authors": "",
    "abstract": "The Amber rules are well-known and widely used for subtyping iso-recursive types. They were first briefly and informally introduced in 1985 by Cardelli in a manuscript describing the Amber language. Despite their use over many years, important aspects of the metatheory of the iso-recursive style Amber rules have not been studied in depth or turn out to be quite challenging to formalize. This article aims to revisit the problem of subtyping iso-recursive types. We start by introducing a novel declarative specification for Amber-style iso-recursive subtyping. Informally, the specification states that two recursive types are subtypes if all their finite unfoldings are subtypes . The Amber rules are shown to have equivalent expressive power to this declarative specification. We then show two variants of sound , complete and decidable algorithmic formulations of subtyping with respect to the declarative specification, which employ the idea of double unfoldings . Compared to the Amber rules, the double unfolding rules have the advantage of: (1) being modular; (2) not requiring reflexivity to be built in; (3) leading to an easy proof of transitivity of subtyping; and (4) being easily applicable to subtyping relations that are not antisymmetric (such as subtyping relations with record types). This work sheds new insights on the theory of subtyping iso-recursive types, and the new rules based on double unfoldings have important advantages over the original Amber rules involving recursive types. All results are mechanically formalized in the Coq theorem prover.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4287832609",
    "type": "article"
  },
  {
    "title": "Generic Bidirectional Typing for Dependent Type Theories",
    "doi": "https://doi.org/10.1145/3715095",
    "publication_date": "2025-02-03",
    "publication_year": 2025,
    "authors": "Thiago Felicissimo",
    "corresponding_authors": "Thiago Felicissimo",
    "abstract": "Bidirectional typing is a discipline in which the typing judgment is decomposed explicitly into inference and checking modes, allowing one to control the flow of type information in typing rules and to specify algorithmically how they should be used. Bidirectional typing has been fruitfully studied and bidirectional systems have been developed for many type theories. However, the formal development of bidirectional typing has until now been kept confined to specific theories, with general guidelines remaining informal. In this work, we give a generic account of bidirectional typing for a general class of dependent type theories. This is done by first giving a general definition of bidirectional type theories, each giving rise to both a declarative and a bidirectional type system. We then show, in a theory-independent fashion, that the two systems are equivalent. Finally, we establish the decidability of bidirectional typing for normalizing theories, yielding a generic typing algorithm that has been implemented in a prototype and used in practice with many theories.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407103011",
    "type": "article"
  },
  {
    "title": "Modal Crash Types for WAR-Aware Intermittent Computing",
    "doi": "https://doi.org/10.1145/3716311",
    "publication_date": "2025-02-04",
    "publication_year": 2025,
    "authors": "Myra Dotzel; Farzaneh Derakhshan; Milijana Surbatovich; Limin Jia",
    "corresponding_authors": "",
    "abstract": "Programs are executed intermittently on devices that experience arbitrary power failures such as Energy Harvesting Devices (EHDs). To ensure progress, intermittent systems need runtime support to checkpoint state and re-execute after power failure by restoring the last saved state. Such re-execution should be correct , i.e., simulated by a continuously-powered execution. We study the logical underpinning of intermittent computing and model checkpoint, crash, restore, and re-execution operations as computation on crash types. We draw inspiration from adjoint logic and define crash types by introducing two adjoint modality operators to model persistent and transient memory values of partial (re-)executions and the transitions between them caused by checkpoints and restoration. Our formalism is general enough to accommodate a variety of checkpointing policies. We define a crash type system for a core calculus. To prove the correctness of intermittent systems, we define a novel logical relation for crash types.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407135100",
    "type": "article"
  },
  {
    "title": "Polynomial Bounds of CFLOBDDs against BDDs",
    "doi": "https://doi.org/10.1145/3716313",
    "publication_date": "2025-02-04",
    "publication_year": 2025,
    "authors": "Xusheng Zhi; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "Binary Decision Diagrams (BDDs) are widely used for the representation of Boolean functions. Context-Free-Language Ordered Decision Diagrams (CFLOBDDs) are a plug-compatible replacement for BDDs—roughly, they are BDDs augmented with a certain form of procedure call. A natural question to ask is, “For a given family of Boolean functions \\(F\\) , what is the relationship between the size of a BDD for \\(f\\in F\\) and the size of a CFLOBDD for \\(f\\) ?” Sistla et al. established that there are best-case families of functions , which demonstrate an inherently exponential separation between CFLOBDDs and BDDs. They showed that there are families of functions \\(\\{f_{n}\\}\\) for which, for all \\(n=2^{k}\\) , the CFLOBDD for \\(f_{n}\\) (using a particular variable order) is exponentially more succinct than any BDD for \\(f_{n}\\) (i.e., using any variable order). However, they did not give a worst-case bound —i.e., they left open the question, “Is there a family of functions \\(\\{g_{i}\\}\\) for which the size of a CFLOBDD for \\(g_{i}\\) must be substantially larger than a BDD for \\(g_{i}\\) ?” For instance, it could be that there is a family of functions for which the BDDs are exponentially more succinct than any corresponding CFLOBDDs. This paper studies such questions, and answers the second question posed above in the negative. In particular, we show that by using the same variable ordering in the CFLOBDD that is used in the BDD, the size of a CFLOBDD for any function \\(h\\) cannot be far worse than the size of the BDD for \\(h\\) . The bound that relates their sizes is polynomial: If BDD \\(B\\) for function \\(h\\) is of size \\(|B|\\) and uses variable ordering Ord, then the size of the CFLOBDD \\(C\\) for \\(h\\) that also uses Ord is bounded by \\({\\mathcal{O}}(|B|^{3})\\) . The paper also shows that the bound is tight: there is a family of functions for which \\(|C|\\) grows as \\(\\Omega(|B|^{3})\\) .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407135127",
    "type": "article"
  },
  {
    "title": "Will it Fit? Verifying Heap Space Bounds of Concurrent Programs under Garbage Collection",
    "doi": "https://doi.org/10.1145/3716312",
    "publication_date": "2025-02-10",
    "publication_year": 2025,
    "authors": "Alexandre Moine; Arthur Charguéraud; François Pottier",
    "corresponding_authors": "",
    "abstract": "We present IrisFit, a Separation Logic with space credits for reasoning about heap space in a concurrent call-by-value language equipped with tracing garbage collection and shared mutable state. We point out a fundamental difficulty in the analysis of the worst-case heap space complexity of concurrent programs in the presence of tracing garbage collection: if garbage collection phases and program steps can be arbitrarily interleaved, then there exist undesirable scenarios where a root held by a sleeping thread prevents a possibly large amount of memory from being freed. To remedy this problem and eliminate such undesirable scenarios, we propose several language features, namely possibly-blocking memory allocation, polling points, and protected sections. Polling points are meant to be automatically inserted by the compiler; protected sections are delimited by the programmer and represent regions where no polling points must be inserted. The heart of our contribution is IrisFit, a novel program logic that can establish worst-case heap space complexity bounds and whose reasoning rules can take advantage of the presence of protected sections. IrisFit is formalized inside the Coq proof assistant, on top of the Iris Separation Logic framework. We prove that IrisFit offers both a safety guarantee—programs cannot crash and cannot exceed a heap space limit—and a liveness guarantee—provided enough polling points have been inserted, every memory allocation request is satisfied in bounded time. We illustrate the use of IrisFit via several case studies, including a version of Treiber's stack whose worst-case behavior relies on the presence of protected sections.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407317977",
    "type": "article"
  },
  {
    "title": "Sound Static Data Race Verification for C: Is the Race Lost?",
    "doi": "https://doi.org/10.1145/3732933",
    "publication_date": "2025-04-29",
    "publication_year": 2025,
    "authors": "Karoliine Holter; Simmo Saan; Patrick Lam; Vesal Vojdani",
    "corresponding_authors": "",
    "abstract": "Sound static data race freedom verification has been a long-standing challenge in the field of programming languages. While actively researched a decade ago, most practical data race detection tools have since abandoned soundness. Is sound static race freedom verification for real-world C programs a lost cause? In this work, we investigate the obstacles to making significant progress in automated race freedom verification. We selected a benchmark suite of real-world programs and, as our primary contribution, extracted a set of coding idioms that represent fundamental barriers to verification. We expressed these idioms as micro-benchmarks and contributed them as evaluation tasks for the International Competition on Software Verification, SV-COMP. To understand the current state, we measure how sound automated verification tools competing in SV-COMP perform on these idioms and also when used out of the box on the real-world programs. For 8 of the 20 coding idioms, there does exist an automated race freedom verifier that can verify it; however, we also found significant unsoundness in leading verifiers, including Goblint and Deagle. Five of the 7 tools failed to return any result on any real-world benchmarks under our chosen resource limitations, with the remaining 2 tools verifying race freedom for 2 of the 18 programs and crashing or returning inconclusive results on the others. We thus show that state-of-the-art verifiers have both superficial and fundamental barriers to correctly analyzing real-world programs. These barriers constitute the open problems that must be solved to make progress on automated static data race freedom verification.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409920357",
    "type": "article"
  },
  {
    "title": "Scoped effects, scoped operations, and parameterized algebraic theories",
    "doi": "https://doi.org/10.1145/3731678",
    "publication_date": "2025-05-05",
    "publication_year": 2025,
    "authors": "Cristina Matache; Sam Lindley; Sean Moss; Sam Staton; Nicolas Wu; Zhixuan Yang",
    "corresponding_authors": "",
    "abstract": "Notions of computation can be modelled by monads. Algebraic effects offer a characterization of monads in terms of algebraic operations and equational axioms, where operations are basic programming features, such as reading or updating the state, and axioms specify observably equivalent expressions. However, many useful programming features depend on additional mechanisms such as delimited scopes or dynamically allocated resources. Such mechanisms can be supported via extensions to algebraic effects including scoped effects and parameterized algebraic theories . We present a fresh perspective on scoped effects by translation into a variation of parameterized algebraic theories. The translation enables a new approach to equational reasoning for scoped effects and gives rise to an alternative characterization of monads in terms of generators and equations involving both scoped and algebraic operations. We demonstrate the power of our approach by way of equational characterizations of several known models of scoped effects.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410089372",
    "type": "article"
  },
  {
    "title": "A Modular Approach to Metatheoretic Reasoning for Extensible Languages",
    "doi": "https://doi.org/10.1145/3731555",
    "publication_date": "2025-05-07",
    "publication_year": 2025,
    "authors": "Dawn Michaelson; Gopalan Nadathur; Eric Van Wyk",
    "corresponding_authors": "",
    "abstract": "This paper concerns the development of metatheory for extensible languages. It starts with the view that programming languages tailored to specific application domains are to be constructed by composing components from an open library of independently-developed extensions to a host language. In this context, static analyses (such as typing) and dynamic semantics (such as evaluation) are described via relations whose specifications are distributed across the host language and extensions and are given in a rule-based fashion. Metatheoretic properties, which ensure that static analyses accurately gauge runtime behavior, are represented by formulas over such relations. These properties may be fundamental to the language or they may pertain to analyses introduced by individual extensions. We consider the problem of modular metatheory , by which we mean that proofs of relevant properties should be constructible by reasoning independently within each component in the library. To solve this problem, we propose the twin ideas of decomposing proofs around language fragments and of reasoning generically about extensions based on broad, a priori constraints imposed on their behavior. We establish the soundness of these styles of reasoning by showing how complete proofs of the properties can be automatically constructed for any language obtained by composing the independent parts. Precision in these arguments results from framing them within a logic that encodes inductive, rule-based specifications via least fixed-point definitions. We have implemented our ideas in a language specification system called Sterling and a proof assistant called Extensibella and have used them to validate the examples that motivate the theoretical discussions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410155745",
    "type": "article"
  },
  {
    "title": "Trocq: Proof Transfer for Free, Beyond Equivalence and Univalence",
    "doi": "https://doi.org/10.1145/3737283",
    "publication_date": "2025-05-26",
    "publication_year": 2025,
    "authors": "Cyril Cohen; Enzo Crance; Assia Mahboubi",
    "corresponding_authors": "",
    "abstract": "This article presents Trocq, a new proof transfer framework for dependent type theory. Trocq is based on a novel formulation of type equivalence, used to generalize the univalent parametricity translation. This framework takes care of avoiding dependency on the axiom of univalence when possible, and may be used with more relations than just equivalences. We have implemented a corresponding plugin for the Rocq/Coq interactive theorem prover, in the Coq-Elpi meta-language.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410734372",
    "type": "article"
  },
  {
    "title": "Rate-Based Session Types for IoT Devices",
    "doi": "https://doi.org/10.1145/3754449",
    "publication_date": "2025-07-24",
    "publication_year": 2025,
    "authors": "Grant Iraci; Cheng-En Chuang; Raymond Hu; Lukasz Ziarek",
    "corresponding_authors": "",
    "abstract": "We develop a session types framework for implementing and validating rate-based message passing systems in Internet of Things (IoT) domains. To model the indefinite repetition present in many embedded and IoT systems, we introduce a timed process calculus with a periodic recursion primitive. This allows us to model rate-based computations and communications inherent to these application domains. We introduce a definition of rate based session types in a binary session types setting and a new compatibility relationship, which we call rate compatibility . Programs which type check enjoy the standard session types guarantees as well as rate error freedom — meaning processes which exchanges messages do so at the same rate . Rate compatibility is defined through a new notion of type expansion, a relation that allows communication between processes of differing periods by synthesizing and checking a common superperiod type. We prove type preservation and rate error freedom for our system, and show a decidable method for type checking based on computing superperiods for a collection of processes. We implement a prototype of our type system including rate compatibility via an embedding into the native type system of Rust. We apply this framework to a range of examples from our target domain such as Android software sensors, wearable devices, and sound processing. Our framework is used to implement a heart rate sensor application that runs on a commercially available smartwatch.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412627771",
    "type": "article"
  },
  {
    "title": "Changing of the Seasons at TOPLAS",
    "doi": "https://doi.org/10.1145/3768632",
    "publication_date": "2025-09-18",
    "publication_year": 2025,
    "authors": "Colin S. Gordon",
    "corresponding_authors": "Colin S. Gordon",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414321081",
    "type": "article"
  },
  {
    "title": "Type-Safe Compilation of Dynamic Inheritance via Merging",
    "doi": "https://doi.org/10.1145/3765518",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Yaozhu Sun; Xuejing Huang; Bruno C. d. S. Oliveira",
    "corresponding_authors": "",
    "abstract": "Inheritance is a key concept in many programming languages. Dynamically typed languages, such as JavaScript, often support powerful forms of dynamic inheritance. However, dynamic inheritance poses significant challenges for static typing. Most statically typed languages only provide static inheritance to achieve type safety at the cost of flexibility. This paper presents a compiler for the CP language, which is a statically typed language that supports dynamic inheritance via a merge operator and also has an expressive form of parametric polymorphism. The merge operator enables a form of multiple inheritance and first-class classes, as well as virtual classes and family polymorphism. With these features, CP allows the development of highly modular and loosely coupled components. However, the efficient compilation of CP code is non-trivial, especially if separate compilation is desired. In particular, subtyping in CP is coercive for type safety, which poses significant challenges in obtaining an efficient compilation scheme. We show how CP is compilable to languages supporting extensible records or similar data structures, where record labels are generated from types for efficient lookup on merges. The main ideas of the compilation scheme are formalized in Coq and proven to be type-safe. The concrete implementation of the CP compiler targets JavaScript, where records are modeled as JavaScript objects. We conduct an empirical evaluation with various benchmarks and evaluate the impact of several CP-specific optimizations. With our optimizations, CP can be orders of magnitude faster than with a naive compilation scheme for merges, obtaining performance on par with class-based JavaScript programs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414787048",
    "type": "article"
  },
  {
    "title": "Incremental computation of dominator trees",
    "doi": "https://doi.org/10.1145/244795.244799",
    "publication_date": "1997-03-03",
    "publication_year": 1997,
    "authors": "Vugranam C. Sreedhar; Guang R. Gao; Yong-Fong Lee",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Incremental computation of dominator trees Authors: Vugranam C. Sreedhar Hewlett-Packard Company, Cupertino, CA Hewlett-Packard Company, Cupertino, CAView Profile , Guang R. Gao McGill Univ., Montre´al, P.Q., Canada McGill Univ., Montre´al, P.Q., CanadaView Profile , Yong-Fong Lee Intel Corporation, Santa Clara, CA Intel Corporation, Santa Clara, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 19Issue 2March 1997 pp 239–252https://doi.org/10.1145/244795.244799Published:03 March 1997Publication History 14citation600DownloadsMetricsTotal Citations14Total Downloads600Last 12 Months40Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2046567643",
    "type": "article"
  },
  {
    "title": "Parameter passing and control stack management in Prolog implementation revisited",
    "doi": "https://doi.org/10.1145/236114.236120",
    "publication_date": "1996-11-01",
    "publication_year": 1996,
    "authors": "Neng‐Fa Zhou",
    "corresponding_authors": "Neng‐Fa Zhou",
    "abstract": "Parameter passing and control stack management are two of the crucial issues in Prolog implementation. In the Warren Abstract Machine (WAM), the most widely used abstract machine for Prolog implementation, arguments are passed through argument registers, and the information associated with procedure calls is stored in possibly two frames. Although accessing registers is faster than accessing memory, this scheme requires the argument registers to be saved and restored for back tracking and makes it difficult to implement full tail recursion elimination. These disadvantages may far outweigh the advantage in emulator-based implementations because registers are actually simulated by using memory. In this article, we reconsider the two crucial issues and describe a new abstract machine called ATOAM (yet Another Tree-Oriented Abstract Machine). The ATOAM differs from the WAM mainly in that (1) arguments are passed directly into stack frames, (2) only one frame is used for each procedure call, and (3) procedures are translated into matching trees is possible, and clauses in each procedure are indexed on all input arguments. The above-mentioned inefficiencies of the WAM do not exist in to he ATOAM because backtracking requires less bookkeeping operations, and tail recursion can be handled in most cases like a loop statement in procedural languages. An ATOAM-emulator-based Prolog system called B-Prolog has been implemented, which is available through anonymous ftp from ftp.kyutech.ac.jp (131.206.1.101) in the directory pub/Language/prolog .B-Prolog is comparable in performance with and can sometimes be significatly faster than emulated SICStus-Prolog. By measuring the numbers of memory and register references made in both systems, we found that passing arguments in stack is no worse than passing arguments in registers even if accessing memory is four times as expensive as accessing registers.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1983792792",
    "type": "article"
  },
  {
    "title": "A formal definition of priority in CSP",
    "doi": "https://doi.org/10.1145/155183.155221",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Colin Fidge",
    "corresponding_authors": "Colin Fidge",
    "abstract": "The process models of Ada and occam are formally based on the CSP process algebra. However, for fine-tuning real-time performance, they include “prioritized” constructs that have no counterparts in CSP. These constructs therefore lack any formal definition, a situation that leaves room for misunderstandings. We extend CSP with a formal definition of the notion of priority. The definition is then used to assess the transputer implementation of priority in occam and the definition of priority in Ada.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2007911006",
    "type": "article"
  },
  {
    "title": "Equality-based flow analysis versus recursive types",
    "doi": "https://doi.org/10.1145/295656.295662",
    "publication_date": "1998-11-01",
    "publication_year": 1998,
    "authors": "Jens Palsberg",
    "corresponding_authors": "Jens Palsberg",
    "abstract": "Equality-based control-flow analysis has been studied by Henglein, Bondorf and Jørgensen, DeFouw, Grove, and Chambers, and others. It is faster than the subset-based-0-CFA, but also more approximate. Heintze asserted in 1995 that a program can be safety checked with an equality-based control-flow analysis if and only if it can be typed with recursive types. In this article we falsify Heintze's assertion, and we present a type system equivalent to equality-based control-flow analysis. The new type system contains both recursive types and an unusual notion of subtyping. We have s ≤ t if s and t unfold to the same regular tree, and we have ⊥≤t≤⊤ where t is a function type. In particular, there is no nontrivial subtyping between function types.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2013172387",
    "type": "article"
  },
  {
    "title": "Transformations of CCP programs",
    "doi": "https://doi.org/10.1145/503502.503504",
    "publication_date": "2001-05-01",
    "publication_year": 2001,
    "authors": "Sandro Etalle; Maurizio Gabbrielli; Maria Chiara Meo",
    "corresponding_authors": "",
    "abstract": "We introduce a transformation system for concurrent constraint programming (CCP). We define suitable applicability conditions for the transformations that guarantee the input/output CCP semantics is also preserved when distinguishing deadlocked computations from successful ones and when considering intermediate results of (possibly) nonterminating computations.The system allows us to optimize CCP programs while preserving their intended meaning: In addition to the usual benefits for sequential declarative languages, the transformation of concurrent programs can also lead to the elimination of communication channels and of synchronization points, to the transformation of nondeterministic computations into deterministic ones, and to the crucial saving of computational space. Furthermore, since the transformation system preserves the deadlock behavior of programs, it can be used for proving deadlock-freeness of a given program with respect to a class of queries. To this aim, it is sometimes sufficient to apply our transformations and to specialize the resulting program with respect to the given queries in such a way that the obtained program is trivially deadlock-free.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2021515159",
    "type": "article"
  },
  {
    "title": "Coordinating first-order multiparty interactions",
    "doi": "https://doi.org/10.1145/177492.177739",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Yuh-Jzer Joung; Scott A. Smolka",
    "corresponding_authors": "",
    "abstract": "A first-order multiparty interaction is an abstraction mechanism that defines communication among a set of formal process roles . Actual processes participate in a first-order interaction by enroling into roles, and execution of the interaction can proceed when all roles are filled by distinct processes. As in CSP, enrolement statements can serve as guards in alternative commands. The enrolement guard-scheduling problem then is to enable the execution of first-order interactions through the judicious scheduling of roles to processes that are currently ready to execute enrolement guards. We present a fully distributed and message-efficient algorithm for the enrolement guard-scheduling problem, the first such solution of which we are aware. We also describe several extensions of the algorithm, including: generic roles; dynamically changing environments , where processes can be created and destroyed at run time; and nested-enrolement , which allows interactions to be nested.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2035293913",
    "type": "article"
  },
  {
    "title": "Denotational semantics of a goal-directed language",
    "doi": "https://doi.org/10.1145/111186.104659",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "David Gudeman",
    "corresponding_authors": "David Gudeman",
    "abstract": "Goal-directed evaluation is a very expressive programming language paradigm that is supported in relatively few languages. It is characterized by evaluation of expressions in an attempt to meet some goal, with resumption of previous expressions on failure. This paradigm is found in SNOBL4 in its pattern-matching facilities, and in Icon as a general part of the language. This paper presents a denotational semantics of Icon and shows how Icon is in fact a combination of two distinct paradigms, goal-directed evaluation and functional application. The two paradigms are not supported separately in different contexts, but integrated fully into a single evaluation mechanism.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2039766682",
    "type": "article"
  },
  {
    "title": "Cost-optimal code motion",
    "doi": "https://doi.org/10.1145/295656.295664",
    "publication_date": "1998-11-01",
    "publication_year": 1998,
    "authors": "Max Hailperin",
    "corresponding_authors": "Max Hailperin",
    "abstract": "We generalize Knoop et al.'s Lazy Code Motion (LCM) algorithm for partial redundancy elimination so that the generalized version also performs strength reduction. Although Knoop et al. have themselves extended LCM to strength reduction with their Lazy Strength Reduction algorithm, our approach differs substantially from theirs and results in a broader class of candidate expressions, stronger safety guarantees, and the elimination of the potential for performance loss instead of gain. Also, our general framework is not limited to traditional strength reduction, but rather can also handle a wide variety of optimizations in which data-flow information enables the replacement of a computation with a less expensive one. As a simple example, computations can be hoisted to points where they are constant foldable. Another example we sketch is the hoisting of polymorphic operations to points where type analysis provides leverage for optimization. Our general approach consists of placing computations so as to minimize their cost, rather than merely their number. So long as the cost differences between flowgraph nodes obey a certain natural constraint, a cost-optimal code motion transformation that does not unnecessarily prolong the lifetime of temporary varibles can be found using techniques completely analogous to LCM. Specifically, the cost differences can be discovered using a wide variety of forward data-flow analyses in a manner which we describe.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2074672347",
    "type": "article"
  },
  {
    "title": "Incremental generation of lexical scanners",
    "doi": "https://doi.org/10.1145/133233.133240",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "Jan Heering; Paul Klint; J. Rekers",
    "corresponding_authors": "",
    "abstract": "It is common practice to specify textual patterns by means of a set of regular expressions and to transform this set into a finite automaton to be used for the scanning of input strings. In many applications, the cost of this preprocessing phase can be amortized over many uses of the constructed automaton. In this paper new techniques for lazy and incremental scanner generation are presented. The lazy technique postpones the construction of parts of the automaton until they are really needed during the scanning of input. The incremental technique allows modifications to the original set of regular expressions to be made and reuses major parts of the previous automaton. This is interesting in applications such as environments for the interactive development of language definitions in which modifications to the definition of lexical syntax and the uses of the generated scanners alternate frequently.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W1990575438",
    "type": "article"
  },
  {
    "title": "Handling irreducible loops",
    "doi": "https://doi.org/10.1145/567097.567098",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Sebastian Unger; Frank Mueller",
    "corresponding_authors": "",
    "abstract": "This paper addresses the question of how to handle irreducible regions during optimization, which has become even more relevant for contemporary processors since recent VLIW-like architectures highly rely on instruction scheduling. The contributions of this paper are twofold. First, a method of optimized node splitting to transform irreducible regions of control flow into reducible regions is formally defined and its correctness is shown. This method is superior to approaches previously published since it reduces the number of replicated nodes by comparison. Second, three methods that handle regions of irreducible control flow are evaluated with respect to their impact on compiler optimizations. First, traditional node splitting is evaluated. Second, optimized node splitting is implemented. Third, DJ-Graphs are utilized to recognize nesting of irreducible (and reducible) loops and apply common loop optimizations extended for irreducible loops. Experiments compare the performance of these approaches with unrecognized irreducible loops that cannot be subject to loop optimizations, which is typical for contemporary compilers. Measurements show improvements of 1 to 40% for these methods of handling irreducible loops over the unoptimized case. Optimized node splitting may be chosen to retrofit existing compilers since it has the advantage that it only requires few changes to an optimizing compiler while limiting the code growth of compiled programs compared to traditional node splitting. Recognizing loops via DJ-Graphs should be chosen for new compiler developments since it requires more changes to the optimizer but does not significantly change the code size of compiled programs while yielding comparable improvements. Handling irreducible loops should even yield more benefits for exploiting instruction-level parallelism of modern architectures in the context of global instruction scheduling and optimization techniques that may introduce irreducible loops, such as enhanced modulo scheduling.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2007427290",
    "type": "article"
  },
  {
    "title": "An object-based programming model for shared data",
    "doi": "https://doi.org/10.1145/128861.128866",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "Gail E. Kaiser; Brent Hailpern",
    "corresponding_authors": "",
    "abstract": "The classical object model supports private data within objects and clean interfaces between objects, and by definition does not permit sharing of data among arbitrary objects. This is a problem for real-world applications, such as advanced financial services and integrated network management, where the same data logically belong to multiple objects and may be distributed over multiple nodes on the network. Rather than give up the advantages of encapsulated objects in modeling real-world entities, we propose a new object model that supports shared data in a distributed environment. The key is separating distribution of computation units from information-hiding concerns. Minimal units of data and control, called facets , may be shared among multiple objects and are grouped into processes . Thus, a single object, or information-hiding unit, may be distributed among multiple processes, or computation units. In other words, different facets of the same object may reside in different address spaces on different machines. We introduce our new object model, describe a motivating example from the financial domain, and then explain facets, objects, and processes, followed by timing and synchronization concerns.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2009412845",
    "type": "article"
  },
  {
    "title": "Adding fair choice to Dijkstra's calculus",
    "doi": "https://doi.org/10.1145/177492.177727",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Manfred Broy; Greg Nelson",
    "corresponding_authors": "",
    "abstract": "The paper studies the incorporation of a fair nondeterministic choice operator into a generalization of Dijkstra's calculus of guarded commands. The generalization drops the law of the excluded miracle to allow commands that correspond to partial relations. Because of fairness, the new operator is not monotonic for the orderings that are generally used for proving the existence of least fixed points for recursive definitions. To prove the existence of fixed points it is necessary to consider several orderings at once, and to restrict the class of recursive definitions.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2040039569",
    "type": "article"
  },
  {
    "title": "A natural semantics for Eiffel dynamic binding",
    "doi": "https://doi.org/10.1145/236114.236118",
    "publication_date": "1996-11-01",
    "publication_year": 1996,
    "authors": "Isabelle Attali; Denis Caromel; Sidi Ould Ehmety",
    "corresponding_authors": "",
    "abstract": "This article formally defines Eiffel dynamic binding in presence of renaming and redefinition. Message passing, inheritance, and polymorphism are expressed in an operational style using natural semantics. From the formal specification, we derive an algorithm to determine the appropriate version of a feature to apply to a given object. This algorithm, based only on the class hierarchy and not using any intermediate structure, gives a practical approach to the understanding of inheritance, renaming, and redefinition in Eiffel.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2094244519",
    "type": "article"
  },
  {
    "title": "The benefits and costs of DyC's run-time optimizations",
    "doi": "https://doi.org/10.1145/365151.367161",
    "publication_date": "2000-09-01",
    "publication_year": 2000,
    "authors": "Brian Grant; Markus Mock; Matthai Philipose; Craig Chambers; Susan J. Eggers",
    "corresponding_authors": "",
    "abstract": "DyC selectively dynamically compiles programs during their execution, utilizing the run-time-computed values of variables and data structures to apply optimizations that are based on partial evaluation. The dynamic optimizations are preplanned at static compile time in order to reduce their run-time cost; we call this staging . DyC's staged optimizations include (1) an advanced binding-time analysis that supports polyvariant specialization (enabling both single-way and multiway complete loop unrolling), polyvariant division, static loads, and static calls, (2) low-cost, dynamic versions of traditional global optimizations, such as zero and copy propagation and dead-assignment elimination, and (3) dynamic peephole optimizations, such as strength reduction. Because of this large suite of optimizations and its low dynamic compilation overhead, DyC achieves good performance improvements on programs that are larger and more complex than the kernels previously targeted by other dynamic compilation systems. This paper evaluates the benefits and costs of applying DyC's optimizations. We assess their impact on the performance of a variety of small to medium-sized programs, both for the regions of code that are actually transformed and for the entire application as a whole. Our study includes an analysis of the contribution to performance of individual optimizations, the performance effect of changing the applications' inputs, and a detailed accounting of dynamic compilation costs.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2155925023",
    "type": "article"
  },
  {
    "title": "Independence in CLP languages",
    "doi": "https://doi.org/10.1145/349214.349224",
    "publication_date": "2000-03-01",
    "publication_year": 2000,
    "authors": "María García de la Banda; Manuel V. Hermenegildo; Kim Marriott",
    "corresponding_authors": "",
    "abstract": "Studying independence of goals has proven very useful in the context of logic programming. In particular, it has provided a formal basis for powerful automatic parallelization tools, since independence ensures that two goals may be evaluated in parallel while preserving correctness and efficiency. We extend the concept of independence to constraint logic programs (CLP) and prove that it also ensures the correctness and efficiency of the parallel evaluation of independent goals. Independence for CLP languages is more complex than for logic programming as search space preservation is necessary but no longer sufficient for ensuring correctness and efficiency. Two additional issues arise. The first is that the cost of constraint solving may depend upon the order constraints are encountered. The second is the need to handle dynamic scheduling. We clarify these issues by proposing various types of search independence and constraint solver independence, and show how they can be combined to allow different optimizations, from parallelism to intelligent backtracking. Suficient conditions for independence which can be evaluated “a priori” at run-time are also proposed. Our study also yields new insights into independence in logic programming languages. In particular, we show that search space preservation is not only a sufficient but also a necessary condition for ensuring correctness and efficiency of parallel execution.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2012571091",
    "type": "article"
  },
  {
    "title": "Efficient incremental LR parsing for syntax-directed editors",
    "doi": "https://doi.org/10.1145/44501.214503",
    "publication_date": "1988-07-01",
    "publication_year": 1988,
    "authors": "Pierpaolo Degano; S. Mannucci; B. Mojana",
    "corresponding_authors": "",
    "abstract": "A technique for generating parsers which is an extension to LR techniques and is based on parsing table splitting, is presented. Then this technique is slightly extended to support incremental syntax analysis. Given a context-free grammar and a set “ IC ” of nonterminals devised to be incremental, a set of subtables is generated to drive the analysis of program fragments derivable from nonterminals in IC . The proposed technique generates parsing tables which are considerably smaller than the standard ones, even when incrementality is not exploited. Thus, these tables may be stored as arrays permitting faster access and accurate error handling. Furthermore, our tables are suitable for generating syntax-directed editors which provide a full analytic mode. The efficiency of the analytic component of a syntax-directed editor obtained in this way and its easy integration with the generative component stress the advantages of incremental program writing.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1975100148",
    "type": "article"
  },
  {
    "title": "A data-driven model for a subset of logic programming",
    "doi": "https://doi.org/10.1145/29873.31333",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Lubomir Bic; Craig A. Lee",
    "corresponding_authors": "",
    "abstract": "There is a direct correspondence between semantic networks and a subset of logic programs, restricted only to binary predicates. The advantage of the latter is that it can describe not only the nodes and arcs comprising a semantic net, but also the data-retrieval operations applied to such nets. The main objective of this paper is to present a data-driven model of computation that permits this subset of logic programs to be executed on a highly parallel computer architecture. We demonstrate how logic programs may be converted into collections of data-flow graphs in which resolution is viewed as a process of finding matches between certain graph templates and portions of the data-flow graphs. This graph fitting process is carried out by messages propagating asynchronously through the data-flow graph; thus computation is entirely data driven, without the need for any centralized control and centralized memory. This permits a potentially large number of independent processing elements to cooperate in solving a given query.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1986536225",
    "type": "article"
  },
  {
    "title": "An axiomatic treatment of exception handling in an expression-oriented language",
    "doi": "https://doi.org/10.1145/24039.24052",
    "publication_date": "1987-07-01",
    "publication_year": 1987,
    "authors": "Shaula Yemini; Daniel M. Berry",
    "corresponding_authors": "",
    "abstract": "An axiomatic semantic definition is given of the replacement model of exception handling in an expression-oriented language. These semantics require only two new proof rules for the most general case. An example is given of a program fragment using this model of exception handling, and these rules are used to verify the consistency of the fragment and its specification.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2055771053",
    "type": "article"
  },
  {
    "title": "An Introduction to S/SL: Syntax/Semantic Language",
    "doi": "https://doi.org/10.1145/357162.357164",
    "publication_date": "1982-04-01",
    "publication_year": 1982,
    "authors": "Richard C. Holt; James R. Cordy; David B. Wortman",
    "corresponding_authors": "",
    "abstract": "introduction Free Access Share on An Introduction to S/SL: Syntax/Semantic Language Authors: Richard C. Holt Computer Systems Research Group, University of Toronto, Toronto, Canada M5S 1A1 Computer Systems Research Group, University of Toronto, Toronto, Canada M5S 1A1View Profile , James R. Cordy Computer Systems Research Group, University of Toronto, Toronto, Canada M5S 1A1 Computer Systems Research Group, University of Toronto, Toronto, Canada M5S 1A1View Profile , David B. Wortman Computer Systems Research Group, University of Toronto, Toronto, Canada M5S 1A1 Computer Systems Research Group, University of Toronto, Toronto, Canada M5S 1A1View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 2pp 149–178https://doi.org/10.1145/357162.357164Published:01 April 1982Publication History 16citation1,268DownloadsMetricsTotal Citations16Total Downloads1,268Last 12 Months366Last 6 weeks44 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1986099663",
    "type": "article"
  },
  {
    "title": "Row replacement algorithms for screen editors",
    "doi": "https://doi.org/10.1145/59287.59290",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "Eugene W. Meyers; Webb Miller",
    "corresponding_authors": "",
    "abstract": "Interactive screen editors repeatedly determine terminal command sequences to update a screen row. Computing an optimal command sequence differs from the traditional sequence comparison problem in that there is a cost for moving the cursor over unedited characters and the cost of an n -character command is not always the cost of n one-character commands. For example, on an ANSI-standard terminal, it takes nine bytes to insert one character, ten to insert two, eleven to insert three, and so on. This paper presents an O ( MN ) dynamic programming algorithm for row replacement where an n -character command costs α n + β for constants α and β. M is the length of the original row and N is the length of its replacement. Also given is an O ( Cost × ( M + N )) “greedy” algorithm for optimal row replacement. Here Cost is the optimal cost (in bytes) of the replacement, so the algorithm is fast when the require d update is small. Though the algorithm is rather complicated, it is fast enough to be useful in practice.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2006597312",
    "type": "article"
  },
  {
    "title": "The PegaSys System: pictures as formal documentation of large programs",
    "doi": "https://doi.org/10.1145/6465.6478",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Mark Moriconi; Dwight F. Hare",
    "corresponding_authors": "",
    "abstract": "PegsSys is an experimental system in which a user formally describes how a program is put together by means of a hierarchically structured collection of pictures, called formal dependency diagrams (FDDs). Icons in an FDD denote a wide range of data and control dependencies among the relatively coarse-grained entities contained in large programs. Dependencies considered atomic with respect to one level in a hierarchy can be decomposed into a number of dependencies at a lower level. Each dependency can be a predefined primitive of the FDD language or it can be defined by a PegaSys user in terms of the primitives. A PegsSys user is given the illusion that logical formulas do not exist, even though PegaSys reasons about them internally. This involves (1) checking whether an FDD is meaningful syntactically, (2) determining whether hierarchical refinements of an FDD are methodologically sound, and (3) deciding whether an FDD hierarchy is logically consistent with the program that it is intended to describe. The techniques used to provide these capabilities are discussed along with the logical properties that enable PegaSys to maintain the user illusion.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2017407866",
    "type": "article"
  },
  {
    "title": "Logic of global synchrony",
    "doi": "https://doi.org/10.1145/973097.973098",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Yifeng Chen; J. W. Sanders",
    "corresponding_authors": "",
    "abstract": "An intermediate-level specification formalism (i.e., specification language supported by laws and a semantic model), Logs, is presented for PRAM and BSP styles of parallel programming. It extends pre-post sequential semantics to reveal states at points of global synchronization. The result is an integration of the pre-post and reactive-process styles of specification. The language consists of only six commands from which other useful commands can be derived. Parallel composition is simply logical conjunction and hence compositional. A simple predicative semantics and a complete set of algebraic laws are presented. Novel ingredients include the separation, in our reactive context, of the processes for nontermination and for abortion which coincide in standard programming models; the use of partitions, combining the terminating behavior of one program with the nonterminating behavior of another; and a fixpoint operator, the partitioned fixpoint. Our semantics benefits from the recent \"healthiness function\" approach for predicative semantics. Use of Logs, along with the laws for reasoning about it, is demonstrated on two problems: matrix multiplication (a terminating numerical computation) and the dining philosophers (a reactive computation). The style of reasoning is so close to programming practice that direct transformation from Logs specifications to real PRAM and BSP programs becomes possible.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2019790901",
    "type": "article"
  },
  {
    "title": "Mixin modules in a call-by-value setting",
    "doi": "https://doi.org/10.1145/1086642.1086644",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Tom Hirschowitz; Xavier Leroy",
    "corresponding_authors": "",
    "abstract": "The ML module system provides powerful parameterization facilities, but lacks the ability to split mutually recursive definitions across modules and provides insufficient support for incremental programming. A promising approach to solve these issues is Ancona and Zucca's mixin module calculus CMS . However, the straightforward way to adapt it to ML fails, because it allows arbitrary recursive definitions to appear at any time, which ML does not otherwise support. In this article, we enrich CMS with a refined type system that controls recursive definitions through the use of dependency graphs. We then develop and prove sound a separate compilation scheme, directed by dependency graphs, that translates mixin modules down to a call-by-value λ-calculus extended with a nonstandard let rec construct.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2026942132",
    "type": "article"
  },
  {
    "title": "An unfold/fold transformation framework for definite logic programs",
    "doi": "https://doi.org/10.1145/982158.982160",
    "publication_date": "2004-05-01",
    "publication_year": 2004,
    "authors": "Abhik Roychoudhury; K. Narayan Kumar; C. R. Ramakrishnan; I. V. Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "Given a logic program P , an unfold/fold program transformation system derives a sequence of programs P = P 0 , P 1 , …, P n , such that P i +1 is derived from P i by application of either an unfolding or a folding step. Unfold/fold transformations have been widely used for improving program efficiency and for reasoning about programs. Unfolding corresponds to a resolution step and hence is semantics-preserving. Folding, which replaces an occurrence of the right hand side of a clause with its head, may on the other hand produce a semantically different program. Existing unfold/fold transformation systems for logic programs restrict the application of folding by placing (usually syntactic) conditions that are sufficient to guarantee the correctness of folding. These restrictions are often too strong, especially when the transformations are used for reasoning about programs. In this article we develop a transformation system (called SCOUT) for definite logic programs that is provably more powerful (in terms of transformation sequences allowed) than existing transformation systems. This extra power is needed for a novel use of logic program transformations: for the verification of a specific class of concurrent systems, called parameterized concurrent systems.Our transformation system is constructed by developing a framework, which is parameterized by a \"measure space\" and associated measure functions. This framework places no syntactic restriction on the application of folding, and it can be used to derive transformation systems (by fixing the measure space and functions). The power of the system is determined by the choice of the measure space and functions; thus the relative power of different transformation systems can be compared by considering their measure spaces and functions. The correctness of these transformation systems follows from the correctness of the framework. We show that various existing transformation systems can be obtained as instances of our framework. We extend the unfold/fold transformation framework with a goal replacement transformation that allows semantically equivalent conjunctions of atoms to be interchanged. We then derive a new transformation system SCOUT as an instance of the framework and show its power relative to the existing transformation systems. SCOUT has been used to inductively prove temporal properties of parameterized concurrent systems (infinite families of finite state concurrent systems). We demonstrate the use of the additional power of SCOUT in constructing such induction proofs.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2045750726",
    "type": "article"
  },
  {
    "title": "A One-Pass Algorithm for Overload Resolution in Ada",
    "doi": "https://doi.org/10.1145/69622.69623",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "T. P. Baker",
    "corresponding_authors": "T. P. Baker",
    "abstract": "article Free AccessA One-Pass Algorithm for Overload Resolution in Ada Author: T. P. Baker Florida State University Florida State UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 601–614https://doi.org/10.1145/69622.69623Published:01 October 1982Publication History 15citation374DownloadsMetricsTotal Citations15Total Downloads374Last 12 Months9Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2069995849",
    "type": "article"
  },
  {
    "title": "Describing and analyzing distributed software system designs",
    "doi": "https://doi.org/10.1145/3916.3989",
    "publication_date": "1985-07-01",
    "publication_year": 1985,
    "authors": "George S. Avrunin; Jack C. Wileden",
    "corresponding_authors": "",
    "abstract": "In this paper we outline an approach to describing and analyzing designs for distributed software systems. A descriptive notation is introduced, and analysis techniques applicable to designs expressed in that notation are presented. The usefulness of the approach is illustrated by applying it to a realistic distributed software-system design problem involving mutual exclusion in a computer network.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2074578336",
    "type": "article"
  },
  {
    "title": "Functional declarative language design and predicate calculus",
    "doi": "https://doi.org/10.1145/1086642.1086647",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Raymond Boute",
    "corresponding_authors": "Raymond Boute",
    "abstract": "In programming language and software engineering, the main mathematical tool is de facto some form of predicate logic. Yet, as elsewhere in applied mathematics, it is used mostly far below its potential, due to its traditional formulation as just a topic in logic instead of a calculus for everyday practical use.The proposed alternative combines a language of utmost simplicity (four constructs only) that is devoid of the defects of common mathematical conventions, with a set of convenient calculation rules that is sufficiently comprehensive to make it practical for everyday use in most (if not all) domains of interest.Its main elements are a functional predicate calculus and concrete generic functionals. The first supports formal calculation with quantifiers with the same fluency as with derivatives and integrals in classical applied mathematics and engineering. The second achieves the same for calculating with functionals, including smooth transition between pointwise and point-free expression.The extensive collection of examples pertains mainly to software specification, language semantics and its mathematical basis, program calculation etc., but occasionally shows wider applicability throughout applied mathematics and engineering. Often it illustrates how formal reasoning guided by the shape of the expressions is an instrument for discovery and expanding intuition, or highlights design opportunities in declarative and (functional) programming languages.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2092276539",
    "type": "article"
  },
  {
    "title": "Specifying the Semantics of while Programs: A Tutorial and Critique of a Paper by Hoare and Lauer",
    "doi": "https://doi.org/10.1145/357146.357151",
    "publication_date": "1981-10-01",
    "publication_year": 1981,
    "authors": "Irene Greif; Albert R. Meyer",
    "corresponding_authors": "",
    "abstract": "article Free AccessSpecifying the Semantics of while Programs: A Tutorial and Critique of a Paper by Hoare and Lauer Authors: Irene Greif Laboratory for Computer Science, M.I.T., 545 Technology Square, Cambridge, MA Laboratory for Computer Science, M.I.T., 545 Technology Square, Cambridge, MAView Profile , Albert R. Meyer Laboratory for Computer Science, M.I.T., 545 Technology Square, Cambridge, MA Laboratory for Computer Science, M.I.T., 545 Technology Square, Cambridge, MAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 4Oct. 1981 pp 484–507https://doi.org/10.1145/357146.357151Published:01 October 1981Publication History 12citation574DownloadsMetricsTotal Citations12Total Downloads574Last 12 Months26Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1978078271",
    "type": "article"
  },
  {
    "title": "Calculational semantics",
    "doi": "https://doi.org/10.1145/1146809.1146814",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Raymond Boute",
    "corresponding_authors": "Raymond Boute",
    "abstract": "The objects of programming semantics, namely, programs and languages, are inherently formal, but the derivation of semantic theories is all too often informal, deprived of the benefits of formal calculation “guided by the shape of the formulas.” Therefore, the main goal of this article is to provide for the study of semantics an approach with the same convenience and power of discovery that calculus has given for many years to applied mathematics, physics, and engineering. The approach uses functional predicate calculus and concrete generic functionals ; in fact, a small part suffices. Application to a semantic theory proceeds by describing program behavior in the simplest possible way, namely by program equations , and discovering the axioms of the theory as theorems by calculation. This is shown in outline for a few theories, and in detail for axiomatic semantics, fulfilling a second goal of this article. Indeed, a chafing problem with classical axiomatic semantics is that some axioms are unintuitive at first, and that justifications via denotational semantics are too elaborate to be satisfactory. Derivation provides more transparency. Calculation of formulas for ante- and postconditions is shown in general, and for the major language constructs in particular. A basic problem reported in the literature, whereby relations are inadequate for handling nondeterminacy and termination, is solved here through appropriately defined program equations. Several variants and an example in mathematical analysis are also presented. One conclusion is that formal calculation with quantifiers is one of the most important elements for unifying continuous and discrete mathematics in general, and traditional engineering with computing science, in particular.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2052721334",
    "type": "article"
  },
  {
    "title": "Derivation of Invariant Assertions During Program Development by Transformation",
    "doi": "https://doi.org/10.1145/357103.357108",
    "publication_date": "1980-07-01",
    "publication_year": 1980,
    "authors": "Manfred Broy; Bernd Krieg-Brückner",
    "corresponding_authors": "",
    "abstract": "Two approaches to the development of efficient and correct iterative programs are contrasted: the construction of an iterative program and a proof of its correctness using invariant assertions of loops, and the construction and proof of a recursive program with a subsequent transformation into an iterative version by schematically applying suitable recursion removal rules. The connection between the approaches is demonstrated by augmenting such transformation rules by inductive assertions. It is argued that the latter approach to program development is superior since the correctness proof of a recursive program is easier in most cases. Considerable verification overhead can be avoided this way, in particular, some difficulties with the interaction of successive loops and their associated invariants.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2059976750",
    "type": "article"
  },
  {
    "title": "Bottom-up shape analysis using LISF",
    "doi": "https://doi.org/10.1145/2039346.2039349",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "Bhargav S. Gulavani; Supratik Chakraborty; G. Ramalingam; Aditya V. Nori",
    "corresponding_authors": "",
    "abstract": "In this article, we present a new shape analysis algorithm. The key distinguishing aspect of our algorithm is that it is completely compositional, bottom-up and noniterative. We present our algorithm as an inference system for computing Hoare triples summarizing heap manipulating programs. Our inference rules are compositional: Hoare triples for a compound statement are computed from the Hoare triples of its component statements. These inference rules are used as the basis for bottom-up shape analysis of programs. Specifically, we present a Logic of Iterated Separation Formulae (LISF), which uses the iterated separating conjunct of Reynolds [2002] to represent program states. A key ingredient of our inference rules is a strong bi-abduction operation between two logical formulas. We describe sound strong bi-abduction and satisfiability procedures for LISF. We have built a tool called S p I n E that implements these inference rules and have evaluated it on standard shape analysis benchmark programs. Our experiments show that S p I n E can generate expressive summaries, which are complete functional specifications in many cases.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2026496705",
    "type": "article"
  },
  {
    "title": "Finite differencing of logical formulas for static analysis",
    "doi": "https://doi.org/10.1145/1749608.1749613",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Thomas Reps; Mooly Sagiv; А. А. Логинов",
    "corresponding_authors": "",
    "abstract": "This article concerns mechanisms for maintaining the value of an instrumentation relation (also known as a derived relation or view ), defined via a logical formula over core relations, in response to changes in the values of the core relations. It presents an algorithm for transforming the instrumentation relation's defining formula into a relation-maintenance formula that captures what the instrumentation relation's new value should be. The algorithm runs in time linear in the size of the defining formula. The technique applies to program analysis problems in which the semantics of statements is expressed using logical formulas that describe changes to core relation values. It provides a way to obtain values of the instrumentation relations that reflect the changes in core relation values produced by executing a given statement. We present experimental evidence that our technique is an effective one: for a variety of benchmarks, the relation-maintenance formulas produced automatically using our approach yield the same precision as the best available hand-crafted ones.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2159279400",
    "type": "article"
  },
  {
    "title": "Contracts for First-Class Classes",
    "doi": "https://doi.org/10.1145/2518189",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "T. Stephen Strickland; Christos Dimoulas; Asumu Takikawa; Matthias Felleisen",
    "corresponding_authors": "",
    "abstract": "First-class classes enable programmers to abstract over patterns in the class hierarchy and to experiment with new forms of object-oriented programming such as mixins and traits. This increase in expressive power calls for tools to control the complexity of the software architecture. A contract system is one possible tool that has seen much use in object-oriented programming languages, but existing contract systems cannot cope with first-class classes. On the one hand, the typical contract language deals only with plain values such as numbers, while classes are higher-order values. On the other hand, contract specifications are usually contained within class definitions, while classes as values call for a separate contract language. This article presents the design and implementation of a contract system for first-class classes as well as a two-pronged evaluation. The first one states and proves a “blame correctness” theorem for a model of our language. The theorem shows that when the contract system assigns blame to a component for a contract violation, the component is indeed responsible for providing the nonconforming value. The second part, consisting of benchmarks and case studies, demonstrates the need for the rich contract language and validates that our implementation approach is performant with respect to time.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1965665477",
    "type": "article"
  },
  {
    "title": "Verifying Invariants of Lock-Free Data Structures with Rely-Guarantee and Refinement Types",
    "doi": "https://doi.org/10.1145/3064850",
    "publication_date": "2017-05-10",
    "publication_year": 2017,
    "authors": "Colin S. Gordon; Michael D. Ernst; Dan Grossman; Matthew Parkinson",
    "corresponding_authors": "",
    "abstract": "Verifying invariants of fine-grained concurrent data structures is challenging, because interference from other threads may occur at any time. We propose a new way of proving invariants of fine-grained concurrent data structures: applying rely-guarantee reasoning to references in the concurrent setting. Rely-guarantee applied to references can verify bounds on thread interference without requiring a whole program to be verified. This article provides three new results. First, it provides a new approach to preserving invariants and restricting usage of concurrent data structures. Our approach targets a space between simple type systems and modern concurrent program logics, offering an intermediate point between unverified code and full verification. Furthermore, it avoids sealing concurrent data structure implementations and can interact safely with unverified imperative code. Second, we demonstrate the approach’s broad applicability through a series of case studies, using two implementations: an axiomatic C oq domain-specific language and a library for Liquid Haskell. Third, these two implementations allow us to compare and contrast verifications by interactive proof (C oq ) and a weaker form that can be expressed using automatically-discharged dependent refinement types (Liquid Haskell).",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2612881331",
    "type": "article"
  },
  {
    "title": "Inferring Lower Runtime Bounds for Integer Programs",
    "doi": "https://doi.org/10.1145/3410331",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Florian Frohn; Matthias Naaf; Marc Brockschmidt; Jürgen Giesl",
    "corresponding_authors": "",
    "abstract": "We present a technique to infer lower bounds on the worst-case runtime complexity of integer programs, where in contrast to earlier work, our approach is not restricted to tail-recursion. Our technique constructs symbolic representations of program executions using a framework for iterative, under-approximating program simplification. The core of this simplification is a method for (under-approximating) program acceleration based on recurrence solving and a variation of ranking functions. Afterwards, we deduce asymptotic lower bounds from the resulting simplified programs using a special-purpose calculus and an SMT encoding. We implemented our technique in our tool LoAT and show that it infers non-trivial lower bounds for a large class of examples.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3100830552",
    "type": "article"
  },
  {
    "title": "Atomicity Refinement for Verified Compilation",
    "doi": "https://doi.org/10.1145/2601339",
    "publication_date": "2014-07-01",
    "publication_year": 2014,
    "authors": "Suresh Jagannathan; Vincent Laporte; Gustavo Petri; David Pichardie; Jan Vítek",
    "corresponding_authors": "",
    "abstract": "We consider the verified compilation of high-level managed languages like Java or C# whose intermediate representations provide support for shared-memory synchronization and automatic memory management. Our development is framed in the context of the Total Store Order relaxed memory model. Ensuring complier correctness is challenging because high-level actions are translated into sequences of nonatomic actions with compiler-injected snippets of racy code; the behavior of this code depends not only on the actions of other threads but also on out-of-order executions performed by the processor. A naïve proof of correctness would require reasoning over all possible thread interleavings. In this article, we propose a refinement-based proof methodology that precisely relates concurrent code expressed at different abstraction levels, cognizant throughout of the relaxed memory semantics of the underlying processor. Our technique allows the compiler writer to reason compositionally about the atomicity of low-level concurrent code used to implement managed services. We illustrate our approach with examples taken from the verification of a concurrent garbage collector.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2023785576",
    "type": "article"
  },
  {
    "title": "Symbolic Disintegration with a Variety of Base Measures",
    "doi": "https://doi.org/10.1145/3374208",
    "publication_date": "2020-05-19",
    "publication_year": 2020,
    "authors": "P. J. Narayanan; Chung-chieh Shan",
    "corresponding_authors": "",
    "abstract": "Disintegration is a relation on measures and a transformation on probabilistic programs that generalizes density calculation and conditioning, two operations widely used for exact and approximate inference. Existing program transformations that find a disintegration or density automatically are limited to a fixed base measure that is an independent product of Lebesgue and counting measures, so they are of no help in practical cases that require tricky reasoning about other base measures. We present the first disintegrator that handles variable base measures, including discrete-continuous mixtures , dependent products , and disjoint sums . By analogy with type inference, our disintegrator can check a given base measure as well as infer an unknown one that is principal. We derive the disintegrator and prove it sound by equational reasoning from semantic specifications. It succeeds in a variety of applications where disintegration and density calculation had not been previously mechanized.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3029528495",
    "type": "article"
  },
  {
    "title": "Typed–Untyped Interactions: A Comparative Analysis",
    "doi": "https://doi.org/10.1145/3579833",
    "publication_date": "2023-01-12",
    "publication_year": 2023,
    "authors": "Ben Greenman; Christos Dimoulas; Matthias Felleisen",
    "corresponding_authors": "",
    "abstract": "The literature presents many strategies for enforcing the integrity of types when typed code interacts with untyped code. This article presents a uniform evaluation framework that characterizes the differences among some major existing semantics for typed–untyped interaction. Type system designers can use this framework to analyze the guarantees of their own dynamic semantics.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4315705023",
    "type": "article"
  },
  {
    "title": "Optimization-Aware Compiler-Level Event Profiling",
    "doi": "https://doi.org/10.1145/3591473",
    "publication_date": "2023-04-10",
    "publication_year": 2023,
    "authors": "Matteo Basso; Aleksandar Prokopec; Andrea Rosà; Walter Binder",
    "corresponding_authors": "",
    "abstract": "Tracking specific events in a program’s execution, such as object allocation or lock acquisition, is at the heart of dynamic analysis. Despite the apparent simplicity of this task, quantifying these events is challenging due to the presence of compiler optimizations. Profiling perturbs the optimizations that the compiler would normally do—a profiled program usually behaves differently than the original one. In this article, we propose a novel technique for quantifying compiler-internal events in the optimized code, reducing the profiling perturbation on compiler optimizations. Our technique achieves this by instrumenting the program from within the compiler, and by delaying the instrumentation until the point in the compilation pipeline after which no subsequent optimizations can remove the events. We propose two different implementation strategies of our technique based on path-profiling, and a modification to the standard path-profiling algorithm that facilitates the use of the proposed strategies in a modern just-in-time (JIT) compiler. We use our technique to analyze the behaviour of the optimizations in Graal, a state-of-the-art compiler for the Java Virtual Machine, identifying the reasons behind a performance improvement of a specific optimization, and the causes behind an unexpected slowdown of another. Finally, our evaluation results show that the two proposed implementations result in a significantly lower execution-time overhead w.r.t. a naive implementation.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4363647646",
    "type": "article"
  },
  {
    "title": "A Model Checker for Operator Precedence Languages",
    "doi": "https://doi.org/10.1145/3608443",
    "publication_date": "2023-08-02",
    "publication_year": 2023,
    "authors": "Michele Chiari; Dino Mandrioli; Francesco Pontiggia; Matteo Pradella",
    "corresponding_authors": "",
    "abstract": "The problem of extending model checking from finite state machines to procedural programs has fostered much research toward the definition of temporal logics for reasoning on context-free structures. The most notable of such results are temporal logics on Nested Words, such as CaRet and NWTL. Recently, Precedence Oriented Temporal Logic (POTL) has been introduced to specify and prove properties of programs coded trough an Operator Precedence Language (OPL). POTL is complete w.r.t. the FO restriction of the MSO logic previously defined as a logic fully equivalent to OPL. POTL increases NWTL’s expressive power in a perfectly parallel way as OPLs are more powerful that nested words. In this article, we produce a model checker, named POMC, for OPL programs to prove properties expressed in POTL. To the best of our knowledge, POMC is the first implemented and openly available model checker for proving tree-structured properties of recursive procedural programs. We also report on the experimental evaluation we performed on POMC on a nontrivial benchmark.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4385497366",
    "type": "article"
  },
  {
    "title": "Circuit Width Estimation via Effect Typing and Linear Dependency",
    "doi": "https://doi.org/10.1145/3737282",
    "publication_date": "2025-05-23",
    "publication_year": 2025,
    "authors": "Andrea Colledan; Ugo Dal Lago; Niki Vazou",
    "corresponding_authors": "",
    "abstract": "Circuit description languages are a class of quantum programming languages in which programs are classical and produce a description of a quantum computation, in the form of a quantum circuit . Since these programs can leverage all the expressive power of high-level classical languages, circuit description languages have been successfully used to describe complex quantum algorithms, whose circuits, however, may involve many more qubits and gate applications than current quantum architectures can actually muster. In this paper, we present Proto-Quipper-R, a circuit description language endowed with a linear dependent type-and-effect system capable of deriving parametric upper bounds on the width of the circuits produced by a program. We prove both the standard type safety results and that the resulting resource analysis is correct with respect to a big-step operational semantics. Lastly, we introduce QuRA, a static analysis tool based on Proto-Quipper-R’s type system, and use it to show that our framework allows for the automatic width verification of realistic quantum algorithms, such as the QFT and Grover’s algorithm.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410635568",
    "type": "article"
  },
  {
    "title": "Outcome Logic: A Unified Approach to the Metatheory of Program Logics with Branching Effects",
    "doi": "https://doi.org/10.1145/3743131",
    "publication_date": "2025-06-06",
    "publication_year": 2025,
    "authors": "Noam Zilberstein",
    "corresponding_authors": "Noam Zilberstein",
    "abstract": "Starting with Hoare Logic over 50 years ago, numerous program logics have been devised to reason about the different kinds of programs encountered in the real world. This includes reasoning about computational effects, particularly those effects that cause the program execution to branch into multiple paths due to, e.g., nondeterministic or probabilistic choice. Outcome Logic reimagines Hoare Logic with branching at its core, using an algebraic representation of choice to capture programs that branch into many outcomes. In this article, we give a comprehensive account of the Outcome Logic metatheory. This includes a relatively complete proof system for Outcome Logic with the ability to reason about general purpose looping. We also show that this proof system applies to programs with various types of branching, that it subsumes some well known logics such as Hoare Logic, and that it facilitates the reuse of proof fragments across different kinds of specifications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411088602",
    "type": "article"
  },
  {
    "title": "Conditional attribute grammars",
    "doi": "https://doi.org/10.1145/225540.225544",
    "publication_date": "1996-01-01",
    "publication_year": 1996,
    "authors": "John Boyland",
    "corresponding_authors": "John Boyland",
    "abstract": "Attribute grammars are a useful formalism for the specification of computations on structured terms. The classical definition of attribute grammars, however, has no way of treating conditionals nonstrictly. Consequently, the natural way of expressing many otherwise well-behaved computations involves a circularity. This article presents conditional attribute grammars , and extension of attribute grammars that enables more precise analysis of conditionals. In conditional attribute grammars, attribute equations may have guards. Equations are active only when their guards are satisfied. The standard attribute grammar evaluation classes are definable for conditional attribute grammars, and the corresponding evaluation techniques can be easily adapted. However, determining membership in standard evaluation classes such as 1-SWEEP, OAG, and SNC is NP-hard.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2000677048",
    "type": "article"
  },
  {
    "title": "Live-structure dataflow analysis for Prolog",
    "doi": "https://doi.org/10.1145/174662.174664",
    "publication_date": "1994-03-01",
    "publication_year": 1994,
    "authors": "Anne Mulkers; William H. Winsborough; Maurice Bruynooghe",
    "corresponding_authors": "",
    "abstract": "For the class of applicative programming languages, efficient methods for reclaiming the memory occupied by released data structures constitute an important aspect of current implementations. The present article addresses the problem of memory reuse for logic programs through program analysis rather than by run-time garbage collection. The aim is to derive run-time properties that can be used at compile time to specialize the target code for a program according to a given set of queries and to automatically introduce destructive assignments in a safe and transparent way so that fewer garbage cells are created. The dataflow analysis is constructed as an application of abstract interpretation for logic programs. An abstract domain for describing structure-sharing and liveness properties is developed as are primitive operations that guarantee a sound and terminating global analysis. We explain our motivation for the design of the abstract domain, make explicit the underlying implementation assumptions, and discuss the precision of the results obtained by a prototype analyzer.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2056315047",
    "type": "article"
  },
  {
    "title": "A complete calculus for the multialgebraic and functional semantics of nondeterminism",
    "doi": "https://doi.org/10.1145/201059.201070",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Michał Walicki; Sigurd Meldal",
    "corresponding_authors": "",
    "abstract": "The current algebraic models for nondeterminism focus on the notion of possibility rather than necessity and consequently equate (nondeterministic) terms that one would intuitively not consider equal. Furthermore, existing models for nondeterminism depart radically from the standard models for (equational) specifications of deterministic operators. One would prefer that a specification language for nondeterministic operators be based on an extension of the standard model concepts, preferably in such a way that the reasoning system for (possibly nondeterministic) operators becomes the standard equational one whenever restricted to the deterministic operators—the objective should be to minimize the departure from the standard frameworks. In this article we define a specification language for nondeterministic operators and multialgebraic semantics. The first complete reasoning system for such specifications is introduced. We also define a transformation of specifications of nondeterministic operators into derived specifications of deterministic ones, obtaining a “computational” semantics of nondeterministic specification by adopting the standard semantics of the derived specification as the semantics of the original one. This semantics turns out to be a refinement of multialgebra semantics. The calculus is shown to be sound and complete also with respect to the new semantics.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2061715716",
    "type": "article"
  },
  {
    "title": "Functions as passive constraints in LIFE",
    "doi": "https://doi.org/10.1145/183432.183526",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Hassan Aı̈t-Kaci; Andreas Podelski",
    "corresponding_authors": "",
    "abstract": "LIFE is a programming language proposing to integrate logic programming, functional programming, and object-oriented programming. It replaces first-order terms with ψ-terms, data structures that allow computing with partial information. These are approximation structures denoting sets of values. LIFE further enriches the expressiveness of ψ-terms with functional dependency constraints. We must explain the meaning and use of functions in LIFE declaratively, as solving partial information constraints. These constraints do not attempt to generate their solutions but behave as demons filtering out anything else. In this manner, LIFE functions act as declarative coroutines. We need to show that the ψ-term's approximation semantics is congruent with an operational semantics viewing functional reduction as an effective enforcing of passive constraints. In this article, we develop a general formal framework for entailment and disentailment of constraints based on a technique called relative simplification. We study its operational and semantical properties, and we use it to account for functional application over ψ-terms in LIFE.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2115662179",
    "type": "article"
  },
  {
    "title": "Metalevel building blocks for modular systems",
    "doi": "https://doi.org/10.1145/177492.177578",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Suresh Jagannathan",
    "corresponding_authors": "Suresh Jagannathan",
    "abstract": "The formal definition of any namespace device found in a programming language can be given in terms of transformations on a semantic environment. It is worthwhile, therefore, to consider the implications of incorporating environments as bona fide data objects in a programming system. In this article, we propose a treatment of environments and the mechanism by which they are reified and manipulated, that addresses these concerns. The language described below (Rascal) permits environments to be reified into data structures, and data structures to be reflected into environments, but gives users great flexibility to constrain the extent and scope of these processes. We argue that the techniques and operators developed define a cohesive basis for building large-scale modular systems using reflective programming techniques.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2025165387",
    "type": "article"
  },
  {
    "title": "Mathematical foundations for time warp systems",
    "doi": "https://doi.org/10.1145/161468.161470",
    "publication_date": "1993-11-01",
    "publication_year": 1993,
    "authors": "Jonathan I. Leivent; Ronald Watro",
    "corresponding_authors": "",
    "abstract": "We develop a simple formal model of the Time Warp approach to distributed computation, prove several important properties of the model, and devise some extensions to Time Warp that provide improved termination behavior. Our model consists of processes that communicate solely via message passing. One of the basic process steps is a rollback operation that includes message retraction via transmission of antimessages. In the model, we consider three problems: safety, progress, and termination. By safety, we mean that for a given system of processes, if a run of the system terminates, then the final system state of the run is identical to the final system state of a rollback-free run. We give premises that imply safety, and a counterexample that shows how safety can fail. By progress, we mean that, as a run of a system proceeds, the minimum timestamp of an unprocessed message always eventually increases. We state three axioms that imply the progress property. By termination, we mean that, if all rollback-free runs of a system terminate, then all runs terminate. The termination property is generally false in existing implementations of Time Warp systems due to the possibility of Time Warp vortices. We define additional mechanisms that can guarantee the termination property for most Time Warp applications.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2043548085",
    "type": "article"
  },
  {
    "title": "Constrained types and their expressiveness",
    "doi": "https://doi.org/10.1145/232706.232715",
    "publication_date": "1996-09-01",
    "publication_year": 1996,
    "authors": "Jens Palsberg; Scott F. Smith",
    "corresponding_authors": "",
    "abstract": "A constrained type consists of both a standard type and a constraint set. Such types enable efficient type inference for object-oriented languages with polymorphism and subtyping, as demonstrated by Eifrig, Smith, and Trifonov. Until now, it has been unclear how expressive constrained types are. In this article we study constrained types without universal quantification. We prove that they accept the same programs as the type system of Amadio and Cardelli with subtyping and recursive types. This result gives a precise connection between constrained types and the standard notion of types.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2060688393",
    "type": "article"
  },
  {
    "title": "Scheduling time-constrained instructions on pipelined processors",
    "doi": "https://doi.org/10.1145/383721.383733",
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Allen Leung; Krishna V. Palem; Amir Pnueli",
    "corresponding_authors": "",
    "abstract": "In this work we investigate the problem of scheduling instructions on idealized microprocessors with multiple pipelines, in the presence of precedence constraints, release-times, deadlines, and latency constraints. A latency of l ij specifies that there must be at least l ij time-steps between the completion time of instruction i and the start time of instruction j . A latency of l ij =−1 can be used to specify that j may be scheduled concurrently with i but not earlier. We present a generic algorithm that runs in O ( n 2 log n α( n )+ ne ) time, given n instructions and e edges in the precedence DAG, where α( n ) is the functional inverse of the Ackermann function. Our algorithm can be used to construct feasible schedules for various classes of instances, including instances with the following configurations: (1) one pipeline, with individual release-times and deadlines and where the latencies between instructions are restricted to 0 and 1; (2) m pipelines, with individual release-times and deadlines, and monotone-interval order precedences; (3) two pipelines with latencies of −1 or 0, and release-times and deadlines; (4) one pipeline, latencies of 0 or 1 and individual processing times that are at least one; (5) m pipelines, intree precedences, constant latencies, and deadlines; (6) m pipelines, outtree precedences, constant latencies, and release-times. For instances with deadlines, optimal schedules that minimize the maximal tardiness can be constructed using binary search, in O (log n ) iterations of our algorithm. We obtain our results using backward scheduling, a very general relaxation method, which extends, unifies, and clarifies many previous results on instruction scheduling for pipelined and parallel machines.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2063771307",
    "type": "article"
  },
  {
    "title": "Influence of cross-interferences on blocked loops",
    "doi": "https://doi.org/10.1145/210184.210185",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "Christine Fricker; Olivier Temam; William Jalby",
    "corresponding_authors": "",
    "abstract": "State-of-the art data locality optimizing algorithms are targeted for local memories rather than for cache memories. Recent work on cache interferences seems to indicate that these phenomena can severely affect blocked algorithms cache performance. Because of cache conflicts, it is not possible to know the precise gain brought by blocking. It is even difficult to determine for which problem sizes blocking is useful. Computing the actual optimal block size is difficult because cache conflicts are highly irregular. In this article, we illustrate the issue of precisely evaluating cross-interferences in blocked loops with blocked matrix-vector multiply. Most significant interference phenomena are captured because unusual parameters such as array base addresses are being considered. The techniques used allow us to compute the precise improvement due to blocking and the threshold value of problem parameters for which the blocked loop should be preferred. It is also possible to derive an expression of the optimal block size as a function of problem parameters. Finally, it is shown that a precise rather than an approximate evaluation of cache conflicts is sometimes necessary to obtain near-optimal performance.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2077903526",
    "type": "article"
  },
  {
    "title": "Indirect distributed garbage collection",
    "doi": "https://doi.org/10.1145/232706.232711",
    "publication_date": "1996-09-01",
    "publication_year": 1996,
    "authors": "José Piquer",
    "corresponding_authors": "José Piquer",
    "abstract": "In new distributed systems, object mobility is usually allowed and is sometimes used by the underlying object manager system to benefit from object access locality. On the other hand, in-transit references to objects can exist at any moment in asynchronous distributed systems. In the presence of object mobility and in-transit references, many garbage collector (GC) algorithms fail to operate correctly. Others need to use the system's object finder to find the objects while performing their work. As a general principle, a GC should never interfere with object manager polices (such as forcing migration or fixing an object to a given processor). However, if the GC uses the object finder, it will change the access pattern of the system, and eventually it could foul the global allocation policy. In this article we propose a new GC family, Indirect Garbage Collectors, allowing to separate the problems of object management (placement, replication, and retrieval) from garbage collection. This property allows our algorithms to be implemented on top of almost any existent distributed object system, without having to use the object finder.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1983702438",
    "type": "article"
  },
  {
    "title": "Generalizing specifications for uniformly implemented loops",
    "doi": "https://doi.org/10.1145/2363.2708",
    "publication_date": "1985-01-02",
    "publication_year": 1985,
    "authors": "Douglas D. Dunlop; Victor R. Basili",
    "corresponding_authors": "",
    "abstract": "The problem of generalizing functional specifications for while loops is considered. This problem occurs frequently when trying to verify that an initialized loop satisfies some functional specification, i.e., produces outputs which are some function of the program inputs. The notion of a valid generalization of a loop specification is defined. A particularly simple valid generalization, a base generalization, is discussed. A property of many commonly occurring while loops, that of being uniformly implemented, is defined. A technique is presented which exploits this property in order to systematically achieve a valid generalization of the loop specification. Two classes of uniformly implemented loops that are particularly susceptible to this form of analysis are defined and discussed. The use of the proposed technique is illustrated with a number of applications. Finally, an implication of the concept of uniform loop implementation for the validation of the obtained generalization is explained.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1985830587",
    "type": "article"
  },
  {
    "title": "Efficient and effective branch reordering using profile data",
    "doi": "https://doi.org/10.1145/586088.586091",
    "publication_date": "2002-11-01",
    "publication_year": 2002,
    "authors": "Minghui Yang; Gang‐Ryung Uh; David Whalley",
    "corresponding_authors": "",
    "abstract": "The conditional branch has long been considered an expensive operation. The relative cost of conditional branches has increased as recently designed machines are now relying on deeper pipelines and higher multiple issue. Reducing the number of conditional branches executed often results in a substantial performance benefit. This paper describes a code-improving transformation to reorder sequences of conditional branches that compare a common variable to constants. The goal is to obtain an ordering where the fewest average number of branches in the sequence will be executed. First, sequences of branches that can be reordered are detected in the control flow. Second, profiling information is collected to predict the probability that each branch will transfer control out of the sequence. Third, the cost of performing each conditional branch is estimated. Fourth, the most beneficial ordering of the branches based on the estimated probability and cost is selected. The most beneficial ordering often includes the insertion of additional conditional branches that did not previously exist in the sequence. Finally, the control flow is restructured to reflect the new ordering. The results of applying the transformation are on average reductions of about 8% fewer instructions executed and 13% branches performed, as well as about a 4% decrease in execution time.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1998298122",
    "type": "article"
  },
  {
    "title": "Access-Right Expressions",
    "doi": "https://doi.org/10.1145/357195.357201",
    "publication_date": "1983-01-01",
    "publication_year": 1983,
    "authors": "Richard B. Kieburtz; Abraham Silberschatz",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Access-Right Expressions Authors: Richard B. Kieburtz Department of Computer Science, State University of New York at Stony Brook, Stony Brook, NY Department of Computer Science, State University of New York at Stony Brook, Stony Brook, NYView Profile , Abraham Silberschatz Department of Computer Science, The University of Texas at Austin, Austin, TX Department of Computer Science, The University of Texas at Austin, Austin, TXView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 1Jan. 1983 pp 78–96https://doi.org/10.1145/357195.357201Published:01 January 1983Publication History 12citation324DownloadsMetricsTotal Citations12Total Downloads324Last 12 Months9Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2004153926",
    "type": "article"
  },
  {
    "title": "Inessential Error Entries and Their Use in LR Parser Optimization",
    "doi": "https://doi.org/10.1145/357162.357165",
    "publication_date": "1982-04-01",
    "publication_year": 1982,
    "authors": "Eljas Soisalon-Soininen",
    "corresponding_authors": "Eljas Soisalon-Soininen",
    "abstract": "article Free Access Share on Inessential Error Entries and Their Use in LR Parser Optimization Author: Eljas Soisalon-Soininen Department of Computer Science, University of Helsinki, Tukholmankatu 2, SF-00250, Helsinki 25, Finland Department of Computer Science, University of Helsinki, Tukholmankatu 2, SF-00250, Helsinki 25, FinlandView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 2April 1982 pp 179–195https://doi.org/10.1145/357162.357165Published:01 April 1982Publication History 10citation294DownloadsMetricsTotal Citations10Total Downloads294Last 12 Months26Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2051690041",
    "type": "article"
  },
  {
    "title": "Efficient subtyping tests with PQ-encoding",
    "doi": "https://doi.org/10.1145/1086642.1086643",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Joseph Gil; Yoav Zibin",
    "corresponding_authors": "",
    "abstract": "Given a type hierarchy, a subtyping test determines whether one type is a direct or indirect descendant of another type. Such tests are a frequent operation during the execution of object-oriented programs. The implementation challenge is in a space-efficient encoding of the type hierarchy that simultaneously permits efficient subtyping tests. We present a new scheme for encoding multiple- and single-inheritance hierarchies, which, in the standard benchmark hierarchies, reduces the footprint of all previously published schemes. Our scheme is called PQ-encoding (PQE) after PQ-trees , a data structure previously used in graph theory for finding the orderings that satisfy a collection of constraints. In particular, we show that in the traditional object layout model, the extra memory requirements for single-inheritance hierarchies is zero. In the PQE subtyping, tests are constant time, and use only two comparisons. The encoding creation time of PQE also compares favorably with previous results. It is less than 1 s on all standard benchmarks on a contemporary architecture, while the average time for processing a type is less than 1 ms. However, PQE is not an incremental algorithm. Other than PQ-trees, PQE employs several novel optimization techniques. These techniques are applicable also in improving the performance of other, previously published, encoding schemes.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1978818681",
    "type": "article"
  },
  {
    "title": "<i>Corrigendum:</i> a new, simpler linear-time dominators algorithm",
    "doi": "https://doi.org/10.1145/1065887.1065888",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Adam L. Buchsbaum; Haim Kaplan; Anne Rogers; Jeffery Westbrook",
    "corresponding_authors": "",
    "abstract": "Corrigendum to ACM Transactions on Programming Languages and Systems , 20(6):1265--1296, 1998.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2007143180",
    "type": "article"
  },
  {
    "title": "PPMexe",
    "doi": "https://doi.org/10.1145/1180475.1180478",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Milenko Drinić; Darko Kirovski; Hoi Vo",
    "corresponding_authors": "",
    "abstract": "With the emergence of software delivery platforms, code compression has become an important system component that strongly affects performance. This article presents PPMexe, a compression mechanism for program binaries that analyzes their syntax and semantics to achieve superior compression ratios. We use the generic paradigm of prediction by partial matching (PPM) as the foundation of our compression codec. PPMexe combines PPM with two preprocessing steps: ( i ) instruction rescheduling to improve prediction rates and ( ii ) heuristic partitioning of a program binary into streams with high autocorrelation. We improve the traditional PPM algorithm by ( iii ) using an additional alphabet of frequent variable-length supersymbols extracted from the input stream of fixed-length symbols. In addition, PPMexe features ( iv ) a low-overhead mechanism that enables decompression starting from an arbitrary instruction of the executable, a property pivotal for runtime software delivery. We implemented PPMexe for x86 binaries and tested it on several large applications. Binaries compressed using PPMexe were 18--24% smaller than files created using off-the-shelf PPMD, one of the best available compressors",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2029886881",
    "type": "article"
  },
  {
    "title": "Reasoning about a Machine with Local Capabilities",
    "doi": "https://doi.org/10.1145/3363519",
    "publication_date": "2019-12-10",
    "publication_year": 2019,
    "authors": "Lau Skorstengaard; Dominique Devriese; Lars Birkedal",
    "corresponding_authors": "",
    "abstract": "Capability machines provide security guarantees at machine level which makes them an interesting target for secure compilation schemes that provably enforce properties such as control-flow correctness and encapsulation of local state. We provide a formalization of a representative capability machine with local capabilities and study a novel calling convention. We provide a logical relation that semantically captures the guarantees provided by the hardware (a form of capability safety) and use it to prove control-flow correctness and encapsulation of local state. The logical relation is not specific to our calling convention and can be used to reason about arbitrary programs.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2995538315",
    "type": "article"
  },
  {
    "title": "A Dynamic Continuation-Passing Style for Dynamic Delimited Continuations",
    "doi": "https://doi.org/10.1145/2794078",
    "publication_date": "2015-10-16",
    "publication_year": 2015,
    "authors": "Dariusz Biernacki; Olivier Danvy; Kevin Millikin",
    "corresponding_authors": "",
    "abstract": "We put a preexisting definitional abstract machine for dynamic delimited continuations in defunctionalized form, and we present the consequences of this adjustment. We first prove the correctness of the adjusted abstract machine. Because it is in defunctionalized form, we can refunctionalize it into a higher-order evaluation function. This evaluation function, which is compositional, is in continuation+state-passing style and threads a trail of delimited continuations and a meta-continuation. Since this style accounts for dynamic delimited continuations, we refer to it as \"dynamic continuation-passing style\" and we present the corresponding dynamic CPS transformation. We show that the notion of computation induced by dynamic CPS takes the form of a continuation monad with a recursive answer type. This continuation monad suggests a new simulation of dynamic delimited continuations in terms of static ones. Finally, we present new applications of dynamic delimited continuations, including a meta-circular evaluator. The significance of the present work is that the computational artifacts surrounding dynamic CPS are not independent designs: they are mechanical consequences of having put the definitional abstract machine in defunctionalized form.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1992292421",
    "type": "article"
  },
  {
    "title": "SHErrLoc",
    "doi": "https://doi.org/10.1145/3121137",
    "publication_date": "2017-08-17",
    "publication_year": 2017,
    "authors": "Danfeng Zhang; Andrew C. Myers; Dimitrios Vytiniotis; Simon Peyton-Jones",
    "corresponding_authors": "",
    "abstract": "We introduce a general way to locate programmer mistakes that are detected by static analyses. The program analysis is expressed in a general constraint language that is powerful enough to model type checking, information flow analysis, dataflow analysis, and points-to analysis. Mistakes in program analysis result in unsatisfiable constraints. Given an unsatisfiable system of constraints, both satisfiable and unsatisfiable constraints are analyzed to identify the program expressions most likely to be the cause of unsatisfiability. The likelihood of different error explanations is evaluated under the assumption that the programmer’s code is mostly correct, so the simplest explanations are chosen, following Bayesian principles. For analyses that rely on programmer-stated assumptions, the diagnosis also identifies assumptions likely to have been omitted. The new error diagnosis approach has been implemented as a tool called SHErrLoc, which is applied to three very different program analyses, such as type inference for a highly expressive type system implemented by the Glasgow Haskell Compiler—including type classes, Generalized Algebraic Data Types (GADTs), and type families. The effectiveness of the approach is evaluated using previously collected programs containing errors. The results show that when compared to existing compilers and other tools, SHErrLoc consistently identifies the location of programmer errors significantly more accurately, without any language-specific heuristics.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2746727911",
    "type": "article"
  },
  {
    "title": "PYE",
    "doi": "https://doi.org/10.1145/3337794",
    "publication_date": "2019-07-02",
    "publication_year": 2019,
    "authors": "Manas Thakur; V. Krishna Nandivada",
    "corresponding_authors": "",
    "abstract": "Languages like Java and C# follow a two-step process of compilation: static compilation and just-in-time (JIT) compilation. As the time spent in JIT compilation gets added to the execution-time of the application, JIT compilers typically sacrifice the precision of program analyses for efficiency. The alternative of performing the analysis for the whole program statically ignores the analysis of libraries (available only at runtime), and thereby generates imprecise results. To address these issues, in this article, we propose a two-step (static+JIT) analysis framework called precise-yet-efficient (PYE) that helps generate precise analysis-results at runtime at a very low cost. PYE achieves the twin objectives of precision and performance during JIT compilation by using a two-pronged approach: (i) It performs expensive analyses during static compilation, while accounting for the unavailability of the runtime libraries by generating partial results, in terms of conditional values , for the input application. (ii) During JIT compilation, PYE resolves the conditions associated with these values, using the pre-computed conditional values for the libraries, to generate the final results. We have implemented the static and the runtime components of PYE in the Soot optimization framework and the OpenJDK HotSpot Server Compiler (C2), respectively. We demonstrate the usability of PYE by instantiating it to perform two context-, flow-, and field-sensitive heap-based analyses: (i) points-to analysis for null-dereference-check elimination; and (ii) escape analysis for synchronization elimination. We evaluate these instantiations against their corresponding state-of-the-art implementations in C2 over a wide range of benchmarks. The extensive evaluation results show that our strategy works quite well and fulfills both the promises it makes: enhanced precision while maintaining efficiency during JIT compilation.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2955188035",
    "type": "article"
  },
  {
    "title": "Consistent Subtyping for All",
    "doi": "https://doi.org/10.1145/3310339",
    "publication_date": "2019-11-21",
    "publication_year": 2019,
    "authors": "Ningning Xie; Xuan Bi; Bruno C. d. S. Oliveira; Tom Schrijvers",
    "corresponding_authors": "",
    "abstract": "Consistent subtyping is employed in some gradual type systems to validate type conversions. The original definition by Siek and Taha serves as a guideline for designing gradual type systems with subtyping. Polymorphic types à la System F also induce a subtyping relation that relates polymorphic types to their instantiations. However, Siek and Taha’s definition is not adequate for polymorphic subtyping. The first goal of this article is to propose a generalization of consistent subtyping that is adequate for polymorphic subtyping and subsumes the original definition by Siek and Taha. The new definition of consistent subtyping provides novel insights with respect to previous polymorphic gradual type systems, which did not employ consistent subtyping. The second goal of this article is to present a gradually typed calculus for implicit (higher-rank) polymorphism that uses our new notion of consistent subtyping. We develop both declarative and (bidirectional) algorithmic versions for the type system. The algorithmic version employs techniques developed by Dunfield and Krishnaswami for higher-rank polymorphism to deal with instantiation. We prove that the new calculus satisfies all static aspects of the refined criteria for gradual typing. We also study an extension of the type system with static and gradual type parameters, in an attempt to support a variant of the dynamic criterion for gradual typing. Assuming a coherence conjecture for the extended calculus, we show that the dynamic gradual guarantee of our source language can be reduced to that of λ B, which, at the time of writing, is still an open question. Most of the metatheory of this article, except some manual proofs for the algorithmic type system and extensions, has been mechanically formalized using the Coq proof assistant.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2990278710",
    "type": "article"
  },
  {
    "title": "Compiler-Driven Software Speculation for Thread-Level Parallelism",
    "doi": "https://doi.org/10.1145/2821505",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Paraskevas Yiapanis; Gavin Brown; Mikel Luján",
    "corresponding_authors": "",
    "abstract": "Current parallelizing compilers can tackle applications exercising regular access patterns on arrays or affine indices, where data dependencies can be expressed in a linear form. Unfortunately, there are cases that independence between statements of code cannot be guaranteed and thus the compiler conservatively produces sequential code. Programs that involve extensive pointer use, irregular access patterns, and loops with unknown number of iterations are examples of such cases. This limits the extraction of parallelism in cases where dependencies are rarely or never triggered at runtime. Speculative parallelism refers to methods employed during program execution that aim to produce a valid parallel execution schedule for programs immune to static parallelization. The motivation for this article is to review recent developments in the area of compiler-driven software speculation for thread-level parallelism and how they came about. The article is divided into two parts. In the first part the fundamentals of speculative parallelization for thread-level parallelism are explained along with a design choice categorization for implementing such systems. Design choices include the ways speculative data is handled, how data dependence violations are detected and resolved, how the correct data are made visible to other threads, or how speculative threads are scheduled. The second part is structured around those design choices providing the advances and trends in the literature with reference to key developments in the area. Although the focus of the article is in software speculative parallelization, a section is dedicated for providing the interested reader with pointers and references for exploring similar topics such as hardware thread-level speculation, transactional memory, and automatic parallelization.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2274823567",
    "type": "article"
  },
  {
    "title": "Homeostasis: Design and Implementation of a Self-Stabilizing Compiler",
    "doi": "https://doi.org/10.1145/3649308",
    "publication_date": "2024-02-23",
    "publication_year": 2024,
    "authors": "Aman Nougrahiya; V. Krishna Nandivada",
    "corresponding_authors": "",
    "abstract": "Mainstream compilers perform a multitude of analyses and optimizations on the given input program. Each analysis (such as points-to analysis) may generate a program-abstraction (such as points-to graph). Each optimization is typically composed of multiple alternating phases of inspection of such program-abstractions and transformations of the program. Upon transformation of a program, the program-abstractions generated by various analyses may become inconsistent with the modified program. Consequently, the correctness of the downstream inspection (and consequent transformation) phases cannot be ensured until the relevant program-abstractions are stabilized ; that is, the program-abstractions are either invalidated or made consistent with the modified program. In general, the existing compiler frameworks do not perform automated stabilization of the program-abstractions and instead leave it to the compiler pass writers to deal with the complex task of identifying the relevant program-abstractions to be stabilized, the points where the stabilization is to be performed, and the exact procedure of stabilization. In this article, we address these challenges by providing the design and implementation of a novel compiler-design framework called Homeostasis . Homeostasis automatically captures all the program changes performed by each transformation phase, and later, triggers the required stabilization using the captured information, if needed. We also provide a formal description of Homeostasis and a correctness proof thereof. To assess the feasibility of using Homeostasis in compilers of parallel programs, we have implemented our proposed idea in IMOP, a compiler framework for OpenMP C programs. Furthermore, to illustrate the benefits of using Homeostasis , we have implemented a set of standard data-flow passes, and a set of involved optimizations that are used to remove redundant barriers in OpenMP C programs. Implementations of none of these optimizations in IMOP required any additional lines of code for stabilization of the program-abstractions. We present an evaluation in the context of these optimizations and analyses, which demonstrates that Homeostasis is efficient and easy to use.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3166827031",
    "type": "article"
  },
  {
    "title": "Decomposition-based Synthesis for Applying Divide-and-Conquer-like Algorithmic Paradigms",
    "doi": "https://doi.org/10.1145/3648440",
    "publication_date": "2024-02-15",
    "publication_year": 2024,
    "authors": "Ruyi Ji; Yuwei Zhao; Yingfei Xiong; Di Wang; Lu Zhang; Zhenjiang Hu",
    "corresponding_authors": "",
    "abstract": "Algorithmic paradigms such as divide-and-conquer (D&amp;C) are proposed to guide developers in designing efficient algorithms, but it can still be difficult to apply algorithmic paradigms to practical tasks. To ease the usage of paradigms, many research efforts have been devoted to the automatic application of algorithmic paradigms. However, most existing approaches to this problem rely on syntax-based program transformations and thus put significant restrictions on the original program. In this article, we study the automatic application of D&amp;C and several similar paradigms, denoted as D&amp;C-like algorithmic paradigms, and aim to remove the restrictions from syntax-based transformations. To achieve this goal, we propose an efficient synthesizer, named AutoLifter , which does not depend on syntax-based transformations. Specifically, the main challenge of applying algorithmic paradigms is from the large scale of the synthesized programs, and AutoLifter addresses this challenge by applying two novel decomposition methods that do not depend on the syntax of the input program, component elimination and variable elimination , to soundly divide the whole problem into simpler subtasks, each synthesizing a sub-program of the final program and being tractable with existing synthesizers. We evaluate AutoLifter on 96 programming tasks related to six different algorithmic paradigms. AutoLifter solves 82/96 tasks with an average time cost of 20.17 s, significantly outperforming existing approaches.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391840543",
    "type": "article"
  },
  {
    "title": "Gradual C0: Symbolic Execution for Gradual Verification",
    "doi": "https://doi.org/10.1145/3704808",
    "publication_date": "2024-12-05",
    "publication_year": 2024,
    "authors": "Jenna DiVincenzo; Ian McCormack; Conrad Zimmerman; Hemant Gouni; Jacob Gorenburg; Jan-Paul Ramos-Dávila; Mona Zhang; Joshua Sunshine; Éric Tanter; Jonathan Aldrich",
    "corresponding_authors": "",
    "abstract": "Current static verification techniques such as separation logic support a wide range of programs. However, such techniques only support complete and detailed specifications, which places an undue burden on users. To solve this problem, prior work proposed gradual verification, which handles complete, partial, or missing specifications by soundly combining static and dynamic checking. Gradual verification has also been extended to programs that manipulate recursive, mutable data structures on the heap. Unfortunately, this extension does not reward users with decreased dynamic checking as more specifications are written and more static guarantees are made. In fact, all properties are checked dynamically regardless of any static guarantees. Additionally, no full-fledged implementation of gradual verification exists so far, which prevents studying its performance and applicability in practice. We present Gradual C0, the first practicable gradual verifier for recursive heap data structures, which targets C0, a safe subset of C designed for education. Static verifiers supporting separation logic or implicit dynamic frames use symbolic execution for reasoning; so Gradual C0, which extends one such verifier, adopts symbolic execution at its core instead of the weakest liberal precondition approach used in prior work. Our approach addresses technical challenges related to symbolic execution with imprecise specifications, heap ownership, and branching in both program statements and specification formulas. We also deal with challenges related to minimizing insertion of dynamic checks and extensibility to other programming languages beyond C0. Finally, we provide the first empirical performance evaluation of a gradual verifier, and found that on average, Gradual C0 decreases run-time overhead between 7.1 and 40.2% compared to the fully dynamic approach used in prior work (for context, the worst cases for the approach by Wise et al. [ 2020 ] range from 0.1 to 4.5 seconds depending on the benchmark). Further, the worst-case scenarios for performance are predictable and avoidable. This work paves the way towards evaluating gradual verification at scale.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405079879",
    "type": "article"
  },
  {
    "title": "Error repair in shift-reduce parsers",
    "doi": "https://doi.org/10.1145/210184.210193",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "Bruce McKenzie; Corey Yeatman; Lorraine de Vere",
    "corresponding_authors": "",
    "abstract": "Local error repair of strings during CFG parsing requires the insertion and deletion of symbols in the region of a syntax error to produce a string that is error free. Rather than precalculating tables at parser generation time to assist in finding such repairs, this article shows how such repairs can be found during shift-reduce parsing by using the parsing tables themselves. This results in a substantial space saving over methods that require precalculated tables. Furthermore, the article shows how the method can be integrated with lookahead to avoid finding repairs that immediately result in further syntax errors. The article presents the results of experiments on a version of the LALR(1)-based parser generator Bison to which the algorithm was added.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2036111443",
    "type": "article"
  },
  {
    "title": "Implementing signatures for C++",
    "doi": "https://doi.org/10.1145/239912.239922",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Gerald Baumgartner; Vincent F. Russo",
    "corresponding_authors": "",
    "abstract": "We outline the design and detail the implementation of a language extension for abstracting types and for decoupling subtyping and inheritance in C++. This extension gives the user more of the flexibility of dynamic typing while retaining the efficiency and security of static typing. After a brief discussion of syntax and semantics of this language extension and examples of its use, we present and analyze three different implementation techniques: a preprocessor to a C++ compiler, an implementation in the front end of a C++ compiler, and a low-level implementation with back-end support. We follow with an analysis of the performance of the three implementation techniques and show that our extension actually allows subtype polymorphism to be implemented more efficiently than with virtual functions. We conclude with a discussion of the lessons we learned for future programming language design.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2102009438",
    "type": "article"
  },
  {
    "title": "Controlled grammatic ambiguity",
    "doi": "https://doi.org/10.1145/177492.177759",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Mikkel Thorup",
    "corresponding_authors": "Mikkel Thorup",
    "abstract": "A new approach to ambiguity of context-free grammars is presented, and within this approach the LL and LR techniques are generalized to solve the following problems for large classes of ambiguous grammars: The user may control the parser generation so as to get a parser which finds some specific parse trees for the sentences. The generalized LL and LR techniques will still guarantee that the resulting parser accepts all sentences and terminates in linear time on all input.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2008902610",
    "type": "article"
  },
  {
    "title": "Cache behavior of combinator graph reduction",
    "doi": "https://doi.org/10.1145/128861.128867",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "Philip Koopman; Peter Lee; Daniel P. Siewiorek",
    "corresponding_authors": "",
    "abstract": "The results of cache-simulation experiments with an abstract machine for reducing combinator graphs are presented. The abstract machine, called TIGRE, exhibits reduction rates that, for similar kinds of combinator graphs on similar kinds of hardware, compare favorably with previously reported techniques. Furthermore, TIGRE maps easily and efficiently onto standard computer architectures, particularly those that allow a restricted form of self-modifying code. This provides some indication that the conventional \"stored program\" organization of computer systems is not necessarily an inappropriate one for functional programming language implementations. This is not to say, however, that present day computer systems are well equipped to reduce combinator graphs. In particular, the behavior of the cache memory has a significant effect on performance. In order to study and quantify this effect, trace-driven cache simulations of a TIGRE graph reducer running on a reduced instruction-set computer are conducted. The results of these simulations are presented with the following hardware-cache parameters varied: cache size, block size, associativity, memory update policy, and write-allocation policy. To begin with, the cache organization of a commercially available system is used and then the performance sensitivity with respect to variations of each parameter are measured. From the results of the simulation study, a conclusion is made that combinator-graph reduction using TIGRE runs most efficiently when using a cache memory with an allocate-on-write-miss strategy, moderately large block size (preferably with subblock placement), and copy-back memory updates.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2012345517",
    "type": "article"
  },
  {
    "title": "Disjunctive program analysis for algebraic data types",
    "doi": "https://doi.org/10.1145/265943.265966",
    "publication_date": "1997-09-01",
    "publication_year": 1997,
    "authors": "Thomas Jensen",
    "corresponding_authors": "Thomas Jensen",
    "abstract": "We describe how binding-time, data-flow, and strictness analyses for languages with higher-order functions and algebraic data types can be obtained by instantiating a generic program logic and axiomatization of the properties analyzed for. A distinctive feature of the analyses is that disjunctions of program properties are represented exactly. This yields analyses of high precision and provides a logical characterization of abstract interpretations involving tensor products and uniform properties of recursive data structures. An effective method for proving properties of a program based on fixed-point iteration is obtained by grouping logically equivalent formulae of the same type into equivalence classes, obtaining a lattice of properties of that type, and then defining an abstract interpretation over these lattices. We demonstrate this in the case of strictness analysis by proving that the strictness abstract interpretation of a program is the equivalence class containing the strongest property provable of the program in the strictness logic.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2049596743",
    "type": "article"
  },
  {
    "title": "Recognizing substrings of LR(k) languages in linear time",
    "doi": "https://doi.org/10.1145/177492.177768",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Joseph Bates; Arnon Lavie",
    "corresponding_authors": "",
    "abstract": "LR parsing techniques have long been studied as being efficient and powerful methods for processing context-free languages. A linear-time algorithm for recognizing languages representable by LR(k) grammars has long been known. Recognizing substrings of a context-free language is at least as hard as recognizing full strings of the language, since the latter problem easily reduces to the former. In this article we present a linear-time algorithm for recognizing substrings of LR(k) languages, thus showing that the substring recognition problem for these languages is no harder than the full string recognition problem. An interesting data structure, the Forest-Structured Stack, allows the algorithm to track all possible parses of a substring without loosing the efficiency of the original LR parser. We present the algorithm, prove its correctness, analyze its complexity, and mention several applications that have been constructed.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2074411969",
    "type": "article"
  },
  {
    "title": "Fixpoint computation for polyvariant static analyses of higher-order applicative programs",
    "doi": "https://doi.org/10.1145/186025.186037",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "John M. Ashley; Charles Consel",
    "corresponding_authors": "",
    "abstract": "This paper presents an optimized general-purpose algorithm for polyvariant, static analyses of higher-order applicative programs. A polyvariant analysis is a very accurate form of analysis that produces many more abstract descriptions for a program than does a conventional analysis. It may also compute intermediate abstract descriptions that are irrelevant to the final result of the analysis. The optimized algorithm addresses this overhead while preserving the accuracy of the analysis. The algorithm is also parameterized over both the abstract domain and degree of polyvariance. We have implemented an instance of our algorithm and evaluated its performance compared to the unoptimized algorithm. Our implementation runs significantly faster on average than the other algorithm for benchmarks reported here.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1998646634",
    "type": "article"
  },
  {
    "title": "“Maximal-munch” tokenization in linear time",
    "doi": "https://doi.org/10.1145/276393.276394",
    "publication_date": "1998-03-01",
    "publication_year": 1998,
    "authors": "Thomas Reps",
    "corresponding_authors": "Thomas Reps",
    "abstract": "The lexical-analysis (or scanning) phase of a compiler attempts to partition an input string into a sequence of tokens. The convention in most languages is that the input is scanned left to right, and each token identified is a “maximal munch” of the remaining input—the longest prefix of the remaining input that is a token of the language. Although most of the standard compiler textbooks present a way to perform maximal-munch tokenization, the algorithm they describe is one that, for certain sets of token definitions, can cause the scanner to exhibit quadratic behavior in the worst case. In the article, we show that maximal-munch tokenization can always be performed in time linear in the size of the input.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2033393158",
    "type": "article"
  },
  {
    "title": "Collecting interpretations of expressions",
    "doi": "https://doi.org/10.1145/103135.103139",
    "publication_date": "1991-04-01",
    "publication_year": 1991,
    "authors": "Paul Hudak; Jonathan M. Young",
    "corresponding_authors": "",
    "abstract": "A collecting interpretation of expressions is an interpretation of a program that allows one to answer questions of the sort: “What are all possible values to which an expression might evaluate during program execution?” Answering such questions in a denotational framework is akin to traditional data flow analysis and, when used in the context of abstract interpretation, allows one to infer properties that approximate the run-time behavior of expression evaluation. Exact collecting interpretations of expressions are developed for three abstract functional languages: a strict first-order language, a nonstrict first-order language, and a nonstrict higher order language (the full untyped lambda calculus with constants). It is argued that the method is simple (in particular, no powerdomains are needed), Natural (it captures the intuitive operational behavior of a cache), yet more expressive than existing methods (it is the first exact collecting interpretation for either nonstrict higher order languages). Correctness of the interpretations with respect to the standard semantics is shown via a generalization of the notion of strictness. It is further shown how to form abstractions of these exact interpretations, using as an example a collecting strictness analysis which yields compile-time information not previously captured by conventional strictness analyses.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2063337452",
    "type": "article"
  },
  {
    "title": "Incremental dynamic semantics for language-based programming environments",
    "doi": "https://doi.org/10.1145/63264.63400",
    "publication_date": "1989-04-01",
    "publication_year": 1989,
    "authors": "Gail E. Kaiser",
    "corresponding_authors": "Gail E. Kaiser",
    "abstract": "Attribute grammars are a formal notation for expressing the static semantics of programming languages—those properties that can be derived from inspection of the program text. Attribute grammars have become popular as a mechanism for generating language-based programming environments that incrementally perform symbol resolution, type checking, code generation, and derivation of other static semantic properties as the program is modified. However, attribute grammars are not suitable for expressing dynamic semantics—those properties that reflect the history of program execution and/or user interactions with the programming environment. This paper presents action equations , an extension of attribute grammars suitable for specifying the static and the dynamic semantics of programming languages. It describes how action equations can be used to generate language-based programming environments that incrementally derive static and dynamic properties as the user modifies and debugs th e program.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2008025193",
    "type": "article"
  },
  {
    "title": "Type elaboration and subtype completion for Java bytecode",
    "doi": "https://doi.org/10.1145/383043.383045",
    "publication_date": "2001-03-01",
    "publication_year": 2001,
    "authors": "Todd B. Knoblock; Jakob Rehof",
    "corresponding_authors": "",
    "abstract": "Java source code is strongly typed, but the translation from Java source to bytecode omits much of the type information originally contained within methods. Type elaboration is a technique for reconstructing strongly typed programs from incompletely typed bytecode by inferring types for local variables. There are situations where, technically, there are not enough types in the original type hierarchy to type a bytecode program. Subtype completion is a technique for adding necessary types to an arbitrary type hierarchy to make type elaboration possible for all verifiable Java bytecode. Type elaboration with subtype completion has been implemented as part of the Marmot Java compiler.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2041466367",
    "type": "article"
  },
  {
    "title": "Rank 2 intersection types for local definitions and conditional expressions",
    "doi": "https://doi.org/10.1145/778559.778560",
    "publication_date": "2003-07-01",
    "publication_year": 2003,
    "authors": "Ferruccio Damiani",
    "corresponding_authors": "Ferruccio Damiani",
    "abstract": "We propose a rank 2 intersection type system with new typing rules for local definitions (let-expressions and letrec-expressions) and conditional expressions (if-expressions and match-expressions). This is a further step towards the use of intersection types in \"real\" programming languages.The technique for typing local definitions relies entirely on the principal typing property (i.e. it does not depend on particulars of rank 2 intersection), so it can be applied to any system with principal typings. The technique for typing conditional expressions, which is based on the idea of introducing metrics on types to \"limit the use\" of the intersection type constructor in the types assigned to the branches of the conditionals, is instead tailored to rank 2 intersection. However, the underlying idea might also be useful for other type systems.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2065693797",
    "type": "article"
  },
  {
    "title": "A note on Hennessy's “symbolic debugging of optimized code”",
    "doi": "https://doi.org/10.1145/2363.215005",
    "publication_date": "1985-01-02",
    "publication_year": 1985,
    "authors": "David Wall; Amitabh Srivastava; Fred Templin",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A note on Hennessy's “symbolic debugging of optimized code” Authors: David Wall Computer Science Department, Whitmore Laboratory, The Pennsylvania State University, University Park, PA Computer Science Department, Whitmore Laboratory, The Pennsylvania State University, University Park, PAView Profile , Amitabh Srivastava Computer Science Department, Whitmore Laboratory, The Pennsylvania State University, University Park, PA Computer Science Department, Whitmore Laboratory, The Pennsylvania State University, University Park, PAView Profile , Fred Templin Computer Science Department, Whitmore Laboratory, The Pennsylvania State University, University Park, PA Computer Science Department, Whitmore Laboratory, The Pennsylvania State University, University Park, PAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 7Issue 1pp 176–181https://doi.org/10.1145/2363.215005Published:02 January 1985Publication History 14citation167DownloadsMetricsTotal Citations14Total Downloads167Last 12 Months14Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2022537085",
    "type": "article"
  },
  {
    "title": "A Syntax-Error-Handling Technique and Its Experimental Analysis",
    "doi": "https://doi.org/10.1145/69575.357232",
    "publication_date": "1983-10-01",
    "publication_year": 1983,
    "authors": "Seppo Sippu; Eljas Soisalon-Soininen",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A Syntax-Error-Handling Technique and Its Experimental Analysis Authors: Seppo Sippu Department of Mathematics and Physics, University of Joensuu, P. O. Box 111, SF-80101 Joensuu 10, Finland Department of Mathematics and Physics, University of Joensuu, P. O. Box 111, SF-80101 Joensuu 10, FinlandView Profile , Eljas Soisalon-Soininen Institut für Angewandte Informatik und Formale Beschreibungsverfahren, Universität Karlsruhe, Postfach 6380, 7500 Karlsruhe 1, West Germany Institut für Angewandte Informatik und Formale Beschreibungsverfahren, Universität Karlsruhe, Postfach 6380, 7500 Karlsruhe 1, West GermanyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 4Oct. 1983 pp 656–679https://doi.org/10.1145/69575.357232Online:01 October 1983Publication History 13citation602DownloadsMetricsTotal Citations13Total Downloads602Last 12 Months25Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2031788245",
    "type": "article"
  },
  {
    "title": "Tailored-List and Recombination-Delaying Buddy Systems",
    "doi": "https://doi.org/10.1145/357233.357239",
    "publication_date": "1984-01-01",
    "publication_year": 1984,
    "authors": "Arie Kaufman",
    "corresponding_authors": "Arie Kaufman",
    "abstract": "article Free Access Share on Tailored-List and Recombination-Delaying Buddy Systems Author: Arie Kaufman Department of Mathematics and Computer Science, Ben-Gurion University of the Negev, P.O.B. 653, Beersheva 84 105, Israel Department of Mathematics and Computer Science, Ben-Gurion University of the Negev, P.O.B. 653, Beersheva 84 105, IsraelView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 101 January 1984pp 118–125https://doi.org/10.1145/357233.357239Published:01 January 1984Publication History 18citation349DownloadsMetricsTotal Citations18Total Downloads349Last 12 Months16Last 6 weeks6 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2053596004",
    "type": "article"
  },
  {
    "title": "Transformations and reduction strategies for typed lambda expressions",
    "doi": "https://doi.org/10.1145/1780.1803",
    "publication_date": "1984-10-01",
    "publication_year": 1984,
    "authors": "Michael Georgeff",
    "corresponding_authors": "Michael Georgeff",
    "abstract": "article Free Access Share on Transformations and reduction strategies for typed lambda expressions Author: Michael Georgeff SRI International SRI InternationalView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 4Oct. 1984 pp 603–631https://doi.org/10.1145/1780.1803Online:01 October 1984Publication History 11citation339DownloadsMetricsTotal Citations11Total Downloads339Last 12 Months10Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1992258859",
    "type": "article"
  },
  {
    "title": "On convergence toward a database of program transformations",
    "doi": "https://doi.org/10.1145/2363.2364",
    "publication_date": "1985-01-02",
    "publication_year": 1985,
    "authors": "D Barstow",
    "corresponding_authors": "D Barstow",
    "abstract": "Several fairly large sets of programming rules have been developed recently. It is natural to ask whether the process of developing such rule bases may converge. Having developed sets of rules for specific programming tasks and domains, will they be helpful when other tasks and domains are considered? While it is too early to give definitive answers, experience with the rules of the PECOS system has been positive. Both during the process of developing the rule set and while developing rules for another domain, the existence of already codified rules proved very helpful.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2001847301",
    "type": "article"
  },
  {
    "title": "Process Communication Based on Input Specifications",
    "doi": "https://doi.org/10.1145/357139.357141",
    "publication_date": "1981-07-01",
    "publication_year": 1981,
    "authors": "Jan van den Bos; Rinus Plasmeijer; Jan Stroet",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Process Communication Based on Input Specifications Authors: Jan van den Bos Nijmegen University, Informatica/Computer Graphics, Nijmegen, The Netherlands Nijmegen University, Informatica/Computer Graphics, Nijmegen, The NetherlandsView Profile , R. Plasmeijer Nijmegen University, Informatica/Computer Graphics, Nijmegen, The Netherlands Nijmegen University, Informatica/Computer Graphics, Nijmegen, The NetherlandsView Profile , Jan W. M. Stroet Nijmegen University, Informatica/Computer Graphics, Nijmegen, The Netherlands Nijmegen University, Informatica/Computer Graphics, Nijmegen, The NetherlandsView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 3July 1981 pp 224–250https://doi.org/10.1145/357139.357141Published:01 July 1981Publication History 13citation305DownloadsMetricsTotal Citations13Total Downloads305Last 12 Months16Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1972652085",
    "type": "article"
  },
  {
    "title": "Quantified types in an imperative language",
    "doi": "https://doi.org/10.1145/1133651.1133653",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Dan Grossman",
    "corresponding_authors": "Dan Grossman",
    "abstract": "We describe universal types, existential types, and type constructors in Cyclone, a strongly typed C-like language. We show how the language naturally supports first-class polymorphism and polymorphic recursion while requiring an acceptable amount of explicit type information. More importantly, we consider the soundness of type variables in the presence of C-style mutation and the address-of operator. For polymorphic references, we describe a solution more natural for the C level than the ML-style “value restriction.” For existential types, we discover and subsequently avoid a subtle unsoundness issue resulting from the address-of operator. We develop a formal abstract machine and type-safety proof that capture the essence of type variables at the C level.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1993794314",
    "type": "article"
  },
  {
    "title": "Design of a Machine-Independent Optimizing System for Emulator Development",
    "doi": "https://doi.org/10.1145/357094.357102",
    "publication_date": "1980-04-01",
    "publication_year": 1980,
    "authors": "Perng-Ti Ma; T. Lewis",
    "corresponding_authors": "",
    "abstract": "Methods are described to translate a certain machine-independent intermediate language (IML) to efficient microprograms for a class of horizontal microprogrammable machines. The IML is compiled directly from a high-level microprogramming language used to implement a virtual instruction set processor as a microprogram. The primary objective of the IML-to-host machine interface design is to facilitate language portability. Transportability is accomplished by use of a field description model and a macro expansion table which describe the host machine to the translator system. Register allocation scheme and control flow analysis are employed to allocate the symbolic variables of the IML to the general-purpose registers of the host machine. A set of 5-tuple microoperations (function, input, output, field, phase) is obtained with the aid of the field description model. Then a compaction algorithm is used to detect the parallelism of microoperations and to generate suboptimal code for a horizontal microprogrammable machine. The study concludes with a description of the effects of the above methods upon the quality of microcode produced for a specific commercial computer.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2001461905",
    "type": "article"
  },
  {
    "title": "Pretty printing with lazy dequeues",
    "doi": "https://doi.org/10.1145/1053468.1053473",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Olaf Chitil",
    "corresponding_authors": "Olaf Chitil",
    "abstract": "There are several purely functional libraries for converting tree structured data into indented text, but they all make use of some backtracking. Over twenty years ago, Oppen published a more efficient imperative implementation of a pretty printer. This article shows that the same efficiency is also obtainable without destructive updates by developing a similar but purely functional Haskell implementation with the same complexity bounds. At its heart lie two lazy double ended queues.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2002988345",
    "type": "article"
  },
  {
    "title": "Is Sometimes Ever Better Than Always?",
    "doi": "https://doi.org/10.1145/357073.357080",
    "publication_date": "1979-10-01",
    "publication_year": 1979,
    "authors": "David Gries",
    "corresponding_authors": "David Gries",
    "abstract": "The “intermittent assertion” method for proving programs correct is explained and compared with the conventional method. Simple conventional proofs of iterative algorithms that compute recursively defined functions, including Ackermann's function, are given.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2133177916",
    "type": "article"
  },
  {
    "title": "Specification of Abstract Data Types in Modula",
    "doi": "https://doi.org/10.1145/357114.357117",
    "publication_date": "1980-10-01",
    "publication_year": 1980,
    "authors": "George W. Ernst; William F. Ogden",
    "corresponding_authors": "",
    "abstract": "The programming language MODULA is extended to permit the formal specification of the structure and functional capabilities of modules. This makes true hierarchical programming possible in MODULA by allowing programmers of higher level parts of a system to ignore completely the internal structure of lower level modules and to rely entirely on the specifications of the capabilities of these modules. An example is included to illustrate this technique. We show that our specification mechanisms are sufficiently powerful to support formal verification rules for modules that have disjoint representations for abstract objects.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2014475417",
    "type": "article"
  },
  {
    "title": "Birrell's distributed reference listing revisited",
    "doi": "https://doi.org/10.1145/1108970.1108976",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Luc Moreau; Peter Dickman; Richard Jones",
    "corresponding_authors": "",
    "abstract": "The Java RMI collector is arguably the most widely used distributed garbage collector. Its distributed reference listing algorithm was introduced by Birrell et al. in the context of Network Objects, where the description was informal and heavily biased toward implementation. In this article, we formalize this algorithm in an implementation-independent manner, which allows us to clarify weaknesses of the initial presentation. In particular, we discover cases critical to the correctness of the algorithm that were not accounted for by Birrell. We use our formalization to derive an invariant-based proof of correctness of the algorithm that avoids notoriously difficult temporal reasoning. Furthermore, we offer a novel graphical representation of the state transition diagram, which we use to provide intuitive explanations of the algorithm and to investigate its tolerance to faults in a systematic manner. Finally, we examine how the algorithm may be optimized, either by placing constraints on message channels or by tightening the coupling between the application program and distributed garbage collector.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2031195450",
    "type": "article"
  },
  {
    "title": "On Parsing and Compiling Arithmetic Expressions on Vector Computers",
    "doi": "https://doi.org/10.1145/357094.357099",
    "publication_date": "1980-04-01",
    "publication_year": 1980,
    "authors": "Charles N. Fischer",
    "corresponding_authors": "Charles N. Fischer",
    "abstract": "The problem of parsing and compiling arithmetic expressions on vector computers is considered. Methods are developed which allow encodings of one or more arithmetic expressions to be transformed directly into encodings of their corresponding derivation trees. The algorithm which performs this transformation is compact, efficient, and able to make extensive use of concurrent vector operations. Routines which concurrently transverse encoded derivation trees in a top-down or bottom-up manner are presented. These routines can be used to structure efficient, compact, and highly concurrent algorithms which complete the process of compiling arithmetic expressions.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2032407593",
    "type": "article"
  },
  {
    "title": "Implicit-signal monitors",
    "doi": "https://doi.org/10.1145/1108970.1108975",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Peter A. Buhr; Ashif S. Harji",
    "corresponding_authors": "",
    "abstract": "An implicit (automatic) signal monitor uses a waituntil predicate statement to construct synchronization, as opposed to an explicit-signal monitor using condition variables and signal/wait statements for synchronization. Of the two synchronization approaches, the implicit-signal monitor is often easier to use and prove correct, but has an inherently high execution cost. Hence, its primary use is for prototyping concurrent systems using monitors, where speed and accuracy of software development override execution performance. After a concurrent system is working, any implicit-signal monitor that is a performance bottleneck can be converted to an explicit-signal monitor. Unfortunately, many monitor-based concurrency systems provide only explicit-signal monitors, precluding the design benefits of implicit-signal monitors.This article presents a historical look at the development of the implicit-signal monitor in relation to its counterpart the explicit-signal monitor. An analysis of the different kinds of implicit-signal monitors shows the effects certain design decisions have on the problems that can be solved and the performance of the solutions. Finally, an extensive discussion is presented on simulating an implicit-signal monitor via different explicit-signal monitors. These simulations are reasonably complex, depending on the kind of explicit-signal monitor available for the simulation and the desired semantics required for the implicit-signal monitor. Interestingly, the complexity of the simulations also illustrates certain deficiencies with explicit-signal monitors, which are discussed in detail. Performance comparisons are made among the different simulations with monitors from the concurrent systems PThreads, Java, and μC++.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2140249069",
    "type": "article"
  },
  {
    "title": "Chaining Span-Dependent Jump Instructions",
    "doi": "https://doi.org/10.1145/357103.357105",
    "publication_date": "1980-07-01",
    "publication_year": 1980,
    "authors": "Bruce W. Leverett; Thomas G. Szymanski",
    "corresponding_authors": "",
    "abstract": "The assembled length of a span-dependent jump instruction depends on the distance between the instruction and its target. Such instructions are found on many computers and typically have two forms, long and short. We consider the problem of minimizing object program length for such machines by chaining together jumps with the same target. Although the problem is NP-complete in its most general form, several mildly restricted forms of the problem exist that are of practical importance and have efficient solutions.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2142601667",
    "type": "article"
  },
  {
    "title": "A static type system for JVM access control",
    "doi": "https://doi.org/10.1145/1180475.1180479",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Tomoyuki Higuchi; Atsushi Ohori",
    "corresponding_authors": "",
    "abstract": "This article presents a static type system for the Java virtual machine (JVM) code that enforces an access control mechanism similar to that found in a Java implementation. In addition to verifying type consistency of a given JVM code, the type system statically verifies whether the code accesses only those resources that are granted by the prescribed access policy. The type system is proved to be sound with respect to an operational semantics that enforces access control dynamically, similar to Java stack inspection. This result ensures that “well-typed code cannot violate access policy.” The authors then develop a type inference algorithm and show that it is sound with respect to the type system. These results allow us to develop a static system for JVM access control, without resorting to costly runtime stack inspection.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2033161285",
    "type": "article"
  },
  {
    "title": "A step towards unifying schedule and storage optimization",
    "doi": "https://doi.org/10.1145/1286821.1286825",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "William Thies; Frédéric Vivien; Saman Amarasinghe",
    "corresponding_authors": "",
    "abstract": "We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find a good storage mapping for a given schedule, a good schedule for a given storage mapping, and a good storage mapping that is valid for all legal (one-dimensional affine) schedules. We consider storage mappings that collapse one dimension of a multidimensional array, and programs that are in a single assignment form and accept a one-dimensional affine schedule. Our method combines affine scheduling techniques with occupancy vector analysis and incorporates general affine dependences across statements and loop nests. We formulate the constraints imposed by the data dependences and storage mappings as a set of linear inequalities, and apply numerical programming techniques to solve for the shortest occupancy vector. We consider our method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2064375932",
    "type": "article"
  },
  {
    "title": "On the complexity of partially-flow-sensitive alias analysis",
    "doi": "https://doi.org/10.1145/1353445.1353447",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Noam Rinetzky; G. Ramalingam; Mooly Sagiv; Eran Yahav",
    "corresponding_authors": "",
    "abstract": "We introduce the notion of a partially -flow-sensitive analysis based on the number of read and write operations that are guaranteed to be analyzed in a sequential manner. We study the complexity of partially-flow-sensitive alias analysis and show that precise alias analysis with a very limited flow-sensitivity is as hard as precise flow-sensitive alias analysis, both when dynamic memory allocation is allowed, as well as in the absence of dynamic memory allocation.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2153716894",
    "type": "article"
  },
  {
    "title": "Feature-Specific Profiling",
    "doi": "https://doi.org/10.1145/3275519",
    "publication_date": "2018-12-19",
    "publication_year": 2018,
    "authors": "Leif Andersen; Vincent St-Amour; Jan Vítek; Matthias Felleisen",
    "corresponding_authors": "",
    "abstract": "While high-level languages come with significant readability and maintainability benefits, their performance remains difficult to predict. For example, programmers may unknowingly use language features inappropriately, which cause their programs to run slower than expected. To address this issue, we introduce feature-specific profiling , a technique that reports performance costs in terms of linguistic constructs. Feature-specific profilers help programmers find expensive uses of specific features of their language. We describe the architecture of a profiler that implements our approach, explain prototypes of the profiler for two languages with different characteristics and implementation strategies, and provide empirical evidence for the approach’s general usefulness as a performance debugging tool.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1007123516",
    "type": "article"
  },
  {
    "title": "Deferring design pattern decisions and automating structural pattern changes using a design-pattern-based programming system",
    "doi": "https://doi.org/10.1145/1498926.1498927",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Steve MacDonald; Kai Tan; Jonathan Schaeffer; Duane Szafron",
    "corresponding_authors": "",
    "abstract": "In the design phase of software development, the designer must make many fundamental design decisions concerning the architecture of the system. Incorrect decisions are relatively easy and inexpensive to fix if caught during the design process, but the difficulty and cost rise significantly if problems are not found until after coding begins. Unfortunately, it is not always possible to find incorrect design decisions during the design phase. To reduce the cost of expensive corrections, it would be useful to have the ability to defer some design decisions as long as possible, even into the coding stage. Failing that, tool support for automating design changes would give more freedom to revisit and change these decisions when needed. This article shows how a design-pattern-based programming system based on generative design patterns can support the deferral of design decisions where possible, and automate changes where necessary. A generative design pattern is a parameterized pattern form that is capable of generating code for different versions of the underlying design pattern. We demonstrate these ideas in the context of a parallel application written with the CO 2 P 3 S pattern-based parallel programming system. We show that CO 2 P 3 S can defer the choice of execution architecture (shared-memory or distributed-memory), and can automate several changes to the application structure that would normally be daunting to tackle late in the development cycle. Although we have done this work with a pattern-based parallel programming system, it can be generalized to other domains.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1996180635",
    "type": "article"
  },
  {
    "title": "Santa Claus",
    "doi": "https://doi.org/10.1145/1734206.1734211",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Peter H. Welch; Jan Bækgaard Pedersen",
    "corresponding_authors": "",
    "abstract": "With the commercial development of multicore processors, the challenges of writing multithreaded programs to take advantage of these new hardware architectures are becoming more and more pertinent. Concurrent programming is necessary to achieve the performance that the hardware offers. Traditional approaches present concurrency as an advanced topic: they have proven difficult to use, reason about with confidence, and scale up to high levels of concurrency. This article reviews process-oriented design , based on Hoare's algebra of Communicating Sequential Processes (CSP), and proposes that this approach to concurrency leads to solutions that are manageable by novice programmers; that is, they are easy to design and maintain, that they are scalable for complexity, obviously correct , and relatively easy to verify using formal reasoning and/or model checkers. These solutions can be developed in conventional programming languages (through CSP libraries) or specialized ones (such as occam-π) in a manner that directly reflects their formal expression. Systems can be developed without needing specialist knowledge of the CSP formalism, since the supporting mathematics is burnt into the tools and languages supporting it. We illustrate these concepts with the Santa Claus problem , which has been used as a challenge for concurrency mechanisms since 1994. We consider this problem as an example control system, producing external signals reporting changes of internal state (that model the external world). We claim our occam-π solution is correct-by-design , but follow this up with formal verification (using the FDR model checker for CSP) that the system is free from deadlock and livelock, that the produced control signals obey crucial ordering constraints, and that the system has key liveness properties.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2019364461",
    "type": "article"
  },
  {
    "title": "Self-stabilization preserving compiler",
    "doi": "https://doi.org/10.1145/1552309.1552312",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Shlomi Dolev; Yinnon Haviv; Mooly Sagiv",
    "corresponding_authors": "",
    "abstract": "Self-stabilization is an elegant approach for designing fault tolerant systems. A system is considered self-stabilizing if, starting in any state, it converges to the desired behavior. Self-stabilizing algorithms were designed for solving fundamental distributed tasks, such as leader election, token circulation and communication network protocols. The algorithms were expressed using guarded commands or pseudo-code. The realization of these algorithms requires the existence of a (self-stabilizing) infrastructure such as a self-stabilizing microprocessor and a self-stabilizing operating system for their execution. Moreover, the high-level description of the algorithms needs to be converted into machine language of the microprocessor. In this article, we present our design for a self-stabilization preserving compiler. The compiler we designed and implemented transforms programs written in a language similar to the abstract state machine (ASM). The compiler preserves the stabilization property of the high level program.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2053434709",
    "type": "article"
  },
  {
    "title": "Interconnectability of Session-Based Logical Processes",
    "doi": "https://doi.org/10.1145/3242173",
    "publication_date": "2018-12-13",
    "publication_year": 2018,
    "authors": "Bernardo Toninho; Nobuko Yoshida",
    "corresponding_authors": "",
    "abstract": "In multiparty session types, interconnection networks identify which roles in a session engage in communication (i.e., two roles are connected if they exchange a message). In session-based interpretations of linear logic the analogue notion corresponds to determining which processes are composed, or cut, using compatible channels typed by linear propositions. In this work, we show that well-formed interactions represented in a session-based interpretation of classical linear logic (CLL) form strictly less-expressive interconnection networks than those of a multiparty session calculus. To achieve this result, we introduce a new compositional synthesis property dubbed partial multiparty compatibility (PMC), enabling us to build a global type denoting the interactions obtained by iterated composition of well-typed CLL threads. We then show that CLL composition induces PMC global types without circular interconnections between three (or more) participants. PMC is then used to define a new CLL composition rule that can form circular interconnections but preserves the deadlock-freedom of CLL.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2904348875",
    "type": "article"
  },
  {
    "title": "Scaling Reliably",
    "doi": "https://doi.org/10.1145/3107937",
    "publication_date": "2017-08-17",
    "publication_year": 2017,
    "authors": "Phil Trinder; Natalia Chechina; Nikolaos Papaspyrou; Konstantinos Sagonas; Simon Thompson; Stephen Adams; Stavros Aronis; Robert Baker; Eva Bihari; Olivier Boudeville; Francesco Cesarini; Maurizio Di Stefano; Sverker Eriksson; Viktória Fördős; Amir Ghaffari; Aggelos Giantsios; Rickard Green; Csaba Hoch; David Klaftenegger; Huiqing Li; Kenneth Lundin; Kenneth MacKenzie; Katerina Roukounaki; Yiannis Tsiouris; Kjell Winblad",
    "corresponding_authors": "",
    "abstract": "Distributed actor languages are an effective means of constructing scalable reliable systems, and the Erlang programming language has a well-established and influential model. While the Erlang model conceptually provides reliable scalability, it has some inherent scalability limits and these force developers to depart from the model at scale. This article establishes the scalability limits of Erlang systems and reports the work of the EU RELEASE project to improve the scalability and understandability of the Erlang reliable distributed actor model. We systematically study the scalability limits of Erlang and then address the issues at the virtual machine, language, and tool levels. More specifically: (1) We have evolved the Erlang virtual machine so that it can work effectively in large-scale single-host multicore and NUMA architectures. We have made important changes and architectural improvements to the widely used Erlang/OTP release. (2) We have designed and implemented Scalable Distributed (SD) Erlang libraries to address language-level scalability issues and provided and validated a set of semantics for the new language constructs. (3) To make large Erlang systems easier to deploy, monitor, and debug, we have developed and made open source releases of five complementary tools, some specific to SD Erlang. Throughout the article we use two case studies to investigate the capabilities of our new technologies and tools: a distributed hash table based Orbit calculation and Ant Colony Optimisation (ACO). Chaos Monkey experiments show that two versions of ACO survive random process failure and hence that SD Erlang preserves the Erlang reliability model. While we report measurements on a range of NUMA and cluster architectures, the key scalability experiments are conducted on the Athos cluster with 256 hosts (6,144 cores). Even for programs with no global recovery data to maintain, SD Erlang partitions the network to reduce network traffic and hence improves performance of the Orbit and ACO benchmarks above 80 hosts. ACO measurements show that maintaining global recovery data dramatically limits scalability; however, scalability is recovered by partitioning the recovery data. We exceed the established scalability limits of distributed Erlang, and do not reach the limits of SD Erlang for these benchmarks at this scale (256 hosts, 6,144 cores).",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2609404595",
    "type": "article"
  },
  {
    "title": "A Simple, Possibly Correct LR Parser for C11",
    "doi": "https://doi.org/10.1145/3064848",
    "publication_date": "2017-09-18",
    "publication_year": 2017,
    "authors": "Jacques-Henri Jourdan; François Pottier",
    "corresponding_authors": "",
    "abstract": "The syntax of the C programming language is described in the C11 standard by an ambiguous context-free grammar, accompanied with English prose that describes the concept of “scope” and indicates how certain ambiguous code fragments should be interpreted. Based on these elements, the problem of implementing a compliant C11 parser is not entirely trivial. We review the main sources of difficulty and describe a relatively simple solution to the problem. Our solution employs the well-known technique of combining an LALR(1) parser with a “lexical feedback” mechanism. It draws on folklore knowledge and adds several original aspects, including a twist on lexical feedback that allows a smooth interaction with lookahead; a simplified and powerful treatment of scopes; and a few amendments in the grammar. Although not formally verified, our parser avoids several pitfalls that other implementations have fallen prey to. We believe that its simplicity, its mostly declarative nature, and its high similarity with the C11 grammar are strong informal arguments in favor of its correctness. Our parser is accompanied with a small suite of “tricky” C11 programs. We hope that it may serve as a reference or a starting point in the implementation of compilers and analysis tools.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2754378868",
    "type": "article"
  },
  {
    "title": "Program Synthesis for Program Analysis",
    "doi": "https://doi.org/10.1145/3174802",
    "publication_date": "2018-05-28",
    "publication_year": 2018,
    "authors": "Cristina David; Pascal Kesseli; Daniel Kroening; Matt Lewis",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a unified framework for designing static analysers based on program synthesis . For this purpose, we identify a fragment of second-order logic with restricted quantification that is expressive enough to model numerous static analysis problems (e.g., safety proving, bug finding, termination and non-termination proving, refactoring). As our focus is on programs that use bit-vectors, we build a decision procedure for this fragment over finite domains in the form of a program synthesiser. We provide instantiations of our framework for solving a diverse range of program verification tasks such as termination, non-termination, safety and bug finding, superoptimisation, and refactoring. Our experimental results show that our program synthesiser compares positively with specialised tools in each area as well as with general-purpose synthesisers.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2784646433",
    "type": "article"
  },
  {
    "title": "Analysis and Optimization of Task Granularity on the Java Virtual Machine",
    "doi": "https://doi.org/10.1145/3338497",
    "publication_date": "2019-07-16",
    "publication_year": 2019,
    "authors": "Andrea Rosà; Eduardo Rosales; Walter Binder",
    "corresponding_authors": "",
    "abstract": "Task granularity, i.e., the amount of work performed by parallel tasks, is a key performance attribute of parallel applications. On the one hand, fine-grained tasks (i.e., small tasks carrying out few computations) may introduce considerable parallelization overheads. On the other hand, coarse-grained tasks (i.e., large tasks performing substantial computations) may not fully utilize the available CPU cores, leading to missed parallelization opportunities. In this article, we provide a better understanding of task granularity for task-parallel applications running on a single Java Virtual Machine in a shared-memory multicore. We present a new methodology to accurately and efficiently collect the granularity of each executed task, implemented in a novel profiler (available open-source) that collects carefully selected metrics from the whole system stack with low overhead, and helps developers locate performance and scalability problems. We analyze task granularity in the DaCapo, ScalaBench, and Spark Perf benchmark suites, revealing inefficiencies related to fine-grained and coarse-grained tasks in several applications. We demonstrate that the collected task-granularity profiles are actionable by optimizing task granularity in several applications, achieving speedups up to a factor of 5.90×. Our results highlight the importance of analyzing and optimizing task granularity on the Java Virtual Machine.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2909382770",
    "type": "article"
  },
  {
    "title": "Practical Subtyping for Curry-Style Languages",
    "doi": "https://doi.org/10.1145/3285955",
    "publication_date": "2019-02-28",
    "publication_year": 2019,
    "authors": "Rodolphe Lepigre; Christophe Raffalli",
    "corresponding_authors": "",
    "abstract": "We present a new, syntax-directed framework for Curry-style type systems with subtyping. It supports a rich set of features, and allows for a reasonably simple theory and implementation. The system we consider has sum and product types, universal and existential quantifiers, and inductive and coinductive types. The latter two may carry size invariants that can be used to establish the termination of recursive programs. For example, the termination of quicksort can be derived by showing that partitioning a list does not increase its size. The system deals with complex programs involving mixed induction and coinduction, or even mixed polymorphism and (co-)induction. One of the key ideas is to separate the notion of size from recursion. We do not check the termination of programs directly, but rather show that their (circular) typing proofs are well-founded. Termination is then obtained using a standard (semantic) normalisation proof. To demonstrate the practicality of the system, we provide an implementation accepting all the examples discussed in the article.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2918429258",
    "type": "article"
  },
  {
    "title": "A Theory of Slicing for Imperative Probabilistic Programs",
    "doi": "https://doi.org/10.1145/3372895",
    "publication_date": "2020-04-17",
    "publication_year": 2020,
    "authors": "Torben Amtoft; Anindya Banerjee",
    "corresponding_authors": "",
    "abstract": "Dedicated to the memory of Sebastian Danicic. We present a theory for slicing imperative probabilistic programs containing random assignments and “observe” statements for conditioning. We represent such programs as probabilistic control-flow graphs (pCFGs) whose nodes modify probability distributions. This allows direct adaptation of standard machinery such as data dependence, postdominators, relevant variables, and so on, to the probabilistic setting. We separate the specification of slicing from its implementation: (1) first, we develop syntactic conditions that a slice must satisfy (they involve the existence of another disjoint slice such that the variables of the two slices are probabilistically independent of each other); (2) next, we prove that any such slice is semantically correct; (3) finally, we give an algorithm to compute the least slice. To generate smaller slices, we may in addition take advantage of knowledge that certain loops will terminate (almost) always. Our results carry over to the slicing of structured imperative probabilistic programs, as handled in recent work by Hur et al. For such a program, we can define its slice, which has the same “normalized” semantics as the original program; the proof of this property is based on a result proving the adequacy of the semantics of pCFGs w.r.t. the standard semantics of structured imperative probabilistic programs.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3022934173",
    "type": "article"
  },
  {
    "title": "Robustly Safe Compilation, an Efficient Form of Secure Compilation",
    "doi": "https://doi.org/10.1145/3436809",
    "publication_date": "2021-02-09",
    "publication_year": 2021,
    "authors": "Marco Patrignani; Deepak Garg",
    "corresponding_authors": "",
    "abstract": "Security-preserving compilers generate compiled code that withstands target-level attacks such as alteration of control flow, data leaks, or memory corruption. Many existing security-preserving compilers are proven to be fully abstract, meaning that they reflect and preserve observational equivalence. Fully abstract compilation is strong and useful but, in certain cases, comes at the cost of requiring expensive runtime constructs in compiled code. These constructs may have no relevance for security, but are needed to accommodate differences between the source and target languages that fully abstract compilation necessarily needs. As an alternative to fully abstract compilation, this article explores a different criterion for secure compilation called robustly safe compilation or RSC . Briefly, this criterion means that the compiled code preserves relevant safety properties of the source program against all adversarial contexts interacting with the compiled program. We show that RSC can be proved more easily than fully abstract compilation and also often results in more efficient code. We also present two different proof techniques for establishing that a compiler attains RSC and, to illustrate them, develop three illustrative robustly safe compilers that rely on different target-level protection mechanisms. We then proceed to turn one of our compilers into a fully abstract one and through this example argue that proving RSC can be simpler than proving full abstraction. To better explain and clarify notions, this article uses syntax highlighting in a way that colourblind and black-8-white readers can benefit from Reference [58]. For a better experience, please print or view this article in colour . 1",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3126315337",
    "type": "article"
  },
  {
    "title": "TaDA Live: Compositional Reasoning for Termination of Fine-grained Concurrent Programs",
    "doi": "https://doi.org/10.1145/3477082",
    "publication_date": "2021-11-10",
    "publication_year": 2021,
    "authors": "Emanuele D’Osualdo; Julian Sutherland; Azadeh Farzan; Philippa Gardner",
    "corresponding_authors": "",
    "abstract": "We present TaDA Live, a concurrent separation logic for reasoning compositionally about the termination of blocking fine-grained concurrent programs. The crucial challenge is how to deal with abstract atomic blocking : that is, abstract atomic operations that have blocking behaviour arising from busy-waiting patterns as found in, for example, fine-grained spin locks. Our fundamental innovation is with the design of abstract specifications that capture this blocking behaviour as liveness assumptions on the environment. We design a logic that can reason about the termination of clients that use such operations without breaking their abstraction boundaries, and the correctness of the implementations of the operations with respect to their abstract specifications. We introduce a novel semantic model using layered subjective obligations to express liveness invariants and a proof system that is sound with respect to the model. The subtlety of our specifications and reasoning is illustrated using several case studies.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3211705538",
    "type": "article"
  },
  {
    "title": "A Relational Program Logic with Data Abstraction and Dynamic Framing",
    "doi": "https://doi.org/10.1145/3551497",
    "publication_date": "2022-08-01",
    "publication_year": 2022,
    "authors": "Anindya Banerjee; Ramana Nagasamudram; David A. Naumann; Mohammad Nikouei",
    "corresponding_authors": "",
    "abstract": "Dedicated to Tony Hoare. In a paper published in 1972, Hoare articulated the fundamental notions of hiding invariants and simulations. Hiding: invariants on encapsulated data representations need not be mentioned in specifications that comprise the API of a module. Simulation: correctness of a new data representation and implementation can be established by proving simulation between the old and new implementations using a coupling relation defined on the encapsulated state. These results were formalized semantically and for a simple model of state, though the paper claimed this could be extended to encompass dynamically allocated objects. In recent years, progress has been made toward formalizing the claim, for simulation, though mainly in semantic developments. In this article, hiding and simulation are combined with the idea in Hoare’s 1969 paper: a logic of programs. For an object-based language with dynamic allocation, we introduce a relational Hoare logic with stateful frame conditions that formalizes encapsulation, hiding of invariants, and couplings that relate two implementations. Relations and other assertions are expressed in first-order logic. Specifications can express a wide range of relational properties such as conditional equivalence and noninterference with declassification. The proof rules facilitate relational reasoning by means of convenient alignments and are shown sound with respect to a conventional operational semantics. A derived proof rule for equivalence of linked programs directly embodies representation independence. Applicability to representative examples is demonstrated using an SMT-based implementation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3097116675",
    "type": "article"
  },
  {
    "title": "Multiple Input Parsing and Lexical Analysis",
    "doi": "https://doi.org/10.1145/3594734",
    "publication_date": "2023-05-03",
    "publication_year": 2023,
    "authors": "Elizabeth Scott; Adrian Johnstone; Robert Walsh",
    "corresponding_authors": "",
    "abstract": "This article introduces two new approaches in the areas of lexical analysis and context-free parsing. We present an extension, MGLL, of generalised parsing which allows multiple input strings to be parsed together efficiently, and we present an enhanced approach to lexical analysis which exploits this multiple parsing capability. The work provides new power to formal language specification and disambiguation, and brings new techniques into the historically well-studied areas of lexical and syntax analysis. It encompasses character-level parsing at one extreme and the classical LEX/YACC style division at the other, allowing the advantages of both approaches.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4367847503",
    "type": "article"
  },
  {
    "title": "Operational semantics-directed compilers and machine architectures",
    "doi": "https://doi.org/10.1145/183432.183458",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "John Hannan",
    "corresponding_authors": "John Hannan",
    "abstract": "We consider the task of automatically constructing intermediate-level machine architectures and compilers generating code for these architectures, given operational semantics for source languages. We use operational semantics in the form of abstract machines given by rewrite systems in which the rewrite rules operate on terms representing states of computations. To construct compilers and new architectures we employ a particular strategy called pass separation, a form of staging transformation, that takes a program p and constructs a pair of programs p 1 , p 2 such that p(x, y) = p 2 (p 1 (x), y) ) for all x,y . If p represents an operational semantics for a language, with arguments x and y denoting a source program and its input data, then pass separation constructs programs p 1 and p 2 corresponding to a compiler and an executor. The compiler translates the source language into an intermediate-level target language, and the executor provides the definition for this language. Our use of pass separation supports the automatic definition of target languages or architectures, and the structure of these architectures is directed by the structure of the given source semantics. These architectures resemble abstract machine languages found in hand-crafted compilers. Our method is restricted to a limited class of abstract machines given as term-rewriting systems, but we argue that this class encompasses a large set of language definitions derived from more natural operational semantics. We provide two examples of our method by constructing compilers and target architectures for a simple functional language and a simple imperative language. Though we construct these architectures automatically, they bear a striking resemblance to existing architectures constructed by hand.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1965802586",
    "type": "article"
  },
  {
    "title": "Reasoning about Grover's quantum search algorithm using probabilistic <i>wp</i>",
    "doi": "https://doi.org/10.1145/319301.319303",
    "publication_date": "1999-05-01",
    "publication_year": 1999,
    "authors": "Michael Butler; Pieter Hartel",
    "corresponding_authors": "",
    "abstract": "Grover's search algorithm is designed to be executed on a quantum-mechanical computer. In this article, the probabilistic wp -calculus is used to model and reason about Grover's algorithm. It is demonstrated that the calculus provides a rigorous programming notation for modeling this and other quantum algorithms and that it also provides a systematic framework of analyzing such algorithms.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2016797939",
    "type": "article"
  },
  {
    "title": "A compiler approach to scalable concurrent-program design",
    "doi": "https://doi.org/10.1145/177492.177612",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Ian Foster; Stephen Taylor",
    "corresponding_authors": "",
    "abstract": "We describe a compilation system for the concurrent programming language Program Composition Notation (PCN). This notation provides a single-assignment programming model that permits concurrent-programming concerns such as decomposition, communication, synchronization, mapping, granularity, and load balancing to be addressed separately in a design. PCN is also extensible with programmer-defined operators , allowing common abstractions to be encapsulated and reused in different contexts. The compilation system incorporates a concurrent-transformation system that allows abstractions to be defined through concurrent source-to-source transformations; these convert programmer-defined operators into a core notation. Run-time techniques allow the core notation to be compiled into a simple concurrent abstract machine which can be implemented in a portable fashion using a run-time library. The abstract machine provides a uniform treatment of single-assignment and mutable data structures, allowing data sharing between concurrent and sequential program segments and permitting integration of sequential C and Fortran code into concurrent programs. This compilation system forms part of a program development toolkit that operates on a wide variety of networked workstations, multicomputers, and shared-memory multiprocessors. The toolkit has been used both to develop substantial applications and to teach introductory concurrent-programming classes, including a freshman course at Caltech.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2051292397",
    "type": "article"
  },
  {
    "title": "Principles and practice of unification factoring",
    "doi": "https://doi.org/10.1145/232706.232722",
    "publication_date": "1996-09-01",
    "publication_year": 1996,
    "authors": "Steven Dawson; C. R. Ramakrishnan; Steven Skiena; Terrance Swift",
    "corresponding_authors": "",
    "abstract": "The efficiency of resolution-based logic programming languages, such as Prolog, depends critically on selecting and executing sets of applicable clause heads to resolve against subgoals. Traditional approaches to this problem have focused on using indexing to determine the smallest possible applicable set. Despite their usefulness, these approaches ignore the nondeterminism inherent in many programming languages to the extent that they do not attempt to optimize execution after the applicable set has been determined. Unification factoring seeks to rectify this omission by regarding the indexing and unification phases of clause resolution as a single process. This article formalizes that process through the construction of factoring automata . A polynomial-time algorithm is given for constructing optimal factoring automata that preserve the clause selection strategy of Prolog. More generally, when the clause selection strategy is not fixed, constructing such an optimal automaton is shown to be NP-complete, solving an open trie minimization problem. Unification factoring is implemented through a source code transformation that preserves the full semantics of Prolog. This transformation is specified in the article, and using it, several well-known programs show significant performance improvements across several different systems. A prototype of unification factoring is available by anonymous ftp.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2089189518",
    "type": "article"
  },
  {
    "title": "Reasoning about probabilistic parallel programs",
    "doi": "https://doi.org/10.1145/177492.177724",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Josyula R. Rao",
    "corresponding_authors": "Josyula R. Rao",
    "abstract": "The use of randomization in the design and analysis of algorithms promises simple and efficient algorithms to difficult problems, some of which may not have a deterministic solution. This gain in simplicity, efficiency, and solvability results in a trade-off of the traditional notion of absolute correctness of algorithms for a more quantitative notion: correctness with a probability between 0 and 1. The addition of the notion of parallelism to the already unintuitive idea of randomization makes reasoning about probabilistic parallel programs all the more tortuous and difficult. In this paper we address the problem of specifying and deriving properties of probabilistic parallel programs that either hold deterministically or with probability 1. We present a proof methodology based on existing proof systems for probabilistic algorithms, the theory of the predicate transformer, and the theory of UNITY. Although the proofs of probabilistic programs are slippery at best, we show that such programs can be derived with the same rigor and elegance that we have seen in the derivation of sequential and parallel programs. By applying this methodology to derive probabilistic programs, we hope to develop tools and techniques that would make randomization a useful paradigm in algorithm design.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2049604087",
    "type": "article"
  },
  {
    "title": "Integrating object-oriented programming and protected objects in Ada 95",
    "doi": "https://doi.org/10.1145/353926.353938",
    "publication_date": "2000-05-01",
    "publication_year": 2000,
    "authors": "Andy Wellings; B. B. Johnson; Bo I. Sandén; Jörg Kienzle; Thomas Wolf; Stephen Michell",
    "corresponding_authors": "",
    "abstract": "Integrating concurrent and object-oriented programming has been an active research topic since the late 1980's. There is a now a plethora of methods for achieving this integration. The majority of approaches have taken a sequential object-oriented language and made it concurrent. A few approaches have taken a concurrent language and made it object-oriented. The most important of this latter class is the Ada 95 language, which is an extension to the object-based concurrent programming language Ada 83. Arguably, Ada 95 does not fully integrate its models of concurrency and object-oriented programming. For example, neither tasks nor protected objects are extensible. This article discusses ways in which protected objects can be made more extensible.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2166835405",
    "type": "article"
  },
  {
    "title": "Linking programs incrementally",
    "doi": "https://doi.org/10.1145/114005.102804",
    "publication_date": "1991-01-01",
    "publication_year": 1991,
    "authors": "Russell W. Quong; Mark Linton",
    "corresponding_authors": "",
    "abstract": "Linking is traditionally a batch process that resolves cross-references between object modules and run-time libraries to produce a stand-alone executable image. Because most program changes only involve a small part of the program, we have implemented an incremental linker, named Inclink, that processes only the changed modules. Inclink generates a new executable in time proportional to the size of change; in contrast, a batch linker generates an executable in time proporitonal to the size of the program. To minimize updates to the executable, Inclink allocates extra space for every module. By allocating 24 percent more space in the executable for overflows, Inclink can update a module in place over 97 percent of the time. Measurements show that Inclink is more than an order of magnitude faster than the UNIX [2] batch linker and that 88 percent of all links will take less than 2 s of CPU time on a MicroVAX-2, independent of program size.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2168191383",
    "type": "article"
  },
  {
    "title": "The receptive distributed π-calculus",
    "doi": "https://doi.org/10.1145/937563.937564",
    "publication_date": "2003-09-01",
    "publication_year": 2003,
    "authors": "Roberto M. Amadio; Gérard Boudol; Cédric Lhoussaine",
    "corresponding_authors": "",
    "abstract": "We study an asynchronous distributed π-calculus, with constructs for localities and migration. We show that a static analysis ensures the receptiveness of channel names, which, together with a simple type system, guarantees the message deliverability property. This property states that any migrating message will find an appropriate receiver at its destination locality. We argue that this distributed, receptive calculus is still expressive enough while allowing for an effective type inference à la ML.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2049623372",
    "type": "article"
  },
  {
    "title": "Table compression for tree automata",
    "doi": "https://doi.org/10.1145/117009.117013",
    "publication_date": "1991-07-01",
    "publication_year": 1991,
    "authors": "Jürgen Börstler; Ulrich Möncke; Reinhard Wilhelm",
    "corresponding_authors": "",
    "abstract": "article Free AccessTable compression for tree automata Authors: Jürgen Börstler Univ. des Saarlandes, Saarbru¨cken, Germany Univ. des Saarlandes, Saarbru¨cken, GermanyView Profile , Ulrich Möncke Univ. des Saarlandes, Saarbru¨cken, Germany Univ. des Saarlandes, Saarbru¨cken, GermanyView Profile , Reinhard Wilhelm Univ. des Saarlandes, Saarbru¨cken, Germany Univ. des Saarlandes, Saarbru¨cken, GermanyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 3July 1991 pp 295–314https://doi.org/10.1145/117009.117013Published:01 July 1991Publication History 9citation528DownloadsMetricsTotal Citations9Total Downloads528Last 12 Months28Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2106992646",
    "type": "article"
  },
  {
    "title": "Efficient demand-driven evaluation. Part 2",
    "doi": "https://doi.org/10.1145/5001.5003",
    "publication_date": "1986-01-02",
    "publication_year": 1986,
    "authors": "Keshav Pingali; Arvind Arvind",
    "corresponding_authors": "",
    "abstract": "In Part 1 of this paper [5], we presented a scheme whereby a compiler could propagate demands through programs in a powerful stream language L. A data-driven evaluation of the transformed program performed exactly the same computation as a demand-driven evaluation of the original program. In this paper we explore a different transformation, which trades the complexity of demand propagation for a bounded amount of extra computation on some data lines.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2918106430",
    "type": "article"
  },
  {
    "title": "Symmetric intertask communication",
    "doi": "https://doi.org/10.1145/4472.4475",
    "publication_date": "1985-10-01",
    "publication_year": 1985,
    "authors": "Nissim Francez; Shaula Yemini",
    "corresponding_authors": "",
    "abstract": "We argue for the need of supporting a symmetric select construct, in which entry calls as well as accepts can be alternatives. We present several situations in which a symmetric select leads to a more natural programming style. We show that several semantic principles are violated by a nonsymmetric select, while being satisfied by a symmetric one. In particular, the suggested symmetric intertask communication mechanism is fully abstract and composable, and has a distributed termination rule which reduces the risk of deadlock. Our discussion is in terms of Ada™.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2046484139",
    "type": "article"
  },
  {
    "title": "Optimal prepaging and font caching",
    "doi": "https://doi.org/10.1145/2363.2367",
    "publication_date": "1985-01-02",
    "publication_year": 1985,
    "authors": "David R. Fuchs; Donald E. Knuth",
    "corresponding_authors": "",
    "abstract": "An efficient algorithm for communicating letter-shape information from a high-speed computer with a large memory to a typesetting device that has a limited memory is presented. The encoding is optimum, in the sense that the total time for typesetting is minimized, using a model that generalizes well-known “demand paging” strategies to the case where changes to the cache are allowed before the associated information is actually needed. Extensive empirical data show that good results are obtained even when difficult technical material is being typeset on a machine that can store information concerning only 100 characters. The methods of this paper are also applicable to other hardware and software caching applications with restricted lookahead.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2070334217",
    "type": "article"
  },
  {
    "title": "The Evaluation of Expressions in Icon",
    "doi": "https://doi.org/10.1145/69622.357184",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "Ralph E. Griswold",
    "corresponding_authors": "Ralph E. Griswold",
    "abstract": "article Free Access Share on The Evaluation of Expressions in Icon Author: Ralph E. Griswold Department of Computer Science, The University of Arizona, Tucson, AZ Department of Computer Science, The University of Arizona, Tucson, AZView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 563–584https://doi.org/10.1145/69622.357184Published:01 October 1982Publication History 11citation476DownloadsMetricsTotal Citations11Total Downloads476Last 12 Months24Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2015160761",
    "type": "article"
  },
  {
    "title": "An interval constraint system for lattice domains",
    "doi": "https://doi.org/10.1145/963778.963779",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Antonio J. Fernández; Patricia M. Hill",
    "corresponding_authors": "",
    "abstract": "We present a generic framework for defining and solving interval constraints on any set of domains (finite or infinite) that are lattices. The approach is based on the use of a single form of constraint similar to that of an indexical used by CLP for finite domains and on a particular generic definition of an interval domain built from an arbitrary lattice. We provide the theoretical foundations for this framework and a schematic procedure for the operational semantics. Examples are provided that illustrate how new (compound) constraint solvers can be constructed from existing solvers using lattice combinators and how different solvers (possibly on distinct domains) can communicate and hence, cooperate in solving a problem. We describe the language clp ( L ), which is a prototype implementation of this framework and discuss ways in which this implementation may be improved.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2010781707",
    "type": "article"
  },
  {
    "title": "Message analysis for concurrent programs using message passing",
    "doi": "https://doi.org/10.1145/1146809.1146813",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Richard Carlsson; Konstantinos Sagonas; Jesper Wilhelmsson",
    "corresponding_authors": "",
    "abstract": "We describe an analysis-driven storage allocation scheme for concurrent systems that use message passing with copying semantics. The basic principle is that in such a system, data which is not part of any message does not need to be allocated in a shared data area. This allows for the deallocation of thread-specific data without requiring global synchronization and often without even triggering garbage collection. On the other hand, data that is part of a message should preferably be allocated on a shared area since this allows for fast ( O (1)) interprocess communication that does not require actual copying. In the context of a dynamically typed, higher-order concurrent functional language, we present a static message analysis which guides the allocation. As shown by our performance evaluation, conducted using a production-quality language implementation, the analysis is effective enough to discover most data which is to be used as a message, and to allow the allocation scheme to combine the best performance characteristics of both a process-centric and a communal memory architecture.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2014199531",
    "type": "article"
  },
  {
    "title": "Dealing with incomplete knowledge on CLP( <i>FD</i> ) variable domains",
    "doi": "https://doi.org/10.1145/1057387.1057389",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Marco Gavanelli; Evelina Lamma; Paola Mello; Michela Milano",
    "corresponding_authors": "",
    "abstract": "Constraint Logic Programming languages on Finite Domains, CLP( FD ), provide a declarative framework for Artificial Intelligence problems. However, in many real life cases, domains are not known and must be acquired or computed. In systems that interact with the outer world, domain elements synthesize information on the environment, they are not all known at the beginning of the computation, and must be retrieved through an expensive acquisition process.In this article, we extend the CLP( FD ) language by combining it with a new sort (called Incrementally specified Sets, I-Set ). In the resulting language, CLP( FD + I-Set ), FD variables can be defined on partially or fully unknown domains ( I-Set ). Domains can be linked each other through relations, and constraints can be imposed on them. We describe a propagation algorithm (called Known Arc Consistency (KAC)) based on known domain elements, and theoretically compare it with arc-consistency.The language can be implemented on top of different CLP systems, thus letting the user exploit different possible semantics for domains (e.g., lists, sets or streams). We state the specifications that the employed system should provide, and we show that two different CLP systems (Conjunto and { log }) can be effectively used.We provide motivating examples and describe promising applications.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2037783464",
    "type": "article"
  },
  {
    "title": "A practical and fast iterative algorithm for ϕ-function computation using DJ graphs",
    "doi": "https://doi.org/10.1145/1065887.1065890",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Dibyendu Das; U. Ramakrishna",
    "corresponding_authors": "",
    "abstract": "We present a new and practical method of computing ϕ-function for all variables in a function for Static Single Assignment (SSA) form. The new algorithm is based on computing the Merge set of each node in the control flow graph of a function (a node here represents a basic block and the terms will be used interchangeably). Merge set of a node n is the set of nodes N , where ϕ-functions may need to be placed if variables are defined in n . It is not necessary for n to have a definition of a variable in it. Thus, the merge set of n is dictated by the underlying structure of the CFG. The new method presented here precomputes the merge set of every node in the CFG using an iterative approach. Later, these merge sets are used to carry out the actual ϕ-function placement. The advantages are in examples where dense definitions of variables are present (i.e., original definitions of variables---user defined or otherwise, in a majority of basic blocks). Our experience with SSA in the High Level Optimizer (optimization levels +O3/+O4) shows that most examples from the Spec2000 benchmark suite require a high percentage of basic blocks to have their ϕ points computed. Previous methods of computing the same relied on the dominance frontier ( DF ) concept, first introduced by Cytron et al. The method presented in this paper gives a new effective iterative solution to the problem. Also, in cases, where the control flow graph does not change, our method does not require any additional computation for new definitions introduced as part of optimizations. We present implementation details with results from Spec2000 benchmarks. Our algorithm runs faster than the existing methods used.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2076015364",
    "type": "article"
  },
  {
    "title": "A practical interprocedural dominance algorithm",
    "doi": "https://doi.org/10.1145/1255450.1255452",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Bjorn De Sutter; Ludo Van Put; Koen De Bosschere",
    "corresponding_authors": "",
    "abstract": "Existing algorithms for computing dominators are formulated for control flow graphs of single procedures. With the rise of computing power, and the viability of whole-program analyses and optimizations, there is a growing need to extend the dominator computation algorithms to context-sensitive interprocedural dominators. Because the transitive reduction of the interprocedural dominator graph is not a tree, as in the intraprocedural case, it is not possible to extend existing algorithms directly. In this article, we propose a new algorithm for computing interprocedural dominators. Although the theoretical complexity of this new algorithm is as high as that of a straightforward iterative solution of the data flow equations, our experimental evaluation demonstrates that the algorithm is practically viable, even for programs consisting of several hundred thousands of basic blocks.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2050584434",
    "type": "article"
  },
  {
    "title": "Enforcing resource bounds via static verification of dynamic checks",
    "doi": "https://doi.org/10.1145/1275497.1275503",
    "publication_date": "2007-08-02",
    "publication_year": 2007,
    "authors": "Ajay Chander; David Espinosa; Nayeem Islam; Peter Lee; George C. Necula",
    "corresponding_authors": "",
    "abstract": "We show how to limit a program's resource usage in an efficient way, using a novel combination of dynamic checks and static analysis. Usually, dynamic checking is inefficient due to the overhead of checks, while static analysis is difficult and rejects many safe programs. We propose a hybrid approach that solves these problems. We split each resource-consuming operation into two parts. The first is a dynamic check, called reserve. The second is the actual operation, called consume, which does not perform any dynamic checks. The programmer is then free to hoist and combine reserve operations. Combining reserve operations reduces their overhead, while hoisting reserve operations ensures that the program does not run out of resources at an inconvenient time. A static verifier ensures that the program reserves resources before it consumes them. This verification is both easier and more flexible than an a priori static verification of resource usage. We present a sound and efficient static verifier based on Hoare logic and linear inequalities. As an example, we present a version of tar written in Java.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2070892181",
    "type": "article"
  },
  {
    "title": "Dually nondeterministic functions",
    "doi": "https://doi.org/10.1145/1391956.1391961",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Joseph M. Morris; Malcolm Tyrrell",
    "corresponding_authors": "",
    "abstract": "Nondeterminacy is a fundamental notion in computing. We show that it can be described by a general theory that accounts for it in the form in which it occurs in many programming contexts, among them specifications, competing agents, data refinement, abstract interpretation, imperative programming, process algebras, and recursion theory. Underpinning these applications is a theory of nondeterministic functions; we construct such a theory. The theory consists of an algebra with which practitioners can reason about nondeterministic functions, and a denotational model to establish the soundness of the theory. The model is based on the idea of free completely distributive lattices over partially ordered sets. We deduce the important properties of nondeterministic functions.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1977975574",
    "type": "article"
  },
  {
    "title": "An improved bound for call strings based interprocedural analysis of bit vector frameworks",
    "doi": "https://doi.org/10.1145/1286821.1286829",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Bageshri Karkare; Uday P. Khedker",
    "corresponding_authors": "",
    "abstract": "Interprocedural data flow analysis extends the scope of analysis across procedure boundaries in search of increased optimization opportunities. Call strings based approach is a general approach for performing flow and context sensitive interprocedural analysis. It maintains a history of calls along with the data flow information in the form of call strings, which are sequences of unfinished calls. Recursive programs may need infinite call strings for interprocedural data flow analysis. For bit vector frameworks this method is believed to require all call strings of lengths up to 3 K , where K is the maximum number of distinct call sites in any call chain. We combine the nature of information flows in bit-vector data flow analysis with the structure of interprocedurally valid paths to bound the call strings. Instead of bounding the length of call strings, we bound the number of occurrences of any call site in a call string. We show that the call strings in which a call site appears at most three times, are sufficient for convergence on interprocedural maximum fixed point solution. Though this results in the same worst case length of call strings, it does not require constructing all call strings up to length 3 K . Our empirical measurements on recursive programs show that our bound reduces the lengths and the number of call strings, and hence the analysis time, significantly.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1992234860",
    "type": "article"
  },
  {
    "title": "Execution suppression",
    "doi": "https://doi.org/10.1145/1745312.1745314",
    "publication_date": "2008-05-24",
    "publication_year": 2008,
    "authors": "Dennis Jeffrey; Vijay Nagarajan; Rajiv Gupta",
    "corresponding_authors": "",
    "abstract": "By studying the behavior of several programs that crash due to memory errors, we observed that locating the errors can be challenging because significant propagation of corrupt memory values can occur prior to the point of the crash. In this article, we present an automated approach for locating memory errors in the presence of memory corruption propagation. Our approach leverages the information revealed by a program crash: when a crash occurs, this reveals a subset of the memory corruption that exists in the execution. By suppressing (nullifying) the effect of this known corruption during execution, the crash is avoided and any remaining (hidden) corruption may then be exposed by subsequent crashes. The newly exposed corruption can then be suppressed in turn. By iterating this process until no further crashes occur, the first point of memory corruption—and the likely root cause of the program failure—can be identified. However, this iterative approach may terminate prematurely, since programs may not crash even when memory corruption is present during execution. To address this, we show how crashes can be exposed in an execution by manipulating the relative ordering of particular variables within memory. By revealing crashes through this variable re-ordering, the effectiveness and applicability of the execution suppression approach can be improved. We describe a set of experiments illustrating the effectiveness of our approach in consistently and precisely identifying the first points of memory corruption in executions that fail due to memory errors. We also discuss a baseline software implementation of execution suppression that incurs an average overhead of 7.2x, and describe how to reduce this overhead to 1.8x through hardware support.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2005139304",
    "type": "article"
  },
  {
    "title": "A Classical Sequent Calculus with Dependent Types",
    "doi": "https://doi.org/10.1145/3230625",
    "publication_date": "2019-03-15",
    "publication_year": 2019,
    "authors": "Étienne Miquey",
    "corresponding_authors": "Étienne Miquey",
    "abstract": "Dependent types are a key feature of the proof assistants based on the Curry-Howard isomorphism. It is well known that this correspondence can be extended to classical logic by enriching the language of proofs with control operators. However, they are known to misbehave in the presence of dependent types, unless dependencies are restricted to values. Moreover, while sequent calculi naturally support continuation-passing-style interpretations, there is no such presentation of a language with dependent types. The main achievement of this article is to give a sequent calculus presentation of a call-by-value language with a control operator and dependent types, and to justify its soundness through a continuation-passing-style translation. We start from the call-by-value version of the λμ˜μ -calculus. We design a minimal language with a value restriction and a type system that includes a list of explicit dependencies to maintain type safety. We then show how to relax the value restriction and introduce delimited continuations to directly prove the consistency by means of a continuation-passing-style translation. Finally, we relate our calculus to a similar system by Lepigre and present a methodology to transfer properties from this system to our own.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2528080869",
    "type": "article"
  },
  {
    "title": "The Chemical Approach to Typestate-Oriented Programming",
    "doi": "https://doi.org/10.1145/3064849",
    "publication_date": "2017-05-26",
    "publication_year": 2017,
    "authors": "Silvia Crafà; Luca Padovani",
    "corresponding_authors": "",
    "abstract": "We introduce a novel approach to typestate-oriented programming based on the chemical metaphor: state and operations on objects are molecules of messages, and state transformations are chemical reactions. This approach allows us to investigate typestate in an inherently concurrent setting, whereby objects can be accessed and modified concurrently by several processes, each potentially changing only part of their state. We introduce a simple behavioral type theory to express in a uniform way both the private and the public interfaces of objects; describe and enforce structured object protocols consisting of possibilities, prohibitions, and obligations; and control object sharing.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2618474473",
    "type": "article"
  },
  {
    "title": "A Logical Analysis of Framing for Specifications with Pure Method Calls",
    "doi": "https://doi.org/10.1145/3174801",
    "publication_date": "2018-05-28",
    "publication_year": 2018,
    "authors": "Anindya Banerjee; David A. Naumann; Mohammad Nikouei",
    "corresponding_authors": "",
    "abstract": "For specifying and reasoning about object-based programs, it is often attractive for contracts to be expressed using calls to pure methods. It is useful for pure methods to have contracts, including read effects, to support local reasoning based on frame conditions. This leads to puzzles such as the use of a pure method in its own contract. These ideas have been explored in connection with verification tools based on axiomatic semantics, guided by the need to avoid logical inconsistency, and focusing on encodings that cater for first-order automated provers. This article adds pure methods and read effects to region logic, a first-order program logic that features frame-based local reasoning and provides modular reasoning principles for end-to-end correctness. Modular reasoning is embodied in a proof rule for linking a module’s method implementations with a client that relies on the method contracts. Soundness is proved with respect to conventional operational semantics and uses an extensional (i.e, relational) interpretation of read effects. Applicability to tools based on SMT solvers is demonstrated through machine-checked verification of examples. The developments in this article can guide the implementations of linking as used in modular verifiers and serve as a basis for studying observationally pure methods and encapsulation.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2806974647",
    "type": "article"
  },
  {
    "title": "Modular Termination Verification of Single-Threaded and Multithreaded Programs",
    "doi": "https://doi.org/10.1145/3210258",
    "publication_date": "2018-07-05",
    "publication_year": 2018,
    "authors": "Bart Jacobs; Dragan Bošnački; Ruurd Kuiper",
    "corresponding_authors": "",
    "abstract": "We propose an approach for the modular specification and verification of total correctness properties of object-oriented programs. The core of our approach is a specification style that prescribes a way to assign a level expression to each method such that each callee’s level is below the caller’s, even in the presence of dynamic binding. The specification style yields specifications that properly hide implementation details. The main idea is to use multisets of method names as levels, and to associate with each object levels that abstractly reflect the way the object is built from other objects. A method’s level is then defined in terms of the method’s own name and the levels associated with the objects passed as arguments. We first present the specification style in the context of programs that do not modify object fields. We then combine it with separation logic and abstract predicate families to obtain an approach for programs with heap mutation. In a third step, we address concurrency, by incorporating an existing approach for verifying deadlock freedom of channels and locks. Our main contribution here is to achieve information hiding by using the proposed termination levels for lock ordering as well. Also, we introduce call permissions to enable elegant verification of termination of programs where threads cause work in other threads, such as in thread pools or fine-grained concurrent algorithms involving compare-and-swap loops. We explain how our approach can be used also to verify the liveness of nonterminating programs.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2824429000",
    "type": "article"
  },
  {
    "title": "Dynamic Deadlock Verification for General Barrier Synchronisation",
    "doi": "https://doi.org/10.1145/3229060",
    "publication_date": "2018-12-11",
    "publication_year": 2018,
    "authors": "Tiago Cogumbreiro; Raymond Hu; Francisco Martins; Nobuko Yoshida",
    "corresponding_authors": "",
    "abstract": "We present Armus, a verification tool for dynamically detecting or avoiding barrier deadlocks. The core design of Armus is based on phasers, a generalisation of barriers that supports split-phase synchronisation, dynamic membership, and optional-waits. This allows Armus to handle the key barrier synchronisation patterns found in modern languages and libraries. We implement Armus for X10 and Java, giving the first sound and complete barrier deadlock verification tools in these settings. Armus introduces a novel event-based graph model of barrier concurrency constraints that distinguishes task-event and event-task dependencies. Decoupling these two kinds of dependencies facilitates the verification of distributed barriers with dynamic membership, a challenging feature of X10. Further, our base graph representation can be dynamically switched between a task-to-task model, Wait-for Graph (WFG), and an event-to-event model, State Graph (SG), to improve the scalability of the analysis. Formally, we show that the verification is sound and complete with respect to the occurrence of deadlock in our core phaser language, and that switching graph representations preserves the soundness and completeness properties. These results are machine checked with the Coq proof assistant. Practically, we evaluate the runtime overhead of our implementations using three benchmark suites in local and distributed scenarios. Regarding deadlock detection, distributed scenarios show negligible overheads and local scenarios show overheads below 1.15×. Deadlock avoidance is more demanding, and highlights the potential gains from dynamic graph selection. In one benchmark scenario, the runtime overheads vary from 1.8× for dynamic selection, 2.6× for SG-static selection, and 5.9× for WFG-static selection.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2998338832",
    "type": "article"
  },
  {
    "title": "Automatic Storage Optimization for Arrays",
    "doi": "https://doi.org/10.1145/2845078",
    "publication_date": "2016-04-08",
    "publication_year": 2016,
    "authors": "Somashekaracharya G. Bhaskaracharya; Uday Bondhugula; Albert Cohen",
    "corresponding_authors": "",
    "abstract": "Efficient memory allocation is crucial for data-intensive applications, as a smaller memory footprint ensures better cache performance and allows one to run a larger problem size given a fixed amount of main memory. In this article, we describe a new automatic storage optimization technique to minimize the dimensionality and storage requirements of arrays used in sequences of loop nests with a predetermined schedule. We formulate the problem of intra-array storage optimization as one of finding the right storage partitioning hyperplanes: each storage partition corresponds to a single storage location. Our heuristic is driven by a dual-objective function that minimizes both the dimensionality of the mapping and the extents along those dimensions. The technique is dimension optimal for most codes encountered in practice. The storage requirements of the mappings obtained also are asymptotically better than those obtained by any existing schedule-dependent technique. Storage reduction factors and other results that we report from an implementation of our technique demonstrate its effectiveness on several real-world examples drawn from the domains of image processing, stencil computations, high-performance computing, and the class of tiled codes in general.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2334282563",
    "type": "article"
  },
  {
    "title": "For a Few Dollars More: Verified Fine-Grained Algorithm Analysis Down to LLVM",
    "doi": "https://doi.org/10.1145/3486169",
    "publication_date": "2022-02-23",
    "publication_year": 2022,
    "authors": "Maximilian P. L. Haslbeck; Peter Lammich",
    "corresponding_authors": "",
    "abstract": "We present a framework to verify both, functional correctness and (amortized) worst-case complexity of practically efficient algorithms. We implemented a stepwise refinement approach, using the novel concept of resource currencies to naturally structure the resource analysis along the refinement chain, and allow a fine-grained analysis of operation counts. Our framework targets the LLVM intermediate representation. We extend its semantics from earlier work with a cost model. As case studies, we verify the amortized constant time push operation on dynamic arrays and the O ( n log n ) introsort algorithm, and refine them down to efficient LLVM implementations. Our sorting algorithm performs on par with the state-of-the-art implementation found in the GNU C++ Library, and provably satisfies the complexity required by the C++ standard.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4213447153",
    "type": "article"
  },
  {
    "title": "Prophecy Made Simple",
    "doi": "https://doi.org/10.1145/3492545",
    "publication_date": "2022-04-06",
    "publication_year": 2022,
    "authors": "Leslie Lamport; Stephan Merz",
    "corresponding_authors": "",
    "abstract": "Prophecy variables were introduced in the article “The Existence of Refinement Mappings” by Abadi and Lamport. They were difficult to use in practice. We describe a new kind of prophecy variable that we find much easier to use. We also reformulate ideas from that article in a more mathematical way.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4226030675",
    "type": "article"
  },
  {
    "title": "Efficient instruction scheduling for delayed-load architectures",
    "doi": "https://doi.org/10.1145/213978.213987",
    "publication_date": "1995-09-01",
    "publication_year": 1995,
    "authors": "Steven M. Kurlander; Todd A. Proebsting; Charles N. Fischer",
    "corresponding_authors": "",
    "abstract": "A fast, optimal code-scheduling algorithm for processors with a delayed load of one instruction cycle is described. The algorithm minimizes both execution time and register use and runs in time proportional to the size of the expression-tree. An extension that spills registers when too few registers are available is also presented. The algorithm also performs very well for delayed loads of greater than one instruction cycle. A heuristic that schedules DAGs and is based on our optimal expression-tree-scheduling algorithm is presented and compared with Goodman and Hsu's algorithm Integrated Prepass Scheduling (IPS). Both schedulers perform well on benchmarks with small basic blocks, but on large basic blocks our scheduler outperforms IPS and is significantly faster.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2026539022",
    "type": "article"
  },
  {
    "title": "A refinement calculus for the synthesis of verified hardware descriptions in VHDL",
    "doi": "https://doi.org/10.1145/262004.262007",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Péter Breuer; Carlos Kloos Delgado; Andrés Marín; Natividad Martínez Madrid; Luis Sánchez Fernández",
    "corresponding_authors": "",
    "abstract": "A formal refinement calculus targeted at system-level descriptions in the IEEE standard hardware description language VHDL is described here. Refinement can be used to develop hardware description code that is “correct by construction”. the calculus is closely related to a Hoare-style programming logic for VHDL and real-time systems in general. That logic and a semantics for a core subset of VHDL are described. The programming logic and the associated refinement calculus are shown to be complete. This means that if there is a code that can be shown to implement a given specification, then it will be derivable from the specification via the calculus.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2026478719",
    "type": "article"
  },
  {
    "title": "TransformGen: automating the maintenance of structure-oriented environments",
    "doi": "https://doi.org/10.1145/177492.177697",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "David Garlan; Charles W. Krueger; Barbara Lerner",
    "corresponding_authors": "",
    "abstract": "A serious problem for programs that use persistent data is that information created and maintained by the program becomes invalid if the persistent types used in the program are modified in a new release. Unfortunately, there has been little systematic treatment of the problem; current approaches are manual, ad hoc, and time consuming both for programmers and users. In this article we present a new approach. Focusing on the special case of managing abstract syntax trees in structure-oriented environments, we show how automatic transformers can be generated in terms of an implementor's changes to the grammar of these environments.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2077157903",
    "type": "article"
  },
  {
    "title": "Interprocedural control flow analysis of first-order programs with tail-call optimization",
    "doi": "https://doi.org/10.1145/262004.262006",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Saumya Debray; Todd A. Proebsting",
    "corresponding_authors": "",
    "abstract": "Knowledge of low-level control flow is essential for many compiler optimizations. In systems with tail-call optimization, the determination of interprocedural control flow is complicated by the fact that because of tail-call optimization, control flow at procedure returns is not readily evident from the call graph of the program. This article shows how interprocedural control-flow analysis of first-order programs can be carried out using well-known concepts from parsing theory. In particular, we show that context-insensitive ( or zeroth-order) control-flow analysis corresponds to the notion of FOLLOW sets in context-free grammars, while context-sensitive (or first-order) control-flow analysis corresponds to the notion of LR(1) items. The control-flow information so obtained can be used to improve the precision of interprocedural dataflow analyses as well as to extend certain low-level code optimizations across procedure boundaries.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2095357205",
    "type": "article"
  },
  {
    "title": "Mechanizing a theory of program composition for UNITY",
    "doi": "https://doi.org/10.1145/504709.504711",
    "publication_date": "2001-09-01",
    "publication_year": 2001,
    "authors": "Lawrence C. Paulson",
    "corresponding_authors": "Lawrence C. Paulson",
    "abstract": "Compositional reasoning must be better understood if non-trivial concurrent programs are to be verified. Chandy and Sanders [2000] have proposed a new approach to reasoning about composition, which Charpentier and Chandy [1999] have illustrated by developing a large example in the UNITY formalism. The present paper describes extensive experiments on mechanizing the compositionality theory and the example, using the proof tool Isabelle. Broader issues are discussed, in particular, the formalization of program states. The usual representation based upon maps from variables to values is contrasted with the alternatives, such as a signature of typed variables. Properties need to be transferred from one program component's signature to the common signature of the system. Safety properties can be so transferred, but progress properties cannot be. Using polymorphism, this problem can be circumvented by making signatures sufficiently flexible. Finally the proof of the example itself is outlined.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2042704814",
    "type": "article"
  },
  {
    "title": "Demand-driven register allocation",
    "doi": "https://doi.org/10.1145/236114.236117",
    "publication_date": "1996-11-01",
    "publication_year": 1996,
    "authors": "Todd A. Proebsting; Charles N. Fischer",
    "corresponding_authors": "",
    "abstract": "A new global register allocation technique, demand-driven register allocation , is described. Demand-driven register allocation quantifies the costs and benefits of allocating variables to registers over live ranges so that high-quality allocations can be made. Local allocation is done first, and then global allocation is done iteratively beginning in the most deeply nested loops. Because local allocation precedes global allocation, demand-driven allocation does not interfere with the use of well-known, high-quality local register allocation and instruction-scheduling techniques.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2072929215",
    "type": "article"
  },
  {
    "title": "Error repair with validation in LR-based parsing",
    "doi": "https://doi.org/10.1145/504083.504084",
    "publication_date": "2001-07-01",
    "publication_year": 2001,
    "authors": "Ik-Soon Kim; Kwang-Moo Choe",
    "corresponding_authors": "",
    "abstract": "When the compiler encounters an error symbol in an erroneous input, the local error-repair method repairs the input by either inserting a repair string before the error symbol or deleting the error symbol. Although the extended FMQ of Fischer et al. and the method of McKenzie et al. report the improved quality of diagnostic messages, they suffer from redundant parse stack configurations.This article proposes an efficient LR error-recovery method, with validation-removing repairs that give the same validation result as a previously considered, lower-cost repair. Moreover, its execution speed is proportional to the length of the stack configuration. The algorithm is implemented on a Bison, GNU LALR(1), parser generating system. Experimental results are presented.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1991218651",
    "type": "article"
  },
  {
    "title": "Type extension through polymorphism",
    "doi": "https://doi.org/10.1145/77606.214515",
    "publication_date": "1990-01-03",
    "publication_year": 1990,
    "authors": "F. Warren Burton",
    "corresponding_authors": "F. Warren Burton",
    "abstract": "A record data type can be extended by addition of more fields. The extended type is a subtype of the original, in that any value of the extended type can be regarded as a value of the original type by ignoring the additional fields. This results in a type hierarchy. Milner [3] has proposed a polymorphic type system. With the Milner approach, the type of a function may contain type variables. This also results in a type hierarchy. In a language with a polymorphic type system, if it is anticipated that a record type will need to be extended, then the record type can be defined to have a dummy extension field . In the parent type, the extension field will have null contents of type void . The type of the extension field can differ with different subtypes. The approach can be extended to allow a type to be subtype of two or more parent types. To a limited extent, this approach can be used in Ada and other languages with generic program units.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2018174322",
    "type": "article"
  },
  {
    "title": "Linguistic support for atomic data types",
    "doi": "https://doi.org/10.1145/78942.78944",
    "publication_date": "1990-04-01",
    "publication_year": 1990,
    "authors": "William E. Weihl",
    "corresponding_authors": "William E. Weihl",
    "abstract": "The problems of concurrency and failures in distributed systems can be addressed by implementing applications in terms of atomic data types: data types whose objects provide serializability and recoverability for transactions using them. The specifications of the types can be used to permit high levels of concurrency among transactions while still ensuring atomicity. However, highly concurrent implementations can be quite complicated. In this paper we analyze the expressive power of existing proposals for language features intended to support the implementation of atomic types. We illustrate several limitations of existing proposals and propose a new approach that avoids these problems.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2090014120",
    "type": "article"
  },
  {
    "title": "The denotational semantics of dynamic networks of processes",
    "doi": "https://doi.org/10.1145/4472.4473",
    "publication_date": "1985-10-01",
    "publication_year": 1985,
    "authors": "Arie de Bruin; W. Böhm",
    "corresponding_authors": "",
    "abstract": "DNP (dynamic networks of processes) is a variant of the language introduced by Kahn and MacQueen [11, 12]. In the language it is possible to create new processes dynamically. We present a complete, formal denotational semantics for the language, along the lines sketched by Kahn and MacQueen. An informal explanation of the formal semantics is also given.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2133673046",
    "type": "article"
  },
  {
    "title": "Error Data Values in the Data-Flow Language VAL",
    "doi": "https://doi.org/10.1145/357162.357167",
    "publication_date": "1982-04-01",
    "publication_year": 1982,
    "authors": "C. S. Wetherell",
    "corresponding_authors": "C. S. Wetherell",
    "abstract": "article Free AccessError Data Values in the Data-Flow Language VAL Author: C. S. Wetherell Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ and Lawrence Livermore National Laboratory and University of California, Davis Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ and Lawrence Livermore National Laboratory and University of California, DavisView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 2April 1982 pp 226–238https://doi.org/10.1145/357162.357167Published:01 April 1982Publication History 12citation262DownloadsMetricsTotal Citations12Total Downloads262Last 12 Months13Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1979635618",
    "type": "article"
  },
  {
    "title": "Automatic Program Improvement: Variable Usage Transformations",
    "doi": "https://doi.org/10.1145/69624.357209",
    "publication_date": "1983-04-01",
    "publication_year": 1983,
    "authors": "Brian Maher; Derek H. Sleeman",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Automatic Program Improvement: Variable Usage Transformations Authors: B. Maher Computer-Based Learning Project, The University, Leeds 2, England Computer-Based Learning Project, The University, Leeds 2, EnglandView Profile , D. H. Sleeman Department of Computer Science, Stanford University, Stanford, CA Department of Computer Science, Stanford University, Stanford, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 2pp 236–264https://doi.org/10.1145/69624.357209Published:01 April 1983Publication History 12citation272DownloadsMetricsTotal Citations12Total Downloads272Last 12 Months9Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2089745307",
    "type": "article"
  },
  {
    "title": "Backtracking in a Generalized Control Setting",
    "doi": "https://doi.org/10.1145/357062.357063",
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "Gary Lindstrom",
    "corresponding_authors": "Gary Lindstrom",
    "abstract": "Backtracking is a powerful conceptual and practical programming language control structure. However, its application in general has been limited to global control over recursive programs. In this paper we explore the coherence and utility of applying backtracking in a more general control setting, namely, block-structured coroutines. The following criteria are proposed for such a control combination to be judged successful: (i) retention of each control form's individual semantics; (ii) coherent semantics for each legal application of the combination; (iii) nonpreeminence of either control form, and (iv) facilitation of genuinely novel programming effects. The attainability of these criteria is assessed, with the aid of an informal language design and three illustrative applications: (i) a dual tree walk program using coroutine-managed backtracking subsystems; (ii) a context-free language intersection tester using bilevel hierarchical backtracking, and (iii) an optimizing computer job scheduler using backtracking in a simulation language context. Full programs are given for each example, phrased in a Pascal extension offering both coroutines and backtracking (expressed through nondeterministic control).",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1988603955",
    "type": "article"
  },
  {
    "title": "An Abstract Type for Statistics Collection in Simula",
    "doi": "https://doi.org/10.1145/357114.357118",
    "publication_date": "1980-10-01",
    "publication_year": 1980,
    "authors": "Carl E. Landwehr",
    "corresponding_authors": "Carl E. Landwehr",
    "abstract": "Although the use of abstract types has been widely advocated as a specification and implementation technique, their use has often been associated with programming languages that are not widely available, and examples published to date are rarely taken from actual applications. SIMULA is a widely available language that supports the use of abstract types. The purposes of this paper are (1) to demonstrate the application of the concepts of data abstraction to a common problem; (2) to demonstrate the use of data abstraction in a widely available language; and (3) to provide a portable facility for statistics collection that may make the use of SIMULA more attractive. A discussion of the background of and requirements for an abstract type for statistics collection is presented, followed by a specification for the type using traces. A SIMULA implementation, with examples of its use, is given. Finally, implementation of the abstract type in other languages is discussed.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2010829990",
    "type": "article"
  },
  {
    "title": "External Representations of Objects of User-Defined Type",
    "doi": "https://doi.org/10.1145/357094.357095",
    "publication_date": "1980-04-01",
    "publication_year": 1980,
    "authors": "Peter Wallis",
    "corresponding_authors": "Peter Wallis",
    "abstract": "The portable programming language (PPL) is one of a number of recently designed programming languages that enable the user to define new types by giving their representations and operations in terms of those of previously available types. Such provisions for the construction of objects of user-defined type have been discussed elsewhere; this work concerns the related problem of the external representations of such objects, both on input-output media and as written constants within the program text. We introduce an enhancement to the PPL design allowing specification of the external representations of objects of user-defined type. This extension to the PPL design means that objects of user-defined type can be read, written, and used as constants exactly as if their representations had been selected by the writer of the PPL compiler. The implementation and use of the added facilities are also discussed.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2083939668",
    "type": "article"
  },
  {
    "title": "Goal-directed weakening of abstract interpretation results",
    "doi": "https://doi.org/10.1145/1286821.1286830",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Sunae Seo; Hongseok Yang; Kwangkeun Yi; Taisook Han",
    "corresponding_authors": "",
    "abstract": "One proposal for automatic construction of proofs about programs is to combine Hoare logic and abstract interpretation. Constructing proofs is in Hoare logic. Discovering programs' invariants is done by abstract interpreters. One problem of this approach is that abstract interpreters often compute invariants that are not needed for the proof goal. The reason is that the abstract interpreter does not know what the proof goal is, so it simply tries to find as strong invariants as possible. These unnecessary invariants increase the size of the constructed proofs. Unless the proof-construction phase is notified which invariants are not needed, it blindly proves all the computed invariants. In this article, we present a framework for designing algorithms, called abstract-value slicers , that slice out unnecessary invariants from the results of forward abstract interpretation. The framework provides a generic abstract-value slicer that can be instantiated into a slicer for a particular abstract interpretation. Such an instantiated abstract-value slicer works as a post-processor to an abstract interpretation in the whole proof-construction process, and notifies to the next proof-construction phase which invariants it does not have to prove. Using the framework, we designed an abstract-value slicer for an existing relational analysis and applied it on programs. In this experiment, the slicer identified 62%--81% of the computed invariants as unnecessary, and resulted in 52%--84% reduction in the size of constructed proofs.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2169751832",
    "type": "article"
  },
  {
    "title": "Normalize, transpose, and distribute",
    "doi": "https://doi.org/10.1145/1330017.1330020",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Daniel E. Cooke; J. Nelson Rushton; Brad Nemanich; Robert G. Watson; Per Andersen",
    "corresponding_authors": "",
    "abstract": "SequenceL is a concise, high-level language with a simple semantics that provides for the automatic derivation of many iterative and parallel control structures. The semantics repeatedly applies a “Normalize-Transpose-Distribute” operation to functions and operators until base cases are discovered. Base cases include the grounding of variables and the application of built-in operators to operands of appropriate types. This article introduces the results of a 24-month effort to reduce the language to a very small set of primitives. Included are comparisons with other languages, the formal syntax and semantics, and the traces of several example problems run with a prototype interpreter developed in 2006.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2140835690",
    "type": "article"
  },
  {
    "title": "Transactional Sapphire",
    "doi": "https://doi.org/10.1145/3226225",
    "publication_date": "2018-12-10",
    "publication_year": 2018,
    "authors": "Tomoharu Ugawa; Carl G. Ritson; Richard Jones",
    "corresponding_authors": "",
    "abstract": "Constructing a high-performance garbage collector is hard. Constructing a fully concurrent ‘on-the-fly’ compacting collector is much more so. We describe our experience of implementing the Sapphire algorithm as the first on-the-fly, parallel, replication copying, garbage collector for the Jikes RVM Java virtual machine (JVM). In part, we explain our innovations such as copying with hardware and software transactions, on-the-fly management of Java’s reference types, and simple, yet correct, lock-free management of volatile fields in a replicating collector. We fully evaluate, for the first time, and using realistic benchmarks, Sapphire’s performance and suitability as a low latency collector. An important contribution of this work is a detailed description of our experience of building an on-the-fly copying collector for a complete JVM with some assurance that it is correct. A key aspect of this is model checking of critical components of this complicated and highly concurrent system.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2808979092",
    "type": "article"
  },
  {
    "title": "Semantic Correctness of Dependence-based Slicing for Interprocedural, Possibly Nonterminating Programs",
    "doi": "https://doi.org/10.1145/3434489",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Abu Naser Masud; Björn Lisper",
    "corresponding_authors": "",
    "abstract": "Existing proofs of correctness for dependence-based slicing methods are limited either to the slicing of intraprocedural programs [2, 39], or the proof is only applicable to a specific slicing method [4, 41]. We contribute a general proof of correctness for dependence-based slicing methods such as Weiser [50, 51], or Binkley et al. [7, 8], for interprocedural, possibly nonterminating programs. The proof uses well-formed weak and strong control closure relations, which are the interprocedural extensions of the generalised weak/strong control closure provided by Danicic et al. [13], capturing various nonterminating-insensitive and nontermination-sensitive control-dependence relations that have been proposed in the literature. Thus, our proof framework is valid for a whole range of existing control-dependence relations. We have provided a definition of semantically correct (SC) slice. We prove that SC slices agree with Weiser slicing, that deterministic SC slices preserve termination, and that nondeterministic SC slices preserve the nondeterministic behavior of the original programs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3119538636",
    "type": "article"
  },
  {
    "title": "A Scheduling Framework for Spatial Architectures Across Multiple Constraint-Solving Theories",
    "doi": "https://doi.org/10.1145/2658993",
    "publication_date": "2014-11-17",
    "publication_year": 2014,
    "authors": "Tony Nowatzki; Michael Sartin-Tarm; Lorenzo De Carli; Karthikeyan Sankaralingam; Cristian Estan; Behnam Robatmili",
    "corresponding_authors": "",
    "abstract": "Spatial architectures provide energy-efficient computation but require effective scheduling algorithms. Existing heuristic-based approaches offer low compiler/architect productivity, little optimality insight, and low architectural portability. We seek to develop a spatial-scheduling framework by utilizing constraint-solving theories and find that architecture primitives and scheduler responsibilities can be related through five abstractions: computation placement, data routing, event timing, resource utilization, and the optimization objective. We encode these responsibilities as 20 mathematical constraints, using SMT and ILP, and create schedulers for the TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using constraint solving is implementable, is practical, and can outperform specialized schedulers.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1990850541",
    "type": "article"
  },
  {
    "title": "A Logical Approach to Deciding Semantic Subtyping",
    "doi": "https://doi.org/10.1145/2812805",
    "publication_date": "2015-10-16",
    "publication_year": 2015,
    "authors": "Nils Gesbert; Pierre Genevès; Nabil Layaïda",
    "corresponding_authors": "",
    "abstract": "We consider a type algebra equipped with recursive, product, function, intersection, union, and complement types, together with type variables. We consider the subtyping relation defined by Castagna and Xu [2011] over such type expressions and show how this relation can be decided in EXPTIME, answering an open question. The novelty, originality and strength of our solution reside in introducing a logical modeling for the semantic subtyping framework. We model semantic subtyping in a tree logic and use a satisfiability-testing algorithm in order to decide subtyping. We report on practical experiments made with a full implementation of the system. This provides a powerful polymorphic type system aiming at maintaining full static type-safety of functional programs that manipulate trees, even with higher-order functions, which is particularly useful in the context of XML.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2131798111",
    "type": "article"
  },
  {
    "title": "An Extension of ATL with Strategy Interaction",
    "doi": "https://doi.org/10.1145/2734117",
    "publication_date": "2015-06-18",
    "publication_year": 2015,
    "authors": "Farn Wang; Sven Schewe; Chung-Hao Huang",
    "corresponding_authors": "",
    "abstract": "We propose an extension to ATL ( alternating-time temporal logic ), called BSIL ( basic strategy-interaction logic ), for specifying collaboration among agents in a multiagent system. We show that BSIL is strictly more expressive than ATL + but incomparable with ATL * , GL ( game logic ), and AMC ( alternating μ-calculus ) in expressiveness. We show that a memoryful strategy is necessary for fulfilling a specification in BSIL. We establish that the BSIL model-checking problem is PSPACE-complete. However, BSIL model checking can be performed in time quadratic in the model for fixed formulas. The BSIL (and hence ATL + ) satisfiability is 2EXPTIME-complete. Finally, we report our experiment with a model checker for BSIL.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2398056971",
    "type": "article"
  },
  {
    "title": "Satisfiability Modulo Ordering Consistency Theory for SC, TSO, and PSO Memory Models",
    "doi": "https://doi.org/10.1145/3579835",
    "publication_date": "2023-01-17",
    "publication_year": 2023,
    "authors": "Hongyu Fan; Zhihang Sun; Fei He",
    "corresponding_authors": "",
    "abstract": "Automatically verifying multi-threaded programs is difficult because of the vast number of thread interleavings, a problem aggravated by weak memory consistency. Partial orders can help with verification because they can represent many thread interleavings concisely. However, there is no dedicated decision procedure for solving partial-order constraints. In this article, we propose a novel ordering consistency theory for concurrent program verification that is applicable not only under sequential consistency, but also under the TSO and PSO weak memory models. We further develop an efficient theory solver, which checks consistency incrementally, generates minimal conflict clauses, and includes a custom propagation procedure. We have implemented our approach in a tool, called Zord , and have conducted extensive experiments on the SV-COMP 2020 ConcurrencySafety benchmarks. Our experimental results show a significant improvement over the state-of-the-art.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4316813703",
    "type": "article"
  },
  {
    "title": "Path analysis and the optimization of nonstrict functional languages",
    "doi": "https://doi.org/10.1145/177492.177497",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Adrienne Bloss",
    "corresponding_authors": "Adrienne Bloss",
    "abstract": "The functional programming style is increasingly popular in the research world, but functional languages still execute slowly relative to imperative languages. This is largely because the power and flexibility of functional languages restrict the amount of information readily available to the compiler, hindering its ability to generate good code. This article demonstrates that information about order of evaluation of expressions can be statically inferred for nonstrict functional programs and that optimizations based on this information can provide substantial speedups at runtime. We present an exact, nonstandard semantics called path semantics that models order of evaluation in a nonstrict, sequential functional language, and its computable abstraction, path analysis . We show how the information inferred by path analysis can be used to implement destructive aggregate updating, in which updates on functional aggregates that are provably not live are done destructively. We also demonstrate a new approach to strictness analysis and show that strictness analysis is subsumed by path analysis. Benchmarks are presented.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1995688020",
    "type": "article"
  },
  {
    "title": "Within ARM's reach",
    "doi": "https://doi.org/10.1145/291889.291903",
    "publication_date": "1998-05-01",
    "publication_year": 1998,
    "authors": "Wan Fokkink; Jasper F.T Kamperman; Pum Walters",
    "corresponding_authors": "",
    "abstract": "A new compilation technique for left-linear term-rewriting systems is presented, where rewrite rules are transformed into so-called minimal rewrite rules. These minimal rules have such a simple form that they can be viewed as instructions for an abstract rewriting machine (ARM).",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2051883829",
    "type": "article"
  },
  {
    "title": "Dependency analysis for Standard ML",
    "doi": "https://doi.org/10.1145/325478.325481",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "M. Blume",
    "corresponding_authors": "M. Blume",
    "abstract": "Automatic dependency analysis is a useful addition to a system like CM, our compilation manager for Standard ML of New Jersey. It relieves the programmer from the tedious and error-prone task of having to specify compilation dependencies by hand and thereby makes its usage more user friendly. But dependency analysis is not easy, as the general problem for Standard ML is NP-complete. Therefore, CM has to impose certain restrictions on the programming language to recover tractability. We prove the NP-completeness result, discuss the restrictions on ML that are used by CM, and provide the resulting analysis algorithms.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2058839085",
    "type": "article"
  },
  {
    "title": "A new framework for elimination-based data flow analysis using DJ graphs",
    "doi": "https://doi.org/10.1145/276393.278523",
    "publication_date": "1998-03-01",
    "publication_year": 1998,
    "authors": "Vugranam C. Sreedhar; Guang R. Gao; Yong-Fong Lee",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A new framework for elimination-based data flow analysis using DJ graphs Authors: Vugranam C. Sreedhar Hewlett-Packard Company Hewlett-Packard CompanyView Profile , Guang R. Gao University of Delaware University of DelawareView Profile , Yong-Fong Lee Intel Corporation Intel CorporationView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 20Issue 2pp 388–435https://doi.org/10.1145/276393.278523Published:01 March 1998Publication History 12citation2,028DownloadsMetricsTotal Citations12Total Downloads2,028Last 12 Months12Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2091789763",
    "type": "article"
  },
  {
    "title": "A systematic study of functional language implementations",
    "doi": "https://doi.org/10.1145/276393.276397",
    "publication_date": "1998-03-01",
    "publication_year": 1998,
    "authors": "Rémi Douence; Pascal Fradet",
    "corresponding_authors": "",
    "abstract": "We introduce a unified framework to describe, relate, compare, and classify functional language implementations. The compilation process is expressed as a succession of program transformations in the common framework. At each step, different transformations model fundamental choices. A benefit of this approach is to structure and decompose the implementation process. The correctness proofs can be tackled independently for each step and amount to proving program transformations in the functional world. This approach also paves the way to formal comparisons by making it possible to estimate the complexity of individual transformations or compositions of them. Our study aims at covering the whole known design space of sequential functional language implementations. In particular, we consider call-by-value, call-by-name, call-by-need reduction strategies as well as environment- and graph-based implementations. We describe for each compilation step the diverse alternatives as program transformations. In some cases, we illustrate how to compare or relate compilation techniques, express global optimizations, or hybrid implementations. We also provide a classification of well-known abstract machines.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2056160788",
    "type": "article"
  },
  {
    "title": "A correctness proof for combinator reduction with cycles",
    "doi": "https://doi.org/10.1145/77606.77612",
    "publication_date": "1990-01-03",
    "publication_year": 1990,
    "authors": "William M. Farmer; John D. Ramsdell; Ronald Watro",
    "corresponding_authors": "",
    "abstract": "Turner popularized a technique of Wadsworth in which a cyclic graph rewriting rule is used to implement reduction of the fixed point combinator Y . We examine the theoretical foundation of this approach. Previous work has concentrated on proving that graph methods are, in a certain sense, sound and complete implementations of term methods. This work is inapplicable to the cyclic Y rule, which is unsound in this sense since graph normal forms can exist without corresponding term normal forms. We define and prove the correctness of combinator head reduction using the cyclic Y rule; the correctness of normal reduction is an immediate consequence. Our proof avoids the use of infinite trees to explain cyclic graphs. Instead, we show how to consider reduction with cycles as an optimization of reduction without cycles.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1974010254",
    "type": "article"
  },
  {
    "title": "Determining the extent of lookahead in syntactic error repair",
    "doi": "https://doi.org/10.1145/44501.44505",
    "publication_date": "1988-07-01",
    "publication_year": 1988,
    "authors": "Jon Mauney; Charles N. Fischer",
    "corresponding_authors": "",
    "abstract": "Many syntactic error repair strategies examine several additional symbols of input to guide the choice of a repair; a problem is determining how many symbols to examine. The goal of gathering all relevant information is discussed and shown to be impractical; instead we can gather all information relevant to choosing among a set of “minimal repairs.” We show that finding symbols with the property “Moderate Phrase-Level Uniqueness” is sufficient to establish that all information relevant to these minimal repairs has been seen. Empirical results on the occurrence of such symbols in Pascal are presented.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2073051381",
    "type": "article"
  },
  {
    "title": "Generation of LR parsers by partial evaluation",
    "doi": "https://doi.org/10.1145/349214.349219",
    "publication_date": "2000-03-01",
    "publication_year": 2000,
    "authors": "Michael Sperber; Peter Thiemann",
    "corresponding_authors": "",
    "abstract": "The combination of modern programming languages and partial evaluation yields new approaches to old problems. In particular, the combination of functional programming and partial evaluation can turn a general parser into a parser generator. We use an inherently functional approach to implement general LR( k ) parsers and specialize them with respect to the input grammars using offline partial evaluation. The functional specification of LR parsing yields a concise implementation of the algorithms themselves. Furthermore, we demonstrate the elegance of the functional approach by incorporating on-the-fly attribute evaluation for S-attributed grammars and two schemes for error recovery, which lend themselves to natural and elegant implementation. The parser require only minor changes to achieve good specialization results. The generated parsers have production quality and match those produced by traditional parser generators in sped and compactness",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1987823535",
    "type": "article"
  },
  {
    "title": "Transformational Derivation of a Garbage Collection Algorithm",
    "doi": "https://doi.org/10.1145/69622.357188",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "Robert Dewar; Micha Shirar; Elia Weixelbaum",
    "corresponding_authors": "",
    "abstract": "article Free AccessTransformational Derivation of a Garbage Collection Algorithm Authors: Robert B. K. Dewar Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, New York, NY Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, New York, NYView Profile , Micha Shirar School of Mathematical Sciences, Tel-Aviv University, Ramat-Aviv, Tel Aviv 69976, Israel School of Mathematical Sciences, Tel-Aviv University, Ramat-Aviv, Tel Aviv 69976, IsraelView Profile , Elia Weixelbaum Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, New York, NY Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, New York, NYView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 650–667https://doi.org/10.1145/69622.357188Published:01 October 1982Publication History 9citation289DownloadsMetricsTotal Citations9Total Downloads289Last 12 Months12Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1992140857",
    "type": "article"
  },
  {
    "title": "Dealing with world-model-based programs",
    "doi": "https://doi.org/10.1145/3318.3479",
    "publication_date": "1985-04-01",
    "publication_year": 1985,
    "authors": "Giuseppina Gini; Maria Gini",
    "corresponding_authors": "",
    "abstract": "We introduce POINTY, an interactive system for constructing world-model-based programs for robots. POINTY combines an interactive programming environment with the teaching-by-guiding methodology that has been successful in industrial robotics. Owing to its ability to control robots in real time, and to interact with the user, POINTY provides a friendly and powerful programming environment for robot applications. In the past few years, POINTY has been in use at Stanford to write, test, and debug various robot programs.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2012608504",
    "type": "article"
  },
  {
    "title": "A transformational approach to binary translation of delayed branches",
    "doi": "https://doi.org/10.1145/641888.641890",
    "publication_date": "2003-03-01",
    "publication_year": 2003,
    "authors": "Norman F. Ramsey; Cristina Cifuentes",
    "corresponding_authors": "",
    "abstract": "A binary translator examines binary code for a source machine and generates code for a target machine. Understanding what to do with delayed branches in binary code can involve tricky case analyses, for example, if there is a branch instruction in a delay slot. This article presents a disciplined method for deriving such case analyses. The method identifies problematic cases, shows the translations for the nonproblematic cases, and gives confidence that all cases are considered. The method supports such common architectures as SPARC, MIPS, and PA-RISC, and it should apply to any tool that analyzes machine instructions. We begin by writing a very simple interpreter for the source machine's code. We then transform the interpreter into an interpreter for a target machine without delayed branches. To maintain the semantics of the program being interpreted, we simultaneously transform the sequence of source-machine instructions into a sequence of target-machine instructions. The transformation of the instructions becomes our algorithm for binary translation.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2004141018",
    "type": "article"
  },
  {
    "title": "Referencing and Retention in Block-Structured Coroutines",
    "doi": "https://doi.org/10.1145/357139.357143",
    "publication_date": "1981-07-01",
    "publication_year": 1981,
    "authors": "Gary Lindstrom; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Referencing and Retention in Block-Structured Coroutines Authors: Gary Lindstrom Department of Computer Science, The University of Utah, 3160 Merrill Engineering Building, Salt Lake City, UT Department of Computer Science, The University of Utah, 3160 Merrill Engineering Building, Salt Lake City, UTView Profile , Mary Lou Soffa Department of Computer Science, University of Pittsburgh, Pittsburgh, PA Department of Computer Science, University of Pittsburgh, Pittsburgh, PAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 3July 1981 pp 263–292https://doi.org/10.1145/357139.357143Published:01 July 1981Publication History 9citation303DownloadsMetricsTotal Citations9Total Downloads303Last 12 Months17Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2028972189",
    "type": "article"
  },
  {
    "title": "Code Generation and Storage Allocation for Machines with Span-Dependent Instructions",
    "doi": "https://doi.org/10.1145/357062.357067",
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "Edward L. Robertson",
    "corresponding_authors": "Edward L. Robertson",
    "abstract": "Many machine languages have two instruction formats, one of which allows addressing of “nearby” operands with “short” (e.g. one word) instructions, while “faraway” operands require “long” format (e.g. two words). Because the distance between an instruction and its operand depends upon the formats of the intervening instructions, the formats of different instructions may be interdependent. An efficient technique is discussed which optimally assigns formats to instructions in a given program and is practical in space as well as time. The more sophisticated problem of arranging operands within programs is discussed. Unfortunately, it is unlikely that an efficient algorithm can guarantee even a good approximation for this problem, since it is shown that r -approximation is NP-complete. Finally, implications of these problems for hardware and software design are considered.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2085255464",
    "type": "article"
  },
  {
    "title": "A provenly correct translation of Fickle into Java",
    "doi": "https://doi.org/10.1145/1216374.1216381",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Davide Ancona; Christopher Anderson; Ferruccio Damiani; Sophia Drossopoulou; Paola Giannini; Elena Zucca",
    "corresponding_authors": "",
    "abstract": "We present a translation from Fickle , a small object-oriented language allowing objects to change their class at runtime, into Java. The translation is provenly correct in the sense that it preserves the static and dynamic semantics. Moreover, it is compatible with separate compilation, since the translation of a Fickle class does not depend on the implementation of used classes. Based on the formal system, we have developed an implementation. The translation turned out to be a more subtle problem than we expected. In this article, we discuss four possible approaches we considered for the design of the translation and to justify our choice, we present formally the translation and proof of preservation of the static and dynamic semantics, and discuss the prototype implementation. Moreover, we outline an alternative translation based on generics that avoids most of the casts (but not all) needed in the previous translation. The language Fickle has undergone and is still undergoing several phases of development. In this article we are discussing the translation of Fickle II .",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1997553382",
    "type": "article"
  },
  {
    "title": "Design, implementation, and evaluation of a compilation server",
    "doi": "https://doi.org/10.1145/1255450.1255451",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Han B. Lee; Amer Diwan; J. Eliot B. Moss",
    "corresponding_authors": "",
    "abstract": "Modern JVM implementations interleave execution with compilation of “hot” methods to achieve reasonable performance. Since compilation overhead impacts the execution time of the application and induces run-time pauses, we explore offloading compilation onto a compilation server. In this article, we present the design, implementation, and evaluation of a compilation server that compiles and optimizes Java bytecodes on behalf of its clients. We show that the compilation server provides the following benefits for our benchmark programs: (i) lower execution time by reducing the compilation overhead and by enabling more aggressive optimizations; (ii) lower memory allocation by eliminating allocations due to optimizing compilation and the footprint of the optimizing compiler; (iii) lower execution time of the application due to sharing of profile information across different runs of the same application and runs of different applications. We implemented the compilation server in Jikes RVM, and our results indicate that it can reduce running time by an average of 20.5%, interruptions due to compilation by an average of 81.0%, and dynamic memory allocation by 8.6% for our benchmark programs. Simulation results indicate that our current implementation of the compilation server can handle more than 50 concurrent clients while still allowing them to outperform the best performing adaptive configuration.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2120729606",
    "type": "article"
  },
  {
    "title": "Mostly static program partitioning of binary executables",
    "doi": "https://doi.org/10.1145/1538917.1538918",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Efe Yardimci; Michael Franz",
    "corresponding_authors": "",
    "abstract": "We have built a runtime compilation system that takes unmodified sequential binaries and improves their performance on off-the-shelf multiprocessors using dynamic vectorization and loop-level parallelization techniques. Our system, Azure, is purely software based and requires no specific hardware support for speculative thread execution, yet it is able to break even in most cases; that is, the achieved speedup exceeds the cost of runtime monitoring and compilation, often by significant amounts. Key to this remarkable performance is an offline preprocessing step that extracts a mostly correct control flow graph (CFG) from the binary program ahead of time. This statically obtained CFG is incomplete in that it may be missing some edges corresponding to computed branches. We describe how such additional control flow edges are discovered and handled at runtime, so that an incomplete static analysis never leads to an incorrect optimization result. The availability of a mostly correct CFG enables us to statically partition a binary executable into single-entry multiple-exit regions and to identify potential parallelization candidates ahead of execution. Program regions that are not candidates for parallelization can thereby be excluded completely from runtime monitoring and dynamic recompilation. Azure's extremely low overhead is a direct consequence of this design.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1984434723",
    "type": "article"
  },
  {
    "title": "Witnessing side effects",
    "doi": "https://doi.org/10.1145/1353445.1353449",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Tachio Terauchi; Alex Aiken",
    "corresponding_authors": "",
    "abstract": "We present a new approach to the old problem of adding global mutable state to purely functional languages. Our idea is to extend the language with “witnesses,” which is based on an arguably more pragmatic motivation than past approaches. We give a semantic condition for correctness and prove it is sufficient. We also give a somewhat surprising static checking algorithm that makes use of a network flow property equivalent to the semantic condition via reduction to a satisfaction problem for a system of linear inequalities.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2003607268",
    "type": "article"
  },
  {
    "title": "Term transformers",
    "doi": "https://doi.org/10.1145/1516507.1516511",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Joseph M. Morris; Alexander Bunkenburg; Malcolm Tyrrell",
    "corresponding_authors": "",
    "abstract": "We present a new approach to adding state and state-changing commands to a term language. As a formal semantics it can be seen as a generalization of predicate transformer semantics, but beyond that it brings additional opportunities for specifying and verifying programs. It is based on a construct called a phrase , which is a term of the form C ▹ t , where C stands for a command and t stands for a term of any type. If R is boolean, C ▹ R is closely related to the weakest precondition wp ( C , R ). The new theory draws together functional and imperative programming in a simple way. In particular, imperative procedures and functions are seen to be governed by the same laws as classical functions. We get new techniques for reasoning about programs, including the ability to dispense with logical variables and their attendant complexities. The theory covers both programming and specification languages, and supports unbounded demonic and angelic nondeterminacy in both commands and terms.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2034373223",
    "type": "article"
  },
  {
    "title": "Verifying safety properties of concurrent heap-manipulating programs",
    "doi": "https://doi.org/10.1145/1745312.1745315",
    "publication_date": "2008-05-24",
    "publication_year": 2008,
    "authors": "Eran Yahav; Mooly Sagiv",
    "corresponding_authors": "",
    "abstract": "We provide a parametric framework for verifying safety properties of concurrent heap-manipulating programs. The framework combines thread-scheduling information with information about the shape of the heap. This leads to verification algorithms that are more precise than existing techniques. The framework also provides a precise shape-analysis algorithm for concurrent programs. In contrast to most existing verification techniques, we do not put a bound on the number of allocated objects. The framework produces interesting results even when analyzing programs with an unbounded number of threads. The framework is applied to successfully verify the following properties of a concurrent program: —Concurrent manipulation of linked-list based ADT preserves the ADT datatype invariant. —The program does not perform inconsistent updates due to interference. —The program does not reach a deadlock. —The program does not produce runtime errors due to illegal thread interactions. We also found bugs in erroneous programs violating such properties. A prototype of our framework has been implemented and applied to small, but interesting, example programs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2152687616",
    "type": "article"
  },
  {
    "title": "On Polymorphic Sessions and Functions",
    "doi": "https://doi.org/10.1145/3457884",
    "publication_date": "2021-06-10",
    "publication_year": 2021,
    "authors": "Bernardo Toninho; Nobuko Yoshida",
    "corresponding_authors": "",
    "abstract": "This work exploits the logical foundation of session types to determine what kind of type discipline for the Λ-calculus can exactly capture, and is captured by, Λ-calculus behaviours. Leveraging the proof theoretic content of the soundness and completeness of sequent calculus and natural deduction presentations of linear logic, we develop the first mutually inverse and fully abstract processes-as-functions and functions-as-processes encodings between a polymorphic session π-calculus and a linear formulation of System F. We are then able to derive results of the session calculus from the theory of the Λ-calculus: (1) we obtain a characterisation of inductive and coinductive session types via their algebraic representations in System F; and (2) we extend our results to account for value and process passing, entailing strong normalisation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3172689801",
    "type": "article"
  },
  {
    "title": "Practical Integrated Analysis of Pointers, Dataflow and Control Flow",
    "doi": "https://doi.org/10.1145/2450136.2450140",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Stefan Staiger-Stöhr",
    "corresponding_authors": "Stefan Staiger-Stöhr",
    "abstract": "This article presents a family of static analyses to determine pointer targets, control flow, and dataflow in combination. The integrated solution to these mutually dependent problems approaches the result from the optimistic side. It is a general strategy for static program analysis and does not need any upfront approximation for one of the problems to overcome the mutual dependencies. A degenerated case yields Andersen’s famous pointer analysis; otherwise, the analyses are flow-sensitive and can support direct and indirect strong updates, within the same cubic asymptotic complexity as known for Andersen, albeit with larger constants. Surprisingly, the ideas behind the integrated analysis are intuitive. The strategy we describe naturally evolves from considering the mutual dependencies between the three problems, or from generalizing Andersen’s analysis to flow sensitivity. Such a flow-sensitive Andersen analysis not only computes pointer targets with higher precision than the original analysis, but it also creates an interprocedural SSA form at the same time. Our extensive experimental evaluation shows that the integrated solution is practical as it can be applied to reasonably large real-world programs within a few seconds or minutes. This uses some optimizations which together achieve a speedup of more than 100 for several programs. We compare several members of the family of analyses, from flow- and field-insensitive to flow- and field-sensitive with strong updates, both with and without optimizations. This gives some insights into the effects of these dimensions of precision on the results. It also sheds new light on the benefits of flow sensitivity versus the costs associated with it.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2125615564",
    "type": "article"
  },
  {
    "title": "Newtonian Program Analysis via Tensor Product",
    "doi": "https://doi.org/10.1145/3024084",
    "publication_date": "2017-03-21",
    "publication_year": 2017,
    "authors": "Thomas Reps; Emma Turetsky; Prathmesh Prabhu",
    "corresponding_authors": "",
    "abstract": "Recently, Esparza et al. generalized Newton’s method—a numerical-analysis algorithm for finding roots of real-valued functions—to a method for finding fixed-points of systems of equations over semirings. Their method provides a new way to solve interprocedural dataflow-analysis problems. As in its real-valued counterpart, each iteration of their method solves a simpler “linearized” problem. One of the reasons this advance is exciting is that some numerical analysts have claimed that “‘all’ effective and fast iterative [numerical] methods are forms (perhaps very disguised) of Newton’s method.” However, there is an important difference between the dataflow-analysis and numerical-analysis contexts: When Newton’s method is used in numerical-analysis problems, commutativity of multiplication is relied on to rearrange an expression of the form “ a * X * b + c * X * d ” into “( a * b + c * d )* X .” Equations with such expressions correspond to path problems described by regular languages. In contrast, when Newton’s method is used for interprocedural dataflow analysis, the “multiplication” operation involves function composition and hence is non-commutative: “ a * X * b + c * X * d ” cannot be rearranged into “( a * b + c * d )* X .” Equations with such expressions correspond to path problems described by linear context-free languages (LCFLs). In this article, we present an improved technique for solving the LCFL sub-problems produced during successive rounds of Newton’s method. Our method applies to predicate abstraction, on which most of today’s software model checkers rely.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2600895373",
    "type": "article"
  },
  {
    "title": "Affine Refinement Types for Secure Distributed Programming",
    "doi": "https://doi.org/10.1145/2743018",
    "publication_date": "2015-08-13",
    "publication_year": 2015,
    "authors": "Michele Bugliesi; Stefano Calzavara; Fabienne Eigner; Matteo Maffei",
    "corresponding_authors": "",
    "abstract": "Recent research has shown that it is possible to leverage general-purpose theorem-proving techniques to develop powerful type systems for the verification of a wide range of security properties on application code. Although successful in many respects, these type systems fall short of capturing resource-conscious properties that are crucial in large classes of modern distributed applications. In this article, we propose the first type system that statically enforces the safety of cryptographic protocol implementations with respect to authorization policies expressed in affine logic. Our type system draws on a novel notion of “exponential serialization” of affine formulas, a general technique to protect affine formulas from the effect of duplication. This technique allows formulate of an expressive logical encoding of the authentication mechanisms underpinning distributed resource-aware authorization policies. We discuss the effectiveness of our approach on two case studies: the EPMO e-commerce protocol and the Kerberos authentication protocol. We finally devise a sound and complete type-checking algorithm, which is the key to achieving an efficient implementation of our analysis technique.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2230191103",
    "type": "article"
  },
  {
    "title": "A First-order Logic with Frames",
    "doi": "https://doi.org/10.1145/3583057",
    "publication_date": "2023-02-09",
    "publication_year": 2023,
    "authors": "Adithya Murali; Lucas Peña; Christof Löding; P. Madhusudan",
    "corresponding_authors": "",
    "abstract": "We propose a novel logic, Frame Logic (FL), that extends first-order logic and recursive definitions with a construct Sp (·) that captures the implicit supports of formulas—the precise subset of the universe upon which their meaning depends. Using such supports, we formulate proof rules that facilitate frame reasoning elegantly when the underlying model undergoes change. We show that the logic is expressive by capturing several data-structures and also exhibit a translation from a precise fragment of separation logic to frame logic. Finally, we design a program logic based on frame logic for reasoning with programs that dynamically update heaps that facilitates local specifications and frame reasoning. This program logic consists of both localized proof rules as well as rules that derive the weakest tightest preconditions in frame logic.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4319734651",
    "type": "article"
  },
  {
    "title": "Focusing on Refinement Typing",
    "doi": "https://doi.org/10.1145/3610408",
    "publication_date": "2023-10-17",
    "publication_year": 2023,
    "authors": "Dimitrios J. Economou; Neel Krishnaswami; Jana Dunfield",
    "corresponding_authors": "",
    "abstract": "We present a logically principled foundation for systematizing, in a way that works with any computational effect and evaluation order, SMT constraint generation seen in refinement type systems for functional programming languages. By carefully combining a focalized variant of call-by-push-value, bidirectional typing, and our novel technique of value-determined indexes, our system generates solvable SMT constraints without existential (unification) variables. We design a polarized subtyping relation allowing us to prove our logically focused typing algorithm is sound, complete, and decidable. We prove type soundness of our declarative system with respect to an elementary domain-theoretic denotational semantics. Type soundness implies, relatively simply, the total correctness and logical consistency of our system. The relative ease with which we obtain both algorithmic and semantic results ultimately stems from the proof-theoretic technique of focalization.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387693588",
    "type": "article"
  },
  {
    "title": "An evaluation of an automatically generated compiler",
    "doi": "https://doi.org/10.1145/213978.213980",
    "publication_date": "1995-09-01",
    "publication_year": 1995,
    "authors": "Anthony M. Sloane",
    "corresponding_authors": "Anthony M. Sloane",
    "abstract": "Compilers or language translators can be generated using a variety of formal specification techniques. Whether generation is worthwhile depends on the effort required to specify the translation task and the quality of the generated compiler. A systematic comparison was conducted between a hand-coded translator for the Icon programming language and one generated by the Eli compiler construction system. A direct comparison could be made since the generated program performs the same translation as the hand-coded program. The results of the comparison show that efficient compilers can be generated from specifications that are much smaller and more problem oriented than the equivalent source code. We also found that further work must be done to reduce the dynamic memory usage of the generated compilers.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2007728761",
    "type": "article"
  },
  {
    "title": "Dynamic currency determination in optimized programs",
    "doi": "https://doi.org/10.1145/295656.295657",
    "publication_date": "1998-11-01",
    "publication_year": 1998,
    "authors": "Dhananjay M. Dhamdhere; K. V. Sankaranarayanan",
    "corresponding_authors": "",
    "abstract": "Compiler optimizations pose many problems to source-level debugging of an optimized program due to reordering, insertion, and deletion of code. On such problem is to determine whether the value of a varible is current at a breakpoint—that is, whether its actual value is the same as its expected value. We use the notion of dynamic currency of a variable in source-level debugging and propose the use of a minimal unrolled graph to reduce the run-time overhead of dynamic currency determination. We prove that the minimal unrolled graph is an adequate basis for performing bit-vector data flow analyses at a breakpoint. This property is used to perform dynamic currency determination. It is also shown to help in recovery of a dynamically noncurrent variable.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2010831067",
    "type": "article"
  },
  {
    "title": "Toward a complete transformational toolkit for compilers",
    "doi": "https://doi.org/10.1145/265943.265944",
    "publication_date": "1997-09-01",
    "publication_year": 1997,
    "authors": "J.A. Bergstra; T. B. Dinesh; John K. Field; Jan Heering",
    "corresponding_authors": "",
    "abstract": "PIM is an equational logic designed to function as a “transformational toolkit” for compilers and other programming tools that analyze and manipulate imperative languages. It has been applied to such problems as program slicing, symbolic evaluation, conditional constant propagation, and dependence analysis. PIM consists of the untyped lambda calculus extended with an algebraic data type that characterizes the behavior of lazy stores and generalized conditionals. A graph form of PIM terms is by design closely related to several intermediate representations commonly used in optimizing compilers. In this article, we show that PIM's core algebraic component, PIM t , possesses a complete equational axiomatization (under the assumption of certain reasonable restrictions on term formation). This has the practical consequence of guaranteeing that every semantics-preserving transformation on a program representable in PIM t can be derived by application of PIM t rules. We systematically derive the complete PIM t logic as the culmination of a sequence of increasingly powerful equational systems starting from a straightforward “interpreter” for closed PIM t terms. This work is an intermediate step in a larger program to develop a set of well-founded tools for manipulation of imperative programs by compilers and other systems that perform program analysis.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2018793132",
    "type": "article"
  },
  {
    "title": "Trace-based network proof systems",
    "doi": "https://doi.org/10.1145/129393.129396",
    "publication_date": "1992-05-01",
    "publication_year": 1992,
    "authors": "Jennifer Widom; David Gries; Fred B. Schneider",
    "corresponding_authors": "",
    "abstract": "We consider incomplete trace-based network proof systems for safety properties, identifying extensions that are necessary and sufficient to achieve relative completeness. We investigate the expressiveness required of any trace logic to encode these extensions.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2096412050",
    "type": "article"
  },
  {
    "title": "Transforming acyclic programs",
    "doi": "https://doi.org/10.1145/183432.183434",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Annalisa Bossi; Sandro Etalle",
    "corresponding_authors": "",
    "abstract": "An unfold/fold transformation system is a source-to-source rewriting methodology devised to improve the efficiency of a program. Any such transformation should preserve the main properties of the initial program: among them, termination. In the field of logic programming, the class of acyclic programs plays an important role in this respect, since it is closely related to the one of terminating programs. The two classes coincide when negation is not allowed in the bodies of the clauses. We prove that the Unfold/Fold transformation system defined by Tamaki and Sato preserves the acyclicity of the initial program. From this result, it follows that when the transformation is applied to an acyclic program, then the finite failure set for definite programs is preserved; in the case of normal programs, all major declarative and operational semantics are preserved as well. These results cannot be extended to the class of left-terminating programs without modifying the definition of the transformation.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2139887734",
    "type": "article"
  },
  {
    "title": "Optimizing compilation of CLP( ℛ )",
    "doi": "https://doi.org/10.1145/295656.295661",
    "publication_date": "1998-11-01",
    "publication_year": 1998,
    "authors": "Andrew D. Kelly; Kim Marriott; Andrew Macdonald; Peter J. Stuckey; Roland H. C. Yap",
    "corresponding_authors": "",
    "abstract": "Constraint Logic Programming (CLP) languages extend logic programming by allowing the use of constraints from different domains such as real numbers or Boolean functions. They have proved to be ideal for expressing problems that require interactive mathematical modeling and complex combinatorial optimization problems. However, CLP languages have mainly been considered as research systems, useful for rapid prototyping, by not really competitive with more conventional programming languages where efficiency is a more important consideration. One promising approach to improving the performance of CLP systems is the use of powerful program optimizations to reduce the cost of constraint solving. We extend work in this area by describing a new optimizing compiler for the CLP language CLP(ℛ). The compiler implements six powerful optimizations: reordering of constraints, removal of redundant variables, and specialization of constraints which cannot fail. Each program optimization is designed to remove the overhead of constraint solving when possible and keep the number of constraints in the store as small as possible. We systematically evaluate the effectiveness of each optimization in isolation and in combination. Our empirical evaluation of the compiler verifies that optimizing compilation can be made efficient enough to allow compilation of real-world programs and that it is worth performing such compilation because it gives significant time and space performance improvements.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1979577606",
    "type": "article"
  },
  {
    "title": "A fully abstract semantics for a first-order functional language with logic variables",
    "doi": "https://doi.org/10.1145/115372.115371",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Radha Jagadeesan; Keshav Pingali; Prakash Panangaden",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A fully abstract semantics for a first-order functional language with logic variables Authors: Radha Jagadeesan Cornell Univ., Ithaca, NY Cornell Univ., Ithaca, NYView Profile , Keshav Pingali Cornell Univ., Ithaca, NY Cornell Univ., Ithaca, NYView Profile , Prakash Panangaden McGill Univ., Montreal, P.Q., Canada McGill Univ., Montreal, P.Q., CanadaView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 4Oct. 1991 pp 577–625https://doi.org/10.1145/115372.115371Published:01 October 1991Publication History 7citation367DownloadsMetricsTotal Citations7Total Downloads367Last 12 Months10Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2037452045",
    "type": "article"
  },
  {
    "title": "Soundness of Hoare's logic: an automated proof using LCF",
    "doi": "https://doi.org/10.1145/9758.11326",
    "publication_date": "1987-01-01",
    "publication_year": 1987,
    "authors": "S. Sokołowski",
    "corresponding_authors": "S. Sokołowski",
    "abstract": "This paper presents a natural deduction proof of Hoare's logic carried out by the Edinburgh LCF theorem prover. The emphasis is on the way Hoare's theory is presented to the LCF, which looks very much like an exposition of syntax and semantics to human readers; and on the programmable heuristics (tactics). We also discuss some problems and possible improvements to the LCF.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1979753046",
    "type": "article"
  },
  {
    "title": "Typed representation of objects by functions",
    "doi": "https://doi.org/10.1145/59287.77345",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "Jørgen Steensgaard‐Madsen",
    "corresponding_authors": "Jørgen Steensgaard‐Madsen",
    "abstract": "A systematic representation of objects grouped into types by constructions similar to the composition of sets in mathematics is proposed. The representation is by lambda expressions, which supports the representation of objects from function spaces. The representation is related to a rather conventional language of type descriptions in a way that is believed to be new. Ordinary control-expressions (i.e.,case- and let-expressions) are derived from the proposed representation.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2045531281",
    "type": "article"
  },
  {
    "title": "Automatic derivation of compiler machine descriptions",
    "doi": "https://doi.org/10.1145/567097.567100",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Christian Collberg",
    "corresponding_authors": "Christian Collberg",
    "abstract": "We describe a method designed to significantly reduce the effort required to retarget a compiler to a new architecture, while at the same time producing fast and effective compilers. The basic idea is to use the native C compiler at compiler construction time to discover architectural features of the new architecture. From this information a formal machine description is produced. Given this machine description, a native code-generator can be generated by a back-end generator such as BEG or burg. A prototype automatic Architecture Discovery Tool (called ADT) has been implemented. This tool is completely automatic and requires minimal input from the user. Given the Internet address of the target machine and the command-lines by which the native C compiler, assembler, and linker are invoked, ADT will generate a BEG machine specification containing the register set, addressing modes, instruction set, and instruction timings for the architecture. The current version of ADT is general enough to produce machine descriptions for the integer instruction sets of common RISC and CISC architectures such as the Sun SPARC, Digital Alpha, MIPS, DEC VAX, and Intel x86.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2053968119",
    "type": "article"
  },
  {
    "title": "Intensional analysis of quantified types",
    "doi": "https://doi.org/10.1145/641888.641889",
    "publication_date": "2003-03-01",
    "publication_year": 2003,
    "authors": "Bratin Saha; Valery Trifonov; Zhong Shao",
    "corresponding_authors": "",
    "abstract": "Compilers for polymorphic languages can use run-time type inspection to support advanced implementation techniques such as tagless garbage collection, polymorphic marshalling, and flattened data structures. Intensional type analysis is a type-theoretic framework for expressing and certifying such type-analyzing computations. Unfortunately, existing approaches to intensional analysis do not work well on quantified types such as existential or polymorphic types. This makes it impossible to code (in a type-safe language) applications such as garbage collection, persistency, or marshalling which must be able to examine the type of any run-time value. We present a typed intermediate language that supports the analysis of quantified types. In particular, we provide both type-level and term-level constructs for analyzing quantified types. Our system supports structural induction on quantified types yet type-checking remains decidable. We also show that our system is compatible with a type-erasure semantics.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1967041135",
    "type": "article"
  },
  {
    "title": "A Note on Median Split Trees",
    "doi": "https://doi.org/10.1145/357084.357092",
    "publication_date": "1980-01-01",
    "publication_year": 1980,
    "authors": "Douglas E. Comer",
    "corresponding_authors": "Douglas E. Comer",
    "abstract": "The median split tree is an elegant technique for searching static sets of keys when the frequency of access is highly skewed. This paper generalizes the median split search technique, relates it to sequential search and binary search, discusses the choice of an optimum search strategy for a given set of keys, and poses several related optimization problems.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2037155564",
    "type": "article"
  },
  {
    "title": "Modular refinement of hierarchic reactive machines",
    "doi": "https://doi.org/10.1145/973097.973101",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Rajeev Alur; Radu Grosu",
    "corresponding_authors": "",
    "abstract": "Scalable formal analysis of reactive programs demands integration of modular reasoning techniques with existing analysis tools. Modular reasoning principles such as abstraction, compositional refinement, and assume-guarantee reasoning are well understood for architectural hierarchy that describes the communication structure between component processes, and have been shown to be useful. In this paper, we develop the theory of modular reasoning for behavior hierarchy that describes control structure using hierarchic modes. From Statecharts to UML, behavior hierarchy has been an integral component of many software design languages, but only syntactically. We present the hierarchic reactive modules language that retains powerful features such as nested modes, mode reuse, exceptions, group transitions, history, and conjunctive modes, and yet has a semantic notion of mode hierarchy. We present an observational trace semantics for modes that provides the basis for mode refinement. We show the refinement to be compositional with respect to the mode constructors, and develop an assume-guarantee reasoning principle.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2047258761",
    "type": "article"
  },
  {
    "title": "Java bytecode verification via static single assignment form",
    "doi": "https://doi.org/10.1145/1377492.1377496",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Andreas Gal; Christian W. Probst; Michael Franz",
    "corresponding_authors": "",
    "abstract": "Java Virtual Machines (JVMs) traditionally perform bytecode verification by way of an iterative dataflow analysis. Bytecode verification is necessary to ensure type safety because temporary variables in the JVM are not statically typed. We present an alternative verification mechanism that transforms JVM bytecode into Static Single Assignment Form (SSA) and thereby propagates definitions directly to uses. Type checking at control flow merge points can then be performed in a single pass. Our prototype implementation of the new algorithm is faster than the standard JVM bytecode verifier. It has the additional benefit of generating SSA as a side effect, which may be immediately useful for a subsequent dynamic compilation stage.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1970927204",
    "type": "article"
  },
  {
    "title": "Static Backward Slicing of Non-deterministic Programs and Systems",
    "doi": "https://doi.org/10.1145/2886098",
    "publication_date": "2018-08-25",
    "publication_year": 2018,
    "authors": "Sebastian Danicic; Michael R. Laurence",
    "corresponding_authors": "",
    "abstract": "A theory of slicing non-deterministic programs and systems is developed. Non-deterministic programs and systems are represented as non-deterministic program graphs (NDPGs) that allow arbitrary non-deterministic branching to be expressed. Structural and semantic relationships that must exist between an NDPG and (1) its non-termination insensitive (weak) slices and (2) its non-termination sensitive (strong) slices are defined. Weak and strong commitment closure are introduced. These are the NDPG equivalents of being closed under non-termination sensitive and non-termination insensitive control dependence; properties defined on subsets of vertices of the equivalent deterministic structure: the control flow graph. It is proved that if a subset of the vertices of an NDPG is both data dependence closed and (weak/strong) commitment closed, then the resulting induced graph will, indeed, satisfy our structural and semantic requirements. O ( n 3 ) algorithms for computing minimal data and weak/strong commitment closed sets are given. The resulting induced graphs are thus guaranteed to be weak and strong slices, respectively. It is demonstrated, with examples, that programs written in Dijkstra's non-deterministic guarded command language (DNGCL) can be converted to NDPGs to which our slicing algorithms can then be applied. It is proved that the resulting slices (NDPGs) can always be converted back to valid DNGCL programs, highlighting the applicability of our approach to slicing at the source code level.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2889339478",
    "type": "article"
  },
  {
    "title": "Adaptive Static Analysis via Learning with Bayesian Optimization",
    "doi": "https://doi.org/10.1145/3121135",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Kihong Heo; Hakjoo Oh; Hongseok Yang; Kwangkeun Yi",
    "corresponding_authors": "",
    "abstract": "Building a cost-effective static analyzer for real-world programs is still regarded an art. One key contributor to this grim reputation is the difficulty in balancing the cost and the precision of an analyzer. An ideal analyzer should be adaptive to a given analysis task and avoid using techniques that unnecessarily improve precision and increase analysis cost. However, achieving this ideal is highly nontrivial, and it requires a large amount of engineering efforts. In this article, we present a new learning-based approach for adaptive static analysis. In our approach, the analysis includes a sophisticated parameterized strategy that decides, for each part of a given program, whether to apply a precision-improving technique to that part or not. We present a method for learning a good parameter for such a strategy from an existing codebase via Bayesian optimization. The learnt strategy is then used for new, unseen programs. Using our approach, we developed partially flow- and context-sensitive variants of a realistic C static analyzer. The experimental results demonstrate that using Bayesian optimization is crucial for learning from an existing codebase. Also, they show that among all program queries that require flow- or context-sensitivity, our partially flow- and context-sensitive analysis answers 75% of them, while increasing the analysis cost only by 3.3× of the baseline flow- and context-insensitive analysis, rather than 40× or more of the fully sensitive version.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2901239170",
    "type": "article"
  },
  {
    "title": "Optimal Choice of When to Garbage Collect",
    "doi": "https://doi.org/10.1145/3282438",
    "publication_date": "2019-01-04",
    "publication_year": 2019,
    "authors": "Nicholas Jacek; Meng‐Chieh Chiu; Benjamin M. Marlin; J. Eliot B. Moss",
    "corresponding_authors": "",
    "abstract": "We consider the ultimate limits of program-specific garbage collector (GC) performance for real programs. We first characterize the GC schedule optimization problem. Based on this characterization, we develop a linear-time dynamic programming solution that, given a program run and heap size, computes an optimal schedule of collections for a non-generational collector. Using an analysis of a heap object graph of the program, we compute a property of heap objects that we call their pre-birth time. This information enables us to extend the non-generational GC schedule problem to the generational GC case in a way that also admits a dynamic programming solution with cost quadratic in the length of the trace (number of objects allocated). This improves our previously reported approximately optimal result. We further extend the two-generation dynamic program to any number of generations, allowing other generalizations as well. Our experimental results for two generations on traces from Java programs of the DaCapo benchmark suite show that there is considerable promise to reduce garbage collection costs for some programs by developing program-specific collection policies. For a given space budget, optimal schedules often obtain modest but useful time savings, and for a given time budget, optimal schedules can obtain considerable space savings.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2910047701",
    "type": "article"
  },
  {
    "title": "CSS Minification via Constraint Solving",
    "doi": "https://doi.org/10.1145/3310337",
    "publication_date": "2019-06-19",
    "publication_year": 2019,
    "authors": "Matthew Hague; Anthony W. Lin; Chih-Duo Hong",
    "corresponding_authors": "",
    "abstract": "Minification is a widely accepted technique that aims at reducing the size of the code transmitted over the web. This article concerns the problem of semantic-preserving minification of Cascading Style Sheets (CSS)—the de facto language for styling web documents—based on merging similar rules. The cascading nature of CSS makes the semantics of CSS files sensitive to the ordering of rules in the file. To automatically identify rule-merging opportunities that best minimise file size, we reduce the rule-merging problem to a problem concerning “CSS-graphs,” i.e., node-weighted bipartite graphs with a dependency ordering on the edges, where weights capture the number of characters. Constraint solving plays a key role in our approach. Transforming a CSS file into a CSS-graph problem requires us to extract the dependency ordering on the edges (an NP-hard problem), which requires us to solve the selector intersection problem. To this end, we provide the first full formalisation of CSS3 selectors (the most stable version of CSS) and reduce their selector intersection problem to satisfiability of quantifier-free integer linear arithmetic, for which highly optimised SMT-solvers are available. To solve the above NP-hard graph optimisation problem, we show how Max-SAT solvers can be effectively employed. We have implemented our rule-merging algorithm and tested it against approximately 70 real-world examples (including examples from each of the top 20 most popular websites). We also used our benchmarks to compare our tool against six well-known minifiers (which implement other optimisations). Our experiments suggest that our tool produced larger savings. A substantially better minification rate was shown when our tool is used together with these minifiers.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2950200854",
    "type": "article"
  },
  {
    "title": "Higher-order Demand-driven Program Analysis",
    "doi": "https://doi.org/10.1145/3310340",
    "publication_date": "2019-07-02",
    "publication_year": 2019,
    "authors": "Leandro Facchinetti; Zachary Palmer; Scott F. Smith",
    "corresponding_authors": "",
    "abstract": "Developing accurate and efficient program analyses for languages with higher-order functions is known to be difficult. Here we define a new higher-order program analysis, Demand-Driven Program Analysis (DDPA), which extends well-known demand-driven lookup techniques found in first-order program analyses to higher-order programs. This task presents several unique challenges to obtain good accuracy, including the need for a new method for demand-driven lookup of non-local variable values. DDPA is flow- and context-sensitive and provably polynomial-time. To efficiently implement DDPA, we develop a novel pushdown automaton metaprogramming framework, the Pushdown Reachability automaton. The analysis is formalized and proved sound, and an implementation is described.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2954535803",
    "type": "article"
  },
  {
    "title": "<b>CSim</b> <sup> <i>2</i> </sup>",
    "doi": "https://doi.org/10.1145/3436808",
    "publication_date": "2021-02-09",
    "publication_year": 2021,
    "authors": "David Sanán; Yongwang Zhao; Shang‐Wei Lin; Yang Liu",
    "corresponding_authors": "",
    "abstract": "To make feasible and scalable the verification of large and complex concurrent systems, it is necessary the use of compositional techniques even at the highest abstraction layers. When focusing on the lowest software abstraction layers, such as the implementation or the machine code, the high level of detail of those layers makes the direct verification of properties very difficult and expensive. It is therefore essential to use techniques allowing to simplify the verification on these layers. One technique to tackle this challenge is top-down verification where by means of simulation properties verified on top layers (representing abstract specifications of a system) are propagated down to the lowest layers (that are an implementation of the top layers). There is no need to say that simulation of concurrent systems implies a greater level of complexity, and having compositional techniques to check simulation between layers is also desirable when seeking for both feasibility and scalability of the refinement verification. In this article, we present CSim 2 a (compositional) rely-guarantee-based framework for the top-down verification of complex concurrent systems in the Isabelle/HOL theorem prover. CSim 2 uses CSimpl, a language with a high degree of expressiveness designed for the specification of concurrent programs. Thanks to its expressibility, CSimpl is able to model many of the features found in real world programming languages like exceptions, assertions, and procedures. CSim 2 provides a framework for the verification of rely-guarantee properties to compositionally reason on CSimpl specifications. Focusing on top-down verification, CSim 2 provides a simulation-based framework for the preservation of CSimpl rely-guarantee properties from specifications to implementations. By using the simulation framework, properties proven on the top layers (abstract specifications) are compositionally propagated down to the lowest layers (source or machine code) in each concurrent component of the system. Finally, we show the usability of CSim 2 by running a case study over two CSimpl specifications of an Arinc-653 communication service. In this case study, we prove a complex property on a specification, and we use CSim 2 to preserve the property on lower abstraction layers.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3127084637",
    "type": "article"
  },
  {
    "title": "A Fresh Look at Zones and Octagons",
    "doi": "https://doi.org/10.1145/3457885",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Graeme Gange; Zequn Ma; Jorge A. Navas; Peter Schachte; Harald Søndergaard; Peter J. Stuckey",
    "corresponding_authors": "",
    "abstract": "Zones and Octagons are popular abstract domains for static program analysis. They enable the automated discovery of simple numerical relations that hold between pairs of program variables. Both domains are well understood mathematically but the detailed implementation of static analyses based on these domains poses many interesting algorithmic challenges. In this article, we study the two abstract domains, their implementation and use. Utilizing improved data structures and algorithms for the manipulation of graphs that represent difference-bound constraints, we present fast implementations of both abstract domains, built around a common infrastructure. We compare the performance of these implementations against alternative approaches offering the same precision. We quantify the differences in performance by measuring their speed and precision on standard benchmarks. We also assess, in the context of software verification, the extent to which the improved precision translates to better verification outcomes. Experiments demonstrate that our new implementations improve the state of the art for both Zones and Octagons significantly.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3196332266",
    "type": "article"
  },
  {
    "title": "Securing Interruptible Enclaved Execution on Small Microprocessors",
    "doi": "https://doi.org/10.1145/3470534",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Matteo Busi; Job Noorman; Jo Van Bulck; Letterio Galletta; Pierpaolo Degano; Jan Tobias Mühlberg; Frank Piessens",
    "corresponding_authors": "",
    "abstract": "Computer systems often provide hardware support for isolation mechanisms such as privilege levels, virtual memory, or enclaved execution. Over the past years, several successful software-based side-channel attacks have been developed that break, or at least significantly weaken, the isolation that these mechanisms offer. Extending a processor with new architectural or micro-architectural features brings a risk of introducing new software-based side-channel attacks. This article studies the problem of extending a processor with new features without weakening the security of the isolation mechanisms that the processor offers. Our solution is heavily based on techniques from research on programming languages. More specifically, we propose to use the programming language concept of full abstraction as a general formal criterion for the security of a processor extension. We instantiate the proposed criterion to the concrete case of extending a microprocessor that supports enclaved execution with secure interruptibility. This is a very relevant instantiation, as several recent papers have shown that interruptibility of enclaves leads to a variety of software-based side-channel attacks. We propose a design for interruptible enclaves and prove that it satisfies our security criterion. We also implement the design on an open-source enclave-enabled microprocessor and evaluate the cost of our design in terms of performance and hardware size.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3197514418",
    "type": "article"
  },
  {
    "title": "Programs from Proofs",
    "doi": "https://doi.org/10.1145/3014427",
    "publication_date": "2017-03-10",
    "publication_year": 2017,
    "authors": "Marie-Christine Jakobs; Heike Wehrheim",
    "corresponding_authors": "",
    "abstract": "Today, software is traded worldwide on global markets, with apps being downloaded to smartphones within minutes or seconds. This poses, more than ever, the challenge of ensuring safety of software in the face of (1) unknown or untrusted software providers together with (2) resource-limited software consumers. The concept of Proof-Carrying Code (PCC), years ago suggested by Necula, provides one framework for securing the execution of untrusted code. PCC techniques attach safety proofs, constructed by software producers, to code. Based on the assumption that checking proofs is usually much simpler than constructing proofs, software consumers should thus be able to quickly check the safety of software. However, PCC techniques often suffer from the size of certificates (i.e., the attached proofs), making PCC techniques inefficient in practice. In this article, we introduce a new framework for the safe execution of untrusted code called Programs from Proofs (PfP). The basic assumption underlying the PfP technique is the fact that the structure of programs significantly influences the complexity of checking a specific safety property. Instead of attaching proofs to program code, the PfP technique transforms the program into an efficiently checkable form, thus guaranteeing quick safety checks for software consumers. For this transformation, the technique also uses a producer-side automatic proof of safety. More specifically, safety proving for the software producer proceeds via the construction of an abstract reachability graph (ARG) unfolding the control-flow automaton (CFA) up to the degree necessary for simple checking. To this end, we combine different sorts of software analysis: expensive analyses incrementally determining the degree of unfolding, and cheap analyses responsible for safety checking. Out of the abstract reachability graph we generate the new program. In its CFA structure, it is isomorphic to the graph and hence another, this time consumer-side, cheap analysis can quickly determine its safety. Like PCC, Programs from Proofs is a general framework instantiable with different sorts of (expensive and cheap) analysis. Here, we present the general framework and exemplify it by some concrete examples. We have implemented different instantiations on top of the configurable program analysis tool CPA checker and report on experiments, in particular on comparisons with PCC techniques.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2596935483",
    "type": "article"
  },
  {
    "title": "Fast strictness analysis based on demand propagation",
    "doi": "https://doi.org/10.1145/218570.218573",
    "publication_date": "1995-11-30",
    "publication_year": 1995,
    "authors": "R. Sekar; I. V. Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Fast strictness analysis based on demand propagation Authors: R. Sekar Bellcore BellcoreView Profile , I. V. Ramakrishnan SUNY at Stony Brook SUNY at Stony BrookView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 17Issue 6pp 896–937https://doi.org/10.1145/218570.218573Published:30 November 1995Publication History 8citation292DownloadsMetricsTotal Citations8Total Downloads292Last 12 Months13Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1971417898",
    "type": "article"
  },
  {
    "title": "Using dataflow analysis techniques to reduce ownership overhead in cache coherence protocols",
    "doi": "https://doi.org/10.1145/236114.236116",
    "publication_date": "1996-11-01",
    "publication_year": 1996,
    "authors": "Jonas Skeppstedt; Per Stenström",
    "corresponding_authors": "",
    "abstract": "In this article, we explore the potential of classical dataflow analysis techniques in removing overhead in write-invalidate cache coherence protocols for shared-memory multiprocessors. We construct the compiler algorithms with varying degree of sophistication that detect loads followed by stores to the same address. Such loads are marked and constitute a hint to the cache to obtain an exclusive copy of the block so that the subsequent store does not introduce access penalties. The simplest of the three compiler algorithms analyzes the existence of load-store sequences within each basic blocks of code whereas the other two analyze load-store sequences across basic blocks at the intraprocedural level. The algorithms have been incorporated into an optimizing C compiler, and we have evaluated their efficiencies by compiling and executing seven parallel programs on a simulated multiprocessor. Our results show that the detection efficiency of the most aggressive algorithm is 96% or higher for four of the seven programs studied. We also compare the efficiency of these static algorithms with that of dynamic hardware-based algorithms that reduce ownership overhead. We find that the static analysis using classical dataflow analysis results in similar performance improvements as dynamic hardware-based approaches.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1979576862",
    "type": "article"
  },
  {
    "title": "Compile-time memory reuse in logic programming languages through update in place",
    "doi": "https://doi.org/10.1145/319301.319309",
    "publication_date": "1999-05-01",
    "publication_year": 1999,
    "authors": "Gudjón Gudjónsson; William H. Winsborough",
    "corresponding_authors": "",
    "abstract": "Standard implementation techniques for single-assignment languages modify a data structure without destroying the original, which may subsequently be accessed. Instead a variant structure is created by using newly allocated cells to represent the changed portion and to replace any cell that references a newly allocated cell. The rest of the original structure is shared by the variant. The effort required to leave the original uncorrupted is unnecessary when the program will never reference the original again. This effort includes allocating and initializing new cells, as well as garbage collecting replaced cells. This article specifies a transformation system that introduces update-in-place operations, making Prolog programs update recursive data structures much as an imperative program would. The article introduces the notion of a reuse map, which formalizes reallocation decisions. Because optimal memory reuse is intractable, a heuristical method is presented that performs well in practice. Small Prolog programs that manipulate recursive data structures have their speed increased up to about five times (naive recurse), not counting any speedup obtained by avoiding garbage collection. Quicksort is about three times as fast, merge-sort about one and a half, matrix transposition about twice, and Gaussian elimination is about 1.2 times as fast.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1980962065",
    "type": "article"
  },
  {
    "title": "Specificational functions",
    "doi": "https://doi.org/10.1145/319301.319350",
    "publication_date": "1999-05-01",
    "publication_year": 1999,
    "authors": "Joseph M. Morris; Alexander Bunkenburg",
    "corresponding_authors": "",
    "abstract": "Mathematics supplies us with various operators for creating functions from relations, sets, known functions, and so on. Function inversion is a simple example. These operations are useful in specifying programs. However, many of them have strong constraints on their arguments to ensure that the result is indeed a function. For example, only functions that are bijective may be inverted. This is a serious impediment to their use in specifications, because at best it limits the specifier's expressive power, and at worst it imposes strong proof obligations on the programmer. We propose to loosen the definition of functions so that the constraints on operations such as inversion can be greatly relaxed. The specificational functions that emerge generalize traditional functions in that their application to some arguments may yield no good outcome, while for other arguments their application may yield any of several outcomes unpredictably. While these functions are not in general algorithmic, they can serve as specifications of traditional functions as embodied in programming languages. The idea of specificational functions is not new, but accommodating them in all their generality without falling foul of a myriad of anomalies has proved elusive. We investigate the technical problems that have hindered their use, and propose solutions. In particular, we develop a formal axiomatization for reasoning about specificational functions, and we prove its consistency by constructing a model.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2294188518",
    "type": "article"
  },
  {
    "title": "Efficient algorithms for automatic construction and compactification of parsing",
    "doi": "https://doi.org/10.1145/29873.29876",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Daniel J. Rosenkrantz; Harry B. Hunt",
    "corresponding_authors": "",
    "abstract": "Several computational problems about grammars are studied. Efficient algorithms are presented for the problems of (1) determining, for a given semantic grammar, if there exists a related parsing grammar in some specified grammar class, and (2) finding such a related parsing grammar when one exists. The two grammars are to be related by mergers of nonterminals and/or terminals. Efficient algorithms are presented for most of the grammar classes used in compilers. We also study the problem of (3) determining which terminals of a grammar are good candidates for merger into common lexical tokens of the corresponding parsing grammar.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2001658918",
    "type": "article"
  },
  {
    "title": "ECCS and LIPS: two languages for OSI systems specification and verification",
    "doi": "https://doi.org/10.1145/63264.63402",
    "publication_date": "1989-04-01",
    "publication_year": 1989,
    "authors": "Vincenza Carchiolo; A. Di Stefano; Alberto Faro; Giuseppe Pappalardo",
    "corresponding_authors": "",
    "abstract": "An issue of current interest in the Open Systems Interconnection (OSI) field is the choice of a language well suited to specification and verification. For this purpose, two languages based on Milner's communication calculi are proposed, respectively intended for the specification of asynchronous and synchronous OSI systems. A formal verification method, relying upon the algebraic foundations of the two languages, is introduced and illustrated by means of examples based on nontrivial protocols and services.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2041000557",
    "type": "article"
  },
  {
    "title": "On Kilbury's modification of Earley's algorithm",
    "doi": "https://doi.org/10.1145/88616.88637",
    "publication_date": "1990-10-01",
    "publication_year": 1990,
    "authors": "Hans Leiß",
    "corresponding_authors": "Hans Leiß",
    "abstract": "We improve on J. Kilbury's proposal to interchange “predictor” and “scanner” in Earley's parser. This modification of Earley's parser can trivially be combined with those suggested by S. Graham, M. Harrison, and W. Ruzzo, leading to smaller parse tables and almost the power of lookahead 1. Along these lines we can also obtain Earley-parsers having partial lookahead r ≥ 1, without storing right contexts. Parse trees with shared structure can be stored in the parse tables directly, rather than constructing the trees from “dotted rules.\"",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2066059524",
    "type": "article"
  },
  {
    "title": "A Statement-Oriented Approach to Data Abstraction",
    "doi": "https://doi.org/10.1145/357121.357122",
    "publication_date": "1981-01-01",
    "publication_year": 1981,
    "authors": "Jørgen Steensgaard‐Madsen",
    "corresponding_authors": "Jørgen Steensgaard‐Madsen",
    "abstract": "article Free Access Share on A Statement-Oriented Approach to Data Abstraction Author: J. Steensgaard-Madsen DIKU, Sigurdsgade 41, DK-2200 Copenhagen N, Denmark DIKU, Sigurdsgade 41, DK-2200 Copenhagen N, DenmarkView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 1Jan. 1981 pp 1–10https://doi.org/10.1145/357121.357122Published:01 January 1981Publication History 8citation211DownloadsMetricsTotal Citations8Total Downloads211Last 12 Months5Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1972601492",
    "type": "article"
  },
  {
    "title": "Polymorphic specialization for ML",
    "doi": "https://doi.org/10.1145/1011508.1011510",
    "publication_date": "2004-07-01",
    "publication_year": 2004,
    "authors": "Simon Helsen; Peter Thiemann",
    "corresponding_authors": "",
    "abstract": "We present a framework for offline partial evaluation for call-by-value functional programming languages with an ML-style typing discipline. This includes a binding-time analysis which is (1) polymorphic with respect to binding times; (2) allows the use of polymorphic recursion with respect to binding times; (3) is applicable to a polymorphically typed term; and (4) is proven correct with respect to a novel small-step specialization semantics.The main innovation is to build the analysis on top of the region calculus of Tofte and Talpin [1994], thus leveraging the tools and techniques developed for it. Our approach factorizes the binding-time analysis into region inference and a subsequent constraint analysis. The key insight underlying our framework is to consider binding times as properties of regions.Specialization is specified as a small-step semantics, building on previous work on syntactic-type soundness results for the region calculus. Using similar syntactic proof techniques, we prove soundness of the binding-time analysis with respect to the specializer. In addition, we prove that specialization preserves the call-by-value semantics of the region calculus by showing that the reductions of the specializer are contextual equivalences in the region calculus.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2023809115",
    "type": "article"
  },
  {
    "title": "EDO",
    "doi": "https://doi.org/10.1145/1111596.1111598",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Takeshi Ogasawara; Hideaki Komatsu; Toshio Nakatani",
    "corresponding_authors": "",
    "abstract": "Optimizing exception handling is critical for programs that frequently throw exceptions. We observed that there are many such exception-intensive programs written in Java. There are two commonly used exception handling techniques, stack unwinding and stack cutting. Stack unwinding optimizes the normal path by leaving the exception handling path unoptimized, while stack cutting optimizes the exception handling path by adding extra work to the normal path. However, there has been no single exception handling technique to optimize the exception handling path without incurring any overhead to the normal path.We propose a new technique called Exception-Directed Optimization (EDO) that optimizes exception-intensive programs without slowing down exception-minimal programs. It is a feedback-directed dynamic optimization consisting of three steps: exception path profiling, exception path inlining, and throw elimination. Exception path profiling attempts to detect hot exception paths. Exception path inlining embeds every hot exception path into the corresponding catching method. Throw elimination replaces a throw with a branch to the corresponding handler. We implemented EDO in IBM's production Just-in-Time compiler and made several experiments. In summary, it improved the performance of exception-intensive programs by up to 18.3% without decreasing the performance of exception-minimal programs for SPECjvm98. We also found an opportunity for performance improvement using EDO in the startup of a Java application server.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1990111829",
    "type": "article"
  },
  {
    "title": "Fast interprocedural linear two-variable equalities",
    "doi": "https://doi.org/10.1145/2049706.2049710",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Andrea Flexeder; Markus Müller-Olm; Michael Petter; Helmut Seidl",
    "corresponding_authors": "",
    "abstract": "In this article we provide an interprocedural analysis of linear two-variable equalities. The novel algorithm has a worst-case complexity of 𝒪( n ⋅ k 4 ), where k is the number of variables and n is the program size. Thus, it saves a factor of k 4 in comparison to a related algorithm based on full linear algebra. We also indicate how the practical runtime can be further reduced significantly. The analysis can be applied, for example, for register coalescing, for identifying local variables and thus for interprocedurally observing stack pointer modifications as well as for an analysis of array index expressions, when analyzing low-level code.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2035168696",
    "type": "article"
  },
  {
    "title": "Towards Porting Operating Systems with Program Synthesis",
    "doi": "https://doi.org/10.1145/3563943",
    "publication_date": "2022-09-20",
    "publication_year": 2022,
    "authors": "Jingmei Hu; Eric Lu; David A. Holland; Ming Kawaguchi; Stephen Chong; Margo Seltzer",
    "corresponding_authors": "",
    "abstract": "The end of Moore's Law has ushered in a diversity of hardware not seen in decades. Operating system (and system software) portability is accordingly becoming increasingly critical. Simultaneously, there has been tremendous progress in program synthesis. We set out to explore the feasibility of using modern program synthesis to generate the machine-dependent parts of an operating system. Our ultimate goal is to generate new ports automatically from descriptions of new machines. One of the issues involved is writing specifications, both for machine-dependent operating system functionality and for instruction set architectures. We designed two domain-specific languages: Alewife for machine-independent specifications of machine-dependent operating system functionality and Cassiopea for describing instruction set architecture semantics. Automated porting also requires an implementation. We developed a toolchain that, given an Alewife specification and a Cassiopea machine description, specializes the machine-independent specification to the target instruction set architecture and synthesizes an implementation in assembly language with a customized symbolic execution engine. Using this approach, we demonstrate successful synthesis of a total of 140 OS components from two pre-existing OSes for four real hardware platforms. We also developed several optimization methods for OS-related assembly synthesis to improve scalability. The effectiveness of our languages and ability to synthesize code for all 140 specifications is evidence of the feasibility of program synthesis for machine-dependent OS code. However, many research challenges remain; we also discuss the benefits and limitations of our synthesis-based approach to automated OS porting.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4223911053",
    "type": "article"
  },
  {
    "title": "A Type Discipline for Message Passing Parallel Programs",
    "doi": "https://doi.org/10.1145/3552519",
    "publication_date": "2022-08-10",
    "publication_year": 2022,
    "authors": "Vasco T. Vasconcelos; Francisco Martins; Hugo A. López; Nobuko Yoshida",
    "corresponding_authors": "",
    "abstract": "We present ParTypes, a type discipline for parallel programs. The model we have in mind comprises a fixed number of processes running in parallel and communicating via collective operations or point-to-point synchronous message exchanges. A type describes a protocol to be followed by each processes in a given program. We present the type theory, a core imperative programming language and its operational semantics, and prove that type checking is decidable (up to decidability of semantic entailment) and that well-typed programs do not deadlock and always terminate. The article is accompanied by a large number of examples drawn from the literature on parallel programming.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4297830949",
    "type": "article"
  },
  {
    "title": "Proving Correctness of Parallel Implementations of Transition System Models",
    "doi": "https://doi.org/10.1145/3660630",
    "publication_date": "2024-04-20",
    "publication_year": 2024,
    "authors": "Frank de Boer; Einar Broch Johnsen; Violet Ka I Pun; Silvia Lizeth Tapia Tarifa",
    "corresponding_authors": "",
    "abstract": "This article addresses the long-standing problem of program correctness for programs that describe systems of parallel executing processes. We propose a new method for proving correctness of parallel implementations of high-level models expressed as transition systems. The implementation language underlying the method is based on the concurrency model of actors and active objects. The method defines program correctness in terms of a simulation relation between the transition system that specifies the program semantics of the parallel program and the transition system that is described by the correctness specification. The simulation relation itself abstracts from the fine-grained interleaving of parallel processes by exploiting a global confluence property of the concurrency model of the implementation language considered in this article. As a proof of concept, we apply our method to the correctness of a parallel simulator of multicore memory systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4394979942",
    "type": "article"
  },
  {
    "title": "Backtracking without trailing in CLP (ℜ <sub>Lin</sub> )",
    "doi": "https://doi.org/10.1145/210184.210192",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "Pascal Van Hentenryck; Viswanath Ramachandran",
    "corresponding_authors": "",
    "abstract": "Existing CLP languages support backtracking by generalizing traditional Prolog implementations: modifications to the constraint system are trailed and restored on backtracking. Although simple and efficient, trailing may be very demanding in memory space, since the constraint system may potentially be saved at each choice point. This article proposes a new implementation scheme for backtracking in CLP languages over linear (rational or real) arithmetic. The new scheme, called semantic backtracking , does not use trailing but rather exploits the semantics of the constraints to undo the effect of newly added constraints. Semantic backtracking reduces the space complexity compared to implementations based on trailing by making it essentially independent of the number of choice points. In addition, semantic backtracking introduces negligible space and time overhead on deterministic programs. The price for this improvement is an increase in backtracking time, although constraint-solving time may actually decrease. The scheme has been implemented as part of a complete CLP system CLP (ℜ Lin ) and compared analytically and experimentally with optimized trailing implementations. Experimental results on small and real-life problems indicate that semantic backtracking produces significant reduction in memory space, while keeping the time overhead reasonably small.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1986909452",
    "type": "article"
  },
  {
    "title": "Axiomatic bootstrapping",
    "doi": "https://doi.org/10.1145/197320.197336",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Andrew W. Appel",
    "corresponding_authors": "Andrew W. Appel",
    "abstract": "If a compiler for language L is implemented in L , then it should be able to compile itself. But for systems used interactively commands are compiled and immediately executed, and these commands may invoke the compiler; so there is the question of how ever to cross-compile for another architecture. Also, where the compiler writes binary files of static type information that must then be read in by the bootstrapped interactive compiler, how can one ever change the format of digested type information in binary files? Here I attempt an axiomatic clarification of the bootstrapping technique, using Standard ML of New Jersey as a case study. This should be useful to implementors of any self-applicable interactive compiler with nontrivial object-file and runtime-system compatibility problems.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2012210398",
    "type": "article"
  },
  {
    "title": "Type matching, type-graphs, and the Schanuel conjecture",
    "doi": "https://doi.org/10.1145/133233.133247",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "J. Katzenelson; Shlomit S. Pinter; E. Schenfeld",
    "corresponding_authors": "",
    "abstract": "This work considers type systems that are defined by type-graphs (tgraphs), which are rooted directed graphs with order among the edges leaving each node. Tgraphs are uniquely mapped into polynomials which, in turn, are each evaluated at a special point to yield an irrational number named the tgraph's magic number . This special point is chosen using the Schanuel conjecture. It is shown that each tgraph can be uniquely represented by this magic number; namely, types are equal if and only if the corresponding magic numbers are equal. Since irrational numbers require infinite precision, the algorithm for generating magic numbers is carried out using a double-precision floating-point approximation. This approximation is viewed as a hashing scheme, mapping the infinite domain of the irrational numbers into finite computer words. The proposed hashing scheme was investigated experimentally, with the conclusion that it is a good and practical hashing method. In tests involving over a million randomly chosen tgraphs, we have not encountered a single collision. We conclude that this method for representation and management of types is practical, and offers novel possibilities for enforcing strict type matching at link time among separately compiled modules.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2012837083",
    "type": "article"
  },
  {
    "title": "Space/time-efficient scheduling and execution of parallel irregular computations",
    "doi": "https://doi.org/10.1145/295656.295660",
    "publication_date": "1998-11-01",
    "publication_year": 1998,
    "authors": "Tao Yang; Cong Fu",
    "corresponding_authors": "",
    "abstract": "In this article we investigate the trade-off between time and space efficiency in scheduling and executing parallel irregular computations on distributed-memory machines. We employ acyclic task dependence graphs to model irregular parallelism with mixed granularity, and we use direct remote memory access to support fast communication. We propose new scheduling techniques and a run-time active memory management scheme to improve memory utilization while retaining good time efficiency, and we provide a theoretical analysis on correctness and performance. This work is implemented in the context of the RAPID system which uses an inspector/executor approach to parallelize irregular computations at run-ti me. We demostrate the effectiveness of the proposed techniques on several irregular applications such as sparse matrix code and the fast multipole method for particle simulation. Our experimental results on Cray-T3E show that problems large sizes can be solved under limited space capacity, and that the loss of execution efficiency caused by the extra memory management overhead is reasonable.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2056456096",
    "type": "article"
  },
  {
    "title": "Specification and verification of liveness properties of cyclic, concurrent processes",
    "doi": "https://doi.org/10.1145/42192.42195",
    "publication_date": "1988-01-01",
    "publication_year": 1988,
    "authors": "Joylyn Reed; Raymond T. Yeh",
    "corresponding_authors": "",
    "abstract": "A technique is described for software specification and verification of concurrent, distributed systems. The complete specification of a program is given in terms of a hierarchical structure of module specifications. Module external specifications are abstract; module internal specifications are descriptions of internal implementations, either in terms of submodules or actual code. The verification that an implementation satisfies its specification is language independent for the former and language dependent for the latter. Distinguishing the liveness properites provided by a module and the liveness properties required by a module (from its comodules) allows the specification and verification of a given module to be independent from the specification and verification of its comodules.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1999763016",
    "type": "article"
  },
  {
    "title": "The undecidability of associativity and commutativity analysis",
    "doi": "https://doi.org/10.1145/570886.570889",
    "publication_date": "2002-09-01",
    "publication_year": 2002,
    "authors": "Arthur Charlesworth",
    "corresponding_authors": "Arthur Charlesworth",
    "abstract": "Associativity is required for the use of general scans and reductions in parallel languages. Some systems also require functions used with scans and reductions to be commutative. We prove the undecidability of both associativity and commutativity. Thus, it is impossible in general for a compiler to check for those conditions. We also prove the stronger result that the resulting relations fail to be recursively enumerable. We prove that these results hold for the kind of function subprograms of practical interest in such a situation: function subprograms that, due to syntactical restrictions, are guaranteed to halt. Thus, our results are stronger than one can obtain from Rice's Theorem. We also obtain limitations concerning the construction of functions and limitations concerning compiler-generated run-time checks. In addition, we prove an undecidability result about programmer-constructed run-time checks.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2029279761",
    "type": "article"
  },
  {
    "title": "Morris's Garbage Compaction Algorithm Restores Reference Counts",
    "doi": "https://doi.org/10.1145/357062.357070",
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "David S. Wise",
    "corresponding_authors": "David S. Wise",
    "abstract": "The two-pass compaction algorithm of F.L. Morris, which follows upon the mark phase in a garbage collector, may be modified to recover reference counts for a hybrid storage management system. By counting the executions of two loops in that algorithm where upward and downward references, respectively, are forwarded to the relocation address of one node, we can initialize a count of active references and then update it but once. The reference count may share space with the mark bit in each node, but it may not share the additional space required in each pointer by Morris's algorithm, space which remains unused outside the garbage collector.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2005589860",
    "type": "article"
  },
  {
    "title": "An Exercise in Program Explanation",
    "doi": "https://doi.org/10.1145/357121.357128",
    "publication_date": "1981-01-01",
    "publication_year": 1981,
    "authors": "Jayadev Misra",
    "corresponding_authors": "Jayadev Misra",
    "abstract": "article An Exercise in Program Explanation Share on Author: Jayadev Misra Department of Computer Sciences, College of Natural Sciences, The University of Texas at Austin, Austin, TX Department of Computer Sciences, College of Natural Sciences, The University of Texas at Austin, Austin, TXView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 1Jan. 1981 pp 104–109https://doi.org/10.1145/357121.357128Published:01 January 1981 4citation294DownloadsMetricsTotal Citations4Total Downloads294Last 12 Months6Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2088811798",
    "type": "article"
  },
  {
    "title": "Editing Data Structures",
    "doi": "https://doi.org/10.1145/357133.357134",
    "publication_date": "1981-04-01",
    "publication_year": 1981,
    "authors": "Christopher W. Fraser; Andy Lopez",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Editing Data Structures Authors: Christopher W. Fraser Department of Computer Science, The University of Arizona, Tucson, AZ Department of Computer Science, The University of Arizona, Tucson, AZView Profile , A. A. Lopez Computer Services Center, University of Minnesota, Morris, MN Computer Services Center, University of Minnesota, Morris, MNView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 2pp 115–125https://doi.org/10.1145/357133.357134Published:01 April 1981Publication History 7citation621DownloadsMetricsTotal Citations7Total Downloads621Last 12 Months66Last 6 weeks9 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2133229286",
    "type": "article"
  },
  {
    "title": "A fast, memory-efficient register allocation framework for embedded systems",
    "doi": "https://doi.org/10.1145/1034774.1034776",
    "publication_date": "2004-11-01",
    "publication_year": 2004,
    "authors": "Sathyanarayanan Thammanur; Santosh Pande",
    "corresponding_authors": "",
    "abstract": "In this work, we describe a \"just-in-time,\" &lt;i&gt;usage density-based register allocator&lt;/i&gt; geared toward embedded systems with a limited general-purpose register set wherein speed, code size, and memory requirements are of equal concern. The main attraction of the allocator is that it does not make use of the traditional live range and interval analysis nor does it perform advanced optimizations based on range &lt;i&gt;splitting&lt;/i&gt; but results in very good code quality. We circumvent the need for traditional analysis by using a measure of &lt;i&gt;usage density&lt;/i&gt; of a variable. The usage density of a variable at a program point represents both the frequency and the density of the uses. We contend that by using this measure we can capture both &lt;i&gt;range&lt;/i&gt; and &lt;i&gt;frequency&lt;/i&gt; information which is essentially used by the good allocators based on &lt;i&gt;splitting&lt;/i&gt;. We describe a framework based on this measure which has a linear complexity in terms of the program size. We perform comparisons with the static allocators based on graph coloring and the ones targeted toward just-in-time compilation systems like linear scan of live ranges. Through comparisons with graph coloring (Brigg's style) and live range-based (linear scan) allocators, we show that the memory footprint and the size of our allocator are smaller by 20% to 30%. The speed of allocation is comparable and the speed of the generated code is better and its size smaller. These attributes make the allocator an attractive candidate for performing a fast, memory-efficient register allocation for embedded devices with a small number of registers.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2080468511",
    "type": "article"
  },
  {
    "title": "On minimizing materializations of array-valued temporaries",
    "doi": "https://doi.org/10.1145/1186632.1186637",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Daniel J. Rosenkrantz; Lenore Mullin; Harry B. Hunt",
    "corresponding_authors": "",
    "abstract": "We consider the analysis and optimization of code utilizing operations and functions operating on entire arrays. Models are developed for studying the minimization of the number of materializations of array-valued temporaries in basic blocks, each consisting of a sequence of assignment statements involving array-valued variables. We derive lower bounds on the number of materializations required, and develop several algorithms minimizing the number of materializations, subject to a simple constraint on allowable statement rearrangement. In contrast, we also show that when statement rearrangement is unconstrained, minimizing the number of materializations becomes NP-complete, even for very simple basic blocks.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1993283251",
    "type": "article"
  },
  {
    "title": "An accurate cost model for guiding data locality transformations",
    "doi": "https://doi.org/10.1145/1086642.1086646",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Xavier Vera; Jaume Abella; Josep Llosa; Antonio González",
    "corresponding_authors": "",
    "abstract": "Caches have become increasingly important with the widening gap between main memory and processor speeds. Small and fast cache memories are designed to bridge this discrepancy. However, they are only effective when programs exhibit sufficient data locality.The performance of the memory hierarchy can be improved by means of data and loop transformations. Tiling is a loop transformation that aims at reducing capacity misses by shortening the reuse distance. Padding is a data layout transformation targeted to reduce conflict misses.This article presents an accurate cost model that describes misses across different hierarchy levels and considers the effects of other hardware components such as branch predictors. The cost model drives the application of tiling and padding transformations. We combine the cost model with a genetic algorithm to compute the tile and pad factors that enhance the program performance.To validate our strategy, we ran experiments for a set of benchmarks on a large set of modern architectures. Our results show that this scheme is useful to optimize programs' performance. When compared to previous approaches, we observe that with a reasonable compile-time overhead, our approach gives significant performance improvements for all studied kernels on all architectures.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2055182305",
    "type": "article"
  },
  {
    "title": "Remote specialization for efficient embedded operating systems",
    "doi": "https://doi.org/10.1145/1377492.1377497",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Sapan Bhatia; Charles Consel; Calton Pu",
    "corresponding_authors": "",
    "abstract": "Prior to their deployment on an embedded system, operating systems are commonly tailored to reduce code size and improve runtime performance. Program specialization is a promising match for this process: it is predictable and modules, and it allows the reuse of previously implemented specializations. A specialization engine for embedded systems must overcome three main obstacles: (i) Reusing existing compilers for embedded systems, (ii) supporting specialization on a resource-limited system and (iii) coping with dynamic applications by supporting specialization on demand. In this article, we describe a runtime specialization infrastructure that addresses these problems. Our solution proposes: (i) Specialization in two phases of which the former generates specialized C templates and the latter uses a dedicated compiler to generate efficient native code. (ii) A virtualization mechanism that facilitates specialization of code at a remote location. (iii) An API and supporting OS extensions that allow applications to produce, manage and dispose of specialized code. We evaluate our work through two case studies: (i) The TCP/IP implementation of Linux and (ii) The TUX embedded web server. We report appreciable improvements in code size and performance. We also quantify the overhead of specialization and argue that a specialization server can scale to support a sizable workload.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2002226126",
    "type": "article"
  },
  {
    "title": "Efficient dynamic dispatching with type slicing",
    "doi": "https://doi.org/10.1145/1290520.1290525",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Joseph Gil; Yoav Zibin",
    "corresponding_authors": "",
    "abstract": "A fundamental problem in the implementation of object-oriented languages is that of a frugal implementation of dynamic dispatching, that is, a small footprint data structure that supports quick response to runtime dispatching queries of the following format: which method should be executed in response to a certain message sent to a given object. Previous theoretical algorithms for this problem tend to be impractical due to their conceptual complexity and large hidden constants. In contrast, successful practical heuristics lack theoretical support. The contribution of this article is in a novel type slicing technique, which results in two dispatching schemes: TS and CT d . We make the case for these schemes both practically and theoretically. The empirical findings on a corpus of 35 hierarchies totaling some 64 thousand types from eight different languages, demonstrate improvement over previous results in terms of the space required for the representation, and the time required for computing it. The theoretical analysis is with respect to ι, the best possible compression factor of the dispatching matrix. The results are expressed as a function of a parameter κ, which can be thought of as a metric of the complexity of the topology of a multiple inheritance hierarchy. In single inheritance hierarchies κ = 1, but although κ can be in the order of the size of the hierarchy, it is typically a small constant in actual use of inheritance; in our corpus, the median value of κ is 5, while its average is 6.4. The TS scheme generalizes the famous interval containment technique to multiple inheritance. TS achieves a compression factor of ι/κ, that is, our generalization comes with an increase to the space requirement by a small factor of κ. The pay is in the dispatching time, which is no longer constant as in a naive matrix implementation, but logarithmic in the number of different method implementations. In practice, dispatching uses one indirect branch and, on average, only 2.5 binary branches. The CT schemes are a sequence of algorithms CT 1 , CT 2 , CT 3 , …, where CT d uses d memory dereferencing operations during dispatch, and achieves a compression factor of 1/ d ι 1−1/ d in a single inheritance setting. A generalization of these algorithms to a multiple inheritance setting, increases the space by a factor of (2κ) 1−1/ d . This trade-off represents the first bounds on the compression ratio of constant-time dispatching algorithms. We also present an incremental variant of the CT d suited for languages such as Java.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2084103658",
    "type": "article"
  },
  {
    "title": "The Systematic Design of Responsibility Analysis by Abstract Interpretation",
    "doi": "https://doi.org/10.1145/3484938",
    "publication_date": "2021-12-09",
    "publication_year": 2021,
    "authors": "Chaoqiang Deng; Patrick Cousot",
    "corresponding_authors": "",
    "abstract": "Given a behavior of interest, automatically determining the corresponding responsible entity (i.e., the root cause) is a task of critical importance in program static analysis. In this article, a novel definition of responsibility based on the abstraction of trace semantics is proposed, which takes into account the cognizance of observer, which, to the best of our knowledge, is a new innovative idea in program analysis. Compared to current dependency and causality analysis methods, the responsibility analysis is demonstrated to be more precise on various examples. However, the concrete trace semantics used in defining responsibility is uncomputable in general, which makes the corresponding concrete responsibility analysis undecidable. To solve this problem, the article proposes a sound framework of abstract responsibility analysis, which allows a balance between cost and precision. Essentially, the abstract analysis builds a trace partitioning automaton by an iteration of over-approximating forward reachability analysis with trace partitioning and under/over-approximating backward impossible failure accessibility analysis, and determines the bounds of potentially responsible entities along paths in the automaton. Unlike the concrete responsibility analysis that identifies exactly a single action as the responsible entity along every concrete trace, the abstract analysis may lose some precision and find multiple actions potentially responsible along each automaton path. However, the soundness is preserved, and every responsible entity in the concrete is guaranteed to be also found responsible in the abstract.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200158711",
    "type": "article"
  },
  {
    "title": "Typing linear constraints",
    "doi": "https://doi.org/10.1145/1749608.1749610",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Salvatore Ruggieri; Frédéric Mesnard",
    "corresponding_authors": "",
    "abstract": "We present a type system for linear constraints over the reals intended for reasoning about the input-output directionality of variables. Types model the properties of definiteness, range width or approximation, lower and upper bounds of variables in a linear constraint. Several proof procedures are presented for inferring the type of a variable and for checking validity of type assertions. We rely on theory and tools for linear programming problems, linear algebra, parameterized polyhedra and negative constraints. An application of the type system is proposed in the context of the static analysis of constraint logic programs. Type assertions are at the basis of the extension of well-moding from pure logic programming. The proof procedures (both for type assertion validity and for well-moding) are implemented and their computational complexity is discussed. We report experimental results demonstrating the efficiency in practice of the proposed approach.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1995986239",
    "type": "article"
  },
  {
    "title": "ThisType for Object-Oriented Languages",
    "doi": "https://doi.org/10.1145/2888392",
    "publication_date": "2016-04-08",
    "publication_year": 2016,
    "authors": "Sukyoung Ryu",
    "corresponding_authors": "Sukyoung Ryu",
    "abstract": "In object-oriented programs, objects often provide methods whose parameter types or return types are the object types themselves. For example, the parameter types of binary methods are the types of their receiver objects, and the return types of some factory methods are the types of their enclosing objects. However, most object-oriented languages do not support such methods precisely because their type systems do not support explicit recursive types, which lead to a mismatch between subclassing and subtyping. This mismatch means that an expression of a subclass may not always be usable in a context where an expression of a superclass is expected, which is not intuitive in an object-oriented setting. Researchers have proposed various type-sound approaches to support methods with types of their enclosing object types denoted by some variants of ThisType, but they reject reasonable and useful methods due to unpermissive type systems or they use less precise declared inexact types rather than runtime exact types. In this article, we present a thorough approach to support methods with ThisTypes: from a new encoding of objects in a typed lambda calculus that allows subtyping by subclassing to an open-source implementation as an extension of the Java programming language. We first provide real-world examples that motivate the need for ThisTyped methods to precisely describe desired properties of programs. We define a new object encoding that enables subtyping by subclassing even in the presence of negative occurrences of type recursion variables by distinguishing object types from existential object types. Based on this object encoding, we formalize language features to support ThisTyped methods with a core calculus CoreThisJava, and prove its type soundness. Finally, we provide ThisJava, a prototype implementation of the calculus, to show its backward compatibility, and we make it publicly available. We believe that our approach theoretically expands the long pursuit of an object-oriented language with ThisTypes to support more useful methods with more precise types.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2313939972",
    "type": "article"
  },
  {
    "title": "Exploiting Partially Context-sensitive Profiles to Improve Performance of Hot Code",
    "doi": "https://doi.org/10.1145/3612937",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Maja Vukasović; Aleksandar Prokopec",
    "corresponding_authors": "",
    "abstract": "Availability of profiling information is a major advantage of just-in-time (JIT) compilation. Profiles guide the compilation order and optimizations, thus substantially improving program performance. Ahead-of-time (AOT) compilation can also utilize profiles, obtained during separate profiling runs of the programs. Profiles can be context-sensitive, i.e., each profile entry is associated with a call-stack. To ease profile collection and reduce overheads, many systems collect partially context-sensitive profiles, which record only a call-stack suffix. Despite prior related work, partially context-sensitive profiles have the potential to further improve compiler optimizations. In this article, we describe a novel technique that exploits partially context-sensitive profiles to determine which portions of code are hot and compile them with additional compilation budget. This technique is applicable to most AOT compilers that can access partially context-sensitive profiles, and its goal is to improve program performance without significantly increasing code size. The technique relies on a new hot-code-detection algorithm to reconstruct hot regions based on the partial profiles. The compilation ordering and the inlining of the compiler are modified to exploit the information about the hot code. We formally describe the proposed algorithm and its heuristics and then describe our implementation inside GraalVM Native Image, a state-of-the-art AOT compiler for Java. Evaluation of the proposed technique on 16 benchmarks from DaCapo, Scalabench, and Renaissance suites shows a performance improvement between 22% and 40% on 4 benchmarks, and between 2.5% and 10% on 5 benchmarks. Code-size increase ranges from 0.8%–9%, where 10 benchmarks exhibit an increase of less than 2.5%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386710140",
    "type": "article"
  },
  {
    "title": "The definition of dependence distance",
    "doi": "https://doi.org/10.1145/183432.183440",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Michael Wolfe",
    "corresponding_authors": "Michael Wolfe",
    "abstract": "Several definitions of dependence distance can be found in the literature. A single coherent definition is the vector distance between the iteration vectors of two iterations involved in a dependence relation. Different ways to associate iteration vectors with iterations can give different dependence distances to the same program, and have different advantages.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2044247685",
    "type": "article"
  },
  {
    "title": "Lazy and incremental program generation",
    "doi": "https://doi.org/10.1145/177492.177750",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Jan Heering; Paul Klint; J. Rekers",
    "corresponding_authors": "",
    "abstract": "Current program generators usually operate in a greedy manner in the sense that a program must be generated in its entirety before it can be used. If generation time is scarce, or if the input to the generator is subject to modification, it may be better to be more cautious and to generate only those parts of the program that are indispensable for processing the particular data at hand. We call this lazy program generation . Another, closely related strategy is incremental program generation . When its input is modified, an incremental generator will try to make a corresponding modification in its output rather than generate a completely new program. It may be advantageous to use a combination of both strategies in program generators that have to operate in a highly dynamic and/or interactive environment.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2160601420",
    "type": "article"
  },
  {
    "title": "On the optimality of change propagation for incremental evaluation of hierarchical attribute grammars",
    "doi": "https://doi.org/10.1145/225540.225542",
    "publication_date": "1996-01-01",
    "publication_year": 1996,
    "authors": "Alan Carle; Lori Pollock",
    "corresponding_authors": "",
    "abstract": "Several new attribute grammar dialects have recently been developed, all with the common goal of allowing large, complex language translators to be specified through a modular composition of smaller attribute grammars. We refer to the class of dialects as hierarchical attribute grammars . In this short article, we present a characterization of optimal incremental evaluation that indicates the unsuitability of change propagation as the basis of an optimal incremental evaluator for hierarchical attribute grammars. This result lends strong support to the use of incremental evaluators based on more applicative approaches to attribute evaluation, such as Carle and Pollock's evaluator based on more applicative approaches to attribute evaluation, such as Carle and Pollock's evaluator based on caching of partially attributed subtree, Pugh's evaluator based on function caching of semantic functions, and Swierstra and Vogt's evaluator based on functions, and Swierstra and Vogt's evaluator based on function caching of visit sequences.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2067069132",
    "type": "article"
  },
  {
    "title": "Efficient construction of LR( <i>k</i> ) states and tables",
    "doi": "https://doi.org/10.1145/114005.102809",
    "publication_date": "1991-01-01",
    "publication_year": 1991,
    "authors": "M. Anicona; Gabriella Dodero; V. Gianuzzi; M. Morgavi",
    "corresponding_authors": "",
    "abstract": "A new method for building LR( k ) states and parsing tables is presented. The method aims at giving a feasible construction of a collection of LR( k ) parsing tables, especially when k &gt; 1. for nontrivial grammars. To this purpose, the algorithm first attempts to build a set of normal states for the given grammar, each one associated to a single parsing action in { accept, reduce, shift }. When such an action cannot be uniquely determined, that is, when up to k input symbols have to be examined (inadequacy), further states, belonging to a new type, called look-ahead states, are computed. The action associated with inadequate states is a new parsing action, look . States are built without actual computation of the FIRST k and EFF k functions; that is, nonterminals are kept in the context string of items composing each state, and their expansion to terminals is deferred until indispensable to solve inadequacy. The aforementioned method is illustrated; then the canonical collection of states and the canonical tables are compared with those obtained from the proposed method. A sufficient condition is stated, by which the size of parsing tables, obtained by applying this new method, is smaller than that of canonical tables. Experimental results show that such a condition is verified by the grammars of several programming languagues and that significant speed is gained by avoiding the computation of the FIRST k function.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2072083409",
    "type": "article"
  },
  {
    "title": "Addendum to \"The promotion and accumulation strategies in transformational programming\"",
    "doi": "https://doi.org/10.1145/3550149",
    "publication_date": "1985-07-01",
    "publication_year": 1985,
    "authors": "Richard Bird",
    "corresponding_authors": "Richard Bird",
    "abstract": "No abstract available.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1482508253",
    "type": "article"
  },
  {
    "title": "A Note on the Drinking Philosophers Problem.",
    "doi": null,
    "publication_date": "1988-01-01",
    "publication_year": 1988,
    "authors": "Sandra Murphy; Anand Shankar",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W185301169",
    "type": "article"
  },
  {
    "title": "Conversion from data-driven to synchronous execution in loop programs",
    "doi": "https://doi.org/10.1145/29873.31334",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Janice E. Cuny; Lawrence Snyder",
    "corresponding_authors": "",
    "abstract": "Conversion algorithms are presented that would enable programmers to write programs in a high-level, data flow language and then run those programs on a synchronous machine. A model of interprocess communication systems is developed in which both data-driven and synchronous execution modes are represented. Balancing equations are used to characterize a subclass of parallel programs, called loop programs, for which conversions are possible. We show that all loop programs having the finite buffer property can be converted into synchronous mode. Finally two algorithms for the conversion of loop programs are presented and discussed.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2052420412",
    "type": "article"
  },
  {
    "title": "Proving Failure-Free Properties of Concurrent Systems Using Temporal Logic",
    "doi": "https://doi.org/10.1145/2993.357245",
    "publication_date": "1984-04-01",
    "publication_year": 1984,
    "authors": "Richard Alan Karp",
    "corresponding_authors": "Richard Alan Karp",
    "abstract": "article Free Access Share on Proving Failure-Free Properties of Concurrent Systems Using Temporal Logic Author: Richard Alan Karp Tri-Data, 505 East Middlefield Road, P. O. Box 7505, Mountain View, CA Tri-Data, 505 East Middlefield Road, P. O. Box 7505, Mountain View, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 2pp 239–253https://doi.org/10.1145/2993.357245Published:01 April 1984Publication History 7citation286DownloadsMetricsTotal Citations7Total Downloads286Last 12 Months18Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2083002236",
    "type": "article"
  },
  {
    "title": "Handling Operator Precedence in Arithmetic Expressions with Tree Transformations",
    "doi": "https://doi.org/10.1145/357121.357127",
    "publication_date": "1981-01-01",
    "publication_year": 1981,
    "authors": "Wilf R. LaLonde; Jim des Rivières",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Handling Operator Precedence in Arithmetic Expressions with Tree Transformations Authors: Wilf R. LaLonde Department of Systems Engineering and Computing Science, Carleton University, Ottawa, Canada K1S 5B6 Department of Systems Engineering and Computing Science, Carleton University, Ottawa, Canada K1S 5B6View Profile , Jim des Rivieres Computing Services, Carleton University, Ottawa, Canada K1S 5B6 Computing Services, Carleton University, Ottawa, Canada K1S 5B6View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 1Jan. 1981 pp 83–103https://doi.org/10.1145/357121.357127Published:01 January 1981Publication History 4citation722DownloadsMetricsTotal Citations4Total Downloads722Last 12 Months106Last 6 weeks10 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2002781652",
    "type": "article"
  },
  {
    "title": "A Cost Model for the Internal Organization of B <sup>+</sup> -Tree Nodes",
    "doi": "https://doi.org/10.1145/357146.357152",
    "publication_date": "1981-10-01",
    "publication_year": 1981,
    "authors": "Wilfred J. Hansen",
    "corresponding_authors": "Wilfred J. Hansen",
    "abstract": "article Free Access Share on A Cost Model for the Internal Organization of B+-Tree Nodes Author: Wilfred J. Hansen Department of Computer Science, University of Pittsburgh, Pittsburgh, PA Department of Computer Science, University of Pittsburgh, Pittsburgh, PAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 4pp 508–532https://doi.org/10.1145/357146.357152Published:01 October 1981Publication History 6citation711DownloadsMetricsTotal Citations6Total Downloads711Last 12 Months11Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2075371427",
    "type": "article"
  },
  {
    "title": "Register allocation for software pipelined multidimensional loops",
    "doi": "https://doi.org/10.1145/1377492.1377498",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Hongbo Rong; Alban Douillet; Guang R. Gao",
    "corresponding_authors": "",
    "abstract": "This article investigates register allocation for software pipelined multidimensional loops where the execution of successive iterations from an n -dimensional loop is overlapped. For single loop software pipelining, the lifetimes of a loop variable in successive iterations of the loop form a repetitive pattern. An effective register allocation method is to represent the pattern as a vector of lifetimes (or a vector lifetime using Rau's terminology [Rau 1992]) and map it to rotating registers. Unfortunately, the software pipelined schedule of a multidimensional loop is considerably more complex and so are the vector lifetimes in it. In this article, we develop a way to normalize and represent the vector lifetimes, which captures their complexity, while exposing their regularity that enables a simple solution. The problem is formulated as bin-packing of the multidimensional vector lifetimes on the surface of a space-time cylinder. A metric, called distance, is calculated either conservatively or aggressively to guide the bin-packing process, so that there is no overlapping between any two vector lifetimes, and the register requirement is minimized. This approach subsumes the classical register allocation for software pipelined single loops as a special case. The method has been implemented in the ORC compiler and produced code for the IA-64 architecture. Experimental results show the effectiveness. Several strategies for register allocation are compared and analyzed.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2018235128",
    "type": "article"
  },
  {
    "title": "Relations as an abstraction for BDD-based program analysis",
    "doi": "https://doi.org/10.1145/1377492.1377494",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Ondřej Lhoták; Laurie Hendren",
    "corresponding_authors": "",
    "abstract": "In this article we present Jedd, a language extension to Java that supports a convenient way of programming with Binary Decision Diagrams (BDDs). The Jedd language abstracts BDDs as database-style relations and operations on relations, and provides static type rules to ensure that relational operations are used correctly. The article provides a description of the Jedd language and reports on the design and implementation of the Jedd translator and associated runtime system. Of particular interest is the approach to assigning attributes from the high-level relations to physical domains in the underlying BDDs, which is done by expressing the constraints as a SAT problem and using a modern SAT solver to compute the solution. Further, a runtime system is defined that handles memory management issues and supports a browsable profiling tool for tuning the key BDD operations. The motivation for designing Jedd was to support the development of interrelated whole program analyses based on BDDs. We have successfully used Jedd to build Paddle, a framework of context-sensitive program analyses, including points-to analysis and call graph construction, as well as several client analyses.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2083104467",
    "type": "article"
  },
  {
    "title": "Type Inference for C",
    "doi": "https://doi.org/10.1145/3421472",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Leandro T. C. Melo; Rodrigo Ribeiro; Breno Campos Ferreira Guimarães; Fernando Magno Quintão Pereira",
    "corresponding_authors": "",
    "abstract": "Type inference is a feature that is common to a variety of programming languages. While, in the past, it has been prominently present in functional ones (e.g., ML and Haskell), today, many object-oriented/multi-paradigm languages such as C# and C++ offer, to a certain extent, such a feature. Nevertheless, type inference still is an unexplored subject in the realm of C. In particular, it remains open whether it is possible to devise a technique that encompasses the idiosyncrasies of this language. The first difficulty encountered when tackling this problem is that parsing C requires, not only syntactic, but also semantic information. Yet, greater challenges emerge due to C’s intricate type system. In this work, we present a unification-based framework that lets us infer the missing struct, union, enum, and typedef declarations in a program. As an application of our technique, we investigate the reconstruction of partial programs. Incomplete source code naturally appears in software development: during design and while evolving, testing, and analyzing programs; therefore, understanding it is a valuable asset. With a reconstructed well-typed program, one can: (i) enable static analysis tools in scenarios where components are absent; (ii) improve precision of “zero setup” static analysis tools; (iii) apply stub generators, symbolic executors, and testing tools on code snippets; and (iv) provide engineers with an assortment of compilable benchmarks for performance and correctness validation. We evaluate our technique on code from a variety of C libraries, including GNU’s Coreutils and on snippets from popular projects such as CPython, FreeBSD, and Git.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3012611775",
    "type": "article"
  },
  {
    "title": "Active Learning for Inference and Regeneration of Applications that Access Databases",
    "doi": "https://doi.org/10.1145/3430952",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Jiasi Shen; Martin Rinard",
    "corresponding_authors": "",
    "abstract": "We present K onure , a new system that uses active learning to infer models of applications that retrieve data from relational databases. K onure comprises a domain-specific language (each model is a program in this language) and associated inference algorithm that infers models of applications whose behavior can be expressed in this language. The inference algorithm generates inputs and database contents, runs the application, then observes the resulting database traffic and outputs to progressively refine its current model hypothesis. Because the technique works with only externally observable inputs, outputs, and database contents, it can infer the behavior of applications written in arbitrary languages using arbitrary coding styles (as long as the behavior of the application is expressible in the domain-specific language). K onure also implements a regenerator that produces a translated Python implementation of the application that systematically includes relevant security and error checks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3123214346",
    "type": "article"
  },
  {
    "title": "On a Technique for Transparently Empowering Classical Compiler Optimizations on Multithreaded Code",
    "doi": "https://doi.org/10.1145/2220365.2220368",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Pramod G. Joisha; Robert Schreiber; Prithviraj Banerjee; Hans‐J. Boehm; Dhruva R. Chakrabarti",
    "corresponding_authors": "",
    "abstract": "A large body of data-flow analyses exists for analyzing and optimizing sequential code. Unfortunately, much of it cannot be directly applied on parallel code, for reasons of correctness. This article presents a technique to automatically, aggressively, yet safely apply sequentially-sound data-flow transformations, without change , on shared-memory programs. The technique is founded on the notion of program references being “siloed” on certain control-flow paths. Intuitively, siloed references are free of interference from other threads within the confines of such paths. Data-flow transformations can, in general, be unblocked on siloed references. The solution has been implemented in a widely used compiler. Results on benchmarks from SPLASH-2 show that performance improvements of up to 41% are possible, with an average improvement of 6% across all the tested programs over all thread counts.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1980599496",
    "type": "article"
  },
  {
    "title": "A reexamination of “Optimization of array subscript range checks”",
    "doi": "https://doi.org/10.1145/201059.201063",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Wei-Ngan Chin; Eak-Khoon Goh",
    "corresponding_authors": "",
    "abstract": "Jonathan Asuru proposed recently an enhanced method for optimizing array subscript range checks. The proposed method is however unsafe and may generate optimized programs whose behavior is different from the original program. Two main flaws in Asuru's method are described, together with suggested remedies and improvements.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2059153210",
    "type": "article"
  },
  {
    "title": "Magma2: a language oriented toward experiments in control",
    "doi": "https://doi.org/10.1145/1780.1784",
    "publication_date": "1984-10-01",
    "publication_year": 1984,
    "authors": "Franco Turini",
    "corresponding_authors": "Franco Turini",
    "abstract": "article Free Access Share on Magma2: a language oriented toward experiments in control Author: Franco Turini Univ. di Pisa, Pisa, Italy Univ. di Pisa, Pisa, ItalyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 4Oct. 1984 pp 468–486https://doi.org/10.1145/1780.1784Published:01 October 1984Publication History 8citation277DownloadsMetricsTotal Citations8Total Downloads277Last 12 Months18Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2049033163",
    "type": "article"
  },
  {
    "title": "Communicating Sequential Processes for Centralized and Distributed Operating System Design",
    "doi": "https://doi.org/10.1145/2993.2381",
    "publication_date": "1984-04-01",
    "publication_year": 1984,
    "authors": "M.E.C. Hull; R. M. McKeag",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Communicating Sequential Processes for Centralized and Distributed Operating System Design Authors: M. Elizabeth C. Hull School of Computer Science, Ulster Polytechnic, Shore Road, Newtownabbey, Co Antrim BT37 0QB, Northern Ireland and The Queens's University of Belfast School of Computer Science, Ulster Polytechnic, Shore Road, Newtownabbey, Co Antrim BT37 0QB, Northern Ireland and The Queens's University of BelfastView Profile , R. M. McKeag Department of Computer Science, The Queen's University of Belfast, Belfast BT7 1NN, Northern Ireland Department of Computer Science, The Queen's University of Belfast, Belfast BT7 1NN, Northern IrelandView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 2pp 175–191https://doi.org/10.1145/2993.2381Published:01 April 1984Publication History 5citation532DownloadsMetricsTotal Citations5Total Downloads532Last 12 Months13Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2055244898",
    "type": "article"
  },
  {
    "title": "Extending Graham-Glanville techniques for optimal code generation",
    "doi": "https://doi.org/10.1145/371880.371881",
    "publication_date": "2000-11-01",
    "publication_year": 2000,
    "authors": "Maya Madhavan; Priti Shankar; Siddartha Rai; Uma Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "We propose a new technique for constructing code-generator generators, which combines the advantages of the Graham-Glanville parsing technique and the bottom-up tree parsing approach. Machine descriptions are similar to Yacc specifications. The construction effectively generates a pushdown automaton as the matching device. This device is able to handle ambigious grammars, and can be used to generate locally optimal code without the use of heuristics. Cost computations are performed at preprocessing time. The class of regular tree grammars augmented with costs that can be handled by our system properly includes those that can be handled by bottom-up systems based on finite-state tree parsing automata. Parsing time is linear in the size of the subject tree. We have tested the system on specifications for some systems and report table sizes.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2118249151",
    "type": "article"
  },
  {
    "title": "Eliminating synchronization bottlenecks using adaptive replication",
    "doi": "https://doi.org/10.1145/641909.641911",
    "publication_date": "2003-05-01",
    "publication_year": 2003,
    "authors": "Martin Rinard; Pedro C. Diniz",
    "corresponding_authors": "",
    "abstract": "This article presents a new technique, adaptive replication, for automatically eliminating synchronization bottlenecks in multithreaded programs that perform atomic operations on objects. Synchronization bottlenecks occur when multiple threads attempt to concurrently update the same object. It is often possible to eliminate synchronization bottlenecks by replicating objects. Each thread can then update its own local replica without synchronization and without interacting with other threads. When the computation needs to access the original object, it combines the replicas to produce the correct values in the original object. One potential problem is that eagerly replicating all objects may lead to performance degradation and excessive memory consumption.Adaptive replication eliminates unnecessary replication by dynamically detecting contention at each object to find and replicate only those objects that would otherwise cause synchronization bottlenecks. We have implemented adaptive replication in the context of a parallelizing compiler for a subset of C++. Given an unannotated sequential program written in C++, the compiler automatically extracts the concurrency, determines when it is legal to apply adaptive replication, and generates parallel code that uses adaptive replication to efficiently eliminate synchronization bottlenecks.In addition to automatic parallelization and adaptive replication, our compiler also implements a lock coarsening transformation that increases the granularity at which the computation locks objects. The advantage is a reduction in the frequency with which the computation acquires and releases locks; the potential disadvantage is the introduction of new synchronization bottlenecks caused by increases in the sizes of the critical sections. Because the adaptive replication transformation takes place at lock acquisition sites, there is a synergistic interaction between lock coarsening and adaptive replication. Lock coarsening drives down the overhead of using adaptive replication, and adaptive replication eliminates synchronization bottlenecks associated with the overaggressive use of lock coarsening.Our experimental results show that, for our set of benchmark programs, the combination of lock coarsening and adaptive replication can eliminate synchronization bottlenecks and significantly reduce the synchronization and replication overhead as compared to versions that use none or only one of the transformations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2151979043",
    "type": "article"
  },
  {
    "title": "Automatic discovery of covariant read-only fields",
    "doi": "https://doi.org/10.1145/1053468.1053472",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Jens Palsberg; Tian Zhao; Trevor Jim",
    "corresponding_authors": "",
    "abstract": "Read-only fields are useful in object calculi, pi calculi, and statically typed intermediate languages because they admit covariant subtyping, unlike updateable fields. For example, Glew's translation of classes and objects to an intermediate calculus relies crucially on covariant subtyping of read-only fields to ensure that subclasses are translated to subtypes.In this article, we present a type inference algorithm for an Abadi--Cardelli object calculus in which fields are marked either as updateable or as read-only. The type inference problem is P-complete, and our algorithm runs in O ( n 3 ) time. The same complexity results hold for the calculus in which the fields are not explicitly annotated as updateable or read-only; perhaps surprisingly, the annotations do not make type inference easier. We show that type inference is equivalent to the problem of solving type constraints, and this forms the core of our algorithm and implementation.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1992228832",
    "type": "article"
  },
  {
    "title": "Extensible objects without labels",
    "doi": "https://doi.org/10.1145/1018203.1018206",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "C. Addison Stone",
    "corresponding_authors": "C. Addison Stone",
    "abstract": "Typed object calculi that permit adding new methods to existing objects must address the problem of name clashes: what happens if a new method is added to an object already having one with the same name but a different type? Most systems statically forbid such clashes by restricting the allowable subtypings. In contrast, by reconsidering the runtime meaning of object extension, the object calculus studied in the author's previous work with Jon Riecke allowed any object to be soundly extended with any method of any name, with unrestricted width subtyping. That language permitted a simple encoding of classes as object-generators. Because of width subtyping, subclasses could be typechecked and compiled with little knowledge of the class hierarchy and without any information about superclasses' private components; this made derived classes more robust to changes in the implementations of base classes. However, the system was not well suited for encoding mixins or by-name subtyping of objects.This article addresses those deficiencies by presenting the Calculus of Objects and Indices (COI), a lower-level typed object calculus in which extensible objects are more analogous to tuples than to records. An object is simply a finite sequence of unnamed components referenced by their index in the sequence. Names are then reintroduced by allowing these indices to be first-class values (analogous to pointers to members in C++) that can be bound to variables. Since variables---unlike record labels---freely alpha-vary, difficulties caused by statically undetectable name clashes disappear.By combining COI objects with standard type-theoretic mechanisms, one can encode mixins and classes having the by-name subtyping of languages like C++ or Java but with the robustness of the object-generator encodings. Using records, more standard extensible objects with named components can also be encoded.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2074678391",
    "type": "article"
  },
  {
    "title": "A Programming Language for Data Privacy with Accuracy Estimations",
    "doi": "https://doi.org/10.1145/3452096",
    "publication_date": "2021-06-08",
    "publication_year": 2021,
    "authors": "Elisabet Lobo-Vesga; Alejandro Russo; Marco Gaboardi",
    "corresponding_authors": "",
    "abstract": "Differential privacy offers a formal framework for reasoning about the privacy and accuracy of computations on private data. It also offers a rich set of building blocks for constructing private data analyses. When carefully calibrated, these analyses simultaneously guarantee the privacy of the individuals contributing their data, and the accuracy of the data analysis results, inferring useful properties about the population. The compositional nature of differential privacy has motivated the design and implementation of several programming languages to ease the implementation of differentially private analyses. Even though these programming languages provide support for reasoning about privacy, most of them disregard reasoning about the accuracy of data analyses. To overcome this limitation, we present DPella, a programming framework providing data analysts with support for reasoning about privacy, accuracy, and their trade-offs. The distinguishing feature of DPella is a novel component that statically tracks the accuracy of different data analyses. To provide tight accuracy estimations, this component leverages taint analysis for automatically inferring statistical independence of the different noise quantities added for guaranteeing privacy. We evaluate our approach by implementing several classical queries from the literature and showing how data analysts can calibrate the privacy parameters to meet the accuracy requirements, and vice versa.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3027019508",
    "type": "article"
  },
  {
    "title": "Safe-by-default Concurrency for Modern Programming Languages",
    "doi": "https://doi.org/10.1145/3462206",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Lun Liu; Todd Millstein; Madanlal Musuvathi",
    "corresponding_authors": "",
    "abstract": "Modern “safe” programming languages follow a design principle that we call safety by default and performance by choice . By default, these languages enforce important programming abstractions, such as memory and type safety, but they also provide mechanisms that allow expert programmers to explicitly trade some safety guarantees for increased performance. However, these same languages have adopted the inverse design principle in their support for multithreading. By default, multithreaded programs violate important abstractions, such as program order and atomic access to individual memory locations to admit compiler and hardware optimizations that would otherwise need to be restricted. Not only does this approach conflict with the design philosophy of safe languages, but very little is known about the practical performance cost of providing a stronger default semantics. In this article, we propose a safe-by-default and performance-by-choice multithreading semantics for safe languages, which we call volatile -by-default . Under this semantics, programs have sequential consistency (SC) by default, which is the natural “interleaving” semantics of threads. However, the volatile -by-default design also includes annotations that allow expert programmers to avoid the associated overheads in performance-critical code. We describe the design, implementation, optimization, and evaluation of the volatile -by-default semantics for two different safe languages: Java and Julia. First, we present V BD-HotSpot and V BDA-HotSpot, modifications of Oracle’s HotSpot JVM that enforce the volatile -by-default semantics on Intel x86-64 hardware and ARM-v8 hardware. Second, we present S C-Julia, a modification to the just-in-time compiler within the standard Julia implementation that provides best-effort enforcement of the volatile -by-default semantics on x86-64 hardware for the purpose of performance evaluation. We also detail two different implementation techniques: a baseline approach that simply reuses existing mechanisms in the compilers for handling atomic accesses, and a speculative approach that avoids the overhead of enforcing the volatile -by-default semantics until there is the possibility of an SC violation. Our results show that the cost of enforcing SC is significant but arguably still acceptable for some use cases today. Further, we demonstrate that compiler optimizations as well as programmer annotations can reduce the overhead considerably.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3198655453",
    "type": "article"
  },
  {
    "title": "Nested Session Types",
    "doi": "https://doi.org/10.1145/3539656",
    "publication_date": "2022-07-15",
    "publication_year": 2022,
    "authors": "Ankush Das; Henry DeYoung; Andreia Mordido; Frank Pfenning",
    "corresponding_authors": "",
    "abstract": "Session types statically describe communication protocols between concurrent message-passing processes. Unfortunately, parametric polymorphism even in its restricted prenex form is not fully understood in the context of session types. In this article, we present the metatheory of session types extended with prenex polymorphism and, as a result, nested recursive datatypes. Remarkably, we prove that type equality is decidable by exhibiting a reduction to trace equivalence of deterministic first-order grammars. Recognizing the high theoretical complexity of the latter, we also propose a novel type equality algorithm and prove its soundness. We observe that the algorithm is surprisingly efficient and, despite its incompleteness, sufficient for all our examples. We have implemented our ideas by extending the Rast programming language with nested session types. We conclude with several examples illustrating the expressivity of our enhanced type system.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4285489720",
    "type": "article"
  },
  {
    "title": "Session Coalgebras: A Coalgebraic View on Regular and Context-free Session Types",
    "doi": "https://doi.org/10.1145/3527633",
    "publication_date": "2022-07-15",
    "publication_year": 2022,
    "authors": "Alex C. Keizer; Henning Basold; Jorge A. Pérez",
    "corresponding_authors": "",
    "abstract": "Compositional methods are central to the verification of software systems. For concurrent and communicating systems, compositional techniques based on behavioural type systems have received much attention. By abstracting communication protocols as types, these type systems can statically check that channels in a program interact following a certain protocol—whether messages are exchanged in the intended order. In this article, we put on our coalgebraic spectacles to investigate session types , a widely studied class of behavioural type systems. We provide a syntax-free description of session-based concurrency as states of coalgebras. As a result, we rediscover type equivalence, duality, and subtyping relations in terms of canonical coinductive presentations. In turn, this coinductive presentation enables us to derive a decidable type system with subtyping for the π-calculus, in which the states of a coalgebra will serve as channel protocols. Going full circle, we exhibit a coalgebra structure on an existing session type system, and show that the relations and type system resulting from our coalgebraic perspective coincide with existing ones. We further apply to session coalgebras the coalgebraic approach to regular languages via the so-called rational fixed point, inspired by the trinity of automata, regular languages, and regular expressions with session coalgebras, rational fixed point, and session types, respectively. We establish a suitable restriction on session coalgebras that determines a similar trinity, and reveals the mismatch between usual session types and our syntax-free coalgebraic approach. Furthermore, we extend our coalgebraic approach to account for context-free session types, by equipping session coalgebras with a stack.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4285489726",
    "type": "article"
  },
  {
    "title": "Immutability and Encapsulation for Sound OO Information Flow Control",
    "doi": "https://doi.org/10.1145/3573270",
    "publication_date": "2022-12-02",
    "publication_year": 2022,
    "authors": "Tobias Runge; Marco Servetto; Alex Potanin; Ina Schaefer",
    "corresponding_authors": "",
    "abstract": "Security-critical software applications contain confidential information which has to be protected from leaking to unauthorized systems. With language-based techniques, the confidentiality of applications can be enforced. Such techniques are for example type systems that enforce an information flow policy through typing rules. The precision of such type systems, especially in object-oriented languages, is an area of active research: an appropriate system should not reject too many secure programs while soundly preserving noninterference. In this work, we introduce the language SIFO which supports information flow control for an object-oriented language with type modifiers. Type modifiers increase the precision of the type system by utilizing immutability and uniqueness properties of objects for the detection of information leaks. We present SIFO informally by using examples to demonstrate the applicability of the language, formalize the type system, prove noninterference, implement SIFO as a pluggable type system in the programming language L42, and evaluate it with a feasibility study and a benchmark.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4311443586",
    "type": "article"
  },
  {
    "title": "Cliché-based program editors",
    "doi": "https://doi.org/10.1145/174625.174628",
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Richard C. Waters",
    "corresponding_authors": "Richard C. Waters",
    "abstract": "article Free Access Share on Cliché-based program editors Author: Richard C. Waters Mitsubishi Electric Labs, Cambridge, MA Mitsubishi Electric Labs, Cambridge, MAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 16Issue 1Jan. 1994 pp 102–150https://doi.org/10.1145/174625.174628Online:01 January 1994Publication History 4citation475DownloadsMetricsTotal Citations4Total Downloads475Last 12 Months7Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1997480114",
    "type": "article"
  },
  {
    "title": "A further note on Hennessy's “Symbolic debugging of optimized code”",
    "doi": "https://doi.org/10.1145/169701.214526",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Max Copperman; Charles E. McDowell",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A further note on Hennessy's “Symbolic debugging of optimized code” Authors: Max Copperman Board of Studies in Computer and Information Sciences, University of California at Santa Cruz, Santa Cruz, CA Board of Studies in Computer and Information Sciences, University of California at Santa Cruz, Santa Cruz, CAView Profile , Charles E. McDowell Board of Studies in Computer and Information Sciences, University of California at Santa Cruz, Santa Cruz, CA Board of Studies in Computer and Information Sciences, University of California at Santa Cruz, Santa Cruz, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 2pp 357–365https://doi.org/10.1145/169701.214526Published:01 April 1993Publication History 6citation274DownloadsMetricsTotal Citations6Total Downloads274Last 12 Months10Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2049837989",
    "type": "article"
  },
  {
    "title": "A generalized iterative construct and its semantics",
    "doi": "https://doi.org/10.1145/29873.30391",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Ed Anson",
    "corresponding_authors": "Ed Anson",
    "abstract": "A new programming language construct, called DOupon, subsumes Dijkstra's selective (IF) and iterative (DO) constructs. DOupon has a predicate transformer approximately equivalent in complexity to that for DO. In addition, it simplifies a wide variety of algorithms, in form as well as in discovery and proof. Several theorems are demonstrated that are useful for correctness proofs and for optimization and that are not applicable to DO or IF. The general usefulness of DOupon derives from a separation of the concerns of invariance, through iteration, from those of termination.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1967292878",
    "type": "article"
  },
  {
    "title": "Polymorphic typing of variables and references",
    "doi": "https://doi.org/10.1145/229542.229544",
    "publication_date": "1996-05-01",
    "publication_year": 1996,
    "authors": "Geoffrey Smith; Dennis Volpano",
    "corresponding_authors": "",
    "abstract": "In this article we consider the polymorphic type checking of an imperative language. Our language contains variables , first-class references (pointers), and first-class functions. Variables, as in traditional imperative languages, are implicitly dereferenced, and their addresses ( L -values) are not first-class values. Variables are easier to type check than references and, in many cases, lead to more general polymorphic types. We present a polymorphic type system for our language and prove that it is sound. Programs that use variables sometimes require weak types, as in Tofte's type system for Standard ML, but such weak types arise far less frequently with variables than with references",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2112976841",
    "type": "article"
  },
  {
    "title": "High-Level Language Implications of the Proposed IEEE Floating-Point Standard",
    "doi": "https://doi.org/10.1145/357162.357168",
    "publication_date": "1982-04-01",
    "publication_year": 1982,
    "authors": "Richard J. Fateman",
    "corresponding_authors": "Richard J. Fateman",
    "abstract": "article Free Access Share on High-Level Language Implications of the Proposed IEEE Floating-Point Standard Author: Richard J. Fateman Computer Science Division, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA Computer Science Division, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 2pp 239–257https://doi.org/10.1145/357162.357168Published:01 April 1982Publication History 3citation381DownloadsMetricsTotal Citations3Total Downloads381Last 12 Months9Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2009913507",
    "type": "article"
  },
  {
    "title": "First-class monadic schedules",
    "doi": "https://doi.org/10.1145/1011508.1011509",
    "publication_date": "2004-07-01",
    "publication_year": 2004,
    "authors": "Rajiv Mirani; Paul Hudak",
    "corresponding_authors": "",
    "abstract": "Parallel functional languages often use meta-linguistic annotations to provide control over parallel evaluation. In this paper we explore a flexible mechanism to control when an expression is evaluated: first-class monadic schedules . We discuss the advantages of using such first-class values over traditional annotation-based systems. In particular, it is often desirable to make decisions about the operational behavior of parallel programs depending on the dynamic state of the system. For example, we may want to measure the system load before deciding to evaluate expressions in parallel. For this purpose, we show how monads can be used to access dynamic system parameters in a referentially transparent manner (up to termination).As a mechanism to reason about schedules, we present a set of algebraic properties that any implementation of schedules must satisfy. We also describe an implementation that translates schedules into a dialect of Scheme extended with futures . We prove that this implementation satisfies the given set of algebraic properties, and give performance results for a parallel solution to the n -body problem using the Barnes--Hut method.Although our ideas were developed specifically for nonstrict functional languages such as Haskell, we briefly discuss how they can be used with strict functional languages and imperative languages as well.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2078213398",
    "type": "article"
  },
  {
    "title": "Two-dimensional bidirectional object layout",
    "doi": "https://doi.org/10.1145/1387673.1387677",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Joseph Gil; William Pugh; Grant Weddell; Yoav Zibin",
    "corresponding_authors": "",
    "abstract": "Object layout schemes used in C++ and other languages rely on (sometimes numerous) compiler generated fields. We describe a language-independent object layout scheme, which is space optimal, that is, objects are contiguous, and contain no compiler generated fields other than a single type identifier. As in C++ and other multiple inheritance languages such as CECIL and DYLAN, the new scheme sometimes requires extra levels of indirection to access some of the fields. Using a data set of 28 hierarchies, totaling almost 50,000 types, we show that this scheme improves field access efficiency over standard implementations, and competes favorably with (the non-space-optimal) highly optimized C++ specific implementations. The benchmark includes an analytical model for computing the frequency of indirections in a sequence of field access operations. Our layout scheme relies on whole-program analysis, which requires about 10 microseconds per type on a contemporary architecture (Pentium III, 900Mhz, 256MB machine), even in very large hierarchies. We also present a layout scheme for separate compilation using the user-annotation of virtual inheritance edge that is used in C++.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2011384897",
    "type": "article"
  },
  {
    "title": "A proof theory for machine code",
    "doi": "https://doi.org/10.1145/1286821.1286827",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Atsushi Ohori",
    "corresponding_authors": "Atsushi Ohori",
    "abstract": "This article develops a proof theory for low-level code languages. We first define a proof system, which we refer to as the sequential sequent calculus , and show that it enjoys the cut elimination property and that its expressive power is the same as that of the natural deduction proof system. We then establish the Curry-Howard isomorphism between this proof system and a low-level code language by showing the following properties: (1) the set of proofs and the set of typed codes is in one-to-one correspondence, (2) the operational semantics of the code language is directly derived from the cut elimination procedure of the proof system, and (3) compilation and decompilation algorithms between the code language and the typed lambda calculus are extracted from the proof transformations between the sequential sequent calculus and the natural deduction proof system. This logical framework serves as a basis for the development of type systems of various low-level code languages, type-preserving compilation, and static code analysis.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2072210851",
    "type": "article"
  },
  {
    "title": "Decomposing bytecode verification by abstract interpretation",
    "doi": "https://doi.org/10.1145/1452044.1452047",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "Cinzia Bernardeschi; N. De Francesco; Giuseppe Lettieri; Luca Martini; Paolo Masci",
    "corresponding_authors": "",
    "abstract": "Bytecode verification is a key point in the security chain of the Java platform. This feature is only optional in many embedded devices since the memory requirements of the verification process are too high. In this article we propose an approach that significantly reduces the use of memory by a serial/parallel decomposition of the verification into multiple specialized passes. The algorithm reduces the type encoding space by operating on different abstractions of the domain of types. The results of our evaluation show that this bytecode verification can be performed directly on small memory systems. The method is formalized in the framework of abstract interpretation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2150877629",
    "type": "article"
  },
  {
    "title": "ML, Visibly Pushdown Class Memory Automata, and Extended Branching Vector Addition Systems with States",
    "doi": "https://doi.org/10.1145/3310338",
    "publication_date": "2019-04-26",
    "publication_year": 2019,
    "authors": "Conrad Cotton-Barratt; Andrzej S. Murawski; C.-H. Luke Ong",
    "corresponding_authors": "",
    "abstract": "We prove that the observational equivalence problem for a finitary fragment of the programming langauge ML is recursively equivalent to the reachability problem for extended branching vector addition systems with states (EBVASS). This result has two natural and independent parts. We first prove that the observational equivalence problem is equivalent to the emptiness problem for a new class of class memory automata equipped with a visibly pushdown stack, called Visibly Pushdown Class Memory Automata (VPCMA). Our proof uses the fully abstract game semantics of the language. We then prove that the VPCMA emptiness problem is equivalent to the reachability problem for EBVASS. The results of this article complete our programme to give an automata classification of the ML types with respect to the observational equivalence problem for closed terms.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2942696941",
    "type": "article"
  },
  {
    "title": "Generalized Points-to Graphs",
    "doi": "https://doi.org/10.1145/3382092",
    "publication_date": "2020-05-19",
    "publication_year": 2020,
    "authors": "Pritam Gharat; Uday P. Khedker; Alan Mycroft",
    "corresponding_authors": "",
    "abstract": "Computing precise (fully flow- and context-sensitive) and exhaustive (as against demand-driven) points-to information is known to be expensive. Top-down approaches require repeated analysis of a procedure for separate contexts. Bottom-up approaches need to model unknown pointees accessed indirectly through pointers that may be defined in the callers and hence do not scale while preserving precision. Therefore, most approaches to precise points-to analysis begin with a scalable but imprecise method and then seek to increase its precision. We take the opposite approach in that we begin with a precise method and increase its scalability. In a nutshell, we create naive but possibly non-scalable procedure summaries and then use novel optimizations to compact them while retaining their soundness and precision. For this purpose, we propose a novel abstraction called the generalized points-to graph (GPG), which views points-to relations as memory updates and generalizes them using the counts of indirection levels leaving the unknown pointees implicit. This allows us to construct GPGs as compact representations of bottom-up procedure summaries in terms of memory updates and control flow between them. Their compactness is ensured by strength reduction (which reduces the indirection levels), control flow minimization (which removes control flow edges while preserving soundness and precision), and call inlining (which enhances the opportunities of these optimizations). The effectiveness of GPGs lies in the fact that they discard as much control flow as possible without losing precision. This is the reason GPGs are very small even for main procedures that contain the effect of the entire program. This allows our implementation to scale to 158 kLoC for C programs. At a more general level, GPGs provide a convenient abstraction to represent and transform memory in the presence of pointers. Future investigations can try to combine it with other abstractions for static analyses that can benefit from points-to information.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3032226677",
    "type": "article"
  },
  {
    "title": "Chocola",
    "doi": "https://doi.org/10.1145/3427201",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Janwillem Swalens; Joeri De Koster; Wolfgang De Meuter",
    "corresponding_authors": "",
    "abstract": "Programmers often combine different concurrency models in a single program, in each part of the program using the model that fits best. Many programming languages, such as Clojure, Scala, and Java, cater to this need by supporting different concurrency models. However, existing programming languages often combine concurrency models in an ad hoc way, and the semantics of the combinations are not always well defined. This article studies the combination of three concurrency models: futures, transactions, and actors. We show that a naive combination of these models invalidates the guarantees they normally provide, thereby breaking the assumptions of programmers. Hence, we present Chocola : a unified language of futures, transactions, and actors that maintains the guarantees of all three models wherever possible, even when they are combined. We describe and formalize the semantics of this language and prove the guarantees it provides. We also provide an implementation as an extension of Clojure and demonstrated that it can improve the performance of three benchmark applications for relatively little effort from the developer.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3128475688",
    "type": "article"
  },
  {
    "title": "Mathematical foundation of trace scheduling",
    "doi": "https://doi.org/10.1145/1961204.1961206",
    "publication_date": "2011-04-01",
    "publication_year": 2011,
    "authors": "Utpal Banerjee",
    "corresponding_authors": "Utpal Banerjee",
    "abstract": "Since its introduction by Joseph A. Fisher in 1979, trace scheduling has influenced much of the work on compile-time ILP (Instruction Level Parallelism) transformations. Initially developed for use in microcode compaction, it quickly became the main technique for machine-level compile-time parallelism exploitation. Although it has been used since the 1980s in many state-of-the-art compilers (e.g., Intel, Fujitsu, HP), a rigorous theory of trace scheduling is still lacking in the existing literature. This is reflected in the ad hoc way compensation code is inserted after a trace compaction, in the total absence of any attempts to measure the size of that compensation code, and so on. The aim of this article is to create a mathematical theory of the foundation of trace scheduling. We give a clear algorithm showing how to insert compensation code after a trace is replaced with its schedule, and then prove that the resulting program is indeed equivalent to the original program. We derive an upper bound on the size of that compensation code, and show that this bound can be actually attained. We also give a very simple proof that the trace scheduling algorithm always terminates.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1968972437",
    "type": "article"
  },
  {
    "title": "An Abstract Model of Certificate Translation",
    "doi": "https://doi.org/10.1145/1985342.1985344",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Gilles Barthe; César Kunz",
    "corresponding_authors": "",
    "abstract": "A certificate is a mathematical object that can be used to establish that a piece of mobile code satisfies some security policy. In general, certificates cannot be generated automatically. There is thus an interest in developing methods to reuse certificates generated for source code to provide strong guarantees of the compiled code correctness. Certificate translation is a method to transform certificates of program correctness along semantically justified program transformations. These methods have been developed in previous work, but they were strongly dependent on particular programming and verification settings. This article provides a more general development in the setting of abstract interpretation, showing the scalability of certificate translation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1991179242",
    "type": "article"
  },
  {
    "title": "Efficient Identification of Linchpin Vertices in Dependence Clusters",
    "doi": "https://doi.org/10.1145/2491522.2491524",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "David Binkley; Nicolas Gold; Mark Harman; Syed Mohammed Shamsul Islam; Jens Krinke; Zheng Li",
    "corresponding_authors": "",
    "abstract": "Several authors have found evidence of large dependence clusters in the source code of a diverse range of systems, domains, and programming languages. This raises the question of how we might efficiently locate the fragments of code that give rise to large dependence clusters. We introduce an algorithm for the identification of linchpin vertices, which hold together large dependence clusters, and prove correctness properties for the algorithm’s primary innovations. We also report the results of an empirical study concerning the reduction in analysis time that our algorithm yields over its predecessor using a collection of 38 programs containing almost half a million lines of code. Our empirical findings indicate improvements of almost two orders of magnitude, making it possible to process larger programs for which it would have previously been impractical.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2152997261",
    "type": "article"
  },
  {
    "title": "DRF <i>x</i>",
    "doi": "https://doi.org/10.1145/2925988",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Daniel Marino; Abhayendra Singh; Todd Millstein; Madanlal Musuvathi; Satish Narayanasamy",
    "corresponding_authors": "",
    "abstract": "The most intuitive memory model for shared-memory multi-threaded programming is sequential consistency (SC), but it disallows the use of many compiler and hardware optimizations and thus affects performance. Data-race-free (DRF) models, such as the C++11 memory model, guarantee SC execution for data-race-free programs. But these models provide no guarantee at all for racy programs, compromising the safety and debuggability of such programs. To address the safety issue, the Java memory model, which is also based on the DRF model, provides a weak semantics for racy executions. However, this semantics is subtle and complex, making it difficult for programmers to reason about their programs and for compiler writers to ensure the correctness of compiler optimizations. We present the drf x memory model, which is simple for programmers to understand and use while still supporting many common optimizations. We introduce a memory model (MM) exception that can be signaled to halt execution. If a program executes without throwing this exception, then drf x guarantees that the execution is SC. If a program throws an MM exception during an execution, then drf x guarantees that the program has a data race. We observe that SC violations can be detected in hardware through a lightweight form of conflict detection. Furthermore, our model safely allows aggressive compiler and hardware optimizations within compiler-designated program regions. We formalize our memory model, prove several properties of this model, describe a compiler and hardware design suitable for drf x , and evaluate the performance overhead due to our compiler and hardware requirements.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2519348496",
    "type": "article"
  },
  {
    "title": "Procedural implementation of algebraic specification",
    "doi": "https://doi.org/10.1145/161468.161473",
    "publication_date": "1993-11-01",
    "publication_year": 1993,
    "authors": "Huimin Lin",
    "corresponding_authors": "Huimin Lin",
    "abstract": "An implementation of an algebraic specification in an imperative programming language consists of a representation type, together with an invariant and an equivalence relation over it, and a procedure for each operator in the specification. A formal technique is developed to check the correctness of an implementation with respect to its specification. Here “correctness” means that the implementation satisfies the axioms and preserves the behavior of the specification. Within legal representing value space, a correct implementation behaves like a desirable model of the specification. A notion of implementation refinement is also proposed, and it is shown that the correctness relation between implementations and specifications is preserved by implementation refinement. In the extreme case the procedures in an implementation may be pre-post-condition pairs. Such abstract implementations can be refined into executable code by refining the abstract procedures in it. In this way a formal link between the algebraic and the pre- post-condition specification techniques is established.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1970116714",
    "type": "article"
  },
  {
    "title": "A comparative evaluation of object definition techniques for large prototype systems",
    "doi": "https://doi.org/10.1145/88616.88639",
    "publication_date": "1990-10-01",
    "publication_year": 1990,
    "authors": "Jack C. Wileden; Lori A. Clarke; Alexander L. Wolf",
    "corresponding_authors": "",
    "abstract": "Although prototyping has long been touted as a potentially valuable software engineering activity, it has never achieved widespread use by developers of large-scale, production software. This is probably due in part to an incompatibility between the languages and tools traditionally available for prototyping (e.g., LISP or Smalltalk) and the needs of large-scale-software developers, who must construct and experiment with large prototypes. The recent surge of interest in applying prototyping to the development of large-scale, production software will necessitate improved prototyping languages and tools appropriate for constructing and experimenting with large, complex prototype systems. We explore techniques aimed at one central aspect of prototyping that we feel is especially significant for large prototypes, namely that aspect concerned with the definition of data objects. We characterize and compare various techniques that might be useful in defining data objects in large prototype systems, after first discussing some distinguishing characteristics of large prototype systems and identifying some requirements that they imply. To make the discussion more concrete, we describe our implementations of three techniques that represent different possibilities within the range of object definition techniques for large prototype systems.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2005000255",
    "type": "article"
  },
  {
    "title": "Recursion As an Effective Step in Program Development",
    "doi": "https://doi.org/10.1145/357233.357236",
    "publication_date": "1984-01-01",
    "publication_year": 1984,
    "authors": "Livio Colussi",
    "corresponding_authors": "Livio Colussi",
    "abstract": "article Free AccessRecursion As an Effective Step in Program Development Author: L. Colussi Centro di Calcolo, Sezione Scientifica, Universita di Padova, Via Belzoni 7, 35100 Padova, Italy Centro di Calcolo, Sezione Scientifica, Universita di Padova, Via Belzoni 7, 35100 Padova, ItalyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 1Jan. 1984 pp 55–67https://doi.org/10.1145/357233.357236Published:01 January 1984Publication History 5citation343DownloadsMetricsTotal Citations5Total Downloads343Last 12 Months15Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2060317138",
    "type": "article"
  },
  {
    "title": "A methodology for synthesis of recursive functional programs",
    "doi": "https://doi.org/10.1145/24039.24071",
    "publication_date": "1987-07-01",
    "publication_year": 1987,
    "authors": "Debasish Banerjee",
    "corresponding_authors": "Debasish Banerjee",
    "abstract": "John Backus introduced the Functional Programming (FP) system, the variable-free applicative system having reduction semantics. Backus has also introduced a unique expansion technique for reasoning about a class of recursive FP programs. As a natural outgrowth of this expansion technique, an FP program synthesis methodology is described in this paper. The methodology synthesizes recursive FP programs of the form f = p →, q ; E ( f . h ) from their preformulated case-by-case descriptions, which in turn come from given input-output example specifications. After explaining the methodology informally, formalization in the form of a definition and a synthesis theorem is introduced. A sufficient condition for the functional form E , for successful synthesis under the present methodology, is obtained structurally. Several illustrative examples of synthesis are also included.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2089009297",
    "type": "article"
  },
  {
    "title": "A balanced code placement framework",
    "doi": "https://doi.org/10.1145/365151.365161",
    "publication_date": "2000-09-01",
    "publication_year": 2000,
    "authors": "Reinhard von Hanxleden; Ken Kennedy",
    "corresponding_authors": "",
    "abstract": "Give-N-Take is a code placement framework which uses a generic producer-consumer mechanism. An instance of this could be a communication step between a processor that computes (produces) some data, and other processors that subsequently reference (consume) these data in an expression. An advantage of Give-N-Take over traditional partial redundancy elimination techniques is its concept of production regions , instead of single locations, which can be beneficial for general latency hiding. Give-N-Take also guarantees balanced production, i.e., each production will be started and stopped exactly once. The framework can also take advantage of production coming “for free,” as induced by side effects, without disturbing balance. Give-N-Take can place production either before or after consumption, and it also provides the option to speculatively hoist code out of potentially zero-trip loop (nest) constructs. Give-N-Take uses a fast elimination method based on Tarjan intervals, with a complexity linear in the program size in most cases. We have implemented Give-N-Take as partof a Fortran D compiler prototype, where it solves various communication generation problems associated with compiling data-parallel languages onto distributed-memory architectures.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2031880149",
    "type": "article"
  },
  {
    "title": "An improved storage management scheme for block structured languages",
    "doi": "https://doi.org/10.1145/117009.117016",
    "publication_date": "1991-07-01",
    "publication_year": 1991,
    "authors": "Thomas P. Murtagh",
    "corresponding_authors": "Thomas P. Murtagh",
    "abstract": "article Free AccessAn improved storage management scheme for block structured languages Author: Thomas P. Murtagh Williams College, Williamstown, MA Williams College, Williamstown, MAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 13Issue 3July 1991 pp 372–398https://doi.org/10.1145/117009.117016Published:01 July 1991Publication History 5citation301DownloadsMetricsTotal Citations5Total Downloads301Last 12 Months18Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1991380088",
    "type": "article"
  },
  {
    "title": "Defining context-dependent syntax without using contexts",
    "doi": "https://doi.org/10.1145/169683.174159",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Martin Odersky",
    "corresponding_authors": "Martin Odersky",
    "abstract": "article Free Access Share on Defining context-dependent syntax without using contexts Author: Martin Odersky Yale Univ., New Haven, CT Yale Univ., New Haven, CTView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 3pp 535–562https://doi.org/10.1145/169683.174159Published:01 July 1993Publication History 6citation655DownloadsMetricsTotal Citations6Total Downloads655Last 12 Months157Last 6 weeks122 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2016865764",
    "type": "article"
  },
  {
    "title": "Optimization of functional programs by grammar thinning",
    "doi": "https://doi.org/10.1145/201059.201067",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Adam Brooks Webber",
    "corresponding_authors": "Adam Brooks Webber",
    "abstract": "We describe a new technique for optimizing first-order functional programs. Programs are represented as graph grammars, and optimization proceeds by counterexample: when a graph generated by the grammar is found to contain an unnecessary computation, the optimizer attempts to reformulates the grammar so that it never again generates any graph that contains that counterexample. This kind of program reformulation corresponds to an interesting problem on context-free grammars. Our reformulation technique is derived from an (approximate) solution to this CFG problem. An optimizer called Thinner is the proof of concept for this technique. Thinner is a fully automatic, source-to-source optimizer for a Lisp-like language of purely functional, first-order programs. Thinner rediscovers a wide variety of common compiler optimizations. It also finds other more exotic transformations, including the well-known Fibonacci reformulation and the Knuth-Morris-Pratt optimization.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2032552383",
    "type": "article"
  },
  {
    "title": "Subsequence references",
    "doi": "https://doi.org/10.1145/133233.133234",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "Wilfred J. Hansen",
    "corresponding_authors": "Wilfred J. Hansen",
    "abstract": "Arrays of characters are a basic data type in many programming languages, but strings and substrings are seldom accorded first-class status as parameters and return values. Such status would enable a routine that calls a search function to readily access context on both sides of a return value. To enfranchise substrings, this paper describes a new data type for substrings as a special case of one for general subsequences. The key idea is that values are not sequences or references to positions in sequences, but rather references to subsequences. Primitive operations on the data type are constants, concatenation, and four new functions— base , start , next , and extent —which map subsequence references to subsequence references. This paper informally presents the data type, demonstrates its convenience for defining search functions, and shows how it can be concisely implemented. Examples are given in Ness, a language incorporating the new data type, which is implemented as part of the Andrew User Interface System.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2051837461",
    "type": "article"
  },
  {
    "title": "Computer-assisted microanalysis of parallel programs",
    "doi": "https://doi.org/10.1145/111186.126699",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "Timothy J. Hickey; Jacques Cohen; Hirofumi Hotta; Thierry Petitjean",
    "corresponding_authors": "",
    "abstract": "This paper consists of two parts: the first provides the theoretical foundations for analyzing parallel programs and illustrates how the theory can be applied to estimate the execution time of a class of parallel programs being executed on a MIMD computer. The second part describes a program analysis system, based on the theoretical model, which allows a user to interactively analyze the results of executing (or simulating the execution) of such parallel programs. Several examples illustrating the use of the tool are presented. A novel contribution is the separation (both at the conceptual and the implementation levels) of the machine-independent and the machine-dependent parts of the analysis. This separation enables the users of the system to establish speed-up curves for machines having varying characteristics.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2079712609",
    "type": "article"
  },
  {
    "title": "A Directly Executable Encoding for APL",
    "doi": "https://doi.org/10.1145/579.580",
    "publication_date": "1984-07-01",
    "publication_year": 1984,
    "authors": "Richard F. Hobson",
    "corresponding_authors": "Richard F. Hobson",
    "abstract": "article Free AccessA Directly Executable Encoding for APL Author: Richard F. Hobson Simon Fraser University Simon Fraser UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 3pp 314–332https://doi.org/10.1145/579.580Published:01 July 1984Publication History 3citation326DownloadsMetricsTotal Citations3Total Downloads326Last 12 Months39Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1969867595",
    "type": "article"
  },
  {
    "title": "A Model for Implementing EUCLID Modules and Prototypes",
    "doi": "https://doi.org/10.1145/69622.357183",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "Richard C. Holt; David B. Wortman",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A Model for Implementing EUCLID Modules and Prototypes Authors: Richard C. Holt Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada M5S 1A1 Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada M5S 1A1View Profile , David B. Wortman Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada M5S 1A1 Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada M5S 1A1View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4Oct. 1982 pp 552–562https://doi.org/10.1145/69622.357183Published:01 October 1982Publication History 5citation236DownloadsMetricsTotal Citations5Total Downloads236Last 12 Months6Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2013267634",
    "type": "article"
  },
  {
    "title": "A Multiprocessing Approach to Compile-Time Symbol Resolution",
    "doi": "https://doi.org/10.1145/357121.357123",
    "publication_date": "1981-01-01",
    "publication_year": 1981,
    "authors": "Fabrice André; J. P. Banâtre; Jean-Paul Routeau",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A Multiprocessing Approach to Compile-Time Symbol Resolution Authors: F. André Institut de recherche en informatique et systèmes aléatoires (IRISA), Campus de Beaulieu, Avenue du General Leclerc, 35042--Rennes Cédex, France Institut de recherche en informatique et systèmes aléatoires (IRISA), Campus de Beaulieu, Avenue du General Leclerc, 35042--Rennes Cédex, FranceView Profile , J. P. Banatre Institut de recherche en informatique et systèmes aléatoires (IRISA), Campus de Beaulieu, Avenue du General Leclerc, 35042--Rennes Cédex, France Institut de recherche en informatique et systèmes aléatoires (IRISA), Campus de Beaulieu, Avenue du General Leclerc, 35042--Rennes Cédex, FranceView Profile , J. P. Routeau Institut de recherche en informatique et systèmes aléatoires (IRISA), Campus de Beaulieu, Avenue du General Leclerc, 35042--Rennes Cédex, France Institut de recherche en informatique et systèmes aléatoires (IRISA), Campus de Beaulieu, Avenue du General Leclerc, 35042--Rennes Cédex, FranceView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 1Jan. 1981 pp 11–23https://doi.org/10.1145/357121.357123Online:01 January 1981Publication History 4citation215DownloadsMetricsTotal Citations4Total Downloads215Last 12 Months4Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2039515644",
    "type": "article"
  },
  {
    "title": "A Flexible Notation for Syntactic Definitions",
    "doi": "https://doi.org/10.1145/357153.357159",
    "publication_date": "1982-01-01",
    "publication_year": 1982,
    "authors": "M. Howard Williams",
    "corresponding_authors": "M. Howard Williams",
    "abstract": "article Free Access Share on A Flexible Notation for Syntactic Definitions Author: M. Howard Williams Computer Science Department, Heriot-Watt University, 79 Grassmarket, Edinburgh EH1 2HJ, Scotland Computer Science Department, Heriot-Watt University, 79 Grassmarket, Edinburgh EH1 2HJ, ScotlandView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 1pp 113–119https://doi.org/10.1145/357153.357159Published:01 January 1982Publication History 3citation770DownloadsMetricsTotal Citations3Total Downloads770Last 12 Months17Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2052698481",
    "type": "article"
  },
  {
    "title": "User Format Control in a LISP Prettyprinter",
    "doi": "https://doi.org/10.1145/69575.357225",
    "publication_date": "1983-10-01",
    "publication_year": 1983,
    "authors": "Richard C. Waters",
    "corresponding_authors": "Richard C. Waters",
    "abstract": "article Free Access Share on User Format Control in a LISP Prettyprinter Author: Richard C. Waters MIT Artificial Intelligence Laboratory, 545 Technology Square, Cambridge, MA MIT Artificial Intelligence Laboratory, 545 Technology Square, Cambridge, MAView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 4pp 513–531https://doi.org/10.1145/69575.357225Published:01 October 1983Publication History 5citation316DownloadsMetricsTotal Citations5Total Downloads316Last 12 Months20Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2070713365",
    "type": "article"
  },
  {
    "title": "Corrigendum: “Distributed Termination”",
    "doi": "https://doi.org/10.1145/357103.357113",
    "publication_date": "1980-07-01",
    "publication_year": 1980,
    "authors": "Nissim Francez",
    "corresponding_authors": "Nissim Francez",
    "abstract": "No abstract available.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2079462608",
    "type": "erratum"
  },
  {
    "title": "Natural semantics as a static program analysis framework",
    "doi": "https://doi.org/10.1145/982158.982161",
    "publication_date": "2004-05-01",
    "publication_year": 2004,
    "authors": "Sabine Glesner; Wolf Zimmermann",
    "corresponding_authors": "",
    "abstract": "Natural semantics specifications have become mainstream in the formal specification of programming language semantics during the last 10 years. In this article, we set up sorted natural semantics as a specification framework which is able to express static semantic information of programming languages declaratively in a uniform way and allows one at the same time to generate corresponding analyses. Such static semantic information comprises context-sensitive properties which are checked in the semantic analysis phase of compilers as well as further static program analyses such as, for example, classical data and control flow analyses or type and effect systems. The latter require fixed-point analyses to determine their solutions. We show that, given a sorted natural semantics specification, we can generate the corresponding analysis. Therefore, we classify the solution of such an analysis by the notion of a proof tree. We show that a proof tree can be computed by solving an equivalent residuation problem. In case of the semantic analysis, this solution can be found by a basic algorithm. We show that its efficiency can be enhanced using solution strategies. We also demonstrate our prototype implementation of the basic algorithm which proves its applicability in practical situations. With the results of this article, we have established natural semantics as a framework which closes the gap between declarative and operational specification methods for static semantic properties as well as between specification frameworks for the semantic analysis. In particular, we show that natural semantics is expressive enough to define fixed-point program analyses.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2002854819",
    "type": "article"
  },
  {
    "title": "Comparing conservative coalescing criteria",
    "doi": "https://doi.org/10.1145/1065887.1065894",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Max Hailperin",
    "corresponding_authors": "Max Hailperin",
    "abstract": "Graph-coloring register allocators can eliminate copy instructions from a program by coalescing the interference graph nodes corresponding to the source and destination. Briggs showed that by limiting coalescing to those situations that he dubbed “conservative,” it could be prevented from causing spilling, that is, a situation where the allocator fails to assign a register to each live range. George and Appel adopted Briggs's conservativeness criterion in general, but provided an alternative criterion (the George test) to use in those cases where one of the nodes has been “precolored,” that is, preassigned a specific register. They motivated this alternative criterion by efficiency considerations, and provided no indication of the relative power of the two criteria. Thus it remained an open question whether the efficiency had been bought at the expense of reduced coalescing. Their implementation also used a limited version of the Briggs test, in place of the original, full version, without any comment on the impact of this substitution. In this article, we also present an analogously limited version of the George test.Thus we are now confronted with four different criteria for conservative coalescing: the full and limited Briggs tests and the full and limited George tests. We present a number of theorems characterizing the relative power of these different criteria, and a number of theorems characterizing the form of safety that each achieves. For example, we show that for coalescing with precolored nodes, the full George criterion is strictly more powerful than the full Briggs criterion, while offering an equally strong safety guarantee. Thus no coalesces are lost through George and Appel's introduction of the George test, and some can be gained without sacrificing safety.We also show that George and Appel's limited version of the Briggs test is probably undesirable. Although a slightly stronger safety result applies to it than to the full Briggs test, this comes at the expense of eliminating all coalesces that can reduce spilling.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2090378971",
    "type": "article"
  },
  {
    "title": "Polymorphic Iterable Sequential Effect Systems",
    "doi": "https://doi.org/10.1145/3450272",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Colin S. Gordon",
    "corresponding_authors": "Colin S. Gordon",
    "abstract": "Effect systems are lightweight extensions to type systems that can verify a wide range of important properties with modest developer burden. But our general understanding of effect systems is limited primarily to systems where the order of effects is irrelevant. Understanding such systems in terms of a semilattice of effects grounds understanding of the essential issues, and provides guidance when designing new effect systems. By contrast, sequential effect systems -- where the order of effects is important -- lack an established algebraic structure on effects. We present an abstract polymorphic effect system parameterized by an effect quantale -- an algebraic structure with well-defined properties that can model the effects of a range of existing sequential effect systems. We define effect quantales, derive useful properties, and show how they cleanly model a variety of known sequential effect systems. We show that for most effect quantales, there is an induced notion of iterating a sequential effect; that for systems we consider the derived iteration agrees with the manually designed iteration operators in prior work; and that this induced notion of iteration is as precise as possible when defined. We also position effect quantales with respect to work on categorical semantics for sequential effect systems, clarifying the distinctions between these systems and our own in the course of giving a thorough survey of these frameworks. Our derived iteration construct should generalize to these semantic structures, addressing limitations of that work. Finally, we consider the relationship between sequential effects and Kleene Algebras, where the latter may be used as instances of the former.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3155971776",
    "type": "article"
  },
  {
    "title": "Limitations of Partial Compaction",
    "doi": "https://doi.org/10.1145/2994597",
    "publication_date": "2017-03-06",
    "publication_year": 2017,
    "authors": "Nachshon Cohen; Erez Petrank",
    "corresponding_authors": "",
    "abstract": "Compaction of a managed heap is a costly operation to be avoided as much as possible in commercial runtimes. Instead, partial compaction is often used to defragment parts of the heap and avoid space blowup. Previous study of compaction limitation provided some initial asymptotic bounds but no implications for practical systems. In this work, we extend the theory to obtain better bounds and make them strong enough to become meaningful for modern systems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2591862005",
    "type": "article"
  },
  {
    "title": "Verifying Reliability Properties Using the Hyperball Abstract Domain",
    "doi": "https://doi.org/10.1145/3156017",
    "publication_date": "2017-12-19",
    "publication_year": 2017,
    "authors": "Jacob Lidman; Sally A. McKee",
    "corresponding_authors": "",
    "abstract": "Modern systems are increasingly susceptible to soft errors that manifest themselves as bit flips and possibly alter the semantics of an application. We would like to measure the quality degradation on semantics due to such bit flips, and thus we introduce a Hyperball abstract domain that allows us to determine the worst-case distance between expected and actual results. Similar to intervals, hyperballs describe a connected and dense space. The semantics of low-level code in the presence of bit flips is hard to accurately describe in such a space. We therefore combine the Hyperball domain with an existing affine system abstract domain that we extend to handle bit flips, which are introduce as disjunctions. Bit-flips can reduce the precision of our analysis, and we therefor introduce the Scale domain as a disjunctive refinement to minimize precision loss. This domain bounds the number of disjunctive elements by quantifying the over-approximation of different partitions and uses submodular optimization to find a good partitioning (within a bound of optimal). We evaluate these domains to show benefits and potential problems. For the application we examine here, adding the Scale domain to the Hyperball abstraction improves accuracy by up to two orders of magnitude. Our initial results demonstrate the feasibility of this approach, although we would like to further improve execution efficiency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2779212808",
    "type": "article"
  },
  {
    "title": "POP-PL",
    "doi": "https://doi.org/10.1145/3210256",
    "publication_date": "2018-07-05",
    "publication_year": 2018,
    "authors": "Spencer P. Florence; Burke Fetscher; Matthew Flatt; William H. Temps; Vincent St-Amour; Tina Kiguradze; Dennis P. West; Charlotte M. Niznik; Paul R. Yarnold; Robert Bruce Findler; Steven M. Belknap",
    "corresponding_authors": "",
    "abstract": "A medical prescription is a set of health care instructions that govern the plan of care for an individual patient, which may include orders for drug therapy, diet, clinical assessment, and laboratory testing. Clinicians have long used algorithmic thinking to describe and implement prescriptions but without the benefit of a formal programming language. Instead, medical algorithms are expressed using a natural language patois, flowcharts, or as structured data in an electronic medical record system. The lack of a prescription programming language inhibits expressiveness; results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse; and increases the risk of fatal medical error. This article reports on the design and evaluation of Patient-Oriented Prescription Programming Language (POP-PL), a domain-specific programming language designed for expressing prescriptions. The language is based around the idea that programs and humans have complementary strengths that, when combined properly, can make for safer, more accurate performance of prescriptions. Use of POP-PL facilitates automation of certain low-level vigilance tasks, freeing up human cognition for abstract thinking, compassion, and human communication. We implemented this language and evaluated its design attempting to write prescriptions in the new language and evaluated its usability by assessing whether clinicians can understand and modify prescriptions written in the language. We found that some medical prescriptions can be expressed in a formal domain-specific programming language, and we determined that medical professionals can understand and correctly modify programs written in POP-PL. We also discuss opportunities for refining and further developing POP-PL.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2826545012",
    "type": "article"
  },
  {
    "title": "A note on “On the conversion of indirect to direct recursion”",
    "doi": "https://doi.org/10.1145/267959.269973",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "Ting Yu; Owen Kaser",
    "corresponding_authors": "",
    "abstract": "In the article “On the Conversion of Indirect to Direct Recursion”( ACM Lett. Program. Lang. 2, 1-4. pp. 151-164), a method was introduced to convert indirect to direct recursion. It was claimed that for any call graph, there is a mutual-recursion elimination sequence if and only if no strongly connected component contains two node-disjoint circuits. We first give a counterexample and then provide a correction.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1970224142",
    "type": "article"
  },
  {
    "title": "Production trees: a compact representation of parsed programs",
    "doi": "https://doi.org/10.1145/77606.77609",
    "publication_date": "1990-01-03",
    "publication_year": 1990,
    "authors": "Vance Waddle",
    "corresponding_authors": "Vance Waddle",
    "abstract": "Abstract syntax trees were devised as a compact alternative to parse trees, because parse trees are known to require excessive amounts of storage to represent parsed programs. However, the savings that abstract syntax trees actually achieve have never been precisely described because the necessary analysis has been missing. Without it, one can only measure particular cases that may not adequately represent all the possible behaviors. We introduce a data structure, production trees, that are more compact than either abstract syntax trees or parse trees. Further, we develop the necessary analysis to characterize the storage requirements of parse trees, abstract syntax trees, and production trees and relate the size of all three to the size of the program's text. The analysis yields the parameters needed to characterize these storage behaviors over their entire range. We flesh out the analysis by measuring these parameters for a sample of “C” programs. For these programs, production trees were from 1/15 to 1/23 the size of the corresponding parse tree, l/2.7 the size of a (minimal) abstract syntax tree, and averaged only 2.83 times the size of the program text.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1972256049",
    "type": "article"
  },
  {
    "title": "Combinatory formulations of concurrent languages",
    "doi": "https://doi.org/10.1145/267959.269967",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "N. Raja; R. K. Shyamasundar",
    "corresponding_authors": "",
    "abstract": "We design a system with six Basic Combinators and prove that it is powerful enough to embed the full asynchronous π-calculus, including replication. Our theory for constructing Combinatory Versions of concurrent languages is based on a method, used by Quine and Bernays, for the general elimination of variables in linguistic formalisms. Our combinators are designed to eliminate the requirement of names that are bound by an input prefix . They also eliminate the need for input prefix, output prefix, and the accompanying mechanism of substitution . We define a notion of bisimulation for the combinatory version and show that the combinatory version preserves the semantics of the original calculus. One of the distinctive features of the approach is that it can be used to rework several process algebras in order to derive equivalent combinatory versions.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2075336500",
    "type": "article"
  },
  {
    "title": "Data descriptors: a compile-time model of data and addressing",
    "doi": "https://doi.org/10.1145/24039.24051",
    "publication_date": "1987-07-01",
    "publication_year": 1987,
    "authors": "Richard C. Holt",
    "corresponding_authors": "Richard C. Holt",
    "abstract": "Data descriptors, which have evolved from Wilcox's value descriptors [16], are a notation for representing run-time data objects at compile time. One of the principal reasons for developing this notation was to aid in the rapid construction of code generators, especially for new microprocessors. Each data descriptor contains a base, a displacement, and a level of indirection. For example, a variable x lying at displacement 28 from base register B3 is represented by this data descriptor: @B3.28. The general form of a data descriptor is @ k b.d.i where k gives the number of levels of indirection, b is a base, d is a displacement, and I is an index. Data descriptors are convenient for representing addressing in Fortran (with static allocation and common blocks), in Pascal and Turing (with automatic allocation and stack frames), and in more general languages such as Euclid and PL/I. This generality of data descriptors allows code generation to be largely independent of the source language. Data descriptors are able to encode the addressing modes of typical computer architectures such as the IBM 360 and the PDP-11. This generality of data descriptors allows code generation to be largely machine independent. This paper gives a machine independent method of storage allocation that uses data descriptors. Techniques are given for local optimization of basic arithmetic and addressing code using data descriptors. Target machine dependencies are isolated so that the part of the code generator that handles high-level addressing (such as subscripting) is machine independent. The techniques described in this paper have proven effective in the rapid development of a number of production code generators.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2095119364",
    "type": "article"
  },
  {
    "title": "The Construction of Stack-Controlling LR Parsers for Regular Right Part Grammars",
    "doi": "https://doi.org/10.1145/357133.357138",
    "publication_date": "1981-04-01",
    "publication_year": 1981,
    "authors": "Wilf R. LaLonde",
    "corresponding_authors": "Wilf R. LaLonde",
    "abstract": "article Free AccessThe Construction of Stack-Controlling LR Parsers for Regular Right Part Grammars Author: Wilf R. LaLonde School of Computer Science, Carleton University, Colonel By Drive, Ottawa, Canada K1S 5B6 School of Computer Science, Carleton University, Colonel By Drive, Ottawa, Canada K1S 5B6View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 2pp 168–206https://doi.org/10.1145/357133.357138Published:01 April 1981Publication History 2citation429DownloadsMetricsTotal Citations2Total Downloads429Last 12 Months21Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2055300214",
    "type": "article"
  },
  {
    "title": "Environmental Bisimulations for Probabilistic Higher-order Languages",
    "doi": "https://doi.org/10.1145/3350618",
    "publication_date": "2019-10-12",
    "publication_year": 2019,
    "authors": "Davide Sangiorgi; Valeria Vignudelli",
    "corresponding_authors": "",
    "abstract": "Environmental bisimulations for probabilistic higher-order languages are studied. In contrast with applicative bisimulations, environmental bisimulations are known to be more robust and do not require sophisticated techniques such as Howe’s in the proofs of congruence. As representative calculi, call-by-name and call-by-value λ-calculus, and a (call-by-value) λ-calculus extended with references (i.e., a store) are considered. In each case, full abstraction results are derived for probabilistic environmental similarity and bisimilarity with respect to contextual preorder and contextual equivalence, respectively. Some possible enhancements of the (bi)simulations, as “up-to techniques,” are also presented. Probabilities force a number of modifications to the definition of environmental bisimulations in non-probabilistic languages. Some of these modifications are specific to probabilities, others may be seen as general refinements of environmental bisimulations, applicable also to non-probabilistic languages. Several examples are presented, to illustrate the modifications and the differences.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2981096575",
    "type": "article"
  },
  {
    "title": "Armada: Automated Verification of Concurrent Code with Sound Semantic Extensibility",
    "doi": "https://doi.org/10.1145/3502491",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Jacob R. Lorch; Yixuan Chen; Manos Kapritsos; Haojun Ma; Bryan Parno; Shaz Qadeer; Upamanyu Sharma; James R. Wilcox; Xueyuan Zhao",
    "corresponding_authors": "",
    "abstract": "Safely writing high-performance concurrent programs is notoriously difficult. To aid developers, we introduce Armada, a language and tool designed to formally verify such programs with relatively little effort. Via a C-like language and a small-step, state-machine-based semantics, Armadagives developers the flexibility to choose arbitrary memory layout and synchronization primitives so that they are never constrained in their pursuit of performance. To reduce developer effort, Armadaleverages SMT-powered automation and a library of powerful reasoning techniques, including rely-guarantee, TSO elimination, reduction, and pointer analysis. All of these techniques are proven sound, and Armadacan be soundly extended with additional strategies over time. Using Armada, we verify five concurrent case studies and show that we can achieve performance equivalent to that of unverified code.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4220777158",
    "type": "article"
  },
  {
    "title": "Containerless Plurals: Separating Number from Type in Object-Oriented Programming",
    "doi": "https://doi.org/10.1145/3527635",
    "publication_date": "2022-04-21",
    "publication_year": 2022,
    "authors": "Friedrich Steimann",
    "corresponding_authors": "Friedrich Steimann",
    "abstract": "To let expressions evaluate to no or many objects, most object-oriented programming languages require the use of special constructs that encode these cases as single objects or values. While the requirement to treat these standard situations idiomatically seems to be broadly accepted, I argue that its alternative, letting expressions evaluate to any number of objects directly, has several advantages that make it worthy of consideration. As a proof of concept, I present a core object-oriented programming language, dubbed Num , which separates number from type so that the type of an expression is independent of the number of objects it may evaluate to, thus removing one major obstacle to using no, one, and many objects uniformly. Furthermore, Num abandons null references, replaces the nullability of reference types with the more general notion of countability, and allows methods to be invoked on any number of objects, including no object. To be able to adapt behavior to the actual number of receivers, Num complements instance methods with plural methods, that is, with methods that operate on a number of objects jointly and that replace static methods known from other languages. An implementation of Num in Prolog and accompanying type and number safety proofs are presented.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4224285350",
    "type": "article"
  },
  {
    "title": "Reactive Imperative Programming with Dataflow Constraints",
    "doi": "https://doi.org/10.1145/2623200",
    "publication_date": "2014-11-17",
    "publication_year": 2014,
    "authors": "Camil Demetrescu; Irene Finocchi; Andrea Ribichini",
    "corresponding_authors": "",
    "abstract": "Dataflow languages provide natural support for specifying constraints between objects in dynamic applications, where programs need to react efficiently to changes in their environment. In this article, we show that one-way dataflow constraints, largely explored in the context of interactive applications, can be seamlessly integrated in any imperative language and can be used as a general paradigm for writing performance-critical reactive applications that require efficient incremental computations. In our framework, programmers can define ordinary statements of the imperative host language that enforce constraints between objects stored in special memory locations designated as “reactive.” Reactive objects can be of any legal type in the host language, including primitive data types, pointers, arrays, and structures. Statements defining constraints are automatically re-executed every time their input memory locations change, letting a program behave like a spreadsheet where the values of some variables depend on the values of other variables. The constraint-solving mechanism is handled transparently by altering the semantics of elementary operations of the host language for reading and modifying objects. We provide a formal semantics and describe a concrete embodiment of our technique into C/C++, showing how to implement it efficiently in conventional platforms using off-the-shelf compilers. We discuss common coding idioms and relevant applications to reactive scenarios, including incremental computation, observer design pattern, data structure repair, and software visualization. The performance of our implementation is compared to problem-specific change propagation algorithms, as well as to language-centric approaches such as self-adjusting computation and subject/observer communication mechanisms, showing that the proposed approach is efficient in practice.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2017385491",
    "type": "article"
  },
  {
    "title": "A Derivative-based Parser Generator for Visibly Pushdown Grammars",
    "doi": "https://doi.org/10.1145/3591472",
    "publication_date": "2023-04-08",
    "publication_year": 2023,
    "authors": "Xiaodong Jia; Ashish Kumar; Gang Tan",
    "corresponding_authors": "",
    "abstract": "In this article, we present a derivative-based, functional recognizer and parser generator for visibly pushdown grammars. The generated parser accepts ambiguous grammars and produces a parse forest containing all valid parse trees for an input string in linear time. Each parse tree in the forest can then be extracted also in linear time. Besides the parser generator, to allow more flexible forms of the visibly pushdown grammars, we also present a translator that converts a tagged CFG to a visibly pushdown grammar in a sound way, and the parse trees of the tagged CFG are further produced by running the semantic actions embedded in the parse trees of the translated visibly pushdown grammar. The performance of the parser is compared with popular parsing tools, including ANTLR, GNU Bison, and other popular hand-crafted parsers. The correctness and the time complexity of the core parsing algorithm are formally verified in the proof assistant Coq.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4362721661",
    "type": "article"
  },
  {
    "title": "Optimizing Homomorphic Evaluation Circuits by Program Synthesis and Time-bounded Exhaustive Search",
    "doi": "https://doi.org/10.1145/3591622",
    "publication_date": "2023-08-02",
    "publication_year": 2023,
    "authors": "Dong-Kwon Lee; Woosuk Lee; Hakjoo Oh; Kwangkeun Yi",
    "corresponding_authors": "",
    "abstract": "We present a new and general method for optimizing homomorphic evaluation circuits. Although fully homomorphic encryption (FHE) holds the promise of enabling safe and secure third party computation, building FHE applications has been challenging due to their high computational costs. Domain-specific optimizations require a great deal of expertise on the underlying FHE schemes and FHE compilers that aim to lower the hurdle, generate outcomes that are typically sub-optimal, as they rely on manually-developed optimization rules. In this article, based on the prior work of FHE compilers, we propose a method for automatically learning and using optimization rules for FHE circuits. Our method focuses on reducing the maximum multiplicative depth, the decisive performance bottleneck, of FHE circuits by combining program synthesis, term rewriting, and equality saturation. It first uses program synthesis to learn equivalences of small circuits as rewrite rules from a set of training circuits. Then, we perform term rewriting on the input circuit to obtain a new circuit that has lower multiplicative depth. Our rewriting method uses the equational matching with generalized version of the learned rules, and its soundness property is formally proven. Our optimizations also try to explore every possible alternative order of applying rewrite rules by time-bounded exhaustive search technique called equality saturation. Experimental results show that our method generates circuits that can be homomorphically evaluated 1.08×–3.17× faster (with the geometric mean of 1.56×) than the state-of-the-art method. Our method is also orthogonal to existing domain-specific optimizations.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385497424",
    "type": "article"
  },
  {
    "title": "LoRe: A Programming Model for Verifiably Safe Local-First Software",
    "doi": "https://doi.org/10.1145/3633769",
    "publication_date": "2023-12-01",
    "publication_year": 2023,
    "authors": "Julian Haas; Ragnar Mogk; Elena Yanakieva; Annette Bieniusa; Mira Mezini",
    "corresponding_authors": "",
    "abstract": "Local-first software manages and processes private data locally while still enabling collaboration between multiple parties connected via partially unreliable networks. Such software typically involves interactions with users and the execution environment (the outside world). The unpredictability of such interactions paired with their decentralized nature make reasoning about the correctness of local-first software a challenging endeavor. Yet, existing solutions to develop local-first software do not provide support for automated safety guarantees and instead expect developers to reason about concurrent interactions in an environment with unreliable network conditions. We propose LoRe , a programming model and compiler that automatically verifies developer-supplied safety properties for local-first applications. LoRe combines the declarative data flow of reactive programming with static analysis and verification techniques to precisely determine concurrent interactions that violate safety invariants and to selectively employ strong consistency through coordination where required. We propose a formalized proof principle and demonstrate how to automate the process in a prototype implementation that outputs verified executable code. Our evaluation shows that LoRe simplifies the development of safe local-first software when compared to state-of-the-art approaches and that verification times are acceptable.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389245905",
    "type": "article"
  },
  {
    "title": "On the complexity of dataflow analysis of logic programs",
    "doi": "https://doi.org/10.1145/201059.201068",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Saumya Debray",
    "corresponding_authors": "Saumya Debray",
    "abstract": "It is widely held that there is a correlation between complexity and precision in dataflow analysis, in the sense that the more precise an analysis algorithm, the more computationally expensive it must be. The details of this relationship, however, appear to not have been explored extensively. This article reports some results on this correlation in the context of logic programs. A formal notion of the “precision” of an analysis algorithm is proposed, and this is used to characterize the worst-case computational complexity of a number of dataflow analyses with different degrees of precision. While this article considers the analysis of logic programs, the technique proposed, namely the use of “exactness sets” to study relationships between complexity and precision of analyses, is not specific to logic programming in any way, and is equally applicable to flow analyses of other language families.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1993089083",
    "type": "article"
  },
  {
    "title": "A linear-time scheme for version reconstruction",
    "doi": "https://doi.org/10.1145/177492.177705",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Lin Yu; Daniel J. Rosenkrantz",
    "corresponding_authors": "",
    "abstract": "An efficient scheme to store and reconstruct versions of sequential files is presented. The reconstruction scheme involves building a data structure representing a complete version, and then successively modifying this data structure by applying a sequence of specially formatted differential files to it. Each application of a differential file produces a representation of an intermediate version, with the final data structure representing the requested version. The scheme uses a linked list to represent an intermediate version, instead of a sequential array, as is used traditionally. A new format for differential files specifying changes to this linked list data structure is presented. The specification of each change points directly to where the change is to take place, thereby obviating a search. Algorithms are presented for using such a new format differential file to transform the representation of a version, and for reconstructing a requested version. Algorithms are also presented for generating the new format differential files, both for the case of a forward differential specifying how to transform the representation of an old version to the representation of a new version, and for the case of a reverse differential specifying how to transform the representation of a new version to the representation of an old version. The new version reconstruction scheme takes time linear in the sum of the size of the initial complete version and the sizes of the file differences involved in reconstructing the requested version. In contrast, the classical scheme for reconstructing versions takes time proportional to the sum of the sizes of the sequence of versions involved in the reconstruction, and therefore has a worst-case time that is quadratic in the sum of the size of the initial complete version and the sizes of the file differences. The time cost of the new differential file generation scheme is comparable to the time cost of the classical differential file generation scheme.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2023722412",
    "type": "article"
  },
  {
    "title": "On failure of the pruning technique in “Error repair in shift-reduce parsers”",
    "doi": "https://doi.org/10.1145/314602.314603",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Eberhard Bertsch; Mark-Jan Nederhof",
    "corresponding_authors": "",
    "abstract": "A previous article presented a technique to compute the least-cost error repair by incrementally generating configurations that result from inserting and deleting tokens a syntactically incorrect input. An additional mechanism to improve the run-time efficiency of this algorithm by pruning some of the configurations was discussed as well. In this communication we show that the pruning mechanism may lead to suboptimal repairs or may block all repairs. Certain grammatical errors in a common construct of the Java programming language also lead to the above kind of failure.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2030478693",
    "type": "article"
  },
  {
    "title": "Jump Minimization in Linear Time",
    "doi": "https://doi.org/10.1145/1780.357262",
    "publication_date": "1984-10-01",
    "publication_year": 1984,
    "authors": "Maya Ramanath; Marvin Solomon",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Jump Minimization in Linear Time Authors: M. V. S. Ramanath Computer Science Department, University of Western Ontario, London, Ontario, Canada N6A 5B6 Computer Science Department, University of Western Ontario, London, Ontario, Canada N6A 5B6View Profile , Marvin Solomon Computer Sciences Department, University of Wisconsin-Madison, 1210 West Dayton St., Madison, WI Computer Sciences Department, University of Wisconsin-Madison, 1210 West Dayton St., Madison, WIView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 4Oct. 1984 pp 527–545https://doi.org/10.1145/1780.357262Online:01 October 1984Publication History 2citation329DownloadsMetricsTotal Citations2Total Downloads329Last 12 Months8Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2093832962",
    "type": "article"
  },
  {
    "title": "Unassigned objects",
    "doi": "https://doi.org/10.1145/1780.1785",
    "publication_date": "1984-10-01",
    "publication_year": 1984,
    "authors": "Robert I. Winner",
    "corresponding_authors": "Robert I. Winner",
    "abstract": "article Free Access Share on Unassigned objects Author: Robert I. Winner Vanderbilt University Vanderbilt UniversityView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 4Oct. 1984 pp 449–467https://doi.org/10.1145/1780.1785Online:01 October 1984Publication History 3citation259DownloadsMetricsTotal Citations3Total Downloads259Last 12 Months16Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2294320284",
    "type": "article"
  },
  {
    "title": "Effective sign extension elimination for java",
    "doi": "https://doi.org/10.1145/1111596.1111599",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Motohiro Kawahito; Hideaki Komatsu; Toshio Nakatani",
    "corresponding_authors": "",
    "abstract": "Computer designs are shifting from 32-bit architectures to 64-bit architectures, while most of the programs available today are still designed for 32-bit architectures. Java, for example, specifies the frequently used “int” as a 32-bit signed integer. If such Java programs are executed on a 64-bit architecture, many 32-bit signed integers must be sign-extended to 64-bit signed integers for correct operations. This causes serious performance overhead. In this article, we present a fast and effective algorithm for eliminating sign extensions. We implemented this algorithm in the IBM Java Just-in-Time (JIT) compiler for IA-64. Our experimental results show that our algorithm effectively eliminates the majority of sign extensions. They also show that it improves performance by 6.9% for jBYTEmark and 3.3% for SPECjvm98 over the previously known best algorithm, while it increases JIT compilation time by only 0.11%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2023879442",
    "type": "article"
  },
  {
    "title": "Sublinear-space evaluation algorithms for attribute grammars",
    "doi": "https://doi.org/10.1145/24039.214529",
    "publication_date": "1987-07-01",
    "publication_year": 1987,
    "authors": "Thomas Reps; Alan Demers",
    "corresponding_authors": "",
    "abstract": "A major drawback of attribute-grammar-based systems is that they are profligate consumers of storage. This paper concerns new storage-management techniques that reduce the number of attribute values retained at any stage of attribute evaluation; it presents an algorithm for evaluating an n -attribute tree that never retains more than O (log n ) attribute values. This method is optimal, although it may require nonlinear time. A second algorithm, which never retains more than O (√ n ) attribute values, is also presented, both as an introduction to the O (log n ) method and because it works in linear time.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1984964548",
    "type": "article"
  },
  {
    "title": "Deducing fairness properties in UNITY logic—a new completeness result",
    "doi": "https://doi.org/10.1145/200994.200997",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Yih-Kuen Tsay; Rajive Bagrodia",
    "corresponding_authors": "",
    "abstract": "We explore the use of UNITY logic in specifying and verifying fairness properties of UNITY and UNITY-like programs whose semantics can be modeled by weakly fair transition systems. For such programs, strong fairness properties in the form of “if p holds infinitely often then q also holds infinitely often □◊p⇒□◊q, can be expressed as conditional UNITY properties of the form of “Hypothesis: true→p Conclusion:true→q”. We show that UNITY logic is relatively complete for proving such properties; in the process, a simple inference rule is derived. Specification and verification of weak fairness properties are also discussed.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2007051783",
    "type": "article"
  },
  {
    "title": "Efficient high-level iteration with accumulators",
    "doi": "https://doi.org/10.1145/63264.63401",
    "publication_date": "1989-04-01",
    "publication_year": 1989,
    "authors": "Robert D. Cameron",
    "corresponding_authors": "Robert D. Cameron",
    "abstract": "Accumulators are proposed as a new type of high-level iteration construct for imperative languages. Accumulators are user-programmed mechanisms for successively combining a sequence of values into a single result value. The accumulated result can either be a simple numeric value such as the sum of a series or a data structure such as a list. Accumulators naturally complement constructs that allow iteration through user-programmed sequences of values such as the iterators of CLU and the generators of Alphard. A practical design for high-level iteration is illustrated by way of an extension to Modula-2 called Modula Plus. The extension incorporates both a redesigned mechanism for iterators as well as the accumulator design. Several applications are illustrated including both numeric and data structure accumulation. It is shown that the design supports efficient iteration both because it is amenable to implementation via in-line coding and because it allows high-level iteration concepts to be implemented as encapsulations of efficient low-level manipulations.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2029421624",
    "type": "article"
  },
  {
    "title": "Retargetable microcode synthesis",
    "doi": "https://doi.org/10.1145/22719.23717",
    "publication_date": "1987-03-20",
    "publication_year": 1987,
    "authors": "Robert A. Mueller; Joseph Varghese",
    "corresponding_authors": "",
    "abstract": "Most work on automating the translation of high-level microprogramming languages into microcode has dealt with lexical and syntactic analysis and the use of manually produced macro tables for code generation. We describe an approach to and some results on the formalization and automation of the more difficult problem of retargeting local code generation in a machine-independent, optimizing microcode synthesis system. Whereas this problem is similar in many ways to that of retargeting local code generation in high-level language compilers, there are some major differences that call for new approaches. The primary issues addressed in this paper are the representation of target microprogrammable machines, the intermediate representation of local microprogram function, and general algorithmic methods for deriving local microcode from target machine and microcode function specifications. Of particular interest are the use of formal semantics and data flow principles in achieving both a general and reasonably efficient solution. Examples of the modeling of a representative horizontal machine (the PUMA) and the generation of microcode for the PUMA machine model from our working implementation are presented.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2044648928",
    "type": "article"
  },
  {
    "title": "A Hierarchical Approach to Formal Semantics With Application to the Definition of PL/ CS",
    "doi": "https://doi.org/10.1145/357062.357069",
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "Robert L. Constable; James E. Donahue",
    "corresponding_authors": "",
    "abstract": "We describe a means of presenting hierarchically organized formal definitions of programming languages using the denotational approach of D. Scott and C. Strachey. As an example of our approach, we give the semantics of PL/CS, an instructional variant of PL/I. We also discuss the implications of this approach to language design, pointing out some cases where the wrong choices may cause the hierarchy to collapse into chaotic rubble.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2039035012",
    "type": "article"
  },
  {
    "title": "The Compilation of Loop Induction Expressions",
    "doi": "https://doi.org/10.1145/357062.357065",
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "Richard L. Sites",
    "corresponding_authors": "Richard L. Sites",
    "abstract": "In an optimizing compiler, it is often desirable to compile subscript expressions such as Abase-2 + I*2 so that the value of the expression is available in a register and is simply incremented whenever I is incremented, thus avoiding the multiplication inside the loop. This change is effected by a standard optimization called strength reduction. Program loops often contain several such expressions stemming perhaps from references to operands A(I), A(I+1), B(I), and C(K,J,I,L). Under what circumstances can we do better than keeping four addresses in four separate registers or temporaries? A general technique is presented which minimizes the number of registers needed to hold such values, while simultaneously minimizing the amount of computation inside the loop. In the above collection, it is possible to use as few as two registers for the four I-dependent values.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2056096949",
    "type": "article"
  },
  {
    "title": "A Space Efficient Dynamic Allocation Algorithm for Queuing Messages",
    "doi": "https://doi.org/10.1145/357073.357082",
    "publication_date": "1979-10-01",
    "publication_year": 1979,
    "authors": "E. P. Beyer; Peter Buneman",
    "corresponding_authors": "",
    "abstract": "A simple dynamic allocation algorithm is described for queuing variable length messages in memory. The algorithm makes use of the ability of many operating system to increase and decrease available memory as required. Some results describing its efficiency are presented.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2104229441",
    "type": "article"
  },
  {
    "title": "On exponential-time completeness of the circularity problem for attribute grammars",
    "doi": "https://doi.org/10.1145/963778.963783",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Pei‐Chi Wu",
    "corresponding_authors": "Pei‐Chi Wu",
    "abstract": "Attribute grammars (AGs) are a formal technique for defining semantics of programming languages. Existing complexity proofs on the circularity problem of AGs are based on automata theory, such as writing pushdown acceptor and alternating Turing machines. They reduced the acceptance problems of above automata, which are exponential-time (EXPTIME) complete, to the AG circularity problem. These proofs thus show that the circularity problem is EXPTIME- hard , at least as hard as the most difficult problems in EXPTIME. However, none has shown that the problem is EXPTIME-complete. This paper presents an alternating Turing machine for the circularity problem. The alternating Turing machine requires polynomial space. Thus, the circularity problem is in EXPTIME and is then EXPTIME-complete.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2090873650",
    "type": "article"
  },
  {
    "title": "An optimization framework for embedded processors with auto-addressing mode",
    "doi": "https://doi.org/10.1145/1734206.1734208",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Xiaotong Zhuang; Santosh Pande",
    "corresponding_authors": "",
    "abstract": "Modern embedded processors with dedicated address generation unit support memory accesses through auto-increment/decrement addressing mode. The auto-increment/decrement mode, if properly utilized, can save address arithmetic instructions, reduce static and dynamic memory footprint of the program, and speed up the execution as well. Liao [1995, 1996] categorized this problem as Simple Offset Assignment (SOA) and General Offset Assignment (GOA), which involves storage layout of variables and assignment of address registers, respectively, proposing several heuristic solutions. This article proposes a new direction for investigating the solution space of the problem. The general idea [Zhuang 2003] is to perform simplification of the underlying access graph through coalescence of the memory locations of program variables. A comprehensive framework is proposed including coalescence-based offset assignment and post/pre-optimization. Variables not interfering with others (not simultaneously live at any program point) can be coalesced into the same memory location. Coalescing allows simplifications of the access graph yielding better SOA solutions; it also reduces the address register pressure to such low values that some GOA solutions become optimal. Moreover, it can reduce the memory footprint both statically and at runtime for stack variables. Our second optimization (post/pre-optimization) considers both post- and pre-modification mode for optimizing code across basic blocks, which makes it useful. Making use of both addressing modes further reduces SOA/GOA cost and our post/pre-optimization phase is optimal in selecting post or pre mode after variable offsets have been determined. We have shown the advantages of our framework over previous approaches to capture more opportunities to reduce both stack size and SOA/GOA cost, leading to more speedup.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2049373920",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1749608",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present a type system for linear constraints over the reals intended for reasoning about the input-output directionality of variables. Types model the properties of definiteness, range width or approximation, lower and upper bounds of variables in a ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4253559753",
    "type": "paratext"
  },
  {
    "title": "Determinacy testing for nondeterminate logic programming languages",
    "doi": "https://doi.org/10.1145/174625.174626",
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Evan Tick; Mark Korsloot",
    "corresponding_authors": "",
    "abstract": "This paper describes an algorithm for the code generation of determinacy testing for nondeterminate flat concurrent logic programming languages. Languages such as Andorra and Pandora require that procedure invocations suspend if there is more than one candidate clause potentially satisfying the goal. The algorithm described has been developed specifically for a variant of flat Pandora based on FGHC, although the concepts are general. We have extended Kliger and Shapiro's decision-graph construction algorithm to compile “don't-know” procedures that must suspend for nondeterminate goal invocation. The determinacy test is compiled into a decision graph quite different from those of committed-choice procedures: They are more similar to decision trees optimized by code sharing. We present both empirical data of compilation results (code size and graph characteristics), and a correctness proof for our code-generation algorithm.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1968639270",
    "type": "article"
  },
  {
    "title": "Technical Correspondence: On Apt, Francez, and de Roever's ``A Proof System for Communicating Sequential Processes''",
    "doi": "https://doi.org/10.1145/2166.357222",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Abha Moitra",
    "corresponding_authors": "Abha Moitra",
    "abstract": "article Free AccessTechnical Correspondence: On Apt, Francez, and de Roever's ``A Proof System for Communicating Sequential Processes'' Author: Abha Moitra National Centre for Software Development and Computing Techniques, Tata Institute of Fundamental Research, Homi Bhabha Road, Bombay 400 005, India National Centre for Software Development and Computing Techniques, Tata Institute of Fundamental Research, Homi Bhabha Road, Bombay 400 005, IndiaView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3July 1983 pp 500–501https://doi.org/10.1145/2166.357222Published:01 July 1983Publication History 2citation197DownloadsMetricsTotal Citations2Total Downloads197Last 12 Months5Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2043531933",
    "type": "article"
  },
  {
    "title": "Completeness and Complexity of Reasoning about Call-by-Value in Hoare Logic",
    "doi": "https://doi.org/10.1145/3477143",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Frank S. de Boer; Hans-Dieter A. Hiep",
    "corresponding_authors": "",
    "abstract": "We provide a sound and relatively complete Hoare logic for reasoning about partial correctness of recursive procedures in presence of local variables and the call-by-value parameter mechanism and in which the correctness proofs support contracts and are linear in the length of the program. We argue that in spite of the fact that Hoare logics for recursive procedures were intensively studied, no such logic has been proposed in the literature.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3210461265",
    "type": "article"
  },
  {
    "title": "On Time-sensitive Control Dependencies",
    "doi": "https://doi.org/10.1145/3486003",
    "publication_date": "2021-12-09",
    "publication_year": 2021,
    "authors": "Martin Hecker; Simon Bischof; Gregor Snelting",
    "corresponding_authors": "",
    "abstract": "We present efficient algorithms for time-sensitive control dependencies (CDs). If statement y is time-sensitively control dependent on statement x , then x decides not only whether y is executed but also how many timesteps after x . If y is not standard control dependent on x , but time-sensitively control dependent, then y will always be executed after x , but the execution time between x and y varies. This allows us to discover, e.g., timing leaks in security-critical software. We systematically develop properties and algorithms for time-sensitive CDs, as well as for nontermination-sensitive CDs. These work not only for standard control flow graphs (CFGs) but also for CFGs lacking a unique exit node (e.g., reactive systems). We show that Cytron’s efficient algorithm for dominance frontiers [ 10 ] can be generalized to allow efficient computation not just of classical CDs but also of time-sensitive and nontermination-sensitive CDs. We then use time-sensitive CDs and time-sensitive slicing to discover cache timing leaks in an AES implementation. Performance measurements demonstrate scalability of the approach.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4200296941",
    "type": "article"
  },
  {
    "title": "Allocating architected registers through differential encoding",
    "doi": "https://doi.org/10.1145/1216374.1216377",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Xiaotong Zhuang; Santosh Pande",
    "corresponding_authors": "",
    "abstract": "Micro-architecture designers are very cautious about expanding the number of architected and exposed registers in the instruction set because increasing the register field adds to the code size, raises the I-cache and memory pressure, and may complicate the processor pipeline. Especially for low-end processors, encoding space could be extremely limited due to area and power considerations. On the other hand, the number of architected registers exposed to the compiler could directly affect the effectiveness of compiler analysis and optimization. For high-performance computers, register pressure can be higher than the available registers in some regions. This could be due to optimizations like aggressive function inlining, software pipelining, etc. The compiler cannot effectively perform compilation and optimization if only a small number of registers are exposed through the ISA. Therefore, it is crucial that more architected registers are available at the compiler's disposal, without expanding the code size significantly. In this article, we devise a new register encoding scheme, called differential encoding, that allows more registers to be addressed in the operand field of instructions than the direct encoding currently being used. We show that this can be implemented with very low overhead. Based upon differential encoding, we apply it in several ways such that the extra architected registers can benefit the performance. Three schemes are devised to integrate differential encoding with register allocation. We demonstrate that differential register allocation is helpful in improving the performance of both high-end and low-end processors. Moreover, we can combine it with software pipelining to provide more registers and reduce spills. Our results show that differential encoding significantly reduces the number of spills and speeds-up program execution. For a low-end configuration, we achieve over 14% speedup while keeping code size almost unaffected. For a high-end VLIW in-order machine, it can significantly speed-up loops with high register pressure (about 80% speedup) and the overall speedup is about 15%. Moreover, our scheme can be applied in an adaptive manner, making its overhead much smaller.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1979318138",
    "type": "article"
  },
  {
    "title": "Comments on Georgeff's “transformations and reduction strategies for typed lambda expressions”",
    "doi": "https://doi.org/10.1145/5956.215007",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Flemming Nielson; Hanne Rus Nielson",
    "corresponding_authors": "",
    "abstract": "In his recent paper, Georgeff [1] considers the evaluation of lambda expres sions (with reducing reflexive types) on a variant of Landin's SECD machine. It is observed that for so-called simple expressions the SECD machine will construct stack-like environment structures, whereas, in general, they are tree-like. Georgeff then explores the idea of transforming any (typed) lambda expression into simple form and then using the SECD machine for these expressions. However, such transformations might be undesirable if the language is going to be interpreted directly. This leads Georgeff to modify the SECD machine such that it constructs stack-like environments for all expressions. Briefly, the idea in this modification is to let the machine construct over- applied closures for function-valued expressions in operand positions and to let it apply the closure in situ in the case of function-valued expressions in operator positions. The construction of overapplied closures implies that tree- like environment structures are avoided. Georgeff claims that by applying expressions in operator positions in situ it is avoided that operand values are “needlessly popped from the stack during creation of the overapplied closure, only to be reinstated on entry to the closure” [ 1, p. 620]. However, the example of Figure 1 shows that Georgeff's construction does not overcome this problem: the transitions 14, 15, and 16 move the on constant 3 from the stack to the closure and back to the stack. Intuitively, the reason is that the construction does not recognize correctly whether a sub-expression in operator position is in fact “basic-valued.” An expression (or, more precisely, its closure) occurring in operator position will look for its remaining arguments on the current stack and on the stacks of the dump. The prediction of the number of available arguments is important in order to determine whether the expression is “basic-valued,” and should therefore be treated specially. Georgeff suggests the following function for counting the number of avail- able arguments on the stacks: def totapps (ST) let (S, E, C, D) = ST let n = (napps C) if n = 0 then n else (+ n (totapps D)) end where the function (napps C) returns the number of consecutive apply operators at the beginning of the control C. For the configuration of line 14 of Figure 1 this gives totapps (. . .) = 2 napps (AάA) = 1 and totapps (d1) = 1 — this means that the closure [(βγ), u, (2), e1] can find 2 arguments on the stacks although its functionality is 1! Hence, the predicate, basic-valued, fails, although it ought to succeed. We suggest replacing the function above with def totapps (ST) let (S, E, C, D) = ST let n = (napps C) if length C ≠ n then,/B&gt; n else (+ n (totapps D)) end where length C is the length of the list C. The intuition behind this suggestion is that, if the control C of some configuration is not just a list of apply symbols, then the subexpression at hand will be turned into operand position before a basic value can be returned. Returning to the example mentioned above, note that we now get totapps (...) = 1 because napps (AάA) = 1 and length (AάA) ≠ 1, and the needless movements on the stack are avoided.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1978940055",
    "type": "article"
  },
  {
    "title": "Search direction by goal failure in goal-oriented programming",
    "doi": "https://doi.org/10.1145/78942.78946",
    "publication_date": "1990-04-01",
    "publication_year": 1990,
    "authors": "Jared L. Darlington",
    "corresponding_authors": "Jared L. Darlington",
    "abstract": "A new approach to goal-oriented programming is described, whereby the search for values of variables to satisfy a goal is invariably directed by that goal or by information provided by its failure. This goal-directed approach is in contrast to that employed by logic programming systems, which attempt to satisfy a goal that has failed by resatisfying an already tested goal, and which furthermore do this in a way determined solely by the order of facts and rules in the database and without reference to the goal that has failed. Proposed changes in the control structure of logic programs designed to improve their execution serve more to reduce the search space than to add goal direction. A goal-directed language that embodies the new approach is presented. It is at the same time a functional programming language and a specification interpreter for the direct execution and testing of functional specifications, and permits the user to write executable program descriptions in which some of the constituent functions are fully defined while others are “merely” specified. The language has been successfully tested on examples drawn from such fields as deductive question answering and problem solving, where it compares favorably with the logic programming languages.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1988866903",
    "type": "article"
  },
  {
    "title": "Encapsulation constructs in systems programming languages",
    "doi": "https://doi.org/10.1145/2993.69615",
    "publication_date": "1984-04-01",
    "publication_year": 1984,
    "authors": "William F. Appelbe; Anders P. Ravn",
    "corresponding_authors": "",
    "abstract": "article Free AccessEncapsulation constructs in systems programming languages Authors: W. F. Appelbe University of California at San Diego University of California at San DiegoView Profile , A. P. Ravn Copenhagen University, Denmark Copenhagen University, DenmarkView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 2April 1984 pp 129–158https://doi.org/10.1145/2993.69615Published:01 April 1984Publication History 2citation630DownloadsMetricsTotal Citations2Total Downloads630Last 12 Months63Last 6 weeks14 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1989143099",
    "type": "article"
  },
  {
    "title": "Simple, efficient, asynchronous parallel algorithms for maximization",
    "doi": "https://doi.org/10.1145/42190.42278",
    "publication_date": "1988-04-01",
    "publication_year": 1988,
    "authors": "Albert Greenberg; Boris D. Lubachevsky; Andrew Odlyzko",
    "corresponding_authors": "",
    "abstract": "The problem of computing the maximum of n inputs on an asynchronous parallel computer is considered. In general, the inputs may arrive staggered in time, the number of processors available to the maximization algorithm may vary during its execution, and the number of inputs, n , may be initially unknown. Two simple, efficient algorithms to compute the maximum are presented. Each algorithm may be invoked asynchronously, as new inputs and processors arrive. Performance measures that account for the response times of the invocations are introduced, and the algorithms are analyzed under these measures.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2053010999",
    "type": "article"
  },
  {
    "title": "(De/Re)-Composition of Data-Parallel Computations via Multi-Dimensional Homomorphisms",
    "doi": "https://doi.org/10.1145/3665643",
    "publication_date": "2024-05-22",
    "publication_year": 2024,
    "authors": "Ari Rasch",
    "corresponding_authors": "Ari Rasch",
    "abstract": "Data-parallel computations, such as linear algebra routines and stencil computations, constitute one of the most relevant classes in parallel computing, e.g., due to their importance for deep learning. Efficiently de-composing such computations for the memory and core hierarchies of modern architectures and re-composing the computed intermediate results back to the final result—we say (de/re)-composition for short—is key to achieve high performance for these computations on, e.g., GPU and CPU. Current high-level approaches to generating data-parallel code are often restricted to a particular subclass of data-parallel computations and architectures (e.g., only linear algebra routines on only GPU or only stencil computations), and/or the approaches rely on a user-guided optimization process for a well-performing (de/re)-composition of computations, which is complex and error prone for the user. We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of Multi-Dimensional Homomorphisms (MDHs). Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking, and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc.), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world datasets and for a variety of data-parallel computations, including linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4398197904",
    "type": "article"
  },
  {
    "title": "Universal Composability is Robust Compilation",
    "doi": "https://doi.org/10.1145/3698234",
    "publication_date": "2024-10-10",
    "publication_year": 2024,
    "authors": "Marco Patrignani; Robert Künnemann; Riad S. Wahby; Ethan Cecchetti",
    "corresponding_authors": "",
    "abstract": "This paper discusses the relationship between two frameworks: universal composability (UC) and robust compilation ( RC ). In cryptography, UC is a framework for the specification and analysis of cryptographic protocols with a strong compositionality guarantee: UC protocols remain secure even when composed with other protocols. In programming language security, RC is a novel framework for determining secure compilation by proving whether compiled programs are as secure as their source-level counterparts no matter what target-level code they interact with. Presently, these disciplines are studied in isolation, though we argue that there is a deep connection between them and exploring this connection will benefit both research fields. This paper formally proves the connection between UC and RC and then it explores the benefits of this connection (focussing on perfect, rather than computational UC). For this, this paper first identifies which conditions must programming languages fulfil in order to possibly attain UC-like composition. Then, it proves UC of both an existing and a new commitment protocol as a corollary of the related compilers attaining RC . Finally, it mechanises these proofs in Deepsec, obtaining symbolic guarantees that the protocol is indeed UC. Our connection lays the groundwork towards a better and deeper understanding of both UC and RC , and the benefits we showcase from this connection provide evidence of scalable mechanised proofs for UC. In order to differentiate various sub-parts of the UC and RC frameworks, we use syntax highlighting to a degree that colourblind and black&amp;white readers can benefit from [78]. UC talks about ideal functionalities (typeset in a verbatim, emerald font) and protocol implementations (typeset in a sans-serif, orange font). RC deals with source languages (typeset in an italics, blue font) and target ones (typeset in a bold, red font). Elements that are common to each framework are typeset in a black, sans-serif font for UC and in a black, italicised font for RC to avoid repetition. For a better experience, please view this paper in colour.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403298574",
    "type": "article"
  },
  {
    "title": "TeraHeap: Exploiting Flash Storage for Mitigating DRAM Pressure in Managed Big Data Frameworks",
    "doi": "https://doi.org/10.1145/3700593",
    "publication_date": "2024-10-15",
    "publication_year": 2024,
    "authors": "Iacovos G. Kolokasis; Giannos Evdorou; Shoaib Akram; Christos Kozanitis; Anastasios Papagiannis; Foivos S. Zakkak; Polyvios Pratikakis; Angelos Bilas",
    "corresponding_authors": "",
    "abstract": "Big data analytics frameworks, such as Spark and Giraph, need to process and cache massive datasets that do not always fit on the managed heap. Therefore, frameworks temporarily move long-lived objects outside the heap (off-heap) on a fast storage device. However, this practice results in (1) high serialization/deserialization (S/D) cost and (2) high memory pressure when off-heap objects are moved back for processing. In this article, we propose TeraHeap , a system that eliminates S/D overhead and expensive GC scans for a large portion of objects in analytics frameworks. TeraHeap relies on three concepts: (1) It eliminates S/D by extending the managed runtime (JVM) to use a second high-capacity heap (H2) over a fast storage device. (2) It offers a simple hint-based interface, allowing analytics frameworks to leverage object knowledge to populate H2. (3) It reduces GC cost by fencing the collector from scanning H2 objects while maintaining the illusion of a single managed heap, ensuring memory safety. We implement TeraHeap in OpenJDK8 and OpenJDK17 and evaluate it with fifteen widely used applications in two real-world big data frameworks, Spark and Giraph. We find that for the same DRAM size, TeraHeap improves performance by up to 73% and 28% compared to native Spark and Giraph. Also, it can still provide better performance by consuming up to \\(4.6\\times\\) and \\(1.2\\times\\) less DRAM than native Spark and Giraph, respectively. TeraHeap can also be used for in-memory frameworks and applying it to the Neo4j Graph Data Science library improves its performance by up to 26%. Finally, it outperforms Panthera, a state-of-the-art garbage collector for hybrid DRAM-NVM memories, by up to 69%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403428009",
    "type": "article"
  },
  {
    "title": "A Layered Approach to Intensional Analysis in Type Theory",
    "doi": "https://doi.org/10.1145/3707203",
    "publication_date": "2024-12-05",
    "publication_year": 2024,
    "authors": "Jason Z. S. Hu; Brigitte Pientka",
    "corresponding_authors": "",
    "abstract": "We introduce layering to modal type theory to support meta-programming and intensional analysis coherently. In particular, we demonstrate this idea by developing a 2-layered modal type theory. At the core of this type theory (layer 0) is a simply typed λ -calculus with no modality. Layer 1 is obtained by extending the core language with one layer of contextual ◻ types to support pattern matching on potentially open code from layer 0 while retaining normalization. Although both layers fundamentally share a uniform syntax and the same typing judgment, we only allow computation at layer 1. As a consequence, layer 0 accurately captures the syntactic representation of code in contrast to the computational behaviors at layer 1. Moreover, the uniform syntax at both layers enables quotation and code running. The system is justified by normalization by evaluation (NbE) using a presheaf model. The normalization algorithm extracted from the model is sound and complete and is implemented in Agda. Layered modal type theory provides a uniform foundation for meta-programming with intensional analysis. We see this work as an important step towards a foundational way to support meta-programming in proof assistants.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405080277",
    "type": "article"
  },
  {
    "title": "Technical correspondence",
    "doi": "https://doi.org/10.1145/151646.151652",
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "Alan Finlay; Lloyd Allison",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Technical correspondence: a correction to the denotational semantics for the Prolog of Nicholson and Foo Authors: Alan Finlay Monash Univ. Monash Univ.View Profile , Lloyd Allison Monash Univ. Monash Univ.View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 15Issue 1Jan. 1993 pp 206–208https://doi.org/10.1145/151646.151652Published:01 January 1993Publication History 0citation179DownloadsMetricsTotal Citations0Total Downloads179Last 12 Months5Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1979616517",
    "type": "article"
  },
  {
    "title": "Notes on “A methodology for implementing highly concurrent data objects”",
    "doi": "https://doi.org/10.1145/200994.200999",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Joseph P. Skudlarek",
    "corresponding_authors": "Joseph P. Skudlarek",
    "abstract": "No abstract available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1994843813",
    "type": "article"
  },
  {
    "title": "Reply to “Type-extension tests can be performed in constant time”",
    "doi": "https://doi.org/10.1145/115372.214521",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Niklaus Wirth",
    "corresponding_authors": "Niklaus Wirth",
    "abstract": "No abstract available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2014796431",
    "type": "article"
  },
  {
    "title": "Safe",
    "doi": "https://doi.org/10.1145/200994.201002",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Alex Aiken; John H. Williams; Edward L. Wimmers",
    "corresponding_authors": "",
    "abstract": "Language designers and implementors have avoided specifying and preserving the meaning of programs that produce errors. This is apparently because being forced to preserve error behavior limits severely the scope of program optimization, even for correct programs. However, error behavior preservation is desirable for debugging, and error behavior must be preserved in any language that permits user-generated errors (i.e., exceptions). This article presents a technique for expressing general program transformations for languages that possess a rich collection of distinguishable error values. This is accomplished by defining a higher-order function called Safe , which can be used to annotate those portions of a program that are guaranteed not to produce errors. It is shown that this facilitates the expression of very general program transformations, effectively giving program transformations in a language with many error values the same power and generality as program transformations in a language with only a single error value. Using the semantic properties of Safe , it is possible to provide some useful sufficient conditions for establishing the correctness of transformations in the presence of errors. In particular, a Substitutability theorem is proven, which can be used to justify “in-context” optimizations: transformations that alter the meanings of subexpressions without changing the meaning of the whole program. Finally, the effectiveness of the technique is demonstrated by some examples of its use in an optimizing compiler.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2035959718",
    "type": "article"
  },
  {
    "title": "Two issues in parallel language design",
    "doi": "https://doi.org/10.1145/197320.197325",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Alexander Böhm; R. R. Oldehoeft",
    "corresponding_authors": "",
    "abstract": "In this article, we discuss two programming language features that have value for expressibility and efficiency: nonstrictness and nondeterminism. Our work arose while assessing ways to enhance a currently successful language, SISAL [McGraw et al. 1985]. The questions of how best to include these features, if at all, has led not to conclusions but to an impetus to explore the answers in an objective way. We will retain strictness for efficiency reasons and explore the limits it may impose, and we will experiment with a carefully controlled form of nondeterminism to assess its expressive power.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2043934218",
    "type": "article"
  },
  {
    "title": "A Coroutine Approach to Parsing",
    "doi": "https://doi.org/10.1145/357103.357106",
    "publication_date": "1980-07-01",
    "publication_year": 1980,
    "authors": "Hanan Samet",
    "corresponding_authors": "Hanan Samet",
    "abstract": "A method is presented for parsing syntactic constructs that are permitted to appear independently anywhere in a program. Some examples include comments, macros, and constructs for conditional compilation. Each such construct is defined by its own grammar and parsed by a separate coroutine. The coroutine model of parsing allows the program text to be parsed in one pass despite the syntactic inconsistencies among the program text and the additional constructs. The usefulness of the model is demonstrated by showing how a production language parsing method is extended to handle multiple syntax specifications. The implementation of conditional compilation by carrying along two parses in a coroutine manner is also given. The utility of the model is further demonstrated by showing its adaptation to a recursive descent parser.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2110473566",
    "type": "article"
  },
  {
    "title": "An Effective Fusion and Tile Size Model for PolyMage",
    "doi": "https://doi.org/10.1145/3404846",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Abhinav Jangda; Uday Bondhugula",
    "corresponding_authors": "",
    "abstract": "Effective models for fusion of loop nests continue to remain a challenge in both general-purpose and domain-specific language (DSL) compilers. The difficulty often arises from the combinatorial explosion of grouping choices and their interaction with parallelism and locality. This article presents a new fusion algorithm for high-performance domain-specific compilers for image processing pipelines. The fusion algorithm is driven by dynamic programming and explores spaces of fusion possibilities not covered by previous approaches, and it is also driven by a cost function more concrete and precise in capturing optimization criteria than prior approaches. The fusion model is particularly tailored to the transformation and optimization sequence applied by PolyMage and Halide, two recent DSLs for image processing pipelines. Our model-driven technique when implemented in PolyMage provides significant improvements (up to 4.32×) over PolyMage’s approach (which uses auto-tuning to aid its model) and over Halide’s automatic approach (by up to 2.46×) on two state-of-the-art shared-memory multicore architectures.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3102973548",
    "type": "article"
  },
  {
    "title": "Interprocedural Context-Unbounded Program Analysis Using Observation Sequences",
    "doi": "https://doi.org/10.1145/3418583",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Peizun Liu; Thomas Wahl; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "A classical result by Ramalingam about synchronization-sensitive interprocedural program analysis implies that reachability for concurrent threads running recursive procedures is undecidable. A technique proposed by Qadeer and Rehof, to bound the number of context switches allowed between the threads, leads to an incomplete solution that is, however, believed to catch “most bugs” in practice, as errors tend to occur within few contexts. The question of whether the technique can also prove the absence of bugs at least in some cases has remained largely open. Toward closing this gap, we introduce in this article the generic verification paradigm of observation sequences for resource-parameterized programs. Such a sequence observes how increasing the resource parameter affects the reachability of states satisfying a given property. The goal is to show that increases beyond some “cutoff” parameter value have no impact on the reachability—the sequence has converged . This allows us to conclude that the property holds for all parameter values. We applied this paradigm to the context- unbounded program analysis problem, choosing the resource to be the number of permitted thread context switches. The result is a partially correct interprocedural reachability analysis technique for concurrent shared-memory programs. Our technique may not terminate but is able to both refute and prove context-unbounded safety for such programs. We demonstrate the effectiveness and efficiency of the technique using a variety of benchmark programs. The safe instances cannot be proved safe by earlier, context-bounded methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3112178287",
    "type": "article"
  },
  {
    "title": "Bounded Abstract Effects",
    "doi": "https://doi.org/10.1145/3492427",
    "publication_date": "2022-01-12",
    "publication_year": 2022,
    "authors": "William Melicher; Anlun Xu; Valerie Zhao; Alex Potanin; Jonathan Aldrich",
    "corresponding_authors": "",
    "abstract": "Effect systems have been a subject of active research for nearly four decades, with the most notable practical example being checked exceptions in programming languages such as Java. While many exception systems support abstraction, aggregation, and hierarchy (e.g., via class declaration and subclassing mechanisms), it is rare to see such expressive power in more generic effect systems. We designed an effect system around the idea of protecting system resources and incorporated our effect system into the Wyvern programming language. Similar to type members, a Wyvern object can have effect members that can abstract lower-level effects, allow for aggregation, and have both lower and upper bounds, providing for a granular effect hierarchy. We argue that Wyvern’s effects capture the right balance of expressiveness and power from the programming language design perspective. We present a full formalization of our effect-system design, showing that it allows reasoning about authority and attenuation. Our approach is evaluated through a security-related case study.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4206781617",
    "type": "article"
  },
  {
    "title": "Strong-separation Logic",
    "doi": "https://doi.org/10.1145/3498847",
    "publication_date": "2022-02-23",
    "publication_year": 2022,
    "authors": "Jens Katelaan; Florian Zuleger",
    "corresponding_authors": "",
    "abstract": "Most automated verifiers for separation logic are based on the symbolic-heap fragment, which disallows both the magic-wand operator and the application of classical Boolean operators to spatial formulas. This is not surprising, as support for the magic wand quickly leads to undecidability, especially when combined with inductive predicates for reasoning about data structures. To circumvent these undecidability results, we propose assigning a more restrictive semantics to the separating conjunction. We argue that the resulting logic, strong-separation logic, can be used for symbolic execution and abductive reasoning just like “standard” separation logic, while remaining decidable even in the presence of both the magic wand and inductive predicates (we consider a list-segment predicate and a tree predicate)—a combination of features that leads to undecidability for the standard semantics.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4213454333",
    "type": "article"
  },
  {
    "title": "Runtime Complexity Bounds Using Squeezers",
    "doi": "https://doi.org/10.1145/3527632",
    "publication_date": "2022-07-15",
    "publication_year": 2022,
    "authors": "Oren Ish-Shalom; Shachar Itzhaky; Noam Rinetzky; Sharon Shoham",
    "corresponding_authors": "",
    "abstract": "Determining upper bounds on the time complexity of a program is a fundamental problem with a variety of applications, such as performance debugging, resource certification, and compile-time optimizations. Automated techniques for cost analysis excel at bounding the resource complexity of programs that use integer values and linear arithmetic. Unfortunately, they fall short when the complexity depends more intricately on the evolution of data during execution. In such cases, state-of-the-art analyzers have shown to produce loose bounds, or even no bound at all. We propose a novel technique that generalizes the common notion of recurrence relations based on ranking functions. Existing methods usually unfold one loop iteration and examine the resulting arithmetic relations between variables. These relations assist in establishing a recurrence that bounds the number of loop iterations. We propose a different approach, where we derive recurrences by comparing whole traces with whole traces of a lower rank, avoiding the need to analyze the complexity of intermediate states. We offer a set of global properties, defined with respect to whole traces, that facilitate such a comparison and show that these properties can be checked efficiently using a handful of local conditions. To this end, we adapt state squeezers , an induction mechanism previously used for verifying safety properties. We demonstrate that this technique encompasses the reasoning power of bounded unfolding, and more. We present some seemingly innocuous, yet intricate, examples that previous tools based on cost relations and control flow analysis fail to solve, and that our squeezer-powered approach succeeds.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4285489730",
    "type": "article"
  },
  {
    "title": "Two Parametricities Versus Three Universal Types",
    "doi": "https://doi.org/10.1145/3539657",
    "publication_date": "2022-09-21",
    "publication_year": 2022,
    "authors": "Dominique Devriese; Marco Patrignani; Frank Piessens",
    "corresponding_authors": "",
    "abstract": "The formal calculus System F models the essence of polymorphism and abstract data types, features that exist in many programming languages. The calculus’ core property is parametricity: a theorem expressing the language’s abstractions and validating important principles like information hiding and modularity. When System F is combined with features like recursive types, mutable state, continuations or exceptions, the formulation of parametricity needs to be adapted to follow suit, for example using techniques like step-indexing, Kripke world-indexing or biorthogonality. However, it is less clear how this formulation should change when System F is combined with untyped languages, gradual types, dynamic sealing and runtime type analysis (typecase) alongside type generation. Extensions of System F with these features have been proven to satisfy forms of parametricity (with Kripke worlds carrying semantic interpretations of types). However, the relative power of the modified formulations of parametricity with respect to others and the relative expressiveness of System F with and without these extensions are unknown. In this paper, we explain that the aforementioned different settings have a common characteristic: they do not enforce or preserve the lexical scope of System F’s type variables. Formally, this results in the existence of a universal type (note: this is not the same as a universally-quantified type). We explain why standard parametricity is incompatible with such a type and how type worlds resolve this. Building on these insights, we answer two open conjectures from the literature, negatively, and we point out a deficiency in current proposals for combining System F with gradual types.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4296709663",
    "type": "article"
  },
  {
    "title": "The Tortoise and the Hare Algorithm for Finite Lists, Compositionally",
    "doi": "https://doi.org/10.1145/3564619",
    "publication_date": "2022-09-26",
    "publication_year": 2022,
    "authors": "Olivier Danvy",
    "corresponding_authors": "Olivier Danvy",
    "abstract": "In the tortoise-and-hare algorithm, when the fast pointer reaches the end of a finite list, the slow pointer points to the middle of this list. In the early 2000’s, this property was found to make it possible to program a palindrome detector for immutable lists that operates in one recursive traversal of the given list and performs the smallest possible number of comparisons, using the “There And Back Again” (TABA) recursion pattern. In this article, this palindrome detector is reconstructed in OCaml, formalized with the Coq Proof Assistant, and proved to be correct. More broadly, this article presents a compositional account of the tortoise-and-hare algorithm for finite lists. Concretely, compositionality means that programs that use a fast and a slow pointer can be expressed with an ordinary fold function for lists and reasoned about using ordinary structural induction on the given list. This article also contains a dozen new applications of the TABA recursion pattern and of its tail-recursive variant, “There and Forth Again”.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4297219110",
    "type": "article"
  },
  {
    "title": "Limits and Difficulties in the Design of Under-Approximation Abstract Domains",
    "doi": "https://doi.org/10.1145/3666014",
    "publication_date": "2024-06-09",
    "publication_year": 2024,
    "authors": "Flavio Ascari; Roberto Bruni; Roberta Gori",
    "corresponding_authors": "",
    "abstract": "The main goal of most static analyses is to prove the absence of bugs : if the analysis reports no alarms then the program will not exhibit any unwanted behaviours. For this reason, they are designed to over-approximate program behaviours and, consequently, they can report some false alarms. O’Hearn’s recent work on incorrectness has renewed the interest in the use of under-approximations for bug finding , because they only report true alarms. In principle, Abstract Interpretation techniques can handle under-approximations as well as over-approximations, but, in practice, few attempts were developed for the former, notwithstanding the much wider literature on the latter. In this paper we investigate the possibility of exploiting under-approximation abstract domains for bug-finding analyses. First we restrict to consider concrete powerset domains and highlight some intuitive asymmetries between over- and under-approximations. Then, we prove that the effectiveness of abstract domains defined by Under-approximation Galois connection is limited, because the analysis is likely to return trivial results whenever common transfer functions are encoded in the program. To this aim, we introduce the original concepts of non-emptying functions and highly surjective function family and we prove the nonexistence of abstract domains able to under-approximate such functions in a non-trivial way. We show many examples of finite and infinite numerical domains, as well as other generic domains. In all such cases, we prove the impossibility of performing nontrivial analyses via under-approximating Galois connections.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399507453",
    "type": "article"
  },
  {
    "title": "Synthesizing Invariants for Polynomial Programs by Semidefinite Programming",
    "doi": "https://doi.org/10.1145/3708559",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Hao Wu; Qiuye Wang; Xue Bai; Naijun Zhan; Lihong Zhi; Zhihong Yang",
    "corresponding_authors": "",
    "abstract": "Constraint-solving-based program invariant synthesis takes a parametric invariant template and encodes the (inductive) invariant conditions into constraints. The problem of characterizing the set of all valid parameter assignments is referred to as the strong invariant synthesis problem , while the problem of finding a concrete valid parameter assignment is called the weak invariant synthesis problem . For both problems, the challenge lies in solving or reducing the encoded constraints, which are generally non-convex and lack efficient solvers. In this paper, we propose two novel algorithms for synthesizing invariants of polynomial programs using semidefinite programming (SDP): (1) The Cluster algorithm targets the strong invariant synthesis problem for polynomial invariant templates. Leveraging robust optimization techniques, it solves a series of SDP relaxations and yields a sequence of increasingly precise under-approximations of the set of valid parameter assignments. We prove the algorithm’s soundness, convergence, and weak completeness under a specific robustness assumption on templates. Moreover, the outputs can simplify the weak invariant synthesis problem. (2) The Mask algorithm addresses the weak invariant synthesis problem in scenarios where the aforementioned robustness assumption does not hold, rendering the Cluster algorithm ineffective. It identifies a specific subclass of invariant templates, termed masked templates, involving parameterized polynomial equalities and known inequalities. By applying variable substitution, the algorithm transforms constraints into an equivalent form amenable to SDP relaxations. Both algorithms have been implemented and demonstrated superior performance compared to state-of-the-art methods in our empirical evaluation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405549773",
    "type": "article"
  },
  {
    "title": "Algorithm 568: PDS–A Portable Directory System",
    "doi": "https://doi.org/10.1145/357133.357137",
    "publication_date": "1981-04-01",
    "publication_year": 1981,
    "authors": "David R. Hanson",
    "corresponding_authors": "David R. Hanson",
    "abstract": "article Free Access Share on Algorithm 568: PDS–A Portable Directory System Author: David R. Hanson Department of Computer Science, The University of Arizona, Tucson, AZ Department of Computer Science, The University of Arizona, Tucson, AZView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 2pp 162–167https://doi.org/10.1145/357133.357137Published:01 April 1981Publication History 2citation429DownloadsMetricsTotal Citations2Total Downloads429Last 12 Months8Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2023367393",
    "type": "article"
  },
  {
    "title": "Nonsequentiality and Concrete Activity Phases in Discrete-Event Simulation Languages",
    "doi": "https://doi.org/10.1145/357139.357144",
    "publication_date": "1981-07-01",
    "publication_year": 1981,
    "authors": "John A. Barnden",
    "corresponding_authors": "John A. Barnden",
    "abstract": "article Free Access Share on Nonsequentiality and Concrete Activity Phases in Discrete-Event Simulation Languages Author: J. A. Barnden Computer Science Department, Indiana University, Bloomington, IN Computer Science Department, Indiana University, Bloomington, INView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 3July 1981 pp 293–317https://doi.org/10.1145/357139.357144Published:01 July 1981Publication History 1citation245DownloadsMetricsTotal Citations1Total Downloads245Last 12 Months6Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2035121232",
    "type": "article"
  },
  {
    "title": "Compact Storage of Binary Trees",
    "doi": "https://doi.org/10.1145/357172.357174",
    "publication_date": "1982-07-01",
    "publication_year": 1982,
    "authors": "P. Sipala",
    "corresponding_authors": "P. Sipala",
    "abstract": "article Free Access Share on Compact Storage of Binary Trees Author: Paolo Sipala Istituto di elettrotecnica e di elettronica, Università degli Studi di Trieste, Via Valerio, 10, 34127 Trieste, Italy Istituto di elettrotecnica e di elettronica, Università degli Studi di Trieste, Via Valerio, 10, 34127 Trieste, ItalyView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 3July 1982 pp 345–361https://doi.org/10.1145/357172.357174Published:01 July 1982Publication History 0citation493DownloadsMetricsTotal Citations0Total Downloads493Last 12 Months39Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2048754970",
    "type": "article"
  },
  {
    "title": "Technical Correspondence: On Francez's ``Distributed Termination''",
    "doi": "https://doi.org/10.1145/357121.357131",
    "publication_date": "1981-01-01",
    "publication_year": 1981,
    "authors": "C. Mohan",
    "corresponding_authors": "C. Mohan",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2053785305",
    "type": "article"
  },
  {
    "title": "On the (non-) relationship between SLR(1) and NQLALR(1) grammars",
    "doi": "https://doi.org/10.1145/42190.42276",
    "publication_date": "1988-04-01",
    "publication_year": 1988,
    "authors": "Manuel E. Bermudez; Karl M. Schimpf",
    "corresponding_authors": "",
    "abstract": "A popular but “not-quite” correct technique for computing LALR(1) look-ahead sets has been formalized by DeRemer and Pennello and dubbed NQLALR(l). They also claim that the class of SLR(l) grammars is a subset of the class of NQLALR(1) grammars. We prove here that no such relationship exists between those two classes. We do so with a counterexample that, ironically, appeared in DeRemer and Pennello's own paper.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1968432556",
    "type": "article"
  },
  {
    "title": "Global Data Flow Analysis Problems Arising in Locally Least-Cost Error Recovery",
    "doi": "https://doi.org/10.1145/2993.357243",
    "publication_date": "1984-04-01",
    "publication_year": 1984,
    "authors": "Roland Backhouse",
    "corresponding_authors": "Roland Backhouse",
    "abstract": "article Free Access Share on Global Data Flow Analysis Problems Arising in Locally Least-Cost Error Recovery Author: Roland Backhouse Department of Computer Science, University of Essex, Wivenhoe Park, Colchester CO4 3SQ, U.K. Department of Computer Science, University of Essex, Wivenhoe Park, Colchester CO4 3SQ, U.K.View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 2April 1984 pp 192–214https://doi.org/10.1145/2993.357243Online:01 April 1984Publication History 1citation276DownloadsMetricsTotal Citations1Total Downloads276Last 12 Months8Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2049806588",
    "type": "article"
  },
  {
    "title": "Efficient Demand-Driven Evaluation (II).",
    "doi": null,
    "publication_date": "1983-09-01",
    "publication_year": 1983,
    "authors": "Keshav Pingali; Arvind Arvind",
    "corresponding_authors": "",
    "abstract": "Abstract : In Part I of this paper, we presented a scheme whereby a compiler could propagate demands through programs in a powerful stream language L. A data-driven evaluation of the transformed program performed exactly the same computation as a demand-driven evaluation of the original program. In this paper, we explore a different transformation which trades the complexity of demand propagation for a bounded amount of extra computation on some data lines. We showed that a lazy progam can be associated with any LD program and explored the concept of safe LD programs which were LD programs that performed a bounded amount of extra computation over that performed by their corresponding lazy programs. Since the set of safe programs is not closed under composition, we defined strongly-safe programs as being that subset of safe programs which is closed under composition. A class of strongly-safe programs is the set of all LD programs that are safe and input-output equivalent to their corresponding lazy programs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2916738081",
    "type": "article"
  },
  {
    "title": "Thinking Inside the Box",
    "doi": "https://doi.org/10.1145/2866576",
    "publication_date": "2016-04-08",
    "publication_year": 2016,
    "authors": "Gregor Wagner; Per Larsen; Stefan Brunthaler; Michael Franz",
    "corresponding_authors": "",
    "abstract": "The web browser is the “new desktop.” Not only do many users spend most of their time using the browser, the browser has also become host to rich and dynamic applications that were previously tailored to each individual operating system. The lingua franca of web scripting, JavaScript, was pivotal in this development. Imagine that all desktop applications allocated memory from a single heap managed by the operating system. To reclaim memory upon application shutdown, all processes would then be garbage collected—not just the one being quit. While operating systems improved upon this approach long ago, this was how browsers managed memory until recently. This article explores compartmentalized memory management, an approach tailored specifically to web browsers. The idea is to partition the JavaScript heap into compartments and allocate objects to compartments based on their origin. All objects in the same compartment reference each other direct, whereas cross-origin references go through wrapper objects. We carefully evaluate our techniques using Mozilla’s Firefox browser—which now ships with our enhancements—and demonstrate the benefits of collecting each compartment independently. This simultaneously improves runtime performance (up to 36%) and reduces garbage collection pause times (up to 75%) as well as the memory footprint of the browser. In addition, enforcing the same-origin security policy becomes simple and efficient with compartments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2331817679",
    "type": "article"
  },
  {
    "title": "An Abstract Interpretation-Based Model of Tracing Just-in-Time Compilation",
    "doi": "https://doi.org/10.1145/2853131",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Stefano Dissegna; Francesco Logozzo; Francesco Ranzato",
    "corresponding_authors": "",
    "abstract": "Tracing just-in-time compilation is a popular compilation technique for the efficient implementation of dynamic languages, which is commonly used for JavaScript, Python, and PHP. It relies on two key ideas. First, it monitors program execution in order to detect so-called hot paths, that is, the most frequently executed program paths. Then, hot paths are optimized by exploiting some information on program stores that is available and therefore gathered at runtime. The result is a residual program where the optimized hot paths are guarded by sufficient conditions ensuring some form of equivalence with the original program. The residual program is persistently mutated during its execution, for example, to add new optimized hot paths or to merge existing paths. Tracing compilation is thus fundamentally different from traditional static compilation. Nevertheless, despite the practical success of tracing compilation, very little is known about its theoretical foundations. We provide a formal model of tracing compilation of programs using abstract interpretation. The monitoring phase (viz., hot path detection) corresponds to an abstraction of the trace semantics of the program that captures the most frequent occurrences of sequences of program points together with an abstraction of their corresponding stores, for example, a type environment. The optimization phase (viz., residual program generation) corresponds to a transform of the original program that preserves its trace semantics up to a given observation as modeled by some abstraction. We provide a generic framework to express dynamic optimizations along hot paths and to prove them correct. We instantiate it to prove the correctness of dynamic type specialization and constant variable folding. We show that our framework is more general than the model of tracing compilation introduced by Guo and Palsberg [2011], which is based on operational bisimulations. In our model, we can naturally express hot path reentrance and common optimizations like dead-store elimination, which are either excluded or unsound in Guo and Palsberg’s framework.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W249110966",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2836326",
    "publication_date": "2015-10-16",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Tree automata and transducers are used in a wide range of applications in software engineering. While these formalisms are of immense practical use, they can only model finite alphabets. To overcome this problem we augment tree automata and transducers ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231736677",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2683389",
    "publication_date": "2014-11-17",
    "publication_year": 2014,
    "authors": "Jens Palsberg",
    "corresponding_authors": "Jens Palsberg",
    "abstract": "Editorial I thank the associate editors who continue to serve TOPLAS, and I also thank Matthew Dwyer who in Summer 2014 reached the end of his term and stepped down as associate editor. Welcome to Kathleen Fisher who is a new associate editor. Jens Palsberg Editor in Chief",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232666190",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2633904",
    "publication_date": "2014-07-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This paper defines a new variant of program slicing, called specialization slicing, and presents an algorithm for the specialization-slicing problem that creates an optimal output slice. An algorithm for specialization slicing is polyvariant: for a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234060851",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2684821",
    "publication_date": "2014-10-28",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article considers some known abstract domains for affine-relation analysis (ARA), along with several variants, and studies how they relate to each other. The various domains represent sets of points that satisfy affine relations over variables that ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235378105",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2785583",
    "publication_date": "2015-06-18",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Data races are one of the main causes of concurrency problems in multithreaded programs. Whether all data races are bad, or some are harmful and others are harmless, is still the subject of vigorous scientific debate [Narayanasamy et al. 2007; Boehm ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236083205",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2982214",
    "publication_date": "2016-10-13",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present a modular approach to automatic complexity analysis of integer programs. Based on a novel alternation between finding symbolic time bounds for program parts and using these to infer bounds on the absolute values of program variables, we can ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241518618",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2688877",
    "publication_date": "2015-01-20",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "I thank the associate editors who continue to serve TOPLAS, and I also thank Matthew Dwyer who in Summer 2014 reached the end of his term and stepped down as associate editor.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243870583",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2597180",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Through the use of conditional compilation and related tools, many software projects can be used to generate a huge number of related programs. The problem of typing such variational software is difficult. The brute-force strategy of generating all ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244691083",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2764452",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Runtime analysis provides an effective method for measuring the sensitivity of programs to rounding errors. To date, implementations have required significant changes to source code, detracting from their widespread application. In this work, we present ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244818558",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2914585",
    "publication_date": "2016-05-02",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In object-oriented programs, objects often provide methods whose parameter types or return types are the object types themselves. For example, the parameter types of binary methods are the types of their receiver objects, and the return types of some ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249084458",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2866613",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Synchronization constructs lie at the heart of any reliable concurrent program. Many such constructs are standard (e.g., locks, queues, stacks, and hash-tables). However, many concurrent applications require custom synchronization constructs with ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251750970",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2669618",
    "publication_date": "2014-09-25",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we present a general method for achieving global static analyzers that are precise and sound, yet also scalable. Our method, on top of the abstract interpretation framework, is a general sparse analysis technique that supports ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255695435",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2807424",
    "publication_date": "2015-08-13",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Recent research has shown that it is possible to leverage general-purpose theorem-proving techniques to develop powerful type systems for the verification of a wide range of security properties on application code. Although successful in many respects, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256324662",
    "type": "paratext"
  },
  {
    "title": "Parallel programming with control abstraction",
    "doi": "https://doi.org/10.1145/177492.177584",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Lawrence A. Crowl; Thomas J. LeBlanc",
    "corresponding_authors": "",
    "abstract": "Parallel programming involves finding the potential parallelism in an application and mapping it to the architecture at hand. Since a typical application has more potential parallelism than any single architecture can exploit effectively, programmers usually limit their focus to the parallelism that the available control constructs express easily and that the given architecture exploits efficiently. This approach produces programs that exhibit much less parallelism that exists in the application, and whose performance depends critically on the underlying hardware and software. We argue for an alternative approach based on control abstraction . Control abstraction is the process by which programmers define new control constructs, specifying constraints on statement ordering separately from an implementation of that ordering. With control abstraction programmers can define and use a rich variety of control constructs to represent an algorithm's potential parallelism. Since control abstraction separates the definition of a construct from its implementation, a construct may have several different implementations, each exploiting a different subset of the parallelism admitted by the construct. By selecting an implementation for each control construct using annotations, a programmer can vary the parallelism in a program to best exploit the underlying hardware without otherwise changing the source code. This approach produces programs that exhibit most of the potential parallelism in an algorithm, and whose performance can be tuned simply by choosing among the various implementations for the control constructs in use.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1983939376",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1890028",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present MorphJ: a language for specifying general classes whose members are produced by iterating over members of other classes. We call this technique “class morphing” or just “morphing.” Morphing extends the notion of genericity so that not only ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230003745",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2039346",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "While model checking of pushdown systems is by now an established technique in software verification, temporal logics and automata traditionally used in this area are unattractive on two counts. First, logics and automata traditionally used in model ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230635048",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1961204",
    "publication_date": "2011-04-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Type constraints express subtype relationships between the types of program expressions, for example, those relationships that are required for type correctness. Type constraints were originally proposed as a convenient framework for solving type ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237456259",
    "type": "paratext"
  },
  {
    "title": "Editorial note",
    "doi": "https://doi.org/10.1145/2049706.2049707",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Jens Palsberg",
    "corresponding_authors": "Jens Palsberg",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238424804",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2049706",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238556083",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2450136.2450141",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Jens Palsberg",
    "corresponding_authors": "Jens Palsberg",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238765031",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2542180",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Differential privacy is a notion of confidentiality that allows useful computations on sensible data while protecting the privacy of individuals. Proving differential privacy is a difficult and error-prone task that calls for principled approaches and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238935548",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1889997",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241046147",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1985342",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241246883",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1889997.1889998",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Jens Palsberg",
    "corresponding_authors": "Jens Palsberg",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241565222",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2450136",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "ML modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. Mixin modules ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249072226",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2491522",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present Relational Hoare Type Theory (RHTT), a novel language and verification system capable of expressing and verifying rich information flow and access control policies via dependent types. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252911786",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2560142",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253567425",
    "type": "paratext"
  },
  {
    "title": "On the performance of balanced hashing functions when the keys are not equiprobable",
    "doi": "https://doi.org/10.1145/357084.357089",
    "publication_date": "1980-01-01",
    "publication_year": 1980,
    "authors": "Christos H. Papadimitriou; Philip A. Bernstein",
    "corresponding_authors": "",
    "abstract": "The cost (expected number of accesses per retrieval) of hashing functions is examined without the assumption that it is equally probable for all keys to be present in the table. It is shown that the obvious strategy—trying to balance the sums of probabilities of the keys mapped to any given address—may be suboptimal; however, the difference from the exactly optimal distribution cannot be large.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2016643759",
    "type": "article"
  },
  {
    "title": "Corrigendum: “A New Approach to Proving the Correctness of Multiprocess Programs”",
    "doi": "https://doi.org/10.1145/357084.357093",
    "publication_date": "1980-01-01",
    "publication_year": 1980,
    "authors": "Leslie Lamport",
    "corresponding_authors": "Leslie Lamport",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2034672230",
    "type": "erratum"
  },
  {
    "title": "La dolce vita at TOPLAS",
    "doi": "https://doi.org/10.1145/1734206.1734207",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Kathryn S. McKinley; Keshav Pingali",
    "corresponding_authors": "",
    "abstract": "research-article Free Access Share on La dolce vita at TOPLAS Authors: Kathryn S. Mckinley View Profile , Keshav Pingali View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 32Issue 4April 2010 Article No.: 10pp 1–6https://doi.org/10.1145/1734206.1734207Published:22 April 2010Publication History 0citation379DownloadsMetricsTotal Citations0Total Downloads379Last 12 Months29Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1977804871",
    "type": "article"
  },
  {
    "title": "La prossima vita at TOPLAS",
    "doi": "https://doi.org/10.1145/1749608.1749609",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Kathryn S. McKinley; Keshav Pingali",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2018454565",
    "type": "article"
  },
  {
    "title": "Essential AOP",
    "doi": "https://doi.org/10.1145/2362389.2362391",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Bruno De Fraine; Erik Ernst; Mario Südholt",
    "corresponding_authors": "",
    "abstract": "Aspect-oriented programming (AOP) has produced interesting language designs, but also ad hoc semantics that needs clarification. We contribute to this clarification with a calculus that models essential AOP, both simpler and more general than existing formalizations. In AOP, advice may intercept method invocations, and proceed executes the suspended call. Proceed is an ad hoc mechanism, only usable inside advice bodies. Many pointcut mechanisms, for example, wildcards, also lack regularity. We model proceed using first-class closures, and shift complexity from pointcuts to ordinary object-oriented code. Two well-known pointcut categories, call and execution, are commonly considered similar.We formally expose their differences, and resolve the associated soundness problem. Our calculus includes type ranges, an intuitive and concise alternative to explicit type variables that allows advice to be polymorphic over intercepted methods. We use calculus parameters to cover type safety for a wide design space of other features. Type soundness is verified in Coq.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2018654076",
    "type": "article"
  },
  {
    "title": "Detecting bugs in register allocation",
    "doi": "https://doi.org/10.1145/1734206.1734212",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Yuqiang Huang; Bruce R. Childers; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "Although register allocation is critical for performance, the implementation of register allocation algorithms is difficult, due to the complexity of the algorithms and target machine architectures. It is particularly difficult to detect register allocation errors if the output code runs to completion, as bugs in the register allocator can cause the compiler to produce incorrect output code. The output code may even execute properly on some test data, but errors can remain. In this article, we propose novel data flow analyses to statically check that the value flow of the output code from the register allocator is the same as the value flow of its input code. The approach is accurate, fast, and can identify and report error locations and types. It is independent of the register allocator and uses only the input and output code of the register allocator. It can be used with different register allocators, including those that perform coalescing and rematerialization. The article describes our approach, called SARAC, and a tool that statically checks a register allocation and reports the errors and their types that it finds. The tool has an average compile-time overhead of only 8% and a modest average memory overhead of 85KB. Our techniques can be used by compiler developers during regression testing and as a command-line-enabled debugging pass for mysterious compiler behavior.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2043465108",
    "type": "article"
  },
  {
    "title": "Editorial: Remembrances of things past",
    "doi": "https://doi.org/10.1145/1516507.1538825",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Keshav Pingali; Kathryn S. McKinley",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2054096256",
    "type": "editorial"
  },
  {
    "title": "Erratum: Efficient constraint propagation engines (ACM Transactions on Programming Languages and Systems 31:1 DOI 10.1145/1452044/1452046)",
    "doi": null,
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Christian Schulte; PJ Stuckey",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3088393986",
    "type": "erratum"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2400676",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Integrated Development Environments (IDEs) increase programmer productivity, providing rapid, interactive feedback based on the syntax and semantics of a language. Unlike conventional parsing algorithms, scannerless generalized-LR parsing supports the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230246339",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2362389",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Designers often apply manual or semi-automatic loop and data transformations on array- and loop-intensive programs to improve performance. It is crucial that such transformations preserve the functionality of the program. This article presents an ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231293706",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1596527",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A dependence cluster is a set of program statements, all of which are mutually inter-dependent. This article reports a large scale empirical study of dependence clusters in C program source code. The study reveals that large dependence clusters are ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234116311",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1552309",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "On modern computer systems, the memory performance of an application depends on its locality. For a single execution, locality-correlated measures like average miss rate or working-set size have long been analyzed using reuse distance—the number of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234377346",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1538917",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238825723",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1709093",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238844662",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1745312",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a type system for lock-freedom in the π-calculus, which guarantees that certain communications will eventually succeed. Distinguishing features of our type system are: it can verify lock-freedom of concurrent programs that have sophisticated ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239324763",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1734206",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245678273",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1498926",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In the design phase of software development, the designer must make many fundamental design decisions concerning the architecture of the system. Incorrect decisions are relatively easy and inexpensive to fix if caught during the design process, but the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246111255",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2160910",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "TOPLAS will have four issues per year from now on! The goal is to have each of the four issues contain 5--6 articles, instead of the previous schedule of six issues that each typically contained 3--4 articles.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246262459",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1667048",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246426330",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2220365",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Finite-state properties account for an important class of program properties, typically related to the order of operations invoked on objects. Many library implementations therefore include manually written finite-state monitors to detect violations of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247304783",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2160910.2180860",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "Jens Palsberg",
    "corresponding_authors": "Jens Palsberg",
    "abstract": "TOPLAS will have four issues per year from now on! The goal is to have each of the four issues contain 5--6 articles, instead of the previous schedule of six issues that each typically contained 3--4 articles. Historically, TOPLAS had four issues per volume from 1980 to 1992, and all but three of those issues contained 5 or more articles. I hope that the TOPLAS readers will enjoy the fewer, thicker issues of TOPLAS. I thank the associate editors who continue to serve TOPLAS, and I also thank Wei Li and Aaron Stump who in the Winter of 2012 reached the end of their terms and stepped down as associate editors. Welcome to Michael Hicks who is a new associate editor.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252189559",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1516507",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a sequent calculus derived from the λ―μμ˜-calculus of Curien and Herbelin that is expressive enough to directly represent the fine details of program evaluation using typical abstract machines. Not only does the calculus easily encode the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252822950",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1462166",
    "publication_date": "2009-02-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255342532",
    "type": "paratext"
  },
  {
    "title": "Compilation of Acyclic Smooth Programs for Parallel Execution",
    "doi": "https://doi.org/10.1145/357121.357124",
    "publication_date": "1981-01-01",
    "publication_year": 1981,
    "authors": "Otto C. Juelich; Clinton R. Foulk",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Compilation of Acyclic Smooth Programs for Parallel Execution Authors: Otto C. Juelich Missile Systems Division, Rockwell International, Columbus, OH Missile Systems Division, Rockwell International, Columbus, OHView Profile , Clinton R. Foulk Department of Computer and Information Science, The Ohio State University, 2036 Neil Avenue Mall, Columbus, OH Department of Computer and Information Science, The Ohio State University, 2036 Neil Avenue Mall, Columbus, OHView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 1Jan. 1981 pp 24–48https://doi.org/10.1145/357121.357124Published:01 January 1981Publication History 1citation201DownloadsMetricsTotal Citations1Total Downloads201Last 12 Months5Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2005194473",
    "type": "article"
  },
  {
    "title": "Introduction to special ESOP'05 issue",
    "doi": "https://doi.org/10.1145/1275497.1275498",
    "publication_date": "2007-08-02",
    "publication_year": 2007,
    "authors": "Mooly Sagiv",
    "corresponding_authors": "Mooly Sagiv",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2008878763",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1286821.1293892",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Kathryn S. McKinley; Keshav Pingali",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Editorial: A changing of the guard Editors: Kathryn S. McKinley View Profile , Keshav Pingali View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 29Issue 6pp 30–eshttps://doi.org/10.1145/1286821.1293892Published:01 October 2007Publication History 0citation406DownloadsMetricsTotal Citations0Total Downloads406Last 12 Months13Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2101757223",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1330017",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We show that reverse-mode AD (Automatic Differentiation)—a generalized gradient-calculation operator—can be incorporated as a first-class function in an augmented lambda calculus, and therefore into a functional-programming language. Closure is achieved,...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229510082",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1353445",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Current critical systems often use a lot of floating-point computations, and thus the testing or static analysis of programs containing floating-point operators has become a priority. However, correctly defining the semantics of common implementations ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230112924",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1391956",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230270762",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1452044",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Reasoning about multithreaded object-oriented programs is difficult, due to the nonlocal nature of object aliasing and data races. We propose a programming regime (or programming model) that rules out data races, and enables local reasoning in the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231547502",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3092741",
    "publication_date": "2017-07-20",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "JavaScript is the source of many security problems, including cross-site scripting attacks and malicious advertising code. Central to these problems is the fact that code from untrusted sources runs with full privileges. Information flow controls help ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232241922",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3062396",
    "publication_date": "2017-05-29",
    "publication_year": 2017,
    "authors": "Thomas Reps; Emma Turetsky; Prathmesh Prabhu",
    "corresponding_authors": "",
    "abstract": "Modern architectures rely on memory fences to prevent undesired weakenings of memory consistency. As the fences’ semantics may be subtle, the automation of their placement is highly desirable. But precise methods for restoring consistency do not scale ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234715810",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1387673",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Malware detection is a crucial aspect of software security. Current malware detectors work by checking for signatures, which attempt to capture the syntactic characteristics of the machine-level byte sequence of the malware. This reliance on a syntactic ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236652279",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3133234",
    "publication_date": "2017-09-18",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The syntax of the C programming language is described in the C11 standard by an ambiguous context-free grammar, accompanied with English prose that describes the concept of “scope” and indicates how certain ambiguous code fragments should be ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238214196",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1290520",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Despite significant progress in the theory and practice of program analysis, analyzing properties of heap data has not reached the same level of maturity as the analysis of static and stack data. The spatial and temporal structure of stack and static ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240293612",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1377492",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Foreign function interfaces (FFIs) allow components in different languages to communicate directly with each other. While FFIs are useful, they often require writing tricky low-level code and include little or no static safety checking, thus providing a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241479390",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1286821",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The π-calculus, a calculus of mobile processes, can compositionally represent dynamics of major programming constructs by decomposing them into name passing. The present work reports our experience in using a linear/affine typed π-calculus for the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241756623",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1232420",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Predicate abstraction is the basis of many program verification tools. Until now, the only known way to overcome the inherent limitation of predicate abstraction to safety properties was to manually annotate the finite-state abstraction of a program. We ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242569423",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1216374",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242790007",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3050768",
    "publication_date": "2017-03-23",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Compaction of a managed heap is a costly operation to be avoided as much as possible in commercial runtimes. Instead, partial compaction is often used to defragment parts of the heap and avoid space blowup. Previous study of compaction limitation ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245021349",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1275497",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present a precise correspondence between separation logic and a simple notion of predicate BI, extending the earlier correspondence given between part of separation logic and propositional BI. Moreover, we introduce the notion of a BI hyperdoctrine, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245894488",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1255450",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Modern JVM implementations interleave execution with compilation of “hot” methods to achieve reasonable performance. Since compilation overhead impacts the execution time of the application and induces run-time pauses, we explore offloading compilation ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250615789",
    "type": "paratext"
  },
  {
    "title": "Foreword",
    "doi": "https://doi.org/10.1145/3052720",
    "publication_date": "2017-03-23",
    "publication_year": 2017,
    "authors": "Andrew Myers",
    "corresponding_authors": "Andrew Myers",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251576246",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1180475",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We study HMG(X), an extension of the constraint-based type system HM(X) with deep pattern matching, polymorphic recursion, and guarded algebraic data types. Guarded algebraic data types subsume the concepts known in the literature as indexed types, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253944236",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1232420.1232421",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Martı́n Abadi; Jens Palsberg",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254521178",
    "type": "editorial"
  },
  {
    "title": "Synchronous Deterministic Parallel Programming for Multi-Cores with ForeC",
    "doi": "https://doi.org/10.1145/3591594",
    "publication_date": "2023-04-10",
    "publication_year": 2023,
    "authors": "Eugene Yip; Alain Girault; Partha S. Roop; Morteza Biglari-Abhari",
    "corresponding_authors": "",
    "abstract": "Embedded real-time systems are tightly integrated with their physical environment. Their correctness depends both on the outputs and timeliness of their computations. The increasing use of multi-core processors in such systems is pushing embedded programmers to be parallel programming experts. However, parallel programming is challenging because of the skills, experiences, and knowledge needed to avoid common parallel programming traps and pitfalls. This article proposes the ForeC synchronous multi-threaded programming language for the deterministic, parallel, and reactive programming of embedded multi-cores. The synchronous semantics of ForeC is designed to greatly simplify the understanding and debugging of parallel programs. ForeC ensures that ForeC programs can be compiled efficiently for parallel execution and be amenable to static timing analysis. ForeC’s main innovation is its shared variable semantics that provides thread isolation and deterministic thread communication. All ForeC programs are correct by construction and deadlock free because no non-deterministic constructs are needed. We have benchmarked our ForeC compiler with several medium-sized programs (e.g., a 2.274-line ForeC program with up to 26 threads and distributed on up to 10 cores, which was based on a 2.155-line non-multi-threaded C program). These benchmark programs show that ForeC can achieve better parallel performance than Esterel, a widely used imperative synchronous language for concurrent safety-critical systems, and is competitive in performance to OpenMP, a popular desktop solution for parallel programming (which implements classical multi-threading, hence is intrinsically non-deterministic). We also demonstrate that the worst-case execution time of ForeC programs can be estimated to a high degree of precision.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4363651373",
    "type": "article"
  },
  {
    "title": "Prisma : A Tierless Language for Enforcing Contract-client Protocols in Decentralized Applications",
    "doi": "https://doi.org/10.1145/3604629",
    "publication_date": "2023-07-18",
    "publication_year": 2023,
    "authors": "David Richter; David Kretzler; Pascal Weisenburger; Guido Salvaneschi; Sebastian Faust; Mira Mezini",
    "corresponding_authors": "",
    "abstract": "Decentralized applications (dApps) consist of smart contracts that run on blockchains and clients that model collaborating parties. dApps are used to model financial and legal business functionality. Today, contracts and clients are written as separate programs—in different programming languages—communicating via send and receive operations. This makes distributed program flow awkward to express and reason about, increasing the potential for mismatches in the client-contract interface, which can be exploited by malicious clients, potentially leading to huge financial losses. In this article, we present Prisma , a language for tierless decentralized applications, where the contract and its clients are defined in one unit and pairs of send and receive actions that “belong together” are encapsulated into a single direct-style operation, which is executed differently by sending and receiving parties. This enables expressing distributed program flow via standard control flow and renders mismatching communication impossible. We prove formally that our compiler preserves program behavior in presence of an attacker controlling the client code. We systematically compare Prisma with mainstream and advanced programming models for dApps and provide empirical evidence for its expressiveness and performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4384662007",
    "type": "article"
  },
  {
    "title": "A Structured APL System",
    "doi": "https://doi.org/10.1145/69622.357185",
    "publication_date": "1982-10-01",
    "publication_year": 1982,
    "authors": "Janick Bergeron; A. Dubuque",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A Structured APL System Authors: J. Bergeron Département d'Informatique, Faculté des Sciences et de Génie, Université Laval, Cité Universitaire, Québec, P.Q., Canada G1K 7P4 Département d'Informatique, Faculté des Sciences et de Génie, Université Laval, Cité Universitaire, Québec, P.Q., Canada G1K 7P4View Profile , A. Dubuque DMR and Associates, 2535 Boulevard Laurier, Québec, P.Q., Canada G1V 4M3 DMR and Associates, 2535 Boulevard Laurier, Québec, P.Q., Canada G1V 4M3View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 4pp 585–600https://doi.org/10.1145/69622.357185Published:01 October 1982Publication History 0citation278DownloadsMetricsTotal Citations0Total Downloads278Last 12 Months15Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1973661831",
    "type": "article"
  },
  {
    "title": "The ML approach to the readable all-purpose language",
    "doi": "https://doi.org/10.1145/5397.5918",
    "publication_date": "1986-04-01",
    "publication_year": 1986,
    "authors": "Christopher R. Spooner",
    "corresponding_authors": "Christopher R. Spooner",
    "abstract": "The ideal computer language is seen as one that would be as readable as natural language, and so adaptable that it could serve as the only language a user need ever know. An approach to language design has emerged that shows promise of allowing one to come much closer to that ideal than might reasonably have been expected. Using this approach, a language referred to as ML has been developed, and has been implemented as a language-creation system in which user-defined procedures invoked at translation time translate the source to some object code. In this way the user can define both the syntax and the semantics of the source language. Both language and implementation are capable of further development. This paper describes the approach, the language, and the implementation and recommends areas for further work.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1987690177",
    "type": "article"
  },
  {
    "title": "Tichy's response to R. W. Schwanke and G. E. Kaiser's “Smarter Recompilation”",
    "doi": "https://doi.org/10.1145/48022.214507",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "Walter F. Tichy",
    "corresponding_authors": "Walter F. Tichy",
    "abstract": "Schwanke and Kaiser's extension of smart recompilation is an intriguing idea. Their mechanism aims at delaying recompilation work by permitting “harmless” compilation inconsistencies to remain after changes. Full consistency can be reestablished at a later time, after the change has been tested in a subpart of the system. If the change was inadequate, then no needless compilation work was performed. This strategy is used frequently in practice, by exploiting loopholes in system generation tools. Schwanke and Kaiser's mechanism is novel in that it makes this practice safe. The compiler is aware of the inconsistencies, and will not overlook dangerous ones. Furthermore, it can help reestablish full consistency once a change is deemed acceptable. Smarter recompilation defines a harmless inconsistency as follows. If a declaration is changed, this action is treated as introducing a new version of the declaration. The coexistence of both the old and new versions in the same configuration is a harmless inconsistency as long as the uses of the two versions do not conflict. The system must be separable into two partitions, one that uses the old version and the other the new one, such that the interface between the two depends on neither. Since the inconsistent declarations do not cross the interface, the two partitions may even communicate with each other. Of course, inconsistencies not captured by the type system cannot be treated in this way. Schwanke and Kaiser's note leaves a few minor questions unanswered. For instance, a new or changed declaration might cause a redeclaration or overloading error that can only be detected by recompilation. Is this potential problem left undetected until full consistency is desired, or is it checked immediately? If a declaration is deleted that is still in use, is the deletion treated as an error or as a delayed deletion that will take effect after the last use disappears? If the old and new versions of a procedure operate on the same data structure, is it always desirable to let both versions coexist, or can the programmer indicate that the old version should be eliminated before the next program execution? Perhaps a future paper about an implementation will clarify these points. Smarter recompilation also opens the door for more powerful programming tools. For example, since the mechanism maintains cross-reference information, a tool like Masterscope [1] could be built relatively easily. The tool would have the advantage that cross-reference information is immediately available once a module has been compiled. Only little additional data would be needed to classify the uses of symbols. The information could also be exploited by a Maintainer's Assistant. This program helps with reestablishing consistency after changes by suggesting corrections of the affected program parts. For example, it could attempt to make call sites of changed procedures consistent with their headers, update operations on changed record fields, or perform some simple program transformations in response to data structure changes.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2081412201",
    "type": "article"
  },
  {
    "title": "Matching-based incremental evaluators for hierarchical attribute grammar dialects",
    "doi": "https://doi.org/10.1145/201059.201071",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Alan Carle; Lori Pollock",
    "corresponding_authors": "",
    "abstract": "Although attribute grammars have been very effective for defining individual modules of language translators, they have been rather ineffective for specifying large program-transformational systems. Recently, several new attribute grammar “dialects” have been developed that support the modular specification of these systems by allowing modules, each described by an attribute grammar, to be composed to form a complete system. Acceptance of these new hierarchical attribute grammar dialects requires the availability of efficient batch and incremental evaluators for hierarchical specifications. This paper addresses the problem of developing efficient incremental evaluators for hierarchical specifications. A matching-based approach is taken in order to exploit existing optimal change propagation algorithms for nonhierarchical attribute grammars. A sequence of four new matching algorithms is presented, each increasing the number of previously computed attribute values that are made available for reuse during the incremental update.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2082963135",
    "type": "article"
  },
  {
    "title": "Graph translation schemes to generate compiler parts",
    "doi": "https://doi.org/10.1145/29873.29874",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Michael Sonnenschein",
    "corresponding_authors": "Michael Sonnenschein",
    "abstract": "Graph translation schemes (GTSs) are a generalization of attribute grammars and of some ideas in Koster's language CDL2 They are specially designed to support a compiler writer in defining parts of the back-end of his compiler, but they can also be useful for the specification of the analysis pass of a compiler. GTSs combine elements of functional and of algorithmic specification techniques to allow iterative attribute evaluation and attributing of program graphs. GTSs consist of only a few syntactical elements. We present operational semantics and discuss improvements in the efficiency of the proposed implementation of GTSs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2086502488",
    "type": "article"
  },
  {
    "title": "Extending attribute grammars to support programming-in-the-large",
    "doi": "https://doi.org/10.1145/186025.186091",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Josephine Micallef; Gail E. Kaiser",
    "corresponding_authors": "",
    "abstract": "Attribute grammars add specification of static semantic properties to context-free grammars, which, in turn, describe the syntactic structure of program units. However, context-free grammars cannot express programming-in-the-large features common in modern programming languages, including unordered collections of units, included units, and sharing of included units. We present extensions to context-free grammars, and corresponding extensions to attribute grammars, suitable for defining such features. We explain how batch and incremental attribute-evaluation algorithms can be adapted to support these extensions, resulting in a uniform approach to intraunit and interunit static semantic analysis and translation of multiunit programs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2136486688",
    "type": "article"
  },
  {
    "title": "Clarification of \"Feeding Inputs on Demand\" in Efficient Demand-Driven Evaluation Part 1",
    "doi": "https://doi.org/10.1145/5001.3639574",
    "publication_date": "1986-01-02",
    "publication_year": 1986,
    "authors": "Keshav Pingali; Arvind Arvind",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W40745859",
    "type": "article"
  },
  {
    "title": "Epochs",
    "doi": "https://doi.org/10.1145/111186.116785",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "Jon A. Solworth",
    "corresponding_authors": "Jon A. Solworth",
    "abstract": "To date, the implementation of message passing languages has required hte communications variables (sometimes called ports) either to be limited to the number of physical communications registers in the machine or to be mapped to memory. Neither solution is satisfactory. Limiting the number of variables decreases modularity and efficiency of parallel programs. Mapping variables to memory increases the cost of communications and the granularity of parallelism. We present here a new programming language construct called epochs . Epochs are a scoping mechanism within which the programmer can declare communications variables, which are live only during the scope of that epoch. To limit the range of time a register has to be allocated for a communications variable, the compiler ensures that all processors enter an epoch simultaneously. The programming style engendered fits somewhere between the SIMD data parallel and the MIMD process spawning models. We describe an implementation for epochs including an efficient synchronization mechanism, a means of statically binding registers to communications variables, and a method of fusing epochs to reduce synchronization overhead.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4254161753",
    "type": "article"
  },
  {
    "title": "Editor's Foreword to “Static Backward Slicing of Non-Deterministic Programs and Systems”",
    "doi": "https://doi.org/10.1145/3243871",
    "publication_date": "2018-08-25",
    "publication_year": 2018,
    "authors": "Andrew C. Myers",
    "corresponding_authors": "Andrew C. Myers",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2893100133",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on ESOP 2018",
    "doi": "https://doi.org/10.1145/3368252",
    "publication_date": "2019-12-10",
    "publication_year": 2019,
    "authors": "Amal Ahmed",
    "corresponding_authors": "Amal Ahmed",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2998539611",
    "type": "article"
  },
  {
    "title": "確率的高次言語のための環境バイシミュレーション【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Davide Sangiorgi; Vignudelli Valeria",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3178111462",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3229520",
    "publication_date": "2018-06-02",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a unified framework for designing static analysers based on program synthesis. For this purpose, we identify a fragment of second-order logic with restricted quantification that is expressive enough to model numerous static ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231378084",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3324782",
    "publication_date": "2019-06-10",
    "publication_year": 2019,
    "authors": "Andrew Myers",
    "corresponding_authors": "Andrew Myers",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233147127",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3292525",
    "publication_date": "2018-12-13",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Building a cost-effective static analyzer for real-world programs is still regarded an art. One key contributor to this grim reputation is the difficulty in balancing the cost and the precision of an analyzer. An ideal analyzer should be adaptive to a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233424561",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3236464",
    "publication_date": "2018-08-29",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We study algorithmic questions wrt algebraic path properties in concurrent systems, where the transitions of the system are labeled from a complete, closed semiring. The algebraic path properties can model dataflow analysis problems, the shortest path ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234194373",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3173093",
    "publication_date": "2018-01-12",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Non-termination is the root cause of a variety of program bugs, such as hanging programs and denial-of-service vulnerabilities. This makes an automated analysis that can prove the absence of such bugs highly desirable. To scale termination checks to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240594807",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1146809",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243945947",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3343145",
    "publication_date": "2019-07-20",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Developing accurate and efficient program analyses for languages with higher-order functions is known to be difficult. Here we define a new higher-order program analysis, Demand-Driven Program Analysis (DDPA), which extends well-known demand-driven ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244777314",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3373084",
    "publication_date": "2019-12-10",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Consistent subtyping is employed in some gradual type systems to validate type conversions. The original definition by Siek and Taha serves as a guideline for designing gradual type systems with subtyping. Polymorphic types à la System F also induce a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247585368",
    "type": "paratext"
  },
  {
    "title": "Guest editorial",
    "doi": "https://doi.org/10.1145/1018203.1018204",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Martin Odersky; Benjamin C. Pierce",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248218611",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3299867",
    "publication_date": "2019-03-01",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present Armus, a verification tool for dynamically detecting or avoiding barrier deadlocks. The core design of Armus is based on phasers, a generalisation of barriers that supports split-phase synchronisation, dynamic membership, and optional-waits. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248747183",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3366632",
    "publication_date": "2019-12-05",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We study the problem of developing efficient approaches for proving worst-case bounds of non-deterministic recursive programs. Ranking functions are sound and complete for proving termination and worst-case bounds of non-recursive programs. First, we ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248922681",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1186632",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article presents a design for the Denali-2 superoptimizer, which will generate minimum-instruction-length machine code for realistic machine architectures using automatic theorem-proving technology: specifically, using E-graph matching (a technique ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251230869",
    "type": "paratext"
  },
  {
    "title": "Corrigendum to “Cross-Language Interoperability in a Multi-Language Runtime”, by Grimmer et al., ACM Transactions on Programming Languages and Systems (TOPLAS) Volume 40, Issue 2, Article No. 8",
    "doi": "https://doi.org/10.1145/3283723",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Corrigendum to “Cross-Language Interoperability in a Multi-Language Runtime”, by Grimmer et al., ACM Transactions on Programming Languages and Systems (TOPLAS) Volume 40, Issue 2, Article No. 8ACM Transactions on Programming Languages and SystemsVolume 40Issue 4December 2018 Article No.: 18pp 1https://doi.org/10.1145/3283723Published:16 November 2018Publication History 0citation299DownloadsMetricsTotal Citations0Total Downloads299Last 12 Months25Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteView all FormatsPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253477605",
    "type": "erratum"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1133651",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Regular expression patterns provide a natural, declarative way to express constraints on semistructured data and to extract relevant information from it. Indeed, it is a core feature of the programming language Perl, surfaces in various UNIX tools such ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254579273",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3320016",
    "publication_date": "2019-06-21",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Dependent types are a key feature of the proof assistants based on the Curry-Howard isomorphism. It is well known that this correspondence can be extended to classical logic by enriching the language of proofs with control operators. However, they are ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255082506",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1119479",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255416063",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1152649",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We develop the mechanism of variant parametric types as a means to enhance synergy between parametric and inclusion polymorphism in object-oriented programming languages. Variant parametric types are used to control both the subtyping between different ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256172793",
    "type": "paratext"
  },
  {
    "title": "Corrigendum to “Type-driven Gradual Security with References,” by Toro et al., ACM Transactions on Programming Languages and Systems (TOPLAS) Volume 40, Issue 4, Article No. 16",
    "doi": "https://doi.org/10.1145/3387725",
    "publication_date": "2020-05-19",
    "publication_year": 2020,
    "authors": "toro",
    "corresponding_authors": "toro",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245220477",
    "type": "erratum"
  },
  {
    "title": "Introduction to the Special Issue on ESOP 2021",
    "doi": "https://doi.org/10.1145/3524730",
    "publication_date": "2022-07-15",
    "publication_year": 2022,
    "authors": "Nobuko Yoshida",
    "corresponding_authors": "Nobuko Yoshida",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4285489716",
    "type": "article"
  },
  {
    "title": "Solving Program Sketches with Large Integer Values",
    "doi": "https://doi.org/10.1145/3532849",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Qinheping Hu; Rishabh Singh; Loris D’Antoni",
    "corresponding_authors": "",
    "abstract": "Program sketching is a program synthesis paradigm in which the programmer provides a partial program with holes and assertions. The goal of the synthesizer is to automatically find integer values for the holes so that the resulting program satisfies the assertions. The most popular sketching tool, Sketch , can efficiently solve complex program sketches but uses an integer encoding that often performs poorly if the sketched program manipulates large integer values. In this article, we propose a new solving technique that allows Sketch to handle large integer values while retaining its integer encoding. Our technique uses a result from number theory, the Chinese Remainder Theorem, to rewrite program sketches to only track the remainders of certain variable values with respect to several prime numbers. We prove that our transformation is sound and the encoding of the resulting programs are exponentially more succinct than existing Sketch encodings. We evaluate our technique on a variety of benchmarks manipulating large integer values. Our technique provides speedups against both existing Sketch solvers and can solve benchmarks that existing Sketch solvers cannot handle.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4286273216",
    "type": "article"
  },
  {
    "title": "Guest editorial",
    "doi": "https://doi.org/10.1145/514952.514953",
    "publication_date": "2002-03-01",
    "publication_year": 2002,
    "authors": "Kathleen Fisher; Benjamin C. Pierce",
    "corresponding_authors": "",
    "abstract": "The Eighth International Workshop on Foundations of Object-Oriented Languages (FOOL 8) was held on January 20, 2001, colocated with the ACM Symposium on Principles of Programming Languages in London. Kathleen Fisher chaired the program committee. Six contributed papers were presented. After the workshop, extended versions of three were solicited for this special issue of TOPLAS ; the two articles that were ultimately submitted were reviewed, revised, and accepted following standard TOPLAS procedures.The special issue opens with Type-Preserving Compilation of Featherweight Java by Christopher League, Zhong Shao, and Valery Trifonov. The goal of the authors is to establish a foundation for certifying compilation of Java-like class-based languages. To this end, the authors give an encoding of core Java features in a typed intermediate language suitable for use within a type-preserving compiler. Because of its intended use, the authors focus on developing an efficient encoding. They show that the type erasure of their implementation matches the standard vtable self-application semantics of message sending.The authors use Featherweight Java (FJ), which models the core features of Java, as their source language. Their translation targets a variant of F ω , already implemented as part of the SML/NJ compiler, using its row polymorphism and existential and recursive types to encode FJ. The authors show that the translation from FJ to their target preserves types.In the issue's second article, More Dynamic Object Reclassification: Fickle II , Sophia Drossopoulou, Ferruccio Damiani, Mariangiola Dezani-Ciancaglini, and Paola Giannini explore the issue of dynamic reclassification, by which an object changes its class membership at runtime while retaining its identity. This ability helps model real-world situations where an object has different roles over time---for example, a person who is first a student and then graduates to become a teacher.The authors focus their study on the design of a language, Fickle II , which extends an imperative, typed, class-based, object-oriented language with a reclassification operation. To specify the behavior of their language, the authors give an operational semantics. They then develop a type and effect system and show that the type system is sound, in the sense that a well-typed program cannot get stuck under the operational semantics.We would like to thank our colleagues who served on the FOOL 8 program committee and those who participated in the anonymous reviewing process for TOPLAS . Their efforts contributed greatly to the quality of this special issue.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247642794",
    "type": "editorial"
  },
  {
    "title": "Introduction to the Special Section on ESOP 2020",
    "doi": "https://doi.org/10.1145/3484490",
    "publication_date": "2021-11-15",
    "publication_year": 2021,
    "authors": "Péter Müller",
    "corresponding_authors": "Péter Müller",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3214291771",
    "type": "article"
  },
  {
    "title": "Technical Correspondence",
    "doi": "https://doi.org/10.1145/357133.357544",
    "publication_date": "1981-04-01",
    "publication_year": 1981,
    "authors": "Jakob Nielsen; Carl E. Landwehr",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Technical Correspondence: On Landwehr's “An Abstract Type for Statistics Collection” Authors: Jakob Nielsen Department of Computer Science, University of Aarhus, Ny Munkegade, DK-8000 Aarhus C, Denmark Department of Computer Science, University of Aarhus, Ny Munkegade, DK-8000 Aarhus C, DenmarkView Profile , Carl E. Landwehr Code 7522, Naval Research Laboratory, Washington, DC Code 7522, Naval Research Laboratory, Washington, DCView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 2April 1981 https://doi.org/10.1145/357133.357544Online:01 April 1981Publication History 0citation195DownloadsMetricsTotal Citations0Total Downloads195Last 12 Months2Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1971944454",
    "type": "article"
  },
  {
    "title": "Corrigendum: ``The Design and Application of a Retargetable Peephole Optimizer''",
    "doi": "https://doi.org/10.1145/357121.357129",
    "publication_date": "1981-01-01",
    "publication_year": 1981,
    "authors": "Jack W. Davidson; Christopher W. Fraser",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2062499784",
    "type": "erratum"
  },
  {
    "title": "Corrigendum: ``External Representations of Objects of User-Defined Type''",
    "doi": "https://doi.org/10.1145/357121.357130",
    "publication_date": "1981-01-01",
    "publication_year": 1981,
    "authors": "Peter J. L. Wallis",
    "corresponding_authors": "Peter J. L. Wallis",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2071811077",
    "type": "erratum"
  },
  {
    "title": "Proving Concurrent Constraint Programs Correct",
    "doi": null,
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "F.S. deBoer",
    "corresponding_authors": "F.S. deBoer",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2744563849",
    "type": "article"
  },
  {
    "title": "Technical Correspondence",
    "doi": "https://doi.org/10.1145/357146.357545",
    "publication_date": "1981-10-01",
    "publication_year": 1981,
    "authors": "R. L. Earle; Richard B. Kieburtz; Abraham Silberschatz",
    "corresponding_authors": "",
    "abstract": "article Free AccessTechnical Correspondence Authors: R. L. Earle Department of Mathematical Sciences, The University of Texas at Dallas, Richardson, TX Department of Mathematical Sciences, The University of Texas at Dallas, Richardson, TXView Profile , R. B. Kieburtz Department of Computer Science, SUNY at Stony Brook, Stony Brook, NY Department of Computer Science, SUNY at Stony Brook, Stony Brook, NYView Profile , A. Silberschatz Department of Computer Sciences, The University of Texas at Austin, Austin, TX Department of Computer Sciences, The University of Texas at Austin, Austin, TXView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 3Issue 4pp 533–536https://doi.org/10.1145/357146.357545Published:01 October 1981Publication History 0citation179DownloadsMetricsTotal Citations0Total Downloads179Last 12 Months2Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246499022",
    "type": "article"
  },
  {
    "title": "Arrays without indices",
    "doi": null,
    "publication_date": "1982-03-01",
    "publication_year": 1982,
    "authors": "Juris Reinfelds",
    "corresponding_authors": "Juris Reinfelds",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1487306850",
    "type": "article"
  },
  {
    "title": "Technical Correspondence: On Tanenbaum, van Staveren, and Stevenson's ``Using Peephole Optimization on Intermediate Code''",
    "doi": "https://doi.org/10.1145/2166.357220",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Steven Pemberton",
    "corresponding_authors": "Steven Pemberton",
    "abstract": "article Free AccessTechnical Correspondence: On Tanenbaum, van Staveren, and Stevenson's ``Using Peephole Optimization on Intermediate Code'' Author: Steven Pemberton Department of Computing and Cybernetics, Brighton Polytechnic, Moulsecoomb, Brighton, England Department of Computing and Cybernetics, Brighton Polytechnic, Moulsecoomb, Brighton, EnglandView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3pp 499–500https://doi.org/10.1145/2166.357220Published:01 July 1983Publication History 0citation230DownloadsMetricsTotal Citations0Total Downloads230Last 12 Months8Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1977917720",
    "type": "article"
  },
  {
    "title": "On iterative constructs",
    "doi": "https://doi.org/10.1145/77606.214517",
    "publication_date": "1990-01-03",
    "publication_year": 1990,
    "authors": "David Lorge Parnas",
    "corresponding_authors": "David Lorge Parnas",
    "abstract": "The wheel is repeatedly reinvented because it is a good idea. Perhaps Anson's \"A Generalized Iterative Construct and Its Semantics\" [1] confirms that “A Generalized Control Structure and Its Formal Definition” [2], and the earlier “An Alternative Control Structure and its Formal Definition” [3] presented good ideas. However, there are several misstatements in [1] that should be corrected. The discussion of these issues is made a bit academic by the four-year delay between Anson's submission of his paper (which apparently coincided with the publication of [2]) and the publication of [1]. In that time a generalization of both schemes has been published as a Technical Report [6] and has been submitted for publication. In this generalization the decision about whether a command is iterating or terminating can be made during execution, and the semantics must be that of DO TERM. Further generalizations make the seman tics of the constructs more practical, since side-effects are accurately treated in all cases. A method for reducing the length of guards and avoiding duplicated subexpressions is also provided.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2009996995",
    "type": "article"
  },
  {
    "title": "Technical Correspondence: On Steensgaard-Madsen's ``A Statement-Oriented Approach to Data Abstraction''",
    "doi": "https://doi.org/10.1145/357153.357160",
    "publication_date": "1982-01-01",
    "publication_year": 1982,
    "authors": "John Ellis",
    "corresponding_authors": "John Ellis",
    "abstract": "article Free Access Share on Technical Correspondence: On Steensgaard-Madsen's ``A Statement-Oriented Approach to Data Abstraction'' Author: John R. Ellis Department of Computer Science, Yale University, 10 Hillhouse Avenue, New Haven, CT Department of Computer Science, Yale University, 10 Hillhouse Avenue, New Haven, CTView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 1Jan. 1982 pp 120–122https://doi.org/10.1145/357153.357160Published:01 January 1982Publication History 0citation210DownloadsMetricsTotal Citations0Total Downloads210Last 12 Months6Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2015150388",
    "type": "article"
  },
  {
    "title": "Technical Correspondence: Steensgaard-Madsen's reply",
    "doi": "https://doi.org/10.1145/357153.357161",
    "publication_date": "1982-01-01",
    "publication_year": 1982,
    "authors": "Jørgen Steensgaard‐Madsen",
    "corresponding_authors": "Jørgen Steensgaard‐Madsen",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2026564729",
    "type": "article"
  },
  {
    "title": "A note on Cohen's “eliminating redundant recursive calls”",
    "doi": "https://doi.org/10.1145/4472.215006",
    "publication_date": "1985-10-01",
    "publication_year": 1985,
    "authors": "Norman Cohen",
    "corresponding_authors": "Norman Cohen",
    "abstract": "article Free Access Share on A note on Cohen's “eliminating redundant recursive calls” Author: Norman H. Cohen Softech Inc. 705 Masons Mill Business Park, Huntingdon Valley, PA 19006 Softech Inc. 705 Masons Mill Business Park, Huntingdon Valley, PA 19006View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 7Issue 4Oct. 1985 pp 680–685https://doi.org/10.1145/4472.215006Online:01 October 1985Publication History 0citation193DownloadsMetricsTotal Citations0Total Downloads193Last 12 Months4Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2047710302",
    "type": "article"
  },
  {
    "title": "A worst case of circularity test algorithms for attribute grammars",
    "doi": "https://doi.org/10.1145/201059.201064",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Pei‐Chi Wu; Feng-Jian Wang",
    "corresponding_authors": "",
    "abstract": "Although the circularity test problem for attribute grammars (AGs) has been proven to be intrinsically exponential, to date, a worst case for the existing circularity test algorithms has yet to be presented. This note presents a worst-case AG in which the number of incomparable dependency graphs induced at the root is exponential. The worst case can help to clarify the complexity of the problem.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2070324299",
    "type": "article"
  },
  {
    "title": "Technical Correspondence: On LaLonde and des Rivieres' ``Handling Operator Precedence in Arithmetic Expressions with Tree Transformations''",
    "doi": "https://doi.org/10.1145/357195.357203",
    "publication_date": "1983-01-01",
    "publication_year": 1983,
    "authors": "Peter Henderson",
    "corresponding_authors": "Peter Henderson",
    "abstract": "article Free Access Share on Technical Correspondence: On LaLonde and des Rivieres' ``Handling Operator Precedence in Arithmetic Expressions with Tree Transformations'' Author: Peter B. Henderson Institute for Computer Science and Technology, National Bureau of Standards, Washington, DC Institute for Computer Science and Technology, National Bureau of Standards, Washington, DCView Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 1Jan. 1983 pp 122–125https://doi.org/10.1145/357195.357203Published:01 January 1983Publication History 0citation232DownloadsMetricsTotal Citations0Total Downloads232Last 12 Months6Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2091922121",
    "type": "article"
  },
  {
    "title": "Type checking concurrent I/O",
    "doi": "https://doi.org/10.1145/203095.203097",
    "publication_date": "1995-05-01",
    "publication_year": 1995,
    "authors": "W. Homer Carlisle",
    "corresponding_authors": "W. Homer Carlisle",
    "abstract": "In parallel programming languages multityped data structures may be shared by two or more processes. Process I/O to these structures is assumed to be physically interleaved but logically parallel. This article addresses a syntactic mechanism to specify a type for such structures and extends an example language and its type-checking algorithm to these structures.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2156140543",
    "type": "article"
  },
  {
    "title": "Guest Editor's Introduction",
    "doi": "https://doi.org/10.1145/2166.357213",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Richard A. DeMillo",
    "corresponding_authors": "Richard A. DeMillo",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234947083",
    "type": "article"
  },
  {
    "title": "ACM Algorithms Policy",
    "doi": "https://doi.org/10.1145/357172.357180",
    "publication_date": "1982-07-01",
    "publication_year": 1982,
    "authors": "Fred T. Krogh",
    "corresponding_authors": "Fred T. Krogh",
    "abstract": "article Free AccessACM Algorithms Policy Author: Fred T. Krogh View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 4Issue 3July 1982 pp 518–521https://doi.org/10.1145/357172.357180Published:01 July 1982Publication History 0citation195DownloadsMetricsTotal Citations0Total Downloads195Last 12 Months14Last 6 weeks8 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242118066",
    "type": "article"
  },
  {
    "title": "Corrigenda",
    "doi": "https://doi.org/10.1145/69575.357546",
    "publication_date": "1983-10-01",
    "publication_year": 1983,
    "authors": "Thomas Reps; Tim Tietelbaum; Alan Demers",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249089906",
    "type": "article"
  },
  {
    "title": "ACM Algorithms Policy",
    "doi": "https://doi.org/10.1145/2166.357223",
    "publication_date": "1983-07-01",
    "publication_year": 1983,
    "authors": "Fred T. Krogh",
    "corresponding_authors": "Fred T. Krogh",
    "abstract": "article Free Access Share on ACM Algorithms Policy Author: F. T. Krogh View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 5Issue 3pp 502–505https://doi.org/10.1145/2166.357223Published:01 July 1983Publication History 0citation163DownloadsMetricsTotal Citations0Total Downloads163Last 12 Months3Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255786195",
    "type": "article"
  },
  {
    "title": "Reply to \"Subtypes and Quantification\".",
    "doi": null,
    "publication_date": "1991-01-01",
    "publication_year": 1991,
    "authors": "F. Warren Burton",
    "corresponding_authors": "F. Warren Burton",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W15902113",
    "type": "article"
  },
  {
    "title": "Technical Correspondence: Comments on Soisalon-Soininen's ``Inessential Error Entries and Their Use in LR Parser Optimization''",
    "doi": "https://doi.org/10.1145/579.357255",
    "publication_date": "1984-07-01",
    "publication_year": 1984,
    "authors": "Wilf R. LaLonde",
    "corresponding_authors": "Wilf R. LaLonde",
    "abstract": "article Free Access Share on Technical Correspondence: Comments on Soisalon-Soininen's ``Inessential Error Entries and Their Use in LR Parser Optimization'' Author: Wilf R. LaLonde School of Computer Science, Carleton University, Colonel By Drive, Ottawa, Canada K1S 5B6 School of Computer Science, Carleton University, Colonel By Drive, Ottawa, Canada K1S 5B6View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 3pp 432–439https://doi.org/10.1145/579.357255Published:01 July 1984Publication History 0citation244DownloadsMetricsTotal Citations0Total Downloads244Last 12 Months8Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1967336974",
    "type": "article"
  },
  {
    "title": "A short proof of a conjecture of De Remer and Pennello",
    "doi": "https://doi.org/10.1145/5397.30850",
    "publication_date": "1986-04-01",
    "publication_year": 1986,
    "authors": "Thomas J. Sager",
    "corresponding_authors": "Thomas J. Sager",
    "abstract": "In this paper we offer a short proof of the DeRemer-Pennello conjecture that if the LR(0) automaton for a grammar G contains a state p and a nonterminal A such that ( p , A ) is a nonterminal transition, ( p , A ) includes + ( p , A ) and Read ( p , A ) is not empty, then grammar G is not LR( k ) for any k .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1978547655",
    "type": "article"
  },
  {
    "title": "Some comments on “a denotational semantics for Prolog”",
    "doi": "https://doi.org/10.1145/177492.177605",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Bijan Arbab; Daniel M. Berry",
    "corresponding_authors": "",
    "abstract": "Two independently derived denotational semantics for Prolog are contrasted, Arbab and Berry's for the full language and Nicholson and Foo's for a databaseless language. Using the ideas suggested by the former, the latter can be easily extended to include the database operations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2042608853",
    "type": "article"
  },
  {
    "title": "Guest editor's introduction to the special section on the third international conference on computer languages",
    "doi": "https://doi.org/10.1145/128861.128863",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "Alexander L. Wolf",
    "corresponding_authors": "Alexander L. Wolf",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2058741001",
    "type": "article"
  },
  {
    "title": "Subtypes and quantification",
    "doi": "https://doi.org/10.1145/115372.214523",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Dennis Volpano",
    "corresponding_authors": "Dennis Volpano",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2059630968",
    "type": "article"
  },
  {
    "title": "Strictness optimization for graph reduction machines (why id might not be strict)",
    "doi": "https://doi.org/10.1145/186025.186040",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Marcel Beemster",
    "corresponding_authors": "Marcel Beemster",
    "abstract": "Strictness optimizations in the implementation of lazy functional languages are not always valid. In nonoptimized graph reduction, evaluation always takes place at the request of case analysis or a primitive operation. Hence, the result of a reduction is always a data value and never a function. This implies that in an implementation no argument satisfaction check is required. But in the presence of strict arguments, “premature” reduction may take place outside the scope of a case or primitive operation. This causes problems in graph reducers that use an aggressive take . Two solutions are presented, one based on a run-time argument satisfaction check, the other on a weakened strictness analyzer. Experimental results are used to compare the two solutions and show that the cost of the aggressive take can be arbitrarily high for specific programs. The experimental results enable a trade-off to be made by the reduction machine designer.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2171059373",
    "type": "article"
  },
  {
    "title": "ACM Algorithms Policy",
    "doi": "https://doi.org/10.1145/579.357256",
    "publication_date": "1984-07-01",
    "publication_year": 1984,
    "authors": "Fred T. Krogh",
    "corresponding_authors": "Fred T. Krogh",
    "abstract": "article Free Access Share on ACM Algorithms Policy Author: F. T. Krogh View Profile Authors Info & Claims ACM Transactions on Programming Languages and SystemsVolume 6Issue 3July 1984 pp 440–443https://doi.org/10.1145/579.357256Published:01 July 1984Publication History 0citation184DownloadsMetricsTotal Citations0Total Downloads184Last 12 Months5Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244866522",
    "type": "article"
  }
]