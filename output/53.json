[
  {
    "title": "The dynamics of viral marketing",
    "doi": "https://doi.org/10.1145/1232722.1232727",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Jure Leskovec; Lada A. Adamic; Bernardo A. Huberman",
    "corresponding_authors": "",
    "abstract": "We present an analysis of a person-to-person recommendation network, consisting of 4 million people who made 16 million recommendations on half a million products. We observe the propagation of recommendations and the cascade sizes, which we explain by a simple stochastic model. We analyze how user behavior varies within user communities defined by a recommendation network. Product purchases follow a ‘long tail’ where a significant share of purchases belongs to rarely sold items. We establish how the recommendation network grows over time and how effective it is from the viewpoint of the sender and receiver of the recommendations. While on average recommendations are not very effective at inducing purchases and do not spread very far, we present a model that successfully identifies communities, product, and pricing categories for which viral marketing seems to be very effective.",
    "cited_by_count": 2085,
    "openalex_id": "https://openalex.org/W1994473607",
    "type": "article"
  },
  {
    "title": "Efficient algorithms for Web services selection with end-to-end QoS constraints",
    "doi": "https://doi.org/10.1145/1232722.1232728",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Tao Yu; Yue Zhang; Kwei-Jay Lin",
    "corresponding_authors": "",
    "abstract": "Service-Oriented Architecture (SOA) provides a flexible framework for service composition. Using standard-based protocols (such as SOAP and WSDL), composite services can be constructed by integrating atomic services developed independently. Algorithms are needed to select service components with various QoS levels according to some application-dependent performance requirements. We design a broker-based architecture to facilitate the selection of QoS-based services. The objective of service selection is to maximize an application-specific utility function under the end-to-end QoS constraints. The problem is modeled in two ways: the combinatorial model and the graph model. The combinatorial model defines the problem as a multidimension multichoice 0-1 knapsack problem (MMKP). The graph model defines the problem as a multiconstraint optimal path (MCOP) problem. Efficient heuristic algorithms for service processes of different composition structures are presented in this article and their performances are studied by simulations. We also compare the pros and cons between the two models.",
    "cited_by_count": 1181,
    "openalex_id": "https://openalex.org/W1965810726",
    "type": "article"
  },
  {
    "title": "Recommending friends and locations based on individual location history",
    "doi": "https://doi.org/10.1145/1921591.1921596",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Yu Zheng; Lizhu Zhang; Zhengxin Ma; Xing Xie; Wei‐Ying Ma",
    "corresponding_authors": "",
    "abstract": "The increasing availability of location-acquisition technologies (GPS, GSM networks, etc.) enables people to log the location histories with spatio-temporal data. Such real-world location histories imply, to some extent, users' interests in places, and bring us opportunities to understand the correlation between users and locations. In this article, we move towards this direction and report on a personalized friend and location recommender for the geographical information systems (GIS) on the Web. First, in this recommender system, a particular individual's visits to a geospatial region in the real world are used as their implicit ratings on that region. Second, we measure the similarity between users in terms of their location histories and recommend to each user a group of potential friends in a GIS community. Third, we estimate an individual's interests in a set of unvisited regions by involving his/her location history and those of other users. Some unvisited locations that might match their tastes can be recommended to the individual. A framework, referred to as a hierarchical-graph-based similarity measurement (HGSM), is proposed to uniformly model each individual's location history, and effectively measure the similarity among users. In this framework, we take into account three factors: 1) the sequence property of people's outdoor movements, 2) the visited popularity of a geospatial region, and 3) the hierarchical property of geographic spaces. Further, we incorporated a content-based method into a user-based collaborative filtering algorithm, which uses HGSM as the user similarity measure, to estimate the rating of a user on an item. We evaluated this recommender system based on the GPS data collected by 75 subjects over a period of 1 year in the real world. As a result, HGSM outperforms related similarity measures, namely similarity-by-count, cosine similarity, and Pearson similarity measures. Moreover, beyond the item-based CF method and random recommendations, our system provides users with more attractive locations and better user experiences of recommendation.",
    "cited_by_count": 481,
    "openalex_id": "https://openalex.org/W1982397092",
    "type": "article"
  },
  {
    "title": "Understanding transportation modes based on GPS data for web applications",
    "doi": "https://doi.org/10.1145/1658373.1658374",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Yu Zheng; Yukun Chen; Quannan Li; Xing Xie; Wei‐Ying Ma",
    "corresponding_authors": "",
    "abstract": "User mobility has given rise to a variety of Web applications, in which the global positioning system (GPS) plays many important roles in bridging between these applications and end users. As a kind of human behavior, transportation modes, such as walking and driving, can provide pervasive computing systems with more contextual information and enrich a user's mobility with informative knowledge. In this article, we report on an approach based on supervised learning to automatically infer users' transportation modes, including driving, walking, taking a bus and riding a bike, from raw GPS logs. Our approach consists of three parts: a change point-based segmentation method, an inference model and a graph-based post-processing algorithm. First, we propose a change point-based segmentation method to partition each GPS trajectory into separate segments of different transportation modes. Second, from each segment, we identify a set of sophisticated features, which are not affected by differing traffic conditions (e.g., a person's direction when in a car is constrained more by the road than any change in traffic conditions). Later, these features are fed to a generative inference model to classify the segments of different modes. Third, we conduct graph-based postprocessing to further improve the inference performance. This postprocessing algorithm considers both the commonsense constraints of the real world and typical user behaviors based on locations in a probabilistic manner. The advantages of our method over the related works include three aspects. (1) Our approach can effectively segment trajectories containing multiple transportation modes. (2) Our work mined the location constraints from user-generated GPS logs, while being independent of additional sensor data and map information like road networks and bus stops. (3) The model learned from the dataset of some users can be applied to infer GPS data from others. Using the GPS logs collected by 65 people over a period of 10 months, we evaluated our approach via a set of experiments. As a result, based on the change-point-based segmentation method and Decision Tree-based inference model, we achieved prediction accuracy greater than 71 percent. Further, using the graph-based post-processing algorithm, the performance attained a 4-percent enhancement.",
    "cited_by_count": 467,
    "openalex_id": "https://openalex.org/W2022749020",
    "type": "article"
  },
  {
    "title": "Comparison of collaborative filtering algorithms",
    "doi": "https://doi.org/10.1145/1921591.1921593",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Fidel Cacheda; Víctor Carneiro; Diego Fernández; Vreixo Formoso",
    "corresponding_authors": "",
    "abstract": "The technique of collaborative filtering is especially successful in generating personalized recommendations. More than a decade of research has resulted in numerous algorithms, although no comparison of the different strategies has been made. In fact, a universally accepted way of evaluating a collaborative filtering algorithm does not exist yet. In this work, we compare different techniques found in the literature, and we study the characteristics of each one, highlighting their principal strengths and weaknesses. Several experiments have been performed, using the most popular metrics and algorithms. Moreover, two new metrics designed to measure the precision on good items have been proposed. The results have revealed the weaknesses of many algorithms in extracting information from user profiles especially under sparsity conditions. We have also confirmed the good results of SVD-based techniques already reported by other authors. As an alternative, we present a new approach based on the interpretation of the tendencies or differences between users and items. Despite its extraordinary simplicity, in our experiments, it obtained noticeably better results than more complex algorithms. In fact, in the cases analyzed, its results are at least equivalent to those of the best approaches studied. Under sparsity conditions, there is more than a 20% improvement in accuracy over the traditional user-based algorithms, while maintaining over 90% coverage. Moreover, it is much more efficient computationally than any other algorithm, making it especially adequate for large amounts of data.",
    "cited_by_count": 437,
    "openalex_id": "https://openalex.org/W2081332171",
    "type": "article"
  },
  {
    "title": "Friendship prediction and homophily in social media",
    "doi": "https://doi.org/10.1145/2180861.2180866",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Luca Maria Aiello; Alain Barrat; Rossano Schifanella; Ciro Cattuto; Benjamin Markines; Filippo Menczer",
    "corresponding_authors": "",
    "abstract": "Social media have attracted considerable attention because their open-ended nature allows users to create lightweight semantic scaffolding to organize and share content. To date, the interplay of the social and topical components of social media has been only partially explored. Here, we study the presence of homophily in three systems that combine tagging social media with online social networks. We find a substantial level of topical similarity among users who are close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local similarity between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar interests are more likely to be friends, and therefore topical similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on several datasets, confirming that social networks constructed from topical similarity capture actual friendship accurately. When combined with topological features, topical similarity achieves a link prediction accuracy of about 92%.",
    "cited_by_count": 427,
    "openalex_id": "https://openalex.org/W2118027753",
    "type": "article"
  },
  {
    "title": "Trust and nuanced profile similarity in online social networks",
    "doi": "https://doi.org/10.1145/1594173.1594174",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Jennifer Golbeck",
    "corresponding_authors": "Jennifer Golbeck",
    "abstract": "Online social networks, where users maintain lists of friends and express their preferences for items like movies, music, or books, are very popular. The Web-based nature of this information makes it ideal for use in a variety of intelligent systems that can take advantage of the users' social and personal data. For those systems to be effective, however, it is important to understand the relationship between social and personal preferences. In this work we investigate features of profile similarity and how those relate to the way users determine trust. Through a controlled study, we isolate several profile features beyond overall similarity that affect how much subjects trust hypothetical users. We then use data from FilmTrust, a real social network where users rate movies, and show that the profile features discovered in the experiment allow us to more accurately predict trust than when using only overall similarity. In this article, we present these experimental results and discuss the potential implications for using trust in user interfaces.",
    "cited_by_count": 384,
    "openalex_id": "https://openalex.org/W2045049260",
    "type": "article"
  },
  {
    "title": "Crawling Ajax-Based Web Applications through Dynamic Analysis of User Interface State Changes",
    "doi": "https://doi.org/10.1145/2109205.2109208",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Ali Mesbah; Arie van Deursen; Stefan Lenselink",
    "corresponding_authors": "",
    "abstract": "Using JavaScript and dynamic DOM manipulation on the client side of Web applications is becoming a widespread approach for achieving rich interactivity and responsiveness in modern Web applications. At the same time, such techniques---collectively known as Ajax ---shatter the concept of webpages with unique URLs, on which traditional Web crawlers are based. This article describes a novel technique for crawling Ajax -based applications through automatic dynamic analysis of user-interface-state changes in Web browsers. Our algorithm scans the DOM tree, spots candidate elements that are capable of changing the state, fires events on those candidate elements, and incrementally infers a state machine that models the various navigational paths and states within an Ajax application. This inferred model can be used in program comprehension and in analysis and testing of dynamic Web states, for instance, or for generating a static version of the application. In this article, we discuss our sequential and concurrent Ajax crawling algorithms. We present our open source tool called Crawljax , which implements the concepts and algorithms discussed in this article. Additionally, we report a number of empirical studies in which we apply our approach to a number of open-source and industrial Web applications and elaborate on the obtained results.",
    "cited_by_count": 291,
    "openalex_id": "https://openalex.org/W2011539648",
    "type": "article"
  },
  {
    "title": "Polarization and Fake News",
    "doi": "https://doi.org/10.1145/3316809",
    "publication_date": "2019-03-27",
    "publication_year": 2019,
    "authors": "Michela Del Vicario; Walter Quattrociocchi; Antonio Scala; Fabiana Zollo",
    "corresponding_authors": "",
    "abstract": "Users’ polarization and confirmation bias play a key role in misinformation spreading on online social media. Our aim is to use this information to determine in advance potential targets for hoaxes and fake news. In this article, we introduce a framework for promptly identifying polarizing content on social media and, thus, “predicting” future fake news topics. We validate the performances of the proposed methodology on a massive Italian Facebook dataset, showing that we are able to identify topics that are susceptible to misinformation with 77% accuracy. Moreover, such information may be embedded as a new feature in an additional classifier able to recognize fake news with 91% accuracy. The novelty of our approach consists in taking into account a series of characteristics related to users’ behavior on online social media such as Facebook, making a first, important step towards the mitigation of misinformation phenomena by supporting the identification of potential misinformation targets and thus the design of tailored counter-narratives.",
    "cited_by_count": 255,
    "openalex_id": "https://openalex.org/W2963877803",
    "type": "article"
  },
  {
    "title": "A hybrid approach for efficient Web service composition with end-to-end QoS constraints",
    "doi": "https://doi.org/10.1145/2180861.2180864",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Mohammad Alrifai; Thomas Risse; Wolfgang Nejdl",
    "corresponding_authors": "",
    "abstract": "Dynamic selection of Web services at runtime is important for building flexible and loosely-coupled service-oriented applications. An abstract description of the required services is provided at design-time, and matching service offers are located at runtime. With the growing number of Web services that provide the same functionality but differ in quality parameters (e.g., availability, response time), a decision needs to be made on which services should be selected such that the user's end-to-end QoS requirements are satisfied. Although very efficient, local selection strategy fails short in handling global QoS requirements. Solutions based on global optimization, on the other hand, can handle global constraints, but their poor performance renders them inappropriate for applications with dynamic and realtime requirements. In this article we address this problem and propose a hybrid solution that combines global optimization with local selection techniques to benefit from the advantages of both worlds. The proposed solution consists of two steps: first, we use mixed integer programming (MIP) to find the optimal decomposition of global QoS constraints into local constraints. Second, we use distributed local selection to find the best Web services that satisfy these local constraints. The results of experimental evaluation indicate that our approach significantly outperforms existing solutions in terms of computation time while achieving close-to-optimal results.",
    "cited_by_count": 230,
    "openalex_id": "https://openalex.org/W2071104838",
    "type": "article"
  },
  {
    "title": "“HOT” ChatGPT: The Promise of ChatGPT in Detecting and Discriminating Hateful, Offensive, and Toxic Comments on Social Media",
    "doi": "https://doi.org/10.1145/3643829",
    "publication_date": "2024-02-02",
    "publication_year": 2024,
    "authors": "Lingyao Li; Lizhou Fan; Shubham Atreja; Libby Hemphill",
    "corresponding_authors": "",
    "abstract": "Harmful textual content is pervasive on social media, poisoning online communities and negatively impacting participation. A common approach to this issue is developing detection models that rely on human annotations. However, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. Generative AI models have the potential to understand and detect harmful textual content. We used ChatGPT to investigate this potential and compared its performance with MTurker annotations for three frequently discussed concepts related to harmful textual content on social media: Hateful, Offensive, and Toxic (HOT). We designed five prompts to interact with ChatGPT and conducted four experiments eliciting HOT classifications. Our results show that ChatGPT can achieve an accuracy of approximately 80% when compared to MTurker annotations. Specifically, the model displays a more consistent classification for non-HOT comments than HOT comments compared to human annotations. Our findings also suggest that ChatGPT classifications align with the provided HOT definitions. However, ChatGPT classifies “hateful” and “offensive” as subsets of “toxic.” Moreover, the choice of prompts used to interact with ChatGPT impacts its performance. Based on these insights, our study provides several meaningful implications for employing ChatGPT to detect HOT content, particularly regarding the reliability and consistency of its performance, its understanding and reasoning of the HOT concept, and the impact of prompts on its performance. Overall, our study provides guidance on the potential of using generative AI models for moderating large volumes of user-generated textual content on social media.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W4391473457",
    "type": "article"
  },
  {
    "title": "Envisioning Information Access Systems: What Makes for Good Tools and a Healthy Web?",
    "doi": "https://doi.org/10.1145/3649468",
    "publication_date": "2024-02-26",
    "publication_year": 2024,
    "authors": "Chirag Shah; Emily M. Bender",
    "corresponding_authors": "",
    "abstract": "We observe a recent trend toward applying large language models (LLMs) in search and positioning them as effective information access systems. While the interfaces may look appealing and the apparent breadth of applicability is exciting, we are concerned that the field is rushing ahead with a technology without sufficient study of the uses it is meant to serve, how it would be used, and what its use would mean. We argue that it is important to reassert the central research focus of the field of information retrieval, because information access is not merely an application to be solved by the so-called ‘AI’ techniques du jour. Rather, it is a key human activity, with impacts on both individuals and society. As information scientists, we should be asking what do people and society want and need from information access systems and how do we design and build systems to meet those needs? With that goal, in this conceptual article we investigate fundamental questions concerning information access from user and societal viewpoints. We revisit foundational work related to information behavior, information seeking, information retrieval, information filtering, and information access to resurface what we know about these fundamental questions and what may be missing. We then provide our conceptual framing about how we could fill this gap, focusing on methods as well as experimental and evaluation frameworks. We consider the Web as an information ecosystem and explore the ways in which synthetic media, produced by LLMs and otherwise, endangers that ecosystem. The primary goal of this conceptual article is to shed light on what we still do not know about the potential impacts of LLM-based information access systems, how to advance our understanding of user behaviors, and where the next generations of students, scholars, and developers could fruitfully invest their energies.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W4392163191",
    "type": "article"
  },
  {
    "title": "Declarative specification and verification of service choreographiess",
    "doi": "https://doi.org/10.1145/1658373.1658376",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Marco Montali; Maja Pešić; Wil M. P. van der Aalst; Federico Chesani; Paola Mello; Sergio Storari",
    "corresponding_authors": "",
    "abstract": "Service-oriented computing, an emerging paradigm for architecting and implementing business collaborations within and across organizational boundaries, is currently of interest to both software vendors and scientists. While the technologies for implementing and interconnecting basic services are reaching a good level of maturity, modeling service interaction from a global viewpoint, that is, representing service choreographies, is still an open challenge. The main problem is that, although declarativeness has been identified as a key feature, several proposed approaches specify choreographies by focusing on procedural aspects, leading to over-constrained and over-specified models. To overcome these limits, we propose to adopt DecSerFlow, a truly declarative language, to model choreographies. Thanks to its declarative nature, DecSerFlow semantics can be given in terms of logic-based languages. In particular, we present how DecSerFlow can be mapped onto Linear Temporal Logic and onto Abductive Logic Programming . We show how the mappings onto both formalisms can be concretely exploited to address the enactment of DecSerFlow models, to enrich its expressiveness and to perform a variety of different verification tasks. We illustrate the advantages of using a declarative language in conjunction with logic-based semantics by applying our approach to a running example.",
    "cited_by_count": 231,
    "openalex_id": "https://openalex.org/W2022078754",
    "type": "article"
  },
  {
    "title": "Not quite the average",
    "doi": "https://doi.org/10.1145/1326561.1326566",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Harald Weinreich; Hartmut Obendorf; Eelco Herder; Matthias Mayer",
    "corresponding_authors": "",
    "abstract": "In the past decade, the World Wide Web has been subject to dramatic changes. Web sites have evolved from static information resources to dynamic and interactive applications that are used for a broad scope of activities on a daily basis. To examine the consequences of these changes on user behavior, we conducted a long-term client-side Web usage study with twenty-five participants. This report presents results of this study and compares the user behavior with previous long-term browser usage studies, which range in age from seven to thirteen years. Based on the empirical data and the interview results, various implications for the interface design of browsers and Web sites are discussed. A major finding is the decreasing prominence of backtracking in Web navigation. This can largely be attributed to the increasing importance of dynamic, service-oriented Web sites. Users do not navigate on these sites searching for information, but rather interact with an online application to complete certain tasks. Furthermore, the usage of multiple windows and tabs has partly replaced back button usage, posing new challenges for user orientation and backtracking. We found that Web browsing is a rapid activity even for pages with substantial content, which calls for page designs that allow for cursory reading. Click maps provide additional information on how users interact with the Web on page level. Finally, substantial differences were observed between users, and characteristic usage patterns for different types of Web sites emphasize the need for more adaptive and customizable Web browsers.",
    "cited_by_count": 225,
    "openalex_id": "https://openalex.org/W2077124362",
    "type": "article"
  },
  {
    "title": "Methods for extracting place semantics from Flickr tags",
    "doi": "https://doi.org/10.1145/1462148.1462149",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Tye Rattenbury; Mor Naaman",
    "corresponding_authors": "",
    "abstract": "We describe an approach for extracting semantics for tags, unstructured text-labels assigned to resources on the Web, based on each tag's usage patterns. In particular, we focus on the problem of extracting place semantics for tags that are assigned to photos on Flickr, a popular-photo sharing Web site that supports location (latitude/longitude) metadata for photos. We propose the adaptation of two baseline methods, inspired by well-known burst-analysis techniques, for the task; we also describe two novel methods, TagMaps and scale-structure identification. We evaluate the methods on a subset of Flickr data. We show that our scale-structure identification method outperforms existing techniques and that a hybrid approach generates further improvements (achieving 85% precision at 81% recall). The approach and methods described in this work can be used in other domains such as geo-annotated Web pages, where text terms can be extracted and associated with usage patterns.",
    "cited_by_count": 212,
    "openalex_id": "https://openalex.org/W1976050545",
    "type": "article"
  },
  {
    "title": "Automatic tag recommendation algorithms for social recommender systems",
    "doi": "https://doi.org/10.1145/1921591.1921595",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Yang Song; Lu Zhang; C. Lee Giles",
    "corresponding_authors": "",
    "abstract": "The emergence of Web 2.0 and the consequent success of social network Web sites such as Del.icio.us and Flickr introduce us to a new concept called social bookmarking, or tagging. Tagging is the action of connecting a relevant user-defined keyword to a document, image, or video, which helps the user to better organize and share their collections of interesting stuff. With the rapid growth of Web 2.0, tagged data is becoming more and more abundant on the social network Web sites. An interesting problem is how to automate the process of making tag recommendations to users when a new resource becomes available. In this article, we address the issue of tag recommendation from a machine learning perspective. From our empirical observation of two large-scale datasets, we first argue that the user-centered approach for tag recommendation is not very effective in practice. Consequently, we propose two novel document-centered approaches that are capable of making effective and efficient tag recommendations in real scenarios. The first, graph-based, method represents the tagged data in two bipartite graphs, (document, tag) and (document, word), then finds document topics by leveraging graph partitioning algorithms. The second, prototype-based, method aims at finding the most representative documents within the data collections and advocates a sparse multiclass Gaussian process classifier for efficient document classification. For both methods, tags are ranked within each topic cluster/class by a novel ranking method. Recommendations are performed by first classifying a new document into one or more topic clusters/classes, and then selecting the most relevant tags from those clusters/classes as machine-recommended tags. Experiments on real-world data from Del.icio.us, CiteULike, and BibSonomy examine the quality of tag recommendation as well as the efficiency of our recommendation algorithms. The results suggest that our document-centered models can substantially improve the performance of tag recommendations when compared to the user-centered methods, as well as topic models LDA and SVM classifiers.",
    "cited_by_count": 174,
    "openalex_id": "https://openalex.org/W1976859382",
    "type": "article"
  },
  {
    "title": "Beyond Social Graphs",
    "doi": "https://doi.org/10.1145/2382616.2382620",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Christo Wilson; Alessandra Sala; Krishna P. N. Puttaswamy; Ben Y. Zhao",
    "corresponding_authors": "",
    "abstract": "Social networks are popular platforms for interaction, communication, and collaboration between friends. Researchers have recently proposed an emerging class of applications that leverage relationships from social networks to improve security and performance in applications such as email, Web browsing, and overlay routing. While these applications often cite social network connectivity statistics to support their designs, researchers in psychology and sociology have repeatedly cast doubt on the practice of inferring meaningful relationships from social network connections alone. This leads to the question: “Are social links valid indicators of real user interaction? If not, then how can we quantify these factors to form a more accurate model for evaluating socially enhanced applications?” In this article, we address this question through a detailed study of user interactions in the Facebook social network. We propose the use of “interaction graphs” to impart meaning to online social links by quantifying user interactions. We analyze interaction graphs derived from Facebook user traces and show that they exhibit significantly lower levels of the “small-world” properties present in their social graph counterparts. This means that these graphs have fewer “supernodes” with extremely high degree, and overall graph diameter increases significantly as a result. To quantify the impact of our observations, we use both types of graphs to validate several well-known social-based applications that rely on graph properties to infuse new functionality into Internet applications, including Reliable Email (RE), SybilGuard, and the weighted cascade influence maximization algorithm. The results reveal new insights into each of these systems, and confirm our hypothesis that to obtain realistic and accurate results, ongoing research on social network applications studies of social applications should use real indicators of user interactions in lieu of social graphs.",
    "cited_by_count": 166,
    "openalex_id": "https://openalex.org/W2118129996",
    "type": "article"
  },
  {
    "title": "A distributed service-oriented architecture for business process execution",
    "doi": "https://doi.org/10.1145/1658373.1658375",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Guoli Li; Vinod Muthusamy; Hans‐Arno Jacobsen",
    "corresponding_authors": "",
    "abstract": "The Business Process Execution Language (BPEL) standardizes the development of composite enterprise applications that make use of software components exposed as Web services. BPEL processes are currently executed by a centralized orchestration engine, in which issues such as scalability, platform heterogeneity, and division across administrative domains can be difficult to manage. We propose a distributed agent-based orchestration engine in which several lightweight agents execute a portion of the original business process and collaborate in order to execute the complete process. The complete set of standard BPEL activities are supported, and the transformations of several BPEL activities to the agent-based architecture are described. Evaluations of an implementation of this architecture demonstrate that agent-based execution scales better than a non-distributed approach, with at least 70% and 120% improvements in process execution time, and throughput, respectively, even with a large number of concurrent process instances. In addition, the distributed architecture successfully executes large processes that are shown to be infeasible to execute with a nondistributed engine.",
    "cited_by_count": 164,
    "openalex_id": "https://openalex.org/W2027402731",
    "type": "article"
  },
  {
    "title": "Mobile information access",
    "doi": "https://doi.org/10.1145/1232722.1232726",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Karen Church; Barry Smyth; Paul Cotter; Keith Bradley",
    "corresponding_authors": "",
    "abstract": "It is likely that mobile phones will soon come to rival more traditional devices as the primary platform for information access. Consequently, it is important to understand the emerging information access behavior of mobile Internet (MI) users especially in relation to their use of mobile handsets for information browsing and query-based search. In this article, we describe the results of a recent analysis of the MI habits of more than 600,000 European MI users, with a particular emphasis on the emerging interest in mobile search. We consider a range of factors including whether there are key differences between browsing and search behavior on the MI compared to the Web. We highlight how browsing continues to dominate mobile information access, but go on to show how search is becoming an increasingly popular information access alternative especially in relation to certain types of mobile handsets and information needs. Moreover, we show that sessions involving search tend to be longer and more data-rich than those that do not involve search. We also look at the type of queries used during mobile search and the way that these queries tend to be modified during the course of a mobile search session. Finally we examine the overlap among mobile search queries and the different topics mobile users are interested in.",
    "cited_by_count": 159,
    "openalex_id": "https://openalex.org/W1970123191",
    "type": "article"
  },
  {
    "title": "A Comprehensive Survey and Classification of Approaches for Community Question Answering",
    "doi": "https://doi.org/10.1145/2934687",
    "publication_date": "2016-08-16",
    "publication_year": 2016,
    "authors": "Ivan Srba; Mária Bieliková",
    "corresponding_authors": "",
    "abstract": "Community question-answering (CQA) systems, such as Yahoo! Answers or Stack Overflow, belong to a prominent group of successful and popular Web 2.0 applications, which are used every day by millions of users to find an answer on complex, subjective, or context-dependent questions. In order to obtain answers effectively, CQA systems should optimally harness collective intelligence of the whole online community, which will be impossible without appropriate collaboration support provided by information technologies. Therefore, CQA became an interesting and promising subject of research in computer science and now we can gather the results of 10 years of research. Nevertheless, in spite of the increasing number of publications emerging each year, so far the research on CQA systems has missed a comprehensive state-of-the-art survey. We attempt to fill this gap by a review of 265 articles published between 2005 and 2014, which were selected from major conferences and journals. According to this evaluation, at first we propose a framework that defines descriptive attributes of CQA approaches. Second, we introduce a classification of all approaches with respect to problems they are aimed to solve. The classification is consequently employed in a review of a significant number of representative approaches, which are described by means of attributes from the descriptive framework. As a part of the survey, we also depict the current trends as well as highlight the areas that require further attention from the research community.",
    "cited_by_count": 149,
    "openalex_id": "https://openalex.org/W2514077680",
    "type": "article"
  },
  {
    "title": "Emergence of consensus and shared vocabularies in collaborative tagging systems",
    "doi": "https://doi.org/10.1145/1594173.1594176",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Valentin Robu; Harry Halpin; Hana Shepherd",
    "corresponding_authors": "",
    "abstract": "This article uses data from the social bookmarking site del.icio.us to empirically examine the dynamics of collaborative tagging systems and to study how coherent categorization schemes emerge from unsupervised tagging by individual users. First, we study the formation of stable distributions in tagging systems, seen as an implicit form of “consensus” reached by the users of the system around the tags that best describe a resource. We show that final tag frequencies for most resources converge to power law distributions and we propose an empirical method to examine the dynamics of the convergence process, based on the Kullback-Leibler divergence measure. The convergence analysis is performed for both the most utilized tags at the top of tag distributions and the so-called long tail. Second, we study the information structures that emerge from collaborative tagging, namely tag correlation (or folksonomy) graphs. We show how community-based network techniques can be used to extract simple tag vocabularies from the tag correlation graphs by partitioning them into subsets of related tags. Furthermore, we also show, for a specialized domain, that shared vocabularies produced by collaborative tagging are richer than the vocabularies which can be extracted from large-scale query logs provided by a major search engine. Although the empirical analysis presented in this article is based on a set of tagging data obtained from del.icio.us, the methods developed are general, and the conclusions should be applicable across other websites that employ tagging.",
    "cited_by_count": 136,
    "openalex_id": "https://openalex.org/W2157700119",
    "type": "article"
  },
  {
    "title": "Browser Fingerprinting",
    "doi": "https://doi.org/10.1145/3386040",
    "publication_date": "2020-04-09",
    "publication_year": 2020,
    "authors": "Pierre Laperdrix; Nataliia Bielova; Benoît Baudry; Gildas Avoine",
    "corresponding_authors": "",
    "abstract": "With this article, we survey the research performed in the domain of browser fingerprinting, while providing an accessible entry point to newcomers in the field. We explain how this technique works and where it stems from. We analyze the related work in detail to understand the composition of modern fingerprints and see how this technique is currently used online. We systematize existing defense solutions into different categories and detail the current challenges yet to overcome.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W3015392158",
    "type": "article"
  },
  {
    "title": "Progress on Website Accessibility?",
    "doi": "https://doi.org/10.1145/2435215.2435217",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Vicki L. Hanson; John T. Richards",
    "corresponding_authors": "",
    "abstract": "Over 100 top-traffic and government websites from the United States and United Kingdom were examined for evidence of changes on accessibility indicators over the 14-year period from 1999 to 2012, the longest period studied to date. Automated analyses of WCAG 2.0 Level A Success Criteria found high percentages of violations overall. Unlike more circumscribed studies, however, these sites exhibited improvements over the years on a number of accessibility indicators, with government sites being less likely than topsites to have accessibility violations. Examination of the causes of success and failure suggests that improving accessibility may be due, in part, to changes in website technologies and coding practices rather than a focus on accessibility per se.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2144882903",
    "type": "article"
  },
  {
    "title": "Detecting Cyberbullying and Cyberaggression in Social Media",
    "doi": "https://doi.org/10.1145/3343484",
    "publication_date": "2019-08-31",
    "publication_year": 2019,
    "authors": "Despoina Chatzakou; Ilias Leontiadis; Jeremy Blackburn; Emiliano De Cristofaro; Gianluca Stringhini; Athena Vakali; Nicolas Kourtellis",
    "corresponding_authors": "",
    "abstract": "Cyberbullying and cyberaggression are increasingly worrisome phenomena affecting people across all demographics. More than half of young social media users worldwide have been exposed to such prolonged and/or coordinated digital harassment. Victims can experience a wide range of emotions, with negative consequences such as embarrassment, depression, isolation from other community members, which embed the risk to lead to even more critical consequences, such as suicide attempts. In this work, we take the first concrete steps to understand the characteristics of abusive behavior in Twitter, one of today’s largest social media platforms. We analyze 1.2 million users and 2.1 million tweets, comparing users participating in discussions around seemingly normal topics like the NBA, to those more likely to be hate-related, such as the Gamergate controversy, or the gender pay inequality at the BBC station. We also explore specific manifestations of abusive behavior, i.e., cyberbullying and cyberaggression, in one of the hate-related communities (Gamergate). We present a robust methodology to distinguish bullies and aggressors from normal Twitter users by considering text, user, and network-based attributes. Using various state-of-the-art machine-learning algorithms, we classify these accounts with over 90% accuracy and AUC. Finally, we discuss the current status of Twitter user accounts marked as abusive by our methodology and study the performance of potential mechanisms that can be used by Twitter to suspend users in the future.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2980963255",
    "type": "article"
  },
  {
    "title": "Extracting and Summarizing Situational Information from the Twitter Social Media during Disasters",
    "doi": "https://doi.org/10.1145/3178541",
    "publication_date": "2018-07-17",
    "publication_year": 2018,
    "authors": "Koustav Rudra; Niloy Ganguly; Pawan Goyal; Saptarshi Ghosh",
    "corresponding_authors": "",
    "abstract": "Microblogging sites like Twitter have become important sources of real-time information during disaster events. A large amount of valuable situational information is posted in these sites during disasters; however, the information is dispersed among hundreds of thousands of tweets containing sentiments and opinions of the masses. To effectively utilize microblogging sites during disaster events, it is necessary to not only extract the situational information from the large amounts of sentiments and opinions, but also to summarize the large amounts of situational information posted in real-time. During disasters in countries like India, a sizable number of tweets are posted in local resource-poor languages besides the normal English-language tweets. For instance, in the Indian subcontinent, a large number of tweets are posted in Hindi/Devanagari (the national language of India), and some of the information contained in such non-English tweets is not available (or available at a later point of time) through English tweets. In this work, we develop a novel classification-summarization framework which handles tweets in both English and Hindi—we first extract tweets containing situational information, and then summarize this information. Our proposed methodology is developed based on the understanding of how several concepts evolve in Twitter during disaster. This understanding helps us achieve superior performance compared to the state-of-the-art tweet classifiers and summarization approaches on English tweets. Additionally, to our knowledge, this is the first attempt to extract situational information from non-English tweets.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W2884234259",
    "type": "article"
  },
  {
    "title": "Understanding latent interactions in online social networks",
    "doi": "https://doi.org/10.1145/2517040",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Jing Jiang; Christo Wilson; Xiao Wang; Wenpeng Sha; Peng Huang; Yafei Dai; Ben Y. Zhao",
    "corresponding_authors": "",
    "abstract": "Popular online social networks (OSNs) like Facebook and Twitter are changing the way users communicate and interact with the Internet. A deep understanding of user interactions in OSNs can provide important insights into questions of human social behavior and into the design of social platforms and applications. However, recent studies have shown that a majority of user interactions on OSNs are latent interactions , that is, passive actions, such as profile browsing, that cannot be observed by traditional measurement techniques. In this article, we seek a deeper understanding of both active and latent user interactions in OSNs. For quantifiable data on latent user interactions, we perform a detailed measurement study on Renren, the largest OSN in China with more than 220 million users to date. All friendship links in Renren are public, allowing us to exhaustively crawl a connected graph component of 42 million users and 1.66 billion social links in 2009. Renren also keeps detailed, publicly viewable visitor logs for each user profile. We capture detailed histories of profile visits over a period of 90 days for users in the Peking University Renren network and use statistics of profile visits to study issues of user profile popularity, reciprocity of profile visits, and the impact of content updates on user popularity. We find that latent interactions are much more prevalent and frequent than active events, are nonreciprocal in nature, and that profile popularity is correlated with page views of content rather than with quantity of content updates. Finally, we construct latent interaction graphs as models of user browsing behavior and compare their structural properties, evolution, community structure, and mixing times against those of both active interaction graphs and social graphs.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W2049297549",
    "type": "article"
  },
  {
    "title": "A Spatial-Temporal QoS Prediction Approach for Time-aware Web Service Recommendation",
    "doi": "https://doi.org/10.1145/2801164",
    "publication_date": "2016-02-08",
    "publication_year": 2016,
    "authors": "Xinyu Wang; Jianke Zhu; Zibin Zheng; Wenjie Song; Yuanhong Shen; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "Due to the popularity of service-oriented architectures for various distributed systems, an increasing number of Web services have been deployed all over the world. Recently, Web service recommendation became a hot research topic, one that aims to accurately predict the quality of functional satisfactory services for each end user. Generally, the performance of Web service changes over time due to variations of service status and network conditions. Instead of employing the conventional temporal models, we propose a novel spatial-temporal QoS prediction approach for time-aware Web service recommendation, where a sparse representation is employed to model QoS variations. Specifically, we make a zero-mean Laplace prior distribution assumption on the residuals of the QoS prediction, which corresponds to a Lasso regression problem. To effectively select the nearest neighbor for the sparse representation of temporal QoS values, the geo-location of web service is employed to reduce searching range while improving prediction accuracy. The extensive experimental results demonstrate that the proposed approach outperforms state-of-art methods with more than 10% improvement on the accuracy of temporal QoS prediction for time-aware Web service recommendation.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2279773298",
    "type": "article"
  },
  {
    "title": "Imaginary People Representing Real Numbers",
    "doi": "https://doi.org/10.1145/3265986",
    "publication_date": "2018-11-01",
    "publication_year": 2018,
    "authors": "Jisun An; Haewoon Kwak; Soon-Gyo Jung; Joni Salminen; M. Admad; Bernard J. Jansen",
    "corresponding_authors": "",
    "abstract": "We develop a methodology to automate creating imaginary people, referred to as personas, by processing complex behavioral and demographic data of social media audiences. From a popular social media account containing more than 30 million interactions by viewers from 198 countries engaging with more than 4,200 online videos produced by a global media corporation, we demonstrate that our methodology has several novel accomplishments, including: (a) identifying distinct user behavioral segments based on the user content consumption patterns; (b) identifying impactful demographics groupings; and (c) creating rich persona descriptions by automatically adding pertinent attributes, such as names, photos, and personal characteristics. We validate our approach by implementing the methodology into an actual working system; we then evaluate it via quantitative methods by examining the accuracy of predicting content preference of personas, the stability of the personas over time, and the generalizability of the method via applying to two other datasets. Research findings show the approach can develop rich personas representing the behavior and demographics of real audiences using privacy-preserving aggregated online social media data from major online platforms. Results have implications for media companies and other organizations distributing content via online platforms.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2898706124",
    "type": "article"
  },
  {
    "title": "Cashtag Piggybacking",
    "doi": "https://doi.org/10.1145/3313184",
    "publication_date": "2019-04-03",
    "publication_year": 2019,
    "authors": "Stefano Cresci; Fabrizio Lillo; Daniele Regoli; Serena Tardelli; Maurizio Tesconi",
    "corresponding_authors": "",
    "abstract": "Microblogs are increasingly exploited for predicting prices and traded volumes of stocks in financial markets. However, it has been demonstrated that much of the content shared in microblogging platforms is created and publicized by bots and spammers. Yet, the presence (or lack thereof) and the impact of fake stock microblogs has never systematically been investigated before. Here, we study 9M tweets related to stocks of the 5 main financial markets in the US. By comparing tweets with financial data from Google Finance, we highlight important characteristics of Twitter stock microblogs. More importantly, we uncover a malicious practice - referred to as cashtag piggybacking - perpetrated by coordinated groups of bots and likely aimed at promoting low-value stocks by exploiting the popularity of high-value ones. Among the findings of our study is that as much as 71% of the authors of suspicious financial tweets are classified as bots by a state-of-the-art spambot detection algorithm. Furthermore, 37% of them were suspended by Twitter a few months after our investigation. Our results call for the adoption of spam and bot detection techniques in all studies and applications that exploit user-generated content for predicting the stock market.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W4288373381",
    "type": "article"
  },
  {
    "title": "Cookie Banners and Privacy Policies: Measuring the Impact of the GDPR on the Web",
    "doi": "https://doi.org/10.1145/3466722",
    "publication_date": "2021-07-14",
    "publication_year": 2021,
    "authors": "Michael Kretschmer; Jan Pennekamp; Klaus Wehrle",
    "corresponding_authors": "",
    "abstract": "The General Data Protection Regulation (GDPR) is in effect since May of 2018. As one of the most comprehensive pieces of legislation concerning privacy, it sparked a lot of discussion on the effect it would have on users and providers of online services in particular, due to the large amount of personal data processed in this context. Almost three years later, we are interested in revisiting this question to summarize the impact this new regulation has had on actors in the World Wide Web. Using Scopus, we obtain a vast corpus of academic work to survey studies related to changes on websites since and around the time the GDPR went into force. Our findings show that the emphasis on privacy increased w.r.t. online services, but plenty potential for improvements remains. Although online services are on average more transparent regarding data processing practices in their public data policies, a majority of these policies still either lack information required by the GDPR (e.g., contact information for users to file privacy inquiries) or do not provide this information in a user-friendly form. Additionally, we summarize that online services more often provide means for their users to opt out of data processing, but regularly obstruct convenient access to such means through unnecessarily complex and sometimes illegitimate interface design. Our survey further details that this situation contradicts the preferences expressed by users both verbally and through their actions, and researchers have proposed multiple approaches to facilitate GDPR-conform data processing without negatively impacting the user experience. Thus, we compiled reoccurring points of criticism by privacy researchers and data protection authorities into a list of four guidelines for service providers to consider.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W3179937365",
    "type": "article"
  },
  {
    "title": "Disentangling Decentralized Finance (DeFi) Compositions",
    "doi": "https://doi.org/10.1145/3532857",
    "publication_date": "2022-10-08",
    "publication_year": 2022,
    "authors": "Stefan Kitzler; Friedhelm Victor; Pietro Saggese; Bernhard Haslhofer",
    "corresponding_authors": "",
    "abstract": "We present a measurement study on compositions of Decentralized Finance (DeFi) protocols, which aim to disrupt traditional finance and offer services on top of distributed ledgers, such as Ethereum. Understanding DeFi compositions is of great importance, as they may impact the development of ecosystem interoperability, are increasingly integrated with web technologies, and may introduce risks through complexity. Starting from a dataset of 23 labeled DeFi protocols and 10,663,881 associated Ethereum accounts, we study the interactions of protocols and associated smart contracts. From a network perspective, we find that decentralized exchange (DEX) and lending protocol account nodes have high degree and centrality values, that interactions among protocol nodes primarily occur in a strongly connected component, and that known community detection methods cannot disentangle DeFi protocols. Therefore, we propose an algorithm to decompose a protocol call into a nested set of building blocks that may be part of other DeFi protocols. This allows us to untangle and study protocol compositions. With a ground truth dataset that we have collected, we can demonstrate the algorithm’s capability by finding that swaps are the most frequently used building blocks. As building blocks can be nested, that is, contained in each other, we provide visualizations of composition trees for deeper inspections. We also present a broad picture of DeFi compositions by extracting and flattening the entire nested building block structure across multiple DeFi protocols. Finally, to demonstrate the practicality of our approach, we present a case study that is inspired by the recent collapse of the UST stablecoin in the Terra ecosystem. Under the hypothetical assumption that the stablecoin USD Tether would experience a similar fate, we study which building blocks — and, thereby, DeFi protocols — would be affected. Overall, our results and methods contribute to a better understanding of a new family of financial products.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W4303647478",
    "type": "article"
  },
  {
    "title": "Fake News Propagation: A Review of Epidemic Models, Datasets, and Insights",
    "doi": "https://doi.org/10.1145/3522756",
    "publication_date": "2022-03-30",
    "publication_year": 2022,
    "authors": "Simone Raponi; Zeinab Khalifa; Gabriele Oligeri; Roberto Di Pietro",
    "corresponding_authors": "",
    "abstract": "Fake news propagation is a complex phenomenon influenced by a multitude of factors whose identification and impact assessment is challenging. Although many models have been proposed in the literature, the one capturing all the properties of a real fake-news propagation phenomenon is inevitably still missing. Modern propagation models, mainly inspired by old epidemiological models, attempt to approximate the fake-news propagation phenomena by blending psychological factors, social relations, and user behavior. This work provides an in-depth analysis of the current state of fake-news propagation models supported by real-world datasets. We highlighted similarities and differences in the modeling approaches, wrapping up the main research trends. Propagation models, transitions, network topologies, and performance metrics have been identified and discussed in detail. The thorough analysis we provided in this article, coupled with the highlighted research hints, have a high potential to pave the way for future research in the area.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W4220653940",
    "type": "review"
  },
  {
    "title": "Opinion Leaders for Information Diffusion Using Graph Neural Network in Online Social Networks",
    "doi": "https://doi.org/10.1145/3580516",
    "publication_date": "2023-01-20",
    "publication_year": 2023,
    "authors": "Lokesh Jain; Rahul Katarya; Shelly Sachdeva",
    "corresponding_authors": "",
    "abstract": "Various opportunities are available to depict different domains due to the diverse nature of social networks and researchers' insatiable. An opinion leader is a human entity or cluster of people who can redirect human assessment strategy by intellectual skills in a social network. A more comprehensive range of approaches is developed to detect opinion leaders based on network-specific and heuristic parameters. For many years, deep learning–based models have solved various real-world multifaceted, graph-based problems with high accuracy and efficiency. The Graph Neural Network (GNN) is a deep learning–based model that modernized neural networks’ efficiency by analyzing and extracting latent dependencies and confined embedding via messaging and neighborhood aggregation of data in the network. In this article, we have proposed an exclusive GNN for Opinion Leader Identification (GOLI) model utilizing the power of GNNs to categorize the opinion leaders and their impact on online social networks. In this model, we first measure the n-node neighbor's reputation of the node based on materialized trust. Next, we perform centrality conciliation instead of the input data's conventional node-embedding mechanism. We experiment with the proposed model on six different online social networks consisting of billions of users’ data to validate the model's authenticity. Finally, after training, we found the top-N opinion leaders for each dataset and analyzed how the opinion leaders are influential in information diffusion. The training-testing accuracy and error rate are also measured and compared with the other state-of-art standard Social Network Analysis (SNA) measures. We determined that the GNN-based model produced high performance concerning accuracy and precision.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W4317536051",
    "type": "article"
  },
  {
    "title": "Analytic modeling of multitier Internet applications",
    "doi": "https://doi.org/10.1145/1232722.1232724",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Bhuvan Urgaonkar; G. Pacifici; Prashant Shenoy; Mike Spreitzer; Asser Tantawi",
    "corresponding_authors": "",
    "abstract": "Since many Internet applications employ a multitier architecture, in this article, we focus on the problem of analytically modeling the behavior of such applications. We present a model based on a network of queues where the queues represent different tiers of the application. Our model is sufficiently general to capture (i) the behavior of tiers with significantly different performance characteristics and (ii) application idiosyncrasies such as session-based workloads, tier replication, load imbalances across replicas, and caching at intermediate tiers. We validate our model using real multitier applications running on a Linux server cluster. Our experiments indicate that our model faithfully captures the performance of these applications for a number of workloads and configurations. Furthermore, our model successfully handles a comprehensive range of resource utilization---from 0 to near saturation for the CPU---for two separate tiers. For a variety of scenarios, including those with caching at one of the application tiers, the average response times predicted by our model were within the 95% confidence intervals of the observed average response times. Our experiments also demonstrate the utility of the model for dynamic capacity provisioning, performance prediction, bottleneck identification, and session policing. In one scenario, where the request arrival rate increased from less than 1500 to nearly 4200 requests/minute, a dynamic provisioning technique employing our model was able to maintain response time targets by increasing the capacity of two of the tiers by factors of 2 and 3.5, respectively.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W2101438247",
    "type": "article"
  },
  {
    "title": "Link analysis for Web spam detection",
    "doi": "https://doi.org/10.1145/1326561.1326563",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Luca Becchetti; Carlos Castillo; Debora Donato; Ricardo Baeza‐Yates; Stefano Leonardi",
    "corresponding_authors": "",
    "abstract": "We propose link-based techniques for automatic detection of Web spam, a term referring to pages which use deceptive techniques to obtain undeservedly high scores in search engines. The use of Web spam is widespread and difficult to solve, mostly due to the large size of the Web which means that, in practice, many algorithms are infeasible. We perform a statistical analysis of a large collection of Web pages. In particular, we compute statistics of the links in the vicinity of every Web page applying rank propagation and probabilistic counting over the entire Web graph in a scalable way. These statistical features are used to build Web spam classifiers which only consider the link structure of the Web, regardless of page contents. We then present a study of the performance of each of the classifiers alone, as well as their combined performance, by testing them over a large collection of Web link spam. After tenfold cross-validation, our best classifiers have a performance comparable to that of state-of-the-art spam classifiers that use content attributes, but are orthogonal to content-based methods.",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W1988814344",
    "type": "article"
  },
  {
    "title": "A framework for QoS-based Web service contracting",
    "doi": "https://doi.org/10.1145/1541822.1541825",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Marco Comuzzi; Barbara Pernici",
    "corresponding_authors": "",
    "abstract": "The extensive adoption of Web service-based applications in dynamic business scenarios, such as on-demand computing or highly reconfigurable virtual enterprises, advocates for methods and tools for the management of Web service nonfunctional aspects, such as Quality of Service (QoS). Concerning contracts on Web service QoS, the literature has mostly focused on the contract definition and on mechanisms for contract enactment, such as the monitoring of the satisfaction of negotiated QoS guarantees. In this context, this article proposes a framework for the automation of the Web service contract specification and establishment. An extensible model for defining both domain-dependent and domain-independent Web service QoS dimensions and a method for the automation of the contract establishment phase are proposed. We describe a matchmaking algorithm for the ranking of functionally equivalent services, which orders services on the basis of their ability to fulfill the service requestor requirements, while maintaining the price below a specified budget. We also provide an algorithm for the configuration of the negotiable part of the QoS Service-Level Agreement (SLA), which is used to configure the agreement with the top-ranked service identified in the matchmaking phase. Experimental results show that, in a utility theory perspective, the contract establishment phase leads to efficient outcomes. We envision two advanced application scenarios for the Web service contracting framework proposed in this article. First, it can be used to enhance Web services self-healing properties in reaction to QoS-related service failures; second, it can be exploited in process optimization for the online reconfiguration of candidate Web services QoS SLAs.",
    "cited_by_count": 116,
    "openalex_id": "https://openalex.org/W2013484127",
    "type": "article"
  },
  {
    "title": "BrowserShield",
    "doi": "https://doi.org/10.1145/1281480.1281481",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Charles Reis; John Dunagan; Helen J. Wang; Opher Dubrovsky; Saher Esmeir",
    "corresponding_authors": "",
    "abstract": "Vulnerability-driven filtering of network data can offer a fast and easy-to-deploy alternative or intermediary to software patching, as exemplified in Shield [Wang et al. 2004]. In this article, we take Shield's vision to a new domain, inspecting and cleansing not just static content, but also dynamic content. The dynamic content we target is the dynamic HTML in Web pages, which have become a popular vector for attacks. The key challenge in filtering dynamic HTML is that it is undecidable to statically determine whether an embedded script will exploit the browser at runtime. We avoid this undecidability problem by rewriting web pages and any embedded scripts into safe equivalents, inserting checks so that the filtering is done at runtime. The rewritten pages contain logic for recursively applying runtime checks to dynamically generated or modified web content, based on known vulnerabilities. We have built and evaluated BrowserShield , a general framework that performs this dynamic instrumentation of embedded scripts, and that admits policies for customized runtime actions like vulnerability-driven filtering. We also explore other applications on top of BrowserShield.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2032095999",
    "type": "article"
  },
  {
    "title": "Supporting the dynamic evolution of Web service protocols in service-oriented architectures",
    "doi": "https://doi.org/10.1145/1346337.1346241",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Seung Hwan Ryu; Fabio Casati; Halvard Skogsrud; Boualem Benatallah; Régis Saint-Paul",
    "corresponding_authors": "",
    "abstract": "In service-oriented architectures, everything is a service and everyone is a service provider. Web services (or simply services) are loosely coupled software components that are published, discovered, and invoked across the Web. As the use of Web service grows, in order to correctly interact with them, it is important to understand the business protocols that provide clients with the information on how to interact with services. In dynamic Web service environments, service providers need to constantly adapt their business protocols for reflecting the restrictions and requirements proposed by new applications, new business strategies, and new laws, or for fixing problems found in the protocol definition. However, the effective management of such a protocol evolution raises critical problems: one of the most critical issues is how to handle instances running under the old protocol when it has been changed. Simple solutions, such as aborting them or allowing them to continue to run according to the old protocol, can be considered, but they are inapplicable for many reasons (for example, the loss of work already done and the critical nature of work). In this article, we present a framework that supports service managers in managing the business protocol evolution by providing several features, such as a variety of protocol change impact analyses automatically determining which ongoing instances can be migrated to the new version of protocol, and data mining techniques inferring interaction patterns used for classifying ongoing instances migrateable to the new protocol. To support the protocol evolution process, we have also developed database-backed GUI tools on top of our existing system. The proposed approach and tools can help service managers in managing the evolution of ongoing instances when the business protocols of services with which they are interacting have changed.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W1970178948",
    "type": "article"
  },
  {
    "title": "Design trade-offs for search engine caching",
    "doi": "https://doi.org/10.1145/1409220.1409223",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Ricardo Baeza‐Yates; Aristides Gionis; Flavio Junqueira; Vanessa Murdock; Vassilis Plachouras; Fabrizio Silvestri",
    "corresponding_authors": "",
    "abstract": "In this article we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year, we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log influence the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W1990129631",
    "type": "article"
  },
  {
    "title": "Fast and Compact Web Graph Representations",
    "doi": "https://doi.org/10.1145/1841909.1841913",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Francisco Claude; Gonzalo Navarro",
    "corresponding_authors": "",
    "abstract": "Compressed graph representations, in particular for Web graphs, have become an attractive research topic because of their applications in the manipulation of huge graphs in main memory. The state of the art is well represented by the WebGraph project, where advantage is taken of several particular properties of Web graphs to offer a trade-off between space and access time. In this paper we show that the same properties can be exploited with a different and elegant technique that builds on grammar-based compression. In particular, we focus on Re-Pair and on Ziv-Lempel compression, which, although cannot reach the best compression ratios of WebGraph, achieve much faster navigation of the graph when both are tuned to use the same space. Moreover, the technique adapts well to run on secondary memory and in distributed scenarios. As a byproduct, we introduce an approximate Re-Pair version that works efficiently with severely limited main memory.",
    "cited_by_count": 101,
    "openalex_id": "https://openalex.org/W2147344024",
    "type": "article"
  },
  {
    "title": "Characterizing Web-Based Video Sharing Workloads",
    "doi": "https://doi.org/10.1145/1961659.1961662",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Siddharth Mitra; Mayank Agrawal; Amit Kumar Singh Yadav; Niklas Carlsson; Derek L. Eager; Anirban Mahanti",
    "corresponding_authors": "",
    "abstract": "Video sharing services that allow ordinary Web users to upload video clips of their choice and watch video clips uploaded by others have recently become very popular. This article identifies invariants in video sharing workloads, through comparison of the workload characteristics of four popular video sharing services. Our traces contain metadata on approximately 1.8 million videos which together have been viewed approximately 6 billion times. Using these traces, we study the similarities and differences in use of several Web 2.0 features such as ratings, comments, favorites, and propensity of uploading content. In general, we find that active contribution, such as video uploading and rating of videos, is much less prevalent than passive use. While uploaders in general are skewed with respect to the number of videos they upload, the fraction of multi-time uploaders is found to differ by a factor of two between two of the sites. The distributions of lifetime measures of video popularity are found to have heavy-tailed forms that are similar across the four sites. Finally, we consider implications for system design of the identified invariants. To gain further insight into caching in video sharing systems, and the relevance to caching of lifetime popularity measures, we gathered an additional dataset tracking views to a set of approximately 1.3 million videos from one of the services, over a twelve-week period. We find that lifetime popularity measures have some relevance for large cache (hot set) sizes (i.e., a hot set defined according to one of these measures is indeed relatively “hot”), but that this relevance substantially decreases as cache size decreases, owing to churn in video popularity.",
    "cited_by_count": 99,
    "openalex_id": "https://openalex.org/W2034377483",
    "type": "article"
  },
  {
    "title": "Should We Use the Sample? Analyzing Datasets Sampled from Twitter’s Stream API",
    "doi": "https://doi.org/10.1145/2746366",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Yazhe Wang; Jamie Callan; Baihua Zheng",
    "corresponding_authors": "",
    "abstract": "Researchers have begun studying content obtained from microblogging services such as Twitter to address a variety of technological, social, and commercial research questions. The large number of Twitter users and even larger volume of tweets often make it impractical to collect and maintain a complete record of activity; therefore, most research and some commercial software applications rely on samples, often relatively small samples, of Twitter data. For the most part, sample sizes have been based on availability and practical considerations. Relatively little attention has been paid to how well these samples represent the underlying stream of Twitter data. To fill this gap, this article performs a comparative analysis on samples obtained from two of Twitter’s streaming APIs with a more complete Twitter dataset to gain an in-depth understanding of the nature of Twitter data samples and their potential for use in various data mining tasks.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2244033198",
    "type": "article"
  },
  {
    "title": "Analyzing and Mining Comments and Comment Ratings on the Social Web",
    "doi": "https://doi.org/10.1145/2628441",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Stefan Siersdorfer; Sergiu Chelaru; José San Pedro; İsmail Sengör Altıngövde; Wolfgang Nejdl",
    "corresponding_authors": "",
    "abstract": "An analysis of the social video sharing platform YouTube and the news aggregator Yahoo! News reveals the presence of vast amounts of community feedback through comments for published videos and news stories, as well as through metaratings for these comments. This article presents an in-depth study of commenting and comment rating behavior on a sample of more than 10 million user comments on YouTube and Yahoo! News. In this study, comment ratings are considered first-class citizens. Their dependencies with textual content, thread structure of comments, and associated content (e.g., videos and their metadata) are analyzed to obtain a comprehensive understanding of the community commenting behavior. Furthermore, this article explores the applicability of machine learning and data mining to detect acceptance of comments by the community, comments likely to trigger discussions, controversial and polarizing content, and users exhibiting offensive commenting behavior. Results from this study have potential application in guiding the design of community-oriented online discussion platforms.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2075046382",
    "type": "article"
  },
  {
    "title": "A Large-scale Behavioural Analysis of Bots and Humans on Twitter",
    "doi": "https://doi.org/10.1145/3298789",
    "publication_date": "2019-02-05",
    "publication_year": 2019,
    "authors": "Zafar Gilani; Reza Farahbakhsh; Gareth Tyson; Jon Crowcroft",
    "corresponding_authors": "",
    "abstract": "Recent research has shown a substantial active presence of bots in online social networks (OSNs). In this article, we perform a comparative analysis of the usage and impact of bots and humans on Twitter—one of the largest OSNs in the world. We collect a large-scale Twitter dataset and define various metrics based on tweet metadata. Using a human annotation task, we assign “bot” and “human” ground-truth labels to the dataset and compare the annotations against an online bot detection tool for evaluation. We then ask a series of questions to discern important behavioural characteristics of bots and humans using metrics within and among four popularity groups. From the comparative analysis, we draw clear differences and interesting similarities between the two entities.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2914084490",
    "type": "article"
  },
  {
    "title": "<scp>RoSGAS</scp> : Adaptive Social Bot Detection with Reinforced Self-supervised GNN Architecture Search",
    "doi": "https://doi.org/10.1145/3572403",
    "publication_date": "2022-12-02",
    "publication_year": 2022,
    "authors": "Yingguang Yang; Renyu Yang; Yangyang Li; Kai Cui; Zhiqin Yang; Yue Wang; Jie Xu; Haiyong Xie",
    "corresponding_authors": "",
    "abstract": "Social bots are referred to as the automated accounts on social networks that make attempts to behave like human. While Graph Neural Networks (GNNs) has been massively applied to the field of social bot detection, a huge amount of domain expertise and prior knowledge is heavily engaged in the state-of-the art approaches to design a dedicated neural network architecture for a specific classification task. Involving oversized nodes and network layers in the model design, however, usually causes the over-smoothing problem and the lack of embedding discrimination. In this paper, we propose RoSGAS, a novel Reinforced and Self-supervised GNN Architecture Search framework to adaptively pinpoint the most suitable multi-hop neighborhood and the number of layers in the GNN architecture. More specifically, we consider the social bot detection problem as a user-centric subgraph embedding and classification task. We exploit heterogeneous information network to present the user connectivity by leveraging account metadata, relationships, behavioral features and content features. RoSGAS uses a multi-agent deep reinforcement learning (RL) mechanism for navigating the search of optimal neighborhood and network layers to learn individually the subgraph embedding for each target user. A nearest neighbor mechanism is developed for accelerating the RL training process, and RoSGAS can learn more discriminative subgraph embedding with the aid of self-supervised learning. Experiments on 5 Twitter datasets show that RoSGAS outperforms the state-of-the-art approaches in terms of accuracy, training efficiency and stability, and has better generalization when handling unseen samples.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W4282982824",
    "type": "article"
  },
  {
    "title": "SpotSpam: Intention Analysis–driven SMS Spam Detection Using BERT Embeddings",
    "doi": "https://doi.org/10.1145/3538491",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "C. Oswald; Sona Elza Simon; Arnab Bhattacharya",
    "corresponding_authors": "",
    "abstract": "Short Message Service (SMS) is one of the widely used mobile applications for global communication for personal and business purposes. Its widespread use for customer interaction, business updates, and reminders has made it a billion-dollar industry in “Text Marketing.” Along with valid SMS, a tsunami of spam messages also pop up that serve various purposes for the sender and the majority of them are fraudulent. Filtering spam SMS in an accurate manner is a crucial and challenging task that will benefit human lives both mentally and economically. Some of the challenges in the filtering of spam SMS include less number of characters, texts in informal languages, lack of public SMS spam corpus, and so on. Focusing solely on the textual features of the SMS is a major handicap of the existing methods, as it lacks in dynamically adapting to the increasing number of new keywords and jargon. In this article, we develop an intention-based approach of SMS spam filtering that efficiently handles dynamic keywords by focusing on the semantics of the words. We capture both semantic and textual features of the short-text messages based on 13 pre-defined intention labels. Moreover, the contextual embeddings of the texts are generated using various pre-trained NLP (Natural Language Processing) models. Finally, intention scores are computed for the pre-defined labels and a bunch of supervised learning classifiers are employed for filtering as spam or ham. Our approaches are evaluated on the SMS Spam Collection [ 24 ] benchmark dataset, and extensive experimentation shows interesting results. Our model did remarkably well with an accuracy of 98.07%, Precision and Recall of ∼ 0.97, which is better than few of the existing state-of-the-art alternatives. Though the accuracy of our approach is not the best among other existing approaches, the model is highly stable due to its emphasis on extracting the contextual features from the text through intention labels.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W4281399463",
    "type": "article"
  },
  {
    "title": "Understanding the Contribution of Recommendation Algorithms on Misinformation Recommendation and Misinformation Dissemination on Social Networks",
    "doi": "https://doi.org/10.1145/3616088",
    "publication_date": "2023-08-14",
    "publication_year": 2023,
    "authors": "R. S. Pathak; Francesca Spezzano; Maria Soledad Pera",
    "corresponding_authors": "",
    "abstract": "Social networks are a platform for individuals and organizations to connect with each other and inform, advertise, spread ideas, and ultimately influence opinions. These platforms have been known to propel misinformation. We argue that this could be compounded by the recommender algorithms that these platforms use to suggest items potentially of interest to their users, given the known biases and filter bubbles issues affecting recommender systems. While much has been studied about misinformation on social networks, the potential exacerbation that could result from recommender algorithms in this environment is in its infancy. In this manuscript, we present the result of an in-depth analysis conducted on two datasets ( Politifact FakeNewsNet dataset and HealthStory FakeHealth dataset ) in order to deepen our understanding of the interconnection between recommender algorithms and misinformation spread on Twitter. In particular, we explore the degree to which well-known recommendation algorithms are prone to be impacted by misinformation. Via simulation, we also study misinformation diffusion on social networks, as triggered by suggestions produced by these recommendation algorithms. Outcomes from this work evidence that misinformation does not equally affect all recommendation algorithms. Popularity-based and network-based recommender algorithms contribute the most to misinformation diffusion. Users who are known to be superspreaders are known to directly impact algorithmic performance and misinformation spread in specific scenarios. Findings emerging from our exploration result in a number of implications for researchers and practitioners to consider when designing and deploying recommender algorithms in social networks.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4385806860",
    "type": "article"
  },
  {
    "title": "A Multi-Task Graph Neural Network with Variational Graph Auto-Encoders for Session-Based Travel Packages Recommendation",
    "doi": "https://doi.org/10.1145/3577032",
    "publication_date": "2023-02-01",
    "publication_year": 2023,
    "authors": "Guixiang Zhu; Jie Cao; Lei Chen; Youquan Wang; Zhan Bu; Shuxin Yang; Jianqing Wu; Zhiping Wang",
    "corresponding_authors": "",
    "abstract": "Session-based travel packages recommendation aims to predict users’ next click based on their current and historical sessions recorded by Online Travel Agencies (OTAs). Recently, an increasing number of studies attempted to apply Graph Neural Networks (GNNs) to the session-based recommendation and obtained promising results. However, most of them do not take full advantage of the explicit latent structure from attributes of items, making learned representations of items less effective and difficult to interpret. Moreover, they only combine historical sessions (long-term preferences) with a current session (short-term preference) to learn a unified representation of users, ignoring the effects of historical sessions for the current session. To this end, this article proposes a novel session-based model named STR-VGAE, which fills subtasks of the travel packages recommendation and variational graph auto-encoders simultaneously. STR-VGAE mainly consists of three components: travel packages encoder , users behaviors encoder , and interaction modeling . Specifically, the travel packages encoder module is used to learn a unified travel package representation from co-occurrence attribute graphs by using multi-view variational graph auto-encoders and a multi-view attention network. The users behaviors encoder module is used to encode user’ historical and current sessions with a personalized GNN, which considers the effects of historical sessions on the current session, and coalesce these two kinds of session representations to learn the high-quality users’ representations by exploiting a gated fusion approach. The interaction modeling module is used to calculate recommendation scores over all candidate travel packages. Extensive experiments on a real-life tourism e-commerce dataset from China show that STR-VGAE yields significant performance advantages over several competitive methods, meanwhile provides an interpretation for the generated recommendation list.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4318816006",
    "type": "article"
  },
  {
    "title": "Reinforced MOOCs Concept Recommendation in Heterogeneous Information Networks",
    "doi": "https://doi.org/10.1145/3580510",
    "publication_date": "2023-03-01",
    "publication_year": 2023,
    "authors": "Jibing Gong; Yao Wan; Ye Liu; Xuewen Li; Yi Zhao; Cheng Wang; Laurent Charlin; Xiaohan Fang; Wenzheng Feng; Jingyi Zhang; Jie Tang",
    "corresponding_authors": "",
    "abstract": "Massive open online courses (MOOCs) , which offer open access and widespread interactive participation through the internet, are quickly becoming the preferred method for online and remote learning. Several MOOC platforms offer the service of course recommendation to users, to improve the learning experience of users. Despite the usefulness of this service, we consider that recommending courses to users directly may neglect their varying degrees of expertise. To mitigate this gap, we examine an interesting problem of concept recommendation in this paper, which can be viewed as recommending knowledge to users in a fine-grained way. We put forward a novel approach, termed HinCRec-RL, for C oncept Rec ommendation in MOOCs, which is based on H eterogeneous I nformation N etworks and R einforcement L earning . In particular, we propose to shape the problem of concept recommendation within a reinforcement learning framework to characterize the dynamic interaction between users and knowledge concepts in MOOCs. Furthermore, we propose to form the interactions among users, courses, videos, and concepts into a heterogeneous information network (HIN) to learn the semantic user representations better. We then employ an attentional graph neural network to represent the users in the HIN, based on meta-paths. Extensive experiments are conducted on a real-world dataset collected from a Chinese MOOC platform, XuetangX , to validate the efficacy of our proposed HinCRec-RL. Experimental results and analysis demonstrate that our proposed HinCRec-RL performs well when compared with several state-of-the-art models.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4322736931",
    "type": "article"
  },
  {
    "title": "Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies",
    "doi": "https://doi.org/10.1145/3732294",
    "publication_date": "2025-05-06",
    "publication_year": 2025,
    "authors": "Chirag Shah; Ryen W. White; Reid Andersen; Georg Buscher; Scott Counts; Sarkar Snigdha Sarathi Das; Ali Montazeralghaem; Sathish Manivannan; J. Neville; Nagu Rangan; Tara Safavi; Siddharth Suri; Mengting Wan; Leijie Wang; Longqi Yang",
    "corresponding_authors": "",
    "abstract": "Understanding user intents in information access scenarios can help us provide more relevant and personalized search results and recommendations. However, analyzing user intents is not easy, especially for emerging forms of Web search such as Artificial Intelligence (AI)-driven chat. To understand user intents from retrospective log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or Machine-Learned (ML) labeling, which is either expensive or inflexible for large and dynamic datasets. Large Language Models (LLMs) could generate rich and relevant concepts, descriptions, and examples for user intents using log data of user interactions. However, using LLMs to generate a user intent taxonomy and applying it for a given Information Retrieval (IR) application can be problematic for two main reasons: (1) such a taxonomy is not externally validated; and (2) there may be an undesirable feedback loop if an LLM does both these tasks without external validation. To address this, we propose a new methodology with human experts and assessors to verify the quality of the LLM-generated taxonomy. We also present an end-to-end pipeline that uses an LLM with Human-in-the-Loop (HITL) to produce, refine, and apply labels for user intent analysis in log data. We demonstrate its effectiveness by uncovering new insights into user intents from search and chat logs from the Microsoft Bing Web search engine. The novelty in this research stems from the method for generating purpose-driven user intent taxonomies with strong validation. Our approach not only helps remove methodological and practical bottlenecks from intent-focused research, but also provides a new framework for generating, validating, and applying other kinds of taxonomies in a scalable and adaptable way, with reasonable human effort.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4410125989",
    "type": "article"
  },
  {
    "title": "Adaptive quality of service management for enterprise services",
    "doi": "https://doi.org/10.1145/1326561.1326569",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Daniel Gmach; Stefan Krompaß; Andreas Scholz; Martin Wimmer; Alfons Kemper",
    "corresponding_authors": "",
    "abstract": "In the past, enterprise resource planning systems were designed as monolithic software systems running on centralized mainframes. Today, these systems are (re-)designed as a repository of enterprise services that are distributed throughout the available computing infrastructure. These service oriented architectures (SOAs) require advanced automatic and adaptive management concepts in order to achieve a high quality of service level in terms of, for example, availability, responsiveness, and throughput. The adaptive management has to allocate service instances to computing resources, adapt the resource allocation to unforeseen load fluctuations, and intelligently schedule individual requests to guarantee negotiated service level agreements (SLAs). Our AutoGlobe platform provides such a comprehensive adaptive service management comprising —static service-to-server allocation based on automatically detected service utilization patterns, —adaptive service management based on a fuzzy controller that remedies exceptional situations by automatically initiating, for example, service migration, service replication (scale-out), and —adaptive scheduling of individual service requests that prioritizes requests depending on the current degree of service level conformance. All three complementary control components are described in detail, and their effectiveness is analyzed by means of realistic business application scenarios.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2059261773",
    "type": "article"
  },
  {
    "title": "A survey of query log privacy-enhancing techniques from a policy perspective",
    "doi": "https://doi.org/10.1145/1409220.1409222",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "A. Feder Cooper",
    "corresponding_authors": "A. Feder Cooper",
    "abstract": "As popular search engines face the sometimes conflicting interests of protecting privacy while retaining query logs for a variety of uses, numerous technical measures have been suggested to both enhance privacy and preserve at least a portion of the utility of query logs. This article seeks to assess seven of these techniques against three sets of criteria: (1) how well the technique protects privacy, (2) how well the technique preserves the utility of the query logs, and (3) how well the technique might be implemented as a user control. A user control is defined as a mechanism that allows individual Internet users to choose to have the technique applied to their own query logs.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2134506494",
    "type": "article"
  },
  {
    "title": "Visualizing tags over time",
    "doi": "https://doi.org/10.1145/1255438.1255439",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Micah Dubinko; Ravi Kumar; Joseph Magnani; Jasmine Novak; Prabhakar Raghavan; Andrew Tomkins",
    "corresponding_authors": "",
    "abstract": "We consider the problem of visualizing the evolution of tags within the Flickr (flickr.com) online image sharing community. Any user of the Flickr service may append a tag to any photo in the system. Over the past year, users have on average added over a million tags each week. Understanding the evolution of these tags over time is therefore a challenging task. We present a new approach based on a characterization of the most interesting tags associated with a sliding interval of time. An animation provided via Flash in a Web browser allows the user to observe and interact with the interesting tags as they evolve over time. New algorithms and data structures are required to support the efficient generation of this visualization. We combine a novel solution to an interval covering problem with extensions to previous work on score aggregation in order to create an efficient backend system capable of producing visualizations at arbitrary scales on this large dataset in real time.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2066595497",
    "type": "article"
  },
  {
    "title": "Tracking Web spam with HTML style similarities",
    "doi": "https://doi.org/10.1145/1326561.1326564",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Tanguy Urvoy; Emmanuel Chauveau; Pascal Filoche; Thomas Lavergne",
    "corresponding_authors": "",
    "abstract": "Automatically generated content is ubiquitous in the web: dynamic sites built using the three-tier paradigm are good examples (e.g., commercial sites, blogs and other sites edited using web authoring software), as well as less legitimate spamdexing attempts (e.g., link farms, faked directories). Those pages built using the same generating method (template or script) share a common “look and feel” that is not easily detected by common text classification methods, but is more related to stylometry. In this work we study and compare several HTML style similarity measures based on both textual and extra-textual features in HTML source code. We also propose a flexible algorithm to cluster a large collection of documents according to these measures. Since the proposed algorithm is based on locality sensitive hashing (LSH), we first review this technique. We then describe how to use the HTML style similarity clusters to pinpoint dubious pages and enhance the quality of spam classifiers. We present an evaluation of our algorithm on the WEBSPAM-UK2006 dataset.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2053377618",
    "type": "article"
  },
  {
    "title": "The comparative effectiveness of sponsored and nonsponsored links for Web e-commerce queries",
    "doi": "https://doi.org/10.1145/1232722.1232725",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Bernard J. Jansen",
    "corresponding_authors": "Bernard J. Jansen",
    "abstract": "The predominant business model for Web search engines is sponsored search, which generates billions in yearly revenue. But are sponsored links providing online consumers with relevant choices for products and services? We address this and related issues by investigating the relevance of sponsored and nonsponsored links for e-commerce queries on the major search engines. The results show that average relevance ratings for sponsored and nonsponsored links are practically the same, although the relevance ratings for sponsored links are statistically higher. We used 108 ecommerce queries and 8,256 retrieved links for these queries from three major Web search engines: Yahoo!, Google, and MSN. In addition to relevance measures, we qualitatively analyzed the e-commerce queries, deriving five categorizations of underlying information needs. Product-specific queries are the most prevalent (48%). Title (62%) and summary (33%) are the primary basis for evaluating sponsored links with URL a distant third (2%). To gauge the effectiveness of sponsored search campaigns, we analyzed the sponsored links from various viewpoints. It appears that links from organizations with large sponsored search campaigns are more relevant than the average sponsored link. We discuss the implications for Web search engines and sponsored search as a long-term business model and as a mechanism for finding relevant information for searchers.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W2100957653",
    "type": "article"
  },
  {
    "title": "Learning Deterministic Regular Expressions for the Inference of Schemas from XML Data",
    "doi": "https://doi.org/10.1145/1841909.1841911",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Geert Jan Bex; Wouter Gelade; Frank Neven; Stijn Vansummeren",
    "corresponding_authors": "",
    "abstract": "Inferring an appropriate DTD or XML Schema Definition (XSD) for a given collection of XML documents essentially reduces to learning deterministic regular expressions from sets of positive example words. Unfortunately, there is no algorithm capable of learning the complete class of deterministic regular expressions from positive examples only, as we will show. The regular expressions occurring in practical DTDs and XSDs, however, are such that every alphabet symbol occurs only a small number of times. As such, in practice it suffices to learn the subclass of deterministic regular expressions in which each alphabet symbol occurs at most k times, for some small k . We refer to such expressions as k -occurrence regular expressions ( k -OREs for short). Motivated by this observation, we provide a probabilistic algorithm that learns k -OREs for increasing values of k, and selects the deterministic one that best describes the sample based on a Minimum Description Length argument. The effectiveness of the method is empirically validated both on real world and synthetic data. Furthermore, the method is shown to be conservative over the simpler classes of expressions considered in previous work.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2569219525",
    "type": "article"
  },
  {
    "title": "Framework for Web service query algebra and optimization",
    "doi": "https://doi.org/10.1145/1326561.1326567",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Qi Yu; Athman Bouguettaya",
    "corresponding_authors": "",
    "abstract": "We present a query algebra that supports optimized access of Web services through service-oriented queries. The service query algebra is defined based on a formal service model that provides a high-level abstraction of Web services across an application domain. The algebra defines a set of algebraic operators. Algebraic service queries can be formulated using these operators. This allows users to query their desired services based on both functionality and quality. We provide the implementation of each algebraic operator. This enables the generation of Service Execution Plans (SEPs) that can be used by users to directly access services. We present an optimization algorithm by extending the Dynamic Programming (DP) approach to efficiently select the SEPs with the best user-desired quality. The experimental study validates the proposed algorithm by demonstrating significant performance improvement compared with the traditional DP approach.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2139657108",
    "type": "article"
  },
  {
    "title": "Learning about the world through long-term query logs",
    "doi": "https://doi.org/10.1145/1409220.1409224",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Matthew Richardson",
    "corresponding_authors": "Matthew Richardson",
    "abstract": "In this article, we demonstrate the value of long-term query logs. Most work on query logs to date considers only short-term (within-session) query information. In contrast, we show that long-term query logs can be used to learn about the world we live in. There are many applications of this that lead not only to improving the search engine for its users, but also potentially to advances in other disciplines such as medicine, sociology, economics, and more. In this article, we will show how long-term query logs can be used for these purposes, and that their potential is severely reduced if the logs are limited to short time horizons. We show that query effects are long-lasting, provide valuable information, and might be used to automatically make medical discoveries, build concept hierarchies, and generally learn about the sociological behavior of users. We believe these applications are only the beginning of what can be done with the information contained in long-term query logs, and see this work as a step toward unlocking their potential.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2136814520",
    "type": "article"
  },
  {
    "title": "Extraction and classification of dense implicit communities in the Web graph",
    "doi": "https://doi.org/10.1145/1513876.1513879",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Yon Dourisboure; Filippo Geraci; Marco Pellegrini",
    "corresponding_authors": "",
    "abstract": "The World Wide Web (WWW) is rapidly becoming important for society as a medium for sharing data, information, and services, and there is a growing interest in tools for understanding collective behavior and emerging phenomena in the WWW. In this article we focus on the problem of searching and classifying communities in the Web. Loosely speaking a community is a group of pages related to a common interest. More formally, communities have been associated in the computer science literature with the existence of a locally dense subgraph of the Web graph (where Web pages are nodes and hyperlinks are arcs of the Web graph). The core of our contribution is a new scalable algorithm for finding relatively dense subgraphs in massive graphs. We apply our algorithm on Web graphs built on three publicly available large crawls of the Web (with raw sizes up to 120M nodes and 1G arcs). The effectiveness of our algorithm in finding dense subgraphs is demonstrated experimentally by embedding artificial communities in the Web graph and counting how many of these are blindly found. Effectiveness increases with the size and density of the communities: it is close to 100% for communities of thirty nodes or more (even at low density). It is still about 80% even for communities of twenty nodes with density over 50% of the arcs present. At the lower extremes the algorithm catches 35% of dense communities made of ten nodes. We also develop some sufficient conditions for the detection of a community under some local graph models and not-too-restrictive hypotheses. We complete our Community Watch system by clustering the communities found in the Web graph into homogeneous groups by topic and labeling each group by representative keywords.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2040886165",
    "type": "article"
  },
  {
    "title": "Engineering rich internet applications with a model-driven approach",
    "doi": "https://doi.org/10.1145/1734200.1734204",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Piero Fraternali; Sara Comai; Alessandro Bozzon; Giovanni Toffetti",
    "corresponding_authors": "",
    "abstract": "Rich Internet Applications (RIAs) have introduced powerful novel functionalities into the Web architecture, borrowed from client-server and desktop applications. The resulting platforms allow designers to improve the user's experience, by exploiting client-side data and computation, bidirectional client-server communication, synchronous and asynchronous events, and rich interface widgets. However, the rapid evolution of RIA technologies challenges the Model-Driven Development methodologies that have been successfully applied in the past decade to traditional Web solutions. This paper illustrates an evolutionary approach for incorporating a wealth of RIA features into an existing Web engineering methodology and notation. The experience demonstrates that it is possible to model RIA application requirements at a high-level using a platform-independent notation, and generate the client-side and server-side code automatically. The resulting approach is evaluated in terms of expressive power, ease of use, and implementability.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2075298213",
    "type": "article"
  },
  {
    "title": "A Comprehensive Study of Features and Algorithms for URL-Based Topic Classification",
    "doi": "https://doi.org/10.1145/1993053.1993057",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Eda Baykan; Monika Henzinger; Ludmila Marian; Ingmar Weber",
    "corresponding_authors": "",
    "abstract": "Given only the URL of a Web page, can we identify its topic? We study this problem in detail by exploring a large number of different feature sets and algorithms on several datasets. We also show that the inherent overlap between topics and the sparsity of the information in URLs makes this a very challenging problem. Web page classification without a page’s content is desirable when the content is not available at all, when a classification is needed before obtaining the content, or when classification speed is of utmost importance. For our experiments we used five different corpora comprising a total of about 3 million (URL, classification) pairs. We evaluated several techniques for feature generation and classification algorithms. The individual binary classifiers were then combined via boosting into metabinary classifiers. We achieve typical F-measure values between 80 and 85, and a typical precision of around 86. The precision can be pushed further over 90 while maintaining a typical level of recall between 30 and 40.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2067698488",
    "type": "article"
  },
  {
    "title": "Analysis of Search and Browsing Behavior of Young Users on the Web",
    "doi": "https://doi.org/10.1145/2555595",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Sergio Duarte Torres; Ingmar Weber; Djoerd Hiemstra",
    "corresponding_authors": "",
    "abstract": "The Internet is increasingly used by young children for all kinds of purposes. Nonetheless, there are not many resources especially designed for children on the Internet and most of the content online is designed for grown-up users. This situation is problematic if we consider the large differences between young users and adults since their topic interests, computer skills, and language capabilities evolve rapidly during childhood. There is little research aimed at exploring and measuring the difficulties that children encounter on the Internet when searching for information and browsing for content. In the first part of this work, we employed query logs from a commercial search engine to quantify the difficulties children of different ages encounter on the Internet and to characterize the topics that they search for. We employed query metrics (e.g., the fraction of queries posed in natural language), session metrics (e.g., the fraction of abandoned sessions), and click activity (e.g., the fraction of ad clicks). The search logs were also used to retrace stages of child development. Concretely, we looked for changes in interests (e.g., the distribution of topics searched) and language development (e.g., the readability of the content accessed and the vocabulary size). In the second part of this work, we employed toolbar logs from a commercial search engine to characterize the browsing behavior of young users, particularly to understand the activities on the Internet that trigger search. We quantified the proportion of browsing and search activity in the toolbar sessions and we estimated the likelihood of a user to carry out search on the Web vertical and multimedia verticals (i.e., videos and images) given that the previous event is another search event or a browsing event. We observed that these metrics clearly demonstrate an increased level of confusion and unsuccessful search sessions among children. We also found a clear relation between the reading level of the clicked pages and characteristics of the users such as age and educational attainment. In terms of browsing behavior, children were found to start their activities on the Internet with a search engine (instead of directly browsing content) more often than adults. We also observed a significantly larger amount of browsing activity for the case of teenager users. Interestingly we also found that if children visit knowledge-related Web sites (i.e., information-dense pages such as Wikipedia articles), they subsequently do more Web searches than adults. Additionally, children and especially teenagers were found to have a greater tendency to engage in multimedia search, which calls to improve the aggregation of multimedia results into the current search result pages.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2171253691",
    "type": "article"
  },
  {
    "title": "Scanpath Trend Analysis on Web Pages",
    "doi": "https://doi.org/10.1145/2970818",
    "publication_date": "2016-11-15",
    "publication_year": 2016,
    "authors": "Sukru Eraslan; Yeliz Yeşilada; Simon Harper",
    "corresponding_authors": "",
    "abstract": "Eye tracking studies have widely been used in improving the design and usability of web pages and in the research of understanding how users navigate them. However, there is limited research in clustering users’ eye movement sequences (i.e., scanpaths) on web pages to identify a general direction they follow. Existing research tends to be reductionist, which means that the resulting path is so short that it is not useful. Moreover, there is little work on correlating users’ scanpaths with visual elements of web pages and the underlying source code, which means the result cannot be used for further processing. In order to address these limitations, we introduce a new concept in clustering scanpaths called Scanpath Trend Analysis (STA) that not only considers the visual elements visited by all users, but also considers the visual elements visited by the majority in any order. We present an algorithm which automatically does this trend analysis to identify a trending scanpath for multiple web users in terms of visual elements of a web page. In contrast to existing research, the STA algorithm first analyzes the most visited visual elements in given scanpaths, clusters the scanpaths by arranging these visual elements based on their overall positions in the individual scanpaths, and then constructs a trending scanpath in terms of these visual elements. This algorithm was experimentally evaluated by an eye tracking study on six web pages for two different kinds of tasks (12 cases in total). Our experimental results show that the STA algorithm generates a trending scanpath that addresses the reductionist problem of existing work by preventing the loss of commonly visited visual elements for all cases. Based on the statistical tests, the STA algorithm also generates a trending scanpath that is significantly more similar to the inputted scanpaths compared to other existing work in 10 out of 12 cases. In the remaining cases, the STA algorithm still performs significantly better than some other existing work. This algorithm contributes to behavior analysis research on the web that can be used for different purposes: for example, re-engineering web pages guided by the trending scanpath to improve users’ experience or guiding designers to improve their design.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2555597023",
    "type": "article"
  },
  {
    "title": "“The Enemy Among Us”",
    "doi": "https://doi.org/10.1145/3324997",
    "publication_date": "2019-07-26",
    "publication_year": 2019,
    "authors": "Wafa Alorainy; Pete Burnap; Han Liu; Matthew Williams",
    "corresponding_authors": "",
    "abstract": "Offensive or antagonistic language targeted at individuals and social groups based on their personal characteristics (also known as cyber hate speech or cyberhate) has been frequently posted and widely circulated via the World Wide Web. This can be considered as a key risk factor for individual and societal tension surrounding regional instability. Automated Web-based cyberhate detection is important for observing and understanding community and regional societal tension—especially in online social networks where posts can be rapidly and widely viewed and disseminated. While previous work has involved using lexicons, bags-of-words, or probabilistic language parsing approaches, they often suffer from a similar issue, which is that cyberhate can be subtle and indirect—thus, depending on the occurrence of individual words or phrases, can lead to a significant number of false negatives, providing inaccurate representation of the trends in cyberhate. This problem motivated us to challenge thinking around the representation of subtle language use, such as references to perceived threats from “the other” including immigration or job prosperity in a hateful context. We propose a novel “othering” feature set that utilizes language use around the concept of “othering” and intergroup threat theory to identify these subtleties, and we implement a wide range of classification methods using embedding learning to compute semantic distances between parts of speech considered to be part of an “othering” narrative. To validate our approach, we conducted two sets of experiments. The first involved comparing the results of our novel method with state-of-the-art baseline models from the literature. Our approach outperformed all existing methods. The second tested the best performing models from the first phase on unseen datasets for different types of cyberhate, namely religion, disability, race, and sexual orientation. The results showed F-measure scores for classifying hateful instances obtained through applying our model of 0.81, 0.71, 0.89, and 0.72, respectively, demonstrating the ability of the “othering” narrative to be an important part of model generalization.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2966244612",
    "type": "article"
  },
  {
    "title": "A Survey of Figurative Language and Its Computational Detection in Online Social Networks",
    "doi": "https://doi.org/10.1145/3375547",
    "publication_date": "2020-02-07",
    "publication_year": 2020,
    "authors": "Muhammad Abulaish; Ashraf Kamal; Mohammed J. Zaki",
    "corresponding_authors": "",
    "abstract": "The frequent usage of figurative language on online social networks, especially on Twitter, has the potential to mislead traditional sentiment analysis and recommender systems. Due to the extensive use of slangs, bashes, flames, and non-literal texts, tweets are a great source of figurative language, such as sarcasm, irony, metaphor, simile, hyperbole, humor, and satire. Starting with a brief introduction of figurative language and its various categories, this article presents an in-depth survey of the state-of-the-art techniques for computational detection of seven different figurative language categories, mainly on Twitter. For each figurative language category, we present details about the characterizing features, datasets, and state-of-the-art computational detection approaches. Finally, we discuss open challenges and future directions of research for each figurative language category.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W3013347530",
    "type": "article"
  },
  {
    "title": "On Scalability of Association-rule-based Recommendation",
    "doi": "https://doi.org/10.1145/3398202",
    "publication_date": "2020-06-21",
    "publication_year": 2020,
    "authors": "Zhiang Wu; Changsheng Li; Jie Cao; Yong Ge",
    "corresponding_authors": "",
    "abstract": "The association-rule-based approach is one of the most common technologies for building recommender systems and it has been extensively adopted for commercial use. A variety of techniques, mainly including eligible rule selection and multiple rules combination, have been developed to create effective recommendation. Unfortunately, little attention has been paid to the scalability concern of rule-based recommendation methods. However, the computational complexity of rule-based methods shall increase drastically with the growth of both online customers and rules, which are usually several millions in typical e-commerce platforms. Moreover, the dynamic change of users’ actions requires rule-based methods make recommendations in nearly real-time, which further highlights the scalability issue of rule-based recommender systems. In this article, we present a distributed framework that can scale different association-rule-based recommendation methods in a unified way. Specifically, based on the summarization of existing rule-based approaches, a generic tree-type structure is defined to store separate kinds of patterns, and an efficient algorithm is designed for mining eligible patterns along with computing recommendation scores. To handle the ever-increasing number of online customers, a distributed framework is proposed, where two load-balanced strategies for partitioning tree are put forward to fit sparse and dense data, respectively. Extensive experiments on five real-life data sets demonstrate that the efficiency of association-rule-based recommender systems can be significantly improved by the proposed framework.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W3048820656",
    "type": "article"
  },
  {
    "title": "Image Privacy Prediction Using Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3386082",
    "publication_date": "2020-04-09",
    "publication_year": 2020,
    "authors": "Ashwini Tonge; Cornelia Caragea",
    "corresponding_authors": "",
    "abstract": "Images today are increasingly shared online on social networking sites such as Facebook, Flickr, and Instagram. Image sharing occurs not only within a group of friends but also more and more outside a user’s social circles for purposes of social discovery. Despite that current social networking sites allow users to change their privacy preferences, this is often a cumbersome task for the vast majority of users on the Web, who face difficulties in assigning and managing privacy settings. When these privacy settings are used inappropriately, online image sharing can potentially lead to unwanted disclosures and privacy violations. Thus, automatically predicting images’ privacy to warn users about private or sensitive content before uploading these images on social networking sites has become a necessity in our current interconnected world. In this article, we explore learning models to automatically predict appropriate images’ privacy as private or public using carefully identified image-specific features. We study deep visual semantic features that are derived from various layers of Convolutional Neural Networks (CNNs) as well as textual features such as user tags and deep tags generated from deep CNNs. Particularly, we extract deep (visual and tag) features from four pre-trained CNN architectures for object recognition, i.e., AlexNet, GoogLeNet, VGG-16, and ResNet, and compare their performance for image privacy prediction. The results of our experiments obtained on a Flickr dataset of 32,000 images show that ResNet yeilds the best results for this task among all four networks. We also fine-tune the pre-trained CNN architectures on our privacy dataset and compare their performance with the models trained on pre-trained features. The results show that even though the overall performance obtained using the fine-tuned networks is comparable to that of pre-trained networks, the fine-tuned networks provide an improved performance for the private class. The results also show that the learning models trained on features extracted from ResNet outperform the state-of-the-art models for image privacy prediction. We further investigate the combination of user tags and deep tags derived from CNN architectures using two settings: (1) Support Vector Machines trained on the bag-of-tags features and (2) text-based CNN. We compare these models with the models trained on ResNet visual features and show that, even though the models trained on the visual features perform better than those trained on the tag features, the combination of deep visual features with image tags shows improvements in performance over the individual feature sets. We also compare our models with prior privacy prediction approaches and show that for private class, we achieve an improvement of ≈ 10% over prior CNN-based privacy prediction approaches. Our code, features, and the dataset used in experiments are available at https://github.com/ashwinitonge/deepprivate.git.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W3015444470",
    "type": "article"
  },
  {
    "title": "Summarizing Web Archive Corpora via Social Media Storytelling by Automatically Selecting and Visualizing Exemplars",
    "doi": "https://doi.org/10.1145/3606030",
    "publication_date": "2023-07-03",
    "publication_year": 2023,
    "authors": "Shawn Jones; Martin Klein; Michele C. Weigle; Michael L. Nelson",
    "corresponding_authors": "",
    "abstract": "People often create themed collections to make sense of an ever-increasing number of archived web pages. Some of these collections contain hundreds of thousands of documents. Thousands of collections exist, many covering the same topic. Few collections include standardized metadata. This scale makes understanding a collection an expensive proposition. Our Dark and Stormy Archives (DSA) five-process model implements a novel summarization method to help users understand a collection by combining web archives and social media storytelling. The five processes of the DSA model are: select exemplars, generate story metadata, generate document metadata, visualize the story, and distribute the story. Selecting exemplars produces a set of k documents from the N documents in the collection, where k &lt; &lt; N , thus reducing the number of documents visitors need to review to understand a collection. Generating story and document metadata selects images, titles, descriptions, and other content from these exemplars. Visualizing the story ties this metadata together in a format the visitor can consume. Without distributing the story, it is not shared for others to consume. We present a research study demonstrating that our algorithmic primitives can be combined to select relevant exemplars that are otherwise undiscoverable using a conventional search engine and query generation methods. Having demonstrated improved methods for selecting exemplars, we visualize the story. Previous work established that the social card is the best format for visitors to consume surrogates. The social card combines metadata fields, including the document’s title, a brief description, and a striking image. Social cards are commonly found on social media platforms. We discovered that these platforms perform poorly for mementos and rely on web page authors to supply the necessary values for these metadata fields. With web archives, we often encounter archived web pages that predate the existence of this metadata. To generate this missing metadata and ensure that storytelling is available for these documents, we apply machine learning to generate the images needed for social cards with a Precision@1 of 0.8314. We also provide the length values needed for executing automatic summarization algorithms to generate document descriptions. Applying these concepts helps us create the visualizations needed to fulfill the final processes of story generation. We close this work with examples and applications of this technology.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4382987138",
    "type": "article"
  },
  {
    "title": "Community-enhanced Link Prediction in Dynamic Networks",
    "doi": "https://doi.org/10.1145/3580513",
    "publication_date": "2023-01-24",
    "publication_year": 2023,
    "authors": "Mukesh Kumar; Shivansh Mishra; Shashank Sheshar Singh; Bhaskar Biswas",
    "corresponding_authors": "",
    "abstract": "The growing popularity of online social networks is quite evident nowadays and provides an opportunity to allow researchers in finding solutions for various practical applications. Link prediction is the technique of understanding network structure and identifying missing and future links in social networks. One of the well-known classes of methods in link prediction is a similarity-based method, which uses local and global topological information of the network to predict missing links. Some methods also exist based on quasi-local features to achieve a trade-off between local and global information on static networks. These quasi-local similarity-based methods are not best suited for considering community information in dynamic networks, failing to balance accuracy and efficiency. Therefore, a community-enhanced framework is presented in this article to predict missing links on dynamic social networks. First, a link prediction framework is presented to predict missing links using parameterized influence regions of nodes and their contribution in community partitions. Then, a unique feature set is generated using local, global, and quasi-local similarity-based as well as community information-based features. This feature set is further optimized using scoring-based feature selection methods to select only the most relevant features. Finally, four machine learning-based classification models are used for link prediction. The experiments are performed on six well-known dynamic networks and three performance metrics, and the results demonstrate that the proposed method outperforms the state-of-the-art methods.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4317904911",
    "type": "article"
  },
  {
    "title": "DCDIMB: Dynamic Community-based Diversified Influence Maximization using Bridge Nodes",
    "doi": "https://doi.org/10.1145/3664618",
    "publication_date": "2024-05-11",
    "publication_year": 2024,
    "authors": "Sunil Kumar Meena; Shashank Sheshar Singh; Kuldeep Singh",
    "corresponding_authors": "",
    "abstract": "Influence maximization (IM) is the fundamental study of social network analysis. The IM problem finds the top k nodes that have maximum influence in the network. Most of the studies in IM focus on maximizing the number of activated nodes in the static social network. But in real life, social networks are dynamic in nature. This work addresses the diversification of activated nodes in the dynamic social network. This work proposes an objective function that maximizes the number of communities by utilizing bridge nodes. We also propose a diffusion model that considers the role of inactive nodes in influencing a node. We prove the submodularity, and monotonicity of the objective function under the proposed diffusion model. This work analyzes the impact of different ratios of bridge nodes in the seed set on real-world and synthetic datasets. Furthermore, we prove the NP-Hardness of the objective function under the proposed diffusion model. The experiments are conducted on various real-world and synthetic datasets with known and unknown community information. The proposed work experimentally shows that the objective function gives the maximum number of communities considering bridge nodes compared with the benchmark algorithms.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4396834135",
    "type": "article"
  },
  {
    "title": "Visually Wired NFTs: Exploring the Role of Inspiration in Non-Fungible Tokens",
    "doi": "https://doi.org/10.1145/3703411",
    "publication_date": "2025-01-06",
    "publication_year": 2025,
    "authors": "Lucio La Cava; Davide Costa; Andrea Tagarelli",
    "corresponding_authors": "",
    "abstract": "The fervor for Non-Fungible Tokens (NFTs) attracted countless creators, leading to a Big Bang of digital assets driven by latent or explicit forms of inspiration, as in many creative processes. This work exploits Vision Transformers and graph-based modeling to delve into visual inspiration phenomena between NFTs over the years, i.e., the visual influence that can be detected whenever an NFT appears to be visually close to another that was published earlier in the market. Our goals include unveiling the main structural traits that shape visual inspiration networks, exploring the interrelation between visual inspiration and asset performances, investigating crypto influence on inspiration processes, and explaining the inspiration relationships among NFTs. Our findings unveil how the pervasiveness of inspiration led to a temporary saturation of the visual feature space, the impact of the dichotomy between inspiring and inspired NFTs on their financial performance, and an intrinsic self-regulatory mechanism between markets and inspiration waves. Our work can serve as a starting point for gaining a broader view of the evolution of Web3.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406110495",
    "type": "article"
  },
  {
    "title": "Modeling process-driven and service-oriented architectures using patterns and pattern primitives",
    "doi": "https://doi.org/10.1145/1281480.1281484",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Uwe Zdun; Carsten Hentrich; Schahram Dustdar",
    "corresponding_authors": "",
    "abstract": "Service-oriented architectures are increasingly used in the context of business processes. However, the proven practices for process-oriented integration of services are not well documented yet. In addition, modeling approaches for the integration of processes and services are neither mature nor do they exactly reflect the proven practices. In this article, we propose a pattern language for process-oriented integration of services to describe the proven practices. Our main contribution is a modeling concept based on pattern primitives for these patterns. A pattern primitive is a fundamental, precisely specified modeling element that represents a pattern. We present a catalog of pattern primitives that are precisely modeled using OCL constraints and map these primitives to the patterns in the pattern language of process-oriented integration of services. We also present a model validation tool that we have developed to support modeling the process-oriented integration of services, and an industrial case study in which we have applied our results.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2064500605",
    "type": "article"
  },
  {
    "title": "Decoding the structure of the WWW",
    "doi": "https://doi.org/10.1145/1255438.1255442",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "M. Ángeles Serrano; Ana Gabriela Maguitman; Marián Boguñá; Santo Fortunato; Alessandro Vespignani",
    "corresponding_authors": "",
    "abstract": "The understanding of the immense and intricate topological structure of the World Wide Web (WWW) is a major scientific and technological challenge. This has been recently tackled by characterizing the properties of its representative graphs, in which vertices and directed edges are identified with Web pages and hyperlinks, respectively. Data gathered in large-scale crawls have been analyzed by several groups resulting in a general picture of the WWW that encompasses many of the complex properties typical of rapidly evolving networks. In this article, we report a detailed statistical analysis of the topological properties of four different WWW graphs obtained with different crawlers. We find that, despite the very large size of the samples, the statistical measures characterizing these graphs differ quantitatively, and in some cases qualitatively, depending on the domain analyzed and the crawl used for gathering the data. This spurs the issue of the presence of sampling biases and structural differences of Web crawls that might induce properties not representative of the actual global underlying graph. In short, the stability of the widely accepted statistical description of the Web is called into question. In order to provide a more accurate characterization of the Web graph, we study statistical measures beyond the degree distribution, such as degree-degree correlation functions or the statistics of reciprocal connections. The latter appears to enclose the relevant correlations of the WWW graph and carry most of the topological information of the Web. The analysis of this quantity is also of major interest in relation to the navigability and searchability of the Web.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W1971892934",
    "type": "article"
  },
  {
    "title": "Mitigating application-level denial of service attacks on Web servers",
    "doi": "https://doi.org/10.1145/1377488.1377489",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Mudhakar Srivatsa; Arun Iyengar; Jian Yin; Ling Liu",
    "corresponding_authors": "",
    "abstract": "Recently, we have seen increasing numbers of denial of service (DoS) attacks against online services and Web applications either for extortion reasons or for impairing and even disabling the competition. These DoS attacks have increasingly targeted the application level. Application-level DoS attacks emulate the same request syntax and network-level traffic characteristics as those of legitimate clients, thereby making the attacks much harder to detect and counter. Moreover, such attacks often target bottleneck resources such as disk bandwidth, database bandwidth, and CPU resources. In this article, we propose handling DoS attacks by using a twofold mechanism. First, we perform admission control to limit the number of concurrent clients served by the online service. Admission control is based on port hiding that renders the online service invisible to unauthorized clients by hiding the port number on which the service accepts incoming requests. Second, we perform congestion control on admitted clients to allocate more resources to good clients. Congestion control is achieved by adaptively setting a client's priority level in response to the client's requests in a way that can incorporate application-level semantics. We present a detailed evaluation of the proposed solution using two sample applications: Apache HTTPD and the TPCW benchmark (running on Apache Tomcat and IBM DB2). Our experiments show that the proposed solution incurs low performance overhead and is resilient to DoS attacks.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2081029799",
    "type": "article"
  },
  {
    "title": "Automatic annotation of Web services based on workflow definitions",
    "doi": "https://doi.org/10.1145/1346337.1346239",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Khalid Belhajjame; Suzanne M. Embury; Norman W. Paton; Robert Stevens; Carole Goble",
    "corresponding_authors": "",
    "abstract": "Semantic annotations of web services can support the effective and efficient discovery of services, and guide their composition into workflows. At present, however, the practical utility of such annotations is limited by the small number of service annotations available for general use. Manual annotation of services is a time consuming and thus expensive task, so some means are required by which services can be automatically (or semi-automatically) annotated. In this paper, we show how information can be inferred about the semantics of operation parameters based on their connections to other (annotated) operation parameters within tried-and-tested workflows. Because the data links in the workflows do not necessarily contain every possible connection of compatible parameters, we can infer only constraints on the semantics of parameters. We show that despite their imprecise nature these so-called loose annotations are still of value in supporting the manual annotation task, inspecting workflows and discovering services. We also show that derived annotations for already annotated parameters are useful. By comparing existing and newly derived annotations of operation parameters, we can support the detection of errors in existing annotations, the ontology used for annotation and in workflows. The derivation mechanism has been implemented, and its practical applicability for inferring new annotations has been established through an experimental evaluation. The usefulness of the derived annotations is also demonstrated.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2030584203",
    "type": "article"
  },
  {
    "title": "Models and framework for supporting runtime decisions in Web-based systems",
    "doi": "https://doi.org/10.1145/1377488.1377491",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Mauro Andreolini; Sara Casolari; Michele Colajanni",
    "corresponding_authors": "",
    "abstract": "Efficient management of distributed Web-based systems requires several mechanisms that decide on request dispatching, load balance, admission control, request redirection. The algorithms behind these mechanisms typically make fast decisions on the basis of the load conditions of the system resources. The architecture complexity and workloads characterizing most Web-based services make it extremely difficult to deduce a representative view of a resource load from collected measures that show extreme variability even at different time scales. Hence, any decision based on instantaneous or average views of the system load may lead to useless or even wrong actions. As an alternative, we propose a two-phase strategy that first aims to obtain a representative view of the load trend from measured system values and then applies this representation to support runtime decision systems. We consider two classical problems behind decisions: how to detect significant and nontransient load changes of a system resource and how to predict its future load behavior. The two-phase strategy is based on stochastic functions that are characterized by a computational complexity that is compatible with runtime decisions. We describe, test, and tune the two-phase strategy by considering as a first example a multitier Web-based system that is subject to different classes of realistic and synthetic workloads. Also, we integrate the proposed strategy into a framework that we validate by applying it to support runtime decisions in a cluster Web system and in a locally distributed Network Intrusion Detection System.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2125751211",
    "type": "article"
  },
  {
    "title": "Unified publication and discovery of semantic Web services",
    "doi": "https://doi.org/10.1145/1541822.1541826",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Thomi Pilioura; Aphrodite Tsalgatidou",
    "corresponding_authors": "",
    "abstract": "The challenge of publishing and discovering Web services has recently received lots of attention. Various solutions to this problem have been proposed which, apart from their offered advantages, suffer the following disadvantages: (i) most of them are syntactic-based, leading to poor precision and recall, (ii) they are not scalable to large numbers of services, and (iii) they are incompatible, thus yielding in cumbersome service publication and discovery. This article presents the principles, the functionality, and the design of PYRAMID-S which addresses these disadvantages by providing a scalable framework for unified publication and discovery of semantically enhanced services over heterogeneous registries. PYRAMID-S uses a hybrid peer-to-peer topology to organize Web service registries based on domains. In such a topology, each Registry retains its autonomy, meaning that it can use the publication and discovery mechanisms as well as the ontology of its choice. The viability of this approach is demonstrated through the implementation and experimental analysis of a prototype.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2140748359",
    "type": "article"
  },
  {
    "title": "Combating spam in tagging systems",
    "doi": "https://doi.org/10.1145/1409220.1409225",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Georgia Koutrika; Frans Adjie Effendi; Zolt ́n Gyöngyi; Paul Heymann; Héctor García-Molina",
    "corresponding_authors": "",
    "abstract": "Tagging systems allow users to interactively annotate a pool of shared resources using descriptive strings called tags . Tags are used to guide users to interesting resources and help them build communities that share their expertise and resources. As tagging systems are gaining in popularity, they become more susceptible to tag spam : misleading tags that are generated in order to increase the visibility of some resources or simply to confuse users. Our goal is to understand this problem better. In particular, we are interested in answers to questions such as: How many malicious users can a tagging system tolerate before results significantly degrade? What types of tagging systems are more vulnerable to malicious attacks? What would be the effort and the impact of employing a trusted moderator to find bad postings? Can a system automatically protect itself from spam, for instance, by exploiting user tag patterns? In a quest for answers to these questions, we introduce a framework for modeling tagging systems and user tagging behavior. We also describe a method for ranking documents matching a tag based on taggers' reliability. Using our framework, we study the behavior of existing approaches under malicious attacks and the impact of a moderator and our ranking method.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2021196181",
    "type": "article"
  },
  {
    "title": "Protecting browsers from DNS rebinding attacks",
    "doi": "https://doi.org/10.1145/1462148.1462150",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Collin Jackson; Adam Barth; Andrew Bortz; Weidong Shao; Dan Boneh",
    "corresponding_authors": "",
    "abstract": "DNS rebinding attacks subvert the same-origin policy of browsers, converting them into open network proxies. Using DNS rebinding, an attacker can circumvent organizational and personal firewalls, send spam email, and defraud pay-per-click advertisers. We evaluate the cost effectiveness of mounting DNS rebinding attacks, finding that an attacker requires less than $100 to hijack 100,000 IP addresses. We analyze defenses to DNS rebinding attacks, including improvements to the classic “DNS pinning,” and recommend changes to browser plug-ins, firewalls, and Web servers. Our defenses have been adopted by plug-in vendors and by a number of open-source firewall implementations.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2079569045",
    "type": "article"
  },
  {
    "title": "Coordinating the web of services for a smart home",
    "doi": "https://doi.org/10.1145/2460383.2460389",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Eirini Kaldeli; Ehsan Ullah Warriach; Alexander Lazovik; Marco Aiello",
    "corresponding_authors": "",
    "abstract": "Domotics, concerned with the realization of intelligent home environments, is a novel field which can highly benefit from solutions inspired by service-oriented principles to enhance the convenience and security of modern home residents. In this work, we present an architecture for a smart home, starting from the lower device interconnectivity level up to the higher application layers that undertake the load of complex functionalities and provide a number of services to end-users. We claim that in order for smart homes to exhibit a genuinely intelligent behavior, the ability to compute compositions of individual devices automatically and dynamically is paramount. To this end, we incorporate into the architecture a composition component that employs artificial intelligence domain-independent planning to generate compositions at runtime, in a constantly evolving environment. We have implemented a fully working prototype that realizes such an architecture, and have evaluated it both in terms of performance as well as from the end-user point of view. The results of the evaluation show that the service-oriented architectural design and the support for dynamic compositions is quite efficient from the technical point of view, and that the system succeeds in satisfying the expectations and objectives of the users.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2030989609",
    "type": "article"
  },
  {
    "title": "Cost-Aware Strategies for Query Result Caching in Web Search Engines",
    "doi": "https://doi.org/10.1145/1961659.1961663",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Rifat Ozcan; İsmail Sengör Altıngövde; Özgür Ulusoy",
    "corresponding_authors": "",
    "abstract": "Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2075279061",
    "type": "article"
  },
  {
    "title": "Identifying Web Spam with the Wisdom of the Crowds",
    "doi": "https://doi.org/10.1145/2109205.2109207",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Yiqun Liu; Fei Chen; Weize Kong; Huijia Yu; Min Zhang; Shaoping Ma; Liyun Ru",
    "corresponding_authors": "",
    "abstract": "Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam-detection techniques are usually designed for specific, known types of Web spam and are incapable of dealing with newly appearing spam types efficiently. With user-behavior analyses from Web access logs, a spam page-detection algorithm is proposed based on a learning scheme. The main contributions are the following. (1) User-visiting patterns of spam pages are studied, and a number of user-behavior features are proposed for separating Web spam pages from ordinary pages. (2) A novel spam-detection framework is proposed that can detect various kinds of Web spam, including newly appearing ones, with the help of the user-behavior analysis. Experiments on large-scale practical Web access log data show the effectiveness of the proposed features and the detection framework.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W1991839217",
    "type": "article"
  },
  {
    "title": "Neighbor Selection and Weighting in User-Based Collaborative Filtering",
    "doi": "https://doi.org/10.1145/2579993",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Alejandro Bellogín; Pablo Castells; Iván Cantador",
    "corresponding_authors": "",
    "abstract": "User-based collaborative filtering systems suggest interesting items to a user relying on similar-minded people called neighbors. The selection and weighting of these neighbors characterize the different recommendation approaches. While standard strategies perform a neighbor selection based on user similarities, trust-aware recommendation algorithms rely on other aspects indicative of user trust and reliability. In this article we restate the trust-aware recommendation problem, generalizing it in terms of performance prediction techniques, whose goal is to predict the performance of an information retrieval system in response to a particular query. We investigate how to adopt the preceding generalization to define a unified framework where we conduct an objective analysis of the effectiveness (predictive power) of neighbor scoring functions. The proposed framework enables discriminating whether recommendation performance improvements are caused by the used neighbor scoring functions or by the ways these functions are used in the recommendation computation. We evaluated our approach with several state-of-the-art and novel neighbor scoring functions on three publicly available datasets. By empirically comparing four neighbor quality metrics and thirteen performance predictors, we found strong predictive power for some of the predictors with respect to certain metrics. This result was then validated by checking the final performance of recommendation strategies where predictors are used for selecting and/or weighting user neighbors. As a result, we have found that, by measuring the predictive power of neighbor performance predictors, we are able to anticipate which predictors are going to perform better in neighbor-scoring-powered versions of a user-based collaborative filtering algorithm.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2150978351",
    "type": "article"
  },
  {
    "title": "<i>MyAdChoices</i>",
    "doi": "https://doi.org/10.1145/2996466",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Javier Parra‐Arnau; Jagdish Prasad Achara; Claude Castelluccia",
    "corresponding_authors": "",
    "abstract": "The intrusiveness and the increasing invasiveness of online advertising have, in the last few years, raised serious concerns regarding user privacy and Web usability. As a reaction to these concerns, we have witnessed the emergence of a myriad of ad-blocking and antitracking tools, whose aim is to return control to users over advertising. The problem with these technologies, however, is that they are extremely limited and radical in their approach: users can only choose either to block or allow all ads. With around 200 million people regularly using these tools, the economic model of the Web—in which users get content free in return for allowing advertisers to show them ads—is at serious peril. In this article, we propose a smart Web technology that aims at bringing transparency to online advertising, so that users can make an informed and equitable decision regarding ad blocking. The proposed technology is implemented as a Web-browser extension and enables users to exert fine-grained control over advertising, thus providing them with certain guarantees in terms of privacy and browsing experience, while preserving the Internet economic model. Experimental results in a real environment demonstrate the suitability and feasibility of our approach, and provide preliminary findings on behavioral targeting from real user browsing profiles.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2266124366",
    "type": "article"
  },
  {
    "title": "Clickstream User Behavior Models",
    "doi": "https://doi.org/10.1145/3068332",
    "publication_date": "2017-07-25",
    "publication_year": 2017,
    "authors": "Gang Wang; Xinyi Zhang; Shiliang Tang; Christo Wilson; Hai-Tao Zheng; Ben Y. Zhao",
    "corresponding_authors": "",
    "abstract": "The next generation of Internet services is driven by users and user-generated content. The complex nature of user behavior makes it highly challenging to manage and secure online services. On one hand, service providers cannot effectively prevent attackers from creating large numbers of fake identities to disseminate unwanted content (e.g., spam). On the other hand, abusive behavior from real users also poses significant threats (e.g., cyberbullying). In this article, we propose clickstream models to characterize user behavior in large online services. By analyzing clickstream traces (i.e., sequences of click events from users), we seek to achieve two goals: (1) detection: to capture distinct user groups for the detection of malicious accounts, and (2) understanding: to extract semantic information from user groups to understand the captured behavior. To achieve these goals, we build two related systems. The first one is a semisupervised system to detect malicious user accounts (Sybils). The core idea is to build a clickstream similarity graph where each node is a user and an edge captures the similarity of two users’ clickstreams. Based on this graph, we propose a coloring scheme to identify groups of malicious accounts without relying on a large labeled dataset. We validate the system using ground-truth clickstream traces of 16,000 real and Sybil users from Renren, a large Chinese social network. The second system is an unsupervised system that aims to capture and understand the fine-grained user behavior. Instead of binary classification (malicious or benign), this model identifies the natural groups of user behavior and automatically extracts features to interpret their semantic meanings. Applying this system to Renren and another online social network, Whisper (100K users), we help service providers identify unexpected user behaviors and even predict users’ future actions. Both systems received positive feedback from our industrial collaborators including Renren, LinkedIn, and Whisper after testing on their internal clickstream data.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2738931638",
    "type": "article"
  },
  {
    "title": "Measuring the Visual Complexities of Web Pages",
    "doi": "https://doi.org/10.1145/2435215.2435216",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Ou Wu; Weiming Hu; Lei Shi",
    "corresponding_authors": "",
    "abstract": "Visual complexities (VisComs) of Web pages significantly affect user experience, and automatic evaluation can facilitate a large number of Web-based applications. The construction of a model for measuring the VisComs of Web pages requires the extraction of typical features and learning based on labeled Web pages. However, as far as the authors are aware, little headway has been made on measuring VisCom in Web mining and machine learning. The present article provides a new approach combining Web mining techniques and machine learning algorithms for measuring the VisComs of Web pages. The structure of a Web page is first analyzed, and the layout is then extracted. Using a Web page as a semistructured image, three classes of features are extracted to construct a feature vector. The feature vector is fed into a learned measuring function to calculate the VisCom of the page. In the proposed approach of the present study, the type of the measuring function and its learning depend on the quantification strategy for VisCom. Aside from using a category and a score to represent VisCom as existing work, this study presents a new strategy utilizing a distribution to quantify the VisCom of a Web page. Empirical evaluation suggests the effectiveness of the proposed approach in terms of both features and learning algorithms.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2022987557",
    "type": "article"
  },
  {
    "title": "Enhancing the trust-based recommendation process with explicit distrust",
    "doi": "https://doi.org/10.1145/2460383.2460385",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Patricia Victor; Nele Verbiest; Chris Cornelis; Martine De Cock",
    "corresponding_authors": "",
    "abstract": "When a Web application with a built-in recommender offers a social networking component which enables its users to form a trust network, it can generate more personalized recommendations by combining user ratings with information from the trust network. These are the so-called trust-enhanced recommendation systems. While research on the incorporation of trust for recommendations is thriving, the potential of explicitly stated distrust remains almost unexplored. In this article, we introduce a distrust-enhanced recommendation algorithm which has its roots in Golbeck's trust-based weighted mean. Through experiments on a set of reviews from Epinions.com, we show that our new algorithm outperforms its standard trust-only counterpart with respect to accuracy, thereby demonstrating the positive effect that explicit distrust can have on trust-based recommendations.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2050355627",
    "type": "article"
  },
  {
    "title": "Semantic contextual advertising based on the open directory project",
    "doi": "https://doi.org/10.1145/2529995.2529997",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Jung‐Hyun Lee; JongWoo Ha; J.H. Jung; SangKeun Lee",
    "corresponding_authors": "",
    "abstract": "Contextual advertising seeks to place relevant textual ads within the content of generic webpages. In this article, we explore a novel semantic approach to contextual advertising. This consists of three tasks: (1) building a well-organized hierarchical taxonomy of topics, (2) developing a robust classifier for effectively finding the topics of pages and ads, and (3) ranking ads based on the topical relevance to pages. First, we heuristically build our own taxonomy of topics from the Open Directory Project (ODP). Second, we investigate how to increase classification accuracy by taking the unique characteristics of the ODP into account. Last, we measure the topical relevance of ads by applying a link analysis technique to the similarity graph carefully derived from our taxonomy. Experiments show that our classification method improves the performance of Ma- F 1 by as much as 25.7% over the baseline classifier. In addition, our ranking method enhances the relevance of ads substantially, up to 10% in terms of precision at k , compared to a representative strategy.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2059792214",
    "type": "article"
  },
  {
    "title": "Constructing and Comparing User Mobility Profiles",
    "doi": "https://doi.org/10.1145/2637483",
    "publication_date": "2014-11-06",
    "publication_year": 2014,
    "authors": "Xihui Chen; Jun Pang; Ran Xue",
    "corresponding_authors": "",
    "abstract": "Nowadays, the accumulation of people's whereabouts due to location-based applications has made it possible to construct their mobility profiles. This access to users' mobility profiles subsequently brings benefits back to location-based applications. For instance, in on-line social networks, friends can be recommended not only based on the similarity between their registered information, for instance, hobbies and professions but also referring to the similarity between their mobility profiles. In this article, we propose a new approach to construct and compare users' mobility profiles. First, we improve and apply frequent sequential pattern mining technologies to extract the sequences of places that a user frequently visits and use them to model his mobility profile. Second, we present a new method to calculate the similarity between two users using their mobility profiles. More specifically, we identify the weaknesses of a similarity metric in the literature, and propose a new one which not only fixes the weaknesses but also provides more precise and effective similarity estimation. Third, we consider the semantics of spatio-temporal information contained in user mobility profiles and add them into the calculation of user similarity. It enables us to measure users' similarity from different perspectives. Two specific types of semantics are explored in this article: location semantics and temporal semantics . Last, we validate our approach by applying it to two real-life datasets collected by Microsoft Research Asia and Yonsei University, respectively. The results show that our approach outperforms the existing works from several aspects.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2006342384",
    "type": "article"
  },
  {
    "title": "Characterizing Web Censorship Worldwide",
    "doi": "https://doi.org/10.1145/2700339",
    "publication_date": "2015-01-23",
    "publication_year": 2015,
    "authors": "Phillipa Gill; Masashi Crete‐Nishihata; Jakub Dalek; Sharon Goldberg; Adam Senft; Greg Wiseman",
    "corresponding_authors": "",
    "abstract": "In this study, we take another look at 5 years of web censorship data gathered by the OpenNet Initiative in 77 countries using user-based testing with locally relevant content. Prior to our work, this data had been analyzed with little automation, focusing on what content had been blocked, rather than how blocking was carried out. In this study, we use more rigorous automation to obtain a longitudinal, global view of the technical means used for web censorship. We also identify blocking that had been missed in prior analyses. Our results point to considerable variability in the technologies used for web censorship, across countries, time, and types of content, and even across ISPs in the same country. In addition to characterizing web censorship in countries that, thus far, have eluded technical analysis, we also discuss the implications of our observations on the design of future network measurement platforms and circumvention technologies.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2053816477",
    "type": "article"
  },
  {
    "title": "The Augmented Web",
    "doi": "https://doi.org/10.1145/2735633",
    "publication_date": "2015-05-19",
    "publication_year": 2015,
    "authors": "Óscar Díaz; Cristóbal Arellano",
    "corresponding_authors": "",
    "abstract": "Today’s web personalization technologies use approaches like user categorization, configuration, and customization but do not fully support individualized requirements. As a significant portion of our social and working interactions are migrating to the web, we can expect an increase in these kinds of minority requirements. Browser-side transcoding holds the promise of facilitating this aim by opening personalization to third parties through web augmentation (WA), realized in terms of extensions and userscripts. WA is to the web what augmented reality is to the physical world: to layer relevant content/layout/navigation over the existing web to improve the user experience. From this perspective, WA is not as powerful as web personalization since its scope is limited to the surface of the web. However, it permits this surface to be tuned by developers other than the sites’ webmasters. This opens up the web to third parties who might come up with imaginative ways of adapting the web surface for their own purposes. Its success is backed up by millions of downloads. This work looks at this phenomenon, delving into the “what,” the “why,” and the “what for” of WA, and surveys the challenges ahead for WA to thrive. To this end, we appraise the most downloaded 45 WA extensions for Mozilla Firefox and Google Chrome as well as conduct a systematic literature review to identify what quality issues received the most attention in the literature. The aim is to raise awareness about WA as a key enabler of the personal web and point out research directions.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2203796190",
    "type": "article"
  },
  {
    "title": "Content Bias in Online Health Search",
    "doi": "https://doi.org/10.1145/2663355",
    "publication_date": "2014-11-06",
    "publication_year": 2014,
    "authors": "Ryen W. White; Ahmed H. Yousef",
    "corresponding_authors": "",
    "abstract": "Search engines help people answer consequential questions. Biases in retrieved and indexed content (e.g., skew toward erroneous outcomes that represent deviations from reality), coupled with searchers' biases in how they examine and interpret search results, can lead people to incorrect answers. In this article, we seek to better understand biases in search and retrieval, and in particular those affecting the accuracy of content in search results, including the search engine index, features used for ranking, and the formulation of search queries. Focusing on the important domain of online health search, this research broadens previous work on biases in search to examine the role of search systems in contributing to biases. To assess bias, we focus on questions about medical interventions and employ reliable ground truth data from authoritative medical sources. In the course of our study, we utilize large-scale log analysis using data from a popular Web search engine, deep probes of result lists on that search engine, and crowdsourced human judgments of search result captions and landing pages. Our findings reveal bias in results, amplifying searchers' existing biases that appear evident in their search activity. We also highlight significant bias in indexed content and show that specific ranking signals and specific query terms support bias. Both of these can degrade result accuracy and increase skewness in search results. Our analysis has implications for bias mitigation strategies in online search systems, and we offer recommendations for search providers based on our findings.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2079106364",
    "type": "article"
  },
  {
    "title": "Modeling, Enacting, and Integrating Custom Crowdsourcing Processes",
    "doi": "https://doi.org/10.1145/2746353",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Stefano Tranquillini; Florian Daniel; Pavel Kucherbaev; Fabio Casati",
    "corresponding_authors": "",
    "abstract": "Crowdsourcing (CS) is the outsourcing of a unit of work to a crowd of people via an open call for contributions. Thanks to the availability of online CS platforms, such as Amazon Mechanical Turk or CrowdFlower, the practice has experienced a tremendous growth over the past few years and demonstrated its viability in a variety of fields, such as data collection and analysis or human computation. Yet it is also increasingly struggling with the inherent limitations of these platforms: each platform has its own logic of how to crowdsource work (e.g., marketplace or contest), there is only very little support for structured work (work that requires the coordination of multiple tasks), and it is hard to integrate crowdsourced tasks into state-of-the-art business process management (BPM) or information systems. We attack these three shortcomings by (1) developing a flexible CS platform (we call it Crowd Computer , or CC) that allows one to program custom CS logics for individual and structured tasks, (2) devising a BPMN--based modeling language that allows one to program CC intuitively, (3) equipping the language with a dedicated visual editor, and (4) implementing CC on top of standard BPM technology that can easily be integrated into existing software and processes. We demonstrate the effectiveness of the approach with a case study on the crowd-based mining of mashup model patterns.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2205712465",
    "type": "article"
  },
  {
    "title": "A Novel Evidence-Based Bayesian Similarity Measure for Recommender Systems",
    "doi": "https://doi.org/10.1145/2856037",
    "publication_date": "2016-05-25",
    "publication_year": 2016,
    "authors": "Guibing Guo; Jie Zhang; Neil Yorke‐Smith",
    "corresponding_authors": "",
    "abstract": "User-based collaborative filtering , a widely used nearest neighbour-based recommendation technique, predicts an item’s rating by aggregating its ratings from similar users. User similarity is traditionally calculated by cosine similarity or the Pearson correlation coefficient . However, both of these measures consider only the direction of rating vectors, and suffer from a range of drawbacks. To overcome these issues, we propose a novel Bayesian similarity measure based on the Dirichlet distribution, taking into consideration both the direction and length of rating vectors. We posit that not all the rating pairs should be equally counted in order to accurately model user correlation. Three different evidence factors are designed to compute the weights of rating pairs. Further, our principled method reduces correlation due to chance and potential system bias. Experimental results on six real-world datasets show that our method achieves superior accuracy in comparison with counterparts.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2402720608",
    "type": "article"
  },
  {
    "title": "Toward Automated Online Photo Privacy",
    "doi": "https://doi.org/10.1145/2983644",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Anna Squicciarini; Cornelia Caragea; Rahul Balakavi",
    "corresponding_authors": "",
    "abstract": "Online photo sharing is an increasingly popular activity for Internet users. More and more users are now constantly sharing their images in various social media, from social networking sites to online communities, blogs, and content sharing sites. In this article, we present an extensive study exploring privacy and sharing needs of users’ uploaded images. We develop learning models to estimate adequate privacy settings for newly uploaded images, based on carefully selected image-specific features. Our study investigates both visual and textual features of images for privacy classification. We consider both basic image-specific features, commonly used for image processing, as well as more sophisticated and abstract visual features. Additionally, we include a visual representation of the sentiment evoked by images. To our knowledge, sentiment has never been used in the context of image classification for privacy purposes. We identify the smallest set of features, that by themselves or combined together with others, can perform well in properly predicting the degree of sensitivity of users’ images. We consider both the case of binary privacy settings (i.e., public, private), as well as the case of more complex privacy options, characterized by multiple sharing options. Our results show that with few carefully selected features, one may achieve high accuracy, especially when high-quality tags are available.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2604613829",
    "type": "article"
  },
  {
    "title": "Analyzing Privacy Policies at Scale",
    "doi": "https://doi.org/10.1145/3230665",
    "publication_date": "2018-12-04",
    "publication_year": 2018,
    "authors": "Shomir Wilson; Florian Schaub; Frederick Liu; Kanthashree Mysore Sathyendra; Daniel Smullen; Sebastian Zimmeck; Rohan Ramanath; Peter Story; Fei Liu; Norman Sadeh; Noah A. Smith",
    "corresponding_authors": "",
    "abstract": "Website privacy policies are often long and difficult to understand. While research shows that Internet users care about their privacy, they do not have the time to understand the policies of every website they visit, and most users hardly ever read privacy policies. Some recent efforts have aimed to use a combination of crowdsourcing, machine learning, and natural language processing to interpret privacy policies at scale, thus producing annotations for use in interfaces that inform Internet users of salient policy details. However, little attention has been devoted to studying the accuracy of crowdsourced privacy policy annotations, how crowdworker productivity can be enhanced for such a task, and the levels of granularity that are feasible for automatic analysis of privacy policies. In this article, we present a trajectory of work addressing each of these topics. We include analyses of crowdworker performance, evaluation of a method to make a privacy-policy oriented task easier for crowdworkers, a coarse-grained approach to labeling segments of policy text with descriptive themes, and a fine-grained approach to identifying user choices described in policy text. Together, the results from these efforts show the effectiveness of using automated and semi-automated methods for extracting from privacy policies the data practice details that are salient to Internet users’ interests.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2903400086",
    "type": "article"
  },
  {
    "title": "QoS-aware Automatic Web Service Composition with Multiple Objectives",
    "doi": "https://doi.org/10.1145/3389147",
    "publication_date": "2020-05-18",
    "publication_year": 2020,
    "authors": "Soumi Chattopadhyay; Ansuman Banerjee",
    "corresponding_authors": "",
    "abstract": "Automatic web service composition has received a significant research attention in service-oriented computing over decades of research. With increasing number of web services, providing an end-to-end Quality of Service (QoS) guarantee in responding to user queries is becoming an important concern. Multiple QoS parameters (e.g., response time, latency, throughput, reliability, availability, success rate) are associated with a service, thereby, service composition with a large number of candidate services is a challenging multi-objective optimization problem. In this article, we study the multi-constrained multi-objective QoS-aware web service composition problem and propose three different approaches to solve the same, one optimal, based on Pareto front construction, and two others based on heuristically traversing the solution space. We compare the performance of the heuristics against the optimal and show the effectiveness of our proposals over other classical approaches for the same problem setting, with experiments on WSC-2009 and ICEBE-2005 datasets.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W3029542544",
    "type": "article"
  },
  {
    "title": "Categorizing Sexism and Misogyny through Neural Approaches",
    "doi": "https://doi.org/10.1145/3457189",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Pulkit Parikh; Harika Abburi; Niyati Chhaya; Manish Gupta; Vasudeva Varma",
    "corresponding_authors": "",
    "abstract": "Sexism, an injustice that subjects women and girls to enormous suffering, manifests in blatant as well as subtle ways. In the wake of growing documentation of experiences of sexism on the web, the automatic categorization of accounts of sexism has the potential to assist social scientists and policymakers in studying and thereby countering sexism. The existing work on sexism classification has certain limitations in terms of the categories of sexism used and/or whether they can co-occur. To the best of our knowledge, this is the first work on the multi-label classification of sexism of any kind(s). 1 We also consider the related task of misogyny classification. While sexism classification is performed on textual accounts describing sexism suffered or observed, misogyny classification is carried out on tweets perpetrating misogyny. We devise a novel neural framework for classifying sexism and misogyny that can combine text representations obtained using models such as Bidirectional Encoder Representations from Transformers with distributional and linguistic word embeddings using a flexible architecture involving recurrent components and optional convolutional ones. Further, we leverage unlabeled accounts of sexism to infuse domain-specific elements into our framework. To evaluate the versatility of our neural approach for tasks pertaining to sexism and misogyny, we experiment with adapting it for misogyny identification. For categorizing sexism, we investigate multiple loss functions and problem transformation techniques to address the multi-label problem formulation. We develop an ensemble approach using a proposed multi-label classification model with potentially overlapping subsets of the category set. Proposed methods outperform several deep-learning as well as traditional machine learning baselines for all three tasks.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W3170818115",
    "type": "article"
  },
  {
    "title": "Toward Fair Recommendation in Two-sided Platforms",
    "doi": "https://doi.org/10.1145/3503624",
    "publication_date": "2021-12-21",
    "publication_year": 2021,
    "authors": "Arpita Biswas; Gourab K Patro; Niloy Ganguly; Krishna P. Gummadi; Abhijnan Chakraborty",
    "corresponding_authors": "",
    "abstract": "Many online platforms today (such as Amazon, Netflix, Spotify, LinkedIn, and AirBnB) can be thought of as two-sided markets with producers and customers of goods and services. Traditionally, recommendation services in these platforms have focused on maximizing customer satisfaction by tailoring the results according to the personalized preferences of individual customers. However, our investigation reinforces the fact that such customer-centric design of these services may lead to unfair distribution of exposure to the producers, which may adversely impact their well-being. However, a pure producer-centric design might become unfair to the customers. As more and more people are depending on such platforms to earn a living, it is important to ensure fairness to both producers and customers. In this work, by mapping a fair personalized recommendation problem to a constrained version of the problem of fairly allocating indivisible goods, we propose to provide fairness guarantees for both sides. Formally, our proposed FairRec algorithm guarantees Maxi-Min Share of exposure for the producers, and Envy-Free up to One Item fairness for the customers. Extensive evaluations over multiple real-world datasets show the effectiveness of FairRec in ensuring two-sided fairness while incurring a marginal loss in overall recommendation quality. Finally, we present a modification of FairRec (named as FairRecPlus ) that at the cost of additional computation time, improves the recommendation performance for the customers, while maintaining the same fairness guarantees.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W4200503933",
    "type": "article"
  },
  {
    "title": "ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval",
    "doi": "https://doi.org/10.1145/3572405",
    "publication_date": "2022-11-22",
    "publication_year": 2022,
    "authors": "Xiao Wang; Craig Macdonald; Nicola Tonellotto; Iadh Ounis",
    "corresponding_authors": "",
    "abstract": "Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models, have shown the usefulness of expanding and reweighting the users’ initial queries using information occurring in an initial set of retrieved documents, known as the pseudo-relevant set. Recently, dense retrieval – through the use of neural contextual language models such as BERT for analysing the documents’ and queries’ contents and computing their relevance scores – has shown a promising performance on several information retrieval tasks still relying on the traditional inverted index for identifying documents relevant to a query. Two different dense retrieval families have emerged: the use of single embedded representations for each passage and query, e.g., using BERT’s [CLS] token, or via multiple representations, e.g., using an embedding for each token of the query and document (exemplified by ColBERT). In this work, we conduct the first study into the potential for multiple representation dense retrieval to be enhanced using pseudo-relevance feedback and present our proposed approach ColBERT-PRF. In particular, based on the pseudo-relevant set of documents identified using a first-pass dense retrieval, ColBERT-PRF extracts the representative feedback embeddings from the document embeddings of the pseudo-relevant set. Among the representative feedback embeddings, the embeddings that most highly discriminate among documents are employed as the expansion embeddings, which are then added to the original query representation. We show that these additional expansion embeddings both enhance the effectiveness of a reranking of the initial query results as well as an additional dense retrieval operation. Indeed, experiments on the MSMARCO passage ranking dataset show that MAP can be improved by up to 26% on the TREC 2019 query set and 10% on the TREC 2020 query set by the application of our proposed ColBERT-PRF method on a ColBERT dense retrieval approach.We further validate the effectiveness of our proposed pseudo-relevance feedback technique for a dense retrieval model on MSMARCO document ranking and TREC Robust04 document ranking tasks. For instance, ColBERT-PRF exhibits up to 21% and 14% improvement in MAP over the ColBERT E2E model on the MSMARCO document ranking TREC 2019 and TREC 2020 query sets, respectively. Additionally, we study the effectiveness of variants of the ColBERT-PRF model with different weighting methods. Finally, we show that ColBERT-PRF can be made more efficient, attaining up to 4.54× speedup over the default ColBERT-PRF model, and with little impact on effectiveness, through the application of approximate scoring and different clustering methods.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4309698332",
    "type": "article"
  },
  {
    "title": "Personalized Visualization Recommendation",
    "doi": "https://doi.org/10.1145/3538703",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Xin Qian; Ryan A. Rossi; Fan Du; Sungchul Kim; Eunyee Koh; Sana Malik; Tak Yeon Lee; Nesreen K. Ahmed",
    "corresponding_authors": "",
    "abstract": "Visualization recommendation work has focused solely on scoring visualizations based on the underlying dataset, and not the actual user and their past visualization feedback. These systems recommend the same visualizations for every user, despite that the underlying user interests, intent, and visualization preferences are likely to be fundamentally different, yet vitally important. In this work, we formally introduce the problem of personalized visualization recommendation and present a generic learning framework for solving it. In particular, we focus on recommending visualizations personalized for each individual user based on their past visualization interactions (e.g., viewed, clicked, manually created) along with the data from those visualizations. More importantly, the framework can learn from visualizations relevant to other users, even if the visualizations are generated from completely different datasets. Experiments demonstrate the effectiveness of the approach as it leads to higher quality visualization recommendations tailored to the specific user intent and preferences. To support research on this new problem, we release our user-centric visualization corpus consisting of 17.4k users exploring 94k datasets with 2.3 million attributes and 32k user-generated visualizations.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3131703615",
    "type": "article"
  },
  {
    "title": "My Tweets Bring All the Traits to the Yard: Predicting Personality and Relational Traits in Online Social Networks",
    "doi": "https://doi.org/10.1145/3523749",
    "publication_date": "2022-05-20",
    "publication_year": 2022,
    "authors": "Dimitra Karanatsiou; Pavlos Sermpezis; Dritjon Gruda; Konstantinos Kafetsios; Ilias Dimitriadis; Athena Vakali",
    "corresponding_authors": "",
    "abstract": "Users in Online Social Networks (OSNs,) leave traces that reflect their personality characteristics. The study of these traces is important for several fields, such as social science, psychology, marketing, and others. Despite a marked increase in research on personality prediction based on online behavior, the focus has been heavily on individual personality traits, and by doing so, largely neglects relational facets of personality. This study aims to address this gap by providing a prediction model for holistic personality profiling in OSNs that includes socio-relational traits (attachment orientations) in combination with standard personality traits. Specifically, we first designed a feature engineering methodology that extracts a wide range of features (accounting for behavior, language, and emotions) from the OSN accounts of users. Subsequently, we designed a machine learning model that predicts trait scores of users based on the extracted features. The proposed model architecture is inspired by characteristics embedded in psychology; i.e, it utilizes interrelations among personality facets and leads to increased accuracy in comparison with other state-of-the-art approaches. To demonstrate the usefulness of this approach, we applied our model on two datasets, namely regular OSN users and opinion leaders on social media, and contrast both samples’ psychological profiles. Our findings demonstrate that the two groups can be clearly separated by focusing on both Big Five personality traits and attachment orientations. The presented research provides a promising avenue for future research on OSN user characterization and classification.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3089027026",
    "type": "article"
  },
  {
    "title": "Incorporating a Triple Graph Neural Network with Multiple Implicit Feedback for Social Recommendation",
    "doi": "https://doi.org/10.1145/3580517",
    "publication_date": "2023-01-21",
    "publication_year": 2023,
    "authors": "Haorui Zhu; Fei Xiong; Hongshu Chen; Xi Xiong; Liang Wang",
    "corresponding_authors": "",
    "abstract": "Graph neural networks have been clearly proven to be powerful in recommendation tasks since they can capture high-order user-item interactions and integrate them with rich attributes. However, they are still limited by the cold-start problem and data sparsity. Using social relationships to assist recommendation is an effective practice, but it can only moderately alleviate these problems. In addition, rich attributes are often unavailable, which prevents graph neural networks from being fully effective. Hence, we propose to enrich the model by mining multiple implicit feedback and constructing a triple GCN component. We have noticed that users may be influenced not only by their trusted friends but also by the ratings that already exist. The implicit influence spreads among the item’s previous and potential raters, and makes a difference on future ratings. The implicit influence is analyzed on the mechanism of information propagation, and fused with the user’s binary implicit attitude, since negative influence propagates as well as the positive one. Furthermore, we leverage explicit feedback, social relationships, and multiple implicit feedback in the triple GCN component. Abundant experiments on real-world datasets reveal that our model has improved significantly in the rating prediction task compared with other state-of-the-art methods.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4317659173",
    "type": "article"
  },
  {
    "title": "Contrastive Graph Similarity Networks",
    "doi": "https://doi.org/10.1145/3580511",
    "publication_date": "2023-01-30",
    "publication_year": 2023,
    "authors": "Luzhi Wang; Yizhen Zheng; Di Jin; Fuyi Li; Yongliang Qiao; Shirui Pan",
    "corresponding_authors": "",
    "abstract": "Graph similarity learning is a significant and fundamental issue in the theory and analysis of graphs, which has been applied in a variety of fields, including object tracking, recommender systems, similarity search, and so on. Recent methods for graph similarity learning that utilize deep learning typically share two deficiencies: (1) they leverage graph neural networks as backbones for learning graph representations but have not well captured the complex information inside data, and (2) they employ a cross-graph attention mechanism for graph similarity learning, which is computationally expensive. Taking these limitations into consideration, a method for graph similarity learning is devised in this study, namely, Contrastive Graph Similarity Network (CGSim). To enhance graph similarity learning, CGSim makes use of the complementary information of two input graphs and captures pairwise relations in a contrastive learning framework. By developing a dual contrastive learning module with a node-graph matching and a graph-graph matching mechanism, our method significantly reduces the quadratic time complexity for cross-graph interaction modeling to linear time complexity. Jointly learning in an end-to-end framework, the graph representation embedding module and the well-designed contrastive learning module can be beneficial to one another. A comprehensive series of experiments indicate that CGSim outperforms state-of-the-art baselines on six datasets and significantly reduces the computational cost, which demonstrates our CGSim model’s superiority over other baselines.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4318477403",
    "type": "article"
  },
  {
    "title": "Heterogeneous Graph Neural Network with Personalized and Adaptive Diversity for News Recommendation",
    "doi": "https://doi.org/10.1145/3649886",
    "publication_date": "2024-03-08",
    "publication_year": 2024,
    "authors": "Guangping Zhang; Dongsheng Li; Hansu Gu; Tun Lu; Ning Gu",
    "corresponding_authors": "",
    "abstract": "The emergence of online media has facilitated the dissemination of news, but has also introduced the problem of information overload. To address this issue, providing users with accurate and diverse news recommendations has become increasingly important. News possesses rich and heterogeneous content, and the factors that attract users to news reading are varied. Consequently, accurate news recommendation requires modeling of both the heterogeneous content of news and the heterogeneous user-news relationships. Furthermore, users’ news consumption is highly dynamic, which is reflected in the differences in topic concentration among different users and in the real-time changes in user interests. To this end, we propose a Heterogeneous Graph Neural Network with Personalized and Adaptive Diversity for News Recommendation (DivHGNN). DivHGNN first represents the heterogeneous content of news and the heterogeneous user-news relationships as an attributed heterogeneous graph. Then, through a heterogeneous node content adapter, it models the heterogeneous node attributes into aligned and fused node representations. With the proposed attributed heterogeneous graph neural network, DivHGNN integrates the heterogeneous relationships to enhance node representation for accurate news recommendations. We also discuss relation pruning, model deployment, and cold-start issues to further improve model efficiency. In terms of diversity, DivHGNN simultaneously models the variance of nodes through variational representation learning for providing personalized diversity. Additionally, a time-continuous exponentially decaying distribution cache is proposed to model the temporal dynamics of user real-time interests for providing adaptive diversity. Extensive experiments on real-world news datasets demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4392595024",
    "type": "article"
  },
  {
    "title": "Integrating Content Moderation Systems with Large Language Models",
    "doi": "https://doi.org/10.1145/3700789",
    "publication_date": "2024-10-17",
    "publication_year": 2024,
    "authors": "Mirko Franco; Ombretta Gaggi; Claudio E. Palazzi",
    "corresponding_authors": "",
    "abstract": "Online Social Networks (OSNs) rely on content moderation systems to ensure platform and user safety by preventing malicious activities, like the spread of harmful content. However, there is a growing consensus suggesting that such systems are unfair to historically marginalized individuals, fragile users, and minorities. Additionally, OSN policies are often hardcoded in AI-based violation classifiers, making personalized content moderation challenging. In addition, there is a need for more communication between users and platform administrators, especially in case of disagreement about a moderation decision. To address these issues, we propose integrating content moderation systems with Large Language Models (LLMs) to enhance support for personal content moderation and improve user-platform communication. We also evaluate the content moderation capabilities of GPT 3.5 and LLaMa 2, comparing them to commercial products, as well as discuss the limitations of our approach and the open research directions.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4403490182",
    "type": "article"
  },
  {
    "title": "Introduction to special section on adversarial issues in Web search",
    "doi": "https://doi.org/10.1145/1326561.1326562",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Marc Najork; Brian D. Davison",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2008030886",
    "type": "article"
  },
  {
    "title": "The effects of proxy bidding and minimum bid increments within eBay auctions",
    "doi": "https://doi.org/10.1145/1255438.1255441",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Alex Rogers; Esther David; Nicholas R. Jennings; Jeremy Schiff",
    "corresponding_authors": "",
    "abstract": "We present a mathematical model of the eBay auction protocol and perform a detailed analysis of the effects that the eBay proxy bidding system and the minimum bid increment have on the auction properties. We first consider the revenue of the auction, and we show analytically that when two bidders with independent private valuations use the eBay proxy bidding system there exists an optimal value for the minimum bid increment at which the auctioneer's revenue is maximized. We then consider the sequential way in which bids are placed within the auction, and we show analytically that independent of assumptions regarding the bidders' valuation distribution or bidding strategy the number of visible bids placed is related to the logarithm of the number of potential bidders. Thus, in many cases, it is only a minority of the potential bidders that are able to submit bids and are visible in the auction bid history (despite the fact that the other hidden bidders are still effectively competing for the item). Furthermore, we show through simulation that the minimum bid increment also introduces an inefficiency to the auction, whereby a bidder who enters the auction late may find that its valuation is insufficient to allow them to advance the current bid by the minimum bid increment despite them actually having the highest valuation for the item. Finally, we use these results to consider appropriate strategies for bidders within real world eBay auctions. We show that while last-minute bidding (sniping) is an effective strategy against bidders engaging in incremental bidding (and against those with common values), in general, delaying bidding is disadvantageous even if delayed bids are sure to be received before the auction closes. Thus, when several bidders submit last-minute bids, we show that rather than seeking to bid as late as possible, a bidder should try to be the first sniper to bid (i.e., it should “snipe before the snipers”).",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2043962114",
    "type": "article"
  },
  {
    "title": "An environment for flexible advanced compensations of Web service transactions",
    "doi": "https://doi.org/10.1145/1346337.1346242",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Michael Schäfer; Peter Dolog; Wolfgang Nejdl",
    "corresponding_authors": "",
    "abstract": "Business to business integration has recently been performed by employing Web service environments. Moreover, such environments are being provided by major players on the technology markets. Those environments are based on open specifications for transaction coordination. When a failure in such an environment occurs, a compensation can be initiated to recover from the failure. However, current environments have only limited capabilities for compensations, and are usually based on backward recovery. In this article, we introduce an environment to deal with advanced compensations based on forward recovery principles. We extend the existing Web service transaction coordination architecture and infrastructure in order to support flexible compensation operations. We use a contract-based approach, which allows the specification of permitted compensations at runtime. We introduce abstract service and adapter components, which allow us to separate the compensation logic from the coordination logic. In this way, we can easily plug in or plug out different compensation strategies based on a specification language defined on top of basic compensation activities and complex compensation types. Experiments with our approach and environment show that such an approach to compensation is feasible and beneficial. Additionally, we introduce a cost-benefit model to evaluate the proposed environment based on net value analysis. The evaluation shows in which circumstances the environment is economical.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W1989392320",
    "type": "article"
  },
  {
    "title": "Classifying search queries using the Web as a source of knowledge",
    "doi": "https://doi.org/10.1145/1513876.1513877",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Evgeniy Gabrilovich; Andrei Broder; Marcus Fontoura; Amruta Joshi; Vanja Josifovski; Lance Riedel; Tong Zhang",
    "corresponding_authors": "",
    "abstract": "We propose a methodology for building a robust query classification system that can identify thousands of query classes, while dealing in real time with the query volume of a commercial Web search engine. We use a pseudo relevance feedback technique: given a query, we determine its topic by classifying the Web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregate account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2158801982",
    "type": "article"
  },
  {
    "title": "Detecting splogs via temporal dynamics using self-similarity analysis",
    "doi": "https://doi.org/10.1145/1326561.1326565",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Yu‐Ru Lin; Hari Sundaram; Yün Chi; Junichi Tatemura; Belle L. Tseng",
    "corresponding_authors": "",
    "abstract": "This article addresses the problem of spam blog (splog) detection using temporal and structural regularity of content, post time and links. Splogs are undesirable blogs meant to attract search engine traffic, used solely for promoting affiliate sites. Blogs represent popular online media, and splogs not only degrade the quality of search engine results, but also waste network resources. The splog detection problem is made difficult due to the lack of stable content descriptors. We have developed a new technique for detecting splogs, based on the observation that a blog is a dynamic, growing sequence of entries (or posts) rather than a collection of individual pages. In our approach, splogs are recognized by their temporal characteristics and content. There are three key ideas in our splog detection framework. (a) We represent the blog temporal dynamics using self-similarity matrices defined on the histogram intersection similarity measure of the time, content, and link attributes of posts, to investigate the temporal changes of the post sequence. (b) We study the blog temporal characteristics using a visual representation derived from the self-similarity measures. The visual signature reveals correlation between attributes and posts, depending on the type of blogs (normal blogs and splogs). (c) We propose two types of novel temporal features to capture the splog temporal characteristics. In our splog detector, these novel features are combined with content based features. We extract a content based feature vector from blog home pages as well as from different parts of the blog. The dimensionality of the feature vector is reduced by Fisher linear discriminant analysis. We have tested an SVM-based splog detector using proposed features on real world datasets, with appreciable results (90% accuracy).",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2035621475",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on recommender systems",
    "doi": "https://doi.org/10.1145/1921591.1921592",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "John Riedl; Barry Smyth",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2050205798",
    "type": "article"
  },
  {
    "title": "A Survey of Requirements Specification in Model-Driven Development of Web Applications",
    "doi": "https://doi.org/10.1145/1961659.1961664",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Pedro Valderas; Vicente Pelechano",
    "corresponding_authors": "",
    "abstract": "Model-driven development has become more and more important in the last few years. In the context of web application development, many web Engineering methods that propose model-driven development processes have appeared. However, earlier stages of these processes are seldom considered and few of these methods rigorously face the problems of specifying web application requirements and translating them into the proper conceptual model. However, it is widely recognized that requirements engineering activities are essential to obtain quality software products. This article surveys Model-driven web engineering methods in a comparative study and analyzes the techniques proposed for specifying functional, data and navigational requirements as well as the mechanisms provided for automatically translating these requirements into conceptual models. Our main goal is to provide a critical view of the support that is provided by these methods for handling web application requirements in order to show their current limitations and strengths.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W1965575775",
    "type": "article"
  },
  {
    "title": "A test-based security certification scheme for web services",
    "doi": "https://doi.org/10.1145/2460383.2460384",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Marco Anisetti; Claudio A. Ardagna; Ernesto Damiani; Francesco Saonara",
    "corresponding_authors": "",
    "abstract": "The Service-Oriented Architecture (SOA) paradigm is giving rise to a new generation of applications built by dynamically composing loosely coupled autonomous services. Clients (i.e., software agents acting on behalf of human users or service providers) implementing such complex applications typically search and integrate services on the basis of their functional requirements and of their trust in the service suppliers. A major issue in this scenario relates to the definition of an assurance technique allowing clients to select services on the basis of their nonfunctional requirements and increasing their confidence that the selected services will satisfy such requirements. In this article, we first present an assurance solution that focuses on security and supports a test-based security certification scheme for Web services. The certification scheme is driven by the security properties to be certified and relies upon a formal definition of the service model. The evidence supporting a certified property is computed using a model-based testing approach that, starting from the service model, automatically generates the test cases to be used in the service certification. We also define a set of indexes and metrics that evaluate the assurance level and the quality of the certification process. Finally, we present our evaluation toolkit and experimental results obtained applying our certification solution to a financial service implementing the Interactive Financial eXchange (IFX) standard.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W1998987919",
    "type": "article"
  },
  {
    "title": "On Computing Deltas of RDF/S Knowledge Bases",
    "doi": "https://doi.org/10.1145/1993053.1993056",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Dimitris Zeginis; Yannis Tzitzikas; Vassilis Christophides",
    "corresponding_authors": "",
    "abstract": "The ability to compute the differences that exist between two RDF/S Knowledge Bases (KB) is an important step to cope with the evolving nature of the Semantic Web (SW). In particular, RDF/S deltas can be employed to reduce the amount of data that need to be exchanged and managed over the network in order to build SW synchronization and versioning services. By considering deltas as sets of change operations, in this article we introduce various RDF/S differential functions which take into account inferred knowledge from an RDF/S knowledge base. We first study their correctness in transforming a source to a target RDF/S knowledge base in conjunction with the semantics of the employed change operations (i.e., with or without side-effects on inferred knowledge). Then we formally analyze desired properties of RDF/S deltas such as size minimality, semantic identity, redundancy elimination, reversibility, and composability, as well as identify those RDF/S differential functions that satisfy them. Subsequently, we experimentally evaluate the computing time and size of the produced deltas over real and synthetic RDF/S knowledge bases.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W1971563190",
    "type": "article"
  },
  {
    "title": "Building Mashups by Demonstration",
    "doi": "https://doi.org/10.1145/1993053.1993058",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Rattapoom Tuchinda; Craig A. Knoblock; Pedro Szekely",
    "corresponding_authors": "",
    "abstract": "The latest generation of WWW tools and services enables Web users to generate applications that combine content from multiple sources. This type of Web application is referred to as a mashup. Many of the tools for constructing mashups rely on a widget paradigm, where users must select, customize, and connect widgets to build the desired application. While this approach does not require programming, the users must still understand programming concepts to successfully create a mashup. As a result, they are put off by the time, effort, and expertise needed to build a mashup. In this article, we describe our programming-by-demonstration approach to building mashup by example. Instead of requiring a user to select and customize a set of widgets, the user simply demonstrates the integration task by example. Our approach addresses the problems of extracting data from Web sources, cleaning and modeling the extracted data, and integrating the data across sources. We implemented these ideas in a system called Karma, and evaluated Karma on a set of 23 users. The results show that, compared to other mashup construction tools, Karma allows more of the users to successfully build mashups and makes it possible to build these mashups significantly faster compared to using a widget-based approach.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W1974810774",
    "type": "article"
  },
  {
    "title": "ClickRank",
    "doi": "https://doi.org/10.1145/2109205.2109206",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Guangyu Zhu; Gilad Mishne",
    "corresponding_authors": "",
    "abstract": "User browsing information, particularly non-search-related activity, reveals important contextual information on the preferences and intents of Web users. In this article, we demonstrate the importance of mining general Web user behavior data to improve ranking and other Web-search experience, with an emphasis on analyzing individual user sessions for creating aggregate models. In this context, we introduce ClickRank , an efficient, scalable algorithm for estimating Webpage and Website importance from general Web user-behavior data. We lay out the theoretical foundation of ClickRank based on an intentional surfer model and discuss its properties. We quantitatively evaluate its effectiveness regarding the problem of Web-search ranking, showing that it contributes significantly to retrieval performance as a novel Web-search feature. We demonstrate that the results produced by ClickRank for Web-search ranking are highly competitive with those produced by other approaches, yet achieved at better scalability and substantially lower computational costs. Finally, we discuss novel applications of ClickRank in providing enriched user Web-search experience, highlighting the usefulness of our approach for nonranking tasks.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W1976841650",
    "type": "article"
  },
  {
    "title": "Estimating Clustering Coefficients and Size of Social Networks via Random Walk",
    "doi": "https://doi.org/10.1145/2790304",
    "publication_date": "2015-09-28",
    "publication_year": 2015,
    "authors": "Liran Katzir; Stephen J. Hardiman",
    "corresponding_authors": "",
    "abstract": "This work addresses the problem of estimating social network measures. Specifically, the measures at hand are the network average and global clustering coefficients and the number of registered users. The algorithms at hand (1) assume no prior knowledge about the network and (2) access the network using only the publicly available interface. More precisely, this work provides (a) a unified approach for clustering coefficients estimation and (b) a new network size estimator. The unified approach for the clustering coefficients yields the first external access algorithm for estimating the global clustering coefficient. The new network size estimator offers improved accuracy compared to prior art estimators. Our approach is to view a social network as an undirected graph and use the public interface to retrieve a random walk. To estimate the clustering coefficient, the connectivity of each node in the random walk sequence is tested in turn. We show that the error drops exponentially in the number of random walk steps. For the network size estimation we offer a generalized view of prior art estimators that in turn yields an improved estimator. All algorithms are validated on several publicly available social network datasets.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W1990706897",
    "type": "article"
  },
  {
    "title": "A Formal Account of the Open Provenance Model",
    "doi": "https://doi.org/10.1145/2734116",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Natalia Kwaśnikowska; Luc Moreau; Jan Van den Bussche",
    "corresponding_authors": "",
    "abstract": "On the Web, where resources such as documents and data are published, shared, transformed, and republished, provenance is a crucial piece of metadata that would allow users to place their trust in the resources they access. The open provenance model (OPM) is a community data model for provenance that is designed to facilitate the meaningful interchange of provenance information between systems. Underpinning OPM is a notion of directed graph, where nodes represent data products and processes involved in past computations and edges represent dependencies between them; it is complemented by graphical inference rules allowing new dependencies to be derived. Until now, however, the OPM model was a purely syntactical endeavor. The present article extends OPM graphs with an explicit distinction between precise and imprecise edges. Then a formal semantics for the thus enriched OPM graphs is proposed, by viewing OPM graphs as temporal theories on the temporal events represented in the graph. The original OPM inference rules are scrutinized in view of the semantics and found to be sound but incomplete. An extended set of graphical rules is provided and proved to be complete for inference. The article concludes with applications of the formal semantics to inferencing in OPM graphs, operators on OPM graphs, and a formal notion of refinement among OPM graphs.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2260225750",
    "type": "article"
  },
  {
    "title": "UsageQoS",
    "doi": "https://doi.org/10.1145/2532635",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Xiaodi Huang",
    "corresponding_authors": "Xiaodi Huang",
    "abstract": "Services are an indispensable component in cloud computing. Web services are particularly important. As an increasing number of Web services provides equivalent functions, one common issue faced by users is the selection of the most appropriate one based on quality. This article presents a conceptual framework that characterizes the quality of Web services, an algorithm that quantifies them, and a system architecture that ranks Web services by using the proposed algorithm. In particular, the algorithm, called UsageQoS that computes the scores of quality of service (QoS) of Web services within a community, makes use of the usage frequencies of Web services. The frequencies are defined as the numbers of times invoked by other services in a given time period. The UsageQoS algorithm is able to optionally take user ratings as its initial input. The proposed approach has been validated by extensively experimenting on several datasets, including two real datasets. The results of the experiments have demonstrated that our approach is capable of estimating QoS parameters of Web services, regardless of whether user ratings are available or not.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2009906975",
    "type": "article"
  },
  {
    "title": "N <scp>auti</scp> LOD",
    "doi": "https://doi.org/10.1145/2697393",
    "publication_date": "2015-01-23",
    "publication_year": 2015,
    "authors": "Valeria Fionda; Giuseppe Pirrò; Claudio Gutiérrez",
    "corresponding_authors": "",
    "abstract": "The Web of Linked Data is a huge graph of distributed and interlinked datasources fueled by structured information. This new environment calls for formal languages and tools to automatize navigation across datasources (nodes in such graph) and enable semantic-aware and Web-scale search mechanisms. In this article we introduce a declarative navigational language for the Web of Linked Data graph called N auti LOD. N auti LOD enables one to specify datasources via the intertwining of navigation and querying capabilities. It also features a mechanism to specify actions (e.g., send notification messages) that obtain their parameters from datasources reached during the navigation. We provide a formalization of the N auti LOD semantics, which captures both nodes and fragments of the Web of Linked Data. We present algorithms to implement such semantics and study their computational complexity. We discuss an implementation of the features of N auti LOD in a tool called swget, which exploits current Web technologies and protocols. We report on the evaluation of swget and its comparison with related work. Finally, we show the usefulness of capturing Web fragments by providing examples in different knowledge domains.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2037516787",
    "type": "article"
  },
  {
    "title": "Elastic Personalized Nonfunctional Attribute Preference and Trade-off Based Service Selection",
    "doi": "https://doi.org/10.1145/2697389",
    "publication_date": "2015-01-23",
    "publication_year": 2015,
    "authors": "Kenneth K. Fletcher; Xiaoqing F. Liu; Mingdong Tang",
    "corresponding_authors": "",
    "abstract": "For service users to get the best service that meet their requirements, they prefer to personalize their nonfunctional attributes, such as reliability and price. However, the personalization makes it challenging for service providers to completely meet users’ preferences, because they have to deal with conflicting nonfunctional attributes when selecting services for users. With this in mind, users may sometimes want to explicitly specify their trade-offs among nonfunctional attributes to make their preferences known to service providers. In this article, we present a novel service selection method based on fuzzy logic that considers users’ personalized preferences and their trade-offs on nonfunctional attributes during service selection. The method allows users to represent their elastic nonfunctional requirements and associated importance using linguistic terms to specify their personalized trade-off strategies. We present examples showing how the service selection framework is used and a prototype with real-world airline services to evaluate the proposed framework's application.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2008203432",
    "type": "article"
  },
  {
    "title": "Exploring the Emerging Type of Comment for Online Videos",
    "doi": "https://doi.org/10.1145/3098885",
    "publication_date": "2017-08-21",
    "publication_year": 2017,
    "authors": "Ming He; Yong Ge; Enhong Chen; Qi Liu; Xuesong Wang",
    "corresponding_authors": "",
    "abstract": "DanMu , an emerging type of user-generated comment, has become increasingly popular in recent years. Many online video platforms such as Tudou.com have provided the DanMu function. Unlike traditional online reviews such as reviews at Youtube.com that are outside the videos, DanMu is a scrolling marquee comment, which is overlaid directly on top of the video and synchronized to a specific playback time. Such comments are displayed as streams of moving subtitles overlaid on the video screen. Viewers could easily write DanMu s while watching videos, and the written DanMu s will be immediately overlaid onto the video and displayed to writers themselves and other viewers as well. Such DanMu systems have greatly enabled users to communicate with each other in a much more direct way, creating a real-time sharing experience. Although there are several unique features of DanMu and has had a great impact on online video systems, to the best of our knowledge, there is no work that has provided a comprehensive study on DanMu . In this article, as a pilot study, we analyze the unique characteristics of DanMu from various perspectives. Specifically, we first illustrate some unique distributions of DanMu s by comparing with traditional reviews (TReviews) that we collected from a real DanMu -enabled online video system. Second, we discover two interesting patterns in DanMu data: a herding effect and multiple-burst phenomena that are significantly different from those in TRviews and reveal important insights about the growth of DanMu s on a video. Towards exploring antecedents of both th herding effect and multiple-burst phenomena, we propose to further detect leading DanMu s within bursts, because those leading DanMu s make the most contribution to both patterns. A framework is proposed to detect leading DanMu s that effectively combines multiple factors contributing to leading DanMu s. Based on the identified characteristics of DanMu , finally we propose to predict the distribution of future DanMu s (i.e., the growth of DanMu s), which is important for many DanMu -enabled online video systems, for example, the predicted DanMu distribution could be an indicator of video popularity. This prediction task includes two aspects: One is to predict which videos future DanMu s will be posted for, and the other one is to predict which segments of a video future DanMu s will be posted on. We develop two sophisticated models to solve both problems. Finally, intensive experiments are conducted with a real-world dataset to validate all methods developed in this article.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2749929993",
    "type": "article"
  },
  {
    "title": "How to Improve Your Search Engine Ranking",
    "doi": "https://doi.org/10.1145/2579990",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Ao-Jan Su; Y. Charlie Hu; Aleksandar Kuzmanovic; Cheng‐Kok Koh",
    "corresponding_authors": "",
    "abstract": "Search engines have greatly influenced the way people access information on the Internet, as such engines provide the preferred entry point to billions of pages on the Web. Therefore, highly ranked Web pages generally have higher visibility to people and pushing the ranking higher has become the top priority for Web masters. As a matter of fact, Search Engine Optimization (SEO) has became a sizeable business that attempts to improve their clients’ ranking. Still, the lack of ways to validate SEO’s methods has created numerous myths and fallacies associated with ranking algorithms. In this article, we focus on two ranking algorithms, Google’s and Bing’s, and design, implement, and evaluate a ranking system to systematically validate assumptions others have made about these popular ranking algorithms. We demonstrate that linear learning models, coupled with a recursive partitioning ranking scheme, are capable of predicting ranking results with high accuracy. As an example, we manage to correctly predict 7 out of the top 10 pages for 78% of evaluated keywords. Moreover, for content-only ranking, our system can correctly predict 9 or more pages out of the top 10 ones for 77% of search terms. We show how our ranking system can be used to reveal the relative importance of ranking features in a search engine’s ranking function, provide guidelines for SEOs and Web masters to optimize their Web pages, validate or disprove new ranking features, and evaluate search engine ranking results for possible ranking bias.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2169527548",
    "type": "article"
  },
  {
    "title": "Detecting Spam and Promoting Campaigns in Twitter",
    "doi": "https://doi.org/10.1145/2846102",
    "publication_date": "2016-02-08",
    "publication_year": 2016,
    "authors": "Xianchao Zhang; Zhaoxing Li; Zhu Shao-ping; Wenxin Liang",
    "corresponding_authors": "",
    "abstract": "Twitter has become a target platform for both promoters and spammers to disseminate their messages, which are more harmful than traditional spamming methods, such as email spamming. Recently, large amounts of campaigns that contain lots of spam or promotion accounts have emerged in Twitter. The campaigns cooperatively post unwanted information, and thus they can infect more normal users than individual spam or promotion accounts. Organizing or participating in campaigns has become the main technique to spread spam or promotion information in Twitter. Since traditional solutions focus on checking individual accounts or messages, efficient techniques for detecting spam and promotion campaigns in Twitter are urgently needed. In this article, we propose a framework to detect both spam and promotion campaigns. Our framework consists of three steps: the first step links accounts who post URLs for similar purposes; the second step extracts candidate campaigns that may be for spam or promotion purposes; and the third step classifies the candidate campaigns into normal, spam, and promotion groups. The key point of the framework is how to measure the similarity between accounts' purposes of posting URLs. We present two measure methods based on Shannon information theory: the first one uses the URLs posted by the users, and the second one considers both URLs and timestamps. Experimental results demonstrate that the proposed methods can extract the majority of the candidate campaigns correctly, and detect promotion and spam campaigns with high precision and recall.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2339765045",
    "type": "article"
  },
  {
    "title": "From Footprint to Evidence",
    "doi": "https://doi.org/10.1145/2996465",
    "publication_date": "2016-12-15",
    "publication_year": 2016,
    "authors": "Guangming Guo; Feida Zhu; Enhong Chen; Qi Liu; Le Wu; Chu Guan",
    "corresponding_authors": "",
    "abstract": "With the booming popularity of online social networks like Twitter and Weibo, online user footprints are accumulating rapidly on the social web. Simultaneously, the question of how to leverage the large-scale user-generated social media data for personal credit scoring comes into the sight of both researchers and practitioners. It has also become a topic of great importance and growing interest in the P2P lending industry. However, compared with traditional financial data, heterogeneous social data presents both opportunities and challenges for personal credit scoring. In this article, we seek a deep understanding of how to learn users’ credit labels from social data in a comprehensive and efficient way. Particularly, we explore the social-data-based credit scoring problem under the micro-blogging setting for its open, simple, and real-time nature. To identify credit-related evidence hidden in social data, we choose to conduct an analytical and empirical study on a large-scale dataset from Weibo, the largest and most popular tweet-style website in China. Summarizing results from existing credit scoring literature, we first propose three social-data-based credit scoring principles as guidelines for in-depth exploration. In addition, we glean six credit-related insights arising from empirical observations of the testbed dataset. Based on the proposed principles and insights, we extract prediction features mainly from three categories of users’ social data, including demographics, tweets, and networks. To harness this broad range of features, we put forward a two-tier stacking and boosting enhanced ensemble learning framework. Quantitative investigation of the extracted features shows that online social media data does have good potential in discriminating good credit users from bad. Furthermore, we perform experiments on the real-world Weibo dataset consisting of more than 7.3 million tweets and 200,000 users whose credit labels are known through our third-party partner. Experimental results show that (i) our approach achieves a roughly 0.625 AUC value with all the proposed social features as input, and (ii) our learning algorithm can outperform traditional credit scoring methods by as much as 17% for social-data-based personal credit scoring.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2565473608",
    "type": "article"
  },
  {
    "title": "Knowledge Graph Embedding",
    "doi": "https://doi.org/10.1145/3132733",
    "publication_date": "2017-12-22",
    "publication_year": 2017,
    "authors": "Yantao Jia; Yuanzhuo Wang; Xiaolong Jin; Hailun Lin; Xueqi Cheng",
    "corresponding_authors": "",
    "abstract": "A knowledge graph is a graph with entities of different types as nodes and various relations among them as edges. The construction of knowledge graphs in the past decades facilitates many applications, such as link prediction, web search analysis, question answering, and so on. Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, for example, TransE, TransH, and TransR, learn the embedding representation by defining a global margin-based loss function over the data. However, the loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidates, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this article, a locally adaptive translation method for knowledge graph embedding, called TransA, is proposed to find the loss function by adaptively determining its margin over different knowledge graphs. Then the convergence of TransA is verified from the aspect of its uniform stability. To make the embedding methods up-to-date when new vertices and edges are added into the knowledge graph, the incremental algorithm for TransA, called iTransA, is proposed by adaptively adjusting the optimal margin over time. Experiments on four benchmark data sets demonstrate the superiority of the proposed method, as compared to the state-of-the-art ones.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2778810234",
    "type": "article"
  },
  {
    "title": "Reliable and Resilient Trust Management in Distributed Service Provision Networks",
    "doi": "https://doi.org/10.1145/2754934",
    "publication_date": "2015-06-16",
    "publication_year": 2015,
    "authors": "Zhiyuan Su; Ling Liu; Mingchu Li; Xinxin Fan; Yang Zhou",
    "corresponding_authors": "",
    "abstract": "Distributed service networks are popular platforms for service providers to offer services to consumers and for service consumers to acquire services from unknown parties. eBay and Amazon are two well-known examples of enabling and hosting such service networks to connect service providers to service consumers. Trust management is a critical component for scaling such distributed service networks to a large and growing number of participants. In this article, we present ServiceTrust ++ , a feedback quality--sensitive and attack resilient trust management scheme for empowering distributed service networks with effective trust management capability. Compared with existing trust models, ServiceTrust ++ has several novel features. First, we present six attack models to capture both independent and colluding attacks with malicious cliques, malicious spies, and malicious camouflages. Second, we aggregate the feedback ratings based on the variances of participants’ feedback behaviors and incorporate feedback similarity as weight into the local trust algorithm. Third, we compute the global trust of a participant by employing conditional trust propagation based on the feedback similarity threshold. This allows ServiceTrust ++ to control and prevent malicious spies and malicious camouflage peers from boosting their global trust scores by manipulating the feedback ratings of good peers and by taking advantage of the uniform trust propagation. Finally, we systematically combine a trust-decaying strategy with a threshold value--based conditional trust propagation to further strengthen the robustness of our global trust computation against sophisticated malicious feedback. Experimental evaluation with both simulation-based networks and real network dataset Epinion show that ServiceTrust ++ is highly resilient against all six attack models and highly effective compared to EigenTrust, the most popular and representative trust propagation model to date.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2251117138",
    "type": "article"
  },
  {
    "title": "Caching to Reduce Mobile App Energy Consumption",
    "doi": "https://doi.org/10.1145/3125778",
    "publication_date": "2017-09-09",
    "publication_year": 2017,
    "authors": "Kaushik Dutta; Debra VanderMeer",
    "corresponding_authors": "",
    "abstract": "Mobile applications consume device energy for their operations, and the fast rate of battery depletion on mobile devices poses a major usability hurdle. After the display, data communication is the second-biggest consumer of mobile device energy. At the same time, software applications that run on mobile devices represent a fast-growing product segment. Typically, these applications serve as front-end display mechanisms, which fetch data from remote servers and display the information to the user in an appropriate format—incurring significant data communication overheads in the process. In this work, we propose methods to reduce energy overheads in mobile devices due to data communication by leveraging data caching technology. A review of existing caching mechanisms revealed that they are primarily designed for optimizing response time performance and cannot be easily ported to mobile devices for energy savings. Further, architectural differences between traditional client-server and mobile communications infrastructures make the use of existing caching technologies unsuitable in mobile devices. In this article, we propose a set of two new caching approaches specifically designed with the constraints of mobile devices in mind: (a) a response caching approach and (b) an object caching approach. Our experiments show that, even for a small cache size of 250MB, object caching can reduce energy consumption on average by 45% compared to the no-cache case, and response caching can reduce energy consumption by 20% compared to the no-cache case. The benefits increase with larger cache sizes. These results demonstrate the efficacy of our proposed method and raise the possibility of significantly extending mobile device battery life.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2754594640",
    "type": "article"
  },
  {
    "title": "A Large-scale Empirical Analysis of Ransomware Activities in Bitcoin",
    "doi": "https://doi.org/10.1145/3494557",
    "publication_date": "2021-12-22",
    "publication_year": 2021,
    "authors": "Kai Wang; Jun Pang; Ding-Jie Chen; Yu Zhao; Dapeng Huang; Chen Chen; Weili Han",
    "corresponding_authors": "",
    "abstract": "Exploiting the anonymous mechanism of Bitcoin, ransomware activities demanding ransom in bitcoins have become rampant in recent years. Several existing studies quantify the impact of ransomware activities, mostly focusing on the amount of ransom. However, victims’ reactions in Bitcoin that can well reflect the impact of ransomware activities are somehow largely neglected. Besides, existing studies track ransom transfers at the Bitcoin address level, making it difficult for them to uncover the patterns of ransom transfers from a macro perspective beyond Bitcoin addresses. In this article, we conduct a large-scale analysis of ransom payments, ransom transfers, and victim migrations in Bitcoin from 2012 to 2021. First, we develop a fine-grained address clustering method to cluster Bitcoin addresses into users, which enables us to identify more addresses controlled by ransomware criminals. Second, motivated by the fact that Bitcoin activities and their participants already formed stable industries, such as Darknet and Miner , we train a multi-label classification model to identify the industry identifiers of users. Third, we identify ransom payment transactions and then quantify the amount of ransom and the number of victims in 63 ransomware activities. Finally, after we analyze the trajectories of ransom transferred across different industries and track victims’ migrations across industries, we find out that to obscure the purposes of their transfer trajectories, most ransomware criminals (e.g., operators of Locky and Wannacry) prefer to spread ransom into multiple industries instead of utilizing the services of Bitcoin mixers. Compared with other industries, Investment is highly resilient to ransomware activities in the sense that the number of users in Investment remains relatively stable. Moreover, we also observe that a few victims become active in the Darknet after paying ransom. Our findings in this work can help authorities deeply understand ransomware activities in Bitcoin. While our study focuses on ransomware, our methods are potentially applicable to other cybercriminal activities that have similarly adopted bitcoins as their payments.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4200420517",
    "type": "article"
  },
  {
    "title": "SHGCN: Socially Enhanced Heterogeneous Graph Convolutional Network for Multi-behavior Prediction",
    "doi": "https://doi.org/10.1145/3617510",
    "publication_date": "2023-08-26",
    "publication_year": 2023,
    "authors": "Lei Zhang; Wuji Zhang; Likang Wu; Ming He; Hongke Zhao",
    "corresponding_authors": "",
    "abstract": "In recent years, multi-behavior information has been utilized to address data sparsity and cold-start issues. The general multi-behavior models capture multiple behaviors of users to make the representation of relevant features more fine-grained and informative. However, most current multi-behavior recommendation methods neglect the exploration of social relations between users. Actually, users’ potential social connections are critical to assist them in filtering multifarious messages, which may be one key for models to tap deeper into users’ interests. Additionally, existing models usually focus on the positive behaviors (e.g., click , follow , and purchase ) of users and tend to ignore the value of negative behaviors (e.g., unfollow and badpost ). In this work, we present a Multi-Behavior Graph (MBG) construction method based on user behaviors and social relationships and then introduce a novel socially enhanced and behavior-aware graph neural network for behavior prediction. Specifically, we propose a Socially Enhanced Heterogeneous Graph Convolutional Network (SHGCN) model, which utilizes behavior heterogeneous graph convolution module and social graph convolution module to effectively incorporate behavior features and social information to achieve precise multi-behavior prediction. In addition, the aggregation pooling mechanism is suggested to integrate the outputs of different graph convolution layers, and a dynamic adaptive loss (DAL) method is presented to explore the weight of each behavior. The experimental results on the datasets of the e-commerce platforms (i.e., Epinions and Ciao) indicate the promising performance of SHGCN. Compared with the most powerful baseline, SHGCN achieves 3.3% and 1.4% uplift in terms of AUC on the Epinions and Ciao datasets. Further experiments, including model efficiency analysis, DAL mechanism, and ablation experiments, confirm the validity of the multi-behavior information and social enhancement.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4386190721",
    "type": "article"
  },
  {
    "title": "A Large-Scale Characterization of How Readers Browse Wikipedia",
    "doi": "https://doi.org/10.1145/3580318",
    "publication_date": "2023-01-14",
    "publication_year": 2023,
    "authors": "Tiziano Piccardi; Martin Gerlach; Akhil Arora; Robert West",
    "corresponding_authors": "",
    "abstract": "Despite the importance and pervasiveness of Wikipedia as one of the largest platforms for open knowledge, surprisingly little is known about how people navigate its content when seeking information. To bridge this gap, we present the first systematic large-scale analysis of how readers browse Wikipedia. Using billions of page requests from Wikipedia's server logs, we measure how readers reach articles, how they transition between articles, and how these patterns combine into more complex navigation paths. We find that navigation behavior is characterized by highly diverse structures. Although most navigation paths are shallow, comprising a single pageload, there is much variety, and the depth and shape of paths vary systematically with topic, device type, and time of day. We show that Wikipedia navigation paths commonly mesh with external pages as part of a larger online ecosystem, and we describe how naturally occurring navigation paths are distinct from targeted navigation in lab-based settings. Our results further suggest that navigation is abandoned when readers reach low-quality pages. Taken together, these insights contribute to a more systematic understanding of readers' information needs and allow for improving their experience on Wikipedia and the Web in general.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4315977505",
    "type": "article"
  },
  {
    "title": "Understanding Rug Pulls: An In-depth Behavioral Analysis of Fraudulent NFT Creators",
    "doi": "https://doi.org/10.1145/3623376",
    "publication_date": "2023-09-11",
    "publication_year": 2023,
    "authors": "Trishie Sharma; Rachit Agarwal; Sandeep K. Shukla",
    "corresponding_authors": "",
    "abstract": "The explosive growth of non-fungible tokens (NFTs) on Web3 has created a new frontier for digital art and collectibles and an emerging space for fraudulent activities. This study provides an in-depth analysis of NFT rug pulls, the fraudulent schemes that steal investors’ funds. From a curated dataset of 760 rug pulls across 10 NFT marketplaces, we examine these schemes’ structural and behavioral properties, identify the characteristics and motivations of rug pullers, and classify NFT projects into 20 groups based on creators’ association with their accounts. Our findings reveal that repeated rug pulls account for a significant proportion of the rise in NFT-related cryptocurrency crimes, with one NFT creator attempting 37 rug pulls within 3 months. Additionally, we identify the largest group of creators influencing the majority of rug pulls and demonstrate the connection between rug pullers of different NFT projects using the same wallets to store and move money. Our study contributes to understanding NFT market risks and provides insights for designing preventative strategies to mitigate future losses.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4386602933",
    "type": "article"
  },
  {
    "title": "PORTRAIT: A Hybrid Approach to Create Extractive Ground-truth Summary for Disaster Event",
    "doi": "https://doi.org/10.1145/3711908",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Piyush Kumar Garg; Roshni Chakraborty; Sourav Kumar Dandapat",
    "corresponding_authors": "",
    "abstract": "Nowadays, Twitter is an important source of information and latest updates during ongoing events, such as disaster events. However, the huge number of tweets posted during a disaster makes identification of relevant information highly challenging. Therefore, a summary of the tweets can help the decision-makers to ensure efficient allocation of resources among the affected population. There exist several automated summarization approaches which can generate a summary given the tweets related to a disaster. Development of these automated summarization approaches require availability of ground-truth summary of the dataset for verification. However, the number of publicly available datasets along with the ground-truth summary for disaster events are still inadequate. To improve this situation, we need to create more number of ground-truth summaries. Existing approaches for ground-truth summary generation rely on the annotators’ wisdom and intuition. This process requires immense human effort and significant time. Moreover, the selection of the important tweets from the humongous set of input tweets often results in sub-optimal choice of tweets in the final summary. Therefore, to handle these challenges, we propose a hybrid approach (PORTRAIT) for ground-truth summary generation, where we partly automate the procedure to improve the quality of ground-truth summary and reduce human effort and time. We validate the effectiveness of PORTRAIT on 9 disaster events through quantitative and qualitative analysis. We prepare and release the ground-truth summaries for 9 disaster events, which consist of both natural and man-made disaster events belonging to 5 different continents.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406245451",
    "type": "article"
  },
  {
    "title": "Twitter User Geolocation Based on Location Feature Enhancement",
    "doi": "https://doi.org/10.1145/3711909",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Meng Zhang; Xiangyang Luo; Ningbo Huang; Yimin Liu; Shaoyong Du",
    "corresponding_authors": "",
    "abstract": "User location discovery from social media is crucial for location-based services like emergency awareness and event monitoring. Existing approaches generally integrate user-generated text features and social relationships, but insufficiently explore location-specific features and geographically proximate relationships, leading to suboptimal accuracy. In this paper, we propose a Twitter user geolocation method based on location feature enhancement, to better capture the location characteristics in users’ tweets and social relationships. Specifically, a user tweet representation algorithm based on location feature separation (TwLS) is designed. By leveraging words’ location-aware weight matrix and pre-trained embeddings, TwLS calculates a tweet representation for each user in every location, explicitly indicating the relevance between users and various locations. Additionally, we develop the local celebrity discovery method (LocCel) to construct social networks by identifying and preserving geographically concentrated high-degree nodes while filtering noise. Thereby LocCel enhances local relationships and strengthens location-proximate connections within the user social network. Experiments on two real-world datasets show that our method outperforms seven baselines, improving user geolocation accuracy by 3.1% ∼ 8.1% and 1.8% ∼ 8.8%, while reducing median error by 22.2% ∼ 52.8% and 19.4% ∼ 50.7%, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406302346",
    "type": "article"
  },
  {
    "title": "Unsupervised Framing Analysis for Social Media Discourse in Polarizing Events",
    "doi": "https://doi.org/10.1145/3711912",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Hernan Sarmiento; Ricardo Córdova; Jorge Ortiz; Felipe Bravo-Márquez; Marcelo Santos; Sebastián Valenzuela",
    "corresponding_authors": "",
    "abstract": "This study investigates the concept of frames in the realm of online polarization, with a focus on social media platforms. The research extends the understanding of how frames—emerging, complex, and often subtle concepts—become prominent in online conversations that are polarized. The study proposes a comprehensive methodology for identifying and characterizing these frames, integrating machine learning techniques, network analysis algorithms, and natural language processing tools. This method aims for generalizability across multiple platforms and types of user engagement. Two novel metrics, homogeneity and relevancy are introduced for the rigorous evaluation of identified frame candidates. Grounded in several foundational presumptions, including the role of topics and multi-word expressions in framing, the study sheds light on how frames emerge and gain significance within digital communities. The research questions explored include the methods for identifying frames, the variability and significance of these frames, and the effectiveness of different computational techniques in this context. To validate the approach, we present a case study of the 2021 Chilean presidential election, using data from both Twitter and WhatsApp platforms. This real-world application allows for the examination of how frames fluctuate in response to events and the specific mechanisms of platforms. Overall, the study makes several key contributions to the field, offering new insights and methodologies for analyzing the complexities of online polarization. It serves as groundwork for future research on the dynamics of online communities, especially those associated with distinctly polarized events.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406302405",
    "type": "article"
  },
  {
    "title": "MSA-Net: A Multi-Scale Information Diffusion Model Awaring User Activity Level",
    "doi": "https://doi.org/10.1145/3711911",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Yinzhou Tang; Jinghua Piao; Huandong Wang; Yue Wang; Yong Li",
    "corresponding_authors": "",
    "abstract": "Modeling information diffusion on social networks can be used to guide the prediction and control of information propagation and improve the structure and functionality of social networks. Existing information diffusion prediction methods can predict information diffusion paths and its volume by modeling social network structure and user behavior. However, none of the existing methods take user activity level, which is proved to be critical in modeling the information diffusion process, into account, thus weaken the prediction accuracy. To solve this problem, this paper proposes a Multi-Scale Activity Network (MSA-Net) to capture topological and historical affect features for different scales and to predict the users who will be affected at a specific future timestamp with the help of user activity level. Specifically, we first learn the network representation of three scales or levels: micro-scale, meso-scale, and macro-scale, which refers to the user level, intra-community level, and inter-community level, respectively. Then, we introduce the user activity level for each user by using user degree and average number of tweets per time unit to model the individual differences of users to achieve a more accurate prediction. Extensive experiments based on real-world datasets show that MSA-Net achieves a 6.14% improvement in terms of precision, a 6.74% improvement in terms of recall metrics, a 4.26% improvement in terms of F1-score, a 3.15% improvement in terms of MAP, and a 25.78% improvement in terms of NRMSE over the best existing baseline. The code and data are available at https://github.com/tsinghua-fib-lab/MSA-Net.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406874971",
    "type": "article"
  },
  {
    "title": "Exploring Suicide Factors in Online Discourse: Sentiment and Thematic Analysis of Reddit",
    "doi": "https://doi.org/10.1145/3716546",
    "publication_date": "2025-02-21",
    "publication_year": 2025,
    "authors": "E. Dan; Jianfeng Zhu; Ruoming Jin",
    "corresponding_authors": "",
    "abstract": "Suicide remains a critical global health issue, with rising numbers claiming more lives each year despite ongoing prevention efforts. Current research has extensively explored factors influencing suicidal tendencies, emphasizing trauma, mental health disorders, and social relationships. However, traditional studies often relied on traditional data sources and often examined risk factors in isolation, which may not fully capture the dynamics observed in social media platforms. To address these limitations, our study utilizes data from r/SuicideWatch and r/Teenagers to analyze the emotional sentiment and explore themes associated with suicidal ideation, with r/Teenagers serving as a comparative reference. By leveraging natural language processing (NLP) techniques and statistical methodologies, including sentiment analysis and BERTopic modeling, we aim to gain deeper insights into the factors contributing to suicidal thoughts. Using TextBlob, our findings reveal a significant difference in sentiment between the two subreddits, with r/SuicideWatch posts predominantly expressing challenges and distressing emotions. Through BERTopic analysis, we identified key themes such as emotional challenges related to romantic relationships, academic pressure, and substance use concerns in r/SuicideWatch, highlighting their strong association with suicidal ideation. While r/Teenagers had some similar themes regarding struggles with loneliness and academics, the topics were focused more on general adolescent concerns. These findings demonstrate that advanced NLP methods can effectively analyze large-scale social media data, providing valuable insights into the multifaceted nature of suicidal ideation and emphasizing the need for targeted intervention strategies. Suggested improvements include enhancing relationship counseling and peer support networks, implementing school-based mental health programs, and leveraging social media for real-time support and awareness campaigns. By understanding the emotional and thematic nuances of online discussions, these strategies can more effectively address the multifaceted factors contributing to mental health challenges and reduce the risk of suicidal behavior.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407809607",
    "type": "article"
  },
  {
    "title": "Exploring Time-Ordered Triadic Closure in Online Social Networks",
    "doi": "https://doi.org/10.1145/3732295",
    "publication_date": "2025-04-24",
    "publication_year": 2025,
    "authors": "Alessia Galdeman; Cheick Tidiane Bâ; Matteo Zignani; Sabrina Gaito",
    "corresponding_authors": "",
    "abstract": "Online social platforms for digital communication necessitate an in-depth understanding of their evolving dynamics, especially after the renewal requests brought about by new paradigms, such as Web3. The dynamics within online social networks (OSNs) are influenced by numerous factors, encompassing user behavior, content generation, platform features, and technological advancements, with triadic closure standing out as a prominent and influential element. In this study, we focus on the temporal aspects of triadic closure and its role in the evolution of OSNs, especially after the advent of the Web3 paradigm. By analyzing networks with timestamped links from diverse platforms based on different architectures, including communication, Web3-based, and trade networks, we developed a comprehensive analytical pipeline to support the study of triadic closure patterns. This pipeline includes an algorithm for the census of time-ordered triads, a vector-based model for representing growing networks (growth triadic profile), the identification of triadic closure rules (TERs), and the evaluation of the speed of the formation of closed triads. Our findings reveal significant variations in the impact of triadic closure across different OSNs, marked by diverse growth triadic profiles and varying formation speeds of closed triads as well as diversity in the predictability of evolutionary patterns based on triads. This study not only enhances the comprehension of triadic closure in the temporal evolution of OSNs but also provides valuable insights to be taken into account for the design and administration of online social platforms.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409771089",
    "type": "article"
  },
  {
    "title": "Introduction",
    "doi": "https://doi.org/10.1145/1232722.1232723",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W4242631771",
    "type": "article"
  },
  {
    "title": "Scalable semantic analytics on social networks for addressing the problem of conflict of interest detection",
    "doi": "https://doi.org/10.1145/1326561.1326568",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Boanerges Aleman-Meza; Meenakshi Nagarajan; Li Ding; Amit Sheth; I. Budak Arpinar; Anupam Joshi; Tim Finin",
    "corresponding_authors": "",
    "abstract": "In this article, we demonstrate the applicability of semantic techniques for detection of Conflict of Interest (COI). We explain the common challenges involved in building scalable Semantic Web applications, in particular those addressing connecting-the-dots problems. We describe in detail the challenges involved in two important aspects on building Semantic Web applications, namely, data acquisition and entity disambiguation (or reference reconciliation). We extend upon our previous work where we integrated the collaborative network of a subset of DBLP researchers with persons in a Friend-of-a-Friend social network (FOAF). Our method finds the connections between people, measures collaboration strength, and includes heuristics that use friendship/affiliation information to provide an estimate of potential COI in a peer-review scenario. Evaluations are presented by measuring what could have been the COI between accepted papers in various conference tracks and their respective program committee members. The experimental results demonstrate that scalability can be achieved by using a dataset of over 3 million entities (all bibliographic data from DBLP and a large collection of FOAF documents).",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2020224194",
    "type": "article"
  },
  {
    "title": "Reporting incentives and biases in online review forums",
    "doi": "https://doi.org/10.1145/1734200.1734202",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Radu Jurca; Florent Garcin; Arjun Talwar; Boi Faltings",
    "corresponding_authors": "",
    "abstract": "Online reviews have become increasingly popular as a way to judge the quality of various products and services. However, recent work demonstrates that the absence of reporting incentives leads to a biased set of reviews that may not reflect the true quality. In this paper, we investigate underlying factors that influence users when reporting feedback. In particular, we study both reporting incentives and reporting biases observed in a widely used review forum, the Tripadvisor Web site. We consider three sources of information: first, the numerical ratings left by the user for different aspects of quality; second, the textual comment accompanying a review; third, the patterns in the time sequence of reports. We first show that groups of users who discuss a certain feature at length are more likely to agree in their ratings. Second, we show that users are more motivated to give feedback when they perceive a greater risk involved in a transaction. Third, a user's rating partly reflects the difference between true quality and prior expectation of quality, as inferred from previous reviews. We finally observe that because of these biases, when averaging review scores there are strong differences between the mean and the median. We speculate that the median may be a better way to summarize the ratings.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2154173182",
    "type": "article"
  },
  {
    "title": "Search-as-a-service",
    "doi": "https://doi.org/10.1145/1594173.1594175",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Aameek Singh; Mudhakar Srivatsa; Ling Liu",
    "corresponding_authors": "",
    "abstract": "With fast-paced growth of digital data and exploding storage management costs, enterprises are looking for new ways to effectively manage their data. One such cost-effective paradigm is the cloud storage model also referred to as Storage-as-a-Service, in which enterprises outsource their storage to a storage service provider (SSP) by storing data (usually encrypted) at a remote SSP-managed site and accessing it over a high speed network. Along with storage capacity used, the SSP often charges clients on the amount of data that is accessed from the SSP site. Thus, it is in the interest of the client enterprise to download only relevant content. This makes search over outsourced storage an important capability. Searching over encrypted outsourced storage, however, is a complex challenge. Each enterprise has different access privileges for different users and this access control needs to be preserved during search (for example, ensuring that a user cannot search through data that is inaccessible from the filesystem due to its permissions). Secondly, the search mechanism has to preserve confidentiality from the SSP and indices can not be stored in plain text. In this article, we present a new filesystem search technique that integrates access control and indexing/search mechanisms into a unified framework to support access control aware search. Our approach performs indexing within the trusted enterprise domain and uses a novel access control barrel (ACB) primitive to encapsulate access control within these indices. The indices are then systematically encrypted and shipped to the SSP for hosting. Unlike existing enterprise search techniques, our approach is resilient to various common attacks that leak private information. Additionally, to the best of our knowledge, our approach is a first such technique that allows search indices to be hosted at the SSP site, thus effectively providing search-as-a-service . This does not require the client enterprise to fully trust the SSP for data confidentiality. We describe the architecture and implementation of our approach and a detailed experimental analysis comparing with other approaches.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2094493303",
    "type": "article"
  },
  {
    "title": "Characterizing Organizational Use of Web-Based Services",
    "doi": "https://doi.org/10.1145/2019643.2019646",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Phillipa Gill; Martin Arlitt; Niklas Carlsson; Anirban Mahanti; Carey Williamson",
    "corresponding_authors": "",
    "abstract": "Today’s Web provides many different functionalities, including communication, entertainment, social networking, and information retrieval. In this article, we analyze traces of HTTP activity from a large enterprise and from a large university to identify and characterize Web-based service usage. Our work provides an initial methodology for the analysis of Web-based services. While it is nontrivial to identify the classes, instances, and providers for each transaction, our results show that most of the traffic comes from a small subset of providers, which can be classified manually. Furthermore, we assess both qualitatively and quantitatively how the Web has evolved over the past decade, and discuss the implications of these changes.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2006522513",
    "type": "article"
  },
  {
    "title": "Semantic content-based recommendation of software services using context",
    "doi": "https://doi.org/10.1145/2516633.2516639",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Li-Wei Liu; Freddy Lécué; Nikolay Mehandjiev",
    "corresponding_authors": "",
    "abstract": "The current proliferation of software services means users should be supported when selecting one service out of the many which meet their needs. Recommender Systems provide such support for selecting products and conventional services, yet their direct application to software services is not straightforward, because of the current scarcity of available user feedback, and the need to fine-tune software services to the context of intended use. In this article, we address these issues by proposing a semantic content-based recommendation approach that analyzes the context of intended service use to provide effective recommendations in conditions of scarce user feedback. The article ends with two experiments based on a realistic set of semantic services. The first experiment demonstrates how the proposed semantic content-based approach can produce effective recommendations using semantic reasoning over service specifications by comparing it with three other approaches. The second experiment demonstrates the effectiveness of the proposed context analysis mechanism by comparing the performance of both context-aware and plain versions of our semantic content-based approach, benchmarked against user-performed selection informed by context.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2059355583",
    "type": "article"
  },
  {
    "title": "Analyzing, Detecting, and Exploiting Sentiment in Web Queries",
    "doi": "https://doi.org/10.1145/2535525",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Sergiu Chelaru; İsmail Sengör Altıngövde; Stefan Siersdorfer; Wolfgang Nejdl",
    "corresponding_authors": "",
    "abstract": "The Web contains an increasing amount of biased and opinionated documents on politics, products, and polarizing events. In this article, we present an indepth analysis of Web search queries for controversial topics, focusing on query sentiment. To this end, we conduct extensive user assessments and discriminative term analyses, as well as a sentiment analysis using the SentiWordNet thesaurus, a lexical resource containing sentiment annotations. Furthermore, in order to detect the sentiment expressed in queries, we build different classifiers based on query texts, query result titles, and snippets. We demonstrate the virtue of query sentiment detection in two different use cases. First, we define a query recommendation scenario that employs sentiment detection of results to recommend additional queries for polarized queries issued by search engine users. The second application scenario is controversial topic discovery, where query sentiment classifiers are employed to discover previously unknown topics that trigger both highly positive and negative opinions among the users of a search engine. For both use cases, the results of our evaluations on real-world data are promising and show the viability and potential of query sentiment analysis in practical scenarios.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W1971469648",
    "type": "article"
  },
  {
    "title": "Web browsing behavior analysis and interactive hypervideo",
    "doi": "https://doi.org/10.1145/2529995.2529996",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Luis A. Leiva; Roberto Vivó",
    "corresponding_authors": "",
    "abstract": "Processing data on any sort of user interaction is well known to be cumbersome and mostly time consuming. In order to assist researchers in easily inspecting fine-grained browsing data, current tools usually display user interactions as mouse cursor tracks, a video-like visualization scheme. However, to date, traditional online video inspection has not explored the full capabilities of hypermedia and interactive techniques. In response to this need, we have developed SMT2ϵ, a Web-based tracking system for analyzing browsing behavior using feature-rich hypervideo visualizations. We compare our system to related work in academia and the industry, showing that ours features unprecedented visualization capabilities. We also show that SMT2ϵ efficiently captures browsing data and is perceived by users to be both helpful and usable. A series of prediction experiments illustrate that raw cursor data are accessible and can be easily handled, providing evidence that the data can be used to construct and verify research hypotheses. Considering its limitations, it is our hope that SMT2ϵ will assist researchers, usability practitioners, and other professionals interested in understanding how users browse the Web.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2044011347",
    "type": "article"
  },
  {
    "title": "A term-based inverted index partitioning model for efficient distributed query processing",
    "doi": "https://doi.org/10.1145/2516633.2516637",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "B. Barla Cambazoğlu; Enver Kayaaslan; Simon Jonassen; Cevdet Aykanat",
    "corresponding_authors": "",
    "abstract": "In a shared-nothing, distributed text retrieval system, queries are processed over an inverted index that is partitioned among a number of index servers. In practice, the index is either document-based or term-based partitioned. This choice is made depending on the properties of the underlying hardware infrastructure, query traffic distribution, and some performance and availability constraints. In query processing on retrieval systems that adopt a term-based index partitioning strategy, the high communication overhead due to the transfer of large amounts of data from the index servers forms a major performance bottleneck, deteriorating the scalability of the entire distributed retrieval system. In this work, to alleviate this problem, we propose a novel inverted index partitioning model that relies on hypergraph partitioning. In the proposed model, concurrently accessed index entries are assigned to the same index servers, based on the inverted index access patterns extracted from the past query logs. The model aims to minimize the communication overhead that will be incurred by future queries while maintaining the computational load balance among the index servers. We evaluate the performance of the proposed model through extensive experiments using a real-life text collection and a search query sample. Our results show that considerable performance gains can be achieved relative to the term-based index partitioning strategies previously proposed in literature. In most cases, however, the performance remains inferior to that attained by document-based partitioning.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2090913810",
    "type": "article"
  },
  {
    "title": "Nucleus Decompositions for Identifying Hierarchy of Dense Subgraphs",
    "doi": "https://doi.org/10.1145/3057742",
    "publication_date": "2017-07-03",
    "publication_year": 2017,
    "authors": "Ahmet Erdem Sarıyüce; C. Seshadhri; Ali Pınar; Ümit V. Çatalyürek",
    "corresponding_authors": "",
    "abstract": "Finding dense substructures in a graph is a fundamental graph mining operation, with applications in bioinformatics, social networks, and visualization to name a few. Yet most standard formulations of this problem (like clique, quasi-clique, densest at-least- k subgraph) are NP-hard. Furthermore, the goal is rarely to find the “true optimum” but to identify many (if not all) dense substructures, understand their distribution in the graph, and ideally determine relationships among them. Current dense subgraph finding algorithms usually optimize some objective and only find a few such subgraphs without providing any structural relations. We define the nucleus decomposition of a graph, which represents the graph as a forest of nuclei . Each nucleus is a subgraph where smaller cliques are present in many larger cliques. The forest of nuclei is a hierarchy by containment, where the edge density increases as we proceed towards leaf nuclei. Sibling nuclei can have limited intersections, which enables discovering overlapping dense subgraphs. With the right parameters, the nucleus decomposition generalizes the classic notions of k -core and k -truss decompositions. We present practical algorithms for nucleus decompositions and empirically evaluate their behavior in a variety of real graphs. The tree of nuclei consistently gives a global, hierarchical snapshot of dense substructures and outputs dense subgraphs of comparable quality with the state-of-the-art solutions that are dense and have non-trivial sizes. Our algorithms can process real-world graphs with tens of millions of edges in less than an hour. We demonstrate how proposed algorithms can be utilized on a citation network. Our analysis showed that dense units identified by our algorithms correspond to coherent articles on a specific area. Our experiments also show that we can identify dense structures that are lost within larger structures by other methods and find further finer grain structure within dense groups.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2728241679",
    "type": "article"
  },
  {
    "title": "A UI-Centric Approach for the End-User Development of Multidevice Mashups",
    "doi": "https://doi.org/10.1145/2735632",
    "publication_date": "2015-06-16",
    "publication_year": 2015,
    "authors": "Cinzia Cappiello; Maristella Matera; Matteo Picozzi",
    "corresponding_authors": "",
    "abstract": "In recent years, models, composition paradigms, and tools for mashup development have been proposed to support the integration of information sources, services and APIs available on the Web. The challenge is to provide a gate to a “programmable Web,” where end users are allowed to construct easily composite applications that merge content and functions so as to satisfy the long tail of their specific needs. The approaches proposed so far do not fully accommodate this vision. This article, therefore, proposes a mashup development framework that is oriented toward the End-User Development. Given the fundamental role of user interfaces (UIs) as a medium easily understandable by the end users, the proposed approach is characterized by UI-centric models able to support a WYSIWYG (What You See Is What You Get) specification of data integration and service orchestration. It, therefore, contributes to the definition of adequate abstractions that, by hiding the technology and implementation complexity, can be adopted by the end users in a kind of “democratic” paradigm for mashup development. This article also shows how model-to-code generative techniques translate models into application schemas, which in turn guide the dynamic instantiation of the composite applications at runtime. This is achieved through lightweight execution environments that can be deployed on the Web and on mobile devices to support the pervasive use of the created applications.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W1474481815",
    "type": "article"
  },
  {
    "title": "Ten Years of Rich Internet Applications",
    "doi": "https://doi.org/10.1145/2626369",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Sven Casteleyn; Irene Garrigós; José-Norberto Mazón",
    "corresponding_authors": "",
    "abstract": "BACKGROUND. The term Rich Internet Applications (RIAs) is generally associated with Web applications that provide the features and functionality of traditional desktop applications. Ten years after the introduction of the term, an ample amount of research has been carried out to study various aspects of RIAs. It has thus become essential to summarize this research and provide an adequate overview. OBJECTIVE. The objective of our study is to assemble, classify, and analyze all RIA research performed in the scientific community, thus providing a consolidated overview thereof, and to identify well-established topics, trends, and open research issues. Additionally, we provide a qualitative discussion of the most interesting findings. This work therefore serves as a reference work for beginning and established RIA researchers alike, as well as for industrial actors that need an introduction in the field, or seek pointers to (a specific subset of) the state-of-the-art. METHOD. A systematic mapping study is performed in order to identify all RIA-related publications, define a classification scheme, and categorize, analyze, and discuss the identified research according to it. RESULTS. Our source identification phase resulted in 133 relevant, peer-reviewed publications, published between 2002 and 2011 in a wide variety of venues. They were subsequently classified according to four facets: development activity, research topic, contribution type, and research type. Pie, stacked bar, and bubble charts were used to depict and analyze the results. A deeper analysis is provided for the most interesting and/or remarkable results. CONCLUSION. Analysis of the results shows that, although the RIA term was coined in 2002, the first RIA-related research appeared in 2004. From 2007 there was a significant increase in research activity, peaking in 2009 and decreasing to pre-2009 levels afterwards. All development phases are covered in the identified research, with emphasis on “design” (33%) and “implementation” (29%). The majority of research proposes a “method” (44%), followed by “model” (22%), “methodology” (18%), and “tools” (16%); no publications in the category “metrics” were found. The preponderant research topic is “models, methods and methodologies” (23%) and, to a lesser extent, “usability and accessibility” and “user interface” (11% each). On the other hand, the topic “localization, internationalization and multilinguality” received no attention at all, and topics such as “deep Web” (under 1%), “business processing”, “usage analysis”, “data management”, “quality and metrics” (all under 2%), “semantics”, and “performance” (slightly above 2%) received very little attention. Finally, there is a large majority of “solution proposals” (66%), few “evaluation research” (14%), and even fewer “validation” (6%), although the latter have been increasing in recent years.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2014016202",
    "type": "article"
  },
  {
    "title": "Modellus",
    "doi": "https://doi.org/10.1145/2180861.2180865",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Peter Desnoyers; Timothy Wood; Prashant Shenoy; Rahul Singh; Sangameshwar Patil; Harrick M. Vin",
    "corresponding_authors": "",
    "abstract": "The rising complexity of distributed server applications in Internet data centers has made the tasks of modeling and analyzing their behavior increasingly difficult. This article presents Modellus , a novel system for automated modeling of complex web-based data center applications using methods from queuing theory, data mining, and machine learning. Modellus uses queuing theory and statistical methods to automatically derive models to predict the resource usage of an application and the workload it triggers; these models can be composed to capture multiple dependencies between interacting applications. Model accuracy is maintained by fast, distributed testing, automated relearning of models when they change, and methods to bound prediction errors in composite models. We have implemented a prototype of Modellus, deployed it on a data center testbed, and evaluated its efficacy for modeling and analysis of several distributed multitier web applications. Our results show that this feature-based modeling technique is able to make predictions across several data center tiers, and maintain predictive accuracy (typically 95% or better) in the face of significant shifts in workload composition; we also demonstrate practical applications of the Modellus system to prediction and provisioning of real-world data center applications.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2038648100",
    "type": "article"
  },
  {
    "title": "A Large-Scale Evaluation of U.S. Financial Institutions’ Standardized Privacy Notices",
    "doi": "https://doi.org/10.1145/2911988",
    "publication_date": "2016-08-26",
    "publication_year": 2016,
    "authors": "Lorrie Faith Cranor; Pedro Giovanni Leon; Blase Ur",
    "corresponding_authors": "",
    "abstract": "Financial institutions in the United States are required by the Gramm-Leach-Bliley Act to provide annual privacy notices. In 2009, eight federal agencies jointly released a model privacy form for these disclosures. While the use of this model privacy form is not required, it has been widely adopted. We automatically evaluated 6,191 U.S. financial institutions’ privacy notices posted on the World Wide Web. We found large variance in stated practices, even among institutions of the same type. While thousands of financial institutions share personal information without providing the opportunity for consumers to opt out, some institutions’ practices are more privacy protective. Regression analyses show that large institutions and those headquartered in the northeastern region share consumers’ personal information at higher rates than all other institutions. Furthermore, our analysis helped us uncover institutions that do not let consumers limit data sharing when legally required to do so, as well as institutions making self-contradictory statements. We discuss implications for privacy in the financial industry, issues with the design and use of the model privacy form on the World Wide Web, and future directions for standardized privacy notice.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2508414746",
    "type": "article"
  },
  {
    "title": "A Model-Based Approach for Crawling Rich Internet Applications",
    "doi": "https://doi.org/10.1145/2626371",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Mustafa Emre Dinçtürk; Guy-Vincent Jourdan; Gregor von Bochmann; Iosif Viorel Onut",
    "corresponding_authors": "",
    "abstract": "New Web technologies, like AJAX, result in more responsive and interactive Web applications, sometimes called Rich Internet Applications (RIAs). Crawling techniques developed for traditional Web applications are not sufficient for crawling RIAs. The inability to crawl RIAs is a problem that needs to be addressed for at least making RIAs searchable and testable. We present a new methodology, called “model-based crawling”, that can be used as a basis to design efficient crawling strategies for RIAs. We illustrate model-based crawling with a sample strategy, called the “hypercube strategy”. The performances of our model-based crawling strategies are compared against existing standard crawling strategies, including breadth-first, depth-first, and a greedy strategy. Experimental results show that our model-based crawling approach is significantly more efficient than these standard strategies.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2019560524",
    "type": "article"
  },
  {
    "title": "Sampling Content from Online Social Networks",
    "doi": "https://doi.org/10.1145/2743023",
    "publication_date": "2015-06-04",
    "publication_year": 2015,
    "authors": "Muhammad Bilal Zafar; Parantapa Bhattacharya; Niloy Ganguly; Krishna P. Gummadi; Saptarshi Ghosh",
    "corresponding_authors": "",
    "abstract": "Analysis of content streams gathered from social networking sites such as Twitter has several applications ranging from content search and recommendation, news detection to business analytics. However, processing large amounts of data generated on these sites in real-time poses a difficult challenge. To cope with the data deluge, analytics companies and researchers are increasingly resorting to sampling. In this article, we investigate the crucial question of how to sample content streams generated by users in online social networks . The traditional method is to randomly sample all the data. For example, most studies using Twitter data today rely on the 1% and 10% randomly sampled streams of tweets that are provided by Twitter. In this paper, we analyze a different sampling methodology, one where content is gathered only from a relatively small sample (&lt;1%) of the user population, namely, the expert users . Over the duration of a month, we gathered tweets from over 500,000 Twitter users who are identified as experts on a diverse set of topics, and compared the resulting expert sampled tweets with the 1% randomly sampled tweets provided publicly by Twitter. We compared the sampled datasets along several dimensions, including the popularity, topical diversity, trustworthiness, and timeliness of the information contained within them, and on the sentiment/opinion expressed on specific topics. Our analysis reveals several important differences in data obtained through the different sampling methodologies, which have serious implications for applications such as topical search, trustworthy content recommendations, breaking news detection, and opinion mining.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2242576003",
    "type": "article"
  },
  {
    "title": "A Fast and Scalable Mechanism for Web Service Composition",
    "doi": "https://doi.org/10.1145/3098884",
    "publication_date": "2017-08-25",
    "publication_year": 2017,
    "authors": "Soumi Chattopadhyay; Ansuman Banerjee; Nilanjan Banerjee",
    "corresponding_authors": "",
    "abstract": "In recent times, automated business processes and web services have become ubiquitous in diverse application spaces. Efficient composition of web services in real time while providing necessary Quality of Service (QoS) guarantees is a computationally complex problem and several heuristic based approaches have been proposed to compose the services optimally. In this article, we present the design of a scalable QoS-aware service composition mechanism that balances the computational complexity of service composition with the QoS guarantees of the composed service and achieves scalability. Our design guarantees a single QoS parameter using an intelligent search and pruning mechanism in the composed service space. We also show that our methodology yields near optimal solutions on real benchmarks. We then enhance our proposed mechanism to guarantee multiple QoS parameters using aggregation techniques. Finally, we explore search time versus solution quality tradeoff using parameterized search algorithms that produce better-quality solutions at the cost of delay. We present experimental results to show the efficiency of our proposed mechanism.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2745591639",
    "type": "article"
  },
  {
    "title": "Semantics-Based Analysis of Content Security Policy Deployment",
    "doi": "https://doi.org/10.1145/3149408",
    "publication_date": "2018-01-27",
    "publication_year": 2018,
    "authors": "Stefano Calzavara; Alvise Rabitti; Michele Bugliesi",
    "corresponding_authors": "",
    "abstract": "Content Security Policy (CSP) is a recent W3C standard introduced to prevent and mitigate the impact of content injection vulnerabilities on websites. In this article, we introduce a formal semantics for the latest stable version of the standard, CSP Level 2. We then perform a systematic, large-scale analysis of the effectiveness of the current CSP deployment, using the formal semantics to substantiate our methodology and to assess the impact of the detected issues. We focus on four key aspects that affect the effectiveness of CSP: browser support, website adoption, correct configuration, and constant maintenance. Our analysis shows that browser support for CSP is largely satisfactory, with the exception of a few notable issues. However, there are several shortcomings relative to the other three aspects. CSP appears to have a rather limited deployment as yet and, more crucially, existing policies exhibit a number of weaknesses and misconfiguration errors. Moreover, content security policies are not regularly updated to ban insecure practices and remove unintended security violations. We argue that many of these problems can be fixed by better exploiting the monitoring facilities of CSP, while other issues deserve additional research, being more rooted into the CSP design.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2785237606",
    "type": "article"
  },
  {
    "title": "BUbiNG",
    "doi": "https://doi.org/10.1145/3160017",
    "publication_date": "2018-05-31",
    "publication_year": 2018,
    "authors": "Paolo Boldi; Andrea Marino; Massimo Santini; Sebastiano Vigna",
    "corresponding_authors": "",
    "abstract": "Although web crawlers have been around for twenty years by now, there is virtually no freely available, open-source crawling software that guarantees high throughput, overcomes the limits of single-machine systems, and, at the same time, scales linearly with the amount of resources available. This article aims at filling this gap, through the description of BUbiNG, our next-generation web crawler built upon the authors’ experience with UbiCrawler [9] and on the last ten years of research on the topic. BUbiNG is an open-source Java fully distributed crawler; a single BUbiNG agent, using sizeable hardware, can crawl several thousand pages per second respecting strict politeness constraints, both host- and IP-based. Unlike existing open-source distributed crawlers that rely on batch techniques (like MapReduce), BUbiNG job distribution is based on modern high-speed protocols to achieve very high throughput.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2805710541",
    "type": "article"
  },
  {
    "title": "Probabilistic QoS Aggregations for Service Composition",
    "doi": "https://doi.org/10.1145/2876513",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Huiyuan Zheng; Jian Yang; Weiliang Zhao",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a comprehensive approach for Quality of Service (QoS) calculation in service composition. Differing from the existing work on QoS aggregations that represent QoS as single values, discrete values with frequencies, or standard statistical distributions, the proposed approach has the capability to handle any type of QoS probability distribution. A set of formulae and algorithms are developed to calculate the QoS of a composite service according to four identified basic patterns as sequential, parallel, conditional, and loop. We demonstrate that the proposed QoS calculation method is much more efficient than existing simulation methods. It has a high scalability and builds a solid foundation for real-time QoS analysis and prediction in service composition. Experiment results are provided to show the effectiveness and efficiency of the proposed method.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2403451182",
    "type": "article"
  },
  {
    "title": "Attributed Collaboration Network Embedding for Academic Relationship Mining",
    "doi": "https://doi.org/10.1145/3409736",
    "publication_date": "2020-11-24",
    "publication_year": 2020,
    "authors": "Wei Wang; Jiaying Liu; Tao Tang; Suppawong Tuarob; Feng Xia; Zhiguo Gong; Irwin King",
    "corresponding_authors": "",
    "abstract": "Finding both efficient and effective quantitative representations for scholars in scientific digital libraries has been a focal point of research. The unprecedented amounts of scholarly datasets, combined with contemporary machine learning and big data techniques, have enabled intelligent and automatic profiling of scholars from this vast and ever-increasing pool of scholarly data. Meanwhile, recent advance in network embedding techniques enables us to mitigate the challenges of large scale and sparsity of academic collaboration networks. In real-world academic social networks, scholars are accompanied with various attributes or features, such as co-authorship and publication records, which result in attributed collaboration networks. It has been observed that both network topology and scholar attributes are important in academic relationship mining. However, previous studies mainly focus on network topology, whereas scholar attributes are overlooked. Moreover, the influence of different scholar attributes are unclear. To bridge this gap, in this work, we present a novel framework of Attributed Collaboration Network Embedding (ACNE) for academic relationship mining. ACNE extracts four types of scholar attributes based on the proposed scholar profiling model, including demographics, research, influence, and sociability. ACNE can learn a low-dimensional representation of scholars considering both scholar attributes and network topology simultaneously. We demonstrate the effectiveness and potentials of ACNE in academic relationship mining by performing collaborator recommendation on two real-world datasets and the contribution and importance of each scholar attribute on scientific collaborator recommendation is investigated. Our work may shed light on academic relationship mining by taking advantage of attributed collaboration network embedding.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W3106777270",
    "type": "article"
  },
  {
    "title": "Cold-start Point-of-interest Recommendation through Crowdsourcing",
    "doi": "https://doi.org/10.1145/3407182",
    "publication_date": "2020-08-25",
    "publication_year": 2020,
    "authors": "Pramit Mazumdar; Bidyut Kr. Patra; Korra Sathya Babu",
    "corresponding_authors": "",
    "abstract": "Recommender system is a popular tool that aims to provide personalized suggestions to user about items, products, services, and so on. Recommender system has effectively been used in online social networks, especially the location-based social networks for providing suggestions for interesting places known as POIs (points-of-interest). Popular recommender systems explore historical data to learn users’ preferences and, subsequently, they recommend locations to an active user. This strategy faces a major problem when a new POI or business evolves in a city. New business has no historical user experience data. Thus, a recommender system fails to gather enough knowledge about the new businesses, resulting in ignoring them during recommendations. This scenario is popularly known as a cold-start POI problem. Users never get recommendations of the new businesses in a city even though they can be relevant to a user. Also, from a business owner’s perspective, such a recommendation strategy does not help its reachability among users. Therefore, it is important for a recommender system to remain updated with new businesses in a city and ensure that all relevant POIs are recommended to a user irrespective of their lifetime. A POI recommendation approach is proposed in this work that can effectively handle the new businesses, or the cold-start POI problem, in a city. We crowdsource descriptions of cold-start POIs from various online social networks. The reviews of users are exploited here to learn the inherent features at the existing POIs and the new crowdsourced POIs. Finally, the proposed approach recommends top- K POIs consisting of the existing and new POIs. We perform experiments on the real-world Yelp dataset, which is one of the largest available data resources containing details on a wide range of businesses, users, and reviews. The proposed approach is compared with four existing POI recommendation approaches. The obtained results show that our approach outperforms others in handling cold-start POIs.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3080278124",
    "type": "article"
  },
  {
    "title": "Nudges to Mitigate Confirmation Bias during Web Search on Debated Topics: Support vs. Manipulation",
    "doi": "https://doi.org/10.1145/3635034",
    "publication_date": "2023-11-30",
    "publication_year": 2023,
    "authors": "Alisa Rieger; Tim Draws; Mariët Theune; Nava Tintarev",
    "corresponding_authors": "",
    "abstract": "When people use web search engines to find information on debated topics, the search results they encounter can influence opinion formation and practical decision-making with potentially far-reaching consequences for the individual and society. However, current web search engines lack support for information-seeking strategies that enable responsible opinion formation, e.g., by mitigating confirmation bias and motivating engagement with diverse viewpoints. We conducted two preregistered user studies to test the benefits and risks of an intervention aimed at confirmation bias mitigation. In the first study, we tested the effect of warning labels, warning of the risk of confirmation bias, combined with obfuscations, hiding selected search results per default. We observed that obfuscations with warning labels effectively reduce engagement with search results. These initial findings did not allow conclusions about the extent to which the reduced engagement was caused by the warning label (reflective nudging element) versus the obfuscation (automatic nudging element). If obfuscation was the primary cause, this would raise concerns about harming user autonomy. We thus conducted a follow-up study to test the effect of warning labels and obfuscations separately. According to our findings, obfuscations run the risk of manipulating behavior instead of guiding it, while warning labels without obfuscations (purely reflective) do not exhaust processing capacities but encourage users to actively choose to decrease engagement with attitude-confirming search results. Therefore, given the risks and unclear benefits of obfuscations and potentially other automatic nudging elements to guide engagement with information, we call for prioritizing interventions that aim to enhance human cognitive skills and agency instead.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4389196007",
    "type": "article"
  },
  {
    "title": "Discovering global network communities based on local centralities",
    "doi": "https://doi.org/10.1145/1326561.1326570",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Bo Yang; Jiming Liu",
    "corresponding_authors": "",
    "abstract": "One of the central problems in studying and understanding complex networks, such as online social networks or World Wide Web, is to discover hidden, either physically (e.g., interactions or hyperlinks) or logically (e.g., profiles or semantics) well-defined topological structures. From a practical point of view, a good example of such structures would be so-called network communities. Earlier studies have introduced various formulations as well as methods for the problem of identifying or extracting communities. While each of them has pros and cons as far as the effectiveness and efficiency are concerned, almost none of them has explicitly dealt with the potential relationship between the global topological property of a network and the local property of individual nodes. In order to study this problem, this paper presents a new algorithm, called ICS, which aims to discover natural network communities by inferring from the local information of nodes inherently hidden in networks based on a new centrality, that is, clustering centrality, which is a generalization of eigenvector centrality. As compared with existing methods, our method runs efficiently with a good clustering performance. Additionally, it is insensitive to its built-in parameters and prior knowledge.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2005844509",
    "type": "article"
  },
  {
    "title": "Do not crawl in the DUST",
    "doi": "https://doi.org/10.1145/1462148.1462151",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Ziv Bar-Yossef; Idit Keidar; Uri Schonfeld",
    "corresponding_authors": "",
    "abstract": "We consider the problem of DUST: Different URLs with Similar Text. Such duplicate URLs are prevalent in Web sites, as Web server software often uses aliases and redirections, and dynamically generates the same page from various different URL requests. We present a novel algorithm, DustBuster , for uncovering DUST; that is, for discovering rules that transform a given URL to others that are likely to have similar content. DustBuster mines DUST effectively from previous crawl logs or Web server logs, without /examining page contents. Verifying these rules via sampling requires fetching few actual Web pages. Search engines can benefit from information about DUST to increase the effectiveness of crawling, reduce indexing overhead, and improve the quality of popularity statistics such as PageRank.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2013761541",
    "type": "article"
  },
  {
    "title": "A large-scale empirical study of P3P privacy policies",
    "doi": "https://doi.org/10.1145/1513876.1513878",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Ian Reay; Scott Dick; James Miller",
    "corresponding_authors": "",
    "abstract": "Numerous studies over the past ten years have shown that concern for personal privacy is a major impediment to the growth of e-commerce. These concerns are so serious that most if not all consumer watchdog groups have called for some form of privacy protection for Internet users. In response, many nations around the world, including all European Union nations, Canada, Japan, and Australia, have enacted national legislation establishing mandatory safeguards for personal privacy. However, recent evidence indicates that Web sites might not be adhering to the requirements of this legislation. The goal of this study is to examine the posted privacy policies of Web sites, and compare these statements to the legal mandates under which the Web sites operate. We harvested all available P3P (Platform for Privacy Preferences Protocol) documents from the 100,000 most popular Web sites (over 3,000 full policies, and another 3,000 compact policies). This allows us to undertake an automated analysis of adherence to legal mandates on Web sites that most impact the average Internet user. Our findings show that Web sites generally do not even claim to follow all the privacy-protection mandates in their legal jurisdiction (we do not examine actual practice, only posted policies). Furthermore, this general statement appears to be true for every jurisdiction with privacy laws and any significant number of P3P policies, including European Union nations, Canada, Australia, and Web sites in the USA Safe Harbor program.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2034561998",
    "type": "article"
  },
  {
    "title": "IRLbot",
    "doi": "https://doi.org/10.1145/1541822.1541823",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Hsin-Tsang Lee; Derek Leonard; Xiaoming Wang; Dmitri Loguinov",
    "corresponding_authors": "",
    "abstract": "This article shares our experience in designing a Web crawler that can download billions of pages using a single-server implementation and models its performance. We first show that current crawling algorithms cannot effectively cope with the sheer volume of URLs generated in large crawls, highly branching spam, legitimate multimillion-page blog sites, and infinite loops created by server-side scripts. We then offer a set of techniques for dealing with these issues and test their performance in an implementation we call IRLbot. In our recent experiment that lasted 41 days, IRLbot running on a single server successfully crawled 6.3 billion valid HTML pages (7.6 billion connection requests) and sustained an average download rate of 319 mb/s (1,789 pages/s). Unlike our prior experiments with algorithms proposed in related work, this version of IRLbot did not experience any bottlenecks and successfully handled content from over 117 million hosts, parsed out 394 billion links, and discovered a subset of the Web graph with 41 billion unique nodes.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W1977836056",
    "type": "article"
  },
  {
    "title": "Browsing on small displays by transforming Web pages into hierarchically structured subpages",
    "doi": "https://doi.org/10.1145/1462148.1462152",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Xiangye Xiao; Qiong Luo; Danfeng Hong; Hongbo Fu; Xing Xie; Wei‐Ying Ma",
    "corresponding_authors": "",
    "abstract": "We propose a new Web page transformation method to facilitate Web browsing on handheld devices such as Personal Digital Assistants (PDAs). In our approach, an original Web page that does not fit on the screen is transformed into a set of subpages, each of which fits on the screen. This transformation is done through slicing the original page into page blocks iteratively, with several factors considered. These factors include the size of the screen, the size of each page block, the number of blocks in each transformed page, the depth of the tree hierarchy that the transformed pages form, as well as the semantic coherence between blocks. We call the tree hierarchy of the transformed pages an SP-tree. In an SP-tree, an internal node consists of a textually enhanced thumbnail image with hyperlinks, and a leaf node is a block extracted from a subpage of the original Web page. We adaptively adjust the fanout and the height of the SP-tree so that each thumbnail image is clear enough for users to read, while at the same time, the number of clicks needed to reach a leaf page is few. Through this transformation algorithm, we preserve the contextual information in the original Web page and reduce scrolling. We have implemented this transformation module on a proxy server and have conducted usability studies on its performance. Our system achieved a shorter task completion time compared with that of transformations from the Opera browser in nine of ten tasks. The average improvement on familiar pages was 44%. The average improvement on unfamiliar pages was 37%. Subjective responses were positive.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2022106151",
    "type": "article"
  },
  {
    "title": "A measurement study of insecure javascript practices on the web",
    "doi": "https://doi.org/10.1145/2460383.2460386",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Chuan Yue; Haining Wang",
    "corresponding_authors": "",
    "abstract": "JavaScript is an interpreted programming language most often used for enhancing webpage interactivity and functionality. It has powerful capabilities to interact with webpage documents and browser windows, however, it has also opened the door for many browser-based security attacks. Insecure engineering practices of using JavaScript may not directly lead to security breaches, but they can create new attack vectors and greatly increase the risks of browser-based attacks. In this article, we present the first measurement study on insecure practices of using JavaScript on the Web. Our focus is on the insecure practices of JavaScript inclusion and dynamic generation, and we examine their severity and nature on 6,805 unique websites. Our measurement results reveal that insecure JavaScript practices are common at various websites: (1) at least 66.4% of the measured websites manifest the insecure practices of including JavaScript files from external domains into the top-level documents of their webpages; (2) over 44.4% of the measured websites use the dangerous eval() function to dynamically generate and execute JavaScript code on their webpages; and (3) in JavaScript dynamic generation, using the document.write() method and the innerHTML property is much more popular than using the relatively secure technique of creating script elements via DOM methods. Our analysis indicates that safe alternatives to these insecure practices exist in common cases and ought to be adopted by website developers and administrators for reducing potential security risks.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W1995714055",
    "type": "article"
  },
  {
    "title": "A language for end-user web augmentation",
    "doi": "https://doi.org/10.1145/2460383.2460388",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Óscar Díaz; Cristóbal Arellano; Maider Azanza",
    "corresponding_authors": "",
    "abstract": "Web augmentation is to the Web what augmented reality is to the physical world: layering relevant content/layout/navigation over the existing Web to customize the user experience. This is achieved through JavaScript (JS) using browser weavers (e.g., Greasemonkey). To date, over 43 million of downloads of Greasemonkey scripts ground the vitality of this movement. However, Web augmentation is hindered by being programming intensive and prone to malware. This prevents end-users from participating as both producers and consumers of scripts: producers need to know JS, consumers need to trust JS. This article aims at promoting end-user participation in both roles. The vision is for end-users to prosume (the act of simultaneously caring for producing and consuming) scripts as easily as they currently prosume their pictures or videos. Encouraging production requires more “natural” and abstract constructs. Promoting consumption calls for augmentation scripts to be easier to understand, share, and trust upon. To this end, we explore the use of Domain-Specific Languages (DSLs) by introducing Sticklet . Sticklet is an internal DSL on JS, where JS generality is reduced for the sake of learnability and reliability. Specifically, Web augmentation is conceived as fixing in existing web sites (i.e., the wall ) HTML fragments extracted from either other sites or Web services (i.e., the stickers ). Sticklet targets hobby programmers as producers, and computer literates as consumers. From a producer perspective, benefits are threefold. As a restricted grammar on top of JS, Sticklet expressions are domain oriented and more declarative than their JS counterparts, hence speeding up development. As syntactically correct JS expressions, Sticklet scripts can be installed as traditional scripts and hence, programmers can continue using existing JS tools. As declarative expressions, they are easier to maintain, and amenable for optimization. From a consumer perspective, domain specificity brings understandability (due to declarativeness), reliability (due to built-in security), and “consumability” (i.e., installation/enactment/sharing of Sticklet expressions are tuned to the shortage of time and skills of the target audience). Preliminary evaluations indicate that 77% of the subjects were able to develop new Sticklet scripts in less than thirty minutes while 84% were able to consume these scripts in less than ten minutes. Sticklet is available to download as a Mozilla add-on.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W1975329220",
    "type": "article"
  },
  {
    "title": "Captions and biases in diagnostic search",
    "doi": "https://doi.org/10.1145/2486040",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Ryen W. White; Eric Horvitz",
    "corresponding_authors": "",
    "abstract": "People frequently turn to the Web with the goal of diagnosing medical symptoms. Studies have shown that diagnostic search can often lead to anxiety about the possibility that symptoms are explained by the presence of rare, serious medical disorders, rather than far more common benign syndromes. We study the influence of the appearance of potentially-alarming content, such as severe illnesses or serious treatment options associated with the queried for symptoms, in captions comprising titles, snippets, and URLs. We explore whether users are drawn to results with potentially-alarming caption content, and if so, the implications of such attraction for the design of search engines. We specifically study the influence of the content of search result captions shown in response to symptom searches on search-result click-through behavior. We show that users are significantly more likely to examine and click on captions containing potentially-alarming medical terminology such as “heart attack” or “medical emergency” independent of result rank position and well-known positional biases in users' search examination behaviors. The findings provide insights about the possible effects of displaying implicit correlates of searchers' goals in search-result captions, such as unexpressed concerns and fears. As an illustration of the potential utility of these results, we developed and evaluated an enhanced click prediction model that incorporates potentially-alarming caption features and show that it significantly outperforms models that ignore caption content. Beyond providing additional understanding of the effects of Web content on medical concerns, the methods and findings have implications for search engine design. As part of our discussion on the implications of this research, we propose procedures for generating more representative captions that may be less likely to cause alarm, as well as methods for learning to more appropriately rank search results from logged search behavior, for examples, by also considering the presence of potentially-alarming content in the captions that motivate observed clicks and down-weighting clicks seemingly driven by searchers' health anxieties.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2171664132",
    "type": "article"
  },
  {
    "title": "Integrating trust management and access control in data-intensive Web applications",
    "doi": "https://doi.org/10.1145/2180861.2180863",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Sabrina De Capitani di Vimercati; Sara Foresti; Sushil Jajodia; Stefano Paraboschi; Giuseppe Psaila; Pierangela Samarati",
    "corresponding_authors": "",
    "abstract": "The widespread diffusion of Web-based services provided by public and private organizations emphasizes the need for a flexible solution for protecting the information accessible through Web applications. A promising approach is represented by credential-based access control and trust management. However, although much research has been done and several proposals exist, a clear obstacle to the realization of their benefits in data-intensive Web applications is represented by the lack of adequate support in the DBMSs. As a matter of fact, DBMSs are often responsible for the management of most of the information that is accessed using a Web browser or a Web service invocation. In this article, we aim at eliminating this gap, and present an approach integrating trust management with the access control of the DBMS. We propose a trust model with a SQL syntax and illustrate an algorithm for the efficient verification of a delegation path for certificates. Our solution nicely complements current trust management proposals allowing the efficient realization of the services of an advanced trust management model within current relational DBMSs. An important benefit of our approach lies in its potential for a robust end-to-end design of security for personal data in Web scenario, where vulnerabilities of Web applications cannot be used to violate the protection of the data residing on the database server. We also illustrate the implementation of our approach within an open-source DBMS discussing design choices and performance impact.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2142807729",
    "type": "article"
  },
  {
    "title": "MultiWiki",
    "doi": "https://doi.org/10.1145/3004296",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Simon Gottschalk; Elena Demidova",
    "corresponding_authors": "",
    "abstract": "In this article, we address the problem of text passage alignment across interlingual article pairs in Wikipedia. We develop methods that enable the identification and interlinking of text passages written in different languages and containing overlapping information. Interlingual text passage alignment can enable Wikipedia editors and readers to better understand language-specific context of entities, provide valuable insights in cultural differences, and build a basis for qualitative analysis of the articles. An important challenge in this context is the tradeoff between the granularity of the extracted text passages and the precision of the alignment. Whereas short text passages can result in more precise alignment, longer text passages can facilitate a better overview of the differences in an article pair. To better understand these aspects from the user perspective, we conduct a user study at the example of the German, Russian, and English Wikipedia and collect a user-annotated benchmark. Then we propose MultiWiki, a method that adopts an integrated approach to the text passage alignment using semantic similarity measures and greedy algorithms and achieves precise results with respect to the user-defined alignment. The MultiWiki demonstration is publicly available and currently supports four language pairs.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2558395983",
    "type": "article"
  },
  {
    "title": "On Obstructing Obscenity Obfuscation",
    "doi": "https://doi.org/10.1145/3032963",
    "publication_date": "2017-04-24",
    "publication_year": 2017,
    "authors": "Sergio Rojas–Galeano",
    "corresponding_authors": "Sergio Rojas–Galeano",
    "abstract": "Obscenity (the use of rude words or offensive expressions) has spread from informal verbal conversations to digital media, becoming increasingly common on user-generated comments found in Web forums, newspaper user boards, social networks, blogs, and media-sharing sites. The basic obscenity-blocking mechanism is based on verbatim comparisons against a blacklist of banned vocabulary; however, creative users circumvent these filters by obfuscating obscenity with symbol substitutions or bogus segmentations that still visually preserve the original semantics, such as writing shit as $h¡;t or s.h.i.t or even worse mixing them as $.h….¡.t . The number of potential obfuscated variants is combinatorial, yielding the verbatim filter impractical. Here we describe a method intended to obstruct this anomaly inspired by sequence alignment algorithms used in genomics, coupled with a tailor-made edit penalty function. The method only requires to set up the vocabulary of plain obscenities; no further training is needed. Its complexity on screening a single obscenity is linear, both in runtime and memory, on the length of the user-generated text. We validated the method on three different experiments. The first one involves a new dataset that is also introduced in this article; it consists of a set of manually annotated real-life comments in Spanish, gathered from the news user boards of an online newspaper, containing this type of obfuscation. The second one is a publicly available dataset of comments in Portuguese from a sports Web site. In these experiments, at the obscenity level, we observed recall rates greater than 90%, whereas precision rates varied between 75% and 95%, depending on their sequence length (shorter lengths yielded a higher number of false alarms). On the other hand, at the comment level, we report recall of 86%, precision of 91%, and specificity of 98%. The last experiment revealed that the method is more effective in matching this type of obfuscation compared to the classical Levenshtein edit distance. We conclude discussing the prospects of the method to help enforcing moderation rules of obscenity expressions or as a preprocessing mechanism for sequence cleaning and/or feature extraction in more sophisticated text categorization techniques.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2609014685",
    "type": "article"
  },
  {
    "title": "Understanding Cross-Site Linking in Online Social Networks",
    "doi": "https://doi.org/10.1145/3213898",
    "publication_date": "2018-09-27",
    "publication_year": 2018,
    "authors": "Qingyuan Gong; Yang Chen; Jiyao Hu; Qiang Cao; Pan Hui; Xin Wang",
    "corresponding_authors": "",
    "abstract": "As a result of the blooming of online social networks (OSNs), a user often holds accounts on multiple sites. In this article, we study the emerging “cross-site linking” function available on mainstream OSN services including Foursquare, Quora, and Pinterest. We first conduct a data-driven analysis on crawled profiles and social connections of all 61.39 million Foursquare users to obtain a thorough understanding of this function. Our analysis has shown that the cross-site linking function is adopted by 57.10% of all Foursquare users, and the users who have enabled this function are more active than others. We also find that the enablement of cross-site linking might lead to privacy risks. Based on cross-site links between Foursquare and external OSN sites, we formulate cross-site information aggregation as a problem that uses cross-site links to stitch together site-local information fields for OSN users. Using large datasets collected from Foursquare, Facebook, and Twitter, we demonstrate the usefulness and the challenges of cross-site information aggregation. In addition to the measurements, we carry out a survey collecting detailed user feedback on cross-site linking. This survey studies why people choose to or not to enable cross-site linking, as well as the motivation and concerns of enabling this function.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2893523217",
    "type": "article"
  },
  {
    "title": "A Supervised Learning Approach to Protect Client Authentication on the Web",
    "doi": "https://doi.org/10.1145/2754933",
    "publication_date": "2015-06-12",
    "publication_year": 2015,
    "authors": "Stefano Calzavara; Gabriele Tolomei; Andrea Casini; Michele Bugliesi; Salvatore Orlando",
    "corresponding_authors": "",
    "abstract": "Browser-based defenses have recently been advocated as an effective mechanism to protect potentially insecure web applications against the threats of session hijacking, fixation, and related attacks. In existing approaches, all such defenses ultimately rely on client-side heuristics to automatically detect cookies containing session information, to then protect them against theft or otherwise unintended use. While clearly crucial to the effectiveness of the resulting defense mechanisms, these heuristics have not, as yet, undergone any rigorous assessment of their adequacy. In this article, we conduct the first such formal assessment, based on a ground truth of 2,464 cookies we collect from 215 popular websites of the Alexa ranking. To obtain the ground truth, we devise a semiautomatic procedure that draws on the novel notion of authentication token , which we introduce to capture multiple web authentication schemes. We test existing browser-based defenses in the literature against our ground truth, unveiling several pitfalls both in the heuristics adopted and in the methods used to assess them. We then propose a new detection method based on supervised learning , where our ground truth is used to train a set of binary classifiers, and report on experimental evidence that our method outperforms existing proposals. Interestingly, the resulting classifiers, together with our hands-on experience in the construction of the ground truth, provide new insight on how web authentication is actually implemented in practice.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2243132820",
    "type": "article"
  },
  {
    "title": "Spam Mobile Apps",
    "doi": "https://doi.org/10.1145/3007901",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Suranga Seneviratne; Aruna Seneviratne; Mohamed Ali Kâafar; Anirban Mahanti; Prasant Mohapatra",
    "corresponding_authors": "",
    "abstract": "The increased popularity of smartphones has attracted a large number of developers to offer various applications for the different smartphone platforms via the respective app markets. One consequence of this popularity is that the app markets are also becoming populated with spam apps. These spam apps reduce the users’ quality of experience and increase the workload of app market operators to identify these apps and remove them. Spam apps can come in many forms such as apps not having a specific functionality, those having unrelated app descriptions or unrelated keywords, or similar apps being made available several times and across diverse categories. Market operators maintain antispam policies and apps are removed through continuous monitoring. Through a systematic crawl of a popular app market and by identifying apps that were removed over a period of time, we propose a method to detect spam apps solely using app metadata available at the time of publication. We first propose a methodology to manually label a sample of removed apps, according to a set of checkpoint heuristics that reveal the reasons behind removal. This analysis suggests that approximately 35% of the apps being removed are very likely to be spam apps. We then map the identified heuristics to several quantifiable features and show how distinguishing these features are for spam apps. We build an Adaptive Boost classifier for early identification of spam apps using only the metadata of the apps. Our classifier achieves an accuracy of over 95% with precision varying between 85% and 95% and recall varying between 38% and 98%. We further show that a limited number of features, in the range of 10--30, generated from app metadata is sufficient to achieve a satisfactory level of performance. On a set of 180,627 apps that were present at the app market during our crawl, our classifier predicts 2.7% of the apps as potential spam. Finally, we perform additional manual verification and show that human reviewers agree with 82% of our classifier predictions.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2605225222",
    "type": "article"
  },
  {
    "title": "Exploring and Analyzing the Tor Hidden Services Graph",
    "doi": "https://doi.org/10.1145/3008662",
    "publication_date": "2017-07-24",
    "publication_year": 2017,
    "authors": "Massimo Bernaschi; Alessandro Celestini; Stefano Guarino; Flavio Lombardi",
    "corresponding_authors": "",
    "abstract": "The exploration and analysis of Web graphs has flourished in the recent past, producing a large number of relevant and interesting research results. However, the unique characteristics of the Tor network limit the applicability of standard techniques and demand for specific algorithms to explore and analyze it. The attention of the research community has focused on assessing the security of the Tor infrastructure (i.e., its ability to actually provide the intended level of anonymity) and on discussing what Tor is currently being used for. Since there are no foolproof techniques for automatically discovering Tor hidden services, little or no information is available about the topology of the Tor Web graph. Even less is known on the relationship between content similarity and topological structure. The present article aims at addressing such lack of information. Among its contributions: a study on automatic Tor Web exploration/data collection approaches; the adoption of novel representative metrics for evaluating Tor data; a novel in-depth analysis of the hidden services graph; a rich correlation analysis of hidden services’ semantics and topology. Finally, a broad interesting set of novel insights/considerations over the Tor Web organization and content are provided.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2737644180",
    "type": "article"
  },
  {
    "title": "Exploiting Usage to Predict Instantaneous App Popularity",
    "doi": "https://doi.org/10.1145/3199677",
    "publication_date": "2019-04-02",
    "publication_year": 2019,
    "authors": "Stephan Sigg; Eemil Lagerspetz; Ella Peltonen; Petteri Nurmi; Sasu Tarkoma",
    "corresponding_authors": "",
    "abstract": "Popularity of mobile apps is traditionally measured by metrics such as the number of downloads, installations, or user ratings. A problem with these measures is that they reflect usage only indirectly. Indeed, retention rates, i.e., the number of days users continue to interact with an installed app, have been suggested to predict successful app lifecycles. We conduct the first independent and large-scale study of retention rates and usage trends on a dataset of app-usage data from a community of 339,842 users and more than 213,667 apps. Our analysis shows that, on average, applications lose 65% of their users in the first week, while very popular applications (top 100) lose only 35%. It also reveals, however, that many applications have more complex usage behaviour patterns due to seasonality, marketing, or other factors. To capture such effects, we develop a novel app-usage trend measure which provides instantaneous information about the popularity of an application. Analysis of our data using this trend filter shows that roughly 40% of all apps never gain more than a handful of users ( Marginal apps). Less than 0.1% of the remaining 60% are constantly popular ( Dominant apps), 1% have a quick drain of usage after an initial steep rise ( Expired apps), and 6% continuously rise in popularity ( Hot apps). From these, we can distinguish, for instance, trendsetters from copycat apps. We conclude by demonstrating that usage behaviour trend information can be used to develop better mobile app recommendations.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2932450467",
    "type": "article"
  },
  {
    "title": "Cross-site Prediction on Social Influence for Cold-start Users in Online Social Networks",
    "doi": "https://doi.org/10.1145/3409108",
    "publication_date": "2021-05-12",
    "publication_year": 2021,
    "authors": "Qingyuan Gong; Yang Chen; Xinlei He; Yu Xiao; Pan Hui; Xin Wang; Xiaoming Fu",
    "corresponding_authors": "",
    "abstract": "Online social networks (OSNs) have become a commodity in our daily life. As an important concept in sociology and viral marketing, the study of social influence has received a lot of attentions in academia. Most of the existing proposals work well on dominant OSNs, such as Twitter, since these sites are mature and many users have generated a large amount of data for the calculation of social influence. Unfortunately, cold-start users on emerging OSNs generate much less activity data, which makes it challenging to identify potential influential users among them. In this work, we propose a practical solution to predict whether a cold-start user will become an influential user on an emerging OSN, by opportunistically leveraging the user’s information on dominant OSNs. A supervised machine learning-based approach is adopted, transferring the knowledge of both the descriptive information and dynamic activities on dominant OSNs. Descriptive features are extracted from the public data on a user’s homepage. In particular, to extract useful information from the fine-grained dynamic activities that cannot be represented by the statistical indices, we use deep learning technologies to deal with the sequential activity data. Using the real data of millions of users collected from Twitter (a dominant OSN) and Medium (an emerging OSN), we evaluate the performance of our proposed framework to predict prospective influential users. Our system achieves a high prediction performance based on different social influence definitions.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3160879595",
    "type": "article"
  },
  {
    "title": "The Internet with Privacy Policies: Measuring The Web Upon Consent",
    "doi": "https://doi.org/10.1145/3555352",
    "publication_date": "2022-08-27",
    "publication_year": 2022,
    "authors": "Nikhil Jha; Martino Trevisan; Luca Vassio; Marco Mellia",
    "corresponding_authors": "",
    "abstract": "To protect users' privacy, legislators have regulated the usage of tracking technologies, mandating the acquisition of users' consent before collecting data. Consequently, websites started showing more and more consent management modules -- i.e., Privacy Banners -- the visitors have to interact with to access the website content. They challenge the automatic collection of Web measurements, primarily to monitor the extensiveness of tracking technologies but also to measure Web performance in the wild. Privacy Banners in fact limit crawlers from observing the actual website content. In this paper, we present a thorough measurement campaign focusing on popular websites in Europe and the US, visiting both landing and internal pages from different countries around the world. We engineer Priv-Accept, a Web crawler able to accept the privacy policies, as most users would do in practice. This let us compare how webpages change before and after. Our results show that all measurements performed not dealing with the Privacy Banners offer a very biased and partial view of the Web. After accepting the privacy policies, we observe an increase of up to 70 trackers, which in turn slows down the webpage load time by a factor of 2x-3x.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3197225896",
    "type": "article"
  },
  {
    "title": "Fuzzy Influence Maximization in Social Networks",
    "doi": "https://doi.org/10.1145/3650179",
    "publication_date": "2024-03-01",
    "publication_year": 2024,
    "authors": "Ahmad Zareie; Rizos Sakellariou",
    "corresponding_authors": "",
    "abstract": "Influence maximization is a fundamental problem in social network analysis. This problem refers to the identification of a set of influential users as initial spreaders to maximize the spread of a message in a network. When such a message is spread, some users may be influenced by it. A common assumption of existing work is that the impact of a message is essentially binary: A user is either influenced (activated) or not influenced (non-activated). However, how strongly a user is influenced by a message may play an important role in this user’s attempt to influence subsequent users and spread the message further; existing methods may fail to model accurately the spreading process and identify influential users. In this article, we propose a novel approach to model a social network as a fuzzy graph where a fuzzy variable is used to represent the extent to which a user is influenced by a message (user’s activation level). By extending a diffusion model to simulate the spreading process in such a fuzzy graph, we conceptually formulate the fuzzy influence maximization problem for which three methods are proposed to identify influential users. Experimental results demonstrate the accuracy of the proposed methods in determining influential users in social networks.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4392357516",
    "type": "article"
  },
  {
    "title": "Crumbled Cookies: Exploring E-commerce Websites? Cookie Policies with Data Protection Regulations",
    "doi": "https://doi.org/10.1145/3708515",
    "publication_date": "2024-12-17",
    "publication_year": 2024,
    "authors": "Nivedita Singh; Yejin Do; Yinglei Yu; Imane Fouad; Jungrae Kim; Hyoungshick Kim",
    "corresponding_authors": "",
    "abstract": "Despite stringent data protection regulations, such as the General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and other country-specific laws, numerous websites continue to use cookies to track user activities, raising significant privacy concerns. This study aims to investigate the compliance of e-commerce websites with these regulations from a cookie perspective and explore potential variations in cookie policies across different countries. We conducted a comprehensive analysis of 360 popular e-commerce websites (44,323 cookies) across multiple countries, examining cookie attributes and their potential links to privacy and security breaches. Our findings revealed that 73% of third-party cookies function as tracker cookies, with around 40% breaching lifecycle regulations. Additionally, 85% are vulnerable to potential cross-site scripting (XSS) attacks, while only 349 out of 44,323 adhere to robust measures aimed at combating cross-site request forgery (CSRF) attacks. We also discovered instances of masquerading cookies, where third-party cookies disguise themselves as first-party cookies, enabling unauthorized user tracking without consent. To the best of our knowledge, this study is the first to comprehensively analyze the compliance of e-commerce websites with the GDPR, CCPA, and country-specific regulations concerning cookie policies across different jurisdictions. Our findings highlight the urgent need for uniform and consistent cookie policies across websites and jurisdictions, as well as robust enforcement mechanisms and increased transparency to ensure compliance with data protection regulations. This research contributes to the ongoing discourse on privacy protection and underscores the importance of addressing the challenges posed by insecure cookie practices in the e-commerce sector.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4405513851",
    "type": "article"
  },
  {
    "title": "Mining Historic Query Trails to Label Long and Rare Search Engine Queries",
    "doi": "https://doi.org/10.1145/1841909.1841912",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Peter Bailey; Ryen W. White; Han Liu; Giridhar Kumaran",
    "corresponding_authors": "",
    "abstract": "Web search engines can perform poorly for long queries (i.e., those containing four or more terms), in part because of their high level of query specificity. The automatic assignment of labels to long queries can capture aspects of a user’s search intent that may not be apparent from the terms in the query. This affords search result matching or reranking based on queries and labels rather than the query text alone. Query labels can be derived from interaction logs generated from many users’ search result clicks or from query trails comprising the chain of URLs visited following query submission. However, since long queries are typically rare, they are difficult to label in this way because little or no historic log data exists for them. A subset of these queries may be amenable to labeling by detecting similarities between parts of a long and rare query and the queries which appear in logs. In this article, we present the comparison of four similarity algorithms for the automatic assignment of Open Directory Project category labels to long and rare queries, based solely on matching against similar satisfied query trails extracted from log data. Our findings show that although the similarity-matching algorithms we investigated have tradeoffs in terms of coverage and accuracy, one algorithm that bases similarity on a popular search result ranking function (effectively regarding potentially-similar queries as “documents”) outperforms the others. We find that it is possible to correctly predict the top label better than one in five times, even when no past query trail exactly matches the long and rare query. We show that these labels can be used to reorder top-ranked search results leading to a significant improvement in retrieval performance over baselines that do not utilize query labeling, but instead rank results using content-matching or click-through logs. The outcomes of our research have implications for search providers attempting to provide users with highly-relevant search results for long queries.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2154347386",
    "type": "article"
  },
  {
    "title": "Using external aggregate ratings for improving individual recommendations",
    "doi": "https://doi.org/10.1145/1921591.1921594",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Akhmed Umyarov; Alexander Tuzhilin",
    "corresponding_authors": "",
    "abstract": "This article describes an approach for incorporating externally specified aggregate ratings information into certain types of recommender systems, including two types of collaborating filtering and a hierarchical linear regression model. First, we present a framework for incorporating aggregate rating information and apply this framework to the aforementioned individual rating models. Then we formally show that this additional aggregate rating information provides more accurate recommendations of individual items to individual users. Further, we experimentally confirm this theoretical finding by demonstrating on several datasets that the aggregate rating information indeed leads to better predictions of unknown ratings. We also propose scalable methods for incorporating this aggregate information and test our approaches on large datasets. Finally, we demonstrate that the aggregate rating information can also be used as a solution to the cold start problem of recommender systems.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2055894522",
    "type": "article"
  },
  {
    "title": "Extracting information networks from the blogosphere",
    "doi": "https://doi.org/10.1145/2344416.2344418",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Yuval Merhav; Filipe Mesquita; Denilson Barbosa; Wai Gen Yee; Ophir Frieder",
    "corresponding_authors": "",
    "abstract": "We study the problem of automatically extracting information networks formed by recognizable entities as well as relations among them from social media sites. Our approach consists of using state-of-the-art natural language processing tools to identify entities and extract sentences that relate such entities, followed by using text-clustering algorithms to identify the relations within the information network. We propose a new term-weighting scheme that significantly improves on the state-of-the-art in the task of relation extraction, both when used in conjunction with the standard tf ċ idf scheme and also when used as a pruning filter. We describe an effective method for identifying benchmarks for open information extraction that relies on a curated online database that is comparable to the hand-crafted evaluation datasets in the literature. From this benchmark, we derive a much larger dataset which mimics realistic conditions for the task of open information extraction. We report on extensive experiments on both datasets, which not only shed light on the accuracy levels achieved by state-of-the-art open information extraction tools, but also on how to tune such tools for better results.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2092922846",
    "type": "article"
  },
  {
    "title": "Navigating tomorrow's web",
    "doi": "https://doi.org/10.1145/2344416.2344420",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Marian Dörk; Carey Williamson; Sheelagh Carpendale",
    "corresponding_authors": "",
    "abstract": "We propose a new way of navigating the Web using interactive information visualizations, and present encouraging results from a large-scale Web study of a visual exploration system. While the Web has become an immense, diverse information space, it has also evolved into a powerful software platform. We believe that the established interaction techniques of searching and browsing do not sufficiently utilize these advances, since information seekers have to transform their information needs into specific, text-based search queries resulting in mostly text-based lists of resources. In contrast, we foresee a new type of information seeking that is high-level and more engaging, by providing the information seeker with interactive visualizations that give graphical overviews and enable query formulation. Building on recent work on faceted navigation, information visualization, and exploratory search, we conceptualize this type of information navigation as visual exploration and evaluate a prototype Web-based system that implements it. We discuss the results of a large-scale, mixed-method Web study that provides a better understanding of the potential benefits of visual exploration on the Web, and its particular performance challenges.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2160165932",
    "type": "article"
  },
  {
    "title": "Efficient Search Engine Measurements",
    "doi": "https://doi.org/10.1145/2019643.2019645",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Ziv Bar-Yossef; Maxim Gurevich",
    "corresponding_authors": "",
    "abstract": "We address the problem of externally measuring aggregate functions over documents indexed by search engines, like corpus size, index freshness, and density of duplicates in the corpus. State of the art estimators for such quantities [Bar-Yossef and Gurevich 2008b; Broder et al. 2006] are biased due to inaccurate approximation of the so called “document degrees”. In addition, the estimators in Bar-Yossef and Gurevich [2008b] are quite costly, due to their reliance on rejection sampling. We present new estimators that are able to overcome the bias introduced by approximate degrees. Our estimators are based on a careful implementation of an approximate importance sampling procedure. Comprehensive theoretical and empirical analysis of the estimators demonstrates that they have essentially no bias even in situations where document degrees are poorly approximated. By avoiding the costly rejection sampling approach, our new importance sampling estimators are significantly more efficient than the estimators proposed in Bar-Yossef and Gurevich [2008b]. Furthermore, building on an idea from Broder et al. [2006], we discuss Rao-Blackwellization as a generic method for reducing variance in search engine estimators. We show that Rao-Blackwellizing our estimators results in performance improvements, without compromising accuracy.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2059553973",
    "type": "article"
  },
  {
    "title": "Quality and Leniency in Online Collaborative Rating Systems",
    "doi": "https://doi.org/10.1145/2109205.2109209",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Hady W. Lauw; Ee‐Peng Lim; Ke Wang",
    "corresponding_authors": "",
    "abstract": "The emerging trend of social information processing has resulted in Web users’ increased reliance on user-generated content contributed by others for information searching and decision making. Rating scores, a form of user-generated content contributed by reviewers in online rating systems, allow users to leverage others’ opinions in the evaluation of objects. In this article, we focus on the problem of summarizing the rating scores given to an object into an overall score that reflects the object’s quality. We observe that the existing approaches for summarizing scores largely ignores the effect of reviewers exercising different standards in assigning scores. Instead of treating all reviewers as equals, our approach models the leniency of reviewers, which refers to the tendency of a reviewer to assign higher scores than other coreviewers. Our approach is underlined by two insights: (1) The leniency of a reviewer depends not only on how the reviewer rates objects, but also on how other reviewers rate those objects and (2) The leniency of a reviewer and the quality of rated objects are mutually dependent. We develop the leniency-aware quality , or LQ model, which solves leniency and quality simultaneously. We introduce both an exact and a ranked solution to the model. Experiments on real-life and synthetic datasets show that LQ is more effective than comparable approaches. LQ is also shown to perform consistently better under different parameter settings.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2093413654",
    "type": "article"
  },
  {
    "title": "Virtual private social networks and a facebook implementation",
    "doi": "https://doi.org/10.1145/2516633.2516636",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Mauro Conti; Arbnor Hasani; Bruno Crispo",
    "corresponding_authors": "",
    "abstract": "The popularity of Social Networking Sites (SNS) is growing rapidly, with the largest sites serving hundreds of millions of users and their private information. The privacy settings of these SNSs do not allow the user to avoid sharing some information (e.g., name and profile picture) with all the other users. Also, no matter the privacy settings, this information is always shared with the SNS (that could sell this information or be hacked). To mitigate these threats, we recently introduced the concept of Virtual Private Social Networks (VPSNs). In this work we propose the first complete architecture and implementation of VPSNs for Facebook. In particular, we address an important problem left unexplored in our previous research—that is the automatic propagation of updated profiles to all the members of the same VPSN. Furthermore, we made an in-depth study on performance and implemented several optimization to reduce the impact of VPSN on user experience. The proposed solution is lightweight, completely distributed, does not depend on the collaboration from Facebook, does not have a central point of failure, it offers (with some limitations) the same functionality as Facebook, and apart from some simple settings, the solution is almost transparent to the user. Thorough experiments, with an extended set of parameters, we have confirmed the feasibility of the proposal and have shown a very limited time-overhead experienced by the user while browsing Facebook pages.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2050054241",
    "type": "article"
  },
  {
    "title": "Improving contextual advertising by adopting collaborative filtering",
    "doi": "https://doi.org/10.1145/2516633.2516635",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Eloisa Vargiu; Alessandro Giuliani; Giuliano Armano",
    "corresponding_authors": "",
    "abstract": "Contextual advertising can be viewed as an information filtering task aimed at selecting suitable ads to be suggested to the final “user”, that is, the Web page in hand. Starting from this insight, in this article we propose a novel system, which adopts a collaborative filtering approach to perform contextual advertising. In particular, given a Web page, the system relies on collaborative filtering to classify the page content and to suggest suitable ads accordingly. Useful information is extracted from “inlinks”, that is, similar pages that link to the Web page in hand. In so doing, collaborative filtering is used in a content-based setting, giving rise to a hybrid contextual advertising system. After being implemented, the system has been experimented with about 15000 Web pages extracted from the Open Directory Project. Comparative experiments with a content-based system have been performed. The corresponding results highlight that the proposed system performs better. A suitable case study is also provided to enable the reader to better understand how the system works and its effectiveness.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2103356780",
    "type": "article"
  },
  {
    "title": "Textual and Content-Based Search in Repositories of Web Application Models",
    "doi": "https://doi.org/10.1145/2579991",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Bojana Bislimovska; Alessandro Bozzon; Marco Brambilla; Piero Fraternali",
    "corresponding_authors": "",
    "abstract": "Model-driven engineering relies on collections of models, which are the primary artifacts for software development. To enable knowledge sharing and reuse, models need to be managed within repositories, where they can be retrieved upon users’ queries. This article examines two different techniques for indexing and searching model repositories, with a focus on Web development projects encoded in a domain-specific language. Keyword-based and content-based search (also known as query-by-example) are contrasted with respect to the architecture of the system, the processing of models and queries, and the way in which metamodel knowledge can be exploited to improve search. A thorough experimental evaluation is conducted to examine what parameter configurations lead to better accuracy and to offer an insight in what queries are addressed best by each system.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2107446001",
    "type": "article"
  },
  {
    "title": "Periodicity in User Engagement with a Search Engine and Its Application to Online Controlled Experiments",
    "doi": "https://doi.org/10.1145/2856822",
    "publication_date": "2017-04-14",
    "publication_year": 2017,
    "authors": "Alexey Drutsa; Gleb Gusev; Pavel Serdyukov",
    "corresponding_authors": "",
    "abstract": "Nowadays, billions of people use the Web in connection with their daily needs. A significant part of these needs are constituted by search tasks that are usually addressed by search engines. Thus, daily search needs result in regular user engagement with a search engine. User engagement with web services was studied in various aspects, but there appears to be little work devoted to its regularity and periodicity. In this article, we study periodicity of user engagement with a popular search engine through applying spectrum analysis to temporal sequences of different engagement metrics. First, we found periodicity patterns of user engagement and revealed classes of users whose periodicity patterns do not change over a long period of time. In addition, we give an exhaustive analysis of the stability and quality of identified clusters. Second, we used the spectrum series as key metrics to evaluate search quality. We found that the novel periodicity metrics outperform the state-of-the-art quality metrics both in terms of significance level ( p -value) and sensitivity to a large set of larges-scale A/B experiments conducted on real search engine users.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2607093266",
    "type": "article"
  },
  {
    "title": "Modeling and Evaluating a Robust Feedback-Based Reputation System for E-Commerce Platforms",
    "doi": "https://doi.org/10.1145/3057265",
    "publication_date": "2017-07-12",
    "publication_year": 2017,
    "authors": "Alexandros Panagopoulos; Eleni Koutrouli; Aphrodite Tsalgatidou",
    "corresponding_authors": "",
    "abstract": "Despite the steady growth of e-commerce communities in the past two decades, little has changed in the way these communities manage reputation for building trust and for protecting their member's financial interests against fraud. As these communities mature and the defects of their reputation systems are revealed, further potential for deception against their members is created, that pushes the need for novel reputation mechanisms. Although a high volume of research works has explored the concepts of reputation and trust in e-communities, most of the proposed reputation systems target decentralized e-communities, focusing on issues related with the decentralized reputation management; they have not thus been integrated in e-commerce platforms. This work's objective is to provide an attackresilient feedback-based reputation system for modern e-commerce platforms, while minimizing the incurred financial burden of potent security schemes. Initially, we discuss a series of attacks and issues in reputation systems and study the different approaches of these problems from related works, while also considering the structural properties, defense mechanisms and policies of existing platforms. Then we present our proposition for a robust reputation system which consists of a novel reputation metric and attack prevention mechanisms. Finally, we describe the simulation framework and tool that we have implemented for thoroughly testing and evaluating the metric's resilience against attacks and present the evaluation experiments and their results. We consider the presented simulation framework as the second contribution of our article, aiming at facilitating the simulation and elaborate evaluation of reputation systems which specifically target e-commerce platforms by thoroughly presenting it, exhibiting its usage and making it available to the research community.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2736292221",
    "type": "article"
  },
  {
    "title": "Canonical Forms for Isomorphic and Equivalent RDF Graphs",
    "doi": "https://doi.org/10.1145/3068333",
    "publication_date": "2017-07-25",
    "publication_year": 2017,
    "authors": "Aidan Hogan",
    "corresponding_authors": "Aidan Hogan",
    "abstract": "Existential blank nodes greatly complicate a number of fundamental operations on Resource Description Framework (RDF) graphs. In particular, the problems of determining if two RDF graphs have the same structure modulo blank node labels (i.e., if they are isomorphic ), or determining if two RDF graphs have the same meaning under simple semantics (i.e., if they are simple-equivalent ), have no known polynomial-time algorithms. In this article, we propose methods that can produce two canonical forms of an RDF graph. The first canonical form preserves isomorphism such that any two isomorphic RDF graphs will produce the same canonical form; this iso-canonical form is produced by modifying the well-known canonical labelling algorithm N auty for application to RDF graphs. The second canonical form additionally preserves simple-equivalence such that any two simple-equivalent RDF graphs will produce the same canonical form; this equi-canonical form is produced by, in a preliminary step, leaning the RDF graph, and then computing the iso-canonical form. These algorithms have a number of practical applications, such as for identifying isomorphic or equivalent RDF graphs in a large collection without requiring pairwise comparison, for computing checksums or signing RDF graphs, for applying consistent Skolemisation schemes where blank nodes are mapped in a canonical manner to Internationalised Resource Identifiers (IRIs), and so forth. Likewise a variety of algorithms can be simplified by presupposing RDF graphs in one of these canonical forms. Both algorithms require exponential steps in the worst case; in our evaluation we demonstrate that there indeed exist difficult synthetic cases, but we also provide results over 9.9 million RDF graphs that suggest such cases occur infrequently in the real world, and that both canonical forms can be efficiently computed in all but a handful of such cases.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2737727228",
    "type": "article"
  },
  {
    "title": "Search and Breast Cancer",
    "doi": "https://doi.org/10.1145/2893481",
    "publication_date": "2016-04-29",
    "publication_year": 2016,
    "authors": "Michael J. Paul; Ryen W. White; Eric Horvitz",
    "corresponding_authors": "",
    "abstract": "We seek to understand the evolving needs of people who are faced with a life-changing medical diagnosis based on analyses of queries extracted from an anonymized search query log. Focusing on breast cancer, we manually tag a set of Web searchers as showing patterns of search behavior consistent with someone grappling with the screening, diagnosis, and treatment of breast cancer. We build and apply probabilistic classifiers to detect these searchers from multiple sessions and to identify the timing of diagnosis using temporal and statistical features. We explore the changes in information seeking over time before and after an inferred diagnosis of breast cancer by aligning multiple searchers by the estimated time of diagnosis. We employ the classifier to automatically identify 1,700 candidate searchers with an estimated 90% precision, and we predict the day of diagnosis within 15 days with an 88% accuracy. We show that the geographic and demographic attributes of searchers identified with high probability are strongly correlated with ground truth of reported incidence rates. We then analyze the content of queries over time for inferred cancer patients, using a detailed ontology of cancer-related search terms. The analysis reveals the rich temporal structure of the evolving queries of people likely diagnosed with breast cancer. Finally, we focus on subtypes of illness based on inferred stages of cancer and show clinically relevant dynamics of information seeking based on the dominant stage expressed by searchers.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2347072622",
    "type": "article"
  },
  {
    "title": "Pre-trained Language Model-based Retrieval and Ranking for Web Search",
    "doi": "https://doi.org/10.1145/3568681",
    "publication_date": "2022-10-20",
    "publication_year": 2022,
    "authors": "Lixin Zou; Weixue Lu; Yiding Liu; Hengyi Cai; Xiaokai Chu; Dehong Ma; Daiting Shi; Yu Sun; Zhicong Cheng; Simiu Gu; Shuaiqiang Wang; Dawei Yin",
    "corresponding_authors": "",
    "abstract": "Pre-trained language representation models (PLMs) such as BERT and Enhanced Representation through kNowledge IntEgration (ERNIE) have been integral to achieving recent improvements on various downstream tasks, including information retrieval. However, it is nontrivial to directly utilize these models for the large-scale web search due to the following challenging issues: (1) the prohibitively expensive computations of massive neural PLMs, especially for long texts in the web document, prohibit their deployments in the web search system that demands extremely low latency; (2) the discrepancy between existing task-agnostic pre-training objectives and the ad hoc retrieval scenarios that demand comprehensive relevance modeling is another main barrier for improving the online retrieval and ranking effectiveness; and (3) to create a significant impact on real-world applications, it also calls for practical solutions to seamlessly interweave the resultant PLM and other components into a cooperative system to serve web-scale data. Accordingly, we contribute a series of successfully applied techniques in tackling these exposed issues in this work when deploying the state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the online search engine system. We first present novel practices to perform expressive PLM-based semantic retrieval with a flexible poly-interaction scheme and cost-efficiently contextualize and rank web documents with a cheap yet powerful Pyramid-ERNIE architecture. We then endow innovative pre-training and fine-tuning paradigms to explicitly incentivize the query-document relevance modeling in PLM-based retrieval and ranking with the large-scale noisy and biased post-click behavioral data. We also introduce a series of effective strategies to seamlessly interwoven the designed PLM-based models with other conventional components into a cooperative system. Extensive offline and online experimental results show that our proposed techniques are crucial to achieving more effective search performance. We also provide a thorough analysis of our methodology and experimental results.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4306873598",
    "type": "article"
  },
  {
    "title": "Keywords-enhanced Deep Reinforcement Learning Model for Travel Recommendation",
    "doi": "https://doi.org/10.1145/3570959",
    "publication_date": "2022-11-11",
    "publication_year": 2022,
    "authors": "Lei Chen; Jie Cao; Weichao Liang; Jia Wu; Qiaolin Ye",
    "corresponding_authors": "",
    "abstract": "Tourism is an important industry and a popular entertainment activity involving billions of visitors per annum. One challenging problem tourists face is identifying satisfactory products from vast tourism information. Most of travel recommendation methods regard the recommendation procedure as a static process and only focus on immediate rewards. Meanwhile, they often infer user intensions from click behaviors and ignore the informative keywords of the clicked products. To this end, in this article, we present a Keywords-enhanced Deep Reinforcement Learning model (KDRL) framework. Specifically, we formalize travel recommendation as a Markov Decision Process and implement it upon the Actor–Critic framework. It integrates keyword information into the reinforcement learning–(RL) based recommendation framework by devising novel state representation and reward function and learns the travel recommendation and keywords generation simultaneously. To the best of our knowledge, this is the first time that keywords are explicitly discussed and used in RL-based travel recommendations. Extensive experiments are performed on the real-world datasets and the results clearly show the superior performance of KDRL compared with the baseline methods.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4308800355",
    "type": "article"
  },
  {
    "title": "Type Information Utilized Event Detection via Multi-Channel GNNs in Electrical Power Systems",
    "doi": "https://doi.org/10.1145/3577031",
    "publication_date": "2023-01-30",
    "publication_year": 2023,
    "authors": "Qian Li; Jianxin Li; Lihong Wang; Cheng Ji; Yiming Hei; Jiawei Sheng; Qingyun Sun; Shan Xue; Pengtao Xie",
    "corresponding_authors": "",
    "abstract": "Event detection in power systems aims to identify triggers and event types, which helps relevant personnel respond to emergencies promptly and facilitates the optimization of power supply strategies. However, the limited length of short electrical record texts causes severe information sparsity, and numerous domain-specific terminologies of power systems makes it difficult to transfer knowledge from language models pre-trained on general-domain texts. Traditional event detection approaches primarily focus on the general domain and ignore these two problems in the power system domain. To address the above issues, we propose a Multi-Channel graph neural network utilizing Type information for Event Detection in power systems, named MC-TED , leveraging a semantic channel and a topological channel to enrich information interaction from short texts. Concretely, the semantic channel refines textual representations with semantic similarity, building the semantic information interaction among potential event-related words. The topological channel generates a relation-type-aware graph modeling word dependencies, and a word-type-aware graph integrating part-of-speech tags. To further reduce errors worsened by professional terminologies in type analysis, a type learning mechanism is designed for updating the representations of both the word type and relation type in the topological channel. In this way, the information sparsity and professional term occurrence problems can be alleviated by enabling interaction between topological and semantic information. Furthermore, to address the lack of labeled data in power systems, we built a Chinese event detection dataset based on electrical Power Event texts, named PoE . In experiments, our model achieves compelling results not only on the PoE dataset, but on general-domain event detection datasets including ACE 2005 and MAVEN.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4318477598",
    "type": "article"
  },
  {
    "title": "User Experience and the Role of Personalization in Critiquing-Based Conversational Recommendation",
    "doi": "https://doi.org/10.1145/3597499",
    "publication_date": "2023-05-18",
    "publication_year": 2023,
    "authors": "Arpit Rana; Scott Sanner; Mohamed Reda Bouadjenek; Ronald Di Carlantonio; Gary Farmaner",
    "corresponding_authors": "",
    "abstract": "Critiquing—where users propose directional preferences to attribute values—has historically been a highly popular method for conversational recommendation. However, with the growing size of catalogs and item attributes, it becomes increasingly difficult and time-consuming to express all of one’s constraints and preferences in the form of critiquing. It is found to be even more confusing in case of critiquing failures: when the system returns no matching items in response to user critiques. To this end, it would seem important to combine a critiquing-based conversational system with a personalized recommendation component to capture implicit user preferences and thus reduce the user’s burden of providing explicit critiques. To examine the impact of such personalization on critiquing, this article reports on a user study with 228 participants to understand user critiquing behavior for two different recommendation algorithms: (i) non-personalized , that recommends any item consistent with the user critiques; and (ii) personalized , which leverages a user’s past preferences on top of user critiques. In the study, we ask users to find a restaurant that they think is the most suitable to a given scenario by critiquing the recommended restaurants at each round of the conversation on the dimensions of price, cuisine, category, and distance. We observe that the non-personalized recommender leads to more critiquing interactions, more severe critiquing failures, overall more time for users to express their preferences, and longer dialogs to find their item of interest. We also observe that non-personalized users were less satisfied with the system’s performance. They find its recommendations less relevant, more unexpected, and somewhat equally diverse and surprising than those of personalized ones. The results of our user study highlight an imperative for further research on the integration of the two complementary components of personalization and critiquing to achieve the best overall user experience in future critiquing-based conversational recommender systems.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4377021837",
    "type": "article"
  },
  {
    "title": "Exif2Vec: A Framework to Ascertain Untrustworthy Crowdsourced Images Using Metadata",
    "doi": "https://doi.org/10.1145/3645094",
    "publication_date": "2024-02-13",
    "publication_year": 2024,
    "authors": "Muhammad Umair; Athman Bouguettaya; Abdallah Lakhdari; Mourad Ouzzani; Yuyun Liu",
    "corresponding_authors": "",
    "abstract": "In the context of social media, the integrity of images is often dubious. To tackle this challenge, we introduce Exif2Vec , a novel framework specifically designed to discover modifications in social media images. The proposed framework leverages an image’s metadata to discover changes in an image. We use a service-oriented approach that considers discovery of changes in images as a service . A novel word-embedding-based approach is proposed to discover semantic inconsistencies in an image metadata that are reflective of the changes in an image. These inconsistencies are used to measure the severity of changes. The novelty of the approach resides in that it does not require the use of images to determine the underlying changes. We use a pretrained Word2Vec model to conduct experiments. The model is validated on two different fact-checked image datasets, i.e., images related to general context and a context-specific image dataset. Notably, our findings showcase the remarkable efficacy of our approach, yielding results of up to 80% accuracy. This underscores the potential of our framework.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391777562",
    "type": "article"
  },
  {
    "title": "Cuckoo Search Optimization-Based Influence Maximization in Dynamic Social Networks",
    "doi": "https://doi.org/10.1145/3690644",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Sunil Kumar Meena; Shashank Sheshar Singh; Kuldeep Singh",
    "corresponding_authors": "",
    "abstract": "Online social networks are crucial in propagating information and exerting influence through word-of-mouth transmission. Influence maximization (IM) is the fundamental task in social network analysis to find the group of nodes that maximizes the influence in the social network. IM has different applications like viral marketing, campaigning, advertising, and so on. Literature has presented various algorithms based on different approaches to address the IM problem, including nature-inspired algorithms. Most of the work focuses on the static social network. The proposed work first employs nature-inspired Cuckoo Search Optimization to solve the IM problem in dynamic networks. The proposed algorithm applies the fuzzy-logic-based technique to optimize the nests. We also perform statistical tests to show the effectiveness of the proposed algorithm with the benchmark algorithms. The experimental results are performed on five datasets and compare the results with the state-of-the-art algorithms. The results show that the proposed algorithm gives better results than the nature-inspired state-of-the-art algorithms.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4401958419",
    "type": "article"
  },
  {
    "title": "From Nodes to Knowledge: Exploring Social Network Analysis in Education",
    "doi": "https://doi.org/10.1145/3707463",
    "publication_date": "2024-12-05",
    "publication_year": 2024,
    "authors": "Shashank Sheshar Singh; Samya Muhuri; Sumit Kumar; Jayendra Barua",
    "corresponding_authors": "",
    "abstract": "In the evolving education landscape, this survey investigates the integration and transformation of educational paradigms using social network analysis (SNA). This paper examines the fundamentals of SNA, including nodes, edges, centrality metrics, and network dynamics, for a comprehensive understanding of the education domain. It guides researchers through various applications of SNA in education, such as student-teacher networks and institutional collaborations, highlighting the advantages and challenges of these complex interactions. The paper assesses the methodologies used in educational SNA, including data collection strategies and the associated ethical considerations. The survey also discusses various case studies and applications where SNA facilitates well-informed decision-making, enhanced academic collaboration, and the evaluation of student performance. This paper focuses on the transformative potential of SNA and acknowledges the limitations, ethical dilemmas, and technological challenges in the field. It concludes with a forward-looking perspective on the future of SNA in education, showcasing supportive technological advancement. This survey highlights the evolution of SNA since its incorporation into educational research and practices.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4405081466",
    "type": "article"
  },
  {
    "title": "Assessing relevance and trust of the deep web sources and results based on inter-source agreement",
    "doi": "https://doi.org/10.1145/2460383.2460390",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Raju Balakrishnan; Subbarao Kambhampati; Manishkumar Jha",
    "corresponding_authors": "",
    "abstract": "Deep web search engines face the formidable challenge of retrieving high-quality results from the vast collection of searchable databases. Deep web search is a two-step process of selecting the high-quality sources and ranking the results from the selected sources. Though there are existing methods for both the steps, they assess the relevance of the sources and the results using the query-result similarity. When applied to the deep web these methods have two deficiencies. First is that they are agnostic to the correctness (trustworthiness) of the results. Second, the query-based relevance does not consider the importance of the results and sources. These two considerations are essential for the deep web and open collections in general. Since a number of deep web sources provide answers to any query, we conjuncture that the agreements between these answers are helpful in assessing the importance and the trustworthiness of the sources and the results. For assessing source quality, we compute the agreement between the sources as the agreement of the answers returned. While computing the agreement, we also measure and compensate for the possible collusion between the sources. This adjusted agreement is modeled as a graph with sources at the vertices. On this agreement graph, a quality score of a source, that we call SourceRank , is calculated as the stationary visit probability of a random walk. For ranking results, we analyze the second-order agreement between the results. Further extending SourceRank to multidomain search, we propose a source ranking sensitive to the query domains. Multiple domain-specific rankings of a source are computed, and these ranks are combined for the final ranking. We perform extensive evaluations on online and hundreds of Google Base sources spanning across domains. The proposed result and source rankings are implemented in the deep web search engine Factal . We demonstrate that the agreement analysis tracks source corruption. Further, our relevance evaluations show that our methods improve precision significantly over Google Base and the other baseline methods. The result ranking and the domain-specific source ranking are evaluated separately.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2019106661",
    "type": "article"
  },
  {
    "title": "Designing and Implementing the OP and OP2 Web Browsers",
    "doi": "https://doi.org/10.1145/1961659.1961665",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Chris Grier; Shuo Tang; Samuel T. King",
    "corresponding_authors": "",
    "abstract": "Current web browsers are plagued with vulnerabilities, providing hackers with easy access to computer systems via browser-based attacks. Browser security efforts that retrofit existing browsers have had limited success because the design of modern browsers is fundamentally flawed. To enable more secure web browsing, we design and implement a new browser, called the OP web browser, that attempts to improve the state-of-the-art in browser security. We combine operating system design principles with formal methods to design a more secure web browser by drawing on the expertise of both communities. Our design philosophy is to partition the browser into smaller subsystems and make all communication between subsystems simple and explicit. At the core of our design is a small browser kernel that manages the browser subsystems and interposes on all communications between them to enforce our new browser security features. To show the utility of our browser architecture, we design and implement three novel security features. First, we develop flexible security policies that allow us to include browser plugins within our security framework. Second, we use formal methods to prove useful security properties including user interface invariants and browser security policy. Third, we design and implement a browser-level information-flow tracking system to enable post-mortem analysis of browser-based attacks. In addition to presenting the OP browser architecture, we discuss the design and implementation of a second version of OP, OP2, that includes features from other secure web browser designs to improve on the overall security and performance of OP. To evaluate our design, we implemented OP2 and tested both performance, memory, and filesystem impact while browsing popular pages. We show that the additional security features in OP and OP2 introduce minimal overhead.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2089180764",
    "type": "article"
  },
  {
    "title": "A Practical Architecture for an Anycast CDN",
    "doi": "https://doi.org/10.1145/2019643.2019644",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Hussein A. Alzoubi; Seungjoon Lee; Michael Rabinovich; Oliver Spatscheck; Jacobus Van der Merwe",
    "corresponding_authors": "",
    "abstract": "IP Anycast has many attractive features for any service that involve the replication of multiple instances across the Internet. IP Anycast allows multiple instances of the same service to be “naturally” discovered, and requests for this service to be delivered to the closest instance. However, while briefly considered as an enabler for content delivery networks (CDNs) when they first emerged, IP Anycast was deemed infeasible in that environment. The main reasons for this decision were the lack of load awareness of IP Anycast and unwanted side effects of Internet routing changes on the IP Anycast mechanism. In this article we re-evaluate IP Anycast for CDNs by proposing a load-aware IP Anycast CDN architecture. Our architecture is prompted by recent developments in route control technology, as well as better understanding of the behavior of IP Anycast in operational settings. Our architecture makes use of route control mechanisms to take server and network load into account to realize load-aware Anycast. We show that the resulting redirection requirements can be formulated as a Generalized Assignment Problem and present practical algorithms that address these requirements while at the same time limiting connection disruptions that plague regular IP Anycast. We evaluate our algorithms through trace based simulation using traces obtained from a production CDN network.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2091407897",
    "type": "article"
  },
  {
    "title": "Test-Based Security Certification of Composite Services",
    "doi": "https://doi.org/10.1145/3267468",
    "publication_date": "2018-12-04",
    "publication_year": 2018,
    "authors": "Marco Anisetti; Claudio A. Ardagna; Ernesto Damiani; Gianluca Polegri",
    "corresponding_authors": "",
    "abstract": "The diffusion of service-based and cloud-based systems has created a scenario where software is often made available as services, offered as commodities over corporate networks or the global net. This scenario supports the definition of business processes as composite services, which are implemented via either static or runtime composition of offerings provided by different suppliers. Fast and accurate evaluation of services’ security properties becomes then a fundamental requirement and is nowadays part of the software development process. In this article, we show how the verification of security properties of composite services can be handled by test-based security certification and built to be effective and efficient in dynamic composition scenarios. Our approach builds on existing security certification schemes for monolithic services and extends them towards service compositions. It virtually certifies composite services, starting from certificates awarded to the component services. We describe three heuristic algorithms for generating runtime test-based evidence of the composite service holding the properties. These algorithms are compared with the corresponding exhaustive algorithm to evaluate their quality and performance. We also evaluate the proposed approach in a real-world industrial scenario, which considers ENGpay online payment system of Engineering Ingegneria Informatica S.p.A. The proposed industrial evaluation presents the utility and generality of the proposed approach by showing how certification results can be used as a basis to establish compliance to Payment Card Industry Data Security Standard.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2902927541",
    "type": "article"
  },
  {
    "title": "Faster Base64 Encoding and Decoding Using AVX2 Instructions",
    "doi": "https://doi.org/10.1145/3132709",
    "publication_date": "2018-07-17",
    "publication_year": 2018,
    "authors": "Wojciech Muła; Daniel Lemire",
    "corresponding_authors": "",
    "abstract": "Web developers use base64 formats to include images, fonts, sounds and other resources directly inside HTML, JavaScript, JSON and XML files. We estimate that billions of base64 messages are decoded every day. We are motivated to improve the efficiency of base64 encoding and decoding. Compared to state-of-the-art implementations, we multiply the speeds of both the encoding (~10x) and the decoding (~7x). We achieve these good results by using the single-instruction-multiple-data (SIMD) instructions available on recent Intel processors (AVX2). Our accelerated software abides by the specification and reports errors when encountering characters outside of the base64 set. It is available online as free software under a liberal license.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3122754715",
    "type": "article"
  },
  {
    "title": "Form-Based Web Service Composition for Domain Experts",
    "doi": "https://doi.org/10.1145/2542168",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Ingo Weber; Hye-Young Paik; Boualem Benatallah",
    "corresponding_authors": "",
    "abstract": "In many cases, it is not cost effective to automate business processes which affect a small number of people and/or change frequently. We present a novel approach for enabling domain experts to model and deploy such processes from their respective domain as Web service compositions. The approach builds on user-editable service, naming and representing Web services as forms. On this basis, the approach provides a visual composition language with a targeted restriction of control-flow expressivity, process simulation, automated process verification mechanisms, and code generation for executing orchestrations. A Web-based service composition prototype implements this approach, including a WS-BPEL code generator. A small lab user study with 14 participants showed promising results for the usability of the system, even for nontechnical domain experts.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2008829494",
    "type": "article"
  },
  {
    "title": "Analysis and Solution of CSS-Sprite Packing Problem",
    "doi": "https://doi.org/10.1145/2818377",
    "publication_date": "2015-12-29",
    "publication_year": 2015,
    "authors": "Jakub Marszałkowski; Jan Mizgajski; Dariusz Mokwa; Maciej Drozdowski",
    "corresponding_authors": "",
    "abstract": "A CSS-sprite packing problem is considered in this article. CSS-sprite is a technique of combining many pictures of a web page into one image for the purpose of reducing network transfer time. The CSS-sprite packing problem is formulated here as an optimization challenge. The significance of geometric packing, image compression and communication performance is discussed. A mathematical model for constructing multiple sprites and optimization of load time is proposed. The impact of PNG-sprite aspect ratio on file size is studied experimentally. Benchmarking of real user web browsers communication performance covers latency, bandwidth, number of concurrent channels as well as speedup from parallel download. Existing software for building CSS-sprites is reviewed. A novel method, called Spritepack , is proposed and evaluated. Spritepack outperforms current software.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2212434113",
    "type": "article"
  },
  {
    "title": "Location-Based Distance Measures for Geosocial Similarity",
    "doi": "https://doi.org/10.1145/3054951",
    "publication_date": "2017-07-03",
    "publication_year": 2017,
    "authors": "Yaron Kanza; Elad Kravi; Eliyahu Safra; Yehoshua Sagiv",
    "corresponding_authors": "",
    "abstract": "This article investigates the problem of geosocial similarity among users of online social networks, based on the locations of their activities (e.g., posting messages or photographs). Finding pairs of geosocially similar users or detecting that two sets of locations (of activities) belong to the same user has important applications in privacy protection, recommendation systems, urban planning, and public health, among others. It is explained and shown empirically that common distance measures between sets of locations are inadequate for determining geosocial similarity. Two novel distance measures between sets of locations are introduced. One is the mutually nearest distance that is based on computing a matching between two sets. The second measure uses a quad-tree index. It is highly scalable but incurs the overhead of creating and maintaining the index. Algorithms with optimization techniques are developed for computing the two distance measures and also for finding the k -most-similar users of a given one. Extensive experiments, using geotagged messages from Twitter, show that the new distance measures are both more accurate and more efficient than existing ones.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2725880948",
    "type": "article"
  },
  {
    "title": "Recommendation in a Changing World",
    "doi": "https://doi.org/10.1145/3108238",
    "publication_date": "2017-08-21",
    "publication_year": 2017,
    "authors": "Yining Liu; Yong Liu; Yanming Shen; Keqiu Li",
    "corresponding_authors": "",
    "abstract": "Users’ preferences, and consequently their ratings and reviews to items, change over time. Likewise, characteristics of items are also time-varying. By dividing data into time periods, temporal Recommender Systems (RSs) improve recommendation accuracy by exploring the temporal dynamics in user rating data. However, temporal RSs have to cope with rating sparsity in each time period. Meanwhile, reviews generated by users contain rich information about their preferences, which can be exploited to address rating sparsity and further improve the performance of temporal RSs. In this article, we develop a temporal rating model with topics that jointly mines the temporal dynamics of both user-item ratings and reviews. Studying temporal drifts in reviews helps us understand item rating evolutions and user interest changes over time. Our model also automatically splits the review text in each time period into interim words and intrinsic words. By linking interim words and intrinsic words to short-term and long-term item features, respectively, we jointly mine the temporal changes in user and item latent features together with the associated review text in a single learning stage. Through experiments on 28 real-world datasets collected from Amazon , we show that the rating prediction accuracy of our model significantly outperforms the existing state-of-art RS models. And our model can automatically identify representative interim words in each time period as well as intrinsic words across all time periods. This can be very useful in understanding the time evolution of users’ preferences and items’ characteristics.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2749348708",
    "type": "article"
  },
  {
    "title": "Web Portals for High-performance Computing",
    "doi": "https://doi.org/10.1145/3197385",
    "publication_date": "2019-02-05",
    "publication_year": 2019,
    "authors": "Patrice Calegari; Marc Levrier; Paweł Balczyński",
    "corresponding_authors": "",
    "abstract": "This article addresses web interfaces for High-performance Computing (HPC) simulation software. First, it presents a brief history, starting in the 1990s with Java applets, of web interfaces used for accessing and making best possible use of remote HPC resources. It introduces HPC web-based portal use cases. Then it identifies and discusses the key features, among functional and non-functional requirements, that characterize such portals. A brief state of the art is then presented. The design and development of Bull extreme factory Computing Studio v3 (XCS3) is chosen as a common thread for showing how the identified key features can all be implemented in one software: multi-tenancy, multi-scheduler compatibility, complete control through an HTTP RESTful API, customizable user interface with Responsive Web Design, HPC application template framework, remote visualization, and access through the Authentication, Authorization, and Accounting security framework with the Role-Based Access Control permission model. Non-functional requirements (security, usability, performance, reliability) are discussed, and the article concludes by giving perspective for future work.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2913438032",
    "type": "article"
  },
  {
    "title": "Joint Credibility Estimation of News, User, and Publisher via Role-relational Graph Convolutional Networks",
    "doi": "https://doi.org/10.1145/3617418",
    "publication_date": "2023-08-26",
    "publication_year": 2023,
    "authors": "Anu Shrestha; Jason Duran; Francesca Spezzano; Edoardo Serra",
    "corresponding_authors": "",
    "abstract": "The presence of fake news on online social media is overwhelming and is responsible for having impacted several aspects of people’s lives, from health to politics, the economy, and response to natural disasters. Although significant effort has been made to mitigate fake news spread, current research focuses on single aspects of the problem, such as detecting fake news spreaders and classifying stories as either factual or fake. In this article, we propose a new method to exploit inter-relationships between stories, sources, and final users and integrate prior knowledge of these three entities to jointly estimate the credibility degree of each entity involved in the news ecosystem. Specifically, we develop a new graph convolutional network, namely, Role-Relational Graph Convolutional Networks (Role-RGCN), to learn, for each node type (or role), a unique node representation space and jointly connect the different representation spaces with edge relations. To test our proposed approach, we conducted an experimental evaluation on the state-of-the-art FakeNewsNet-Politifact dataset and a new dataset with ground truth on news credibility degrees we collected. Experimental results show a superior performance of our Role-RGCN proposed method at predicting the credibility degree of stories, sources, and users compared to state-of-the-art approaches and other baselines.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4386193771",
    "type": "article"
  },
  {
    "title": "A large-scale study on map search logs",
    "doi": "https://doi.org/10.1145/1806916.1806917",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Xiangye Xiao; Qiong Luo; Zhisheng Li; Xing Xie; Wei‐Ying Ma",
    "corresponding_authors": "",
    "abstract": "Map search engines, such as Google Maps, Yahoo! Maps, and Microsoft Live Maps, allow users to explicitly specify a target geographic location, either in keywords or on the map, and to search businesses, people, and other information of that location. In this article, we report a first study on a million-entry map search log. We identify three key attributes of a map search record—the keyword query, the target location and the user location, and examine the characteristics of these three dimensions separately as well as the associations between them. Comparing our results with those previously reported on logs of general search engines and mobile search engines, including those for geographic queries, we discover the following unique features of map search: (1) People use longer queries and modify queries more frequently in a session than in general search and mobile search; People view fewer result pages per query than in general search; (2) The popular query topics in map search are different from those in general search and mobile search; (3) The target locations in a session change within 50 kilometers for almost 80% of the sessions; (4) Queries, search target locations and user locations (both at the city level) all follow the power law distribution; (5) One third of queries are issued for target locations within 50 kilometers from the user locations; (6) The distribution of a query over target locations appears to follow the geographic location of the queried entity.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2046437626",
    "type": "article"
  },
  {
    "title": "Correctness-aware high-level functional matching approaches for semantic Web services",
    "doi": "https://doi.org/10.1145/1346337.1346240",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Islam Elgedawy; Zahir Tari; James A. Thom",
    "corresponding_authors": "",
    "abstract": "Service matching approaches trade precision for recall, creating the need for users to choose the correct services, which obviously is a major obstacle for automating the service discovery and aggregation processes. Our approach to overcome this problem, is to eliminate the appearance of false positives by returning only the correct services. As different users have different semantics for what is correct, we argue that the correctness of the matching results must be determined according to the achievement of users' goals: that only services achieving users' goals are considered correct. To determine such correctness, we argue that the matching process should be based primarily on the high-level functional specifications (namely goals, achievement contexts, and external behaviors). In this article, we propose models, data structures, algorithms, and theorems required to correctly match such specifications. We propose a model called G + , to capture such specifications, for both services and users, in a machine-understandable format. We propose a data structure, called a Concepts Substitutability Graph (CSG), to capture the substitution semantics of application domain concepts in a context-based manner, in order to determine the semantic-preserving mapping transformations required to match different G + models. We also propose a behavior matching approach that is able to match states in an m-to-n manner, such that behavior models with different numbers of state transitions can be matched. Finally, we show how services are matched and aggregated according to their G + models. Results of supporting experiments demonstrate the advantages of the proposed service matching approaches.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2055189624",
    "type": "article"
  },
  {
    "title": "Cookies",
    "doi": "https://doi.org/10.1145/1541822.1541824",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Andrew F. Tappenden; James Miller",
    "corresponding_authors": "",
    "abstract": "The results of an extensive investigation of cookie deployment amongst 100,000 Internet sites are presented. Cookie deployment is found to be approaching universal levels and hence there exists an associated need for relevant Web and software engineering processes, specifically testing strategies which actively consider cookies. The semi-automated investigation demonstrates that over two-thirds of the sites studied deploy cookies. The investigation specifically examines the use of first-party, third-party, sessional, and persistent cookies within Web-based applications, identifying the presence of a P3P policy and dynamic Web technologies as major predictors of cookie usage. The results are juxtaposed with the lack of testing strategies present in the literature. A number of real-world examples, including two case studies are presented, further accentuating the need for comprehensive testing strategies for Web-based applications. The use of antirandom test case generation is explored with respect to the testing issues discussed. Finally, a number of seeding vectors are presented, providing a basis for testing cookies within Web-based applications.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1967729292",
    "type": "article"
  },
  {
    "title": "Exploiting External Collections for Query Expansion",
    "doi": "https://doi.org/10.1145/2382616.2382621",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Wouter Weerkamp; Krisztian Balog; Maarten de Rijke",
    "corresponding_authors": "",
    "abstract": "A persisting challenge in the field of information retrieval is the vocabulary mismatch between a user’s information need and the relevant documents. One way of addressing this issue is to apply query modeling: to add terms to the original query and reweigh the terms. In social media, where documents usually contain creative and noisy language (e.g., spelling and grammatical errors), query modeling proves difficult. To address this, attempts to use external sources for query modeling have been made and seem to be successful. In this article we propose a general generative query expansion model that uses external document collections for term generation: the External Expansion Model (EEM). The main rationale behind our model is our hypothesis that each query requires its own mixture of external collections for expansion and that an expansion model should account for this. For some queries we expect, for example, a news collection to be most beneficial, while for other queries we could benefit more by selecting terms from a general encyclopedia. EEM allows for query-dependent weighing of the external collections. We put our model to the test on the task of blog post retrieval and we use four external collections in our experiments: (i) a news collection, (ii) a Web collection, (iii) Wikipedia, and (iv) a blog post collection. Experiments show that EEM outperforms query expansion on the individual collections, as well as the Mixture of Relevance Models that was previously proposed by Diaz and Metzler [2006]. Extensive analysis of the results shows that our naive approach to estimating query-dependent collection importance works reasonably well and that, when we use “oracle” settings, we see the full potential of our model. We also find that the query-dependent collection importance has more impact on retrieval performance than the independent collection importance (i.e., a collection prior).",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2159676500",
    "type": "article"
  },
  {
    "title": "Control-Flow Patterns for Decentralized RESTful Service Composition",
    "doi": "https://doi.org/10.1145/2535911",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Jesús Bellido; Rosa Alarcón; Cesare Pautasso",
    "corresponding_authors": "",
    "abstract": "The REST architectural style has attracted a lot of interest from industry due to the nonfunctional properties it contributes to Web-based solutions. SOAP/WSDL-based services, on the other hand, provide tools and methodologies that allow the design and development of software supporting complex service arrangements, enabling complex business processes which make use of well-known control-flow patterns. It is not clear if and how such patterns should be modeled, considering RESTful Web services that comply with the statelessness, uniform interface and hypermedia constraints. In this article, we analyze a set of fundamental control-flow patterns in the context of stateless compositions of RESTful services. We propose a means of enabling their implementation using the HTTP protocol and discuss the impact of our design choices according to key REST architectural principles. We hope to shed new light on the design of basic building blocks for RESTful business processes.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2143994368",
    "type": "article"
  },
  {
    "title": "Foundations of Trust and Distrust in Networks",
    "doi": "https://doi.org/10.1145/2628438",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Yi Qian; Sibel Adalı",
    "corresponding_authors": "",
    "abstract": "Modeling trust in very large social networks is a hard problem due to the highly noisy nature of these networks that span trust relationships from many different contexts, based on judgments of reliability, dependability, and competence. Furthermore, relationships in these networks vary in their level of strength. In this article, we introduce a novel extension of structural balance theory as a foundational theory of trust and distrust in networks. Our theory preserves the distinctions between trust and distrust as suggested in the literature, but also incorporates the notion of relationship strength that can be expressed as either discrete categorical values, as pairwise comparisons, or as metric distances. Our model is novel, has sound social and psychological basis, and captures the classical balance theory as a special case. We then propose a convergence model, describing how an imbalanced network evolves towards new balance, and formulate the convergence problem of a social network as a Metric Multidimensional Scaling (MDS) optimization problem. Finally, we show how the convergence model can be used to predict edge signs in social networks and justify our theory through extensive experiments on real datasets.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1983504099",
    "type": "article"
  },
  {
    "title": "Completeness Management for RDF Data Sources",
    "doi": "https://doi.org/10.1145/3196248",
    "publication_date": "2018-07-17",
    "publication_year": 2018,
    "authors": "Fariz Darari; Werner Nutt; Giuseppe Pirrò; Simon Razniewski",
    "corresponding_authors": "",
    "abstract": "The Semantic Web is commonly interpreted under the open-world assumption, meaning that information available (e.g., in a data source) captures only a subset of the reality. Therefore, there is no certainty about whether the available information provides a complete representation of the reality. The broad aim of this article is to contribute a formal study of how to describe the completeness of parts of the Semantic Web stored in RDF data sources. We introduce a theoretical framework allowing augmentation of RDF data sources with statements, also expressed in RDF, about their completeness. One immediate benefit of this framework is that now query answers can be complemented with information about their completeness. We study the impact of completeness statements on the complexity of query answering by considering different fragments of the SPARQL language, including the RDFS entailment regime, and the federated scenario. We implement an efficient method for reasoning about query completeness and provide an experimental evaluation in the presence of large sets of completeness statements.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2884555938",
    "type": "article"
  },
  {
    "title": "Social Networks under Stress",
    "doi": "https://doi.org/10.1145/3295460",
    "publication_date": "2019-02-08",
    "publication_year": 2019,
    "authors": "Daniel M. Romero; Brian Uzzi; Jon Kleinberg",
    "corresponding_authors": "",
    "abstract": "Social network research has begun to take advantage of fine-grained communications regarding coordination, decision-making, and knowledge sharing. These studies, however, have not generally analyzed how external events are associated with a social network’s structure and communicative properties. Here, we study how external events are associated with a network’s change in structure and communications. Analyzing a complete dataset of millions of instant messages among the decision-makers with different roles in a large hedge fund and their network of outside contacts, we investigate the link between price shocks, network structure, and change in the affect and cognition of decision-makers embedded in the network. We also analyze the communication dynamics among specialized teams in the organization. When price shocks occur the communication network tends not to display structural changes associated with adaptiveness such as the activation of weak ties to obtain novel information. Rather, the network “turtles up.” It displays a propensity for higher clustering, strong tie interaction, and an intensification of insider vs. outsider and within-role vs. between-role communication. Further, we find changes in network structure predict shifts in cognitive and affective processes, execution of new transactions, and local optimality of transactions better than prices, revealing the important predictive relationship between network structure and collective behavior within a social network.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2915050900",
    "type": "article"
  },
  {
    "title": "Flexible Construction of Executable Service Compositions from Reusable Semantic Knowledge",
    "doi": "https://doi.org/10.1145/2842628",
    "publication_date": "2016-02-08",
    "publication_year": 2016,
    "authors": "Rik Eshuis; Freddy Lécué; Nikolay Mehandjiev",
    "corresponding_authors": "",
    "abstract": "Most service composition approaches rely on top-down decomposition of a problem and AI-style planning to assemble service components into a meaningful whole, impeding reuse and flexibility. In this article, we propose an approach that starts from declarative knowledge about the semantics of individual service components and algorithmically constructs a full-blown service orchestration process that supports sequence, choice, and parallelism. The output of our algorithm can be mapped directly into a number of service orchestration languages such as OWL-S and BPEL. The approach consists of two steps. First, semantic links specifying data dependencies among the services are derived and organized in a flexible network. Second, based on a user request indicating the desired outcomes from the composition, an executable composition is constructed from the network that satisfies the dependencies. The approach is unique in producing complex compositions out of semantic links between services in a flexible way. It also allows reusing knowledge about semantic dependencies in the network to generate new compositions through new requests and modification of services at runtime. The approach has been implemented in a prototype that outperforms related composition prototypes in experiments.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2300402510",
    "type": "article"
  },
  {
    "title": "Scalable and Efficient Web Search Result Diversification",
    "doi": "https://doi.org/10.1145/2907948",
    "publication_date": "2016-08-16",
    "publication_year": 2016,
    "authors": "Kaweh Djafari Naini; İsmail Sengör Altıngövde; Wolf Siberski",
    "corresponding_authors": "",
    "abstract": "It has been shown that top- k retrieval quality can be considerably improved by taking not only relevance but also diversity into account. However, currently proposed diversification approaches have not put much attention on practical usability in large-scale settings, such as modern web search systems. In this work, we make two contributions toward this goal. First, we propose a combination of optimizations and heuristics for an implicit diversification algorithm based on the desirable facility placement principle, and present two algorithms that achieve linear complexity without compromising the retrieval effectiveness. Instead of an exhaustive comparison of documents, these algorithms first perform a clustering phase and then exploit its outcome to compose the diverse result set. Second, we describe and analyze two variants for distributed diversification in a computing cluster, for large-scale IR where the document collection is too large to keep in one node. Our contribution in this direction is pioneering, as there exists no earlier work in the literature that investigates the effectiveness and efficiency of diversification on a distributed setup. Extensive evaluations on a standard TREC framework demonstrate a competitive retrieval quality of the proposed optimizations to the baseline algorithm while reducing the processing time by more than 80% and up to 97%, and shed light on the efficiency and effectiveness tradeoffs of diversification when applied on top of a distributed architecture.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2518331571",
    "type": "article"
  },
  {
    "title": "Nonlinear Dynamics of Information Diffusion in Social Networks",
    "doi": "https://doi.org/10.1145/3057741",
    "publication_date": "2017-04-24",
    "publication_year": 2017,
    "authors": "Yasuko Matsubara; Yasushi Sakurai; B. Aditya Prakash; Lei Li; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated the faster propagation of news and rumors. How quickly does a piece of news spread over these media? How does its popularity diminish over time? Does the rising and falling pattern follow a simple universal law? In this article, we propose S pike M, a concise yet flexible analytical model of the rise and fall patterns of information diffusion. Our model has the following advantages. First, unification power: it explains earlier empirical observations and generalizes theoretical models including the SI and SIR models. We provide the threshold of the take-off versus die-out conditions for S pike M and discuss the generality of our model by applying it to an arbitrary graph topology. Second, practicality: it matches the observed behavior of diverse sets of real data. Third, parsimony: it requires only a handful of parameters. Fourth, usefulness: it makes it possible to perform analytic tasks such as forecasting, spotting anomalies, and interpretation by reverse engineering the system parameters of interest (quality of news, number of interested bloggers, etc.). We also introduce an efficient and effective algorithm for the real-time monitoring of information diffusion, namely S pike S tream , which identifies multiple diffusion patterns in a large collection of online event streams. Extensive experiments on real datasets demonstrate that S pike M accurately and succinctly describes all patterns of the rise and fall spikes in social networks.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2609338674",
    "type": "article"
  },
  {
    "title": "Analyzing Genetic Testing Discourse on the Web Through the Lens of Twitter, Reddit, and 4chan",
    "doi": "https://doi.org/10.1145/3404994",
    "publication_date": "2020-08-25",
    "publication_year": 2020,
    "authors": "Alexandros Mittos; Savvas Zannettou; Jeremy Blackburn; Emiliano De Cristofaro",
    "corresponding_authors": "",
    "abstract": "Recent progress in genomics has enabled the emergence of a flourishing market for direct-to-consumer (DTC) genetic testing. Companies like 23andMe and AncestryDNA provide affordable health, genealogy, and ancestry reports, and have already tested tens of millions of customers. Consequently, news, experiences, and views on genetic testing are increasingly shared and discussed on social media. At the same time, far-right groups have also taken an interest in genetic testing, using them to attack minorities and prove their genetic “purity.” In this article, we set to study the genetic testing discourse on a number of mainstream and fringe Web communities. We do so in two steps. First, we conduct an exploratory, large-scale analysis of the genetic testing discourse on a mainstream social network such as Twitter. We find that the genetic testing discourse is fueled by accounts that appear to be interested in digital health and technology. However, we also identify tweets with highly racist connotations. This motivates us to explore the connection between genetic testing and racism on platforms with a reputation for toxicity, namely, Reddit and 4chan, where we find that discussions around genetic testing often include highly toxic language expressed through hateful and racist comments. In particular, on 4chan’s politically incorrect board (/pol/), content from genetic testing conversations involves several alt-right personalities and openly anti-semitic rhetoric, often conveyed through memes.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3080248337",
    "type": "article"
  },
  {
    "title": "You, the Web, and Your Device",
    "doi": "https://doi.org/10.1145/3231466",
    "publication_date": "2018-09-27",
    "publication_year": 2018,
    "authors": "Luca Vassio; Idílio Drago; Marco Mellia; Zied Ben Houidi; Mohamed Lamine Lamali",
    "corresponding_authors": "",
    "abstract": "Understanding how people interact with the web is key for a variety of applications, e.g., from the design of effective web pages to the definition of successful online marketing campaigns. Browsing behavior has been traditionally represented and studied by means of clickstreams, i.e., graphs whose vertices are web pages, and edges are the paths followed by users. Obtaining large and representative data to extract clickstreams is however challenging. The evolution of the web questions whether browsing behavior is changing and, by consequence, whether properties of clickstreams are changing. This paper presents a longitudinal study of clickstreams in from 2013 to 2016. We evaluate an anonymized dataset of HTTP traces captured in a large ISP, where thousands of households are connected. We first propose a methodology to identify actual URLs requested by users from the massive set of requests automatically fired by browsers when rendering web pages. Then, we characterize web usage patterns and clickstreams, taking into account both the temporal evolution and the impact of the device used to explore the web. Our analyses precisely quantify various aspects of clickstreams and uncover interesting patterns, such as the typical short paths followed by people while navigating the web, the fast increasing trend in browsing from mobile devices and the different roles of search engines and social networks in promoting content. Finally, we contribute a dataset of anonymized clickstreams to the community to foster new studies (anonymized clickstreams are available to the public at http://bigdata.polito.it/clickstream).",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4289303095",
    "type": "article"
  },
  {
    "title": "Enhancing Conversational Recommendation Systems with Representation Fusion",
    "doi": "https://doi.org/10.1145/3577034",
    "publication_date": "2023-01-19",
    "publication_year": 2023,
    "authors": "Yingxu Wang; Xiaoru Chen; Jinyuan Fang; Zaiqiao Meng; Shangsong Liang",
    "corresponding_authors": "",
    "abstract": "Conversational Recommendation Systems (CRSs) aim to improve recommendation performance by utilizing information from a conversation session. A CRS first constructs questions and then asks users for their feedback in each conversation session to refine better recommendation lists to users. The key design of CRS is to construct proper questions and obtain users’ feedback in response to these questions so as to effectively capture user preferences. Many CRS works have been proposed; however, they suffer from defects when constructing questions for users to answer: (1) employing a dialogue policy agent for constructing questions is one of the most common choices in CRS, but it needs to be trained with a huge corpus, and (2) it is not appropriate that constructing questions from a single policy (e.g., a CRS only selects attributes that the user has interacted with) for all users with different preferences. To address these defects, we propose a novel CRS model, namely a Representation Fusion–based Conversational Recommendation model, where the whole conversation session is divided into two subsessions (i.e., Local Question Search subsession and Global Question Search subsession) and two different question search methods are proposed to construct questions in the corresponding subsessions without employing policy agents. In particular, in the Local Question Search subsession we adopt a novel graph mining method to find questions, where the paths in the graph between users and attributes can eliminate irrelevant attributes; in the Global Question Search subsession we propose to initialize user preference on items with the user and all item historical rating records and construct questions based on user’s preference. Then, we update the embeddings independently over the two subsessions according to user’s feedback and fuse the final embeddings from the two subsessions for the recommendation. Experiments on three real-world recommendation datasets demonstrate that our proposed method outperforms five state-of-the-art baselines.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4317434226",
    "type": "article"
  },
  {
    "title": "<scp>GroupAligner</scp> : A Deep Reinforcement Learning with Domain Adaptation for Social Group Alignment",
    "doi": "https://doi.org/10.1145/3580509",
    "publication_date": "2023-01-24",
    "publication_year": 2023,
    "authors": "Li Sun; Yang Du; Shuai Gao; Junda Ye; Feiyang Wang; Fuxin Ren; Mingchen Liang; Yue Wang; Shuhai Wang",
    "corresponding_authors": "",
    "abstract": "Social network alignment, which aims to uncover the correspondence across different social networks, shows fundamental importance in a wide spectrum of applications such as cross-domain recommendation and information propagation. In the literature, the vast majority of the existing studies focus on the social network alignment at user level. In practice, the user-level alignment usually relies on abundant personal information and high-quality supervision, which is expensive and even impossible in the real-world scenario. Alternatively, we propose to study the problem of social group alignment across different social networks, focusing on the interests of social groups rather than personal information. However, social group alignment is non-trivial and faces significant challenges in both (i) feature inconsistency across different social networks and (ii) group discovery within a social network. To bridge this gap, we present a novel GroupAligner , a deep reinforcement learning with domain adaptation for social group alignment. In GroupAligner , to address the first issue, we propose the cycle domain adaptation approach with the Wasserstein distance to transfer the knowledge from the source social network, aligning the feature space of social networks in the distribution level. To address the second issue, we model the group discovery as a sequential decision process with reinforcement learning in which the policy is parameterized by a proposed p roximity-enhanced G raph N eural N etwork (pGNN) and a GNN-based discriminator to score the reward. Finally, we utilize pre-training and teacher forcing to stabilize the learning process of GroupAligner . Extensive experiments on several real-world datasets are conducted to evaluate GroupAligner , and experimental results show that GroupAligner outperforms the alternative methods for social group alignment.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4317884041",
    "type": "article"
  },
  {
    "title": "Constructing Spatio-Temporal Graphs for Face Forgery Detection",
    "doi": "https://doi.org/10.1145/3580512",
    "publication_date": "2023-01-30",
    "publication_year": 2023,
    "authors": "Zhihua Shang; Hongtao Xie; Lingyun Yu; Zheng-Jun Zha; Yongdong Zhang",
    "corresponding_authors": "",
    "abstract": "Recently, advanced development of facial manipulation techniques threatens web information security, thus, face forgery detection attracts a lot of attention. It is clear that both spatial and temporal information of facial videos contains the crucial manipulation traces, which are inevitably created during the generation process. However, most existing face forgery detectors only focus on the spatial artifacts or the temporal incoherence, and they are struggling to learn a significant and general kind of representations for manipulated facial videos. In this work, we propose to construct spatial-temporal graphs for fake videos to capture the spatial inconsistency and the temporal incoherence at the same time. To model the spatial-temporal relationship among the graph nodes, a novel forgery detector named Spatio-Temporal Graph Network (STGN) is proposed, which contains two kinds of graph-convolution-based units, the Spatial Relation Graph Unit (SRGU) and the Temporal Attention Graph Unit (TAGU). To exploit spatial information, the SRGU models the inconsistency between each pair of patches in the same frame, instead of focusing on the low-level local spatial artifacts which are vulnerable to samples created by unseen manipulation methods. And, the TAGU is proposed to model the long-distance temporal relation among the patches at the same spatial position in different frames with a graph attention mechanism based on the inter-node similarity. With the SRGU and the TAGU, our STGN can combine the discriminative power of spatial inconsistency and the generalization capacity of temporal incoherence for face forgery detection. Our STGN achieves state-of-the-art performances on several popular forgery detection datasets. Extensive experiments demonstrate both the superiority of our STGN on intra manipulation evaluation and the effectiveness for new sorts of face forgery videos on cross manipulation evaluation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4318477391",
    "type": "article"
  },
  {
    "title": "Deep Adaptive Graph Clustering via von Mises-Fisher Distributions",
    "doi": "https://doi.org/10.1145/3580521",
    "publication_date": "2023-01-31",
    "publication_year": 2023,
    "authors": "Pengfei Wang; Daqing Wu; Chong Chen; Kunpeng Liu; Yanjie Fu; Jianqiang Huang; Yuanchun Zhou; Jianfeng Zhan; Xian‐Sheng Hua",
    "corresponding_authors": "",
    "abstract": "Graph clustering has been a hot research topic and is widely used in many fields, such as community detection in social networks. Lots of works combining auto-encoder and graph neural networks have been applied to clustering tasks by utilizing node attributes and graph structure. These works usually assumed the inherent parameters (i.e., size and variance) of different clusters in the latent embedding space are homogeneous, and hence the assigned probability is monotonous over the Euclidean distance between node embeddings and centroids. Unfortunately, this assumption usually does not hold since the size and concentration of different clusters can be quite different, which limits the clustering accuracy. In addition, the node embeddings in deep graph clustering methods are usually L2 normalized so that it lies on the surface of a unit hyper-sphere. To solve this problem, we proposed D eep A daptive G raph C lustering via von Mises-Fisher distributions, namely DAGC. DAGC assumes the node embeddings H can be drawn from a von Mises-Fisher distribution and each cluster k is associated with cluster inherent parameters ρ k which includes cluster center μ and cluster cohesion degree κ. Then we adopt an EM-like approach (i.e., 𝒫( H | ρ ) and 𝒫( ρ | H ), respectively) to learn the embedding and cluster inherent parameters alternately. Specifically, with the node embeddings, we proposed to update the cluster centers in an attraction-repulsion manner to make the cluster centers more separable. And given the cluster inherent parameters, a likelihood-based loss is proposed to make node embeddings more concentrated around cluster centers. Thus, DAGC can simultaneously improve the intra-cluster compactness and inter-cluster heterogeneity. Finally, extensive experiments conducted on four benchmark datasets have demonstrated that the proposed DAGC consistently outperforms the state-of-the-art methods, especially on imbalanced datasets.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4318617442",
    "type": "article"
  },
  {
    "title": "OPERA: Harmonizing Task-Oriented Dialogs and Information Seeking Experience",
    "doi": "https://doi.org/10.1145/3623381",
    "publication_date": "2023-09-11",
    "publication_year": 2023,
    "authors": "Miaoran Li; Baolin Peng; Jianfeng Gao; Zhu Zhang",
    "corresponding_authors": "",
    "abstract": "Existing studies in conversational AI mostly treat task-oriented dialog (TOD) and question answering (QA) as separate tasks. Towards the goal of constructing a conversational agent that can complete user tasks and support information seeking, it is important to develop a system that can handle both TOD and QA with access to various external knowledge sources. In this work, we propose a new task, Open-Book TOD (OB-TOD), which combines TOD with QA and expands the external knowledge sources to include both explicit sources (e.g., the web) and implicit sources (e.g., pre-trained language models). We create a new dataset OB-MultiWOZ, where we enrich TOD sessions with QA-like information-seeking experience grounded on external knowledge. We propose a unified model OPERA ( Op en-book E nd-to-end Task-o r iented Di a log) which can appropriately access explicit and implicit external knowledge to tackle the OB-TOD task. Experimental results show that OPERA outperforms closed-book baselines, highlighting the value of both types of knowledge. 1",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4386607510",
    "type": "article"
  },
  {
    "title": "Modeling web quality using a probabilistic approach",
    "doi": "https://doi.org/10.1145/1806916.1806918",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Ghazwa Malak; Houari Sahraoui; Linda Badri; Mourad Badri",
    "corresponding_authors": "",
    "abstract": "Web-based applications are software systems that continuously evolve to meet users' needs and to adapt to new technologies. Assuring their quality is then a difficult, but essential task. In fact, a large number of factors can affect their quality. Considering these factors and their interaction involves managing uncertainty and subjectivity inherent to this kind of applications. In this article, we present a probabilistic approach for building Web quality models and the associated assessment method. The proposed approach is based on Bayesian Networks. A model is built following a four-step process consisting in collecting quality characteristics, refining them, building a model structure, and deriving the model parameters. The feasibility of the approach is illustrated on the important quality characteristic of Navigability design . To validate the produced model, we conducted an experimental study with 20 subjects and 40 web pages. The results obtained show that the scores given by the used model are strongly correlated with navigability as perceived and experienced by the users.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2088585190",
    "type": "article"
  },
  {
    "title": "Using Interaction Data to Explain Difficulty Navigating Online",
    "doi": "https://doi.org/10.1145/2656343",
    "publication_date": "2014-11-06",
    "publication_year": 2014,
    "authors": "Paul Thomas",
    "corresponding_authors": "Paul Thomas",
    "abstract": "A user's behaviour when browsing a Web site contains clues to that user's experience. It is possible to record some of these behaviours automatically, and extract signals that indicate a user is having trouble finding information. This allows for Web site analytics based on user experiences, not just page impressions. A series of experiments identified user browsing behaviours—such as time taken and amount of scrolling up a page—which predict navigation difficulty and which can be recorded with minimal or no changes to existing sites or browsers. In turn, patterns of page views correlate with these signals and these patterns can help Web authors understand where and why their sites are hard to navigate. A new software tool, “LATTE,” automates this analysis and makes it available to Web authors in the context of the site itself.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1967824422",
    "type": "article"
  },
  {
    "title": "Propagating Both Trust and Distrust with Target Differentiation for Combating Link-Based Web Spam",
    "doi": "https://doi.org/10.1145/2628440",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Xianchao Zhang; You Wang; Nan Mou; Wenxin Liang",
    "corresponding_authors": "",
    "abstract": "Semi-automatic anti-spam algorithms propagate either trust through links from a good seed set (e.g., TrustRank) or distrust through inverse links from a bad seed set (e.g., Anti-TrustRank) to the entire Web. These kinds of algorithms have shown their powers in combating link-based Web spam since they integrate both human judgement and machine intelligence. Nevertheless, there is still much space for improvement. One issue of most existing trust/distust propagation algorithms is that only trust or distrust is propagated and only a good seed set or a bad seed set is used. According to Wu et al. [2006a], a combined usage of both trust and distrust propagation can lead to better results, and an effective framework is needed to realize this insight. Another more serious issue of existing algorithms is that trust or distrust is propagated in nondifferential ways, that is, a page propagates its trust or distrust score uniformly to its neighbors, without considering whether each neighbor should be trusted or distrusted. Such kinds of blind propagating schemes are inconsistent with the original intention of trust/distrust propagation. However, it seems impossible to implement differential propagation if only trust or distrust is propagated. In this article, we take the view that each Web page has both a trustworthy side and an untrustworthy side, and we thusly assign two scores to each Web page: T-Rank, scoring the trustworthiness of the page, and D-Rank, scoring the untrustworthiness of the page. We then propose an integrated framework that propagates both trust and distrust. In the framework, the propagation of T-Rank/D-Rank is penalized by the target's current D-Rank/T-Rank. In other words, the propagation of T-Rank/D-Rank is decided by the target's current (generalized) probability of being trustworthy/untrustworthy; thus a page propagates more trust/distrust to a trustworthy/untrustworthy neighbor than to an untrustworthy/trustworthy neighbor. In this way, propagating both trust and distrust with target differentiation is implemented. We use T-Rank scores to realize spam demotion and D-Rank scores to accomplish spam detection. The proposed Trust-DistrustRank (TDR) algorithm regresses to TrustRank and Anti-TrustRank when the penalty factor is set to 1 and 0, respectively. Thus TDR could be seen as a combinatorial generalization of both TrustRank and Anti-TrustRank. TDR not only makes full use of both trust and distrust propagation, but also overcomes the disadvantages of both TrustRank and Anti-TrustRank. Experimental results on benchmark datasets show that TDR outperforms other semi-automatic anti-spam algorithms for both spam demotion and spam detection tasks under various criteria.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2090552478",
    "type": "article"
  },
  {
    "title": "Efficient Multiview Maintenance under Insertion in Huge Social Networks",
    "doi": "https://doi.org/10.1145/2541290",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Andrea Pugliese; Matthias Bröcheler; V. S. Subrahmanian; Michael Ovelgönne",
    "corresponding_authors": "",
    "abstract": "Applications to monitor various aspects of social networks are becoming increasingly popular. For instance, marketers want to look for semantic patterns relating to the content of tweets and Facebook posts relating to their products. Law enforcement agencies want to track behaviors involving potential criminals on the Internet by looking for certain patterns of behavior. Music companies want to track patterns of spread of illegal music. These applications allow multiple users to specify patterns of interest and monitor them in real time as new data gets added to the Web or to a social network. In this article we develop the concept of social network view servers in which all of these types of applications can be simultaneously monitored. The patterns of interest are expressed as views over an underlying graph or social network database. We show that a given set of views can be compiled in multiple possible ways to take advantage of common substructures and define the concept of an optimal merge . Though finding an optimal merge is shown to be NP-hard, we develop the AddView to find very good merges quickly. We develop a very fast MultiView algorithm that scalably and efficiently maintains multiple subgraph views when insertions are made to the social network database. We show that our algorithm is correct, study its complexity, and experimentally demonstrate that our algorithm can scalably handle updates to hundreds of views on 6 real-world social network databases with up to 540M edges.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2112122712",
    "type": "article"
  },
  {
    "title": "Cache-Based Query Processing for Search Engines",
    "doi": "https://doi.org/10.1145/2382616.2382617",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "B. Barla Cambazoğlu; İsmail Sengör Altıngövde; Rifat Ozcan; Özgür Ulusoy",
    "corresponding_authors": "",
    "abstract": "In practice, a search engine may fail to serve a query due to various reasons such as hardware/network failures, excessive query load, lack of matching documents, or service contract limitations (e.g., the query rate limits for third-party users of a search service). In this kind of scenarios, where the backend search system is unable to generate answers to queries, approximate answers can be generated by exploiting the previously computed query results available in the result cache of the search engine. In this work, we propose two alternative strategies to implement this cache-based query processing idea. The first strategy aggregates the results of similar queries that are previously cached in order to create synthetic results for new queries. The second strategy forms an inverted index over the textual information (i.e., query terms and result snippets) present in the result cache and uses this index to answer new queries. Both approaches achieve reasonable result qualities compared to processing queries with an inverted index built on the collection.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2143713672",
    "type": "article"
  },
  {
    "title": "Value and Misinformation in Collaborative Investing Platforms",
    "doi": "https://doi.org/10.1145/3027487",
    "publication_date": "2017-05-04",
    "publication_year": 2017,
    "authors": "Tianyi Wang; Gang Wang; Bolun Wang; Divya Sambasivan; Zengbin Zhang; Xing Li; Hai-Tao Zheng; Ben Y. Zhao",
    "corresponding_authors": "",
    "abstract": "It is often difficult to separate the highly capable “experts” from the average worker in crowdsourced systems. This is especially true for challenge application domains that require extensive domain knowledge. The problem of stock analysis is one such domain, where even the highly paid, well-educated domain experts are prone to make mistakes. As an extremely challenging problem space, the “wisdom of the crowds” property that many crowdsourced applications rely on may not hold. In this article, we study the problem of evaluating and identifying experts in the context of SeekingAlpha and StockTwits, two crowdsourced investment services that have recently begun to encroach on a space dominated for decades by large investment banks. We seek to understand the quality and impact of content on collaborative investment platforms, by empirically analyzing complete datasets of SeekingAlpha articles (9 years) and StockTwits messages (4 years). We develop sentiment analysis tools and correlate contributed content to the historical performance of relevant stocks. While SeekingAlpha articles and StockTwits messages provide minimal correlation to stock performance in aggregate, a subset of experts contribute more valuable (predictive) content. We show that these authors can be easily identified by user interactions, and investments based on their analysis significantly outperform broader markets. This effectively shows that even in challenging application domains, there is a secondary or indirect wisdom of the crowds. Finally, we conduct a user survey that sheds light on users’ views of SeekingAlpha content and stock manipulation. We also devote efforts to identify potential manipulation of stocks by detecting authors controlling multiple identities.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2612883338",
    "type": "article"
  },
  {
    "title": "ReputationPro",
    "doi": "https://doi.org/10.1145/2697390",
    "publication_date": "2015-01-23",
    "publication_year": 2015,
    "authors": "Haibin Zhang; Yan Wang; Xiuzhen Zhang; Ee‐Peng Lim",
    "corresponding_authors": "",
    "abstract": "In e-commerce environments, the trustworthiness of a seller is utterly important to potential buyers, especially when a seller is not known to them. Most existing trust evaluation models compute a single value to reflect the general trustworthiness of a seller without taking any transaction context information into account. With such a result as the indication of reputation, a buyer may be easily deceived by a malicious seller in a transaction where the notorious value imbalance problem is involved—in other words, a malicious seller accumulates a high-level reputation by selling cheap products and then deceives buyers by inducing them to purchase more expensive products. In this article, we first present a trust vector consisting of three values for contextual transaction trust (CTT). In the computation of CTT values, three identified important context dimensions , including Product Category, Transaction Amount, and Transaction Time, are taken into account. In the meantime, the computation of each CTT value is based on both past transactions and the forthcoming transaction. In particular, with different parameters specified by a buyer regarding context dimensions, different sets of CTT values can be calculated. As a result, all of these trust values can outline the reputation profile of a seller that indicates the dynamic trustworthiness of a seller in different products, product categories, price ranges, time periods, and any necessary combination of them. We name this new model ReputationPro . Nevertheless, in ReputationPro , the computation of reputation profile requires new data structures for appropriately indexing the precomputation of aggregates over large-scale ratings and transaction data in three context dimensions, as well as novel algorithms for promptly answering buyers’ CTT queries. In addition, storing precomputed aggregation results consumes a large volume of space, particularly for a system with millions of sellers. Therefore, reducing storage space for aggregation results is also a great demand. To solve these challenging problems, we first propose a new index scheme CMK-tree by extending the two-dimensional K-D-B-tree that indexes spatial data to support efficient computation of CTT values. Then, we further extend the CMK-tree and propose a CMK-tree RS approach to reducing the storage space allocated to each seller. The two approaches are not only applicable to three context dimensions that are either linear or hierarchical but also take into account the characteristics of the transaction-time model—that is, transaction data is inserted in chronological order. Moreover, the proposed data structures can index each specific product traded in a time period to compute the trustworthiness of a seller in selling a product. Finally, the experimental results illustrate that the CMK-tree is superior in efficiency of computing CTT values to all three existing approaches in the literature. In particular, while answering a buyer’s CTT queries for each brand-based product category, the CMK-tree has almost linear query performance. In addition, with significantly reduced storage space, the CMK-tree RS approach can further improve the efficiency in computing CTT values. Therefore, our proposed ReputationPro model is scalable to large-scale e-commerce Web sites in terms of efficiency and storage space consumption.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1963673633",
    "type": "article"
  },
  {
    "title": "A Comprehensive Study of Techniques for URL-Based Web Page Language Classification",
    "doi": "https://doi.org/10.1145/2435215.2435218",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Eda Baykan; Monika Henzinger; Ingmar Weber",
    "corresponding_authors": "",
    "abstract": "Given only the URL of a Web page, can we identify its language? In this article we examine this question. URL-based language classification is useful when the content of the Web page is not available or downloading the content is a waste of bandwidth and time. We built URL-based language classifiers for English, German, French, Spanish, and Italian by applying a variety of algorithms and features. As algorithms we used machine learning algorithms which are widely applied for text classification and state-of-art algorithms for language identification of text. As features we used words, various sized n-grams, and custom-made features (our novel feature set). We compared our approaches with two baseline methods, namely classification by country code top-level domains and classification by IP addresses of the hosting Web servers. We trained and tested our classifiers in a 10-fold cross-validation setup on a dataset obtained from the Open Directory Project and from querying a commercial search engine. We obtained the lowest F1-measure for English (94) and the highest F1-measure for German (98) with the best performing classifiers. We also evaluated the performance of our methods: (i) on a set of Web pages written in Adobe Flash and (ii) as part of a language-focused crawler. In the first case, the content of the Web page is hard to extract and in the second page downloading pages of the “wrong” language constitutes a waste of bandwidth. In both settings the best classifiers have a high accuracy with an F1-measure between 95 (for English) and 98 (for Italian) for the Adobe Flash pages and a precision between 90 (for Italian) and 97 (for French) for the language-focused crawler.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1984868580",
    "type": "article"
  },
  {
    "title": "EXIP",
    "doi": "https://doi.org/10.1145/2665068",
    "publication_date": "2014-11-06",
    "publication_year": 2014,
    "authors": "Rumen Kyusakov; Pablo Puñal Pereira; Jens Eliasson; Jerker Delsing",
    "corresponding_authors": "",
    "abstract": "Developing and deploying Web applications on networked embedded devices is often seen as a way to reduce the development cost and time to market for new target platforms. However, the size of the messages and the processing requirements of today's Web protocols, such as HTTP and XML, are challenging for the most resource-constrained class of devices that could also benefit from Web connectivity. New Web protocols using binary representations have been proposed for addressing this issue. Constrained Application Protocol (CoAP) reduces the bandwidth and processing requirements compared to HTTP while preserving the core concepts of the Web architecture. Similarly, Efficient XML Interchange (EXI) format has been standardized for reducing the size and processing time for XML structured information. Nevertheless, the adoption of these technologies is lagging behind due to lack of support from Web browsers and current Web development toolkits. Motivated by these problems, this article presents the design and implementation techniques for the EXIP framework for embedded Web development. The framework consists of a highly efficient EXI processor, a tool for EXI data binding based on templates, and a CoAP/EXI/XHTML Web page engine. A prototype implementation of the EXI processor is herein presented and evaluated. It can be applied to Web browsers or thin server platforms using XHTML and Web services for supporting human-machine interactions in the Internet of Things. This article contains four major results: (1) theoretical and practical evaluation of the use of binary protocols for embedded Web programming; (2) a novel method for generation of EXI grammars based on XML Schema definitions; (3) an algorithm for grammar concatenation that produces normalized EXI grammars directly, and hence reduces the number of iterations during grammar generation; (4) an algorithm for efficient representation of possible deviations from the XML schema.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2046431637",
    "type": "article"
  },
  {
    "title": "A Buyer-Friendly and Mediated Watermarking Protocol for Web Context",
    "doi": "https://doi.org/10.1145/2856036",
    "publication_date": "2016-04-29",
    "publication_year": 2016,
    "authors": "Franco Frattolillo",
    "corresponding_authors": "Franco Frattolillo",
    "abstract": "Watermarking protocols are used in conjunction with digital watermarking techniques to protect digital copyright on the Internet. They define the schemes of the web transactions by which buyers can purchase protected digital content distributed by content providers in a secure manner. Over the last few years, significant examples of watermarking protocols have been proposed in literature. However, a detailed examination of such protocols has revealed a number of problems that have to be addressed in order to make them suited for current web context. Therefore, based on the most relevant problems derived from literature, this article identifies the main challenges posed by the development of watermarking protocols for web context and presents a watermarking protocol that follows a new secure, buyer-centric and mediated design approach able to meet such challenges.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2345938683",
    "type": "article"
  },
  {
    "title": "Manipulation among the Arbiters of Collective Intelligence",
    "doi": "https://doi.org/10.1145/3001937",
    "publication_date": "2016-12-24",
    "publication_year": 2016,
    "authors": "Sanmay Das; Allen Lavoie; Malik Magdon‐Ismail",
    "corresponding_authors": "",
    "abstract": "Our reliance on networked, collectively built information is a vulnerability when the quality or reliability of this information is poor. Wikipedia, one such collectively built information source, is often our first stop for information on all kinds of topics; its quality has stood up to many tests, and it prides itself on having a “neutral point of view.” Enforcement of neutrality is in the hands of comparatively few, powerful administrators. In this article, we document that a surprisingly large number of editors change their behavior and begin focusing more on a particular controversial topic once they are promoted to administrator status. The conscious and unconscious biases of these few, but powerful, administrators may be shaping the information on many of the most sensitive topics on Wikipedia; some may even be explicitly infiltrating the ranks of administrators in order to promote their own points of view. In addition, we ask whether administrators who change their behavior in this suspicious manner can be identified in advance. Neither prior history nor vote counts during an administrator’s election are useful in doing so, but we find that an alternative measure, which gives more weight to influential voters, can successfully reject these suspicious candidates. This second result has important implications for how we harness collective intelligence: even if wisdom exists in a collective opinion (like a vote), that signal can be lost unless we carefully distinguish the true expert voter from the noisy or manipulative voter.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2563178664",
    "type": "article"
  },
  {
    "title": "Learning Linear Influence Models in Social Networks from Transient Opinion Dynamics",
    "doi": "https://doi.org/10.1145/3343483",
    "publication_date": "2019-08-31",
    "publication_year": 2019,
    "authors": "Abir De; Sourangshu Bhattacharya; Parantapa Bhattacharya; Niloy Ganguly; Soumen Chakrabarti",
    "corresponding_authors": "",
    "abstract": "Social networks, forums, and social media have emerged as global platforms for forming and shaping opinions on a broad spectrum of topics like politics, sports, and entertainment. Users (also called actors ) often update their evolving opinions, influenced through discussions with other users. Theoretical models and their analysis on understanding opinion dynamics in social networks abound in the literature. However, these models are often based on concepts from statistical physics. Their goal is to establish specific phenomena like steady state consensus or bifurcation. Analysis of transient effects is largely avoided. Moreover, many of these studies assume that actors’ opinions are observed globally and synchronously, which is rarely realistic. In this article, we initiate an investigation into a family of novel data-driven influence models that accurately learn and fit realistic observations. We estimate and do not presume edge strengths from observed opinions at nodes. Our influence models are linear but not necessarily positive or row stochastic in nature. As a consequence, unlike the previous studies, they do not depend on system stability or convergence during the observation period. Furthermore, our models take into account a wide variety of data collection scenarios. In particular, they are robust to missing observations for several timesteps after an actor has changed its opinion. In addition, we consider scenarios where opinion observations may be available only for aggregated clusters of nodes—a practical restriction often imposed to ensure privacy. Finally, to provide a conceptually interpretable design of edge influence, we offer a relatively frugal variant of our influence model, where the strength of influence between two connecting nodes depends on the node attributes (demography, personality, expertise, etc.). Such an approach reduces the number of model parameters, reduces overfitting, and offers a tractable and explicable sketch of edge influences in the context of opinion dynamics. With six real-life datasets crawled from Twitter and Reddit, as well as three more datasets collected from in-house experiments (with 102 volunteers), our proposed system gives a significant accuracy boost over four state-of-the-art baselines.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2987074173",
    "type": "article"
  },
  {
    "title": "Sentiment-Focused Web Crawling",
    "doi": "https://doi.org/10.1145/2644821",
    "publication_date": "2014-11-06",
    "publication_year": 2014,
    "authors": "A. Gural Vural; B. Barla Cambazoğlu; Pınar Karagöz",
    "corresponding_authors": "",
    "abstract": "Sentiments and opinions expressed in Web pages towards objects, entities, and products constitute an important portion of the textual content available in the Web. In the last decade, the analysis of such content has gained importance due to its high potential for monetization. Despite the vast interest in sentiment analysis, somewhat surprisingly, the discovery of sentimental or opinionated Web content is mostly ignored. This work aims to fill this gap and addresses the problem of quickly discovering and fetching the sentimental content present in the Web. To this end, we design a sentiment-focused Web crawling framework. In particular, we propose different sentiment-focused Web crawling strategies that prioritize discovered URLs based on their predicted sentiment scores. Through simulations, these strategies are shown to achieve considerable performance improvement over general-purpose Web crawling strategies in discovery of sentimental Web content.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2013540829",
    "type": "article"
  },
  {
    "title": "Activity Dynamics in Collaboration Networks",
    "doi": "https://doi.org/10.1145/2873060",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Simon Walk; Denis Helić; Florian Geigl; Markus Strohmaier",
    "corresponding_authors": "",
    "abstract": "Many online collaboration networks struggle to gain user activity and become self-sustaining due to the ramp-up problem or dwindling activity within the system. Prominent examples include online encyclopedias such as (Semantic) MediaWikis, Question and Answering portals such as StackOverflow, and many others. Only a small fraction of these systems manage to reach self-sustaining activity, a level of activity that prevents the system from reverting to a nonactive state. In this article, we model and analyze activity dynamics in synthetic and empirical collaboration networks. Our approach is based on two opposing and well-studied principles: (i) without incentives, users tend to lose interest to contribute and thus, systems become inactive, and (ii) people are susceptible to actions taken by their peers (social or peer influence). With the activity dynamics model that we introduce in this article we can represent typical situations of such collaboration networks. For example, activity in a collaborative network, without external impulses or investments, will vanish over time, eventually rendering the system inactive. However, by appropriately manipulating the activity dynamics and/or the underlying collaboration networks, we can jump-start a previously inactive system and advance it toward an active state. To be able to do so, we first describe our model and its underlying mechanisms. We then provide illustrative examples of empirical datasets and characterize the barrier that has to be breached by a system before it can become self-sustaining in terms of critical mass and activity dynamics. Additionally, we expand on this empirical illustration and introduce a new metric p —the Activity Momentum —to assess the activity robustness of collaboration networks.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W229275971",
    "type": "article"
  },
  {
    "title": "Review Summary Generation in Online Systems: Frameworks for Supervised and Unsupervised Scenarios",
    "doi": "https://doi.org/10.1145/3448015",
    "publication_date": "2021-05-13",
    "publication_year": 2021,
    "authors": "Wenjun Jiang; Jing Chen; Xiaofei Ding; Jie Wu; Jiawei He; Guojun Wang",
    "corresponding_authors": "",
    "abstract": "In online systems, including e-commerce platforms, many users resort to the reviews or comments generated by previous consumers for decision making, while their time is limited to deal with many reviews. Therefore, a review summary, which contains all important features in user-generated reviews, is expected. In this article, we study “how to generate a comprehensive review summary from a large number of user-generated reviews.” This can be implemented by text summarization, which mainly has two types of extractive and abstractive approaches. Both of these approaches can deal with both supervised and unsupervised scenarios, but the former may generate redundant and incoherent summaries, while the latter can avoid redundancy but usually can only deal with short sequences. Moreover, both approaches may neglect the sentiment information. To address the above issues, we propose comprehensive Review Summary Generation frameworks to deal with the supervised and unsupervised scenarios. We design two different preprocess models of re-ranking and selecting to identify the important sentences while keeping users’ sentiment in the original reviews. These sentences can be further used to generate review summaries with text summarization methods. Experimental results in seven real-world datasets (Idebate, Rotten Tomatoes Amazon, Yelp, and three unlabelled product review datasets in Amazon) demonstrate that our work performs well in review summary generation. Moreover, the re-ranking and selecting models show different characteristics.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3160369148",
    "type": "article"
  },
  {
    "title": "A User-Centric Analysis of Social Media for Stock Market Prediction",
    "doi": "https://doi.org/10.1145/3532856",
    "publication_date": "2022-09-29",
    "publication_year": 2022,
    "authors": "Mohamed Reda Bouadjenek; Scott Sanner; Ga Wu",
    "corresponding_authors": "",
    "abstract": "Social media platforms such as Twitter or StockTwits are widely used for sharing stock market opinions between investors, traders, and entrepreneurs. Empirically, previous work has shown that the content posted on these social media platforms can be leveraged to predict various aspects of stock market performance. Nonetheless, actors on these social media platforms may not always have altruistic motivations and may instead seek to influence stock trading behavior through the (potentially misleading) information they post. While a lot of previous work has sought to analyze how social media can be used to predict the stock market, there remain many questions regarding the quality of the predictions and the behavior of active users on these platforms. To this end, this article seeks to address a number of open research questions: Which social media platform is more predictive of stock performance? What posted content is actually predictive, and over what time horizon? How does stock market posting behavior vary among different users? Are all users trustworthy or do some user’s predictions consistently mislead about the true stock movement? To answer these questions, we analyzed data from Twitter and StockTwits covering almost 5 years of posted messages spanning 2015 to 2019. The results of this large-scale study provide a number of important insights among which we present the following: (i) StockTwits is a more predictive source of information than Twitter, leading us to focus our analysis on StockTwits; (ii) on StockTwits, users’ self-labeled sentiments are correlated with the stock market but are only slightly predictive in aggregate over the short-term; (iii) there are at least three clear types of temporal predictive behavior for users over a 144 days horizon: short, medium, and long term; and (iv) consistently incorrect users who are reliably wrong tend to exhibit what we conjecture to be “botlike” post content and their removal from the data tends to improve stock market predictions from self-labeled content.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4297999768",
    "type": "article"
  },
  {
    "title": "A Dual-Channel Semi-Supervised Learning Framework on Graphs via Knowledge Transfer and Meta-Learning",
    "doi": "https://doi.org/10.1145/3577033",
    "publication_date": "2023-01-18",
    "publication_year": 2023,
    "authors": "Ziyue Qiao; Pengyang Wang; Pengfei Wang; Zhiyuan Ning; Yanjie Fu; Yi Du; Yuanchun Zhou; Jianqiang Huang; Xian‐Sheng Hua; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "This article studies the problem of semi-supervised learning on graphs, which aims to incorporate ubiquitous unlabeled knowledge (e.g., graph topology, node attributes) with few-available labeled knowledge (e.g., node class) to alleviate the scarcity issue of supervised information on node classification. While promising results are achieved, existing works for this problem usually suffer from the poor balance of generalization and fitting ability due to the heavy reliance on labels or task-agnostic unsupervised information. To address the challenge, we propose a dual-channel framework for semi-supervised learning on G raphs via K nowledge T ransfer between independent supervised and unsupervised embedding spaces, namely, GKT. Specifically, we devise a dual-channel framework including a supervised model for learning the label probability of nodes and an unsupervised model for extracting information from massive unlabeled graph data. A knowledge transfer head is proposed to bridge the gap between the generalization and fitting capability of the two models. We use the unsupervised information to reconstruct batch-graphs to smooth the label probability distribution on the graphs to improve the generalization of prediction. We also adaptively adjust the reconstructed graphs by encouraging the label-related connections to solidify the fitting ability. Since the optimization of the supervised channel with knowledge transfer contains that of the unsupervised channel as a constraint and vice versa, we then propose a meta-learning-based method to solve the bi-level optimization problem, which avoids the negative transfer and further improves the model’s performance. Finally, extensive experiments validate the effectiveness of our proposed framework by comparing state-of-the-art algorithms.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4317209735",
    "type": "article"
  },
  {
    "title": "BehaviorNet: A Fine-grained Behavior-aware Network for Dynamic Link Prediction",
    "doi": "https://doi.org/10.1145/3580514",
    "publication_date": "2023-01-19",
    "publication_year": 2023,
    "authors": "Mingyi Liu; Zhiying Tu; Tonghua Su; Xianzhi Wang; Xiaofei Xu; Zhongjie Wang",
    "corresponding_authors": "",
    "abstract": "Dynamic link prediction has become a trending research subject because of its wide applications in the web, sociology, transportation, and bioinformatics. Currently, the prevailing approach for dynamic link prediction is based on graph neural networks, in which graph representation learning is the key to perform dynamic link prediction tasks. However, there are still great challenges because the structure of graphs evolves over time. A common approach is to represent a dynamic graph as a collection of discrete snapshots, in which information over a period is aggregated through summation or averaging. This way results in some fine-grained time-related information loss, which further leads to a certain degree of performance degradation. We conjecture that such fine-grained information is vital because it implies specific behavior patterns of nodes and edges in a snapshot. To verify this conjecture, we propose a novel fine-grained behavior-aware network (BehaviorNet) for dynamic network link prediction. Specifically, BehaviorNet adapts a transformer-based graph convolution network to capture the latent structural representations of nodes by adding edge behaviors as an additional attribute of edges. GRU is applied to learn the temporal features of given snapshots of a dynamic network by utilizing node behaviors as auxiliary information. Extensive experiments are conducted on several real-world dynamic graph datasets, and the results show significant performance gains for BehaviorNet over several state-of-the-art (SOTA) discrete dynamic link prediction baselines. Ablation study validates the effectiveness of modeling fine-grained edge and node behaviors.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4317433757",
    "type": "article"
  },
  {
    "title": "Reverse Maximum Inner Product Search: Formulation, Algorithms, and Analysis",
    "doi": "https://doi.org/10.1145/3587215",
    "publication_date": "2023-03-16",
    "publication_year": 2023,
    "authors": "Daichi Amagata; Takahiro Hara",
    "corresponding_authors": "",
    "abstract": "The maximum inner product search (MIPS), which finds the item with the highest inner product with a given query user, is an essential problem in the recommendation field. Usually e-commerce companies face situations where they want to promote and sell new or discounted items. In these situations, we have to consider the following questions: Who is interested in the items, and how do we find them? This article answers this question by addressing a new problem called reverse maximum inner product search (reverse MIPS). Given a query vector and two sets of vectors (user vectors and item vectors), the problem of reverse MIPS finds a set of user vectors whose inner product with the query vector is the maximum among the query and item vectors. Although the importance of this problem is clear, its straightforward implementation incurs a computationally expensive cost. We therefore propose Simpfer, a simple, fast, and exact algorithm for reverse MIPS. In an offline phase, Simpfer builds a simple index that maintains a lower bound of the maximum inner product. By exploiting this index, Simpfer judges whether the query vector can have the maximum inner product or not, for a given user vector, in a constant time. Our index enables filtering user vectors, which cannot have the maximum inner product with the query vector, in a batch. We theoretically demonstrate that Simpfer outperforms baselines employing state-of-the-art MIPS techniques. In addition, we answer two new research questions. Can approximation algorithms further improve reverse MIPS processing? Is there an exact algorithm that is faster than Simpfer? For the former, we show that approximation with quality guarantee provides a little speed-up. For the latter, we propose Simpfer++, a theoretically and practically faster algorithm than Simpfer. Our extensive experiments on real datasets show that Simpfer is at least two orders of magnitude faster than the baselines, and Simpfer++ further improves the online processing time.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4327590745",
    "type": "article"
  },
  {
    "title": "Semantic Interaction Matching Network for Few-Shot Knowledge Graph Completion",
    "doi": "https://doi.org/10.1145/3589557",
    "publication_date": "2023-03-30",
    "publication_year": 2023,
    "authors": "Pengfei Luo; Xi Zhu; Tong Xu; Yi Zheng; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "The prosperity of knowledge graphs, as well as related downstream applications, has raised the urgent need for knowledge graph completion techniques that fully support knowledge graph reasoning tasks, especially under the circumstance of training data scarcity. Although large efforts have been made on solving this challenge via few-shot learning tools, they mainly focus on simply aggregating entity neighbors to represent few-shot references, whereas the enhancement from latent semantic correlation within neighbors has been largely ignored. To that end, in this article, we propose a novel few-shot learning solution named SIM, a S emantic I nteraction M atching network that applies a Transformer framework to enhance the entity representation with capturing semantic interaction between entity neighbors. Specifically, we first design an entity-relation fusion module to adaptively encode neighbors with incorporating relation representation. Along this line, Transformer layers are integrated to capture latent correlation within neighbors, as well as the semantic diversification of the support set. Finally, a similarity score is attentively estimated with the attention mechanism. Extensive experiments on two public benchmark datasets demonstrate that our model outperforms a variety of state-of-the-art methods by a significant margin.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4361271817",
    "type": "article"
  },
  {
    "title": "Causality and Correlation Graph Modeling for Effective and Explainable Session-Based Recommendation",
    "doi": "https://doi.org/10.1145/3593313",
    "publication_date": "2023-06-05",
    "publication_year": 2023,
    "authors": "Huizi Wu; Cong Geng; Hui Fang",
    "corresponding_authors": "",
    "abstract": "Session-based recommendation, which has witnessed a booming interest recently, focuses on predicting a user’s next interested item(s) based on an anonymous session. Most existing studies adopt complex deep learning techniques (e.g., graph neural networks) for effective session-based recommendation. However, they merely address co-occurrence between items, but fail to distinguish a causality and correlation relationship. Considering the varied interpretations and characteristics of causality and correlation relationships between items, in this study, we propose a novel method denoted as CGSR by jointly modeling causality and correlation relationships between items. In particular, we construct cause, effect, and correlation graphs from sessions by simultaneously considering the false causality problem. We further design a graph neural network–based method for session-based recommendation. To conclude, we strive to explore the relationship between items from specific “causality” (directed) and “correlation” (undirected) perspectives. Extensive experiments on three datasets show that our model outperforms other state-of-the-art methods in terms of recommendation accuracy. Moreover, we further propose an explainable framework on CGSR and demonstrate the explainability of our model via case studies on an Amazon dataset.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4379388591",
    "type": "article"
  },
  {
    "title": "Random Testing and Evolutionary Testing for Fuzzing GraphQL APIs",
    "doi": "https://doi.org/10.1145/3609427",
    "publication_date": "2023-08-09",
    "publication_year": 2023,
    "authors": "Asma Belhadi; Man Zhang; Andrea Arcuri",
    "corresponding_authors": "",
    "abstract": "The Graph Query Language (GraphQL) is a powerful language for application programming interface (API) manipulation in web services. It has been recently introduced as an alternative solution for addressing the limitations of RESTful APIs. This article introduces an automated solution for GraphQL API testing. We present a full framework for automated API testing, from the schema extraction to test case generation. In addition, we consider two kinds of testing: white-box and black-box testing. The white-box testing is performed when the source code of the GraphQL API is available. Our approach is based on evolutionary search. Test cases are evolved to intelligently explore the solution space while maximizing code coverage and fault-finding criteria. The black-box testing does not require access to the source code of the GraphQL API. It is therefore of more general applicability, albeit it has worse performance. In this context, we use a random search to generate GraphQL data. The proposed framework is implemented and integrated into the open source EvoMaster tool. With enabled white-box heuristics (i.e., white-box mode), experiments on 7 open source GraphQL APIs and three search algorithms show statistically significant improvement of the evolutionary approach compared to the baseline random search. In addition, experiments on 31 online GraphQL APIs reveal the ability of the black-box mode to detect real faults.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4385699319",
    "type": "article"
  },
  {
    "title": "Blockchain Takeovers in Web 3.0: An Empirical Study on the TRON-Steem Incident",
    "doi": "https://doi.org/10.1145/3689431",
    "publication_date": "2024-08-21",
    "publication_year": 2024,
    "authors": "Chao Li; Runhua Xu; Balaji Palanisamy; Li Duan; Meng Shen; Jiqiang Liu; Wei Wang",
    "corresponding_authors": "",
    "abstract": "A fundamental goal of Web 3.0 is to establish a decentralized network and application ecosystem, thereby enabling users to retain control over their data while promoting value exchange. However, the recent Tron-Steem takeover incident poses a significant threat to this vision. In this paper, we present a thorough empirical analysis of the Tron-Steem takeover incident. By conducting a fine-grained reconstruction of the stake and election snapshots within the Steem blockchain, one of the most prominent social-oriented blockchains, we quantify the marked shifts in decentralization pre and post the takeover incident, highlighting the severe threat that blockchain network takeovers pose to the decentralization principle of Web 3.0. Moreover, by employing heuristic methods to identify anomalous voters and conducting clustering analyses on voter behaviors, we unveil the underlying mechanics of takeover strategies employed in the Tron-Steem incident and suggest potential mitigation strategies, which contribute to the enhanced resistance of Web 3.0 networks against similar threats in the future. We believe the insights gleaned from this research help illuminate the challenges imposed by blockchain network takeovers in the Web 3.0 era, suggest ways to foster the development of decentralized technologies and governance, as well as to enhance the protection of Web 3.0 user rights.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401726396",
    "type": "article"
  },
  {
    "title": "Post-hoc Evaluation of Nodes Influence in Information Cascades: The Case of Coordinated Accounts",
    "doi": "https://doi.org/10.1145/3700644",
    "publication_date": "2024-10-17",
    "publication_year": 2024,
    "authors": "Niccolò Di Marco; Sara Brunetti; Matteo Cinelli; Walter Quattrociocchi",
    "corresponding_authors": "",
    "abstract": "In the last years, social media has gained an unprecedented amount of attention, playing a pivotal role in shaping the contemporary landscape of communication and connection. However, Coordinated inauthentic Behaviour (CIB), defined as orchestrated efforts by entities to deceive or mislead users about their identity and intentions, has emerged as a tactic to exploit the online discourse. In this study, we quantify the efficacy of CIB tactics by defining a general framework for evaluating the influence of a subset of nodes in a directed tree. We design two algorithms that provide optimal and greedy post-hoc placement strategies that lead to maximising the configuration influence. We then consider cascades from information spreading on Twitter to compare the observed behaviour with our algorithms. The results show that, according to our model, coordinated accounts are quite inefficient in terms of their network influence, thus suggesting that they may play a less pivotal role than expected. Moreover, the causes of these poor results may be found in two separate aspects: a bad placement strategy and a scarcity of resources.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4403490619",
    "type": "article"
  },
  {
    "title": "Pretending to be a VIP! Characterization and Detection of Fake and Clone Channels on Telegram",
    "doi": "https://doi.org/10.1145/3705014",
    "publication_date": "2024-11-26",
    "publication_year": 2024,
    "authors": "Massimo La Morgia; Alessandro Mei; Alberto Maria Mongardini; Jie Wu",
    "corresponding_authors": "",
    "abstract": "Telegram is a widely used instant messaging app that has gained popularity due to its high level of privacy protection. Telegram has standout social network features like channels, which are virtual rooms where only administrators can post and broadcast messages to all subscribers. However, these same features have also led to the emergence of problematic activities and a significant number of fake accounts. To address these issues, Telegram has introduced verified and scam marks for channels, but only a small number of official channels are currently marked as verified, and only a few fakes as scams. In this research, we conduct a large-scale analysis of Telegram by collecting data from 120,979 different public channels and over 247 million messages. We identify and analyze two types of channels: Clones and fakes. Clones are channels that publish identical content from another channel in order to gain subscribers and promote services. Fakes, on the other hand, are channels that impersonate celebrities or well-known services by posting their own messages. To automatically detect fake channels, we propose a machine learning model that achieves an F1-score of 85.45%. By applying this model to our dataset, we find the main targets of fakes are political figures, well-known people such as actors or singers, and services.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4404721958",
    "type": "article"
  },
  {
    "title": "Leveraging popular destinations to enhance Web search interaction",
    "doi": "https://doi.org/10.1145/1377488.1377490",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Ryen W. White; Mikhail Bilenko; Silviu Cucerzan",
    "corresponding_authors": "",
    "abstract": "This article presents a novel Web search interaction feature that for a given query provides links to Web sites frequently visited by other users with similar information needs. These popular destinations complement traditional search results, allowing direct navigation to authoritative resources for the query topic. Destinations are identified using the history of the search and browsing behavior of many users over an extended time period, and their collective behavior provides a basis for computing source authority. They are drawn from the end of users' postquery browse trails where users may cease searching once they find relevant information. We describe a user study that compared the suggestion of destinations with the previously proposed suggestion of related queries as well as with traditional, unaided Web search. Results show that search enhanced by query suggestions outperforms other systems in terms of subject perceptions and search effectiveness for fact-finding search tasks. However, search enhanced by destination suggestions performs best for exploratory tasks with its best performance obtained from mining past user behavior at query-level granularity. We discuss the implications of these and other findings from our study for the design of search systems that utilize user behavior, in particular, user browse trails and popular destinations.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2110089207",
    "type": "article"
  },
  {
    "title": "Privacy-preserving query log mining for business confidentiality protection",
    "doi": "https://doi.org/10.1145/1806916.1806919",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Bárbara Poblete; Myra Spiliopoulou; Ricardo Baeza‐Yates",
    "corresponding_authors": "",
    "abstract": "We introduce the concern of confidentiality protection of business information for the publication of search engine query logs and derived data. We study business confidentiality, as the protection of nonpublic data from institutions, such as companies and people in the public eye. In particular, we relate this concern to the involuntary exposure of confidential Web site information, and we transfer this problem into the field of privacy-preserving data mining. We characterize the possible adversaries interested in disclosing Web site confidential data and the attack strategies that they could use. These attacks are based on different vulnerabilities found in query log for which we present several anonymization heuristics to prevent them. We perform an experimental evaluation to estimate the remaining utility of the log after the application of our anonymization techniques. Our experimental results show that a query log can be anonymized against these specific attacks while retaining a significant volume of useful data.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2029076473",
    "type": "article"
  },
  {
    "title": "A bottom-up, knowledge-aware approach to integrating and querying web data services",
    "doi": "https://doi.org/10.1145/2493536",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Silvia Quarteroni; Marco Brambilla; Stefano Ceri",
    "corresponding_authors": "",
    "abstract": "As a wealth of data services is becoming available on the Web, building and querying Web applications that effectively integrate their content is increasingly important. However, schema integration and ontology matching with the aim of registering data services often requires a knowledge-intensive, tedious, and error-prone manual process. We tackle this issue by presenting a bottom-up, semi-automatic service registration process that refers to an external knowledge base and uses simple text processing techniques in order to minimize and possibly avoid the contribution of domain experts in the annotation of data services. The first by-product of this process is a representation of the domain of data services as an entity-relationship diagram, whose entities are named after concepts of the external knowledge base matching service terminology rather than being manually created to accommodate an application-specific ontology. Second, a three-layer annotation of service semantics (service interfaces, access patterns, service marts) describing how services “play” with such domain elements is also automatically constructed at registration time. When evaluated against heterogeneous existing data services and with a synthetic service dataset constructed using Google Fusion Tables, the approach yields good results in terms of data representation accuracy. We subsequently demonstrate that natural language processing methods can be used to decompose and match simple queries to the data services represented in three layers according to the preceding methodology with satisfactory results. We show how semantic annotations are used at query time to convert the user's request into an executable logical query. Globally, our findings show that the proposed registration method is effective in creating a uniform semantic representation of data services, suitable for building Web applications and answering search queries.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1970119074",
    "type": "article"
  },
  {
    "title": "Collusive Opinion Fraud Detection in Online Reviews",
    "doi": "https://doi.org/10.1145/3098859",
    "publication_date": "2017-07-24",
    "publication_year": 2017,
    "authors": "Chang Xu; Jie Zhang",
    "corresponding_authors": "",
    "abstract": "We address the collusive opinion fraud problem in online review portals, where groups of people work together to deliver deceptive reviews for manipulating the reputations of targeted items. Such collusive fraud is considered much harder to defend against, since the participants (or colluders) can evade detection by shaping their behaviors collectively so as not to appear suspicious. To alleviate this problem, countermeasures have been proposed that leverage the collective behaviors of colluders. The motivation stems from the observation that colluders typically act in a very synchronized way, as they are instructed by the same campaigns with common items to target and schedules to follow. However, the collective behaviors examined in existing solutions focus mostly on the external appearance of fraud campaigns, such as the campaign size and the size of the targeted item set. These signals may become ineffective once colluders have changed their behaviors collectively. Moreover, the detection algorithms used in existing approaches are designed to only make collusion inference on the input data; predictive models that can be deployed for detecting emerging fraud cannot be learned from the data. In this article, to complement existing studies on collusive opinion fraud characterization and detection, we explore more subtle behavioral trails in collusive fraud practice. In particular, a suite of homogeneity-based measures are proposed to capture the interrelationships among colluders within campaigns. Moreover, a novel statistical model is proposed to further characterize, recognize, and predict collusive fraud in online reviews. The proposed model is fully unsupervised and highly flexible to incorporate effective measures available for better modeling and prediction. Through experiments on two real-world datasets, we show that our method outperforms the state of the art in both characterization and detection abilities.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2738130394",
    "type": "article"
  },
  {
    "title": "A Model of Information Diffusion in Interconnected Online Social Networks",
    "doi": "https://doi.org/10.1145/3160000",
    "publication_date": "2018-05-31",
    "publication_year": 2018,
    "authors": "Rossano Gaeta",
    "corresponding_authors": "Rossano Gaeta",
    "abstract": "Online social networks (OSN) have today reached a remarkable capillary diffusion. There are numerous examples of very large platforms people use to communicate and maintain relationships. People also subscribe to several OSNs, e.g., people create accounts on Facebook, Twitter, and so on. This phenomenon leads to online social internetworking (OSI) scenarios where users who subscribe to multiple OSNs are termed as bridges . Unfortunately, several important features make the study of information propagation in an OSI scenario a difficult task, e.g., correlations in both the structural characteristics of OSNs and the bridge interconnections among them, heterogeneity and size of OSNs, activity factors, cross-posting propensity, and so on. In this article, we propose a directed random graph-based model that is amenable to efficient numerical solution to analyze the phenomenon of information propagation in an OSI scenario; in the model development, we take into account heterogeneity and correlations introduced by both topological (correlations among nodes degrees and among bridge distributions) and user-related factors (activity index, cross-posting propensity). We first validate the model predictions against simulations on snapshots of interconnected OSNs in a reference scenario. Subsequently, we exploit the model to show the impact on the information propagation of several characteristics of the reference scenario, i.e., size and complexity of the OSI scenario, degree distribution and overall number of bridges, growth and decline of OSNs in time, and time-varying cross-posting users propensity.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2806450438",
    "type": "article"
  },
  {
    "title": "Prediction and Predictability for Search Query Acceleration",
    "doi": "https://doi.org/10.1145/2943784",
    "publication_date": "2016-08-16",
    "publication_year": 2016,
    "authors": "Seung-won Hwang; Saehoon Kim; Yuxiong He; Sameh Elnikety; Seungjin Choi",
    "corresponding_authors": "",
    "abstract": "A commercial web search engine shards its index among many servers, and therefore the response time of a search query is dominated by the slowest server that processes the query. Prior approaches target improving responsiveness by reducing the tail latency , or high-percentile response time, of an individual search server. They predict query execution time, and if a query is predicted to be long-running, it runs in parallel; otherwise, it runs sequentially. These approaches are, however, not accurate enough for reducing a high tail latency when responses are aggregated from many servers because this requires each server to reduce a substantially higher tail latency (e.g., the 99.99th percentile), which we call extreme tail latency. To address tighter requirements of extreme tail latency, we propose a new design space for the problem, subsuming existing work and also proposing a new solution space. Existing work makes a prediction using features available at indexing time and focuses on optimizing prediction features for accelerating tail queries. In contrast, we identify “when to predict?” as another key optimization question. This opens up a new solution of delaying a prediction by a short duration to allow many short-running queries to complete without parallelization and, at the same time, to allow the predictor to collect a set of dynamic features using runtime information. This new question expands a solution space in two meaningful ways. First, we see a significant reduction of tail latency by leveraging “dynamic” features collected at runtime that estimate query execution time with higher accuracy. Second, we can ask whether to override prediction when the “predictability” is low. We show that considering predictability accelerates the query by achieving a higher recall. With this prediction, we propose to accelerate the queries that are predicted to be long-running. In our preliminary work, we focused on parallelization as an acceleration scenario. We extend to consider heterogeneous multicore hardware for acceleration. This hardware combines processor cores with different microarchitectures such as energy-efficient little cores and high-performance big cores, and accelerating web search using this hardware has remained an open problem. We evaluate the proposed prediction framework in two scenarios: (1) query parallelization on a multicore processor and (2) query scheduling on a heterogeneous processor. Our extensive evaluation results show that, for both scenarios of query acceleration using parallelization and heterogeneous cores, the proposed framework is effective in reducing the extreme tail latency compared to a start-of-the-art predictor because of its higher recall, and it improves server throughput by more than 70% because of its improved precision.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2510609312",
    "type": "article"
  },
  {
    "title": "User's Web Page Aesthetics Opinion",
    "doi": "https://doi.org/10.1145/3019595",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Silvia Uribe; Federico Álvarez; José Manuel Menéndez",
    "corresponding_authors": "",
    "abstract": "Analyzing a user's first impression of a Web site is essential for interface designers, as it is tightly related to their overall opinion of a site. In fact, this early evaluation affects user navigation behavior. Perceived usability and user interest (e.g., revisiting and recommending the site) are parameters influenced by first opinions. Thus, predicting the latter when creating a Web site is vital to ensure users’ acceptance. In this regard, Web aesthetics is one of the most influential factors in this early perception. We propose the use of low-level image parameters for modeling Web aesthetics in an objective manner, which is an innovative research field. Our model, obtained by applying a stepwise multiple regression algorithm, infers a user's first impression by analyzing three different visual characteristics of Web site screenshots—texture, luminance, and color—which are directly derived from MPEG-7 descriptors. The results obtained over three wide Web site datasets (composed by 415, 42, and 6 Web sites, respectively) reveal a high correlation between low-level parameters and the users’ evaluation, thus allowing a more precise and objective prediction of users’ opinion than previous models that are based on other image characteristics with fewer predictors. Therefore, our model is meant to support a rapid assessment of Web sites in early stages of the design process to maximize the likelihood of the users’ final approval.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2594950807",
    "type": "article"
  },
  {
    "title": "Activity Recommendation with Partners",
    "doi": "https://doi.org/10.1145/3121407",
    "publication_date": "2017-09-09",
    "publication_year": 2017,
    "authors": "Wenting Tu; David W. Cheung; Nikos Mamoulis; Min Yang; Ziyu Lu",
    "corresponding_authors": "",
    "abstract": "Recommending social activities , such as watching movies or having dinner, is a common function found in social networks or e-commerce sites. Besides certain websites which manage activity-related locations (e.g., foursquare.com), many items on product sale platforms (e.g., groupon.com) can naturally be mapped to social activities. For example, movie tickets can be thought of as activity items , which can be mapped as a social activity of “watch a movie.” Traditional recommender systems estimate the degree of interest for a target user on candidate items (or activities), and accordingly, recommend the top- k activity items to the user. However, these systems ignore an important social characteristic of recommended activities: people usually tend to participate in those activities with friends. This article considers this fact for improving the effectiveness of recommendation in two directions. First, we study the problem of activity-partner recommendation ; i.e., for each recommended activity item, find a suitable partner for the user. This (i) saves the user’s time for finding activity partners, (ii) increases the likelihood that the activity item will be selected by the user, and (iii) improves the effectiveness of recommender systems to users overall and enkindles their social enthusiasm. Our partner recommender is built upon the users’ historical attendance preferences, their social context, and geographic information. Moreover, we explore how to leverage the partner recommendation to help improve the effectiveness of recommending activities to users. Assuming that users tend to select the activities for which they can find suitable partners, we propose a partner-aware activity recommendation model, which integrates this hypothesis into conventional recommendation approaches. Finally, the recommended items not only match users’ interests, but also have high chances to be selected by the users, because the users can find suitable partners to attend the corresponding activities together. We conduct experiments on real data to evaluate the effectiveness of activity-partner recommendation and partner-aware activity recommendation. The results verify that (i) suggesting partners greatly improves the likelihood that a recommended activity item is to be selected by the target user and (ii) considering the existence of suitable partners in the ranking of recommended items improves the accuracy of recommendation significantly.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2754405912",
    "type": "article"
  },
  {
    "title": "How Do Home Computer Users Browse the Web?",
    "doi": "https://doi.org/10.1145/3473343",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Kyle Crichton; Nicolas Christin; Lorrie Faith Cranor",
    "corresponding_authors": "",
    "abstract": "With the ubiquity of web tracking, information on how people navigate the internet is abundantly collected yet, due to its proprietary nature, rarely distributed. As a result, our understanding of user browsing primarily derives from small-scale studies conducted more than a decade ago. To provide an broader updated perspective, we analyze data from 257 participants who consented to have their home computer and browsing behavior monitored through the Security Behavior Observatory. Compared to previous work, we find a substantial increase in tabbed browsing and demonstrate the need to include tab information for accurate web measurements. Our results confirm that user browsing is highly centralized, with 50% of internet use spent on 1% of visited websites. However, we also find that users spend a disproportionate amount of time on low-visited websites, areas with a greater likelihood of containing risky content. We then identify the primary gateways to these sites and discuss implications for future research.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3204606269",
    "type": "article"
  },
  {
    "title": "Double Attention Convolutional Neural Network for Sequential Recommendation",
    "doi": "https://doi.org/10.1145/3555350",
    "publication_date": "2022-08-09",
    "publication_year": 2022,
    "authors": "Qi Chen; Guohui Li; Quan Zhou; Si Shi; Deqing Zou",
    "corresponding_authors": "",
    "abstract": "The explosive growth of e-commerce and online service has led to the development of recommender system. Aiming to provide a list of items to meet a user’s personalized need by analyzing his/her interaction 1 history, recommender system has been widely studied in academic and industrial communities. Different from conventional recommender systems, sequential recommender systems attempt to capture the pattern of users’ sequential behaviors and the evolution of users’ preferences. Most of the existing sequential recommendation models only focus on user interaction sequence, but neglect item interaction sequence. An item interaction sequence also contains rich contextual information for capturing the item’s dynamic characteristic, since an item’s dynamic characteristic can be reflected by the users who interact with it in a period. Furthermore, existing dual sequential models use the same method to handle the user interaction sequence and item interaction sequence, and do not consider their different characteristics. Hence, we propose a novel D ouble A ttention C onvolution N eural N etwork (DACNN) , which incorporates user interaction sequence and item interaction sequence into an integrated neural network framework. DACNN leverages the strength of attention mechanism to capture the temporary suitability and adopts CNN to extract local sequential features. Experimental evaluations on the real datasets show that DACNN outperforms the baseline approaches.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4297816417",
    "type": "article"
  },
  {
    "title": "Automated Monitoring of Web User Interfaces",
    "doi": "https://doi.org/10.1145/3708512",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Ennio Visconti; Christos Tsigkanos; Laura Nenzi",
    "corresponding_authors": "",
    "abstract": "Application development for the modern Web involves sophisticated engineering workflows – including user interface (UI) aspects. Such user interfaces comprise Web elements that are typically created with HTML/CSS markup and JavaScript-like languages, yielding Web documents. Their testing entails performing checks to examine visual and structural parts of the resulting UI software against requirements such as usability, accessibility, performance, or, increasingly, compliance with standards. However, current techniques are largely ad-hoc and tailor-made to specific classes of requirements or Web technologies and extensively require human-in-the-loop qualitative evaluations. Web UI evaluation so far has lacked formal foundations, which would provide assurances of compliance with requirements in an automatic manner. To this end, we devise a methodology and accompanying technical framework for web UIs. In our approach, requirements are formally specified in a spatio-temporal logic able to capture both the layout of visual components as well as how they change over time, as a user interacts with them. The technique we advocate is independent of the underlying technologies a Web application may be developed with, as well as the browser and operating system used. To concretely support the specification and evaluation of UI requirements, our framework is grounded on open-source tools for instrumenting, analyzing, and reporting spatio-temporal behaviors in webpages. We demonstrate our approach in practice over Web accessibility standards posing challenges for automated verification.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406915061",
    "type": "article"
  },
  {
    "title": "Trust Models Go to the Web: Learning How to Trust Strangers",
    "doi": "https://doi.org/10.1145/3715882",
    "publication_date": "2025-01-29",
    "publication_year": 2025,
    "authors": "Pasquale De Meo; Ylli Prifti; Alessandro Provetti",
    "corresponding_authors": "",
    "abstract": "We study emerging traits of interpersonal and social trust in online social networks of needs (OSNNs), where trust interactions start online and evolve into in-person meetings. We present a lightweight web scraping solution to harness data from online social networks; thanks to it we were able to monitor a nation-wide portal for childcare and see the evolution of online reviews from both families and carers. We analysed the data by first considering topological information to test centrality metrics as proxies for trustworthiness. Next, we focused on features/profile analysis and tested the Castelfranchi-Falcone trust model from Psychology (CF-T), fitting it to online reviews of childcare services. Even though such reviews are relatively scarce and seemingly skewed, we feature-engineered the CF-T model to predict the evolution of reviews, treated as proxies for trust. By aggregating CF-T scores at regional level we discovered a strong correlation with per capita GDP, which suggests that high levels of trust in social networks of needs reflect social capital.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406930418",
    "type": "article"
  },
  {
    "title": "What Did My Users Experience? Discovering Visual Stimuli on Graphical User Interfaces of the Web",
    "doi": "https://doi.org/10.1145/3715881",
    "publication_date": "2025-02-05",
    "publication_year": 2025,
    "authors": "Raphael Menges; Steffen Staab; Christoph Schaefer; Tina Walber; Chandan Kumar",
    "corresponding_authors": "",
    "abstract": "Main tasks of usability experts for Web sites comprise the analysis of user interaction behavior on graphical user interfaces, the discovery of issues, and the derivation of improvements to the interface. The analysis of user interaction behavior and corresponding discovery of issues are made difficult by modern Web interfaces that incorporate dynamic interface elements and that orchestrate complex reactions to user responses. We propose a semi-automated approach for discovering visual stimuli, which capture summarized views of the interface as encountered by users during interaction. Discovered visual stimuli allow for meaningful aggregations of user interactions based on what users encountered on the interface such that the analysis by usability experts can relate the interface views with user interactions correctly and identify arising issues. We provide WebVSD as an implementation of the approach and perform a set of evaluations with real-world Web sites that show the accuracy of proposed methods in isolation and in the tool chain, as well as case studies and a survey of usability experts indicating the usefulness of the suggested approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407185059",
    "type": "article"
  },
  {
    "title": "BNoteToDanmu: Category-Guided Note-to-Danmu Conversion Method for Learning on Video Sharing Platforms",
    "doi": "https://doi.org/10.1145/3723349",
    "publication_date": "2025-03-13",
    "publication_year": 2025,
    "authors": "Fangyu Yu; Peng Zhang; Siyuan Qiao; Xianghua Ding; Tun Lu; Ning Gu",
    "corresponding_authors": "",
    "abstract": "Danmu (or “bullet screen”), a popular feature on video sharing platforms, plays a crucial role in facilitating knowledge sharing and learning. In recent years, danmu has drawn attention to automatic generation methods. However, existing methods mostly utilize limited content sources, such as the video itself (e.g., subtitles) and neighboring danmus, while other valuable sources remain underexplored. To this end, this paper proposes a Category-Guided Note-to-Danmu conversion model (CG-NTD) by leveraging user-generated notes. The model is designed to identify unique contents within the notes and convert them into danmus, while also showing the source note categories. CG-NTD classifies the notes by fusing them with subtitle and neighboring danmu features. Then, it uses a cross-attention mechanism to integrate the note’s category feature with note, subtitle, and danmu contexts, to identify three keywords from the notes as the generated danmus. Using Bilibili as the research site, we implement a plugin prototype named BNoteToDanmu. Automatic and human evaluations reveal that CG-NTD outperforms BiLSTM, mT5, and BERT baselines in Precision, Recall, and F1-score metrics, and generates more understandable and relevant danmus than ChatGPT. Moreover, the plugin demonstrates promising applications, such as assisting users in viewing videos, posting danmus, and recognizing high-quality notes. These findings offer insights into leveraging user creations to generate danmu to enhance its learning value on video sharing platforms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408412766",
    "type": "article"
  },
  {
    "title": "Privacy Policies and Consent Management Platforms: Growth and Users' Interactions over Time",
    "doi": "https://doi.org/10.1145/3725737",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Nikhil Jha; Martino Trevisan; Marco Mellia; Daniel Fernandez; Rodrigo Irarrazaval",
    "corresponding_authors": "",
    "abstract": "In response to growing concerns about user privacy, legislators have introduced new regulations and laws such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) that force websites to obtain user consent before activating any personal data collection. The cornerstone of this consent-seeking process involves the use of Privacy Banners, the technical tools to collect users’ approval for data collection practices. Consent management platforms (CMPs) have emerged as practical solutions to simplify the configuration and management of such privacy banners for website administrators, allowing them to outsource the complexities of managing user consent and activating advertising features. This paper presents a detailed and longitudinal analysis of the evolution of CMPs spanning nine years. We take a twofold perspective: Firstly, thanks to the HTTP Archive dataset, we provide insights into the growth, market share, and geographical spread of CMPs. Noteworthy observations include the substantial impact of GDPR on the proliferation of CMPs in Europe, where more than 40% of websites currently adopt a CMP. Secondly, we analyse millions of user interactions with a medium-sized CMP present in thousands of websites worldwide. We observe how even small changes in the design of Privacy Banners have a critical impact on the user’s giving or denying their consent to data collection. For instance, over 60% of users do not consent when offered a simple “one-click reject-all” option. Conversely, when opting out requires more than one click, about 90% of users prefer to simply give their consent. This hints that their main objective is to eliminate the annoying privacy banner rather than make an informed decision. Curiously, we observe iOS users exhibit a higher tendency to accept cookies compared with Android users, possibly indicating greater confidence in the privacy offered by Apple devices. We believe the findings of this paper contribute to a deeper understanding of the multifaceted interactions between privacy regulations, technological solutions and user choices in the evolving Web ecosystem. We also show that the availability of large open datasets, although not explicitly designed and collected for our goals, is fundamental to exploring different angles of the internet evolution over time. For this, we make the data and code used in this work available to the community.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409042172",
    "type": "article"
  },
  {
    "title": "Investigating the Luna-Terra Collapse through the Temporal Multilayer Graph Structure of the Ethereum Stablecoin Ecosystem",
    "doi": "https://doi.org/10.1145/3726869",
    "publication_date": "2025-04-03",
    "publication_year": 2025,
    "authors": "Cheick Tidiane Bâ; Benjamin A. Steer; Matteo Zignani; Richard G. Clegg",
    "corresponding_authors": "",
    "abstract": "Blockchain technology and cryptocurrencies have garnered considerable attention over the past fifteen years. The term Web3 (sometimes Web 3.0) has been coined to define a possible direction for the web based on the use of decentralisation via blockchain. Cryptocurrencies are characterised by high market volatility and susceptibility to substantial crashes, issues that require temporal analysis methodologies able to tackle the high temporal resolution, heterogeneity and scale of blockchain data. While existing research attempts to analyse crash events, fundamental questions persist regarding the optimal time scale for analysis, differentiation between long-term and short-term trends, and the identification and characterisation of shock events within these decentralised systems. This paper addresses these issues by examining cryptocurrencies traded on the Ethereum blockchain, with a spotlight on the crash of the stablecoin TerraUSD (UST) and the currency LUNA designed to stabilise it. Utilising complex network analysis and a multi-layer temporal graph allows the study of the correlations between the layers representing the currencies and system evolution across diverse time scales. The investigation sheds light on the strong interconnections among stablecoins pre-crash and the significant post-crash transformations. We identify anomalous signals before, during, and after the collapse, emphasising their impact on graph structure metrics and user movement across layers. This paper is novel in its use of temporal, cross-chain graph analysis to explore a cryptocurrency collapse. It emphasises the importance of temporal analysis for studies on web-derived data. In addition, the methodology shows how graph-based analysis can enhance traditional econometric results. Overall, this research carries implications beyond its field, for example for regulatory agencies aiming to safeguard users could use multi-layer temporal graphs as part of their suite of analysis tools.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409144785",
    "type": "article"
  },
  {
    "title": "Saga: Understanding Stories in Mobile App Reviews",
    "doi": "https://doi.org/10.1145/3728313",
    "publication_date": "2025-04-16",
    "publication_year": 2025,
    "authors": "Hui Guo; Munindar P. Singh",
    "corresponding_authors": "",
    "abstract": "Online storytelling is an essential channel for users to express their experiences and opinions and therefore influence online society. Yet, despite its importance, approaches to story understanding on social media have not advanced sufficiently. Specifically, current approaches can carry out high-level, aggregate analyses on a corpus of stories but do not provide a way of understanding individual stories. We consider a major source of social behavior, app reviews, which surprisingly are rarely studied in social media research. We observe that app reviews often contain one or more stories. These stories exhibit complex structures and are often presented via events that are not placed in their natural order. Accordingly, we introduce Saga, an approach that carries out a deep analysis of the event-based structures and substructures arising in app reviews. Saga’s main contribution is how it goes beyond the state of the art in identifying fine-grained story (sub)structures. In addition, it supports querying stories (and their containing app reviews) according to these (sub)structures. These specific (sub)structures help identify stories that serve different information goals. Saga is evaluated both computationally on a publicly available data source and via a human study validating the helpfulness in addressing various information goals.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409483211",
    "type": "article"
  },
  {
    "title": "Stability-aware Preference Modeling for Sequential Recommendation",
    "doi": "https://doi.org/10.1145/3715147",
    "publication_date": "2025-04-30",
    "publication_year": 2025,
    "authors": "C. M. Wei; Wenjun Jiang; Kenli Li; Jie Wu",
    "corresponding_authors": "",
    "abstract": "Many researchers primarily rely on modeling user interests for sequential recommendation. However, dynamic user behaviors often accompany unstable interaction histories, and modeling user interests alone is insufficient for comprehensive user features. Some studies notice the problem of interest drift, but they are usually limited to modeling at the item-level, unable to perceive the subtle changes at the feature-level. To this end, we propose a Stability-aware Preference Model (SAPM), which consists of three modules. The LSI module for extracting long and short-term interests, the FLC module for extracting feature-level candidate information, and the SAF module for fusing them according to the stability score. In particular, we propose a Multi-head GRU (MHGRU) structure in the LSI module, which is more efficient than the general GRU and has stronger expression ability. Through extensive experiments, our framework shows significant mitigation of the impact of unstable interactions. On the two real data sets, we improve MRR by 5.7% and 14.0% compared with the recent baselines. Moreover, we conduct an in-depth analysis of user interaction stability and obtain several interesting findings that can benefit future studies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409963629",
    "type": "article"
  },
  {
    "title": "Large Language Models in Crisis Informatics for Zero and Few-Shot Classification",
    "doi": "https://doi.org/10.1145/3736160",
    "publication_date": "2025-05-22",
    "publication_year": 2025,
    "authors": "Cinthia Sánchez; Andrés Abeliuk; Bárbara Poblete",
    "corresponding_authors": "",
    "abstract": "This article presents an exploration of the use of pre-trained Large Language Models (LLMs) for crisis classification to address labeled data dependency issues. We present a methodology that enhances open LLMs through fine-tuning, creating zero-shot and few-shot classifiers that approach traditional supervised models in classifying crisis-related messages. A comparative study evaluates crisis classification tasks using general domain pre-trained LLMs, crisis-specific LLMs, and traditional supervised learning methods, establishing a benchmark in the field. Our task-specific fine-tuned Llama model achieved a 69% macro F1 score in classifying humanitarian information—a remarkable 26% improvement compared to the Llama baseline, even with limited training data. Moreover, it outperformed ChatGPT4 by 3% in macro F1. This improvement increased to 71% macro F1 when fine-tuning Llama with multitask data. For the binary classification of messages as related vs. not related to crises, we observed that pre-trained LLMs, such as Llama 2 and ChatGPT4, performed well without fine-tuning, achieving an 87% macro F1 score with ChatGPT4. This research expands our knowledge of how to exploit the potential of LLMs for crisis classification, representing a great opportunity for crisis scenarios that lack labeled data. The findings emphasize the potential of LLMs in crisis informatics to address cold start challenges, especially critical in the initial phases of a disaster, while also showcasing their capacity to attain high accuracy even with limited training data.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410601021",
    "type": "article"
  },
  {
    "title": "An In-depth Analysis of the Linguistic Characteristics of Science Claims on the Web and their Impact on Fact-checking",
    "doi": "https://doi.org/10.1145/3746170",
    "publication_date": "2025-06-25",
    "publication_year": 2025,
    "authors": "Salim Hafid; Sebastian Schellhammer; Yavuz Selim Kartal; Θωμάς Παπαστεργίου; Stefan Dietze; Sandra Bringay; Konstantin Todorov",
    "corresponding_authors": "",
    "abstract": "Web claims, seen as assertions shared on the web and eligible for fact-checking, are at the heart of online discourse. They have been studied extensively on a variety of downstream tasks such as fact-checking, claim retrieval, bias detection, argument mining or viewpoint discovery. On the other hand, claims originating from scientific publications have also been the subject of several downstream NLP tasks. However, research carried out so far has yet to focus on scientific web claims, which are scientific claims made on the web (e.g., on social media and news articles). The process of detecting and fact-checking a claim from the web can be very different depending on whether the claim is scientific or not, thus making it crucial for the developed datasets, methods, and models to make a distinction between the two. With this work, we aim at understanding what makes this distinction necessary, by understanding the linguistic differences between scientific and non-scientific claims on the web, and the impact those differences have on existing downstream tasks. To do so, we manually annotate 1,524 web claims from established benchmarks for fact-checking-related tasks, and we run statistical tests to analyze and compare linguistic features of each group. We find that scientific claims on the web use more analytical speech, but also use more sentiment-related speech, more expressions of physical motion, and have distinct parts of speech (PoS) and punctuation styles. We also conduct experiments showing that BERT-based language models perform worse on scientific web claims by up to 17 F1 points for several downstream tasks. To understand why, we develop a novel methodology to map predictive tokens of language models to explainable linguistic features and find that language models fail to detect a specific subset of predictive features of scientific web claims. We conclude by stating that language models aimed at studying scientific web claims ought to be trained on scientific web discourse, as opposed to being trained only on generic web discourse or only on scientific text from scientific publications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411652471",
    "type": "article"
  },
  {
    "title": "Schema Change Recommendation for User-Curated Webtables Using Temporal Data",
    "doi": "https://doi.org/10.1145/3742920",
    "publication_date": "2025-06-30",
    "publication_year": 2025,
    "authors": "Tobias Bleifuß; Leon Bornemann; Felix Naumann; Divesh Srivastava",
    "corresponding_authors": "",
    "abstract": "On the web, huge corpora of tables exist, which can include millions of tables, as in the case of Wikipedia. Maintaining them can be a time-consuming task and, in the case of many authors and editors, also requires a great deal of coordination to ensure high quality, complete, consistent, and readable schemata. In this work, we investigate how to provide automatic suggestions to improve the schema of webtables, namely how to recommend schema changes. For this purpose, we derive rules from past schema changes via a lattice-based approach and then rank these rules to provide the best-fitting suggestions for each webtable. Making use of the entire edit history of Wikipedia tables, we can compare our suggestions with the changes that were actually performed by editors. We show that for 75.13% of the changes in the test set, we make a correct recommendation, namely a change that was also observed subsequently on Wikipedia. In 58.66% of the cases, our recommendation even covers the entire observed change. Finally, we rank the recommendations with a mean reciprocal rank (MRR) of 0.73 and 0.69 for matches and full matches, respectively. A validation of our approach on three Fandom wikis confirms its effectiveness and generality.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411812836",
    "type": "article"
  },
  {
    "title": "Modeling Disinformation Spread in Social Networks: Phase Transitions and Mean-Field Analysis",
    "doi": "https://doi.org/10.1145/3747287",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Spyridon Evangelatos; Eleni Veroni; Vasilis Efthymiou; Christos D. Nikolopoulos",
    "corresponding_authors": "",
    "abstract": "The pervasive spread of disinformation across social media platforms has become a significant global challenge, disrupting democratic processes, undermining public trust and fueling societal polarization. Existing approaches often neglect the dynamic and structural mechanisms that drive the spread and adoption of false narratives. This paper leverages the well-established principles and methodologies of Statistical Mechanics and introduces a dynamic Mean-Field framework to model the evolution of disinformation within social networks. The framework introduces innovative elements, including heterogeneous coupling strengths to capture diverse social influences among network users, memory effects to account for cognitive inertia or belief re-evaluation and a three-state Potts model to represent polarization and neutrality in opinion dynamics. It employs the concept of effective fields to integrate external disinformation campaigns, facilitating a detailed analysis of critical thresholds and phase transitions. Monte Carlo simulations are performed to further illustrate the transient and equilibrium dynamics of belief adoption and rejection. Our findings provide actionable insights for the disinformation spread and offer a theoretical foundation for designing targeted interventions to mitigate its harmful effects on societies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412593918",
    "type": "article"
  },
  {
    "title": "Full Triple Matcher: Integrating All Triple Elements between Heterogeneous Knowledge Graphs",
    "doi": "https://doi.org/10.1145/3754338",
    "publication_date": "2025-07-24",
    "publication_year": 2025,
    "authors": "Victor Eiti Yamamoto; Hideaki Takeda",
    "corresponding_authors": "",
    "abstract": "Knowledge graphs (KGs) are powerful tools for representing and reasoning over structured information. Their main components include schema, identity, and context. While schema and identity matching are well-established in ontology and entity matching research, context matching remains largely unexplored. This is particularly important because real-world KGs often vary significantly in source, size, and information density—factors not typically represented in the datasets on which current entity matching methods are evaluated. As a result, existing approaches may fall short in scenarios where diverse and complex contexts need to be integrated. To address this gap, we propose a novel KG integration method consisting of label matching and triple matching. We use string manipulation, fuzzy matching, and vector similarity techniques to align entity and predicate labels. Next, we identify mappings between triples that convey comparable information, using these mappings to improve entity-matching accuracy. Our approach demonstrates competitive performance compared to leading systems in the OAEI competition and against supervised methods, achieving high accuracy across diverse test cases. Additionally, we introduce a new dataset derived from the benchmark dataset to evaluate the triple-matching step more comprehensively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412636524",
    "type": "article"
  },
  {
    "title": "Cross-domain Transfer of Valence Preferences via a Meta-optimization Approach",
    "doi": "https://doi.org/10.1145/3749284",
    "publication_date": "2025-07-17",
    "publication_year": 2025,
    "authors": "Chuang Zhao; Hongke Zhao; Ming He; Xiaomeng Li; Jianping Fan",
    "corresponding_authors": "",
    "abstract": "Cross-domain recommendation offers a potential avenue for alleviating data sparsity and cold-start problems. Embedding and mapping, as a classic cross-domain research genre, aims to identify a common mapping function to perform representation transformation between two domains via exploiting the supervision signals of overlapping users. Nevertheless, coarse-grained preference representations, non-personalized mapping functions, and excessive reliance on overlapping users limit their performance, especially in scenarios where overlapping users are sparse. To address the aforementioned challenges, we propose a novel C ross-domain transfer of V alence P references via a M eta-optimization approach , namely CVPM . CVPM formalizes cross-domain interest transfer as a hybrid architecture of parametric meta-learning and self-supervised learning, which not only enables a more nuanced transfer of user preferences but also enhances signal quality by incorporating insights from non-overlapping users. Specifically, drawing on in-depth knowledge into user preferences and valence preference theory, we believe that there exists a significant difference between users’ positive preferences and negative behaviors, and thus employ differentiated encoders to learn their distributions. In particular, we further utilize the pre-trained model and item popularity to sample pseudo-interaction items to ensure the integrity of both distributions. To guarantee the personalized preference transfer, we treat each user’s mapping as two parts, the common transformation and the personalized bias, where the network generating the personalized bias is produced by a meta-learner. Furthermore, beyond the supervised loss for overlapping users, we design contrastive tasks for non-overlapping users from both group and individual levels to avoid model skew and enhance the semantic richness of representations. We construct 6 cross-domain tasks and 1 cross-system task from 10 data sets assessing model performance under both cold-start and warm-start scenarios. Exhaustive data analysis and extensive experimental results demonstrate the effectiveness and advancement of our proposed framework.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413079838",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Temporal Web: Studying Time and the Temporal Dimension",
    "doi": "https://doi.org/10.1145/3743140",
    "publication_date": "2025-08-22",
    "publication_year": 2025,
    "authors": "Omar Alonso; Marc Spaniol; Ricardo Baeza‐Yates",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413412600",
    "type": "article"
  },
  {
    "title": "Beyond Authorial Burden",
    "doi": "https://doi.org/10.1145/3757746",
    "publication_date": "2025-08-29",
    "publication_year": 2025,
    "authors": "Joey Donald Jones; David E. Millard",
    "corresponding_authors": "",
    "abstract": "Interactive Digital Narrative (IDN), a primarily web-first hypertextual medium, creates an overhead of writing for authors. There are many methods and tools that have been proposed for writing IDNs with the goal of reducing this burden, but because there is no comprehensive model of the Authoring Burden assessing the impact and appropriateness of these approaches is difficult. We have undertaken interviews with IDN authors (n=14) to understand how they manage the authoring burden within their own projects. Based on these interviews, and drawing on the existing literature, we propose a model of the Authoring Burden comprised of three parts: Content Creation, Dynamic Authoring, and Programming/Tool Creation. The initial size of this burden is set by the Author’s Goals informed by their Capability and the Audience/Publishing Context. We also find 29 strategies employed by authors to manage the burden. There are five distinct types. Embracing and Reducing strategies impact the overall scale of the challenge, whereas Generative, Reuse, and Decoupling strategies move work between the three parts of the model. We validate our model with focus groups comprising different sets of experts (n=8). Our model shows that many strategies for managing the burden transform rather than reduce work, and that the most appropriate strategy for a given author will be highly dependent on their personal goals and capabilities. It shows claims to alleviate the ’burden’ of authoring labour may often shift the nature of the labour itself, or the design of a given IDN, into unwanted forms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413828409",
    "type": "article"
  },
  {
    "title": "Decentralized Model Selection for Test-Time Adaptation in Heterogeneous Connected Systems",
    "doi": "https://doi.org/10.1145/3764936",
    "publication_date": "2025-08-29",
    "publication_year": 2025,
    "authors": "Yao Du; Cyril Leung; Zehua Wang; Xiaoxiao Li; Victor C. M. Leung",
    "corresponding_authors": "",
    "abstract": "Traditional centralized model training assumes that data samples are readily available and can be processed without constraints. In contrast, decentralized machine learning (DML) addresses the limitation by collaborative model training and inference directly on distributed data sources. The transformation from data centralization to decentralization helps comply with data regulations and improves system scalability with reduced reliance on cloud servers. However, a trade-off between model personalization and generalization exists: the fine-tuning of local training data distribution sacrifices model generalization on the testing data distribution that differs from the training data distribution. To improve the trade-off, we propose a DML framework that can inherently make model personalization and generalization easier by selecting a model among multiple ones judiciously. We develop a scalable selector for model selection and use blockchain to achieve model consensus. The personalized model selector is then proposed for test-time adaptation. Using computer simulations, we show that our method not only outperforms competitive personalization benchmarks but also generalizes well for new data distributions with various shifts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413830730",
    "type": "article"
  },
  {
    "title": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models",
    "doi": "https://doi.org/10.1145/3765895",
    "publication_date": "2025-09-03",
    "publication_year": 2025,
    "authors": "Ala Yankouskaya; Areej Babiker; Shireen L. Rizvi; Sameha Alshakhsi; Magnus Liebherr; Raian Ali",
    "corresponding_authors": "",
    "abstract": "There is growing interest in understanding how people interact with large language models (LLMs) and whether such models elicit dependency or even addictive behaviour. Validated tools to assess the extent to which individuals may become dependent on LLMs are scarce and primarily build on classic behavioral addiction symptoms, adapted to the context of LLM use. We view this as a conceptual limitation, as the LLM-human relationship is more nuanced and warrants a fresh and distinct perspective. To address this gap, we developed and validated a new 12-item questionnaire to measure LLM dependency, referred to as LLM-D12. The scale was based on the authors' prior theoretical work, with items developed accordingly and responses collected from 526 participants in the UK. Exploratory and confirmatory factor analyses, performed on separate halves of the total sample using a split-sample approach, supported a two-factor structure: Instrumental Dependency (six items) and Relationship Dependency (six items). Instrumental Dependency reflects the extent to which individuals rely on LLMs to support or collaborate in decision-making and cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs as socially meaningful, sentient, or companion-like entities. The two-factor structure demonstrated excellent internal consistency and clear discriminant validity. External validation confirmed both the conceptual foundation and the distinction between the two subscales. The psychometric properties and structure of our LLM-D12 scale were interpreted in light of the emerging view that dependency on LLMs does not necessarily indicate dysfunction but may still reflect reliance levels that could become problematic in certain contexts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413955382",
    "type": "article"
  },
  {
    "title": "Connecting Large Language Models with Blockchain: Making Smart Contracts Smarter",
    "doi": "https://doi.org/10.1145/3767296",
    "publication_date": "2025-09-11",
    "publication_year": 2025,
    "authors": "Xueying Zeng; Youquan Xian; Duancheng Xuan; Dou‐Yan Yang; Chunpei Li; J.H Chen; Peng Fan; Peng Liu",
    "corresponding_authors": "",
    "abstract": "Blockchain technology has driven the development of Decentralized Applications (DApps) in areas such as decentralized finance. However, as application scenarios become more complex, the limitations of computational resources and costs gradually lead to insufficient performance. Large Language Models (LLMs), as a promising technology, have the potential to enhance blockchain’s capabilities in complex task governance. However, due to factors such as consensus mechanisms, it is challenging to directly integrate them with blockchain. To address this issue, this paper proposes and implements a general framework for integrating LLMs with blockchain data, C-LLM, which successfully overcomes interoperability barriers between the two. By combining semantic relevance evaluation and truth discovery techniques, this paper presents an innovative data aggregation method, SenteTruth, which effectively improves the correctness and credibility of data generated by LLMs. To validate the framework’s effectiveness, we construct a dataset containing three types of questions, covering Q&amp;A records between 10 oracle nodes and 5 LLM models. Experimental results show that, in the presence of 40% malicious nodes, the proposed method improves data correctness by an average of 17.74% compared to the optimal baseline. This research not only provides an innovative solution for the intelligent application of smart contracts but also demonstrates the potential for deep integration of LLMs and blockchain, driving the development of smarter and more complex application scenarios for smart contracts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414128180",
    "type": "article"
  },
  {
    "title": "Accurate, Generalizable, and Practical Behavioral Models to Identify Impending User Exposure to Malicious Websites",
    "doi": "https://doi.org/10.1145/3768587",
    "publication_date": "2025-09-18",
    "publication_year": 2025,
    "authors": "Jin-Dong Dong; Kyle Crichton; Akira Yamada; Yukiko Sawaya; Lorrie Faith Cranor; Nicolas Christin",
    "corresponding_authors": "",
    "abstract": "To keep users safe online, current protections frequently employ blocklists of known malware and phishing websites. However, such defenses suffer from an inherent gap between malicious content creation and its detection, leaving a window where users are left vulnerable. To address this limitation, earlier research has shown that one could use individual user web browsing behavior to identify imminent exposure to malicious content. While existing methods frequently rely on temporal proximity (e.g., aggregating browsing patterns over the recent past), they do not leverage temporal ordering in user browsing, which results in suboptimal performance and is, in practice, inadequate given the low base rates of malware incidence. We introduce network and browser-level features (e.g., page rank, tab browsing time) and a temporal model that captures user behavior through a time-series representation. This not only improves classification performance by a significant margin (between 93% and 145% F1-score improvements) over previous models, but also maintains strong robustness across completely disparate sets of users. More importantly, our method shows strong resilience to concept drift, as performance holds steady over multiple years of testing. We discuss how this method is capable of anticipating future exposure. We also assess the relative importance of each feature to the performance, as well as their impact on false positive rates—whose minimization is critical to foster adoption. Finally, we discuss use cases for such behavior-based models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414343993",
    "type": "article"
  },
  {
    "title": "A Proxy Re-Encryption Based Trusted Data Sharing Scheme for Digital Twins with Privacy Protection",
    "doi": "https://doi.org/10.1145/3768586",
    "publication_date": "2025-09-25",
    "publication_year": 2025,
    "authors": "Jing Wu; Hongmin Gao; Yuxin Chen; Rongxuan Zhang; Jiancun Hu; Keke Ye; Xiaodan Yan",
    "corresponding_authors": "",
    "abstract": "Against the backdrop of accelerating digitalization, digital twin technology, through virtual-real mapping and real-time interaction mechanisms, has become a crucial supporting technology in fields such as industrial manufacturing and smart cities. However, the massive high-value data it generates faces risks such as sensitive information leakage and single points of failure during the sharing and circulation process. Traditional centralized protection solutions are insufficient to meet the security requirements in its distributed scenarios. To address these problems, this paper adopts a collaborative on-chain and off-chain storage architecture with blockchain and the InterPlanetary File System (IPFS), and proposes a multi-layered privacy protection scheme for data sharing and circulation, achieving distributed data storage and integrity verification. Additionally, a distributed proxy re-encryption algorithm for data sharing and circulation protection is designed, breaking through the centralized proxy bottleneck of traditional solutions, while effectively defending against collusion attacks and single points of failure. Experiments and system evaluations indicate that the proposed scheme has complete functionality and reliable service, and offers good security and performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414514015",
    "type": "article"
  },
  {
    "title": "Beyond Trade-offs: Leveraging Spatiotemporal Heterogeneity of User Preference for Long-term Fairness and Accuracy in Interactive Recommendation",
    "doi": "https://doi.org/10.1145/3769471",
    "publication_date": "2025-09-25",
    "publication_year": 2025,
    "authors": "C. Xia; Xiaoyu Shi; Hong Xie; Yun Lu; Penglong Li; Mingsheng Shang",
    "corresponding_authors": "",
    "abstract": "As recommender systems are essential to various web domains such as e-commerce and web content sharing, providing equitable item exposure regardless of popularity becomes an imperative requirement. However, traditional fairness-aware approaches typically aim to achieve a better trade-off between recommendation accuracy and fairness, and focus on improving the exposure rate of the long-tail items on static settings, evaluating fairness on one-shot recommendation decisions using logged data. Such methods overlook the dynamic nature of user preferences in real-world interactive environments. In contrast, our work seeks a win-win solution that simultaneously enhances recommendation accuracy and fairness over the long term, rather than merely trading off one against the other. To achieve this goal, we empirically demonstrate and analyze the spatiotemporal heterogeneity of user popularity preference. Our findings reveal complementary characteristics that, when fully exploited, can guide personalized strategies for long-term fairness. Building on this insight, we propose HER4IF, a novel hierarchical reinforcement learning framework designed for interactive recommendation. HER4IF decomposes the recommendation process into two key tasks: dynamic fairness control and item recommendation. The high-level agent continuouasly learns adaptive fairness constraints from evolving user popularity preferences, while the low-level agent refines recommendation policies under these personalized constraints. Extensive experiments on three real-world datasets and the interactive recommendation platform KuaiSim demonstrate that HER4IF significantly outperforms state-of-the-art methods, achieving substantial improvements in both fairness and recommendation accuracy. Our code is available at: https://github.com/1163710212/HER4IF.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414514056",
    "type": "article"
  },
  {
    "title": "It's Not Easy: Applying Supervised Machine Learning to Detect Malicious Extensions in the Chrome Web Store",
    "doi": "https://doi.org/10.1145/3770852",
    "publication_date": "2025-10-02",
    "publication_year": 2025,
    "authors": "Ben Rosenzweig; Vincent Valle; Giovanni Apruzzese; Aurore Fass",
    "corresponding_authors": "",
    "abstract": "Google Chrome is the most popular Web browser. Users can customize it with extensions that enhance their browsing experience. The most well-known marketplace of such extensions is the Chrome Web Store (CWS). Developers can upload their extensions on the CWS, but such extensions are made available to users only after a vetting process carried out by Google itself. Unfortunately, some malicious extensions bypass such checks, putting the security and privacy of downstream browser extension users at risk. In this paper, we carry out a comprehensive real-world security analysis of malicious extensions in the CWS. Specifically, we scrutinize the extent to which automated mechanisms reliant on supervised machine learning (ML) can be used to detect malicious extensions on the CWS. To this end, we first collect 7,140 malicious extensions published in 2017–2023 and which have been flagged as malicious by Google. We combine this dataset with 63,598 benign extensions published or updated on the CWS before 2023, and we develop three supervised-ML-based classifiers—leveraging both original features as well as techniques inspired by prior work. We show that, in a “lab setting”, our classifiers work well (e.g., 98% accuracy). Then, we collect a new, and more recent, set of 35,462 extensions from the CWS, published or last updated in 2023, with unknown ground truth. We were eventually able to identify 68 malicious extensions that bypassed the vetting process of the CWS. However, our classifiers also reported over 1k likely malicious extensions which may overestimate their true number. Based on this finding (further supported with other experiments and realistic analyses), we elucidate, for the first time, a strong concept drift effect on browser extensions. We also provide factual evidence that commercial detectors (e.g., VirusTotal) work poorly to detect known malicious extensions. Altogether, our results highlight the fact that detecting malicious browser extensions is a fundamentally hard problem which has not (yet) received an adequate degree of attention. This requires additional work both by the research community and by Google itself—potentially by revising their approaches. In the meantime, we informed Google of our discoveries, and we release our artifacts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414747289",
    "type": "article"
  },
  {
    "title": "Sentiment-Enhanced Cyberbullying Detection Models on Social Media Platforms",
    "doi": "https://doi.org/10.1145/3766075",
    "publication_date": "2025-10-09",
    "publication_year": 2025,
    "authors": "Adamu Gaston Philipo; Jianguo Ding; Doreen Sebastian Sarwatt; Jumanne Ally Mohamed; Afidhu Swaibu Yusufu; Mahmoud Daneshmand; Huansheng Ning",
    "corresponding_authors": "",
    "abstract": "Cyberbullying on social media platforms remains a serious threat to digital well-being, requiring intelligent systems capable of detecting both explicit and subtle, emotionally charged abuse. Sentiment analysis (SA) plays a key role by interpreting emotional tone, polarity, and context, offering more nuanced and timely detection than keyword-based models. Emotions like anger, sarcasm, or veiled hostility often precede cyberbullying, especially during impulsive interactions. SA captures these affective cues, improving sensitivity to implicit abuse and coded language. This study presents the first systematic comparison of sentiment-enhanced transformer models such as ALBERT, DeBERTa, ELECTRA, HateBERT, and DeepSeek-coder-1.3b-base, fine-tuned for cyberbullying detection across Twitter (currently X), IMDB, and Amazon. Models were evaluated on predictive performance (Accuracy, Precision, Recall, F1-score), time and cost efficiency (inference time, memory, CPU/GPU use, energy). ELECTRA + SA outperformed all models, achieving 91.85% accuracy, precision, and recall, and a 91.84% F1-score. It also excelled in efficiency, with 0.069 seconds inference time, 23.92 MB RAM use, 7.2% CPU/GPU usage, and 0.000075 kWh energy consumption, proving highly generalizable, sentiment-sensitive, and suitable for real-time, resource-aware deployment. These results highlight the importance of sentiment integration, dataset diversity, and computational efficiency in building scalable, real-world cyberbullying detection systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414982166",
    "type": "article"
  },
  {
    "title": "You Shall Not Pass (Without Consent): Enforcing Data Sovereignty with Solid Pods",
    "doi": "https://doi.org/10.1145/3771554",
    "publication_date": "2025-10-09",
    "publication_year": 2025,
    "authors": "Tobias Hajszan; Moritz Staudinger; Tomasz Miksa",
    "corresponding_authors": "",
    "abstract": "Privacy-preserving data analysis must carefully balance the need for secure, meaningful computation on sensitive personal data with the fundamental rights of individuals to retain control over their information. Solid (Social Linked Data) presents an open protocol where users store and manage their data in personal, access-controlled pods. However, its potential for integration as a decentralized data store into existing infrastructures for privacy-preserving computations remains underexplored. We address how Solid can be effectively integrated into such platforms to support decentralized data sharing while meeting the technical requirements of privacy-aware research. To address this, we propose the Solid Gateway , a mediator that facilitates consent-driven access to Solid Pods within existing analysis environments. The Solid Gateway introduces request-specific authentication and authorization, manages access permissions, and orchestrates the retrieval of only the data necessary to fulfill individual data requests. Central to this approach is a novel granular data-sharing strategy, which restructures user data into minimal request-specific subsets, thus reducing unnecessary data transfers and limiting the exposure of irrelevant information. This ensures that contributors retain sovereignty over their data, while allowing privacy-preserving analysis to operate on decentralized sources. Our experimental evaluation, conducted on controlled artificial datasets, confirms the feasibility of our integration. The results demonstrate a significant reduction in data exposure while achieving improved data retrieval performance compared to existing approaches. Also, we compare our proposed solution against the WellFort architecture and demonstrate that our approach offers competitive fetch performance and significantly improves processing efficiency. Although the controlled nature of the evaluation limits comparability with existing platforms, it provides a reproducible foundation for future studies and practical deployments. This work contributes a concrete, extensible design for combining Solid with privacy-preserving computation, identifies key trade-offs between privacy, performance, and system complexity, and opens pathways for future research into SPARQL integration, validation with established datasets, and the application of FAIR principles within Solid .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414982168",
    "type": "article"
  },
  {
    "title": "Web3ID: A Privacy-Preserving and DApp-Oriented Decentralized Identity Framework for Web3.0",
    "doi": "https://doi.org/10.1145/3771556",
    "publication_date": "2025-10-10",
    "publication_year": 2025,
    "authors": "Jiakun Hao; Jianbo Gao; Xiang Peng; Yifei He; L. Xia; Zhi Guan; Zhong Chen",
    "corresponding_authors": "",
    "abstract": "With the development of Web3.0, decentralized identity and other blockchain-based identity empower users with control, forming the foundational infrastructure for Web3.0 ecosystems. However, existing identity frameworks remain inadequate in addressing critical challenges such as on-chain privacy during identity management and utilization. While prior works like CanDID, Hades, and CertChain explore blockchain-based identity solutions, they fail to meet the specific reqirements of DApps. Moreover, users on blockchain always store their identity data and digital assets across multiple accounts and DApps, but current identity schemes cannot support the cross-account and DApp identity privacy-preserving utilization. To bridge this gap, we propose Web3ID, the first fully DApp-oriented identity framework. By analyzing Ethereum identity proposals and user behavior patterns, we design the Web3ID featuring: on-chain privacy-preserving identity aggregation protocol, provably secure attribute-based access control model, and zk-rollup enhanced off-chain identity management. Experiments demonstrate that Web3ID enables privacy-preserving identity management and authentication on-chain, and guaranteeing access control completeness. The prototype system achieves a 100× improvement in proof/verification efficiency and reduces storage overhead by 85× compared to pure on-chain implementation through off-chain optimization techniques. Moreover, in comparison with other identity privacy solutions, Web3ID exhibits the lowest gas consumption during on-chain utilization and shows strong scalability. As a fully decentralized identity framework supporting end-to-end DApp integration, Web3ID advances Web3.0’s vision of user sovereignty, decentralization, and interoperability. This work establishes both theoretical and practical foundations for on-chain identity systems in Web3.0 ecosystems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415028111",
    "type": "article"
  },
  {
    "title": "A Blockchain-Assisted Revocable and Efficient ABSE Scheme for Secure Medical Data Sharing",
    "doi": "https://doi.org/10.1145/3771992",
    "publication_date": "2025-10-15",
    "publication_year": 2025,
    "authors": "Hancheng Gao; WU Xiao-yu; Haiping Huang; Qi Li; Xing Yizheng",
    "corresponding_authors": "",
    "abstract": "Efficient and privacy-preserving medical data sharing remains a key challenge in the era of digital healthcare. Existing schemes often suffer from limited access control, substantial computational overhead, and reliance on trusted third parties. This paper proposes a revocable Attribute-Based Searchable Encryption (ABSE) scheme built upon a hierarchical blockchain architecture, which enables a secure keyword search over encrypted data via expressive attribute-based access policies, and a revocation mechanism is integrated to support dynamic user management. We design a lightweight and privacy-preserving computation framework that offloads expensive cryptographic operations to the cloud, thereby reducing the burden on user-side devices. Furthermore, we propose a lightweight consensus protocol, termed lottery consensus, which replaces traditional proof-of-work hash computations with meaningful operations that are tied to the ABSE-based data-sharing process. Extensive theoretical analysis and simulation experiments demonstrate the superior efficiency of the proposed scheme, and formal security proofs demonstrate its robustness against threats.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415207036",
    "type": "article"
  },
  {
    "title": "TLD-SCA: A Transformer-LSTM Detection Model against Side-Channel Attack in Blockchain Payment Channel",
    "doi": "https://doi.org/10.1145/3771990",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Tao Li; Fei Qiao; Yongrui Wang; Kiel Lu; Yan Lv; Y. Wang",
    "corresponding_authors": "",
    "abstract": "Side-channel attack, which exploits time information leakage during the digital signature generation process, poses a severe threat to the confidentiality and integrity of transactions in blockchain payment channels. Currently, Transformer-based detection methods are effective at capturing long-term dependencies, while Long Short-Term Memory (LSTM) networks excel at modeling short-term dynamic time-series features. However, existing approaches struggle to uniformly model both long-term and short-term time-series features, limiting their performance in anomaly detection for complex transaction sequences. In this paper, we propose TLD-SCA, a novel side-channel attack detection model that innovatively integrates Transformer’s capability for global dependency modeling with LSTM’s advantage in capturing local time-series dynamics. This enables long-term and short-term time-series analysis of transaction timing data. Experimental results demonstrate that TLD-SCA significantly outperforms existing methods in terms of accuracy (99.5%), precision (99.2%), and recall (98.3%), thereby providing a higher level of security assurance for blockchain payment channels.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415243194",
    "type": "article"
  },
  {
    "title": "Web3-Based Identity and KYC Innovations for Next-Generation FinTech",
    "doi": "https://doi.org/10.1145/3771991",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Usama Arshad; Abdallah Tubaishat; Sajid Anwar; Zahid Halim; Abedallah Zaid Abualkishik; Abrar Ullah",
    "corresponding_authors": "",
    "abstract": "The growing reliance on digital financial services necessitates a secure, efficient, and privacy-centric approach to identity verification and Know Your Customer (KYC) compliance. Traditional identity management systems rely on centralized databases, making them susceptible to data breaches, inefficiencies, and regulatory constraints. Over 10 billion identity records have been exposed in centralized KYC breaches, leading to a 60% increase in financial fraud cases. The rise of Decentralized Finance (DeFi) has further complicated KYC compliance, requiring innovative solutions that balance privacy and regulatory requirements. This paper proposes a Web3-powered decentralized identity framework that leverages blockchain technology, self-sovereign identity (SSI), verifiable credentials (VCs), and zero-knowledge proofs (ZKPs). By eliminating reliance on centralized authorities, our system enhances data privacy, reducing personally identifiable information (PII) disclosure by 80% while ensuring compliance with AML and GDPR regulations. The integration of zk-SNARKs enables trustless identity verification with an average proof generation time of 12.5 seconds, significantly reducing the 3-5 day verification period required by traditional systems. Smart contract-based KYC automation eliminates intermediaries, cutting compliance costs by 40% and reducing fraud risk by 60%. Through comparative analysis, we highlight that decentralized KYC improves security, cost-effectiveness, and scalability compared to traditional models. Performance evaluation confirms that transaction throughput remains within acceptable blockchain limits, with gas costs stabilized at 35,000-55,000 Gwei per verification request. Despite challenges in regulatory adaptation and zk-SNARK scalability, the proposed model demonstrates the feasibility of Web3-driven identity management for trustless, privacy-preserving, and compliant financial ecosystems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415243198",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Advances in Social Media Technologies and Analysis: Part 2",
    "doi": "https://doi.org/10.1145/3769432",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Barbara Guidi; Andrea Michienzi; Laura Ricci",
    "corresponding_authors": "",
    "abstract": "This article provides an overview of the second part of the ACM TWEB’s Special Issue on Advances in Social Media Technologies and Analysis. It highlights both research and practical applications in this field.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415247013",
    "type": "article"
  },
  {
    "title": "Host-Based P2P Flow Identification and Use in Real-Time",
    "doi": "https://doi.org/10.1145/1961659.1961661",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "John K. Hurley; Emiliano Garcia‐Palacios; Sakir Sezer",
    "corresponding_authors": "",
    "abstract": "Data identification and classification is a key task for any Internet Service Provider (ISP) or network administrator. As port fluctuation and encryption become more common in P2P applications wishing to avoid identification, new strategies must be developed to detect and classify their flows. This article introduces a method of separating P2P and standard web traffic that can be applied as part of an offline data analysis process, based on the activity of the hosts on the network. Heuristics are analyzed and a classification system proposed that focuses on classifying those “long” flows that transfer most of the bytes across a network. The accuracy of the system is then tested using real network traffic from a core Internet router showing misclassification rates as low as 0.54% of flows in some cases. We expand on this proposed strategy to investigate its relevance to real-time, early classification problems. New proposals are made and the results of real-time experiments are compared to those obtained in the offline analysis. It is shown that classification accuracies in the real-time strategy are similar to those achieved in offline analysis with a large portion of the total web and P2P flows correctly identified.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2008422411",
    "type": "article"
  },
  {
    "title": "ACConv -- An Access Control Model for Conversational Web Services",
    "doi": "https://doi.org/10.1145/1993053.1993055",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Federica Paci; Massimo Mecella; Mourad Ouzzani; Elisa Bertino",
    "corresponding_authors": "",
    "abstract": "With organizations increasingly depending on Web services to build complex applications, security and privacy concerns including the protection of access control policies are becoming a serious issue. Ideally, service providers would like to make sure that clients have knowledge of only portions of the access control policy relevant to their interactions to the extent to which they are entrusted by the Web service and without restricting the client’s choices in terms of which operations to execute. We propose ACConv , a novel model for access control in Web services that is suitable when interactions between the client and the Web service are conversational and long-running. The conversation-based access control model proposed in this article allows service providers to limit how much knowledge clients have about the credentials specified in their access policies. This is achieved while reducing the number of times credentials are asked from clients and minimizing the risk that clients drop out of a conversation with the Web service before reaching a final state due to the lack of necessary credentials. Clients are requested to provide credentials, and hence are entrusted with part of the Web service access control policies, only for some specific granted conversations which are decided based on: (1) a level of trust that the Web service provider has vis-à-vis the client, (2) the operation that the client is about to invoke, and (3) meaningful conversations which represent conversations that lead to a final state from the current one. We have implemented the proposed approach in a software prototype and conducted extensive experiments to show its effectiveness.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2129176172",
    "type": "article"
  },
  {
    "title": "Optimizing Whole-Page Presentation for Web Search",
    "doi": "https://doi.org/10.1145/3204461",
    "publication_date": "2018-07-17",
    "publication_year": 2018,
    "authors": "Yue Wang; Dawei Yin; Luo Jie; Pengyuan Wang; Makoto Yamada; Yi Chang; Qiaozhu Mei",
    "corresponding_authors": "",
    "abstract": "Modern search engines aggregate results from different verticals : webpages, news, images, video, shopping, knowledge cards, local maps, and so on. Unlike “ten blue links,” these search results are heterogeneous in nature and not even arranged in a list on the page. This revolution directly challenges the conventional “ranked list” formulation in ad hoc search. Therefore, finding proper presentation for a gallery of heterogeneous results is critical for modern search engines. We propose a novel framework that learns the optimal page presentation to render heterogeneous results onto search result page (SERP). Page presentation is broadly defined as the strategy to present a set of items on SERP, much more expressive than a ranked list. It can specify item positions, image sizes, text fonts, and any other styles as long as variations are within business and design constraints. The learned presentation is content aware, i.e., tailored to specific queries and returned results. Simulation experiments show that the framework automatically learns eye-catchy presentations for relevant results. Experiments on real data show that simple instantiations of the framework already outperform leading algorithm in federated search result presentation. It means the framework can learn its own result presentation strategy purely from data, without even knowing the “probability ranking principle.”",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2884349765",
    "type": "article"
  },
  {
    "title": "A feature-word-topic model for image annotation and retrieval",
    "doi": "https://doi.org/10.1145/2516633.2516634",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Cam-Tu Nguyen; Natsuda Kaothanthong; Takeshi Tokuyama; Xuan-Hieu Phan",
    "corresponding_authors": "",
    "abstract": "Image annotation is a process of finding appropriate semantic labels for images in order to obtain a more convenient way for indexing and searching images on the Web. This article proposes a novel method for image annotation based on combining feature-word distributions, which map from visual space to word space, and word-topic distributions, which form a structure to capture label relationships for annotation. We refer to this type of model as Feature-Word-Topic models. The introduction of topics allows us to efficiently take word associations, such as {ocean, fish, coral} or {desert, sand, cactus}, into account for image annotation. Unlike previous topic-based methods, we do not consider topics as joint distributions of words and visual features, but as distributions of words only. Feature-word distributions are utilized to define weights in computation of topic distributions for annotation. By doing so, topic models in text mining can be applied directly in our method. Our Feature-word-topic model, which exploits Gaussian Mixtures for feature-word distributions, and probabilistic Latent Semantic Analysis (pLSA) for word-topic distributions, shows that our method is able to obtain promising results in image annotation and retrieval.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1995296404",
    "type": "article"
  },
  {
    "title": "Understanding query interfaces by statistical parsing",
    "doi": "https://doi.org/10.1145/2460383.2460387",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Weifeng Su; Wu Hejun; Yafei Li; Jing Zhao; Frederick H. Lochovsky; Hongmin Cai; Tianqiang Huang",
    "corresponding_authors": "",
    "abstract": "Users submit queries to an online database via its query interface. Query interface parsing, which is important for many applications, understands the query capabilities of a query interface. Since most query interfaces are organized hierarchically, we present a novel query interface parsing method, StatParser (Statistical Parser), to automatically extract the hierarchical query capabilities of query interfaces. StatParser automatically learns from a set of parsed query interfaces and parses new query interfaces. StatParser starts from a small grammar and enhances the grammar with a set of probabilities learned from parsed query interfaces under the maximum-entropy principle. Given a new query interface, the probability-enhanced grammar identifies the parse tree with the largest global probability to be the query capabilities of the query interface. Experimental results show that StatParser very accurately extracts the query capabilities and can effectively overcome the problems of existing query interface parsers.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2029593518",
    "type": "article"
  },
  {
    "title": "Fast and Practical Snippet Generation for RDF Datasets",
    "doi": "https://doi.org/10.1145/3365575",
    "publication_date": "2019-11-16",
    "publication_year": 2019,
    "authors": "Daxin Liu; Gong Cheng; Qingxia Liu; Yuzhong Qu",
    "corresponding_authors": "",
    "abstract": "Triple-structured open data creates value in many ways. However, the reuse of datasets is still challenging. Users feel difficult to assess the usefulness of a large dataset containing thousands or millions of triples. To satisfy the needs, existing abstractive methods produce a concise high-level abstraction of data. Complementary to that, we adopt the extractive strategy and aim to select the optimum small subset of data from a dataset as a snippet to compactly illustrate the content of the dataset. This has been formulated as a combinatorial optimization problem in our previous work. In this article, we design a new algorithm for the problem, which is an order of magnitude faster than the previous one but has the same approximation ratio. We also develop an anytime algorithm that can generate empirically better solutions using additional time. To suit datasets that are partially accessible via online query services (e.g., SPARQL endpoints for RDF data), we adapt our algorithms to trade off quality of snippet for feasibility and efficiency in the Web environment. We carry out extensive experiments based on real RDF datasets and SPARQL endpoints for evaluating quality and running time. The results demonstrate the effectiveness and practicality of our proposed algorithms.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2989319206",
    "type": "article"
  },
  {
    "title": "“The Best of Both Worlds!”",
    "doi": "https://doi.org/10.1145/3372497",
    "publication_date": "2020-01-09",
    "publication_year": 2020,
    "authors": "Sukru Eraslan; Yeliz Yeşilada; Simon Harper",
    "corresponding_authors": "",
    "abstract": "Web pages are composed of different kinds of elements (menus, adverts, etc.). Segmenting pages into their elements has long been important in understanding how people experience those pages and in making those experiences “better.” Many approaches have been proposed that relate the resultant elements with the underlying source code; however, they do not consider users’ interactions. Another group of approaches analyses eye movements of users to discover areas that interest or attract them (i.e., areas of interest or AOIs). Although these approaches consider how users interact with web pages, they do not relate AOIs with the underlying source code. We propose a novel approach that integrates web page and eye tracking data driven approaches for automatic AOI detection. This approach segments an entire web page into its AOIs by considering users’ interactions and relates AOIs with the underlying source code. Based on the Adjusted Rand Index measure, our approach provides the most similar segmentation to the ground-truth segmentation compared to its individual components.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2999817231",
    "type": "article"
  },
  {
    "title": "Early Detection of Social Media Hoaxes at Scale",
    "doi": "https://doi.org/10.1145/3407194",
    "publication_date": "2020-08-18",
    "publication_year": 2020,
    "authors": "Arkaitz Zubiaga; Aiqi Jiang",
    "corresponding_authors": "",
    "abstract": "The unmoderated nature of social media enables the diffusion of hoaxes, which in turn jeopardises the credibility of information gathered from social media platforms. Existing research on automated detection of hoaxes has the limitation of using relatively small datasets, owing to the difficulty of getting labelled data. This, in turn, has limited research exploring early detection of hoaxes as well as exploring other factors such as the effect of the size of the training data or the use of sliding windows. To mitigate this problem, we introduce a semi-automated method that leverages the Wikidata knowledge base to build large-scale datasets for veracity classification, focusing on celebrity death reports. This enables us to create a dataset with 4,007 reports including over 13M tweets, 15% of which are fake. Experiments using class-specific representations of word embeddings show that we can achieve F1 scores nearing 72% within 10 minutes of the first tweet being posted when we expand the size of the training data following our semi-automated means. Our dataset represents a realistic scenario with a real distribution of true, commemorative, and false stories, which we release for further use as a benchmark in future research.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3077124311",
    "type": "article"
  },
  {
    "title": "A vlHMM approach to context-aware search",
    "doi": "https://doi.org/10.1145/2490255",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Zhen Liao; Daxin Jiang; Jian Pei; Yalou Huang; Enhong Chen; Huanhuan Cao; Hang Li",
    "corresponding_authors": "",
    "abstract": "Capturing the context of a user's query from the previous queries and clicks in the same session leads to a better understanding of the user's information need. A context-aware approach to document reranking, URL recommendation, and query suggestion may substantially improve users' search experience. In this article, we propose a general approach to context-aware search by learning a variable length hidden Markov model ( vlHMM ) from search sessions extracted from log data. While the mathematical model is powerful, the huge amounts of log data present great challenges. We develop several distributed learning techniques to learn a very large vlHMM under the map-reduce framework. Moreover, we construct feature vectors for each state of the vlHMM model to handle users' novel queries not covered by the training data. We test our approach on a raw dataset consisting of 1.9 billion queries, 2.9 billion clicks, and 1.2 billion search sessions before filtering, and evaluate the effectiveness of the vlHMM learned from the real data on three search applications: document reranking, query suggestion, and URL recommendation. The experiment results validate the effectiveness of vlHMM in the applications of document reranking, URL recommendation, and query suggestion.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2056769738",
    "type": "article"
  },
  {
    "title": "Second Chance",
    "doi": "https://doi.org/10.1145/2536777",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Rifat Ozcan; İsmail Sengör Altıngövde; B. Barla Cambazoğlu; Özgür Ulusoy",
    "corresponding_authors": "",
    "abstract": "Web search engines are known to cache the results of previously issued queries. The stored results typically contain the document summaries and some data that is used to construct the final search result page returned to the user. An alternative strategy is to store in the cache only the result document IDs, which take much less space, allowing results of more queries to be cached. These two strategies lead to an interesting trade-off between the hit rate and the average query response latency. In this work, in order to exploit this trade-off, we propose a hybrid result caching strategy where a dynamic result cache is split into two sections: an HTML cache and a docID cache. Moreover, using a realistic cost model, we evaluate the performance of different result prefetching strategies for the proposed hybrid cache and the baseline HTML-only cache. Finally, we propose a machine learning approach to predict singleton queries, which occur only once in the query stream. We show that when the proposed hybrid result caching strategy is coupled with the singleton query predictor, the hit rate is further improved.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2116105010",
    "type": "article"
  },
  {
    "title": "Discovering Best Teams for Data Leak-Aware Crowdsourcing in Social Networks",
    "doi": "https://doi.org/10.1145/2814573",
    "publication_date": "2016-02-08",
    "publication_year": 2016,
    "authors": "Iheb Ben Amor; Salima Benbernou; Mourad Ouziri; Zaki Malik; Brahim Medjahed",
    "corresponding_authors": "",
    "abstract": "Crowdsourcing is emerging as a powerful paradigm to help perform a wide range of tedious tasks in various enterprise applications. As such applications become more complex, crowdsourcing systems often require the collaboration of several experts connected through professional/social networks and organized in various teams. For instance, a well-known car manufacturer asked fans to contribute ideas for the kinds of technologies that should be incorporated into one of its cars. For that purpose, fans needed to collaborate and form teams competing with each others to come up with the best ideas. However, once teams are formed, each one would like to provide the best solution and treat that solution as a “trade secret,” hence preventing any data leak to its competitors (i.e., the other teams). In this article, we propose a data leak--aware crowdsourcing system called SocialCrowd . We introduce a clustering algorithm that uses social relationships between crowd workers to discover all possible teams while avoiding interteam data leakage. We also define a ranking mechanism to select the “best” team configurations. Our mechanism is based on the semiring approach defined in the area of soft constraints programming. Finally, we present experiments to assess the efficiency of the proposed approach.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2274963131",
    "type": "article"
  },
  {
    "title": "Dynamic, Incremental, and Continuous Detection of Cyberbullying in Online Social Media",
    "doi": "https://doi.org/10.1145/3448014",
    "publication_date": "2021-05-13",
    "publication_year": 2021,
    "authors": "Charalampos Chelmis; Daphney–Stavroula Zois",
    "corresponding_authors": "",
    "abstract": "The potentially detrimental effects of cyberbullying have led to the development of numerous automated, data-driven approaches, with emphasis on classification accuracy. Cyberbullying, as a form of abusive online behavior, although not well-defined, is a repetitive process, i.e., a sequence of aggressive messages sent from a bully to a victim over a period of time with the intent to harm the victim. Existing work has focused on harassment (i.e., using profanity to classify toxic comments independently) as an indicator of cyberbullying, disregarding the repetitive nature of this harassing process. However, raising a cyberbullying alert immediately after an aggressive comment is detected can lead to a high number of false positives. At the same time, two key practical challenges remain unaddressed: (i) detection timeliness, which is necessary to support victims as early as possible, and (ii) scalability to the staggering rates at which content is generated in online social networks. In this work, we introduce CONcISE , a novel approach for timely and accurate Cyberbullying detectiON in online social media SEssions. CONcISE is a two-stage online approach designed to reduce the time to raise a cyberbullying alert by sequentially examining comments as they become available over time, and minimizing the number of feature evaluations necessary for a decision to be made for each comment. Extensive experiments on a real-world Instagram dataset with users and comments demonstrate the effectiveness, scalability, and timeliness of our approach and its benefits over existing methods. Additional experiments using a Twitter dataset offer evidence in support of the potential generalizability of CONcISE to other social media platforms.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3161220072",
    "type": "article"
  },
  {
    "title": "Factorizing Historical User Actions for Next-Day Purchase Prediction",
    "doi": "https://doi.org/10.1145/3468227",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Bang Liu; Hanlin Zhang; Linglong Kong; Di Niu",
    "corresponding_authors": "",
    "abstract": "It is common practice for many large e-commerce operators to analyze daily logged transaction data to predict customer purchase behavior, which may potentially lead to more effective recommendations and increased sales. Traditional recommendation techniques based on collaborative filtering, although having gained success in video and music recommendation, are not sufficient to fully leverage the diverse information contained in the implicit user behavior on e-commerce platforms. In this article, we analyze user action records in the Alibaba Mobile Recommendation dataset from the Alibaba Tianchi Data Lab, as well as the Retailrocket recommender system dataset from the Retail Rocket website. To estimate the probability that a user will purchase a certain item tomorrow, we propose a new model called Time-decayed Multifaceted Factorizing Personalized Markov Chains (Time-decayed Multifaceted-FPMC), taking into account multiple types of user historical actions not only limited to past purchases but also including various behaviors such as clicks, collects and add-to-carts. Our model also considers the time-decay effect of the influence of past actions. To learn the parameters in the proposed model, we further propose a unified framework named Bayesian Sparse Factorization Machines. It generalizes the theory of traditional Factorization Machines to a more flexible learning structure and trains the Time-decayed Multifaceted-FPMC with the Markov Chain Monte Carlo method. Extensive evaluations based on multiple real-world datasets demonstrate that our proposed approaches significantly outperform various existing purchase recommendation algorithms.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3202591630",
    "type": "article"
  },
  {
    "title": "COIP—Continuous, Operable, Impartial, and Privacy-Aware Identity Validity Estimation for OSN Profiles",
    "doi": "https://doi.org/10.1145/3014338",
    "publication_date": "2016-12-13",
    "publication_year": 2016,
    "authors": "Leila Bahri; Barbara Carminati; Elena Ferrari",
    "corresponding_authors": "",
    "abstract": "Identity validation of Online Social Networks’ (OSNs’) peers is a critical concern to the insurance of safe and secure online socializing environments. Starting from the vision of empowering users to determine the validity of OSN identities, we suggest a framework to estimate the trustworthiness of online social profiles based only on the information they contain. Our framework is based on learning identity correlations between profile attributes in an OSN community and on collecting ratings from OSN community members to evaluate the trustworthiness of target profiles. Our system guarantees utility, user anonymity, impartiality in rating, and operability within the dynamics and continuous evolution of OSNs. In this article, we detail the system design, and we prove its correctness against these claimed quality properties. Moreover, we test its effectiveness, feasibility, and efficiency through experimentation on real-world datasets from Facebook and Google+, in addition to using the Adults UCI dataset.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2561829760",
    "type": "article"
  },
  {
    "title": "Into the Unknown: Exploration of Search Engines’ Responses to Users with Depression and Anxiety",
    "doi": "https://doi.org/10.1145/3580283",
    "publication_date": "2023-01-18",
    "publication_year": 2023,
    "authors": "Ashlee Milton; Maria Soledad Pera",
    "corresponding_authors": "",
    "abstract": "Researchers worldwide have explored the behavioral nuances that emerge from interactions of individuals afflicted by mental health disorders (MHD) with persuasive technologies, mainly social media. Yet, there is a gap in the analysis pertaining to a persuasive technology that is part of their everyday lives: web search engines (SE). Each day, users with MHD embark on information seeking journeys using popular SE, like Google or Bing. Every step of the search process for better or worse has the potential to influence a searcher’s mindset. In this work, we empirically investigate what subliminal stimulus SE present to these vulnerable individuals during their searches. For this, we use synthetic queries to produce associated query suggestions and search engine results pages. Then we infer the subliminal stimulus present in text from SE, i.e., query suggestions, snippets, and web resources. Findings from our empirical analysis reveal that the subliminal stimulus displayed by SE at different stages of the information seeking process differ between MHD searchers and our control group composed of “average” SE users. Outcomes from this work showcase open problems related to query suggestions, search engine result pages, and ranking that the information retrieval community needs to address so that SE can better support individuals with MHD.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4317209718",
    "type": "article"
  },
  {
    "title": "Pre-Training Across Different Cities for Next POI Recommendation",
    "doi": "https://doi.org/10.1145/3605554",
    "publication_date": "2023-06-20",
    "publication_year": 2023,
    "authors": "Ke Sun; Tieyun Qian; Chenliang Li; Xuan Ma; Qing Li; Ming Zhong; Yuanyuan Zhu; Mengchi Liu",
    "corresponding_authors": "",
    "abstract": "The Point-of-Interest (POI) transition behaviors could hold absolute sparsity and relative sparsity very differently for different cities. Hence, it is intuitive to transfer knowledge across cities to alleviate those data sparsity and imbalance problems for next POI recommendation. Recently, pre-training over a large-scale dataset has achieved great success in many relevant fields, like computer vision and natural language processing. By devising various self-supervised objectives, pre-training models can produce more robust representations for downstream tasks. However, it is not trivial to directly adopt such existing pre-training techniques for next POI recommendation, due to the lacking of common semantic objects (users or items) across different cities . Thus in this paper, we tackle such a new research problem of pre-training across different cities for next POI recommendation. Specifically, to overcome the key challenge that different cities do not share any common object, we propose a novel pre-training model named CATUS , by transferring the cat egory-level u niversal tran s ition knowledge over different cities. Firstly, we build two self-supervised objectives in CATUS : next category prediction and next POI prediction , to obtain the universal transition-knowledge across different cities and POIs. Then, we design a category-transition oriented sampler on the data level and an implicit and explicit transfer strategy on the encoder level to enhance this transfer process. At the fine-tuning stage, we propose a distance oriented sampler to better align the POI representations into the local context of each city. Extensive experiments on two large datasets consisting of four cities demonstrate the superiority of our proposed CATUS over the state-of-the-art alternatives. The code and datasets are available at https://github.com/NLPWM-WHU/CATUS.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4381379021",
    "type": "article"
  },
  {
    "title": "An Empirical Analysis of Web Storage and Its Applications to Web Tracking",
    "doi": "https://doi.org/10.1145/3623382",
    "publication_date": "2023-09-09",
    "publication_year": 2023,
    "authors": "Zubair Ahmad; Samuele Casarin; Stefano Calzavara",
    "corresponding_authors": "",
    "abstract": "In this article, we present a large-scale empirical analysis of the use of web storage in the wild.By using dynamic taint tracking at the level of JavaScript and by performing an automated classification of the detected information flows, we shed light on the key characteristics of web storage uses in the Tranco Top 10k. Our analysis shows that web storage is routinely accessed by third parties, including known web trackers, who are particularly eager to have both read and write access to persistent web storage information. We then deep dive in web tracking as a prominent case study: our analysis shows that web storage is not yet as popular as cookies for tracking purposes; however, taint tracking is useful to detect potential new trackers not included in standard filter lists. Moreover, we observe that many websites do not comply with the General Data Protection Regulation directives when it comes to their use of web storage.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386574347",
    "type": "article"
  },
  {
    "title": "Triangle-oriented Community Detection Considering Node Features and Network Topology",
    "doi": "https://doi.org/10.1145/3626190",
    "publication_date": "2023-10-02",
    "publication_year": 2023,
    "authors": "Guangliang Gao; Weichao Liang; Ming Yuan; Hanwei Qian; Qun Wang; Jie Cao",
    "corresponding_authors": "",
    "abstract": "The joint use of node features and network topology to detect communities is called community detection in attributed networks. Most of the existing work along this line has been carried out through objective function optimization and has proposed numerous approaches. However, they tend to focus only on lower-order details, i.e., capture node features and network topology from node and edge views, and purely seek a higher degree of optimization to guarantee the quality of the found communities, which exacerbates unbalanced communities and free-rider effect. To further clarify and reveal the intrinsic nature of networks, we conduct triangle-oriented community detection considering node features and network topology. Specifically, we first introduce a triangle-based quality metric to preserve higher-order details of node features and network topology, and then formulate so-called two-level constraints to encode lower-order details of node features and network topology. Finally, we develop a local search framework based on optimizing our objective function consisting of the proposed quality metric and two-level constraints to achieve both non-overlapping and overlapping community detection in attributed networks. Extensive experiments demonstrate the effectiveness and efficiency of our framework and its potential in alleviating unbalanced communities and free-rider effect.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4387259516",
    "type": "article"
  },
  {
    "title": "CoDÆN: Benchmarks and Comparison of Evolutionary Community Detection Algorithms for Dynamic Networks",
    "doi": "https://doi.org/10.1145/3718988",
    "publication_date": "2025-02-20",
    "publication_year": 2025,
    "authors": "Giordano Paoletti; Luca Gioacchini; Marco Mellia; Luca Vassio; Jussara M. Almeida",
    "corresponding_authors": "",
    "abstract": "Web data are often modelled as complex networks in which entities interact and form communities. Nevertheless, web data evolves over time, and network communities change alongside it. This makes Community Detection (CD) in dynamic graphs a relevant problem, calling for evolutionary CD algorithms. The choice and evaluation of such algorithm performance is challenging because of the lack of a comprehensive set of benchmarks and specific metrics. To address these challenges, we propose CoDÆN – COmmunity Detection Algorithms in Evolving Networks – a benchmarking framework for evolutionary CD algorithms in dynamic networks, that we offer as open source to the community. CoDÆN allows us to generate synthetic community-structured graphs with known ground truth and design evolving scenarios combining nine basic graph transformations that modify edges, nodes, and communities. We propose three complementary metrics (i.e. Correctness, Delay, and Stability) to compare evolutionary CD algorithms. Armed with CoDÆN, we consider three evolutionary modularity-based CD approaches, dissecting their performance to gauge the trade-off between the stability of the communities and their correctness. Next, we compare the algorithms in real Web-oriented datasets, confirming such a trade-off. Our findings reveal that algorithms that introduce memory in the graph maximise stability but add delay when abrupt changes occur. Conversely, algorithms that introduce memory by initialising the CD algorithms with the previous solution fail to identify the split and birth of new communities. These observations underscore the value of CoDÆN in facilitating the study and comparison of alternative evolutionary community detection algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407786033",
    "type": "article"
  },
  {
    "title": "Towards Effective Time-Aware Language Representation: Exploring Enhanced Temporal Understanding in Language Models",
    "doi": "https://doi.org/10.1145/3723352",
    "publication_date": "2025-03-13",
    "publication_year": 2025,
    "authors": "Jiexin Wang; Adam Jatowt; Yi Cai",
    "corresponding_authors": "",
    "abstract": "In the evolving field of Natural Language Processing (NLP), understanding the temporal context of text is increasingly critical for applications requiring advanced temporal reasoning. Traditional pre-trained language models like BERT, which rely on synchronic document collections such as BookCorpus and Wikipedia, often fall short in effectively capturing and leveraging temporal information. To address this limitation, we introduce BiTimeBERT 2.0, a novel time-aware language model pre-trained on a temporal news article collection. BiTimeBERT 2.0 incorporates temporal information through three innovative pre-training objectives: Extended Time-Aware Masked Language Modeling (ETAMLM), Document Dating (DD), and Time-Sensitive Entity Replacement (TSER). Each objective is specifically designed to target a distinct dimension of temporal information: ETAMLM enhances the model’s understanding of temporal contexts and relations, DD integrates document timestamps as explicit chronological markers, and TSER focuses on the temporal dynamics of ”Person” entities. Moreover, our refined corpus preprocessing strategy reduces training time by nearly 53%, making BiTimeBERT 2.0 significantly more efficient while maintaining high performance. Experimental results show that BiTimeBERT 2.0 achieves substantial improvements across a broad range of time-related tasks and excels on datasets spanning extensive temporal ranges. These findings underscore BiTimeBERT 2.0’s potential as a powerful tool for advancing temporal reasoning in NLP.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408412978",
    "type": "article"
  },
  {
    "title": "Towards More Personalized Recommendations by Modeling Users? Temporal Behaviors with Task-Based Graph Neural Network (TGNN)",
    "doi": "https://doi.org/10.1145/3736159",
    "publication_date": "2025-05-14",
    "publication_year": 2025,
    "authors": "Maryam Amirizaniani; Shawon Sarkar; Chirag Shah",
    "corresponding_authors": "",
    "abstract": "A recommender system is tasked with effectively analyzing a user’s preferences and interactions to provide personalized recommendations. This calls for extracting and connecting various heterogeneous data while preserving their temporal relations. Graph neural networks (GNNs) have proven to be highly suitable in recommendation systems for connecting different types of user behavioral signals. However, they inherently lack ability to capture temporal aspects of underlying data. This shortcoming prevents them from explicating and utilizing task information, which is shown to be instrumental in many information retrieval applications. To overcome this limitation, we propose a new Task-based Graph Neural Network model (TGNN) focusing on identifying users’ underlying tasks within their temporal multi-behavior, specifically in each session. The model consists of three modules: (1) a sequential meta-path module that captures a temporal sequence of users’ behaviors; (2) a graph neural network layer that models the relationships between different information items and users into task representations; and (3) a recommendation layer that utilizes a collaborative filtering method to generate top-N recommendations based on the model’s comprehension of users’ tasks. The novelty of our approach lies in understanding users’ tasks through their temporal behavior, enabling more accurate personalization. The results of evaluative experiments on three publicly available datasets demonstrate the effectiveness of our task-based recommendation model compared to 10 baselines and indicate a promising research direction for task-oriented recommender systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410374369",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Advances in Social Media Technologies and Analysis: Part 1",
    "doi": "https://doi.org/10.1145/3733835",
    "publication_date": "2025-05-24",
    "publication_year": 2025,
    "authors": "Barbara Guidi; Andrea Michienzi; Laura Ricci",
    "corresponding_authors": "",
    "abstract": "This article provides an overview of the first part of the ACM TWEB’s Special Issue on Advances in Social Media Technologies and Analysis. It highlights both research and practical applications in this field.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410706315",
    "type": "article"
  },
  {
    "title": "A Graph-Based Framework for Temporal and Causal Analysis of Sentiments",
    "doi": "https://doi.org/10.1145/3759440",
    "publication_date": "2025-08-08",
    "publication_year": 2025,
    "authors": "E. Subha; Jothi Prakash; S. Arul Antran Vijay",
    "corresponding_authors": "",
    "abstract": "This research aims to develop a novel framework that uncovers the causal influence of global events on public sentiment through temporal graph modeling and neural causal inference. Global events, such as pandemics, elections, and economic crises, profoundly affect public sentiments, shaping social behaviors and economic outcomes. Traditional models often fall short in capturing the complex, dynamic, and non-linear relationships between these events and sentiments. This paper presents the Neural Temporal Causal Graph Network (NTCGN), a unified framework that integrates temporal graph neural networks with a Causal Attention Network (CAN) to model and interpret these relationships. NTCGN constructs a temporal graph from event data and sentiment-labeled texts, learning dependencies and causal influences through advanced neural architectures. A thorough comparative analysis with state-of-the-art models such as Logistic Regression, SVM, LSTM, and transformer-based models demonstrates NTCGN’s superior performance. Experimental evaluation using the Sentiment140 and Global Database of Events, Language and Tone (GDELT) 2.0 datasets shows NTCGN achieving an accuracy of 0.798 and an F1 score of 0.795, outperforming these baseline models. The model’s causal inference capabilities are validated using the Causal Impact Score (CIS) and Causal Discovery Precision (CDP), highlighting its reliability in identifying true causal links. Visualizations of attention maps and causal pathways enhance interpretability, demonstrating how specific events influence public sentiments. This work provides a robust and interpretable tool for analyzing event-driven sentiment dynamics in real-world applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413251915",
    "type": "article"
  },
  {
    "title": "A Regulatable Blockchain Rewriting Scheme for Identity-Aware Data Modification and User Identity Update",
    "doi": "https://doi.org/10.1145/3765896",
    "publication_date": "2025-09-08",
    "publication_year": 2025,
    "authors": "Fu Zhang; Yuqing Zhang; Zhaofeng Ma; Lin Sun; Yushi Shen",
    "corresponding_authors": "",
    "abstract": "The immutability of blockchain technology, while fundamental to its trustworthiness, introduces significant challenges for dynamic data governance scenarios such as financial transaction corrections, privacy-preserving healthcare data updates, and GDPR compliance. Existing blockchain rewriting schemes based on chameleon hash techniques alleviate some of these issues but suffer from inherent limitations, including complex key management overhead, lack of regulatory mechanisms, and inflexible access policy enforcement. This paper introduces a chameleon hash that supports the identity update, called IDCHU , which is a novel cryptographic primitive integrating identity-based proxy re-encryption and chameleon hash with ephemeral trapdoors to enable secure, traceable and identity-aware data modifications while supporting flexible user identity updates. The proposed regulable blockchain rewriting scheme built upon IDCHU algorithm ensures compliance through three core mechanisms: identity-bound modification authority, time-constrained modification and cryptographic traceability recorded on a regulatory blockchain. We provide formal security proofs for both the IDCHU algorithm and the blockchain rewriting scheme. The experimental evaluations and comparisons validate the scheme’s efficiency, showcasing its practical viability for real-world applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414062860",
    "type": "article"
  },
  {
    "title": "Ads-portal domains",
    "doi": "https://doi.org/10.1145/1734200.1734201",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Mishari Almishari; Xiaowei Yang",
    "corresponding_authors": "",
    "abstract": "An ads-portal domain refers to a Web domain that shows only advertisements, served by a third-party advertisement syndication service, in the form of ads listing. We develop a machine-learning-based classifier to identify ads-portal domains, which has 96% accuracy. We use this classifier to measure the prevalence of ads-portal domains on the Internet. Surprisingly, 28.3/25% of the (two-level) *. com /*. net web domains are ads-portal domains. Also, 41/39.8% of *. com /*. net ads-portal domains are typos of well-known domains, also known as typo-squatting domains. In addition, we use the classifier along with DNS trace files to estimate how often Internet users visit ads-portal domains. It turns out that ∼5% of the two-level *. com , *. net , *. org , *. biz and *. info web domains on the traces are ads-portal domains and ∼50% of these accessed ads-portal domains are typos. These numbers show that ads-portal domains and typo-squatting ads-portal domains are prevalent on the Internet and successful in attracting many visits. Our classifier represents a step towards better categorizing the web documents. It can also be helpful to search engines ranking algorithms, helpful in identifying web spams that redirects to ads-portal domains, and used to discourage access to typo-squatting ads-portal domains.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2055674801",
    "type": "article"
  },
  {
    "title": "AjaxScope",
    "doi": "https://doi.org/10.1145/1841909.1841910",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Emre Kıcıman; Benjamin Livshits",
    "corresponding_authors": "",
    "abstract": "The rise of the software-as-a-service paradigm has led to the development of a new breed of sophisticated, interactive applications often called Web 2.0. While Web applications have become larger and more complex, Web application developers today have little visibility into the end-to-end behavior of their systems. This article presents AjaxScope, a dynamic instrumentation platform that enables cross-user monitoring and just-in-time control of Web application behavior on end-user desktops. AjaxScope is a proxy that performs on-the-fly parsing and instrumentation of JavaScript code as it is sent to users’ browsers. AjaxScope provides facilities for distributed and adaptive instrumentation in order to reduce the client-side overhead, while giving fine-grained visibility into the code-level behavior of Web applications. We present a variety of policies demonstrating the power of AjaxScope, ranging from simple error reporting and performance profiling to more complex memory leak detection and optimization analyses. We also apply our prototype to analyze the behavior of over 90 Web 2.0 applications and sites that use significant amounts of JavaScript.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2121048739",
    "type": "article"
  },
  {
    "title": "A Specialized Search Assistant for Learning Objects",
    "doi": "https://doi.org/10.1145/2019643.2019648",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Cecilia Curlango-Rosas; Gregorio A. Ponce; Gabriel López-Morteo",
    "corresponding_authors": "",
    "abstract": "The Web holds a great quantity of material that can be used to enhance classroom instruction. However, it is not easy to retrieve this material with the search engines currently available. This study produced a specialized search assistant based on Google that significantly increases the number of instances in which teachers find the desired learning objects as compared to using this popular public search engine directly. Success in finding learning objects by study participants went from 80% using Google alone to 96% when using our search assistant in one scenario and, in another scenario, from a 40% success rate with Google alone to 66% with our assistant. This specialized search assistant implements features such as bilingual search and term suggestion which were requested by teacher participants to help improve their searches. Study participants evaluated the specialized search assistant and found it significantly easier to use and more useful than the popular search engine for the purpose of finding learning objects.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2096569231",
    "type": "article"
  },
  {
    "title": "Workload Characterization and Performance Implications of Large-Scale Blog Servers",
    "doi": "https://doi.org/10.1145/2382616.2382619",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Myeongjae Jeon; Youngjae Kim; Jeaho Hwang; Joonwon Lee; Euiseong Seo",
    "corresponding_authors": "",
    "abstract": "With the ever-increasing popularity of Social Network Services (SNSs), an understanding of the characteristics of these services and their effects on the behavior of their host servers is critical. However, there has been a lack of research on the workload characterization of servers running SNS applications such as blog services. To fill this void, we empirically characterized real-world Web server logs collected from one of the largest South Korean blog hosting sites for 12 consecutive days. The logs consist of more than 96 million HTTP requests and 4.7TB of network traffic. Our analysis reveals the following: (i) The transfer size of nonmultimedia files and blog articles can be modeled using a truncated Pareto distribution and a log-normal distribution, respectively; (ii) user access for blog articles does not show temporal locality, but is strongly biased towards those posted with image or audio files. We additionally discuss the potential performance improvement through clustering of small files on a blog page into contiguous disk blocks, which benefits from the observed file access patterns. Trace-driven simulations show that, on average, the suggested approach achieves 60.6% better system throughput and reduces the processing time for file access by 30.8% compared to the best performance of the Ext4 filesystem.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2129246820",
    "type": "article"
  },
  {
    "title": "An Empirical Investigation of Ecommerce-Reputation-Escalation-as-a-Service",
    "doi": "https://doi.org/10.1145/2983646",
    "publication_date": "2017-05-12",
    "publication_year": 2017,
    "authors": "Haitao Xu; Daiping Liu; Haining Wang; Angelos Stavrou",
    "corresponding_authors": "",
    "abstract": "In online markets, a store’s reputation is closely tied to its profitability. Sellers’ desire to quickly achieve a high reputation has fueled a profitable underground business that operates as a specialized crowdsourcing marketplace and accumulates wealth by allowing online sellers to harness human laborers to conduct fake transactions to improve their stores’ reputations. We term such an underground market a seller-reputation-escalation (SRE) market . In this article, we investigate the impact of the SRE service on reputation escalation by performing in-depth measurements of the prevalence of the SRE service, the business model and market size of SRE markets, and the characteristics of sellers and offered laborers. To this end, we have infiltrated five SRE markets and studied their operations using daily data collection over a continuous period of 2 months. We identified more than 11,000 online sellers posting at least 219,165 fake-purchase tasks on the five SRE markets. These transactions earned at least $46,438 in revenue for the five SRE markets, and the total value of merchandise involved exceeded $3,452,530. Our study demonstrates that online sellers using the SRE service can increase their stores’ reputations at least 10 times faster than legitimate ones while about 25% of them were visibly penalized. Even worse, we found a much stealthier and more hazardous service that can, within a single day, boost a seller’s reputation by such a degree that would require a legitimate seller at least a year to accomplish. Armed with our analysis of the operational characteristics of the underground economy, we offer some insights into potential mitigation strategies. Finally, we revisit the SRE ecosystem 1 year later to evaluate the latest dynamism of the SRE markets, especially the statuses of the online stores once identified to launch fake-transaction campaigns on the SRE markets. We observe that the SRE markets are not as active as they were 1 year ago and about 17% of the involved online stores become inaccessible likely because they have been forcibly shut down by the corresponding E-commerce marketplace for conducting fake transactions.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2616619947",
    "type": "article"
  },
  {
    "title": "Multirelational Recommendation in Heterogeneous Networks",
    "doi": "https://doi.org/10.1145/3054952",
    "publication_date": "2017-06-23",
    "publication_year": 2017,
    "authors": "Fatemeh Vahedian; Robin Burke; Bamshad Mobasher",
    "corresponding_authors": "",
    "abstract": "Recommender systems are key components in information-seeking contexts where personalization is sought. However, the dominant framework for recommendation is essentially two dimensional, with the interaction between users and items characterized by a single relation. In many cases, such as social networks, users and items are joined in a complex web of relations, not readily reduced to a single value. Recent multirelational approaches to recommendation focus on the direct, proximal relations in which users and items may participate. Our approach uses the framework of complex heterogeneous networks to represent such recommendation problems. We propose the weighted hybrid of low-dimensional recommenders (WHyLDR) recommendation model, which uses extended relations, represented as constrained network paths, to effectively augment direct relations. This model incorporates influences from both distant and proximal connections in the network. The WHyLDR approach raises the problem of the unconstrained proliferation of components, built from ever-extended network paths. We show that although component utility is not strictly monotonic with path length, a measure based on information gain can effectively prune and optimize such hybrids.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2684996256",
    "type": "article"
  },
  {
    "title": "WISeR",
    "doi": "https://doi.org/10.1145/3061710",
    "publication_date": "2017-07-03",
    "publication_year": 2017,
    "authors": "Devis Bianchini; Valeria De Antonellis; Michele Melchiori",
    "corresponding_authors": "",
    "abstract": "Mashups are agile applications that aggregate RESTful services, developed by third parties, whose functions are exposed as Web Application Program Interfaces (APIs) within public repositories. From mashups developers’ viewpoint, Web API search may benefit from selection criteria that combine several dimensions used to describe the APIs, such as categories, tags, and technical features (e.g., protocols and data formats). Nevertheless, other dimensions might be fruitfully exploited to support Web API search. Among them, past API usage experiences by other developers may be used to suggest the right APIs for a target application. Past experiences might emerge from the co-occurrence of Web APIs in the same mashups. Ratings assigned by developers after using the Web APIs to create their own mashups or after using mashups developed by others can be considered as well. This article aims to advance the current state of the art for Web API search and ranking from mashups developers’ point of view, by addressing two key issues: multi-dimensional modeling and multi-dimensional framework for selection. The model for Web API characterization embraces multiple descriptive dimensions, by considering several public repositories, that focus on different and only partially overlapping dimensions. The proposed Web API selection framework, called WISeR (Web apI Search and Ranking), is based on functions devoted to developers to exploit the multi-dimensional descriptions, in order to enhance the identification of candidate Web APIs to be proposed, according to the given requirements. Furthermore, WISeR adapts to changes that occur during the Web API selection and mashup development, by revising the dimensional attributes in order to conform to developers’ preferences and constraints. We also present an experimental evaluation of the framework.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2726295096",
    "type": "article"
  },
  {
    "title": "Exploring and Analysing the African Web Ecosystem",
    "doi": "https://doi.org/10.1145/3213897",
    "publication_date": "2018-09-27",
    "publication_year": 2018,
    "authors": "Rodérick Fanou; Gareth Tyson; Eder Leão Fernandes; Pierre François; Francisco Valera; Arjuna Sathiaseelan",
    "corresponding_authors": "",
    "abstract": "It is well known that internet infrastructure deployment is progressing at a rapid pace in the African continent. A flurry of recent research has quantified this, highlighting the expansion of its underlying connectivity network. However, improving the infrastructure is not useful without appropriately provisioned services to exploit it. This article measures the availability and utilisation of web infrastructure in Africa. Whereas others have explored web infrastructure in developed regions, we shed light on practices in developing regions. To achieve this, we apply a comprehensive measurement methodology to collect data from a variety of sources. We first focus on Google to reveal that its content infrastructure in Africa is, indeed, expanding. That said, we find that much of its web content is still served from the US and Europe, despite being the most popular website in many African countries. We repeat the same analysis across a number of other regionally popular websites to find that even top African websites prefer to host their content abroad. To explore the reasons for this, we evaluate some of the major bottlenecks facing content delivery networks (CDNs) in Africa. Amongst other factors, we find a lack of peering between the networks hosting our probes, preventing the sharing of CDN servers, as well as poorly configured DNS resolvers. Finally, our mapping of middleboxes in the region reveals that there is a greater presence of transparent proxies in Africa than in Europe or the US. We conclude the work with a number of suggestions for alleviating the issues observed.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2894191260",
    "type": "article"
  },
  {
    "title": "Improving Researcher Homepage Classification with Unlabeled Data",
    "doi": "https://doi.org/10.1145/2767135",
    "publication_date": "2015-10-19",
    "publication_year": 2015,
    "authors": "Sujatha Das Gollapalli; Cornelia Caragea; Prasenjit Mitra; C. Lee Giles",
    "corresponding_authors": "",
    "abstract": "A classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of changing content on the Web? We investigate this question in the context of identifying researcher homepages. We show experimentally that classifiers trained on existing datasets of academic homepages underperform on “non-homepages” present on current-day academic websites. As an alternative to obtaining labeled datasets to retrain classifiers for the new content, in this article we ask the following question: “How can we effectively use the unlabeled data readily available from academic websites to improve researcher homepage classification?” We design novel URL-based features and use them in conjunction with content-based features for representing homepages. Within the co-training framework, these sets of features can be treated as complementary views enabling us to effectively use unlabeled data and obtain remarkable improvements in homepage identification on the current-day academic websites. We also propose a novel technique for “learning a conforming pair of classifiers” that mimics co-training. Our algorithm seeks to minimize a loss (objective) function quantifying the difference in predictions from the two views afforded by co-training. We argue that this loss formulation provides insights for understanding co-training and can be used even in the absence of a validation dataset. Our next set of findings pertains to the evaluation of other state-of-the-art techniques for classifying homepages. First, we apply feature selection (FS) and feature hashing (FH) techniques independently and in conjunction with co-training to academic homepages. FS is a well-known technique for removing redundant and unnecessary features from the data representation, whereas FH is a technique that uses hash functions for efficient encoding of features. We show that FS can be effectively combined with co-training to obtain further improvements in identifying homepages. However, using hashed feature representations, a performance degradation is observed possibly due to feature collisions. Finally, we evaluate other semisupervised algorithms for homepage classification. We show that although several algorithms are effective in using information from the unlabeled instances, co-training that explicitly harnesses the feature split in the underlying instances outperforms approaches that combine content and URL features into a single view.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1980086685",
    "type": "article"
  },
  {
    "title": "Robust detection of semi-structured web records using a DOM structure-knowledge-driven model",
    "doi": "https://doi.org/10.1145/2508434",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Lidong Bing; Wai Lam; Tak-Lam Wong",
    "corresponding_authors": "",
    "abstract": "Web data record extraction aims at extracting a set of similar object records from a single webpage. These records have similar attributes or fields and are presented with a regular format in a coherent region of the page. To tackle this problem, most existing works analyze the DOM tree of an input page. One major limitation of these methods is that the lack of a global view in detecting data records from an input page results in a myopic decision. Their brute-force searching manner in detecting various types of records degrades the flexibility and robustness. We propose a Structure-Knowledge-Oriented Global Analysis (Skoga) framework which can perform robust detection of different-kinds of data records and record regions. The major component of the Skoga framework is a DOM structure-knowledge-driven detection model which can conduct a global analysis on the DOM structure to achieve effective detection. The DOM structure knowledge consists of background knowledge as well as statistical knowledge capturing different characteristics of data records and record regions, as exhibited in the DOM structure. The background knowledge encodes the semantics of labels indicating general constituents of data records and regions. The statistical knowledge is represented by some carefully designed features that capture different characteristics of a single node or a node group in the DOM. The feature weights are determined using a development dataset via a parameter estimation algorithm based on a structured output support vector machine. An optimization method based on the divide-and-conquer principle is developed making use of the DOM structure knowledge to quantitatively infer and recognize appropriate records and regions for a page. Extensive experiments have been conducted on four datasets. The experimental results demonstrate that our framework achieves higher accuracy compared with state-of-the-art methods.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2049020147",
    "type": "article"
  },
  {
    "title": "Information Sharing by Viewers Via Second Screens for In-Real-Life Events",
    "doi": "https://doi.org/10.1145/3009970",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Partha Mukherjee; Bernard J. Jansen",
    "corresponding_authors": "",
    "abstract": "The use of second screen devices with social media facilitates conversational interaction concerning broadcast media events, creating what we refer to as the social soundtrack. In this research, we evaluate the change of the Super Bowl XLIX social soundtrack across three social media platforms on the topical categories of commercials, music, and game at three game phases ( Pre , During , and Post ). We perform statistical analysis on more than 3M, 800K, and 50K posts from Twitter, Instagram, and Tumblr, respectively. Findings show that the volume of posts in the During phase is fewer compared to Pre and Post phases; however, the hourly mean in the During phase is considerably higher than it is in the other two phases. We identify the predominant phase and category of interaction across all three social media sites. We also determine the significance of change in absolute scale across the Super Bowl categories (commercials, music, game) and in both absolute and relative scales across Super Bowl phases ( Pre , During , Post ) for the three social network platforms (Twitter, Tumblr, Instagram). Results show that significant phase-category relationships exist for all three social networks. The results identify the During phase as the predominant one for all three categories on all social media sites with respect to the absolute volume of conversations in a continuous scale. From the relative volume perspective, the During phase is highest for the music category for most social networks. For the commercials and game categories, however, the Post phase is higher than the During phase for Twitter and Instagram, respectively. Regarding category identification, the game category is the highest for Twitter and Instagram but not for Tumblr, which has dominant peaks for music and/or commercials in all three phases. It is apparent that different social media platforms offer various phase and category affordances. These results are important in identifying the influence that second screen technology has on information sharing across different social media platforms and indicates that the viewer role is transitioning from passive to more active.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2596344462",
    "type": "article"
  },
  {
    "title": "What Web Template Extractor Should I Use? A Benchmarking and Comparison for Five Template Extractors",
    "doi": "https://doi.org/10.1145/3316810",
    "publication_date": "2019-03-27",
    "publication_year": 2019,
    "authors": "Julián Alarte; Josep Silva; Salvador Tamarit",
    "corresponding_authors": "",
    "abstract": "A Web template is a resource that implements the structure and format of a website, making it ready for plugging content into already formatted and prepared pages. For this reason, templates are one of the main development resources for website engineers, because they increase productivity. Templates are also useful for the final user, because they provide uniformity and a common look and feel for all webpages. However, from the point of view of crawlers and indexers, templates are an important problem, because templates usually contain irrelevant information, such as advertisements, menus, and banners. Processing and storing this information leads to a waste of resources (storage space, bandwidth, etc.). It has been measured that templates represent between 40% and 50% of data on the Web. Therefore, identifying templates is essential for indexing tasks. There exist many techniques and tools for template extraction, but, unfortunately, it is not clear at all which template extractor should a user/system use, because they have never been compared, and because they present different (complementary) features such as precision, recall, and efficiency. In this work, we compare the most advanced template extractors. We implemented and evaluated five of the most advanced template extractors in the literature. To compare all of them, we implemented a workbench, where they have been integrated and evaluated. Thanks to this workbench, we can provide a fair empirical comparison of all methods using the same benchmarks, technology, implementation language, and evaluation criteria.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2925629331",
    "type": "article"
  },
  {
    "title": "An Experimental Study of Automatic Detection and Measurement of Counterfeit in Brand Search Results",
    "doi": "https://doi.org/10.1145/3378443",
    "publication_date": "2020-02-07",
    "publication_year": 2020,
    "authors": "Claudio Carpineto; Giovanni Romano",
    "corresponding_authors": "",
    "abstract": "Brand search results are poisoned by fake ecommerce websites that infringe on the trademark rights of legitimate holders. In this article, we study how to tackle and measure this problem automatically. We present a pipeline with two machine learning stages that can detect the ecommerce websites present in the list of brand search results and distinguish between legitimate and fake ecommerce websites. For each classification task, we identify and extract suitable learning features and study their relative importance. Through a prototype system termed RI.SI.CO., we show that this approach is feasible, fast, and more accurate than both existing systems for trustworthiness assessment and non-expert humans. We next introduce two complementary metrics for evaluating the counterfeit incidence in brand search results: namely, a chart-based and a single-value measure. They allow us to analyze and compare counterfeit at various levels, including single brands within a specific sector as well as whole sectors. Experimenting with two luxury goods sectors, we report a number of interesting findings about how the main search parameters (e.g., search engine, query type, number of search results seen) affect counterfeiting and how this activity changes with time. On the whole, our research offers new insights and some very practical and useful means of analyzing and measuring counterfeit in brand search results, thus increasing awareness of and knowledge about this phenomenon and enabling targeted anti-counterfeiting actions.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3014303966",
    "type": "article"
  },
  {
    "title": "Exploring Weather Data to Predict Activity Attendance in Event-based Social Network",
    "doi": "https://doi.org/10.1145/3440134",
    "publication_date": "2021-04-22",
    "publication_year": 2021,
    "authors": "Jifeng Zhang; Wenjun Jiang; Jinrui Zhang; Jie Wu; Guojun Wang",
    "corresponding_authors": "",
    "abstract": "Event-based social networks (EBSNs) connect online and offline lives. They allow online users with similar interests to get together in real life. Attendance prediction for activities in EBSNs has attracted a lot of attention and several factors have been studied. However, the prediction accuracy is not very good for some special activities, such as outdoor activities. Moreover, a very important factor, the weather, has not been well exploited. In this work, we strive to understand how the weather factor impacts activity attendance, and we explore it to improve attendance prediction from the organizer’s view. First, we classify activities into two categories: the outdoor and the indoor activities. We study the different ways that weather factors may impact these two kinds of activities. We also introduce a new factor of event duration. By integrating the above factors with user interest and user-event distance, we build a model of attendance prediction with the weather named GBT-W , based on the Gradient Boosting Tree. Furthermore, we develop a platform to help event organizers estimate the possible number of activity attendance with different settings (e.g., different weather, location) to effectively plan their events. We conduct extensive experiments, and the results show that our method has a better prediction performance on both the outdoor and the indoor activities, which validates the reasonability of considering weather and duration.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3158907905",
    "type": "article"
  },
  {
    "title": "A Large-scale Empirical Analysis of Browser Fingerprints Properties for Web Authentication",
    "doi": "https://doi.org/10.1145/3478026",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Nampoina Andriamilanto; Tristan Allard; Gaëtan Le Guelvouit; Alexandre Garel",
    "corresponding_authors": "",
    "abstract": "Modern browsers give access to several attributes that can be collected to form a browser fingerprint. Although browser fingerprints have primarily been studied as a web tracking tool, they can contribute to improve the current state of web security by augmenting web authentication mechanisms. In this article, we investigate the adequacy of browser fingerprints for web authentication. We make the link between the digital fingerprints that distinguish browsers, and the biological fingerprints that distinguish Humans, to evaluate browser fingerprints according to properties inspired by biometric authentication factors. These properties include their distinctiveness, their stability through time, their collection time, their size, and the accuracy of a simple verification mechanism. We assess these properties on a large-scale dataset of 4,145,408 fingerprints composed of 216 attributes and collected from 1,989,365 browsers. We show that, by time-partitioning our dataset, more than 81.3% of our fingerprints are shared by a single browser. Although browser fingerprints are known to evolve, an average of 91% of the attributes of our fingerprints stay identical between two observations, even when separated by nearly six months. About their performance, we show that our fingerprints weigh a dozen of kilobytes and take a few seconds to collect. Finally, by processing a simple verification mechanism, we show that it achieves an equal error rate of 0.61%. We enrich our results with the analysis of the correlation between the attributes and their contribution to the evaluated properties. We conclude that our browser fingerprints carry the promise to strengthen web authentication mechanisms.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3201676293",
    "type": "article"
  },
  {
    "title": "Diversionary Comments under Blog Posts",
    "doi": "https://doi.org/10.1145/2789211",
    "publication_date": "2015-09-24",
    "publication_year": 2015,
    "authors": "Jing Wang; Clement Yu; Philip S. Yu; Bing Liu; Weiyi Meng",
    "corresponding_authors": "",
    "abstract": "There has been a recent swell of interest in the analysis of blog comments. However, much of the work focuses on detecting comment spam in the blogsphere. An important issue that has been neglected so far is the identification of diversionary comments. Diversionary comments are defined as comments that divert the topic from the original post. A possible purpose is to distract readers from the original topic and draw attention to a new topic. We categorize diversionary comments into five types based on our observations and propose an effective framework to identify and flag them. To the best of our knowledge, the problem of detecting diversionary comments has not been studied so far. We solve the problem in two different ways: (i) rank all comments in descending order of being diversionary and (ii) consider it as a classification problem. Our evaluation on 4,179 comments under 40 different blog posts from Digg and Reddit shows that the proposed method achieves the high mean average precision of 91.9% when the problem is considered as a ranking problem and 84.9% of F-measure as a classification problem. Sensitivity analysis indicates that the effectiveness of the method is stable under different parameter settings.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1975236265",
    "type": "article"
  },
  {
    "title": "Fona",
    "doi": "https://doi.org/10.1145/2812812",
    "publication_date": "2015-09-24",
    "publication_year": 2015,
    "authors": "Willian Massami Watanabe; Ana Luiza Dias; Renata Pontin de Mattos Fortes",
    "corresponding_authors": "",
    "abstract": "The Web 2.0 brought new requirements to the architecture of web systems. Web applications’ interfaces are becoming more and more interactive. However, these changes are severely impacting how disabled users interact through assistive technologies with the web. In order to deploy an accessible web application, developers can use WAI-ARIA to design an accessible web application, which manually implements focus and keyboard navigation mechanisms. This article presents a quantitative metric, named Fona, which measures how the Focus Navigation WAI-ARIA requirement has been implemented on the web. Fona counts JavaScript mouse event listeners, HTML elements with role attributes, and TabIndex attributes in the DOM structure of webpages. Fona’s evaluation approach provides a narrow analysis of one single accessibility requirement. But it enables monitoring this accessibility requirement in a large number of webpages. This monitoring activity might be used to give insights about how Focus Navigation and ARIA requirements have been considered by web development teams. Fona is validated comparing the results of a set of WAI-ARIA conformant implementations and a set of webpages formed by Alexa’s 349 top most popular websites. The analysis of Fona’s value for Alexa’s websites highlights that many websites still lack the implementation of Focus Navigation through their JavaScript interactive content.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2044097028",
    "type": "article"
  },
  {
    "title": "Detection of Political Manipulation in Online Communities through Measures of Effort and Collaboration",
    "doi": "https://doi.org/10.1145/2767134",
    "publication_date": "2015-06-12",
    "publication_year": 2015,
    "authors": "Sihyung Lee",
    "corresponding_authors": "Sihyung Lee",
    "abstract": "Online social media allow users to interact with one another by sharing opinions, and these opinions have a critical impact on the way readers think and behave. Accordingly, an increasing number of &lt;i&gt;manipulators&lt;/i&gt; deliberately spread messages to influence the public, often in an organized manner. In particular, political manipulation—manipulation of opponents to win political advantage—can result in serious consequences: antigovernment riots can break out, leading to candidates’ defeat in an election. A few approaches have been proposed to detect such manipulation based on the level of social interaction (i.e., manipulators actively post opinions but infrequently befriend and reply to other users). However, several studies have shown that the interactions can be forged at a low cost and thus may not be effective measures of manipulation. To go one step further, we collect a dataset for real, large-scale political manipulation, which consists of opinions found on Internet forums. These opinions are divided into manipulators and nonmanipulators. Using this collection, we demonstrate that manipulators inevitably work hard, in teams, to quickly influence a large audience. With this in mind, it could be said that a high level of collaborative efforts strongly indicates manipulation. For example, a group of manipulators may jointly post numerous opinions with a consistent theme and selectively recommend the same, well-organized opinion to promote its rank. We show that the effort measures, when combined with a supervised learning algorithm, successfully identify greater than 95% of the manipulators. We believe that the proposed method will help system administrators to accurately detect manipulators in disguise, significantly decreasing the intensity of manipulation.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2266026389",
    "type": "article"
  },
  {
    "title": "DeLink: An Adversarial Framework for Defending against Cross-site User Identity Linkage",
    "doi": "https://doi.org/10.1145/3643828",
    "publication_date": "2024-02-05",
    "publication_year": 2024,
    "authors": "Peng Zhang; Qi Zhou; Tun Lu; Hansu Gu; Ning Gu",
    "corresponding_authors": "",
    "abstract": "Cross-site user identity linkage (UIL) aims to link the identities of the same person across different social media platforms. Social media practitioners and service providers can construct composite user portraits based on cross-site UIL, which helps understand user behavior holistically and conduct accurate recommendations and personalization. However, many social media users expect each profile to stay within the platform where it was created and thus do not want the identities of different platforms to be linked. For this problem, we first investigate the approaches people would like to use to defend against cross-site UIL and the corresponding challenges. Based on the findings, we build an adversarial framework, DeLink, based on the thoughts of adversarial text generation to help people improve their social media screen names to defend against cross-site UIL. DeLink can support both Chinese and English languages and has good generalizability to the varying numbers of social media accounts and different cross-site user identity linkage models. Extensive evaluations validate DeLink’s better performance, including a higher success rate, higher efficiency, less impact on human perception, and capability to defend against different cross-site UIL models.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391540243",
    "type": "article"
  },
  {
    "title": "Re-Identification Attacks against the Topics API",
    "doi": "https://doi.org/10.1145/3675400",
    "publication_date": "2024-06-27",
    "publication_year": 2024,
    "authors": "Nikhil Jha; Martino Trevisan; Emilio Leonardi; Marco Mellia",
    "corresponding_authors": "",
    "abstract": "Recently, Google proposed the Topics API framework as a privacy-friendly alternative for behavioural advertising as a possible solution to balance user’s privacy and advertisement effectiveness. Using the Topics API, the browser builds a user profile based on navigation history, which advertisers can access. The Topics API aim at becoming the new standard for behavioural advertising, thus it is necessary to fully understand its operation and find possible limitations. In this article, we evaluate the robustness of the Topics API to a re-identification attack. To build a user profile, we suppose an attacker accumulates over time the topics a user exposes to different websites. The attacker later re-identifies the same user matching the profiles of their audience. We leverage real traffic traces and realistic population models, and we present increasingly powerful attack threats. We find that the Topics API mitigates but cannot prevent re-identification from taking place, as there is a sizeable chance that a user’s profile remains unique within a website’s audience and the attacker successfully matches it with the profile of the same user on a second website. Depending on environmental factors, the probability of correct re-identification can reach 50%, considering a pool of 1,000 users. We offer the code and data we use in this work to stimulate further studies and the tuning of the Topic API parameters. 1",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400089298",
    "type": "article"
  },
  {
    "title": "FoXtrot",
    "doi": "https://doi.org/10.1145/2344416.2344419",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Iris Miliaraki; Manolis Koubarakis",
    "corresponding_authors": "",
    "abstract": "Publish/subscribe systems have emerged in recent years as a promising paradigm for offering various popular notification services. In this context, many XML filtering systems have been proposed to efficiently identify XML data that matches user interests expressed as queries in an XML query language like XPath. However, in order to offer XML filtering functionality on an Internet-scale, we need to deploy such a service in a distributed environment, avoiding bottlenecks that can deteriorate performance. In this work, we design and implement FoXtrot, a system for filtering XML data that combines the strengths of automata for efficient filtering and distributed hash tables for building a fully distributed system. Apart from structural-matching, performed using automata, we also discuss different methods for evaluating value-based predicates. We perform an extensive experimental evaluation of our system, FoXtrot, on a local cluster and on the PlanetLab network and demonstrate that it can index millions of user queries, achieving a high indexing and filtering throughput. At the same time, FoXtrot exhibits very good load-balancing properties and improves its performance as we increase the size of the network.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2172074767",
    "type": "article"
  },
  {
    "title": "Evaluating Quality in Use of Corporate Web Sites",
    "doi": "https://doi.org/10.1145/3184646",
    "publication_date": "2018-07-17",
    "publication_year": 2018,
    "authors": "Daniela Fogli; Giovanni Guida",
    "corresponding_authors": "",
    "abstract": "In our prior work, we presented a novel approach to the evaluation of quality in use of corporate web sites based on an original quality model (QM-U) and a related methodology (EQ-EVAL). This article focuses on two research questions. The first one aims at investigating whether expected quality obtained through the application of EQ-EVAL methodology by employing a small panel of evaluators is a good approximation of actual quality obtained through experimentation with real users. To answer this research question, a comparative study has been carried out involving 5 evaluators and 50 real users. The second research question aims at demonstrating that the adoption of the EQ-EVAL methodology can provide useful information for web site improvement. Three original indicators, namely coherence, coverage and ranking have been defined to answer this question, and an additional study comparing the assessments of two panels of 5 and 10 evaluators, respectively, has been carried out. The results obtained in both studies are largely positive and provide a rational support for the adoption of the EQ-EVAL methodology.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2883915171",
    "type": "article"
  },
  {
    "title": "User Studies on End-User Service Composition",
    "doi": "https://doi.org/10.1145/3340294",
    "publication_date": "2019-07-26",
    "publication_year": 2019,
    "authors": "Liping Zhao; Pericles Loucopoulos; Evangelia Kavakli; Keletso J. Letsholo",
    "corresponding_authors": "",
    "abstract": "Context: End-user service composition (EUSC) is a service-oriented paradigm that aims to empower end users and allow them to compose their own web applications from reusable service components. User studies have been used to evaluate EUSC tools and processes. Such an approach should benefit software development, because incorporating end users’ feedback into software development should make software more useful and usable. Problem: There is a gap in our understanding of what constitutes a user study and how a good user study should be designed, conducted, and reported. Goal: This article aims to address this gap. Method: The article presents a systematic review of 47 selected user studies for EUSC. Guided by a review framework, the article systematically and consistently assesses the focus, methodology and cohesion of each of these studies. Results: The article concludes that the focus of these studies is clear, but their methodology is incomplete and inadequate, their overall cohesion is poor. The findings lead to the development of a design framework and a set of questions for the design, reporting, and review of good user studies for EUSC. The detailed analysis and the insights obtained from the analysis should be applicable to the design of user studies for service-oriented systems as well and indeed for any user studies related to software artifacts.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2941411146",
    "type": "article"
  },
  {
    "title": "Topic-aware Web Service Representation Learning",
    "doi": "https://doi.org/10.1145/3386041",
    "publication_date": "2020-04-11",
    "publication_year": 2020,
    "authors": "Min Shi; Yufei Tang; Xingquan Zhu; Jianxun Liu",
    "corresponding_authors": "",
    "abstract": "The advent of Service-Oriented Architecture (SOA) has brought a fundamental shift in the way in which distributed applications are implemented. An overwhelming number of Web-based services (e.g., APIs and Mashups) have leveraged this shift and furthered development. Applications designed with SOA principles are typically characterized by frequent dependencies with one another in the form of heterogeneous networks, i.e., annotation relations between tags and services, and composition relations between Mashups and APIs. Although prior work has shown the utility gained by exploring these networks, their analysis is still in its infancy. This article develops an approach to learning representations of the Web service network, which seeks to embed Web services in low-dimensional continuous vectors with preserved information of the network structure, functional tags, and service descriptions, such that services with similar functional properties and network structures are mapped together in the learned latent space. We first propose a topic generative model for constructing two topic distribution networks (Mashup-Topic and API-Topic) from the service content. Then, we present an efficient optimization process to derive low-dimensional vector representations of Web services from a tri-layer bipartite network with the Mashup-Topic and API-Topic networks on two ends and the Mashup-API composition network in the middle. Experiments on real-word datasets have verified that our approach is effective to learn robust low-rank service representations, i.e., 25% F1-measure gain over the state-of-the-art in Web service recommendation task.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3015586310",
    "type": "article"
  },
  {
    "title": "Leveraging Social Feedback to Verify Online Identity Claims",
    "doi": "https://doi.org/10.1145/2543711",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Michael Sirivianos; Kyungbaek Kim; Jian Wei Gan; Xiaowei Yang",
    "corresponding_authors": "",
    "abstract": "Anonymity is one of the main virtues of the Internet, as it protects privacy and enables users to express opinions more freely. However, anonymity hinders the assessment of the veracity of assertions that online users make about their identity attributes, such as age or profession. We propose FaceTrust, a system that uses online social networks to provide lightweight identity credentials while preserving a user’s anonymity. FaceTrust employs a “game with a purpose” design to elicit the opinions of the friends of a user about the user’s self-claimed identity attributes, and uses attack-resistant trust inference to assign veracity scores to identity attribute assertions. FaceTrust provides credentials, which a user can use to corroborate his assertions. We evaluate our proposal using a live Facebook deployment and simulations on a crawled social graph. The results show that our veracity scores are strongly correlated with the ground truth, even when dishonest users make up a large fraction of the social network and employ the Sybil attack.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2006821413",
    "type": "article"
  },
  {
    "title": "Merging Query Results From Local Search Engines for Georeferenced Objects",
    "doi": "https://doi.org/10.1145/2656344",
    "publication_date": "2014-11-06",
    "publication_year": 2014,
    "authors": "Eduard Dragut; Bhaskar DasGupta; Brian P. Beirne; Ali Neyestani; Badr Atassi; Clement Yu; Weiyi Meng",
    "corresponding_authors": "",
    "abstract": "The emergence of numerous online sources about local services presents a need for more automatic yet accurate data integration techniques. Local services are georeferenced objects and can be queried by their locations on a map, for instance, neighborhoods. Typical local service queries (e.g., “French Restaurant in The Loop”) include not only information about “what” (“French Restaurant”) a user is searching for (such as cuisine) but also “where” information, such as neighborhood (“The Loop”). In this article, we address three key problems: query translation, result merging and ranking. Most local search engines provide a (hierarchical) organization of (large) cities into neighborhoods. A neighborhood in one local search engine may correspond to sets of neighborhoods in other local search engines. These make the query translation challenging. To provide an integrated access to the query results returned by the local search engines, we need to combine the results into a single list of results. Our contributions include: (1) An integration algorithm for neighborhoods. (2) A very effective business listing resolution algorithm. (3) A ranking algorithm that takes into consideration the user criteria, user ratings and rankings. We have created a prototype system, Yumi, over local search engines in the restaurant domain. The restaurant domain is a representative case study for the local services. We conducted a comprehensive experimental study to evaluate Yumi. A prototype version of Yumi is available online.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2018268535",
    "type": "article"
  },
  {
    "title": "Conceptual Development of Custom, Domain-Specific Mashup Platforms",
    "doi": "https://doi.org/10.1145/2628439",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Stefano Soi; Florian Daniel; Fabio Casati",
    "corresponding_authors": "",
    "abstract": "Despite the common claim by mashup platforms that they enable end-users to develop their own software, in practice end-users still don't develop their own mashups, as the highly technical or inexistent user bases of today's mashup platforms testify. The key shortcoming of current platforms is their general-purpose nature, that privileges expressive power over intuitiveness. In our prior work, we have demonstrated that a domain-specific mashup approach, which privileges intuitiveness over expressive power, has much more potential to enable end-user development (EUD). The problem is that developing mashup platforms—domain-specific or not—is complex and time consuming . In addition, domain-specific mashup platforms by their very nature target only a small user basis, that is, the experts of the target domain, which makes their development not sustainable if it is not adequately supported and automated. With this article, we aim to make the development of custom, domain-specific mashup platforms cost-effective. We describe a mashup tool development kit (MDK) that is able to automatically generate a mashup platform (comprising custom mashup and component description languages and design-time and runtime environments) from a conceptual design and to provision it as a service . We equip the kit with a dedicated development methodology and demonstrate the applicability and viability of the approach with the help of two case studies.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2158623637",
    "type": "article"
  },
  {
    "title": "A Graph-Based Context-Aware Model to Understand Online Conversations",
    "doi": "https://doi.org/10.1145/3624579",
    "publication_date": "2023-09-18",
    "publication_year": 2023,
    "authors": "Vibhor Agarwal; Anthony P. Young; Sagar Joglekar; Nishanth Sastry",
    "corresponding_authors": "",
    "abstract": "Online forums that allow for participatory engagement between users have been transformative for the public discussion of many important issues. However, such conversations can sometimes escalate into full-blown exchanges of hate and misinformation. Existing approaches in natural language processing (NLP), such as deep learning models for classification tasks, use as inputs only a single comment or a pair of comments depending upon whether the task concerns the inference of properties of the individual comments or the replies between pairs of comments, respectively. However, in online conversations, comments and replies may be based on external context beyond the immediately relevant information that is input to the model. Therefore, being aware of the conversations’ surrounding contexts should improve the model’s performance for the inference task at hand. We propose GraphNLI , 1 a novel graph-based deep learning architecture that uses graph walks to incorporate the wider context of a conversation in a principled manner. Specifically, a graph walk starts from a given comment and samples “nearby” comments in the same or parallel conversation threads, which results in additional embeddings that are aggregated together with the initial comment’s embedding. We then use these enriched embeddings for downstream NLP prediction tasks that are important for online conversations. We evaluate GraphNLI on two such tasks - polarity prediction and misogynistic hate speech detection - and find that our model consistently outperforms all relevant baselines for both tasks. Specifically, GraphNLI with a biased root-seeking random walk performs with a macro- F 1 score of 3 and 6 percentage points better than the best-performing BERT-based baselines for the polarity prediction and hate speech detection tasks, respectively. We also perform extensive ablative experiments and hyperparameter searches to understand the efficacy of GraphNLI. This demonstrates the potential of context-aware models to capture the global context along with the local context of online conversations for these two tasks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386830936",
    "type": "article"
  },
  {
    "title": "CLHHN: Category-aware Lossless Heterogeneous Hypergraph Neural Network for Session-based Recommendation",
    "doi": "https://doi.org/10.1145/3626569",
    "publication_date": "2023-10-04",
    "publication_year": 2023,
    "authors": "Yutao Ma; Wang Ze-sheng; Liwei Huang; Jian Wang",
    "corresponding_authors": "",
    "abstract": "In recent years, session-based recommendation (SBR), which seeks to predict the target user’s next click based on anonymous interaction sequences, has drawn increasing interest for its practicality. The key to completing the SBR task is modeling user intent accurately. Due to the popularity of graph neural networks (GNNs), most state-of-the-art (SOTA) SBR approaches attempt to model user intent from the transitions among items in a session with GNNs. Despite their accomplishments, there are still two limitations. First, most existing SBR approaches utilize limited information from short user–item interaction sequences and suffer from the data sparsity problem of session data. Second, most GNN-based SBR approaches describe pairwise relations between items while neglecting complex and high-order data relations. Although some recent studies based on hypergraph neural networks have been proposed to model complex and high-order relations, they usually output unsatisfactory results due to insufficient relation modeling and information loss. To this end, we propose a category-aware lossless heterogeneous hypergraph neural network (CLHHN) in this article to recommend possible items to the target users by leveraging the category of items. More specifically, we convert each category-aware session sequence with repeated user clicks into a lossless heterogeneous hypergraph consisting of item and category nodes as well as three types of hyperedges, each of which can capture specific relations to reflect various user intents. Then, we design an attention-based lossless hypergraph convolutional network to generate sessionwise and multi-granularity intent-aware item representations. Experiments on three real-world datasets indicate that CLHHN can outperform the SOTA models in making a better tradeoff between prediction performance and training efficiency. An ablation study also demonstrates the necessity of CLHHN’s key components.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4387344551",
    "type": "article"
  },
  {
    "title": "Scouts, promoters, and connectors",
    "doi": "https://doi.org/10.1145/1255438.1255440",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Bharath Kumar Mohan; Benjamin J. Keller; Naren Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors. The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce. We present a characterization of nearest-neighbor collaborative filtering that allows us to disaggregate global recommender performance measures into contributions made by each individual rating. In particular, we formulate three roles--- scouts , promoters , and connectors ---that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected, respectively. These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole. For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling. We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2084222143",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on service oriented computing (SOC)",
    "doi": "https://doi.org/10.1145/1346337.1346338",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Schahram Dustdar; Bernd Krämer",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2072509340",
    "type": "article"
  },
  {
    "title": "A model-driven methodology to the content layout problem in web applications",
    "doi": "https://doi.org/10.1145/2344416.2344417",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Sara Comai; Davide Mazza",
    "corresponding_authors": "",
    "abstract": "This article presents a model-driven approach for the design of the layout in a complex Web application, where large amounts of data are accessed. The aim of this work is to reduce, as much as possible, repetitive tasks and to factor out common aspects into different kinds of rules that can be reused across different applications. In particular, exploiting the conceptual elements of the typical models used for the design of a Web application, it defines presentation and layout rules at different levels of abstraction and granularity. A procedure for the automatic layout of the content of a page is proposed and evaluated, and the layout of advanced Web applications is discussed.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1968113385",
    "type": "article"
  },
  {
    "title": "The parallel path framework for entity discovery on the web",
    "doi": "https://doi.org/10.1145/2516633.2516638",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Tim Weninger; Thomas J. Johnston; Jiawei Han",
    "corresponding_authors": "",
    "abstract": "It has been a dream of the database and Web communities to reconcile the unstructured nature of the World Wide Web with the neat, structured schemas of the database paradigm. Even though databases are currently used to generate Web content in some sites, the schemas of these databases are rarely consistent across a domain. This makes the comparison and aggregation of information from different domains difficult. We aim to make an important step towards resolving this disparity by using the structural and relational information on the Web to (1) extract Web lists, (2) find entity-pages, (3) map entity-pages to a database, and (4) extract attributes of the entities. Specifically, given a Web site and an entity-page (e.g., university department and faculty member home page) we seek to find all of the entity-pages of the same type (e.g., all faculty members in the department), as well as attributes of the specific entities (e.g., their phone numbers, email addresses, office numbers). To do this, we propose a Web structure mining method which grows parallel paths through the Web graph and DOM trees and propagates relevant attribute information forward. We show that by utilizing these parallel paths we can efficiently discover entity-pages and attributes. Finally, we demonstrate the accuracy of our method with a large case study.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2139423731",
    "type": "article"
  },
  {
    "title": "Improving the Accuracy of the Video Popularity Prediction Models through User Grouping and Video Popularity Classification",
    "doi": "https://doi.org/10.1145/3372499",
    "publication_date": "2020-02-07",
    "publication_year": 2020,
    "authors": "Masoud Hassanpour; Seyed Amir Hoseinitabatabaei; Payam Barnaghi; Rahim Tafazolli",
    "corresponding_authors": "",
    "abstract": "This article proposes a novel approach for enhancing the video popularity prediction models. Using the proposed approach, we enhance three popularity prediction techniques that outperform the accuracy of the prior state-of-the-art solutions. The major components of the proposed approach are two novel mechanisms for ” user grouping ” and ” content classification .” The user grouping method is an unsupervised clustering approach that divides the users into an adequate number of user groups with similar interests. The content classification approach identifies the classes of videos with similar popularity growth trends. To predict the popularity of the newly-released videos, our proposed popularity prediction model trains its parameters in each user group and its associated video popularity classes. Evaluations are performed through a 5-fold cross validation and on a dataset containing one month video request records of 26,706 users of BBC iPlayer. Using the proposed grouping technique, user groups of similar interest and up to two video popularity classes for each user group were detected. Our analysis shows that the accuracy of the proposed solution outperforms the state-of-the-art, including Szabo-Huberman (SH), Multivariate Linear (ML), and Multivariate linear Radial Basis Functions (MRBF) models by an average of 45%, 33%, and 24%, respectively. Finally, we discuss how various systems in the network and service management domain such as cache deployment, advertising, and video broadcasting technologies benefit from our findings to illustrate the implications.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2995083367",
    "type": "article"
  },
  {
    "title": "Time-aspect-sentiment Recommendation Models Based on Novel Similarity Measure Methods",
    "doi": "https://doi.org/10.1145/3375548",
    "publication_date": "2020-02-07",
    "publication_year": 2020,
    "authors": "Guohui Li; Qi Chen; Bolong Zheng; Quoc Viet Hung Nguyen; Pan Zhou; Guanfeng Liu",
    "corresponding_authors": "",
    "abstract": "The explosive growth of e-commerce has led to the development of the recommendation system. The recommendation system aims to provide a set of items that meet users’ personalized needs through analyzing users’ consumption records. However, the timeliness of purchasing data and the implicity of feedback data pose severe challenges for the existing recommendation methods. To alleviate these challenges, we exploit the user’s consumption records from the perspectives of user and item, by modeling the data on both item and user level, where the item-level value reflects the grade of item, and the user-level value reflects the user’s purchase intention. In this article, we collect the description information and the reviews of the items from public websites, then adopt sentiment analysis techniques to model the similarities on user level and item level, respectively. In particular, we extend the traditional latent factor model and propose two novel methods— I tem L evel Similarity M atrix F actorization (ILMF) and U ser L evel Similarity M atrix F actorization (ULMF)—by introducing two novel similarity measure methods. In ILMF and ULMF, the consistency between latent factors and explicit aspects is naturally incorporated into learning latent factors of the users and items, such that we can predict the users’ preferences on different items more accurately. Moreover, we propose I tem- U ser L evel Similarity M atrix F actorization (IULMF), which combines these two methods to study their contributions on the final performance. Experimental evaluations on the real datasets show that our methods outperform the baseline approaches in terms of both the precision and NDCG.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3014321680",
    "type": "article"
  },
  {
    "title": "Who Has the Last Word? Understanding How to Sample Online Discussions",
    "doi": "https://doi.org/10.1145/3452936",
    "publication_date": "2021-06-03",
    "publication_year": 2021,
    "authors": "Gioia Boschi; Anthony P. Young; Sagar Joglekar; Chiara Cammarota; Nishanth Sastry",
    "corresponding_authors": "",
    "abstract": "In online debates, as in offline ones, individual utterances or arguments support or attack each other, leading to some subset of arguments (potentially from different sides of the debate) being considered more relevant than others. However, online conversations are much larger in scale than offline ones, with often hundreds of thousands of users weighing in, collaboratively forming large trees of comments by starting from an original post and replying to each other. In large discussions, readers are often forced to sample a subset of the arguments being put forth. Since such sampling is rarely done in a principled manner, users may not read all the relevant arguments to get a full picture of the debate from a sample. This article is interested in answering the question of how users should sample online conversations to selectively favour the currently justified or accepted positions in the debate. We apply techniques from argumentation theory and complex networks to build a model that predicts the probabilities of the normatively justified arguments given their location in idealised online discussions of comments and replies, which we represent as trees. Our model shows that the proportion of replies that are supportive, the distribution of the number of replies that comments receive, and the locations of comments that do not receive replies (i.e., the “leaves” of the reply tree) all determine the probability that a comment is a justified argument given its location. We show that when the distribution of the number of replies is homogeneous along the tree length, for acrimonious discussions (with more attacking comments than supportive ones), the distribution of justified arguments depends on the parity of the tree level, which is the distance from the root expressed as number of edges. In supportive discussions, which have more supportive comments than attacks, the probability of having justified comments increases as one moves away from the root. For discussion trees that have a non-homogeneous in-degree distribution, for supportive discussions we observe the same behaviour as before, while for acrimonious discussions we cannot observe the same parity-based distribution. This is verified with data obtained from the online debating platform Kialo. By predicting the locations of the justified arguments in reply trees, we can therefore suggest which arguments readers should sample, to grasp the currently accepted opinions in such discussions. Our models have important implications for the design of future online debating platforms.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3167254174",
    "type": "article"
  },
  {
    "title": "Measuring International Online Human Values with Word Embeddings",
    "doi": "https://doi.org/10.1145/3501306",
    "publication_date": "2021-12-22",
    "publication_year": 2021,
    "authors": "Gabriel Magno; Virgı́lio Almeida",
    "corresponding_authors": "",
    "abstract": "As the Internet grows in number of users and in the diversity of services, it becomes more influential on peoples lives. It has the potential of constructing or modifying the opinion, the mental perception, and the values of individuals. What is being created and published online is a reflection of people’s values and beliefs. As a global platform, the Internet is a great source of information for researching the online culture of many different countries. In this work we develop a methodology for measuring data from textual online sources using word embedding models, to create a country-based online human values index that captures cultural traits and values worldwide. Our methodology is applied with a dataset of 1.7 billion tweets, and then we identify their location among 59 countries. We create a list of 22 Online Values Inquiries (OVI) , each one capturing different questions from the World Values Survey, related to several values such as religion, science, and abortion. We observe that our methodology is indeed capable of capturing human values online for different counties and different topics. We also show that some online values are highly correlated (up to c = 0.69, p &lt; 0.05) with the corresponding offline values, especially religion-related ones. Our method is generic, and we believe it is useful for social sciences specialists, such as demographers and sociologists, that can use their domain knowledge and expertise to create their own Online Values Inquiries, allowing them to analyze human values in the online environment.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4200489751",
    "type": "article"
  },
  {
    "title": "When Simpler Data Does Not Imply Less Information",
    "doi": "https://doi.org/10.1145/3143402",
    "publication_date": "2018-01-27",
    "publication_year": 2018,
    "authors": "Souneil Park; Aleksandar Matić; Kamini Garg; Nuria Oliver",
    "corresponding_authors": "",
    "abstract": "The exponential growth in smartphone adoption is contributing to the availability of vast amounts of human behavioral data. This data enables the development of increasingly accurate data-driven user models that facilitate the delivery of personalized services which are often free in exchange for the use of its customers' data. Although such usage conventions have raised many privacy concerns, the increasing value of personal data is motivating diverse entities to aggressively collect and exploit the data. In this paper, we unfold profiling scenarios around mobile HTTP(S) traffic, focusing on those that have limited but meaningful segments of the data. The capability of the scenarios to profile personal information is examined with real user data, collected in-the-wild from 61 mobile phone users for a minimum of 30 days. Our study attempts to model heterogeneous user traits and interests, including personality, boredom proneness, demographics, and shopping interests. Based on our modeling results, we discuss various implications to personalization, privacy, and personal data rights.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4300038061",
    "type": "article"
  },
  {
    "title": "JSAnalyzer: A Web Developer Tool for Simplifying Mobile Web Pages through Non-critical JavaScript Elimination",
    "doi": "https://doi.org/10.1145/3550358",
    "publication_date": "2022-08-08",
    "publication_year": 2022,
    "authors": "Moumena Chaqfeh; Russell Coke; Jacinta Hu; Waleed Hashmi; Lakshminarayanan Subramanian; Talal Rahwan; Yasir Zaki",
    "corresponding_authors": "",
    "abstract": "The amount of JavaScript used in web pages has substantially grown in the past decade, leading to large and complex pages that are computationally intensive for handheld mobile devices. Due to the increasing usage of these devices to access today’s web, and to accommodate the needs of a large number of mobile web users who solely rely on low-end devices, we propose “JSAnalyzer,” an easy-to-use tool that enables web developers to quickly optimize JavaScript usage in their pages and to generate simpler versions of these pages for mobile web users. JSAnalyzer is motivated by the widespread use of non-critical JavaScript elements, i.e., those that have negligible (if any) impact on the page’s visual content and interactive functionality. JSAnalyzer allows the developer to selectively enable or disable JavaScript elements in any given page while visually observing their impact on the page to (1) accurately identify any non-critical JavaScript elements and (2) create a simplified page with these elements removed. Our quantitative evaluation shows that, given a low-end mobile phone, JSAnalyzer achieves an increase of nearly 90% in Google’s lighthouse performance score while reducing the page load time by 30%. A qualitative study of 22 users shows that the lighter pages produced by JSAnalyzer maintain more than 90% visual similarity compared to the original pages. Moreover, JSAnalyzer was evaluated by 69 developers, showing that it scores nearly 90% in terms of usefulness and usability while retaining the page’s content and functionality. Finally, we show that JSAnalyzer outperforms state-of-the-art solutions in terms of timing speedups and resource savings.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4290649207",
    "type": "article"
  },
  {
    "title": "An Extended Ultimatum Game for Multi-Party Access Control in Social Networks",
    "doi": "https://doi.org/10.1145/3555351",
    "publication_date": "2022-08-27",
    "publication_year": 2022,
    "authors": "Anna Squicciarini; Sarah Rajtmajer; Yuzhe Gao; Justin Semonsen; Andrew Belmonte; Pratik Agarwal",
    "corresponding_authors": "",
    "abstract": "In this article, we aim to answer an important set of questions about the potential longitudinal effects of repeated sharing and privacy settings decisions over jointly managed content among users in a social network. We model user interactions through a repeated game in a network graph. We present a variation of the one-shot Ultimatum Game, wherein individuals interact with peers to make a decision on a piece of shared content. The outcome of this game is either success or failure, wherein success implies that a satisfactory decision for all parties is made and failure instead implies that the parties could not reach an agreement. Our proposed game is grounded in empirical data about individual decisions in repeated pairwise negotiations about jointly managed content in a social network. We consider both a “continuous” privacy model as well the “discrete” case of a model wherein privacy values are to be chosen among a fixed set of options. We formally demonstrate that over time, the system converges toward a “fair” state, wherein each individual’s preferences are accounted for. Our discrete model is validated by way of a user study, where participants are asked to propose privacy settings for own shared content from a small, discrete set of options.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4293366753",
    "type": "article"
  },
  {
    "title": "Heterogeneous Information Crossing on Graphs for Session-Based Recommender Systems",
    "doi": "https://doi.org/10.1145/3572407",
    "publication_date": "2022-12-13",
    "publication_year": 2022,
    "authors": "Xiaolin Zheng; Rui Wu; Zhongxuan Han; Chaochao Chen; Linxun Chen; Bing Han",
    "corresponding_authors": "",
    "abstract": "Recommender systems are fundamental information filtering techniques to recommend content or items that meet users’ personalities and potential needs. As a crucial solution to address the difficulty of user identification and unavailability of historical information, session-based recommender systems provide recommendation services that only rely on users’ behaviors in the current session. However, most existing studies are not well-designed for modeling heterogeneous user behaviors and capturing the relationships between them in practical scenarios. To fill this gap, in this article, we propose a novel graph-based method, namely H eterogeneous I nformation C rossing on G raphs (HICG). HICG utilizes multiple types of user behaviors in the sessions to construct heterogeneous graphs, and captures users’ current interests with their long-term preferences by effectively crossing the heterogeneous information on the graphs. In addition, we also propose an enhanced version, named HICG-CL, which incorporates the contrastive learning (CL) technique to enhance item representation ability. By utilizing the item co-occurrence relationships across different sessions, HICG-CL improves the recommendation performance of HICG. We conduct extensive experiments on three real-world recommendation datasets, and the results verify that (i) HICG achieves state-of-the-art performance by utilizing multiple types of behaviors on the heterogeneous graph. (ii) HICG-CL further significantly improves the recommendation performance of HICG by the proposed contrastive learning module.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4311316643",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on query log analysis",
    "doi": "https://doi.org/10.1145/1409220.1409221",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Einat Amitay; Andrei Broder",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1967838787",
    "type": "article"
  },
  {
    "title": "Exploring XML web collections with DescribeX",
    "doi": "https://doi.org/10.1145/1806916.1806920",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Mariano P. Consens; Renée J. Miller; Flavio Rizzolo; Alejandro Vaisman",
    "corresponding_authors": "",
    "abstract": "As Web applications mature and evolve, the nature of the semistructured data that drives these applications also changes. An important trend is the need for increased flexibility in the structure of Web documents. Hence, applications cannot rely solely on schemas to provide the complex knowledge needed to visualize, use, query and manage documents. Even when XML Web documents are valid with regard to a schema, the actual structure of such documents may exhibit significant variations across collections for several reasons: the schema may be very lax (e.g., RSS feeds), the schema may be large and different subsets of it may be used in different documents (e.g., industry standards like UBL), or open content models may allow arbitrary schemas to be mixed (e.g., RSS extensions like those used for podcasting). For these reasons, many applications that incorporate XPath queries to process a large Web document collection require an understanding of the actual structure present in the collection, and not just the schema. To support modern Web applications, we introduce DescribeX, a powerful framework that is capable of describing complex XML summaries of Web collections. DescribeX supports the construction of heterogenous summaries that can be declaratively defined and refined by means of axis path regular expression (AxPREs). AxPREs provide the flexibility necessary for declaratively defining complex mappings between instance nodes (in the documents) and summary nodes. These mappings are capable of expressing order and cardinality, among other properties, which can significantly help in the understanding of the structure of large collections of XML documents and enhance the performance of Web applications over these collections. DescribeX captures most summary proposals in the literature by providing (for the first time) a common declarative definition for them. Experimental results demonstrate the scalability of DescribeX summary operations (summary creation, as well as refinement and stabilization, two key enablers for tailoring summaries) on multi-gigabyte Web collections.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2077439276",
    "type": "article"
  },
  {
    "title": "Topic Distillation with Query-Dependent Link Connections and Page Characteristics",
    "doi": "https://doi.org/10.1145/1961659.1961660",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Mingfang Wu; Falk Scholer; Andrew Turpin",
    "corresponding_authors": "",
    "abstract": "Searchers on the Web often aim to find key resources about a topic. Finding such results is called topic distillation. Previous research has shown that the use of sources of evidence such as page indegree and URL structure can significantly improve search performance on interconnected collections such as the Web, beyond the use of simple term distribution statistics. This article presents a new approach to improve topic distillation by exploring the use of external sources of evidence: link structure, including query dependent indegree and outdegree; and web page characteristics, such as the density of anchor links. Our experiments with the TREC .GOV collection, an 18GB crawl of the US .gov domain from 2002, show that using such evidence can significantly improve search effectiveness, with combinations of evidence leading to significant performance gains over both full-text and anchor-text baselines. Moreover, we demonstrate that, at a different scope level, both local query-dependent outdegree and query-dependent indegree out-performed their global query-independent counterparts; and at the same scope level, outdegree out-performed indegree. Adding query-dependent indegree or page characteristics to query-dependent outdegree could have a small, but not significant, improvement.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1979445588",
    "type": "article"
  },
  {
    "title": "Optimal distance bounds for fast search on compressed time-series query logs",
    "doi": "https://doi.org/10.1145/1734200.1734203",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Michail Vlachos; Süleyman S. Kozat; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Consider a database of time-series, where each datapoint in the series records the total number of users who asked for a specific query at an internet search engine. Storage and analysis of such logs can be very beneficial for a search company from multiple perspectives. First, from a data organization perspective, because query Weblogs capture important trends and statistics, they can help enhance and optimize the search experience (keyword recommendation, discovery of news events). Second, Weblog data can provide an important polling mechanism for the microeconomic aspects of a search engine, since they can facilitate and promote the advertising facet of the search engine (understand what users request and when they request it). Due to the sheer amount of time-series Weblogs, manipulation of the logs in a compressed form is an impeding necessity for fast data processing and compact storage requirements. Here, we explicate how to compute the lower and upper distance bounds on the time-series logs when working directly on their compressed form. Optimal distance estimation means tighter bounds, leading to better candidate selection/elimination and ultimately faster search performance. Our derivation of the optimal distance bounds is based on the careful analysis of the problem using optimization principles. The experimental evaluation suggests a clear performance advantage of the proposed method, compared to previous compression/search techniques. The presented method results in a 10--30% improvement on distance estimations, which in turn leads to 25--80% improvement on the search performance.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2007813665",
    "type": "article"
  },
  {
    "title": "Discovery of latent subcommunities in a blog's readership",
    "doi": "https://doi.org/10.1145/1806916.1806921",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Brett Adams; Dinh Phung; Svetha Venkatesh",
    "corresponding_authors": "",
    "abstract": "The blogosphere has grown to be a mainstream forum of social interaction as well as a commercially attractive source of information and influence. Tools are needed to better understand how communities that adhere to individual blogs are constituted in order to facilitate new personal, socially-focused browsing paradigms, and understand how blog content is consumed, which is of interest to blog authors, big media, and search. We present a novel approach to blog subcommunity characterization by modeling individual blog readers using mixtures of an extension to the LDA family that jointly models phrases and time, Ngram Topic over Time (NTOT), and cluster with a number of similarity measures using Affinity Propagation. We experiment with two datasets: a small set of blogs whose authors provide feedback, and a set of popular, highly commented blogs, which provide indicators of algorithm scalability and interpretability without prior knowledge of a given blog. The results offer useful insight to the blog authors about their commenting community, and are observed to offer an integrated perspective on the topics of discussion and members engaged in those discussions for unfamiliar blogs. Our approach also holds promise as a component of solutions to related problems, such as online entity resolution and role discovery.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2152960946",
    "type": "article"
  },
  {
    "title": "Adaptive Knowledge Propagation in Web Ontologies",
    "doi": "https://doi.org/10.1145/3105961",
    "publication_date": "2017-08-21",
    "publication_year": 2017,
    "authors": "Pasquale Minervini; Volker Tresp; Claudia d’Amato; Nicola Fanizzi",
    "corresponding_authors": "",
    "abstract": "We focus on the problem of predicting missing assertions in Web ontologies. We start from the assumption that individual resources that are similar in some aspects are more likely to be linked by specific relations: this phenomenon is also referred to as homophily and emerges in a variety of relational domains. In this article, we propose a method for (1) identifying which relations in the ontology are more likely to link similar individuals and (2) efficiently propagating knowledge across chains of similar individuals. By enforcing sparsity in the model parameters, the proposed method is able to select only the most relevant relations for a given prediction task. Our experimental evaluation demonstrates the effectiveness of the proposed method in comparison to state-of-the-art methods from the literature.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2749539417",
    "type": "article"
  },
  {
    "title": "Q2P",
    "doi": "https://doi.org/10.1145/2873061",
    "publication_date": "2016-04-29",
    "publication_year": 2016,
    "authors": "Wensheng Wu; Weiyi Meng; Weifeng Su; Guangyou Zhou; Yao‐Yi Chiang",
    "corresponding_authors": "",
    "abstract": "We present Q2P, a system that discovers query templates from search engines via their query autocompletion services. Q2P is distinct from the existing works in that it does not rely on query logs of search engines that are typically not readily available. Q2P is also unique in that it uses a trie to economically store queries sampled from a search engine and employs a beam-search strategy that focuses the expansion of the trie on its most promising nodes. Furthermore, Q2P leverages the trie-based storage of query sample to discover query templates using only two passes over the trie. Q2P is a key part of our ongoing project Deep2Q on a template-driven data integration on the Deep Web, where the templates learned by Q2P are used to guide the integration process in Deep2Q. Experimental results on four major search engines indicate that (1) Q2P sends only a moderate number of queries (ranging from 597 to 1,135) to the engines, while obtaining a significant number of completions per query (ranging from 4.2 to 8.5 on the average); (2) a significant number of templates (ranging from 8 to 32 when the minimum support for frequent templates is set to 1%) may be discovered from the samples.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2345799241",
    "type": "article"
  },
  {
    "title": "What Users Actually Do in a Social Tagging System",
    "doi": "https://doi.org/10.1145/2896821",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Stephan Doerfel; Daniel Zöller; Philipp Singer; Thomas Niebler; Andreas Hotho; Markus Strohmaier",
    "corresponding_authors": "",
    "abstract": "Social tagging systems have established themselves as an important part in today’s Web and have attracted the interest of our research community in a variety of investigations. Henceforth, several aspects of social tagging systems have been discussed and assumptions have emerged on which our community builds their work. Yet, testing such assumptions has been difficult due to the absence of suitable usage data in the past. In this work, we thoroughly investigate and evaluate four aspects about tagging systems, covering social interaction, retrieval of posted resources, the importance of the three different types of entities, users, resources, and tags, as well as connections between these entities’ popularity in posted and in requested content. For that purpose, we examine live server log data gathered from the real-world, public social tagging system BibSonomy. Our empirical results paint a mixed picture about the four aspects. Although typical assumptions hold to a certain extent for some, other aspects need to be reflected in a very critical light. Our observations have implications for the understanding of social tagging systems and the way they are used on the Web. We make the dataset used in this work available to other researchers.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2399213570",
    "type": "article"
  },
  {
    "title": "Deep Gated Multi-modal Fusion for Image Privacy Prediction",
    "doi": "https://doi.org/10.1145/3608446",
    "publication_date": "2023-07-22",
    "publication_year": 2023,
    "authors": "Chenye Zhao; Cornelia Caragea",
    "corresponding_authors": "",
    "abstract": "With the rapid development of technologies in mobile devices, people can post their daily lives on social networking sites such as Facebook, Flickr, and Instagram. This leads to new privacy concerns due to people’s lack of understanding that private information can be leaked and used to their detriment. Image privacy prediction models are developed to predict whether images contain sensitive information (private images) or are safe to be shared online (public images). Despite significant progress on this task, there are still some crucial problems that remain to be solved. Firstly, images’ content and tags are found to be useful modalities to automatically predict images’ privacy. To date, most image privacy prediction models use single modalities (image-only or tag-only), which limits their performance. Secondly, we observe that current image privacy prediction models are surprisingly vulnerable to even small perturbations in the input data. Attackers can add small perturbations to input data and easily damage a well-trained image privacy prediction model. To address these challenges, in this article, we propose a new decision-level Gated multi-modal fusion (GMMF) approach that fuses object, scene, and image tags modalities to predict privacy for online images. In particular, the proposed approach identifies fusion weights of class probability distributions generated by single-modal classifiers according to their reliability of the privacy prediction for each target image in a sample-by-sample manner and performs a weighted decision-level fusion, so that modalities with high reliability are assigned with higher fusion weights while ones with low reliability are restrained with lower fusion weights. The results of our experiments show that the gated multi-modal fusion network effectively fuses single modalities and outperforms state-of-the-art models for image privacy prediction. Moreover, we perform adversarial training on our proposed GMMF model using multiple types of noise on input data (i.e., images and/or tags). When some modalities are failed by input data with noise attacks, our approach effectively utilizes clean modalities and minimizes negative influences brought by degraded ones using fusion weights, achieving significantly stronger robustness over traditional fusion methods for image privacy prediction. The robustness of our GMMF model against data noise can even be generalized to more severe noise levels. To the best of our knowledge, we are the first to investigate the robustness of image privacy prediction models against noise attacks. Moreover, as the performance of decision-level multi-modal fusion depends highly on the quality of single-modal networks, we investigate self-distillation on single-modal privacy classifiers and observe that transferring knowledge from a trained teacher model to a student model is beneficial in our proposed approach.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385074030",
    "type": "article"
  },
  {
    "title": "Cache architecture for on-demand streaming on the Web",
    "doi": "https://doi.org/10.1145/1281480.1281483",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Raj Sharman; Shiva Shankar Ramanna; Ram Ramesh; Ram D. Gopal",
    "corresponding_authors": "",
    "abstract": "On-demand streaming from a remote server through best-effort Internet poses several challenges because of network losses and variable delays. The primary technique used to improve the quality of distributed content service is replication. In the context of the Internet, Web caching is the traditional mechanism that is used. In this article we develop a new staged delivery model for a distributed architecture in which video is streamed from remote servers to edge caches where the video is buffered and then streamed to the client through a last-mile connection. The model uses a novel revolving indexed cache buffer management mechanism at the edge cache and employs selective retransmissions of lost packets between the remote and edge cache for a best-effort recovery of the losses. The new Web cache buffer management scheme includes a dynamic adjustment of cache buffer parameters based on network conditions. In addition, performance of buffer management and retransmission policies at the edge cache is modeled and assessed using a probabilistic analysis of the streaming process as well as system simulations. The influence of different endogenous control parameters on the quality of stream received by the client is studied. Calibration curves on the QoS metrics for different network conditions have been obtained using simulations. Edge cache management can be done using these calibration curves. ISPs can make use of calibration curves to set the values of the endogenous control parameters for specific QoS in real-time streaming operations based on network conditions. A methodology to benchmark transmission characteristics using real-time traffic data is developed to enable effective decision making on edge cache buffer allocation and management strategies.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2044671173",
    "type": "article"
  },
  {
    "title": "A Two Phases Self-healing Framework for Service-oriented Systems",
    "doi": "https://doi.org/10.1145/3450443",
    "publication_date": "2021-04-22",
    "publication_year": 2021,
    "authors": "Amal Alhosban; Hassan Ali Al-Ababneh; Khayyam Hashmi; Brahim Medjahed; Hassan Ali Al-Ababneh",
    "corresponding_authors": "",
    "abstract": "Service-Oriented Architectures (SOA) enable the automatic creation of business applications from independently developed and deployed Web services. As Web services are inherently a priori unknown, how to deliver reliable Web services compositions is a significant and challenging problem. Services involved in an SOA often do not operate under a single processing environment and need to communicate using different protocols over a network. Under such conditions, designing a fault management system that is both efficient and extensible is a challenging task. In this article, we propose SFSS, a self-healing framework for SOA fault management. SFSS is predicting, identifying, and solving faults in SOAs. In SFSS, we identified a set of high-level exception handling strategies based on the QoS performances of different component services and the preferences articled by the service consumers. Multiple recovery plans are generated and evaluated according to the performance of the selected component services, and then we execute the best recovery plan. We assess the overall user dependence (i.e., the service is independent of other services) using the generated plan and the available invocation information of the component services. Due to the experiment results, the given technique enhances the service selection quality by choosing the services that have the highest score and betters the overall system performance. The experiment results indicate the applicability of SFSS and show improved performance in comparison to similar approaches.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3159302232",
    "type": "article"
  },
  {
    "title": "Identifying and Evaluating Anomalous Structural Change-based Nodes in Generalized Dynamic Social Networks",
    "doi": "https://doi.org/10.1145/3457906",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Huan Wang; Chunming Qiao; Xuan Guo; Lei Fang; Ying Sha; Zhiguo Gong",
    "corresponding_authors": "",
    "abstract": "Recently, dynamic social network research has attracted a great amount of attention, especially in the area of anomaly analysis that analyzes the anomalous change in the evolution of dynamic social networks. However, most of the current research focused on anomaly analysis of the macro representation of dynamic social networks and failed to analyze the nodes that have anomalous structural changes at a micro level. To identify and evaluate anomalous structural change-based nodes in generalized dynamic social networks that only have limited structural information, this research considers undirected and unweighted graphs and develops a multiple-neighbor superposition similarity method ( ), which mainly consists of a multiple-neighbor range algorithm ( ) and a superposition similarity fluctuation algorithm ( ). introduces observation nodes, characterizes the structural similarities of nodes within multiple-neighbor ranges, and proposes a new multiple-neighbor similarity index on the basis of extensional similarity indices. Subsequently, maximally reflects the structural change of each node, using a new superposition similarity fluctuation index from the perspective of diverse multiple-neighbor similarities. As a result, based on and , not only identifies anomalous structural change-based nodes by detecting the anomalous structural changes of nodes but also evaluates their anomalous degrees by quantifying these changes. Results obtained by comparing with state-of-the-art methods via extensive experiments show that can accurately identify anomalous structural change-based nodes and evaluate their anomalous degrees well.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3172515294",
    "type": "article"
  },
  {
    "title": "Efficient Time-Stamped Event Sequence Anonymization",
    "doi": "https://doi.org/10.1145/2532643",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Reza Sherkat; Jing Li; Nikos Mamoulis",
    "corresponding_authors": "",
    "abstract": "With the rapid growth of applications which generate timestamped sequences (click streams, GPS trajectories, RFID sequences), sequence anonymization has become an important problem, in that should such data be published or shared. Existing trajectory anonymization techniques disregard the importance of time or the sensitivity of events. This article is the first, to our knowledge, thorough study on time-stamped event sequence anonymization. We propose a novel and tunable generalization framework tailored to event sequences. We generalize time stamps using time intervals and events using a taxonomy which models the domain semantics. We consider two scenarios: (i) sharing the data with a single receiver (the SSR setting), where the receiver’s background knowledge is confined to a set of time stamps and time generalization suffices, and (ii) sharing the data with colluding receivers (the SCR setting), where time generalization should be combined with event generalization. For both cases, we propose appropriate anonymization methods that prevent both user identification and event prediction. To achieve computational efficiency and scalability, we propose optimization techniques for both cases using a utility-based index, compact summaries, fast to compute bounds for utility, and a novel taxonomy-aware distance function. Extensive experiments confirm the effectiveness of our approach compared with state of the art, in terms of information loss, range query distortion, and preserving temporal causality patterns. Furthermore, our experiments demonstrate efficiency and scalability on large-scale real and synthetic datasets.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2067256936",
    "type": "article"
  },
  {
    "title": "Relating Reputation and Money in Online Markets",
    "doi": "https://doi.org/10.1145/1841909.1841914",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Ashwin Swaminathan; Renan G. Cattelan; Ydo Wexler; Cherian V. Mathew; Darko Kirovski",
    "corresponding_authors": "",
    "abstract": "Reputation in online economic systems is typically quantified using counters that specify positive and negative feedback from past transactions and/or some form of transaction network analysis that aims to quantify the likelihood that a network user will commit a fraudulent transaction. These approaches can be deceiving to honest users from numerous perspectives. We take a radically different approach with the goal of guaranteeing to a buyer that a fraudulent seller cannot disappear from the system with profit following a set of fabricated transactions that total a certain monetary limit. Even in the case of stolen identity, such an adversary cannot produce illegal profit unless a buyer decides to pay over the suggested limit.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2108932096",
    "type": "article"
  },
  {
    "title": "Effort Mediates Access to Information in Online Social Networks",
    "doi": "https://doi.org/10.1145/2990506",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Jeon-Hyung Kang; Kristina Lerman",
    "corresponding_authors": "",
    "abstract": "Individuals’ access to information in a social network depends on how it is distributed and where in the network individuals position themselves. In addition, individuals vary in how much effort they invest in managing their social connections. Using data from a social media site, we study how the interplay between effort and network position affects social media users’ access to diverse and novel information. Previous studies of the role of networks in information access were limited in their ability to measure the diversity of information. We address this problem by learning the topics of interest to social media users from the messages they share online with followers. We use the learned topics to measure the diversity of information users receive from the people they follow online. We confirm that users in structurally diverse network positions, which bridge otherwise disconnected regions of the follower network, tend to be exposed to more diverse and novel information. We also show that users who invest more effort in their activity on the site are not only located in more structurally diverse positions within the network than the less engaged users but also receive more novel and diverse information when in similar network positions. These findings indicate that the relationship between network structure and access to information in networks is more nuanced than previously thought.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2595856476",
    "type": "article"
  },
  {
    "title": "Integrating Transactions into BPEL Service Compositions",
    "doi": "https://doi.org/10.1145/2757288",
    "publication_date": "2015-05-26",
    "publication_year": 2015,
    "authors": "Chang‐ai Sun; Xin Zhang; Yan Shang; Marco Aiello",
    "corresponding_authors": "",
    "abstract": "The concept of software as a service has been increasingly adopted to develop distributed applications. Ensuring the reliability of loosely coupled compositions is a challenging task because of the open, dynamic, and independent nature of composable services; this is especially true when the execution of a service-based process relies on independent but correlated services. Transactions are the prototypical case of compositions spanning across multiple services and needing properties to be valid throughout the whole execution. Although transaction protocols and service composition languages have been proposed in the past decade, a true viable and effective solution is still missing. In this article, we propose a systematic aspect-based approach to integrating transactions into service compositions, taking into account the well-known protocols: Web Service Transaction and Business Process Execution Language (BPEL). In our approach, transaction policies are first defined as a set of aspects. They are then converted to standard BPEL elements. Finally, these transaction-related elements and the original BPEL process are weaved together, resulting in a transactional executable BPEL process. At runtime, transaction management is the responsibility of a middleware, which implements the coordination framework and transaction protocols followed by the transactional BPEL process and transaction-aware Web services. To automate the proposed approach, we developed a supporting platform called Salan to aid the tasks of defining, validating, and weaving aspect-based transaction policies, and of deploying the transactional BPEL processes. By means of a case study, we demonstrate the proposed approach and evaluate the performance of the supporting platform. Experimental results show that this approach is effective in producing reliable business processes while reducing the need for direct human involvement.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2257590153",
    "type": "article"
  },
  {
    "title": "Classification of Layout vs. Relational Tables on the Web: Machine Learning with Rendered Pages",
    "doi": "https://doi.org/10.1145/3555349",
    "publication_date": "2022-08-09",
    "publication_year": 2022,
    "authors": "Waqar Haider; Yeliz Yeşilada",
    "corresponding_authors": "",
    "abstract": "Table mining on the web is an open problem, and none of the previously proposed techniques provides a complete solution. Most research focuses on the structure of the HTML document, but because of the nature and structure of the web, it is still a challenging problem to detect relational tables. Web Content Accessibility Guidelines (WCAG) also cover a wide range of recommendations for making tables accessible, but our previous work shows that these recommendations are also not followed; therefore, tables are still inaccessible to disabled people and automated processing. We propose a new approach to table mining by not looking at the HTML structure, but rather, the rendered pages by the browser. The first task in table mining on the web is to classify relational vs. layout tables, and here, we propose two alternative approaches for that task. We first introduce our dataset, which includes 725 web pages with 9,957 extracted tables. Our first approach extracts features from a page after being rendered by the browser, then applies several machine learning algorithms in classifying the layout vs. relational tables. The best result is with Random Forest with the accuracy of 97.2% (F1-score: 0.955) with 10-fold cross-validation. Our second approach classifies tables using images taken from the same sources using Convolutional Neural Network (CNN), which gives an accuracy of 95% (F1-score: 0.95). Our work here shows that the web’s true essence comes after it goes through a browser and using the rendered pages and tables, the classification is more accurate compared to literature and paves the way in making the tables more accessible.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4297816490",
    "type": "article"
  },
  {
    "title": "Investment and Risk Management with Online News and Heterogeneous Networks",
    "doi": "https://doi.org/10.1145/3532858",
    "publication_date": "2023-01-04",
    "publication_year": 2023,
    "authors": "Gary Ang; Ee‐Peng Lim",
    "corresponding_authors": "",
    "abstract": "Stock price movements in financial markets are influenced by large volumes of news from diverse sources on the web, e.g., online news outlets, blogs, social media. Extracting useful information from online news for financial tasks, e.g., forecasting stock returns or risks, is, however, challenging due to the low signal-to-noise ratios of such online information. Assessing the relevance of each news article to the price movements of individual stocks is also difficult, even for human experts. In this article, we propose the Guided Global-Local Attention-based Multimodal Heterogeneous Network (GLAM) model, which comprises novel attention-based mechanisms for multimodal sequential and graph encoding, a guided learning strategy, and a multitask training objective. GLAM uses multimodal information, heterogeneous relationships between companies and leverages significant local responses of individual stock prices to online news to extract useful information from diverse global online news relevant to individual stocks for multiple forecasting tasks. Our extensive experiments with multiple datasets show that GLAM outperforms other state-of-the-art models on multiple forecasting tasks and investment and risk management application case-studies.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4313591078",
    "type": "article"
  },
  {
    "title": "Heterogeneous Graph Transformer for Meta-structure Learning with Application in Text Classification",
    "doi": "https://doi.org/10.1145/3580508",
    "publication_date": "2023-01-30",
    "publication_year": 2023,
    "authors": "Shuhai Wang; Xin Liu; Xiao Pan; Hanjie Xu; Mingrui Liu",
    "corresponding_authors": "",
    "abstract": "The prevalent heterogeneous Graph Neural Network (GNN) models learn node and graph representations using pre-defined meta-paths or only automatically discovering meta-paths. However, the existing methods suffer from information loss due to neglecting undiscovered meta-structures with richer semantics than meta-paths in heterogeneous graphs. To take advantage of the current rich meta-structures in heterogeneous graphs, we propose a novel approach called HeGTM to automatically extract essential meta-structures (i.e., meta-paths and meta-graphs) from heterogeneous graphs. The discovered meta-structures can capture more prosperous relations between different types of nodes that can help the model to learn representations. Furthermore, we apply the proposed approach for text classification. Specifically, we first design a heterogeneous graph for the text corpus, and then apply HeGTM on the constructed text graph to learn better text representations that contain various semantic relations. In addition, our approach can also be used as a strong meta-structure extractor for other GNN models. In other words, the auto-discovered meta-structures can replace the pre-defined meta-paths. The experimental results on text classification demonstrate the effectiveness of our approach to automatically extracting informative meta-structures from heterogeneous graphs and its usefulness in acting as a meta-structure extractor for boosting other GNN models.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4318477386",
    "type": "article"
  },
  {
    "title": "Multi-stage reasoning on introspecting and revising bias for visual question answering",
    "doi": "https://doi.org/10.1145/3616399",
    "publication_date": "2023-08-28",
    "publication_year": 2023,
    "authors": "An-An Liu; Zimu Lu; Ning Xu; Min Liu; Chenggang Yan; Bolun Zheng; Bo Lv; Duan Yulong; Zhuang Shao; Xuanya Li",
    "corresponding_authors": "",
    "abstract": "Visual Question Answering (VQA) is a task that involves predicting an answer to a question depending on the content of an image. However, recent VQA methods have relied more on language priors between the question and answer rather than the image content. To address this issue, many debiasing methods have been proposed to reduce language bias in model reasoning. However, the bias can be divided into two categories: good bias and bad bias. Good bias can benefit to the answer prediction, while the bad bias may associate the models with the unrelated information. Therefore, instead of excluding good and bad bias indiscriminately in existing debiasing methods, we proposed a bias discrimination module to distinguish them. Additionally, bad bias may reduce the model’s reliance on image content during answer reasoning and thus attend little on image features updating. To tackle this, we leverage Markov theory to construct a Markov field with image regions and question words as nodes. This helps with feature updating for both image regions and question words, thereby facilitating more accurate and comprehensive reasoning about both the image content and question. To verify the effectiveness of our network, we evaluate our network on VQA v2 and VQA cp v2 datasets and conduct extensive quantity and quality studies to verify the effectiveness of our proposed network. Experimental resu- lts show that our network achieves significant performance against the previous state-of-the-art methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386223789",
    "type": "article"
  },
  {
    "title": "Model-directed Web transactions under constrained modalities",
    "doi": "https://doi.org/10.1145/1281480.1281482",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Zan Sun; Jalal Mahmud; I. V. Ramakrishnan; Saikat Mukherjee",
    "corresponding_authors": "",
    "abstract": "Online transactions (e.g., buying a book on the Web) typically involve a number of steps spanning several pages. Conducting such transactions under constrained interaction modalities as exemplified by small screen handhelds or interactive speech interfaces—the primary mode of communication for visually impaired individuals—is a strenuous, fatigue-inducing activity. But usually one needs to browse only a small fragment of a Web page to perform a transactional step such as a form fillout, selecting an item from a search results list, and so on. We exploit this observation to develop an automata-based process model that delivers only the “relevant” page fragments at each transactional step, thereby reducing information overload on such narrow interaction bandwidths. We realize this model by coupling techniques from content analysis of Web documents, automata learning and statistical classification. The process model and associated techniques have been incorporated into Guide-O, a prototype system that facilitates online transactions using speech/keyboard interface (Guide-O-Speech), or with limited-display size handhelds (Guide-O-Mobile). Performance of Guide-O and its user experience are reported.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1980738933",
    "type": "article"
  },
  {
    "title": "Long-term Measurement and Analysis of the Free Proxy Ecosystem",
    "doi": "https://doi.org/10.1145/3360695",
    "publication_date": "2019-11-26",
    "publication_year": 2019,
    "authors": "Diego Perino; Matteo Varvello; Claudio Soriente",
    "corresponding_authors": "",
    "abstract": "Free web proxies promise anonymity and censorship circumvention at no cost. Several websites publish lists of free proxies organized by country, anonymity level, and performance. These lists index hundreds of thousands of hosts discovered via automated tools and crowd-sourcing. A complex free proxy ecosystem has been forming over the years, of which very little is known. In this article, we shed light on this ecosystem via a distributed measurement platform that leverages both active and passive measurements. Active measurements are carried out by an infrastructure we name ProxyTorrent, which discovers free proxies, assesses their performance, and detects potential malicious activities. Passive measurements focus on proxy performance and usage in the wild, and are accomplished by means of a Chrome extension named Ciao. ProxyTorrent has been running since January 2017, monitoring up to 230K free proxies. Ciao was launched in March 2017 and has thus far served roughly 9.7K users and generated 14TB of traffic. Our analysis shows that less than 2% of the proxies announced on the Web indeed proxy traffic on behalf of users; further, only half of these proxies have decent performance and can be used reliably. Every day, around 5%--10% of the active proxies exhibit malicious behaviors, e.g., advertisement injection, TLS interception, and cryptojacking, and these proxies are also the ones providing the best performance. Through the analysis of more than 14TB of proxied traffic, we show that web browsing is the primary user activity. Geo-blocking avoidance—allegedly a popular use case for free web proxies—accounts for 30% or less of the traffic, and it mostly involves countries hosting popular geo-blocked content.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2991140804",
    "type": "article"
  },
  {
    "title": "SMINT",
    "doi": "https://doi.org/10.1145/3381833",
    "publication_date": "2020-05-03",
    "publication_year": 2020,
    "authors": "Huijun Wu; Chen Wang; Richard Nock; Wei Wang; Jie Yin; Kai Lü; Liming Zhu",
    "corresponding_authors": "",
    "abstract": "Sharing a pre-trained machine learning model, particularly a deep neural network via prediction APIs, is becoming a common practice on machine learning as a service (MLaaS) platforms nowadays. Although deep neural networks (DNN) have shown remarkable successes in many tasks, they are also criticized for the lack of interpretability and transparency. Interpreting a shared DNN model faces two additional challenges compared with interpreting a general model. (1) Limited training data can be disclosed to users. (2) The internal structure of the models may not be available. These two challenges impede the application of most existing interpretability approaches, such as saliency maps or influence functions, for DNN models. Case-based reasoning methods have been used for interpreting decisions; however, how to select and organize the data points under the constraints of shared DNN models is not discussed. Moreover, simply providing cases as explanations may not be sufficient for supporting instance level interpretability. Meanwhile, existing interpretation methods for DNN models generally lack the means to evaluate the reliability of the interpretation. In this article, we propose a framework named Shared Model INTerpreter (SMINT) to address the above limitations. We propose a new data structure called a boundary graph to organize training points to mimic the predictions of DNN models. We integrate local features, such as saliency maps and interpretable input masks, into the data structure to help users to infer the model decision boundaries. We show that the boundary graph is able to address the reliability issues in many local interpretation methods. We further design an algorithm named hidden-layer aware p-test to measure the reliability of the interpretations. Our experiments show that SMINT is able to achieve above 99% fidelity to corresponding DNN models on both MNIST and ImageNet by sharing only a tiny fraction of training data to make these models interpretable. The human pilot study demonstrates that SMINT provides better interpretability compared with existing methods. Moreover, we demonstrate that SMINT is able to assist model tuning for better performance on different user data.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3020915576",
    "type": "article"
  },
  {
    "title": "Web Content Classification Using Distributions of Subjective Quality Evaluations",
    "doi": "https://doi.org/10.1145/2994132",
    "publication_date": "2016-11-15",
    "publication_year": 2016,
    "authors": "Maria Rafalak; Dominik Deja; Adam Wierzbicki; Radosław Nielek; Michał Kąkol",
    "corresponding_authors": "",
    "abstract": "Machine learning algorithms and recommender systems trained on human ratings are widely in use today. However, human ratings may be associated with a high level of uncertainty and are subjective, influenced by demographic or psychological factors. We propose a new approach to the design of object classes from human ratings: the use of entire distributions to construct classes. By avoiding aggregation for class definition, our approach loses no information and can deal with highly volatile or conflicting ratings. The approach is based the concept of the Earth Mover's Distance (EMD), a measure of distance for distributions. We evaluate the proposed approach based on four datasets obtained from diverse Web content or movie quality evaluation services or experiments. We show that clusters discovered in these datasets using the EMD measure are characterized by a consistent and simple interpretation. Quality classes defined using entire rating distributions can be fitted to clusters of distributions in the four datasets using two parameters, resulting in a good overall fit. We also consider the impact of the composition of small samples on the distributions that are the basis of our classification approach. We show that using distributions based on small samples of 10 evaluations is still robust to several demographic and psychological variables. This observation suggests that the proposed approach can be used in practice for quality evaluation, even for highly uncertain and subjective ratings.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2554222760",
    "type": "article"
  },
  {
    "title": "FusE",
    "doi": "https://doi.org/10.1145/3306128",
    "publication_date": "2019-02-17",
    "publication_year": 2019,
    "authors": "Steffen Thoma; Andreas Thalhammer; Andreas Harth; Rudi Studer",
    "corresponding_authors": "",
    "abstract": "Many current web pages include structured data which can directly be processed and used. Search engines, in particular, gather that structured data and provide question answering capabilities over the integrated data with an entity-centric presentation of the results. Due to the decentralized nature of the web, multiple structured data sources can provide similar information about an entity. But data from different sources may involve different vocabularies and modeling granularities, which makes integration difficult. We present FusE, an approach that identifies similar entity-specific data across sources, independent of the vocabulary and data modeling choices. We apply our method along the scenario of a trustable knowledge panel, conduct experiments in which we identify and process entity data from web sources, and compare the output to a competing system. The results underline the advantages of the presented entity-centric data fusion approach.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2917395894",
    "type": "article"
  },
  {
    "title": "An Outsourcing Model for Alert Analysis in a Cybersecurity Operations Center",
    "doi": "https://doi.org/10.1145/3372498",
    "publication_date": "2020-01-09",
    "publication_year": 2020,
    "authors": "Ankit Shah; Rajesh Ganesan; Sushil Jajodia; Hasan Çam",
    "corresponding_authors": "",
    "abstract": "A typical Cybersecurity Operations Center (CSOC) is a service organization. It hires and trains analysts, whose task is to perform analysis of alerts that were generated while monitoring the client’s networks. Due to ever-increasing financial and infrastructure burden on a CSOC driven by the rapidly growing demand for security services, it would become prohibitively expensive to continually expand the size of a CSOC to meet the demands in the future. An alternative solution is to outsource the alert analysis process to on-demand analysts, to provide scalable CSOC service to its clients with features, such as (1) higher throughput, (2) higher quality, and (3) more economical service than the current in-house service. The current outsourcing model is not cost effective and an exact optimization model is computationally inefficient. This article presents a novel two-step sequential mixed integer programming optimization method that is used in the development of a new decision-support business model for outsourcing the alert analysis process. It is demonstrated that through this model, a CSOC can effectively deliver its alert management services with the above-mentioned features. Results indicate that the model is scalable, computationally viable, real-time implementable, and can deliver CSOC services that meet the service-level agreement (SLA) between the CSOC and its client. In addition, the article provides valuable insights into the cost of operating the new business process outsourcing model for cybersecurity services.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3013184828",
    "type": "article"
  },
  {
    "title": "Emotions Behind Drive-by Download Propagation on Twitter",
    "doi": "https://doi.org/10.1145/3408894",
    "publication_date": "2020-08-25",
    "publication_year": 2020,
    "authors": "Amir Javed; Pete Burnap; Matthew Williams; Omer Rana",
    "corresponding_authors": "",
    "abstract": "Twitter has emerged as one of the most popular platforms to get updates on entertainment and current events. However, due to its 280-character restriction and automatic shortening of URLs, it is continuously targeted by cybercriminals to carry out drive-by download attacks, where a user’s system is infected by merely visiting a Web page. Popular events that attract a large number of users are used by cybercriminals to infect and propagate malware by using popular hashtags and creating misleading tweets to lure users to malicious Web pages. A drive-by download attack is carried out by obfuscating a malicious URL in an enticing tweet and used as clickbait to lure users to a malicious Web page. In this article, we answer the following two questions: Why are certain malicious tweets retweeted more than others? Do emotions reflecting in a tweet drive virality? We gathered tweets from seven different sporting events over 3 years and identified those tweets that were used to carry to out a drive-by download attack. From the malicious ( N = 105, 642) and benign ( N = 169, 178) data sample identified, we built models to predict information flow size and survival. We define size as the number of retweets of an original tweet, and survival as the duration of the original tweet’s presence in the study window. We selected the zero-truncated negative binomial (ZTNB) regression method for our analysis based on the distribution exhibited by our dependent size measure and the comparison of results with other predictive models. We used the Cox regression technique to model the survival of information flows as it estimates proportional hazard rates for independent measures. Our results show that both social and content factors are statistically significant for the size and survival of information flows for both malicious and benign tweets. In the benign data sample, positive emotions and positive sentiment reflected in the tweet significantly predict size and survival. In contrast, for the malicious data sample, negative emotions, especially fear, are associated with both size and survival of information flows.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3046569080",
    "type": "article"
  },
  {
    "title": "Dynamic Offloading of Web Application Execution Using Snapshot",
    "doi": "https://doi.org/10.1145/3402124",
    "publication_date": "2020-07-28",
    "publication_year": 2020,
    "authors": "Hyuk-Jin Jeong; InChang Jeong; Soo‐Mook Moon",
    "corresponding_authors": "",
    "abstract": "Mobile web platforms are facing new demands for emerging applications, such as machine learning or augmented reality, which require significant computing powers beyond that of current mobile hardware. Computation offloading can accelerate these apps by offloading the computation-intensive parts of an app from a client to a powerful server. Unfortunately, previous studies of offloading in the field of web apps have a limitation for the offloading target code or require complex user annotations, hindering the widespread use of offloading in web apps. This article proposes a novel offloading system for web apps, which can simplify the offloading process by sending and receiving the execution state of a running web app in the form of another web app called the snapshot . Since running the snapshot restores the whole app state and continues the execution from the point where it was saved, we can offload regular web app computations that affect the DOM state as well as the JavaScript state, and we do not have to pre-install the app binary at the server. Moreover, the snapshot does not require any annotations to be captured, making computation offloading more transparent to app developers. We qualitatively compared the proposed system with previous approaches in terms of programming difficulty and the scope of offloadable codes. In addition, we implemented the proposed system based on a WebKit browser and evaluated the offloading performance with five computation-intensive web apps. Our system achieved significant speedup (from 1.7 to approximately 9.0) in all of the apps, compared to local execution, which proves the feasibility of the proposed approach.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3052799614",
    "type": "article"
  },
  {
    "title": "Camera Brand Congruence and Camera Model Propagation in the Flickr Social Graph",
    "doi": "https://doi.org/10.1145/2019643.2019647",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Adish Singla; Ingmar Weber",
    "corresponding_authors": "",
    "abstract": "Given that my friends on Flickr use cameras of brand X, am I more likely to also use a camera of brand X? Given that one of these friends changes her brand, am I likely to do the same? Do new camera models pop up uniformly in the friendship graph? Or do early adopters then “convert” their friends? Which factors influence the conversion probability of a user? These are the kind of questions addressed in this work. Direct applications involve personalized advertising in social networks. For our study, we crawled a complete connected component of the Flickr friendship graph with a total of 67M edges and 3.9M users. 1.2M of these users had at least one public photograph with valid model metadata, which allowed us to assign camera brands and models to users and time slots. Similarly, we used, where provided in a user’s profile, information about a user’s geographic location and the groups joined on Flickr. Concerning brand congruence, our main findings are the following. First, a pair of friends on Flickr has a higher probability of being congruent, that is, using the same brand, compared to two random users (27% vs. 19%). Second, the degree of congruence goes up for pairs of friends (i) in the same country (29%), (ii) who both only have very few friends (30%), and (iii) with a very high cliqueness (38%). Third, given that a user changes her camera model between March-May 2007 and March-May 2008, high cliqueness friends are more likely than random users to do the same (54% vs. 48%). Fourth, users using high-end cameras are far more loyal to their brand than users using point-and-shoot cameras, with a probability of staying with the same brand of 60% vs 33%, given that a new camera is bought. Fifth, these “expert” users’ brand congruence reaches 66% for high cliqueness friends. All these differences are statistically significant at 1%. As for the propagation of new models in the friendship graph, we observe the following. First, the growth of connected components of users converted to a particular, new camera model differs distinctly from random growth. Second, the decline of dissemination of a particular model is close to random decline. This illustrates that users influence their friends to change to a particular new model, rather than from a particular old model. Third, having many converted friends increases the probability of the user to convert herself. Here differences between friends from the same or from different countries are more pronounced for point-and-shoot than for digital single-lens reflex users. Fourth, there was again a distinct difference between arbitrary friends and high cliqueness friends in terms of prediction quality for conversion.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2015678628",
    "type": "article"
  },
  {
    "title": "HTML Automatic Table Layout",
    "doi": "https://doi.org/10.1145/2435215.2435219",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Kim Marriott; Peter V. Moulder; Nathan Hurst",
    "corresponding_authors": "",
    "abstract": "Automatic layout of tables is required in online applications because of the need to tailor the layout to the viewport width, choice of font, and dynamic content. However, if the table contains text, minimizing the height of the table for a fixed maximum width is NP-hard. Thus, more efficient heuristic algorithms are required. We evaluate the HTML table layout recommendation and find that while it generally produces quite compact layout it is brittle and can lead to quite uncompact layout. We present an alternate heuristic algorithm. It uses a greedy strategy that starts from the widest reasonable layout and repeatedly chooses to narrow the column for which narrowing leads to the least increase in table height. The algorithm is simple, fast enough to be used in online applications, and gives significantly more compact layout than is obtained with HTML’s recommended table layout algorithm.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2034983235",
    "type": "article"
  },
  {
    "title": "Graph Attention Network for Text Classification and Detection of Mental Disorder",
    "doi": "https://doi.org/10.1145/3572406",
    "publication_date": "2022-12-14",
    "publication_year": 2022,
    "authors": "Usman Ahmed; Jerry Chun‐Wei Lin; Gautam Srivastava",
    "corresponding_authors": "",
    "abstract": "A serious issue in today’s society is Depression, which can have a devastating impact on a person’s ability to cope in daily life. Numerous studies have examined the use of data generated directly from users using social media to diagnose and detect Depression as a mental illness. Therefore, this paper investigates the language used in individuals’ personal expressions to identify depressive symptoms via social media. Graph Attention Networks (GATs) are used in this study as a solution to the problems associated with text classification of depression. These GATs can be constructed using masked self-attention layers. Rather than requiring expensive matrix operations such as similarity or knowledge of network architecture, this study implicitly assigns weights to each node in a neighbourhood. This is possible because nodes and words can carry properties and sentiments of their neighbours. Another aspect of the study that contributed to the expansion of the emotion lexicon was the use of hypernyms. As a result, our method performs better when applied to data from the Reddit subreddit Depression. Our experiments show that the emotion lexicon constructed by using the Graph Attention Network ROC achieves 0.91 while remaining simple and interpretable.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4311491308",
    "type": "article"
  },
  {
    "title": "Caching Historical Embeddings in Conversational Search",
    "doi": "https://doi.org/10.1145/3578519",
    "publication_date": "2022-12-29",
    "publication_year": 2022,
    "authors": "Ophir Frieder; Ida Mele; Cristina Ioana Muntean; Franco Maria Nardini; Raffaele Perego; Nicola Tonellotto",
    "corresponding_authors": "",
    "abstract": "Rapid response, namely low latency, is fundamental in search applications; it is particularly so in interactive search sessions, such as those encountered in conversational settings. An observation with a potential to reduce latency asserts that conversational queries exhibit a temporal locality in the lists of documents retrieved. Motivated by this observation, we propose and evaluate a client-side document embedding cache, improving the responsiveness of conversational search systems. By leveraging state-of-the-art dense retrieval models to abstract document and query semantics, we cache the embeddings of documents retrieved for a topic introduced in the conversation, as they are likely relevant to successive queries. Our document embedding cache implements an efficient metric index, answering nearest-neighbor similarity queries by estimating the approximate result sets returned. We demonstrate the efficiency achieved using our cache via reproducible experiments based on TREC CAsT datasets, achieving a hit rate of up to 75% without degrading answer quality. Our achieved high cache hit rates significantly improve the responsiveness of conversational systems while likewise reducing the number of queries managed on the search back-end.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4313331827",
    "type": "article"
  },
  {
    "title": "MuLX-QA: Classifying Multi-Labels and Extracting Rationale Spans in Social Media Posts",
    "doi": "https://doi.org/10.1145/3653303",
    "publication_date": "2024-03-21",
    "publication_year": 2024,
    "authors": "Soham Poddar; Rajdeep Mukherjee; Azlaan Mustafa Samad; Niloy Ganguly; Saptarshi Ghosh",
    "corresponding_authors": "",
    "abstract": "While social media platforms play an important role in our daily lives in obtaining the latest news and trends from across the globe, they are known to be prone to widespread proliferation of harmful information in different forms leading to misconceptions among the masses. Accordingly, several prior works have attempted to tag social media posts with labels/classes reflecting their veracity, sentiments, hate content, and so on. However, in order to have a convincing impact, it is important to additionally extract the post snippets on which the labelling decision is based. We call such a post snippet the rationale . These rationales significantly improve human trust and debuggability of the predictions, especially when detecting misinformation or stigmas from social media posts. These rationale spans or snippets are also helpful in post-classification social analysis, such as for finding out the target communities in hate-speech, or for understanding the arguments or concerns against the intake of vaccines. Also it is observed that a post may express multiple notions of misinformation, hate, sentiment, and the like. Thus, the task of determining (one or multiple) labels for a given piece of text, along with the text snippets explaining the rationale behind each of the identified labels is a challenging multi-label, multi-rationale classification task, which is still nascent in the literature. While transformer -based encoder-decoder generative models such as BART and T5 are well suited for the task, in this work we show how a relatively simpler encoder-only discriminative question-answering (QA) model can be effectively trained using simple template-based questions to accomplish the task. We thus propose MuLX-QA and demonstrate its utility in producing (label, rationale span) pairs in two different settings: multi-class (on the HateXplain dataset related to hate speech on social media), and multi-label (on the CAVES dataset related to COVID-19 anti-vaccine concerns). MuLX-QA outperforms heavier generative models in both settings. We also demonstrate the relative advantage of our proposed model MuLX-QA over strong baselines when trained with limited data. We perform several ablation studies, and experiments to better understand the effect of training MuLX-QA with different question prompts, and draw interesting inferences. Additionally, we show that MuLX-QA is effective on social media posts in resource-poor non-English languages as well. Finally, we perform a qualitative analysis of our model predictions and compare them with those of our strongest baseline.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393041728",
    "type": "article"
  },
  {
    "title": "Know their Customers: An Empirical Study of Online Account Enumeration Attacks",
    "doi": "https://doi.org/10.1145/3664201",
    "publication_date": "2024-05-07",
    "publication_year": 2024,
    "authors": "Maël Maceiras; Kavous Salehzadeh Niksirat; Gaël Bernard; Benoît Garbinato; Mauro Cherubini; Mathias Humbert; Kévin Huguenin",
    "corresponding_authors": "",
    "abstract": "Internet users possess accounts on dozens of online services where they are often identified by one of their e-mail addresses. They often use the same address on multiple services and for communicating with their contacts. In this paper, we investigate attacks that enable an adversary (e.g., company, friend) to determine (stealthily or not) whether an individual, identified by their e-mail address, has an account on certain services (i.e., an account enumeration attack ). Such attacks on account privacy have serious implications as information about one’s accounts can be used to (1) profile them and (2) improve the effectiveness of phishing. We take a multifaceted approach and study these attacks through a combination of experiments (63 services), surveys (318 respondents), and focus groups (13 participants). We demonstrate the high vulnerability of popular services (93.7%) and the concerns of users about their account privacy, as well as their increased susceptibility to phishing e-mails that impersonate services on which they have an account. We also provide findings on the challenges in implementing countermeasures for service providers and on users’ ideas for enhancing their account privacy. Finally, our interaction with national data protection authorities led to the inclusion of recommendations in their developers’ guide.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396695013",
    "type": "article"
  },
  {
    "title": "Fragment of Interest: Personalized Video Fragment Recommendation with Inter-Fragment &amp; Intra-Fragment Contextual Effect",
    "doi": "https://doi.org/10.1145/3700645",
    "publication_date": "2024-10-23",
    "publication_year": 2024,
    "authors": "Jiaqi Wang; R. Kwok; Edith C.‐H. Ngai",
    "corresponding_authors": "",
    "abstract": "In today’s fast-paced digital landscape, the attention span of users consuming video content is alarmingly brief, often as short as 15 seconds for music or entertainment videos and 6 minutes for lecture videos. This presents a significant challenge for video producers and platform providers as they seek to engage users with longer content. One promising solution involves recommending specific fragments within longer videos that align with individual user profiles. In this paper, we address this challenge by introducing a novel framework for video fragment recommendations, guided by three key insights. First, we implement a Self-Attention Block that captures the inter-fragment contextual effect, enhancing the relevance of recommendations. Second, we incorporate video-level preferences to ensure that the fragment recommendations are consistent with users’ overall interests. Third, we propose a Self-Attentive Herding Effect (SAHE) module to model the intra-fragment contextual effect, specifically the herding effect of time-sync comments within a fragment. To evaluate the effectiveness of our proposed method, we conduct extensive experiments comparing our model against the state-of-the-art approaches in terms of NDCG@K and Recall@K. Our results demonstrate that the model effectively leverages inter-fragment, intra-fragment contextual effects, and video-level preferences, outperforming existing methods. Additionally, we carry out empirical experiments to analyze the key components and parameters of the proposed model, providing further insights into its performance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403688099",
    "type": "article"
  },
  {
    "title": "AugmenToxic: Leveraging Reinforcement Learning to Optimize LLM Instruction Fine-Tuning for Data Augmentation to Enhance Toxicity Detection",
    "doi": "https://doi.org/10.1145/3700791",
    "publication_date": "2024-10-29",
    "publication_year": 2024,
    "authors": "Arezo Bodaghi; Benjamin C. M. Fung; Ketra Schmitt",
    "corresponding_authors": "",
    "abstract": "Addressing the challenge of toxic language in online discussions is crucial for the development of effective toxicity detection models. This pioneering work focuses on addressing imbalanced datasets in toxicity detection by introducing a novel approach to augment toxic language data. We create a balanced dataset by instructing fine-tuning of Large Language Models (LLMs) using Reinforcement Learning with Human Feedback (RLHF). Recognizing the challenges in collecting sufficient toxic samples from social media platforms for building a balanced dataset, our methodology involves sentence-level text data augmentation through paraphrasing existing samples using optimized generative LLMs. Leveraging generative LLM, we utilize the Proximal Policy Optimizer (PPO) as the RL algorithm to fine-tune the model further and align it with human feedback. In other words, we start by fine-tuning a LLM using an instruction dataset, specifically tailored for the task of paraphrasing while maintaining semantic consistency. Next, we apply PPO and a reward function, to further fine-tune (optimize) the instruction-tuned LLM. This RL process guides the model in generating toxic responses. We utilize the Google Perspective API as a toxicity evaluator to assess generated responses and assign rewards/penalties accordingly. This approach guides LLMs through PPO and the reward function, transforming minority class samples into augmented versions. The primary goal of our methodology is to create a balanced and diverse dataset to enhance the accuracy and performance of classifiers in identifying instances from the minority class. Utilizing two publicly available toxic datasets, we compared various techniques with our proposed method for generating toxic samples, demonstrating that our approach outperforms all others in producing a higher number of toxic samples. Starting with an initial 16,225 toxic prompts, our method successfully generated 122,951 toxic samples with a toxicity score exceeding 30%. Subsequently, we developed various classifiers using the generated balanced datasets and applied a cost-sensitive learning approach to the original imbalanced dataset. The findings highlight the superior performance of classifiers trained on data generated using our proposed method. These results highlight the importance of employing RL and a data-agnostic model as a reward mechanism for augmenting toxic data, thereby enhancing the robustness of toxicity detection models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403850457",
    "type": "article"
  },
  {
    "title": "Innovative Integration of Q-learning in BSO Algorithm for Adaptive Community Detection in Social Networks",
    "doi": "https://doi.org/10.1145/3707702",
    "publication_date": "2024-12-10",
    "publication_year": 2024,
    "authors": "Zohra Beldi; Malika Bessedik",
    "corresponding_authors": "",
    "abstract": "Community detection in social networks remains a challenging yet crucial task in social network analysis, offering insights into the underlying structures and dynamics of social systems. This study introduces QB-COM, a novel integration of reinforcement learning using Q-learning with Brain Storm Optimization algorithm (BSO), tailored for effective community detection. The proposed approach’s design is distinguished by its utilization of diverse data sources, including the network structure, evolving knowledge from BSO, and Q-learning intelligence, all intended to enhance solution generation and guide the search process adeptly. A thorough comparative analysis is carried out to assess the effectiveness of our algorithm, covering a range of meta-heuristics and heuristics used in community detection. This analysis utilizes diverse real-world networks and evaluation techniques. The results demonstrate QB-COM’s superior performance in accurately identifying true community structures within social networks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405248874",
    "type": "article"
  },
  {
    "title": "On the Aggression Diffusion Modeling and Minimization in Twitter",
    "doi": "https://doi.org/10.1145/3486218",
    "publication_date": "2021-12-09",
    "publication_year": 2021,
    "authors": "Marinos Poiitis; Athena Vakali; Nicolas Kourtellis",
    "corresponding_authors": "",
    "abstract": "Aggression in online social networks has been studied mostly from the perspective of machine learning, which detects such behavior in a static context. However, the way aggression diffuses in the network has received little attention as it embeds modeling challenges. In fact, modeling how aggression propagates from one user to another is an important research topic, since it can enable effective aggression monitoring, especially in media platforms, which up to now apply simplistic user blocking techniques. In this article, we address aggression propagation modeling and minimization in Twitter, since it is a popular microblogging platform at which aggression had several onsets. We propose various methods building on two well-known diffusion models, Independent Cascade ( IC ) and Linear Threshold ( LT ), to study the aggression evolution in the social network. We experimentally investigate how well each method can model aggression propagation using real Twitter data, while varying parameters, such as seed users selection, graph edge weighting, users’ activation timing, and so on. It is found that the best performing strategies are the ones to select seed users with a degree-based approach, weigh user edges based on their social circles’ overlaps, and activate users according to their aggression levels. We further employ the best performing models to predict which ordinary real users could become aggressive (and vice versa) in the future, and achieve up to AUC = 0.89 in this prediction task. Finally, we investigate aggression minimization by launching competitive cascades to “inform” and “heal” aggressors. We show that IC and LT models can be used in aggression minimization, providing less intrusive alternatives to the blocking techniques currently employed by Twitter.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200372824",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1658373",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "User mobility has given rise to a variety of Web applications, in which the global positioning system (GPS) plays many important roles in bridging between these applications and end users. As a kind of human behavior, transportation modes, such as ...",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4243514115",
    "type": "paratext"
  },
  {
    "title": "Improving Conformance of Web Services: A Constraint-based Model-driven Approach",
    "doi": "https://doi.org/10.1145/3580515",
    "publication_date": "2023-01-19",
    "publication_year": 2023,
    "authors": "Chang‐ai Sun; An Fu; Jingting Jia; Meng Li; Jun Han",
    "corresponding_authors": "",
    "abstract": "Web services have been widely used to develop complex distributed software systems in the context of Service Oriented Architecture (SOA). As a standard for describing Web services, the Web Service Description Language (WSDL) provides a universal mechanism to describe the service’s functionalities for the service consumers. However, the current WSDL only provides the description of the interfaces to a Web Service without any restrictions or assumptions on how to properly invoke the service, resulting in divergent understanding of the Web service’s behavior between the service developer and service consumer. A particular challenge is how to make explicit the various behavior assumptions and restrictions of a service (for the user), and make sure that the service implementation conforms to them (for the developer). In this article, we propose a constraint-based model-driven approach to improving the behavior conformance of Web services. In our approach, constraints are introduced in an extended WSDL, called CxWSDL, to formally and explicitly express the implicit restrictions and assumptions on the behavior of a Web service, and then the predefined constraints are used to derive test cases in a model-driven manner to test the service implementation’s conformance to its behavior constraints from the user’s perspective. An empirical study involving four real-life Web services was conducted to evaluate the effectiveness of our approach, and four actual inconsistencies were discovered.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4317502645",
    "type": "article"
  },
  {
    "title": "A Novel Review Helpfulness Measure Based on the User-Review-Item Paradigm",
    "doi": "https://doi.org/10.1145/3585280",
    "publication_date": "2023-02-23",
    "publication_year": 2023,
    "authors": "Luca Pajola; Dongkai Chen; Mauro Conti; V. S. Subrahmanian",
    "corresponding_authors": "",
    "abstract": "Review platforms are viral online services where users share and read opinions about products (e.g., a smartphone) or experiences (e.g., a meal at a restaurant). Other users may be influenced by such opinions when deciding what to buy. The usability of review platforms is currently limited by the massive number of opinions on many products. Therefore, showing only the most helpful reviews for each product is in the best interest of both users and the platform (e.g., Amazon). The current state of the art is far from accurate in predicting how helpful a review is. First, most existing works lack compelling comparisons as many studies are conducted on datasets that are not publicly available. As a consequence, new studies are not always built on top of prior baselines. Second, most existing research focuses only on features derived from the review text, ignoring other fundamental aspects of the review platforms (e.g., the other reviews of a product, the order in which they were submitted). In this article, we first carefully review the most relevant works in the area published during the last 20 years. We then propose the User-Review-Item (URI) paradigm, a novel abstraction for modeling the problem that moves the focus of the feature engineering from the review to the platform level. We empirically validate the URI paradigm on a dataset of products from six Amazon categories with 270 trained models: on average, classifiers gain +4% in F1-score when considering the whole review platform context. In our experiments, we further emphasize some problems with the helpfulness prediction task: (1) the users’ writing style changes over time (i.e., concept drift), (2) past models do not generalize well across different review categories, and (3) past methods to generate the ground truth produced unreliable helpfulness scores, affecting the model evaluation phase.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4321615257",
    "type": "review"
  },
  {
    "title": "Niffler: Real-time Device-level Anomalies Detection in Smart Home",
    "doi": "https://doi.org/10.1145/3586073",
    "publication_date": "2023-03-01",
    "publication_year": 2023,
    "authors": "Haohua Du; Yue Wang; Xiaoya Xu; Mingsheng Liu",
    "corresponding_authors": "",
    "abstract": "Device-level security has become a major concern in smart home systems. Detecting problems in smart home sytems strives to increase accuracy in near real time without hampering the regular tasks of the smart home. The current state of the art in detecting anomalies in smart home devices is mainly focused on the app level, which provides a basic level of security by assuming that the devices are functioning correctly. However, this approach is insufficient for ensuring the overall security of the system, as it overlooks the possibility of anomalies occurring at the lower layers such as the devices. In this article, we propose a novel notion, correlated graph , and with the aid of that, we develop our system to detect misbehaving devices without modifying the existing system. Our correlated graphs explicitly represent the contextual correlations among smart devices with little knowledge about the system. We further propose a linkage path model and a sensitivity ranking method to assist in detecting the abnormalities. We implement a semi-automatic prototype of our approach, evaluate it in real-world settings, and demonstrate its efficiency, which achieves an accuracy of around 90% in near real time.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4322736846",
    "type": "article"
  },
  {
    "title": "To Re-experience the Web: A Framework for the Transformation and Replay of Archived Web Pages",
    "doi": "https://doi.org/10.1145/3589206",
    "publication_date": "2023-03-27",
    "publication_year": 2023,
    "authors": "John A. Berlin; Mat Kelly; Michael L. Nelson; Michele C. Weigle",
    "corresponding_authors": "",
    "abstract": "When replaying an archived web page, or memento , the fundamental expectation is that the page should be viewable and function exactly as it did at the archival time. However, this expectation requires web archives upon replay to modify the page and its embedded resources so that all resources and links reference the archive rather than the original server. Although these modifications necessarily change the state of the representation, it is understood that without them the replay of mementos from the archive would not be possible. The process of replaying mementos and the modifications made to the representations by web archives varies between archives. Because of this, there is no standard terminology for describing the replay and needed modifications. In this article, we propose terminology for describing the existing styles of replay and the modifications made on the part of web archives to mementos to facilitate replay. Because of issues discovered with server-side only modifications, we propose a general framework for the auto-generation of client-side rewriting libraries. Finally, we evaluate the effectiveness of using a generated client-side rewriting library to augment the existing replay systems of web archives by crawling mementos replayed from the Internet Archive’s Wayback Machine with and without the generated client-side rewriter. By using the generated client-side rewriter, we were able to decrease the cumulative number of requests blocked by the content security policy of the Wayback Machine for 577 mementos by 87.5% and increased the cumulative number of requests made by 32.8%. We were also able to replay mementos that were previously not replayable from the Internet Archive. Many of the client-side rewriting ideas described in this work have been implemented into Wombat, a client-side URL rewriting system that is used by the Webrecorder, Pywb, and Wayback Machine playback systems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4361003577",
    "type": "article"
  },
  {
    "title": "Closeness Centrality on Uncertain Graphs",
    "doi": "https://doi.org/10.1145/3604912",
    "publication_date": "2023-06-15",
    "publication_year": 2023,
    "authors": "Zhenfang Liu; Jianxiong Ye; Zhaonian Zou",
    "corresponding_authors": "",
    "abstract": "Centrality is a family of metrics for characterizing the importance of a vertex in a graph. Although a large number of centrality metrics have been proposed, a majority of them ignores uncertainty in graph data. In this article, we formulate closeness centrality on uncertain graphs and define the batch closeness centrality evaluation problem that computes the closeness centrality of a subset of vertices in an uncertain graph. We develop three algorithms, MS-BCC , MG-BCC, and MGMS-BCC , based on sampling to approximate the closeness centrality of the specified vertices. All these algorithms require to perform breadth-first searches (BFS) starting from the specified vertices on a large number of sampled possible worlds of the uncertain graph. To improve the efficiency of the algorithms, we exploit operation-level parallelism of the BFS traversals and simultaneously execute the shared sequences of operations in the breadth-first searches. Parallelization is realized at different levels in these algorithms. The experimental results show that the proposed algorithms can efficiently and accurately approximate the closeness centrality of the given vertices. MGMS-BCC is faster than both MS-BCC and MG-BCC because it avoids more repeated executions of the shared operation sequences in the BFS traversals.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4380677132",
    "type": "article"
  },
  {
    "title": "Scraping Relevant Images from Web Pages without Download",
    "doi": "https://doi.org/10.1145/3616849",
    "publication_date": "2023-08-19",
    "publication_year": 2023,
    "authors": "Erdinç Uzun",
    "corresponding_authors": "Erdinç Uzun",
    "abstract": "Automatically scraping relevant images from web pages is an error-prone and time-consuming task, leading experts to prefer manually preparing extraction patterns for a website. Existing web scraping tools are built on these patterns. However, this manual approach is laborious and requires specialized knowledge. Automatic extraction approaches, while a potential solution, require large training datasets and numerous features, including width, height, pixels, and file size, that can be difficult and time-consuming to obtain. To address these challenges, we propose a semi-automatic approach that does not require an expert, utilizes small training datasets, and has a low error rate while saving time and storage. Our approach involves clustering web pages from a website and suggesting several pages for a non-expert to annotate relevant images. The approach then uses these annotations to construct a learning model based on textual data from the HTML elements. In the experiments, we used a dataset of 635,015 images from 200 news websites, each containing 100 pages, with 22,632 relevant images. When comparing several machine learning methods for both automatic approaches and our proposed approach, the AdaBoost method yields the best performance results. When using automatic extraction approaches, the best f-Measure that can be achieved is 0.805 with a learning model constructed from a large training dataset consisting of 120 websites (12,000 web pages). In contrast, our approach achieved an average f-Measure of 0.958 for 200 websites with only six web pages annotated per website. This means that a non-expert only needs to examine 1,200 web pages to determine the relevant images for 200 websites. Our approach also saves time and storage space by not requiring the download of images and can be easily integrated into currently available web scraping tools, because it is based on textual data.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386001693",
    "type": "article"
  },
  {
    "title": "Edge Caching Placement Strategy based on Evolutionary Game for Conversational Information Seeking in Edge Cloud Computing",
    "doi": "https://doi.org/10.1145/3624985",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Hongjian Shi; Meng Zhang; Ruhui Ma; Liwei Lin; Rui Zhang; Haibing Guan",
    "corresponding_authors": "",
    "abstract": "In Internet applications, network conversation is the primary communication between the user and server. The server needs to efficiently and quickly return the corresponding service according to the conversation sent by the user to improve the users’ Quality of Service. Thus, Conversation Information Seeking (CIS) research has become a hot topic today. In Cloud Computing (CC), a central service mode, the conversation is transmitted between the user and the remote cloud over a long distance. With the explosive growth of Internet applications, network congestion, long-distance communication, and single point of failure have brought new challenges to the centralized service mode. People put forward Edge Cloud Computing (ECC) to meet the new challenges of the centralized service mode of CC. As a distributed service mode, ECC is an extension of CC. By migrating services from the remote cloud to the network edge closer to users, ECC can solve the above challenges in CC well. In ECC, people solve the problem of CIS through edge caching. The current research focuses on designing the edge cache strategy to achieve more predictable caching. In this article, we propose an edge cache placement method Evolutionary Game based Caching Placement Strategy (EG-CPS). This method consists of three modules: the user preference prediction module, the content popularity calculation module, and the cache placement decision module. To maximize the predictability of the cache strategy, we are committed to optimizing the cache hit rate and service latency. The simulation experiment compares the proposed strategy with several other cache strategies. The experimental results illustrate that EG-CPS can reduce up to 2.4% of the original average content request latency, increase the average direct cache hit rate by 1.7%, and increase the average edge cache hit rate by 3.3%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386896815",
    "type": "article"
  },
  {
    "title": "Adoption of Recurrent Innovations: A Large-Scale Case Study on Mobile App Updates",
    "doi": "https://doi.org/10.1145/3626189",
    "publication_date": "2023-10-10",
    "publication_year": 2023,
    "authors": "Fuqi Lin; Xuan Lü; Wei Ai; Huoran Li; Yun Ma; Yulian Yang; Hongfei Deng; Qingxiang Wang; Qiaozhu Mei; Xuanzhe Liu",
    "corresponding_authors": "",
    "abstract": "Modern technology innovations feature a successive and even recurrent procedure. Intervals between old and new generations of technology are shrinking, and the Internet and Web services have facilitated the fast adoption of an innovation even before the convergence of its predecessor. While the adoption and diffusion of innovations have been studied for decades, most theories and analyses focus on single and one-time innovations. Meanwhile, limited work has investigated successive innovations while lacking user-level analysis, possibly due to the unavailability of fine-grained adoption behavior data. In this study, we present the first large-scale analysis of the adoption of recurrent innovations in the context of mobile app updates, investigating how millions of users consume various versions of thousands of apps on their mobile devices. Our analysis reveals novel patterns of crowd and individual adoption behaviors, which suggest the need for new categories of adopters to be added on top of the Rogers model of innovation diffusion. We show that standard machine learning models are able to pick up various sources of signals to predict whether users in these different categories will adopt a new version of an app and how soon they will adopt it.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387502660",
    "type": "article"
  },
  {
    "title": "Characterizing and Predicting Users’ Behavior on Local Search Queries",
    "doi": "https://doi.org/10.1145/3157059",
    "publication_date": "2018-05-27",
    "publication_year": 2018,
    "authors": "Fidel Cacheda; Roi Blanco; Nicola Barbieri",
    "corresponding_authors": "",
    "abstract": "The use of queries to find products and services that are located nearby is increasing rapidly due mainly to the ubiquity of internet access and location services provided by smartphone devices. Local search engines help users by matching queries with a predefined geographical connotation (“local queries”) against a database of local business listings. Local search differs from traditional Web search because, to correctly capture users’ click behavior, the estimation of relevance between query and candidate results must be integrated with geographical signals, such as distance. The intuition is that users prefer businesses that are physically closer to them or in a convenient area (e.g., close to their home). However, this notion of closeness depends upon other factors, like the business category, the quality of the service provided, the density of businesses in the area of interest, the hour of the day, or even the day of the week. In this work, we perform an extensive analysis of online users’ interactions with a local search engine, investigating their intent, temporal patterns, and highlighting relationships between distance-to-business and other factors, such as business reputation, Furthermore, we investigate the problem of estimating the click-through rate on local search ( LCTR ) by exploiting the combination of standard retrieval methods with a rich collection of geo-, user-, and business-dependent features. We validate our approach on a large log collected from a real-world local search service. Our evaluation shows that the non-linear combination of business and user information, geo-local and textual relevance features leads to a significant improvements over existing alternative approaches based on a combination of relevance, distance, and business reputation [1].",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2807701354",
    "type": "article"
  },
  {
    "title": "Localness of Location-based Knowledge Sharing",
    "doi": "https://doi.org/10.1145/2983645",
    "publication_date": "2018-07-17",
    "publication_year": 2018,
    "authors": "Sangkeun Park; Mark S. Ackerman; Uichin Lee",
    "corresponding_authors": "",
    "abstract": "In location-based social Q8A services, people ask a question with a high expectation that local residents who have local knowledge will answer the question. However, little is known about the locality of user activities in location-based social Q8A services. This study aims to deepen our understanding of location-based knowledge sharing by investigating the following: general behavioral characteristics of users, the topical and typological patterns related to geographic characteristics, geographic locality of user activities, and motivations of local knowledge sharing. To this end, we analyzed a 12-month period Q8A dataset from Naver KiN “Here,” a location-based social Q8A mobile app, in addition to a supplementary survey dataset obtained from 285 mobile users. Our results reveal several unique characteristics of location-based social Q8A. When compared with conventional social Q8A sites, users ask and answer different topical/typological questions. In addition, those who answer have a strong spatial locality wherein they primarily have local knowledge in a few regions, in areas such as their home and work. We also find unique motivators such as ownership of local knowledge and a sense of local community. The findings reported in the article have significant implications for the design of Q8A systems, especially location-based social Q8A systems.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2883890100",
    "type": "article"
  },
  {
    "title": "Top-k User-Defined Vertex Scoring Queries in Edge-Labeled Graph Databases",
    "doi": "https://doi.org/10.1145/3213891",
    "publication_date": "2018-09-27",
    "publication_year": 2018,
    "authors": "Francesco Parisi; Noseong Park; Andrea Pugliese; V. S. Subrahmanian",
    "corresponding_authors": "",
    "abstract": "We consider identifying highly ranked vertices in large graph databases such as social networks or the Semantic Web where there are edge labels. There are many applications where users express scoring queries against such databases that involve two elements: (i) a set of patterns describing relationships that a vertex of interest to the user must satisfy and (ii) a scoring mechanism in which the user may use properties of the vertex to assign a score to that vertex. We define the concept of a partial pattern map query (partial PM-query), which intuitively allows us to prune partial matchings, and show that finding an optimal partial PM-query is NP-hard. We then propose two algorithms, PScore_LP and PScore_NWST, to find the answer to a scoring (top- k ) query. In PScore_LP, the optimal partial PM-query is found using a list-oriented pruning method. PScore_NWST leverages node-weighted Steiner trees to quickly compute slightly sub-optimal solutions. We conduct detailed experiments comparing our algorithms with (i) an algorithm (PScore_Base) that computes all answers to the query, evaluates them according to the scoring method, and chooses the top- k , and (ii) two Semantic Web query processing systems (Jena and GraphDB). Our algorithms show better performance than PScore_Base and the Semantic Web query processing systems—moreover, PScore_NWST outperforms PScore_LP on large queries and on queries with a tree structure.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2893228100",
    "type": "article"
  },
  {
    "title": "Layout Cross-Platform and Cross-Browser Incompatibilities Detection using Classification of DOM Elements",
    "doi": "https://doi.org/10.1145/3316808",
    "publication_date": "2019-03-22",
    "publication_year": 2019,
    "authors": "Willian Massami Watanabe; Giovana Lázaro Amêndola; Fagner Christian Paes",
    "corresponding_authors": "",
    "abstract": "Web applications can be accessed through a variety of user agent configurations, in which the browser, platform, and device capabilities are not under the control of developers. In order to grant the compatibility of a web application in each environment, developers must manually inspect their web application in a wide variety of devices, platforms, and browsers. Web applications can be rendered inconsistently depending on the browser, the platform, and the device capabilities which are used. Furthermore, the devices’ different viewport widths impact the way web applications are rendered in them, in which elements can be resized and change their absolute positions in the display. These adaptation strategies must also be considered in automatic incompatibility detection approaches in the state of the art. Hence, we propose a classification approach for detecting Layout Cross-platform and Cross-browser incompatibilities, which considers the adaptation strategies used in responsive web applications. Our approach is an extension of previous Cross-browser incompatibility detection approaches and has the goal of reducing the cost associated with manual inspections in different devices, platforms, and browsers, by automatically detecting Layout incompatibilities in this scenario. The proposed approach classifies each DOM element which composes a web application as an incompatibility or not, based on its attributes, position, alignment, screenshot, and the viewport width of the browser. We report the results of an experiment conducted with 42 Responsive Web Applications, rendered in three devices (Apple iPhone SE, Apple iPhone 8 Plus, and Motorola Moto G4) and browsers (Google Chrome and Apple Safari). The results (with F-measure of 0.70) showed evidence which quantify the effectiveness of our classification approach, and it could be further enhanced for detecting Cross-platform and Cross-browser incompatibilities. Furthermore, in the experiment, our approach also performed better when compared to a former state-of-the-art classification technique for Cross-browser incompatibilities detection.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2929086838",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2180861",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The widespread diffusion of Web-based services provided by public and private organizations emphasizes the need for a flexible solution for protecting the information accessible through Web applications. A promising approach is represented by credential-...",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4246225722",
    "type": "paratext"
  },
  {
    "title": "An Integrated Approach for Improving Brand Consistency of Web Content",
    "doi": "https://doi.org/10.1145/3450445",
    "publication_date": "2021-05-04",
    "publication_year": 2021,
    "authors": "Soumyadeep Roy; Shamik Sural; Niyati Chhaya; Anandhavelu Natarajan; Niloy Ganguly",
    "corresponding_authors": "",
    "abstract": "A consumer-dependent (business-to-consumer) organization tends to present itself as possessing a set of human qualities, which is termed as the brand personality of the company. The perception is impressed upon the consumer through the content, be it in the form of advertisement, blogs or magazines, produced by the organization. A consistent brand will generate trust and retain customers over time as they develop an affinity towards regularity and common patterns. However, maintaining a consistent messaging tone for a brand has become more challenging with the virtual explosion in the amount of content which needs to be authored and pushed to the Internet to maintain an edge in the era of digital marketing. To understand the depth of the problem, we collect around 300K web page content from around 650 companies. We develop trait-specific classification models by considering the linguistic features of the content. The classifier automatically identifies the web articles which are not consistent with the mission and vision of a company and further helps us to discover the conditions under which the consistency cannot be maintained. To address the brand inconsistency issue, we then develop a sentence ranking system that outputs the top three sentences that need to be changed for making a web article more consistent with the company's brand personality.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3161125008",
    "type": "article"
  },
  {
    "title": "Modeling and Simulating the Web of Things from an Information Retrieval Perspective",
    "doi": "https://doi.org/10.1145/3132732",
    "publication_date": "2017-11-14",
    "publication_year": 2017,
    "authors": "Cristyan Manta-Caro; Juan M. Fernández‐Luna",
    "corresponding_authors": "",
    "abstract": "Internet and Web technologies have changed our lives in ways we are not yet fully aware of. In the near future, Internet will interconnect more than 50 billion things in the real world, nodes will sense billions of features and properties of interest, and things will be represented by web-based, bi-directional services with highly dynamic content and real-time data. This is the new era of the Internet and the Web of Things . Since the emergence of such paradigms implies the evolution and integration of the systems with which they interact, it is essential to develop abstract models for representing and simulating the Web of Things in order to establish new approaches. This article describes a Web of Things model based on a structured XML representation. We also present a simulator whose ultimate goal is to encapsulate the expected dynamics of the Web of Things for the future development of information retrieval (IR) systems. The simulator generates a real-time collection of XML documents containing spatio-temporal contexts and textual and sensed information of highly dynamic dimensions. The simulator is characterized by its flexibility and versatility for representing real-world scenarios and offers a unique perspective for information retrieval. In this article, we evaluate and test the simulator in terms of its performance variables for computing resource consumption and present our experimentation with the simulator on three real scenarios by considering the generation variables for the IR document collection.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2770785666",
    "type": "article"
  },
  {
    "title": "Mutexion: Mutually Exclusive Compression System for Mitigating Compression Side-Channel Attacks",
    "doi": "https://doi.org/10.1145/3532850",
    "publication_date": "2022-09-07",
    "publication_year": 2022,
    "authors": "Taegeun Moon; Hyoungshick Kim; Sangwon Hyun",
    "corresponding_authors": "",
    "abstract": "To enhance the performance of web services, web servers often compress data to be delivered. Unfortunately, the data compression technique has also introduced a side effect called compression side-channel attacks (CSCA) . CSCA allows eavesdroppers to unveil secret strings included in the encrypted traffic by observing the length of data. A promising defense technique called Debreach was recently proposed to mitigate CSCA by excluding all secret data in a web page during the compression process. Although Debreach has proven to be safe against CSCA and outperforms other approaches, the exclusion of all secret data from compression eventually resulted in a decreased compression efficiency. In this paper, we present a highly efficient CSCA mitigation system called “Mutexion” ( Mut ually ex clusive compress ion ) which allows us to fully take advantage of compression over an entire web page, including secret data. The key idea behind Mutexion is to fully take advantage of all the matching subsequences within a web page except only for those between secret data and user-controlled data (potentially controlled by an attacker) during the compression process. This approach of Mutexion effectively prevents side-channel leaks of secret data under CSCA misusing user-controlled data in a web page while minimizing the degradation in compression efficiency. It is required for our compressor to trace both secret data and user-controlled data in its compression process of web pages. To meet this requirement, we provide techniques to enable automated annotation of secret and user-controlled data in web pages. We implemented Mutexion as a fully working system to test live web pages and evaluated its performance with respect to security and compression efficiency. Our evaluation results demonstrated that Mutexion effectively prevents CSCA and also achieves almost the same compression ratio as the original zlib, which is vulnerable to CSCA, with a slight increase (0.032 milliseconds (7.9%) on average) in execution time.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4294877146",
    "type": "article"
  },
  {
    "title": "Spotting Flares: The Vital Signs of the Viral Spread of Tweets Made During Communal Incidents",
    "doi": "https://doi.org/10.1145/3550357",
    "publication_date": "2022-09-13",
    "publication_year": 2022,
    "authors": "Apoorva Upadhyaya; Joydeep Chandra",
    "corresponding_authors": "",
    "abstract": "With the increasing use of Twitter for encouraging users to instigate violent behavior with hate and racial content, it becomes necessary to investigate the uniqueness in the dynamics of the spread of tweets made during violent communal incidents and the challenges they pose in early identification of potential viral content. In this article, we study the spread of the tweets made during several violent communal incidents along four major dimensions — the underlying follower network of the users, their structural and engagement characteristics, the cascades, and the cognitive aspects of the content, each of which plays a vital role in the spread of content. Using large public and collected data, we compare these features with tweets related to other subjects from several major domains, such as non-violent political events, celebrities, and technology, that contribute to a large fraction of the viral content over Twitter. We discover that while the spread of cascades and the users involved may provide strong early evidence of the viral content for several domains, the early phases of the spread of viral tweets related to violent communal incidents are characterized by cascades with protracted growth involving fringe or low-importance users, which would possibly make early prediction difficult. Our findings indicate that an interplay of certain network and cascade properties, together with the cognitive characteristics of tweets and the behavioral patterns of the engaging users, may provide stronger early indicators of the virality of this content.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4296593652",
    "type": "article"
  },
  {
    "title": "FinTech on the Web: An Overview",
    "doi": "https://doi.org/10.1145/3572404",
    "publication_date": "2022-12-14",
    "publication_year": 2022,
    "authors": "Chung-Chi Chen; Hen‐Hsen Huang; Hiroya Takamura; Makoto P. Kato; Yu-Lieh Huang",
    "corresponding_authors": "",
    "abstract": "In this article, we provide an overview of ACM TWEB’s special issue, Financial Technology on the Web . This special issue covers diverse topics: (1) a new architecture for leveraging online news to investment and risk management, (2) a cross-platform analysis of the post quality and users’ behaviors, and (3) an empirical study on disentangling decentralized finance compositions. In addition to a guide for the special issue, we also share a brief opinion on the future of financial technology on the Web.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4311491315",
    "type": "article"
  },
  {
    "title": "New Phone, Who Dis? Modeling Millennials’ Backup Behavior",
    "doi": "https://doi.org/10.1145/3208105",
    "publication_date": "2018-12-04",
    "publication_year": 2018,
    "authors": "Elissa M. Redmiles; Eszter Hargittai",
    "corresponding_authors": "",
    "abstract": "Given the ever-rising frequency of malware attacks and other problems leading people to lose their files, backups are an important proactive protective behavior in which users can engage. Backing up files can prevent emotional and financial losses and improve overall user experience. Yet, we find that less than half of young adults perform mobile or computer backups regularly. To understand why, we model the factors that drive mobile and computer backup behavior, and changes in that behavior over time, using data from a panel survey of 384 diverse young adults. We develop a set of models that explain 37% and 38% of the variance in reported mobile and computer backup behaviors, respectively. These models show consistent relationships between Internet skills and backup frequency on both mobile and computer devices. We find that this relationship holds longitudinally: increases in Internet skills lead to increased frequency of computer backups. This article provides a foundation for understanding what drives young adults’ backup behavior. It concludes with recommendations for motivating people to back up, and for future work, modeling similar user behaviors.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2902163892",
    "type": "article"
  },
  {
    "title": "Mining Abstract XML Data-Types",
    "doi": "https://doi.org/10.1145/3267467",
    "publication_date": "2018-12-04",
    "publication_year": 2018,
    "authors": "Dionysis Athanasopoulos; Apostolos Zarras",
    "corresponding_authors": "",
    "abstract": "Schema integration has been a long-standing challenge for the data-engineering community that has received steady attention over the past three decades. General-purpose integration approaches construct unified schemas that encompass all schema elements. Schema integration has been revisited in the past decade in service-oriented computing since the input/output data-types of service interfaces are heterogeneous XML schemas. However, service integration differs from the traditional integration problem, since it should generalize schemas (mining abstract data-types) instead of unifying all schema elements. To mine well-formed abstract data-types, the fundamental Liskov Substitution Principle (LSP), which generally holds between abstract data-types and their subtypes, should be followed. However, due to the heterogeneity of service data-types, the strict employment of LSP is not usually feasible. On top of that, XML offers a rich type system, based on which data-types are defined via combining type patterns (e.g., composition, aggregation). The existing integration approaches have not dealt with the challenges of a defining subtyping relation between XML type patterns. To address these challenges, we propose a relaxed version of LSP between XML type patterns and an automated generalization process for mining abstract XML data-types. We evaluate the effectiveness and the efficiency of the process on the schemas of two datasets against two representative state-of-the-art approaches.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2902768770",
    "type": "article"
  },
  {
    "title": "Combining URL and HTML Features for Entity Discovery in the Web",
    "doi": "https://doi.org/10.1145/3365574",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Edimar Mânica; Carina F. Dorneles; Renata Galante",
    "corresponding_authors": "",
    "abstract": "The web is a large repository of entity-pages. An entity-page is a page that publishes data representing an entity of a particular type, for example, a page that describes a driver on a website about a car racing championship. The attribute values published in the entity-pages can be used for many data-driven companies, such as insurers, retailers, and search engines. In this article, we define a novel method, called SSUP , which discovers the entity-pages on the websites. The novelty of our method is that it combines URL and HTML features in a way that allows the URL terms to have different weights depending on their capacity to distinguish entity-pages from other pages, and thus the efficacy of the entity-page discovery task is increased. SSUP determines the similarity thresholds on each website without human intervention. We carried out experiments on a dataset with different real-world websites and a wide range of entity types. SSUP achieved a 95% rate of precision and 85% recall rate. Our method was compared with two state-of-the-art methods and outperformed them with a precision gain between 51% and 66%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2993537585",
    "type": "article"
  },
  {
    "title": "Roaming Through the Castle Tunnels",
    "doi": "https://doi.org/10.1145/3395050",
    "publication_date": "2020-06-27",
    "publication_year": 2020,
    "authors": "Yun Ma; Ziniu Hu; Diandian Gu; Li Zhou; Qiaozhu Mei; Gang Huang; Xuanzhe Liu",
    "corresponding_authors": "",
    "abstract": "Smartphone applications (a.k.a., apps) have become indispensable in our everyday life and work. In practice, accomplishing a task on smartphones may require the user to navigate among various apps. Unlike Web pages that are inherently interconnected through hyperlinks, apps are usually isolated building blocks, and the lack of direct links between apps has compromised the efficiency of task completion and user experience. In this article, we present the first in-depth empirical study of page-level access behaviors of smartphone users based on a comprehensive dataset collected through an extensive user study. We propose a model to distinguish informational pages and transitional pages , based on which we can extract page-level inter-app navigation. Surprisingly, the transitional pages account for quite substantial time cost and manual actions when navigating from the current informational page to the desirable informational page. We reveal that developing “ tunnels ” between “isolated” apps under specific usage scenarios has a huge potential to reduce the cost of navigation. Our analysis provides some practical implications on how to improve app-navigation experience from both the operating system’s perspective and the developer’s&lt;?brk?&gt; perspective.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3039249244",
    "type": "article"
  },
  {
    "title": "A Structured and Linguistic Approach to Understanding Recovery and Relapse in AA",
    "doi": "https://doi.org/10.1145/3423208",
    "publication_date": "2020-11-05",
    "publication_year": 2020,
    "authors": "S. Lawrence Bailey; Yue Zhang; Arti Ramesh; Jennifer Golbeck; Lise Getoor",
    "corresponding_authors": "",
    "abstract": "Alcoholism, also known as Alcohol Use Disorder (AUD), is a serious problem affecting millions of people worldwide. Recovery from AUD is known to be challenging and often leads to relapse at various points after enrolling in a rehabilitation program such as Alcoholics Anonymous (AA). In this work, we present a structured and linguistic approach using hinge-loss Markov random fields (HL-MRFs) to understand recovery and relapse from AUD using social media data. We evaluate our models on AA-attending users extracted from: (i) the Twitter social network and predict recovery at two different points—90 days and 1 year after the user joins AA, respectively, and (ii) the Reddit AA recovery forums and predict whether the participating user is currently sober. The two datasets present two facets of the same underlying problem of understanding recovery and relapse in AUD users. We flesh out different characteristics in both these datasets: (i) In the Twitter dataset, we focus on the social aspect of the users and the relationship with recovery and relapse, and (ii) in the Reddit dataset, we focus on modeling the linguistic topics and dependency structure to understand users’ recovery journey. We design a unified modeling framework using HL-MRFs that takes the different characteristics of both these platforms into account. Our experiments reveal that our structured and linguistic approach is helpful in predicting recovery in users in both these datasets. We perform extensive quantitative analysis of different groups of features and dependencies among them in both datasets. The interpretable and intuitive nature of our models and analysis is helpful in making meaningful predictions and can potentially be helpful in identifying and preventing relapse early.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3097298084",
    "type": "article"
  },
  {
    "title": "Decoupled Variational Embedding for Signed Directed Networks",
    "doi": "https://doi.org/10.1145/3408298",
    "publication_date": "2020-10-28",
    "publication_year": 2020,
    "authors": "Xu Chen; Jiangchao Yao; Maosen Li; Ya Zhang; Yanfeng Wang",
    "corresponding_authors": "",
    "abstract": "Node representation learning for signed directed networks has received considerable attention in many real-world applications such as link sign prediction, node classification, and node recommendation. The challenge lies in how to adequately encode the complex topological information of the networks. Recent studies mainly focus on preserving the first-order network topology that indicates the closeness relationships of nodes. However, these methods generally fail to capture the high-order topology that indicates the local structures of nodes and serves as an essential characteristic of the network topology. In addition, for the first-order topology, the additional value of non-existent links is largely ignored. In this article, we propose to learn more representative node embeddings by simultaneously capturing the first-order and high-order topology in signed directed networks. In particular, we reformulate the representation learning problem on signed directed networks from a variational auto-encoding perspective and further develop a decoupled variational embedding (DVE) method. DVE leverages a specially designed auto-encoder structure to capture both the first-order and high-order topology of signed directed networks, and thus learns more representative node embeddings. Extensive experiments are conducted on three widely used real-world datasets. Comprehensive results on both link sign prediction and node recommendation task demonstrate the effectiveness of DVE. Qualitative results and analysis are also given to provide a better understanding of DVE.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3097541837",
    "type": "article"
  },
  {
    "title": "Incremental Text Indexing for Fast Disk-Based Search",
    "doi": "https://doi.org/10.1145/2560800",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Giorgos Margaritis; Stergios V. Anastasiadis",
    "corresponding_authors": "",
    "abstract": "Real-time search requires to incrementally ingest content updates and almost immediately make them searchable while serving search queries at low latency. This is currently feasible for datasets of moderate size by fully maintaining the index in the main memory of multiple machines. Instead, disk-based methods for incremental index maintenance substantially increase search latency with the index fragmented across multiple disk locations. For the support of fast search over disk-based storage, we take a fresh look at incremental text indexing in the context of current architectural features. We introduce a greedy method called Selective Range Flush (SRF) to contiguously organize the index over disk blocks and dynamically update it at low cost. We show that SRF requires substantial experimental effort to tune specific parameters for performance efficiency. Subsequently, we propose the Unified Range Flush (URF) method, which is conceptually simpler than SRF, achieves similar or better performance with fewer parameters and less tuning, and is amenable to I/O complexity analysis. We implement interesting variations of the two methods in the Proteus prototype search engine that we developed and do extensive experiments with three different Web datasets of size up to 1TB. Across different systems, we show that our methods offer search latency that matches or reduces up to half the lowest achieved by existing disk-based methods. In comparison to an existing method of comparable search latency on the same system, our methods reduce by a factor of 2.0--2.4 the I/O part of build time and by 21--24% the total build time.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2099540584",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2540635",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Popular online social networks (OSNs) like Facebook and Twitter are changing the way users communicate and interact with the Internet. A deep understanding of user interactions in OSNs can provide important insights into questions of human social ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4247513588",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2516633",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Image annotation is a process of finding appropriate semantic labels for images in order to obtain a more convenient way for indexing and searching images on the Web. This article proposes a novel method for image annotation based on combining feature-...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4248366198",
    "type": "paratext"
  },
  {
    "title": "A Methodology for SIP and SOAP Integration Using Application-Specific Protocol Conversion",
    "doi": "https://doi.org/10.1145/2382616.2382618",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Goran Delač; Ivan Budiselić; Ivan Žužak; I. Skuliber; Tomislav Stefanec",
    "corresponding_authors": "",
    "abstract": "In recent years, the ubiquitous demands for cross-protocol application access are driving the need for deeper integration between SIP and SOAP. In this article we present a novel methodology for integrating these two protocols. Through an analysis of properties of SIP and SOAP we show that integration between these protocols should be based on application-specific converters. We describe a generic SIP/SOAP gateway that implements message handling and network and storage management while relying on application-specific converters to define session management and message mapping for a specific set of SIP and SOAP communication nodes. In order to ease development of these converters, we introduce an XML-based domain-specific language for describing application-specific conversion processes. We show how conversion processes can be easily specified in the language using message sequence diagrams of the desired interaction. We evaluate the presented methodology through performance analysis of the developed prototype gateway and high-level comparison with other solutions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2027871265",
    "type": "article"
  },
  {
    "title": "Studying and Understanding Characteristics of Post-Syncing Practice and Goal in Social Network Sites",
    "doi": "https://doi.org/10.1145/3457986",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Peng Zhang; Baoxi Liu; Xianghua Ding; Tun Lu; Hansu Gu; Ning Gu",
    "corresponding_authors": "",
    "abstract": "Many popular social network sites (SNSs) provide the post-syncing functionality, which allows users to synchronize posts automatically among different SNSs. Nowadays there exists divergence on this functionality from the view of sink SNS. The key to solving this problem is to understand the characteristics of users’ post-syncing practice and goals and evaluate whether they are consistent with an SNS’s norms, cultures, and goals. However, studying and understanding the characteristics of post-syncing practice and goal are challenging tasks as a result of the difficulty of data sampling and the complexity of post-syncing behavior. In this article, we focus on investigating this question by quantitative analysis in combination with qualitative analysis. In the quantitative study, by utilizing 211,233 synced-posts sampled from Weibo, we aim to investigate characteristics of post-syncing from three perspectives: user, content, and goal. The results suggest that post-syncing plays an important role in exhibiting one’s current activities, creations, and skills as well as advertisements but involves a risk of exhibiting personal sensitive profiles. To understand the results, we present an interview-based qualitative study based on thematic analysis. It indicates that the publicity, urgency, and remarkableness of contents and differences of social affordances and social circles between sink SNS and source SNS as well as the one-time consent of post-syncing authentication jointly account for the major role of post-syncing. Based on these results, we propose insights for post-syncing functionality’s adoption, design, and promotion.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3170006035",
    "type": "article"
  },
  {
    "title": "A Bayesian Method for Comparing Hypotheses About Human Trails",
    "doi": "https://doi.org/10.1145/3054950",
    "publication_date": "2017-06-23",
    "publication_year": 2017,
    "authors": "Philipp Singer; Denis Helić; Andreas Hotho; Markus Strohmaier",
    "corresponding_authors": "",
    "abstract": "When users interact with the Web today, they leave sequential digital trails on a massive scale. Examples of such human trails include Web navigation, sequences of online restaurant reviews, or online music play lists. Understanding the factors that drive the production of these trails can be useful, for example, for improving underlying network structures, predicting user clicks, or enhancing recommendations. In this work, we present a method called HypTrails for comparing a set of hypotheses about human trails on the Web, where hypotheses represent beliefs about transitions between states. Our method utilizes Markov chain models with Bayesian inference. The main idea is to incorporate hypotheses as informative Dirichlet priors and to calculate the evidence of the data under them. For eliciting Dirichlet priors from hypotheses, we present an adaption of the so-called (trial) roulette method, and to compare the relative plausibility of hypotheses, we employ Bayes factors. We demonstrate the general mechanics and applicability of HypTrails by performing experiments with (i) synthetic trails for which we control the mechanisms that have produced them and (ii) empirical trails stemming from different domains including Web site navigation, business reviews, and online music played. Our work expands the repertoire of methods available for studying human trails.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2694881798",
    "type": "article"
  },
  {
    "title": "LDoW-PaN",
    "doi": "https://doi.org/10.1145/2983643",
    "publication_date": "2017-07-25",
    "publication_year": 2017,
    "authors": "André Rocha; Cássio Prazeres",
    "corresponding_authors": "",
    "abstract": "This work aimed to propose LDoW-PaN, a Linked Data presentation and navigation model focused on the average user. The LDoW-PaN model is an extension of the Dexter Hypertext Reference Model. Through the LDoW-PaN model, ordinary people—who have no experience with technologies that involve the Linked Data environment—can interact with the Web of Data (RDF) more closely related to how they interact with the Web of Documents (HTML). To evaluate the proposal, some tools were developed, including the following: (i) a Web Service, which implements the lower-level layers of the LDoW-PaN model; (ii) a client-side script library, which implements the presentation and navigation layer; and (iii) a browser extension, which uses these tools to provide Linked Data presentation and navigation to users browsing the Web. The browser extension was developed using user interface approaches that are well known, well accepted, and evaluated by the Web research community, such as faceted navigation and presentation through tooltips. Therefore, the prototype evaluation included: usability evaluation through two classical techniques; computational complexity measures; and an analysis of the performance of the operations provided by the proposed model.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2738147591",
    "type": "article"
  },
  {
    "title": "Efficient Pairwise Penetrating-rank Similarity Retrieval",
    "doi": "https://doi.org/10.1145/3368616",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Weiren Yu; Julie A. McCann; Chengyuan Zhang",
    "corresponding_authors": "",
    "abstract": "Many web applications demand a measure of similarity between two entities, such as collaborative filtering, web document ranking, linkage prediction, and anomaly detection. P-Rank (Penetrating-Rank) has been accepted as a promising graph-based similarity measure, as it provides a comprehensive way of encoding both incoming and outgoing links into assessment. However, the existing method to compute P-Rank is iterative in nature and rather cost-inhibitive. Moreover, the accuracy estimate and stability issues for P-Rank computation have not been addressed. In this article, we consider the optimization techniques for P-Rank search that encompasses its accuracy, stability, and computational efficiency. (1) The accuracy estimation is provided for P-Rank iterations, with the aim to find out the number of iterations, k , required to guarantee a desired accuracy. (2) A rigorous bound on the condition number of P-Rank is obtained for stability analysis. Based on this bound, it can be shown that P-Rank is stable and well-conditioned when the damping factors are chosen to be suitably small. (3) Two matrix-based algorithms, applicable to digraphs and undirected graphs, are, respectively, devised for efficient P-Rank computation, which improves the computational time from O ( kn 3 ) to O (υ n 2 +υ 6 ) for digraphs, and to O (υ n 2 ) for undirected graphs, where n is the number of vertices in the graph, and υ (≪ n ) is the target rank of the graph. Moreover, our proposed algorithms can significantly reduce the memory space of P-Rank computations from O ( n 2 ) to O (υ n +υ 4 ) for digraphs, and to O (υ n ) for undirected graphs, respectively. Finally, extensive experiments on real-world and synthetic datasets demonstrate the usefulness and efficiency of the proposed techniques for P-Rank similarity assessment on various networks.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2995959085",
    "type": "article"
  },
  {
    "title": "Investigating and Modeling the Web Elements’ Visual Feature Influence on Free-viewing Attention",
    "doi": "https://doi.org/10.1145/3409474",
    "publication_date": "2020-11-05",
    "publication_year": 2020,
    "authors": "Sandeep Vidyapu; Vijaya Saradhi Vedula; Samit Bhattacharya",
    "corresponding_authors": "",
    "abstract": "User attentional analyses on web elements help in synthesis and rendering of webpages. However, majority of the existing analyses are limited in incorporating the intrinsic visual features of text and images. This study aimed to analyze the influence of elements’ visual features (font-size, font-family, color, etc., for text; and brightness, color, intensity, etc., for images) besides their position on users’ free-viewing visual attention. The investigation includes: (i) user’s position-based attention allocation on text and image web elements, (ii) identification of informative visual features with respect to the attention, (iii) performance of informative visual features in predicting the ordinal visual attention (fixation-indices). Towards the study, an eye-tracking experiment was conducted with 42 participants on 36 real-world webpages. The analyses revealed: (i) Though users predominantly allocate the initial attention to MiddleCenter}, MiddleLeft, TopCenter, TopLeft regions, the elements in Right and Bottom regions are not completely ignored; (ii) Space -related (column-gap, line-height, padding) and font Size -related (font-size, font-weight) intrinsic text features, and Mid-level Color Histogram intrinsic image features are informative, while position and size are informative for both the types; (iii) the informative visual features predict the ordinal visual attention on an element with 90% average accuracy and 70% micro-F1 score. Our approach finds applications in element-granular web-designing and user attention prediction.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3095443394",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1232722",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Since many Internet applications employ a multitier architecture, in this article, we focus on the problem of analytically modeling the behavior of such applications. We present a model based on a network of queues where the queues represent different ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4246408024",
    "type": "paratext"
  },
  {
    "title": "Optimisation Techniques for Flexible SPARQL Queries",
    "doi": "https://doi.org/10.1145/3532855",
    "publication_date": "2022-06-24",
    "publication_year": 2022,
    "authors": "Riccardo Frosini; Alexandra Poulovassilis; Peter T. Wood; Andrea Calı̀",
    "corresponding_authors": "",
    "abstract": "Resource Description Framework datasets can be queried using the SPARQL language but are often irregularly structured and incomplete, which may make precise query formulation hard for users. The SPARQL AR language extends SPARQL 1.1 with two operators—APPROX and RELAX—to allow flexible querying over property paths. These operators encapsulate different dimensions of query flexibility, namely, approximation and generalisation, and they allow users to query complex, heterogeneous knowledge graphs without needing to know precisely how the data is structured. Earlier work has described the syntax, semantics, and complexity of SPARQL AR , has demonstrated its practical feasibility, but has also highlighted the need for improving the speed of query evaluation. In the present article, we focus on the design of two optimisation techniques targeted at speeding up the execution of SPARQL AR queries and on their empirical evaluation on three knowledge graphs: LUBM, DBpedia, and YAGO. We show that applying these optimisations can result in substantial improvements in the execution times of longer-running queries (sometimes by one or more orders of magnitude) without incurring significant performance penalties for fast queries.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4283387898",
    "type": "article"
  },
  {
    "title": "BanditProp: Bandit Selection of Review Properties for Effective Recommendation",
    "doi": "https://doi.org/10.1145/3532859",
    "publication_date": "2022-07-21",
    "publication_year": 2022,
    "authors": "Xi Wang; Iadh Ounis; Craig Macdonald",
    "corresponding_authors": "",
    "abstract": "Many recent recommendation systems leverage the large quantity of reviews placed by users on items. However, it is both challenging and important to accurately measure the usefulness of such reviews for effective recommendation. In particular, users have been shown to exhibit distinct preferences over different types of reviews (e.g., preferring longer versus shorter or recent versus old reviews), indicating that users might differ in their viewpoints on what makes the reviews useful. Yet, there have been limited studies that account for the personalised usefulness of reviews when estimating the users’ preferences. In this article, we propose a novel neural model, called BanditProp, which addresses this gap in the literature. It first models reviews according to both their content and associated properties (e.g., length, sentiment and recency). Thereafter, it constructs a multi-task learning (MTL) framework to model the reviews’ content encoded with various properties.In such an MTL framework, each task corresponds to producing recommendations focusing on an individual property. Next, we address the selection of the features from reviews with different review properties as a bandit problem using multinomial rewards. We propose a neural contextual bandit algorithm (i.e., ConvBandit) and examine its effectiveness in comparison to eight existing bandit algorithms in addressing the bandit problem. Our extensive experiments on two well-known Amazon and Yelp datasets show that BanditProp can significantly outperform one classic and six existing state-of-the-art recommendation baselines. Moreover, BanditProp using ConvBandit consistently outperforms the use of other bandit algorithms over the two used datasets. In particular, we experimentally demonstrate the effectiveness of our proposed customised multinomial rewards in comparison to binary rewards, when addressing our bandit problem.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4286491795",
    "type": "article"
  },
  {
    "title": "Active Learning for Web Search Ranking via Noise Injection",
    "doi": "https://doi.org/10.1145/2697391",
    "publication_date": "2015-01-23",
    "publication_year": 2015,
    "authors": "Wenbin Cai; Muhan Zhang; Ya Zhang",
    "corresponding_authors": "",
    "abstract": "Learning to rank has become increasingly important for many information retrieval applications. To reduce the labeling cost at training data preparation, many active sampling algorithms have been proposed. In this article, we propose a novel active learning-for-ranking strategy called ranking-based sensitivity sampling (RSS), which is tailored for Gradient Boosting Decision Tree (GBDT), a machine-learned ranking method widely used in practice by major commercial search engines for ranking. We leverage the property of GBDT that samples close to the decision boundary tend to be sensitive to perturbations and design the active learning strategy accordingly. We further theoretically analyze the proposed strategy by exploring the connection between the sensitivity used for sample selection and model regularization to provide a potentially theoretical guarantee w.r.t. the generalization capability. Considering that the performance metrics of ranking overweight the top-ranked items, item rank is incorporated into the selection function. In addition, we generalize the proposed technique to several other base learners to show its potential applicability in a wide variety of applications. Substantial experimental results on both the benchmark dataset and a real-world dataset have demonstrated that our proposed active learning strategy is highly effective in selecting the most informative examples.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2109839440",
    "type": "article"
  },
  {
    "title": "Individual Judgments Versus Consensus",
    "doi": "https://doi.org/10.1145/2834122",
    "publication_date": "2016-01-09",
    "publication_year": 2016,
    "authors": "Hengjie Song; Yonghui Xu; Huaqing Min; Qingyao Wu; Wei Wei; Jianshu Weng; Xiaogang Han; Qiang Yang; Jialiang Shi; Jiaqian Gu; Chunyan Miao; Toyoaki Nishida",
    "corresponding_authors": "",
    "abstract": "Query-URL relevance, measuring the relevance of each retrieved URL with respect to a given query, is one of the fundamental criteria to evaluate the performance of commercial search engines. The traditional way to collect reliable and accurate query-URL relevance requires multiple annotators to provide their individual judgments based on their subjective expertise (e.g., understanding of user intents). In this case, the annotators’ subjectivity reflected in each annotator individual judgment (AIJ) inevitably affects the quality of the ground truth relevance (GTR). But to the best of our knowledge, the potential impact of AIJs on estimating GTRs has not been studied and exploited quantitatively by existing work. This article first studies how multiple AIJs and GTRs are correlated. Our empirical studies find that the multiple AIJs possibly provide more cues to improve the accuracy of estimating GTRs. Inspired by this finding, we then propose a novel approach to integrating the multiple AIJs with the features characterizing query-URL pairs for estimating GTRs more accurately. Furthermore, we conduct experiments in a commercial search engine—Baidu.com—and report significant gains in terms of the normalized discounted cumulative gains.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2265566575",
    "type": "article"
  },
  {
    "title": "W-tree",
    "doi": "https://doi.org/10.1145/2835181",
    "publication_date": "2016-02-08",
    "publication_year": 2016,
    "authors": "Bruno Tenório Ávila; Rafael Dueire Lins",
    "corresponding_authors": "",
    "abstract": "World Wide Web applications need to use, constantly update, and maintain large webgraphs for executing several tasks, such as calculating the web impact factor, finding hubs and authorities, performing link analysis by webometrics tools, and ranking webpages by web search engines. Such webgraphs need to use a large amount of main memory, and, frequently, they do not completely fit in, even if compressed. Therefore, applications require the use of external memory. This article presents a new compact representation for webgraphs, called w-tree , which is designed specifically for external memory. It supports the execution of basic queries (e.g., full read, random read, and batch random read), set-oriented queries (e.g., superset, subset, equality, overlap, range, inlink, and co-inlink), and some advanced queries, such as edge reciprocal and hub and authority. Furthermore, a new layout tree designed specifically for webgraphs is also proposed, reducing the overall storage cost and allowing the random read query to be performed with an asymptotically faster runtime in the worst case. To validate the advantages of the w-tree, a series of experiments are performed to assess an implementation of the w-tree comparing it to a compact main memory representation. The results obtained show that w-tree is competitive in compression time and rate and in query time, which may execute several orders of magnitude faster for set-oriented queries than its competitors. The results provide empirical evidence that it is feasible to use a compact external memory representation for webgraphs in real applications, contradicting the previous assumptions made by several researchers.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2344388090",
    "type": "article"
  },
  {
    "title": "P <scp>ea</scp> CE-Ful Web Event Extraction and Processing as Bitemporal Mutable Events",
    "doi": "https://doi.org/10.1145/2911989",
    "publication_date": "2016-08-16",
    "publication_year": 2016,
    "authors": "Tim Furche; Giovanni Grasso; Michael Huemer; Christian Schallhart; Michael Schrefl",
    "corresponding_authors": "",
    "abstract": "The web is the largest bulletin board of the world. Events of all types, from flight arrivals to business meetings, are announced on this board. Tracking and reacting to such event announcements, however, is a tedious manual task, only slightly alleviated by email or similar notifications. Announcements are published with human readers in mind, and updates or delayed announcements are frequent. These characteristics have hampered attempts at automatic tracking. P ea CE provides the first integrated framework for event processing on top of web event ads, consisting of event extraction, complex event processing, and action execution in response to these events. Given a schema of the events to be tracked, the framework populates this schema by extracting events from announcement sources. This extraction is performed by little programs called wrappers that produce the events including updates and retractions. P ea CE then queries these events to detect complex events, often combining announcements from multiple sources. To deal with updates and delayed announcements, P ea CE’s schemas are bitemporal, to distinguish between occurrence and detection time. This allows complex event specifications to track updates and to react upon differences in occurrence and detection time. In case of new, changing, or deleted events, P ea CE allows one to execute actions, such as tweeting or sending out email notifications. Actions are typically specified as web interactions, for example, to fill and submit a form with attributes of the triggering event. Our evaluation shows that P ea CE’s processing is dominated by the time needed for accessing the web to extract events and perform actions, allotting to 97.4%. Thus, P ea CE requires only 2.6% overhead, and therefore, the complex event processor scales well even with moderate resources. We further show that simple and reasonable restrictions on complex event specifications and the timing of constituent events suffice to guarantee that P ea CE only requires a constant buffer to process arbitrarily many event announcements.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2514016267",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3017848",
    "publication_date": "2016-12-27",
    "publication_year": 2016,
    "authors": "Maria Rafalak; Dominik Deja; Adam Wierzbicki; Michał Ąkol",
    "corresponding_authors": "Adam Wierzbicki",
    "abstract": "Eye tracking studies have widely been used in improving the design and usability of web pages and in the research of understanding how users navigate them. However, there is limited research in clustering users’ eye movement sequences (i.e., scanpaths) ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4230433718",
    "type": "paratext"
  },
  {
    "title": "Validation of an Improved Vision-Based Web Page Parsing Pipeline",
    "doi": "https://doi.org/10.1145/3580519",
    "publication_date": "2023-01-21",
    "publication_year": 2023,
    "authors": "Michael Cormier; Robin Cohen; Richard Mann; Karyn Moffatt; Daniel Vogel; Mengfei Liu; Shangshang Zheng",
    "corresponding_authors": "",
    "abstract": "In this article, we present a novel approach to quantitative evaluation of a model for parsing web pages as visual images, intended to provide improvements for users with assistive needs (cognitive or visual deficits, enabling decluttering or zooming and supporting more effective screen reader output). This segmentation-classification pipeline is tested in stages: We first discuss the validation of the segmentation algorithm, showing that our approach produces automated segmentations that are very similar to those produced by real users when making use of a drawing interface to designate edges and regions. We also examine the properties of these ground truth segmentations produced under different conditions. We then describe our Hidden Markov tree approach for classification and present results which serve provide important validation for this model. The analysis is set against effective choices for dataset and pruning options, measured with respect to manual ground truth labelling of regions. In all, we offer a detailed quantitative validation (focused on complex news pages) of a fully pipelined approach for interpreting web pages as visual images, an approach which enables important advances for users with assistive needs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4317659260",
    "type": "article"
  },
  {
    "title": "Learning Neighbor User Intention on User-Item Interaction Graphs for Better Sequential Recommendation",
    "doi": "https://doi.org/10.1145/3580520",
    "publication_date": "2023-02-01",
    "publication_year": 2023,
    "authors": "Mei Yu; Kun Zhu; Mankun Zhao; Jian Yu; Tianyi Xu; Di Jin; Xuewei Li; Ruiguo Yu",
    "corresponding_authors": "",
    "abstract": "The task of sequential recommendation aims to predict a user’s preference by analyzing the user’s historical behaviours. Existing methods model item transitions through leveraging sequential patterns. However, they mainly consider the target user’s behaviours and dynamic characteristics, while often ignoring high-order collaborative connections when modelling user preferences. Some recent works try to use graph-based methods to introduce high-order collaborative signals for sequential recommendation. However, these methods are flawed by two problems: the sequential patterns cannot be effectively mined and their way of introducing high-order collaborative signals is not suitable for sequential recommendation. To address these problems, we propose to fully exploit sequence features and model high-order collaborative signals for sequential recommendation. We propose a N eighbor user I ntention-based S equential Rec ommender (NISRec), which utilizes the intentions of high-order connected neighbor users as high-order collaborative signals in order to improve recommendation performance for the target user. The NISRec contains two main modules: the neighbor user intention embedding module (NIE) and the fusion module. The NIE module describes both the long-term and short-term intentions of neighbor users and aggregates them separately. The fusion module uses these two types of aggregated intentions to model high-order collaborative signals in both the embedding process and user preference modelling phase for recommendations of the target user. Experimental results show that our new approach outperforms the state-of-the-art methods on both sparse and dense datasets. Extensive studies further show the effectiveness of the diverse neighbor intentions introduced by the NISRec.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4318776448",
    "type": "article"
  },
  {
    "title": "PIDKG: Propagating Interaction Influence on the Dynamic Knowledge Graph for Recommendation",
    "doi": "https://doi.org/10.1145/3593314",
    "publication_date": "2023-05-10",
    "publication_year": 2023,
    "authors": "Chunjing Xiao; Wanlin Ji; Yuxiang Zhang; Shenkai Lv",
    "corresponding_authors": "",
    "abstract": "Modeling the dynamic interactions between users and items on knowledge graphs is crucial for improving the accuracy of recommendation. Although existing methods have made great progress in modeling the dynamic knowledge graphs for recommendation, they usually only consider the mutual influence between users and items involved in the interactions, and ignore the influence propagation from the interacting nodes (i.e., users and items) on dynamic knowledge graphs. In this article, we propose an influence propagation-enhanced deep co-evolutionary method for recommendation, which can capture not only the direct mutual influence between interacting users and items but also influence propagation from multiple interacting nodes to their high-order neighbors at the same time on the dynamic knowledge graph. Specifically, the proposed model consists of two main components: the direct mutual influence component and the influence propagation component. The former captures direct interaction influence between the interacting users and items to generate the effective representations for them. The latter refines their representations via aggregating the interaction influence propagated from multiple interacting nodes. In this process, a neighbor selection mechanism is designed for selecting more effective propagation influence, which can significantly reduce the computational cost and accelerate the training. Finally, the refined representations of users and items are used to predict which item the user is most likely to interact with. The experimental results on three real-world datasets illustrate that the effectiveness and robustness of PIDKG outperform all state-of-the-art baselines and the efficiency of it is faster than most comparative baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4376114790",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Advanced Graph Mining on the Web: Theory, Algorithms, and Applications: Part 1",
    "doi": "https://doi.org/10.1145/3579360",
    "publication_date": "2023-05-22",
    "publication_year": 2023,
    "authors": "Hao Peng; Jian Yang; Jia Wu; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4377244968",
    "type": "article"
  },
  {
    "title": "Privacy Scoring over OSNs: Shared Data Granularity as a Latent Dimension",
    "doi": "https://doi.org/10.1145/3604909",
    "publication_date": "2023-06-17",
    "publication_year": 2023,
    "authors": "Yasir Kilic; Ali İnan",
    "corresponding_authors": "",
    "abstract": "Privacy scoring aims at measuring the privacy violation risk of a user over an online social network (OSN) based on attribute values shared in the user’s OSN profile page and the user’s position in the network. Existing studies on privacy scoring rely on possibly biased or emotional survey data. In this study, we work with real-world data collected from the professional LinkedIn OSN and show that probabilistic scoring models derived from the item response theory fit real-world data better than naive approaches. We also introduce the granularity of the data an OSN user shares on her profile as a latent dimension of the OSN privacy scoring problem. Incorporating data granularity into our model, we build the most comprehensive solution to the OSN privacy scoring problem. Extensive experimental evaluation of various scoring models indicates the effectiveness of the proposed solution.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4381050430",
    "type": "article"
  },
  {
    "title": "Layout Cross-Browser Failure Classification for Mobile Responsive Design Web Applications: Combining Classification Models Using Feature Selection",
    "doi": "https://doi.org/10.1145/3580518",
    "publication_date": "2023-06-17",
    "publication_year": 2023,
    "authors": "Willian Massami Watanabe; Danilo Alves dos Santos; Claiton de Oliveira",
    "corresponding_authors": "",
    "abstract": "Cross-browser incompatibilities (XBIs) are defined as inconsistencies that can be observed in Web applications when they are rendered in a specific browser compared to others. These inconsistencies are associated with differences in the way each browser implements its capabilities and renders Web applications. The inconsistencies range from minor layout differences to lack of core functionalities of Web applications when rendered in specific browsers. The state of the art proposes different approaches for detecting XBIs and many of them are based on classification models, using features extracted from the document object model (DOM) structure (DOM-based approaches) and screenshots (computer vision approaches) of Web applications. To the best of our knowledge, a comparison between DOM-based and computer vision classification models has not yet been reported in the literature, and a combination between both approaches could possibly lead to increased accuracy of classification models. In this article, we extend the use of these classification models for detecting layout XBIs in responsive design Web applications, rendered on different browser viewport widths and devices (iPhone 12 mini, iPhone 12, iPhone 12 Pro Max, and Pixel XL). We investigate the use of state-of-the-art classification models (Browserbite, CrossCheck, and our previous work) for detecting layout cross-browser failures, which consist of layout XBIs that negatively affect the layout of responsive design Web applications. Furthermore, we propose an enhanced classification model that combines features from different state-of-the-art classification models (DOM based and computer vision) using feature selection. We built two datasets for evaluating the efficacy of classification models in separately detecting external and internal layout failures using data from 72 responsive design Web applications. The proposed classification model reported the highest F1-score for detecting external layout failures (0.65) and internal layout failures (0.35), and these results reported significant differences compared to Browserbite and CrossCheck classification models. Nevertheless, the experiment showed a lower accuracy in the classification of internal layout failures and suggests the use of other image similarity metrics or deep learning models for increasing the efficacy of classification models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4381050440",
    "type": "article"
  },
  {
    "title": "Human Team Behavior and Predictability in the Massively Multiplayer Online Game WOT Blitz",
    "doi": "https://doi.org/10.1145/3617509",
    "publication_date": "2023-08-26",
    "publication_year": 2023,
    "authors": "Frank Emmert‐Streib; Shailesh Tripathi; Matthias Dehmer",
    "corresponding_authors": "",
    "abstract": "Massively multiplayer online games (MMOGs) played on the Web provide a new form of social, computer-mediated interactions that allow the connection of millions of players worldwide. The rules governing team-based MMOGs are typically complex and nondeterministic giving rise to an intricate dynamical behavior. However, due to the novelty and complexity of MMOGs, their behavior is understudied. In this article, we investigate the MMOG World of Tanks Blitz by using a combined approach based on data science and complex adaptive systems. We analyze data on the population level to get insights into organizational principles of the game and its game mechanics. For this reason, we study the scaling behavior and the predictability of system variables. As a result, we find a power-law behavior on the population level revealing long-range interactions between system variables. Furthermore, we identify and quantify the predictability of summary statistics of the game and its decomposition into explanatory variables. This reveals a heterogeneous progression through the tiers and identifies only a single system variable as key driver for the win rate.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386191780",
    "type": "article"
  },
  {
    "title": "Multiresolution Local Spectral Attributed Community Search",
    "doi": "https://doi.org/10.1145/3624580",
    "publication_date": "2023-09-19",
    "publication_year": 2023,
    "authors": "Qingqing Li; Huifang Ma; Zhixin Li; Liang Chang",
    "corresponding_authors": "",
    "abstract": "Community search has become especially important in graph analysis task, which aims to identify latent members of a particular community from a few given nodes. Most of the existing efforts in community search focus on exploring the community structure with a single scale in which the given nodes are located. Despite promising results, the following two insights are often neglected. First, node attributes provide rich and highly related auxiliary information apart from network interactions for characterizing the node properties. Attributes may indicate the community assignment of a node with very few links, which would be difficult to determine from the network structure alone. Second, the multiresolution community affords latent information to depict the hierarchical relation of the network and ensure that one of them is closest to the real one. It is essential for users to understand the underlying structure of the network and explore the community with strong structure and attribute cohesiveness at disparate scales. These aspects motivate us to develop a new community search framework called Multiresolution Local Spectral Attributed Community Search (MLSACS). Specifically, inspired by the local modularity, graph wavelets, and scaling functions, we propose a new Multiresolution Local modularity (MLQ) based on a reconstructed node attribute graph. Furthermore, to detect local communities with cohesive structures and attributes at different scales, a sparse indicator vector is developed based on MLQ by solving a linear programming problem. Extensive experimental results on both synthetic and real-world attributed graphs have demonstrated the detected communities are meaningful and the scale can be changed reasonably.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386845734",
    "type": "article"
  },
  {
    "title": "BNoteHelper: A Note-based Outline Generation Tool for Structured Learning on Video-sharing Platforms",
    "doi": "https://doi.org/10.1145/3638775",
    "publication_date": "2023-12-27",
    "publication_year": 2023,
    "authors": "Fangyu Yu; Peng Zhang; Xianghua Ding; Tun Lu; Ning Gu",
    "corresponding_authors": "",
    "abstract": "Usually generated by ordinary users and often not particularly designed for learning, the videos on video-sharing platforms are mostly not structured enough to support learning purposes, although they are increasingly leveraged for that. Most existing studies attempt to structure the video using video summarization techniques. However, these methods focus on extracting information from within the video and aiming to consume the video itself. In this article, we design and implement BNoteHelper, a note-based video outline prototype that generates outline titles by extracting user-generated notes on Bilibili, using the BART model fine-tuned on a built dataset. As a browser plugin, BNoteHelper provides users with video overview and navigation as well as note-taking template, via two main features: outline table and navigation marker. The model and prototype are evaluated through automatic and human evaluations. The automatic evaluation reveals that, both before and after fine-tuning, the BART model outperforms T5-Pegasus in BLEU and Perplexity metrics. Also, the results from user feedback reveal that the generation outline sourced from notes is preferred by users over that sourced from video captions due to its more concise, clear, and accurate characteristics but also too general with less details and diversities sometimes. Two features of the video outline are also found to have respective advantages, especially in holistic and fine-grained aspects. Based on these results, we propose insights into designing a video summary from the user-generated creation perspective, customizing it based on video types, and strengthening the advantages of its different visual styles on video-sharing platforms.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390264293",
    "type": "article"
  },
  {
    "title": "A Clustering-Driven LDAP Framework",
    "doi": "https://doi.org/10.1145/1993053.1993054",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Vassiliki Koutsonikola; Athena Vakali",
    "corresponding_authors": "",
    "abstract": "LDAP directories have proliferated as the appropriate storage framework for various and heterogeneous data sources, operating under a wide range of applications and services. Due to the increased amount and heterogeneity of the LDAP data, there is a requirement for appropriate data organization schemes. The LPAIR &amp; LMERGE (LP-LM) algorithm, presented in this article, is a hierarchical agglomerative structure-based clustering algorithm which can be used for the LDAP directory information tree definition. A thorough study of the algorithm’s performance is provided, which designates its efficiency. Moreover, the Relative Link as an alternative merging criterion is proposed, since as indicated by the experimentation, it can result in more balanced clusters. Finally, the LP and LM Query Engine is presented, which considering the clustering-based LDAP data organization, results in the enhancement of the LDAP server’s performance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2057386141",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1921591",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The technique of collaborative filtering is especially successful in generating personalized recommendations. More than a decade of research has resulted in numerous algorithms, although no comparison of the different strategies has been made. In fact, ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4250038541",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2344416",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article presents a model-driven approach for the design of the layout in a complex Web application, where large amounts of data are accessed. The aim of this work is to reduce, as much as possible, repetitive tasks and to factor out common aspects ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4254997727",
    "type": "paratext"
  },
  {
    "title": "A Study of Web Print",
    "doi": "https://doi.org/10.1145/3068331",
    "publication_date": "2017-07-25",
    "publication_year": 2017,
    "authors": "Georgia Koutrika; Qian Lin",
    "corresponding_authors": "",
    "abstract": "This article analyzes a proprietary log of printed web pages and aims at answering questions regarding the content people print (what), the reasons they print (why), as well as attributes of their print profile (who). We present a classification of pages printed based on their print intent and we describe our methodology for processing the print dataset used in this study. In our analysis, we study the web sites, topics, and print intent of the pages printed along the following aspects: popularity, trends, activity, user diversity, and consistency. We present several findings that reveal interesting insights into printing. We analyze our findings and discuss their impact and directions for future work.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2739412524",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1462148",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We describe an approach for extracting semantics for tags, unstructured text-labels assigned to resources on the Web, based on each tag's usage patterns. In particular, we focus on the problem of extracting place semantics for tags that are assigned to ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4234238993",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1594173",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4256383527",
    "type": "paratext"
  },
  {
    "title": "Utilizing Web Trackers for Sybil Defense",
    "doi": "https://doi.org/10.1145/3450444",
    "publication_date": "2021-04-22",
    "publication_year": 2021,
    "authors": "Marcel Flores; Andrew Kahn; Marc Warrior; Alan Mislove; Aleksandar Kuzmanovic",
    "corresponding_authors": "",
    "abstract": "User tracking has become ubiquitous practice on the Web, allowing services to recommend behaviorally targeted content to users. In this article, we design Alibi, a system that utilizes such readily available personalized content, generated by recommendation engines in real time, as a means to tame Sybil attacks. In particular, by using ads and other tracker-generated recommendations as implicit user “certificates,” Alibi is capable of creating meta-profiles that allow for rapid and inexpensive validation of users’ uniqueness, thereby enabling an Internet-wide Sybil defense service. We demonstrate the feasibility of such a system, exploring the aggregate behavior of recommendation engines on the Web and demonstrating the richness of the meta-profile space defined by such inputs. We further explore the fundamental properties of such meta-profiles, i.e., their construction, uniqueness, persistence, and resilience to attacks. By conducting a user study, we show that the user meta-profiles are robust and show important scaling effects. We demonstrate that utilizing even a moderate number of popular Web sites empowers Alibi to tame large-scale Sybil attacks.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3159099519",
    "type": "article"
  },
  {
    "title": "Queryable Compression on Time-evolving Web and Social Networks with Streaming",
    "doi": "https://doi.org/10.1145/3495012",
    "publication_date": "2021-12-21",
    "publication_year": 2021,
    "authors": "Michael L. Nelson; Sridhar Radhakrishnan; Chandra N. Sekharan; Amlan Chatterjee; Sudhindra Gopal Krishna",
    "corresponding_authors": "",
    "abstract": "Time-evolving web and social network graphs are modeled as a set of pages/individuals (nodes) and their arcs (links/relationships) that change over time. Due to their popularity, they have become increasingly massive in terms of their number of nodes, arcs, and lifetimes. However, these graphs are extremely sparse throughout their lifetimes. For example, it is estimated that Facebook has over a billion vertices, yet at any point in time, it has far less than 0.001% of all possible relationships. The space required to store these large sparse graphs may not fit in most main memories using underlying representations such as a series of adjacency matrices or adjacency lists. We propose building a compressed data structure that has a compressed binary tree corresponding to each row of each adjacency matrix of the time-evolving graph. We do not explicitly construct the adjacency matrix, and our algorithms take the time-evolving arc list representation as input for its construction. Our compressed structure allows for directed and undirected graphs, faster arc and neighborhood queries, as well as the ability for arcs and frames to be added and removed directly from the compressed structure (streaming operations). We use publicly available network data sets such as Flickr, Yahoo!, and Wikipedia in our experiments and show that our new technique performs as well or better than our benchmarks on all datasets in terms of compression size and other vital metrics.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4200495537",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1255438",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We consider the problem of visualizing the evolution of tags within the Flickr (flickr.com) online image sharing community. Any user of the Flickr service may append a tag to any photo in the system. Over the past year, users have on average added over ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4235105492",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1326561",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4255323756",
    "type": "paratext"
  },
  {
    "title": "List of 2016 TWEB Reviewers",
    "doi": "https://doi.org/10.1145/3180440",
    "publication_date": "2018-02-02",
    "publication_year": 2018,
    "authors": "Brian D. Davison",
    "corresponding_authors": "Brian D. Davison",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2883965106",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Advanced Graph Mining on the Web: Theory, Algorithms, and Applications: Part 2",
    "doi": "https://doi.org/10.1145/3631941",
    "publication_date": "2024-01-08",
    "publication_year": 2024,
    "authors": "Hao Peng; Jian Yang; Jia Wu; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Special Issue Part 1 (Issue 3) and Part 2 (Issue 4) of AIEDAM are based on a workshop on Learning and Creativity held at the 2002 conference on Artificial Intelligence in Design, AID '02 (www.cad.strath.ac.uk/AID02_workshop/Workshop_webpage.html; Gero, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390669096",
    "type": "article"
  },
  {
    "title": "Learning Dynamic Multimodal Network Slot Concepts from the Web for Forecasting Environmental, Social and Governance Ratings",
    "doi": "https://doi.org/10.1145/3663674",
    "publication_date": "2024-05-03",
    "publication_year": 2024,
    "authors": "Gary Ang; Ee‐Peng Lim",
    "corresponding_authors": "",
    "abstract": "Dynamic multimodal networks are networks with node attributes from different modalities where the attributes and network relationships evolve across time, i.e., both networks and multimodal attributes are dynamic; for example, dynamic relationship networks between companies that evolve across time due to changes in business strategies and alliances, which are associated with dynamic company attributes from multiple modalities such as textual online news, categorical events, and numerical financial-related data. Such information can be useful in predictive tasks involving companies. Environmental, social, and governance (ESG) ratings of companies are important for assessing the sustainability risks of companies. The process of generating ESG ratings by expert analysts is, however, laborious and time-intensive. We thus explore the use of dynamic multimodal networks extracted from the web for forecasting ESG ratings. Learning such dynamic multimodal networks from the web for forecasting ESG ratings is, however, challenging due to its heterogeneity and the low signal-to-noise ratios and non-stationary distributions of web information. Human analysts cope with such issues by learning concepts from past experience through relational thinking and scanning for such concepts when analyzing new information about a company. In this article, we propose the Dynamic Multimodal Slot Concept Attention-based Network (DynScan) model. DynScan utilizes slot attention mechanisms together with slot concept alignment and disentanglement loss functions to learn latent slot concepts from dynamic multimodal networks to improve performance on ESG rating forecasting tasks. DynScan is evaluated on forecasting tasks on six datasets, comprising three ESG ratings across two sets of companies. Our experiments show that DynScan outperforms other state-of-the-art models on these forecasting tasks. We also visualize the slot concepts learned by DynScan on five synthetic datasets and three real-world datasets and observe distinct and meaningful slot concepts being learned by DynScan across both synthetic and real-world datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396612878",
    "type": "article"
  },
  {
    "title": "INCEPT: A Framework for Duplicate Posts Classification with Combined Text Representations",
    "doi": "https://doi.org/10.1145/3677322",
    "publication_date": "2024-07-15",
    "publication_year": 2024,
    "authors": "Erjon Skenderi; Jukka Huhtamäki; Salla-Maaria Laaksonen; Kostas Stefanidis",
    "corresponding_authors": "",
    "abstract": "Dealing with many of the problems related to the quality of textual content online involves identifying similar content. Algorithmic solutions for duplicate content classification typically rely on text vector representation, which maps textual information into a set of features. Ideally, this representation would capture all aspects of the underlying text, including length, word frequencies, syntax, and semantics. While recent advancements in text representation have led to improved performance, a comprehensive approach that explicitly incorporates all text features has not yet been proposed. In this study, we present the INCEPT framework that utilizes multiple representation methods to detect duplicate text pairs, taking advantage of their individual strengths. The core of our approach involves using a stacking ensemble of pairwise vector distance measurements that are computed from multiple text representation methods. A stacking classifier then utilizes these distance scores as input and learns to identify duplicate posts. We assess the proposed framework’s effectiveness in identifying duplicate posts in an online Question and Answer platform. By combining several text representation methods, INCEPT performs well in the duplicate posts classification task. Our experiments demonstrate that specific framework configurations outperform the accuracy scores obtained from individual text representation methods. Therefore, we also infer that no single text representation method can independently capture a text’s features.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400656960",
    "type": "article"
  },
  {
    "title": "B-TTDb: A Database of Turkish Tweets for Predicting the Top One Hundred Emojis",
    "doi": "https://doi.org/10.1145/3681783",
    "publication_date": "2024-07-24",
    "publication_year": 2024,
    "authors": "Yıltan Bitirim",
    "corresponding_authors": "Yıltan Bitirim",
    "abstract": "Emoji prediction is an important research task that focuses on finding the most appropriate emoji(s) quickly and effortlessly for a specific text. Now that Turkish is on the list of the top 20 most spoken languages in the world and there are a considerable number of Turkish-speaking social media users, studying emoji prediction in Turkish holds significant value. In this study, a Turkish tweets database, named Bitirim's Turkish Tweets Database (B-TTDb), was constructed for academic and industrial studies based on the prediction of the top 100 emojis. B-TTDb consists of four datasets. The first dataset includes raw tweets, the second dataset is the organized version of the first dataset, the third dataset is the pre-processed version of the second dataset, and the last one is the organized version of the third dataset. The last one is the final version and it is named Bitirim's Dataset (B-D). It includes a total of 158,201 unique tweets belonging to the top 100 emoji classes. For database validation, experiments were conducted on B-D with popular machine learning algorithms for the top 10, 20, 50, and 100 emojis. This study could be considered as the first study that contributes to the literature by the first validated large database of Turkish tweets that includes such a large number of emojis. In addition, B-TTDb could be a basis as well as motivation for various further studies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400954245",
    "type": "article"
  },
  {
    "title": "Enhancing Graph Neural Networks via Memorized Global Information",
    "doi": "https://doi.org/10.1145/3689430",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Ruihong Zeng; Jinyuan Fang; Siwei Liu; Zaiqiao Meng; Shangsong Liang",
    "corresponding_authors": "",
    "abstract": "Graph neural networks (GNNs) have gained significant attention for their impressive results on different graph-based tasks. The essential mechanism of GNNs is the message-passing framework, whereby node representations are aggregated from local neighborhoods. Recently, Transformer-based GNNs have been introduced to learn the long-range dependencies, enhancing performance. However, their quadratic computational complexity, due to the attention computation, has constrained their applicability on large-scale graphs. To address this issue, we propose MGIGNN ( M emorized G lobal I nformation G raph N eural N etwork), an innovative approach that leverages memorized global information to enhance existing GNNs in both transductive and inductive scenarios. Specifically, MGIGNN captures long-range dependencies by identifying and incorporating global similar nodes, which are defined as nodes exhibiting similar features, structural patterns and label information within a graph. To alleviate the computational overhead associated with computing embeddings for all nodes, we introduce an external memory module to facilitate the retrieval of embeddings and optimize performance on large graphs. To enhance the memory-efficiency, MGIGNN selectively retrieves global similar nodes from a small set of candidate nodes. These candidate nodes are selected from the training nodes based on a sparse node selection distribution with a Dirichlet prior. This selecting approach not only reduces the memory size required but also ensures efficient utilization of computational resources. Through comprehensive experiments conducted on ten widely-used and real-world datasets, including seven homogeneous datasets and three heterogeneous datasets, we demonstrate that our MGIGNN can generally improve the performance of existing GNNs on node classification tasks under both inductive and transductive settings.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401958364",
    "type": "article"
  },
  {
    "title": "XRAD: Ransomware Address Detection Method based on Bitcoin Transaction Relationships",
    "doi": "https://doi.org/10.1145/3687487",
    "publication_date": "2024-08-20",
    "publication_year": 2024,
    "authors": "Kai Wang; Michael Wen Tong; Jun Pang; Jitao Wang; Weili Han",
    "corresponding_authors": "",
    "abstract": "Recently, there is a surge in ransomware activities that encrypt users’ sensitive data and demand bitcoins for ransom payments to conceal the criminal’s identity. It is crucial for regulatory agencies to identify as many ransomware addresses as possible in order to accurately estimate the impact of these ransomware activities. However, existing methods for detecting ransomware addresses rely primarily on time-consuming data collection and clustering heuristics, and they face two major issues: 1) the features of an address itself are insufficient to accurately represent its activity characteristics, and 2) the number of disclosed ransomware addresses is extremely less than the number of unlabeled addresses. These issues lead to a significant number of ransomware addresses being undetected, resulting in a substantial underestimation of the impact of ransomware activities. To solve the above two issues, we propose an optimized ransomware address detection method based on Bitcoin transaction relationships, named XRAD , to detect more ransomware addresses with high performance. To address the first one, we present a cascade feature extraction method for Bitcoin transactions to aggregate features of related addresses after exploring transaction relationships. To address the second one, we build a classification model based on Positive-Unlabeled learning to detect ransomware addresses with high performance. Extensive experiments demonstrate that XRAD significantly improves average accuracy, recall, and F1 score by 15.07%, 19.71%, and 34.83%, respectively, compared to state-of-the-art methods. In total, XRAD detects 120,335 ransomware activities from 2009 to 2023, revealing a development trend and average ransom payment per year that aligns with three reports by FinCEN, Chainalysis, and Coveware.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401976309",
    "type": "article"
  },
  {
    "title": "Matching Tabular Data to Knowledge Graph with Effective Core Column Set Discovery.",
    "doi": "https://doi.org/10.1145/3694979",
    "publication_date": "2024-09-06",
    "publication_year": 2024,
    "authors": "Jingyi Qiu; Aibo Song; Jiahui Jin; Jiaoyan Chen; Xinyu Zhang; Xiaolin Fang; Tianbo Zhang",
    "corresponding_authors": "",
    "abstract": "Matching tabular data to a knowledge graph (KG) is critical for understanding the semantic column types, column relationships, and entities of a table. Existing matching approaches rely heavily on core columns that represent primary subject entities on which other columns in the table depend. However, discovering these core columns before understanding the table’s semantics is challenging. Most prior works use heuristic rules, such as the leftmost column, to discover a single core column, while an insightful discovery of the core column set that accurately captures the dependencies between columns is often overlooked. To address these challenges, we introduce Dependency-aware Core Column Set Discovery ( DaCo ), an iterative method that uses a novel rough matching strategy to identify both inter-column dependencies and the core column set. Additionally, DaCo can be seamlessly integrated with pre-trained language models, as proposed in the optimization module. Unlike other methods, DaCo does not require labeled data or contextual information, making it suitable for real-world scenarios. In addition, it can identify multiple core columns within a table, which is common in real-world tables. We conduct experiments on six datasets, including five datasets with single core columns and one dataset with multiple core columns. Our experimental results show that DaCo outperforms existing core column set detection methods, further improving the effectiveness of table understanding tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402310837",
    "type": "article"
  },
  {
    "title": "Online Incentive Protocol Design for Reposting Service in Online Social Networks",
    "doi": "https://doi.org/10.1145/3696473",
    "publication_date": "2024-09-19",
    "publication_year": 2024,
    "authors": "Haoran Gu; Shiyuan Zheng; Xudong Liu; Hong Xie; John C. S. Lui",
    "corresponding_authors": "",
    "abstract": "Reposting plays an essential role in boosting visibility on online social networks (OSNs). In this paper, we study the problem of designing “reposting service” in an OSN to incentivize “transactions” between requesters (users who seek to enhance visibility) and suppliers (users who are willing to repost if certain incentives are given), and maximize the welfare increase accumulated through a given time horizon. We formulate a mathematical model for reposting which captures various factors like click-through rates (CTRs), requesters’ valuations and suppliers’ costs. We formulate the problem of maximizing the welfare increase via judiciously assigning suppliers to requesters from two aspects: (a) “user-centric” and (b) “platform-centric”. The user-centric aspect deals with situations where requesters and suppliers collaborate and share valuations and costs. To address the challenge of unknown CTRs, we propose an online learning protocol and achieve a sub-linear regret. The platform-centric aspect corresponds to the scenario where users keep their valuations or costs private. To address the challenges of unknown CTRs, valuations and costs, we design an “explore-then-commit” online protocol. We prove the truthfulness of the proposed online protocol, and we also prove that this protocol has a sub-linear regret. Lastly, we conduct extensive experiments on six public datasets to evaluate the effectiveness and scalability of the proposed protocols.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402645322",
    "type": "article"
  },
  {
    "title": "Efficient State Sharding in Blockchain via Density-based Graph Partitioning",
    "doi": "https://doi.org/10.1145/3697840",
    "publication_date": "2024-09-27",
    "publication_year": 2024,
    "authors": "Bo Yin; Peng Zhang; T T Chen",
    "corresponding_authors": "",
    "abstract": "Sharding is a promising technique for increasing a blockchain system’s throughput by enabling parallel transaction processing. The main challenge of state sharding lies in ensuring the atomicity verification of cross-sharding transactions, which results in double communication overhead and increases the transaction’s confirmation time. Previous research has primarily focused on developing cross-shard protocols for the fast and reliable validation of transactions involving multiple shards. These studies typically generate a large number of cross-shard transactions because they primarily use simple address mapping for state sharding, that is, the prefix/suffix of the account address. In this paper, we propose a state sharding scheme via density-based partitioning of the account-transaction graph. In order to reduce cross-shard transactions, the scheme groups correlated accounts into the same shard by generating the densest subgraphs, as the graph density describes the correlation among accounts, i.e., how often transactions have occurred among accounts. We formulate the graph density-based state sharding problem, with the goal of maximizing the average density across all shards under the workload constraint. We prove the NP-completeness of the problem. To reduce the complexity of finding the densest subgraph, we propose the pruning-based algorithm that reduces the search space by pre-pruning some invalid edges based on the concept of core number. We also extend the linear deterministic greedy algorithm and PageRank algorithm to handle new transactions in the dynamic scenario. We conduct extensive experiments using real transaction data from Ethereum. The experimental results demonstrate a strong correlation between the shard density and the number of cross-shard transactions, and the pruning-based algorithm can reduce the running time by an order of magnitude.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402920422",
    "type": "article"
  },
  {
    "title": "Content Augmented Graph Neural Networks",
    "doi": "https://doi.org/10.1145/3700790",
    "publication_date": "2024-10-24",
    "publication_year": 2024,
    "authors": "Fatemeh Gholamzadeh Nasrabadi; Amirhossein Kashani; Pegah Zahedi; Mostafa Haghir Chehreghani",
    "corresponding_authors": "",
    "abstract": "In recent years, graph neural networks (GNNs) have become a popular tool for solving various problems over graphs. In these models, the link structure of the graph is typically exploited and nodes’ embeddings are iteratively updated based on adjacent nodes. Nodes’ contents are used solely in the form of feature vectors, served as nodes’ first-layer embeddings. However, the filters or convolutions, applied during iterations/layers to these initial embeddings lead to their impact diminish and contribute insignificantly to the final embeddings. In order to address this issue, in this paper we propose augmenting nodes’ embeddings by embeddings generated from their content, at higher GNN layers. More precisely, we propose models wherein a structural embedding using a GNN and a content embedding are computed for each node. These two are combined using a combination layer to form the embedding of a node at a given layer layer. We suggest methods such as using an auto-encoder or building a content graph, to generate content embeddings. In the end, by conducting experiments over several real-world datasets, we demonstrate the high accuracy and performance of our models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403735869",
    "type": "article"
  },
  {
    "title": "DOMAIN: Explainable credibility assessment tools for empowering online readers coping with misinformation",
    "doi": "https://doi.org/10.1145/3696472",
    "publication_date": "2024-10-26",
    "publication_year": 2024,
    "authors": "Danielle Caled; Paula Carvalho; Francisco Sousa; Mário J. Silva",
    "corresponding_authors": "",
    "abstract": "Despite all the fact-checking initiatives on news and social media aimed at countering misinformation, they remain insufficient to promptly address the wide array of misleading information disseminated by both news and social media outlets. Rather than attempting to identify or filter misleading information, this work advocates new tools for assisting online readers in identifying misinformation among the massive online content pushed every day through multiple platforms. We introduce DOMAIN, an article assessment resource bundle comprising a multidimensional indicator to categorize articles into different types (hard news, soft news, opinion, satire, and conspiracy), a set of explanatory metrics to help users understand the results, a tool for verifying the reliability of the article’s source, and a text summary of the assessment. This work also studies how DOMAIN tools impact online readers, specifically focusing on i) understanding the extent to which computer-generated assessments influence human perceptions of credibility; ii) evaluating the effectiveness of automatic article categorization in human assessment of credibility; and iii) identifying the most relevant explanatory metrics for promoting informed and critical consumption of information.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403790092",
    "type": "article"
  },
  {
    "title": "Word-Level Political Sentiments Inferred From Social Media and Application in Recommendation Diversification",
    "doi": "https://doi.org/10.1145/3700643",
    "publication_date": "2024-11-15",
    "publication_year": 2024,
    "authors": "Yihong Zhang; Masumi Shirakawa; Takahiro Hara",
    "corresponding_authors": "",
    "abstract": "Political polarization is commonly observed in democratic countries. While it allows individual citizens to freely choose sides, it also causes the problem of separation and isolation. Especially in information-seeking behaviors, echo chambers and filter bubbles are observed. In this paper, we present a political sentiment dictionary for analyzing political polarization and increasing information heterogeneity. It takes advantage of large-scale social media data and is thus superior in accuracy and coverage compared to manually crafted dictionaries. Generated from Japanese tweets, more than 50k words in this dictionary cover aspects ranging from political parties and public entities to foods and personal hobbies. We describe in detail the method to construct this dictionary, which can be replicated for other languages and countries. We demonstrate the use of this dictionary in the application of recommendation diversification. We show with real-world e-commerce data that the use of the dictionary can generally increase the diversity in product recommendations, effectively mitigating the filter bubbles.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404417983",
    "type": "article"
  },
  {
    "title": "“Double vaccinated, 5G boosted!”: Learning Attitudes Towards COVID-19 Vaccination from Social Media",
    "doi": "https://doi.org/10.1145/3702654",
    "publication_date": "2024-12-04",
    "publication_year": 2024,
    "authors": "Ninghan Chen; Xihui Chen; Zhiqiang Zhong; Jun Pang",
    "corresponding_authors": "",
    "abstract": "The sudden onset of the recently concluded COVID-19 pandemic has driven substantial progress in various scientific fields. One notable example is the comprehension of public vaccination attitudes and the timely monitoring of their fluctuations through social media platforms. This approach can serve as a cost-effective means to supplement surveys in gathering public vaccine hesitancy levels. In this article, we propose a deep learning framework leveraging textual posts on social media to extract and track users' vaccination stances in near real time. Compared to previous works, we integrate into the framework the recent posts of a user's social network friends to collaboratively detect the user's genuine attitude towards vaccination. Based on our annotated dataset from X (formerly known as Twitter), the models instantiated from our framework can increase the performance of attitude extraction by up to 23% compared to the state-of-the-art text-only models. Using this framework, we successfully confirm the feasibility of using social media to track the evolution of vaccination attitudes in real life. In addition, we illustrate the generality of our framework in extracting other public opinions such as political ideology. We further show one practical use of our framework by validating the possibility of forecasting a user's vaccine hesitancy changes with information perceived from social media.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405003589",
    "type": "article"
  },
  {
    "title": "Comparing Echo Chamber Detection Metrics: A Cross-modeling and Cross-platform Analysis of Twitter and Reddit",
    "doi": "https://doi.org/10.1145/3707701",
    "publication_date": "2024-12-10",
    "publication_year": 2024,
    "authors": "Paola Impiccichè; Marco Viviani",
    "corresponding_authors": "",
    "abstract": "Social media platforms have become central arenas for public discourse, enabling the exchange of ideas and information among diverse user groups. However, the rise of echo chambers, where individuals reinforce their existing beliefs through repeated interactions with like-minded users, poses significant challenges to the democratic exchange of ideas and the potential for polarization and information disorder. This paper presents a comparative analysis of the main metrics that have been proposed in the literature for echo chamber detection, with a focus on their application in a cross-platform scenario constituted by the two major social media platforms, i.e., Twitter (now renamed \\(\\mathbb {X} \\) ) and Reddit. The echo chamber detection metrics considered encompass network analysis, content analysis, and hybrid solutions. The findings of this work shed light on the unique dynamics of echo chambers present on the two social media platforms, while also highlighting the strengths and limitations of various metrics employed to identify them, and their transversality to the different social graph modeling and domains considered.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405248842",
    "type": "article"
  },
  {
    "title": "The Pulse of Mood Online: Unveiling Emotional Reactions in a Dynamic Social Media Landscape",
    "doi": "https://doi.org/10.1145/3708513",
    "publication_date": "2024-12-17",
    "publication_year": 2024,
    "authors": "Siyi Guo; Zihao He; Ashwin Rao; Fred Morstatter; P. Jeffrey Brantingham; Kristina Lerman",
    "corresponding_authors": "",
    "abstract": "The rich and dynamic information environment of social media provides researchers, policymakers, and entrepreneurs with opportunities to learn about social phenomena in a timely manner. However, using these data to understand social behavior is difficult due to heterogeneity of topics and events discussed in the highly dynamic online information environment. To address these challenges, we present a method for systematically detecting and measuring emotional reactions to offline events using change point detection on the time series of collective affect, and further explaining these reactions using a transformer-based topic model. We demonstrate the utility of the method by successfully detecting major and smaller events on three different datasets, including (1) a Los Angeles Tweet dataset between Jan. and Aug. 2020, in which we revealed the complex psychological impact of the BlackLivesMatter movement and the COVID-19 pandemic, (2) a dataset related to abortion rights discussions in USA, in which we uncovered the strong emotional reactions to the overturn of Roe v. Wade and state abortion bans, and (3) a dataset about the 2022 French presidential election, in which we discovered the emotional and moral shift from positive before voting to fear and criticism after voting. We further demonstrate the importance of disaggregating data by topics and populations to mitigate potential biases when studying collective emotions. The capability of our method allows for better sensing and monitoring of population’s reactions during crises using online data.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405513527",
    "type": "article"
  },
  {
    "title": "Ransomware Over Modern Web Browsers: A Novel Strain and a New Defense Mechanism",
    "doi": "https://doi.org/10.1145/3708514",
    "publication_date": "2024-12-17",
    "publication_year": 2024,
    "authors": "Harun Oz; Güliz Seray Tuncay; Ahmet Arış; Abbas Acar; Leonardo Babun; A. Selcuk Uluagac",
    "corresponding_authors": "",
    "abstract": "Ransomware is an increasingly prevalent form of malware targeting end-users, governments, and businesses. As it has evolved, adversaries added new capabilities to their arsenal. We propose a next-generation browser-based ransomware, RøB , which performs its malicious actions via web technologies, File System Access API (FSA) and WebAssembly (Wasm). RøB uses this API through the victims’ browsers; hence, it does not require the victims to download and install malicious binaries. We performed extensive evaluations with 3 different OSs, 23 file formats, 29 distinct directories, 5 cloud providers, and 4 antivirus solutions. Our evaluations show that RøB can encrypt various types of files in the local and cloud-integrated directories, external storage devices, and network-shared folders of victims. Our experiments also reveal that popular cloud solutions, Box Individual and Apple iCloud can be severely affected by RøB . Moreover, we conducted tests with commercial antivirus software such as AVG, Avast, Kaspersky, Malware Bytes that perform sensitive directory and suspicious behavior monitoring against ransomware. We verified that RøB can evade these antivirus software and encrypt victim files. Moreover, existing ransomware detection solutions in the literature also cannot be a remedy against RøB due to its distinct features. Therefore, in this paper, we also propose RøBguard , a new detection system for RøB -like attacks. RøBguard monitors the web applications that use the FSA API via function hooking and uses a machine learning classifier to detect RøB -like attacks. We implemented a proof of concept version of RøBguard and our evaluation results show that RøBguard can detect RøB -like browser-based ransomware attacks effectively. We also provide future research directions that should be addressed in this domain.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405513875",
    "type": "article"
  },
  {
    "title": "PatternRank+NN",
    "doi": "https://doi.org/10.1145/3386042",
    "publication_date": "2020-05-03",
    "publication_year": 2020,
    "authors": "Zhijun Xiao; Cuiping Li; Hong Chen",
    "corresponding_authors": "",
    "abstract": "We propose a ranking framework, called PatternRank+NN, for expanding a set of seed entities of a particular class (i.e., entity set expansion) from Web search queries. PatternRank+NN consists of two parts: PatternRank and NN. Unlike the traditional methods, PatternRank brings user behaviors into entity set expansion from Web search queries. PatternRank is a Markov chain which simulates the Web search query process of users on the graph model for Web search query log, and ranks the features of the class. The features in the front rank are used to generate candidate entities of the class. NN, a ranking strategy called Nearest Neighbor, ranks these candidate entities such that the set of seed entities can be expanded from the candidate entities in the front rank. Our experiments demonstrate the superior performance of PatternRank+NN in comparison with the state-of-the-art methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3021683212",
    "type": "article"
  },
  {
    "title": "CBPCS",
    "doi": "https://doi.org/10.1145/3411494",
    "publication_date": "2020-10-29",
    "publication_year": 2020,
    "authors": "Jian Cao; Tingjie Jia; Shiyou Qian; Haiyan Zhao; Jie Wang",
    "corresponding_authors": "",
    "abstract": "With the development of cloud computing and the advent of the Web 2.0 era, composing a set of Web services as a service process is becoming a common practice to provide more functional services. However, a service process involves multiple service invocations over the network, which incurs a huge time cost and could become a bottleneck to performance. To accelerate its execution, we propose an engine-side cache-block-based service process caching strategy (CBPCS). It is based on, and derives its advantages from, three key ideas. First, the invocation of Web services embodies semantics, which enables the application of semantic-based caching. Second, cache blocks are identified from a service process, and each block is equipped with a separate cache so that the time overhead of service invocation and caching can be minimized. Third, a replacement strategy is introduced taking into account time and space factors to manage the space allocation for a process with multiple caches. The algorithms and methods used in CBPCS are introduced in detail. Moreover, how CBPCS can be applied to multiple service process models is also investigated. Finally, CBPCS is validated via comparison experiments, which shows the considerable improvements of CBPCS over other strategies.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3094652234",
    "type": "article"
  },
  {
    "title": "sGrow: Explaining the Scale-Invariant Strength Assortativity of Streaming Butterflies",
    "doi": "https://doi.org/10.1145/3572408",
    "publication_date": "2022-12-14",
    "publication_year": 2022,
    "authors": "Aida Sheshbolouki; M. TAMER ÖZSU",
    "corresponding_authors": "",
    "abstract": "Bipartite graphs are rich data structures with prevalent applications and characteristic structural features. However, less is known about their growth patterns, particularly in streaming settings. Current works study the patterns of static or aggregated temporal graphs optimized for certain downstream analytics or ignoring multipartite/non-stationary data distributions, emergence patterns of subgraphs, and streaming paradigms. To address these, we perform statistical network analysis over web log streams and identify the governing patterns underlying the bursty emergence of mesoscopic building blocks, 2, 2-bicliques, leading to a phenomenon that we call scale-invariant strength assortativity of streaming butterflies . We provide the graph-theoretic explanation of this phenomenon. We further introduce a set of micro-mechanics in the body of a streaming growth algorithm, sGrow , to pinpoint the generative origins. sGrow supports streaming paradigms, emergence of four-vertex graphlets, and provides user-specified configurations for the scale, burstiness, level of strength assortativity, probability of out-of-order records, generation time, and time-sensitive connections. Comprehensive evaluations on pattern reproducing and stress testing validate the effectiveness, efficiency, and robustness of sGrow in realization of the observed patterns independent of initial conditions, scale, temporal characteristics, and model configurations. Theoretical and experimental analysis verify sGrow ’s robustness in generating streaming graphs based on user-specified configurations that affect the scale and burstiness of the stream, level of strength assortativity, probability of out-of-order streaming records, generation time, and time-sensitive connections.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4311491436",
    "type": "article"
  },
  {
    "title": "Special Issue on Conversational Information Seeking",
    "doi": "https://doi.org/10.1145/3688392",
    "publication_date": "2024-10-08",
    "publication_year": 2024,
    "authors": "Wenqiang Lei; Richang Hong; Hamed Zamani; Paweł Budzianowski; Vanessa Murdock; Emine Yilmaz",
    "corresponding_authors": "",
    "abstract": "In this article, we provide an overview of ACM TWEB’s Special Issue on Conversational Information Seeking. It highlights both research and practical applications in this field. The article also discusses the future potential of conversational information seeking technology.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403219949",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2788341",
    "publication_date": "2015-06-20",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In recent years, models, composition paradigms, and tools for mashup development have been proposed to support the integration of information sources, services and APIs available on the Web. The challenge is to provide a gate to a “programmable Web,” ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229950665",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2932204",
    "publication_date": "2016-05-25",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "User-based collaborative filtering, a widely used nearest neighbour-based recommendation technique, predicts an item’s rating by aggregating its ratings from similar users. User similarity is traditionally calculated by cosine similarity or the Pearson ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230489772",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2870642",
    "publication_date": "2016-02-17",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A CSS-sprite packing problem is considered in this article. CSS-sprite is a technique of combining many pictures of a web page into one image for the purpose of reducing network transfer time. The CSS-sprite packing problem is formulated here as an ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232559691",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2686863",
    "publication_date": "2014-11-06",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The emergence of numerous online sources about local services presents a need for more automatic yet accurate data integration techniques. Local services are georeferenced objects and can be queried by their locations on a map, for instance, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238878501",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2726021",
    "publication_date": "2015-01-23",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "For service users to get the best service that meet their requirements, they prefer to personalize their nonfunctional attributes, such as reliability and price. However, the personalization makes it challenging for service providers to completely meet ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240995992",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2988335",
    "publication_date": "2016-08-29",
    "publication_year": 2016,
    "authors": "Lorrie Faith Cranor; Giovanni Leon; Blase Ur",
    "corresponding_authors": "Lorrie Faith Cranor",
    "abstract": "It has been shown that top-k retrieval quality can be considerably improved by taking not only relevance but also diversity into account. However, currently proposed diversification approaches have not put much attention on practical usability in large-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246976029",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2639948",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Modeling trust in very large social networks is a hard problem due to the highly noisy nature of these networks that span trust relationships from many different contexts, based on judgments of reliability, dependability, and competence. Furthermore, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249496485",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2755995",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "editorial Free AccessEditorialACM Transactions on the WebVolume 9Issue 2Article No.: 6pp 1–2https://doi.org/10.1145/2755995Published:13 May 2015Publication History 0citation82DownloadsMetricsTotal Citations0Total Downloads82Last 12 Months2Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251862923",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2776789",
    "publication_date": "2015-05-26",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Crowdsourcing (CS) is the outsourcing of a unit of work to a crowd of people via an open call for contributions. Thanks to the availability of online CS platforms, such as Amazon Mechanical Turk or CrowdFlower, the practice has experienced a tremendous ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251910946",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2830542",
    "publication_date": "2015-10-26",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251944745",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2600093",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The Internet is increasingly used by young children for all kinds of purposes. Nonetheless, there are not many resources especially designed for children on the Internet and most of the content online is designed for grown-up users. This situation is ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255961442",
    "type": "paratext"
  },
  {
    "title": "Semantic Table Retrieval Using Keyword and Table Queries",
    "doi": "https://doi.org/10.1145/3441690",
    "publication_date": "2021-05-13",
    "publication_year": 2021,
    "authors": "Shuo Zhang; Krisztian Balog",
    "corresponding_authors": "",
    "abstract": "Tables on the Web contain a vast amount of knowledge in a structured form. To tap into this valuable resource, we address the problem of table retrieval: answering an information need with a ranked list of tables. We investigate this problem in two different variants, based on how the information need is expressed: as a keyword query or as an existing table (“query-by-table”). The main novel contribution of this work is a semantic table retrieval framework for matching information needs (keyword or table queries) against tables. Specifically, we (i) represent queries and tables in multiple semantic spaces (both discrete sparse and continuous dense vector representations) and (ii) introduce various similarity measures for matching those semantic representations. We consider all possible combinations of semantic representations and similarity measures and use these as features in a supervised learning model. Using two purpose-built test collections based on Wikipedia tables, we demonstrate significant and substantial improvements over state-of-the-art baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3163913310",
    "type": "article"
  },
  {
    "title": "Sampling Graphlets of Multiplex Networks: A Restricted Random Walk Approach",
    "doi": "https://doi.org/10.1145/3456291",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Simiao Jiao; Zihui Xue; Xiaowei Chen; Yuedong Xu",
    "corresponding_authors": "",
    "abstract": "Graphlets are induced subgraph patterns that are crucial to the understanding of the structure and function of a large network. A lot of effort has been devoted to calculating graphlet statistics where random walk-based approaches are commonly used to access restricted graphs through the available application programming interfaces (APIs). However, most of them merely consider individual networks while overlooking the strong coupling between different networks. In this article, we estimate the graphlet concentration in multiplex networks with real-world applications. An inter-layer edge connects two nodes in different layers if they actually belong to the same node. The access to a multiplex network is restrictive in the sense that the upper layer allows random walk sampling, whereas the nodes of lower layers can be accessed only through the inter-layer edges and only support random node or edge sampling. To cope with this new challenge, we define a suit of two-layer graphlets and propose novel random walk sampling algorithms to estimate the proportion of all the three-node graphlets. An analytical bound on the sampling steps is proved to guarantee the convergence of our unbiased estimator. We further generalize our algorithm to explore the tradeoff between the estimated accuracy of different graphlets when the sample budget is split into different layers. Experimental evaluation on real-world and synthetic multiplex networks demonstrates the accuracy and high efficiency of our unbiased estimators.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3166461260",
    "type": "article"
  },
  {
    "title": "Context-aware Distance Measures for Dynamic Networks",
    "doi": "https://doi.org/10.1145/3476228",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Yiji Zhao; Youfang Lin; Zhihao Wu; Yang Wang; Haomin Wen",
    "corresponding_authors": "",
    "abstract": "Dynamic networks are widely used in the social, physical, and biological sciences as a concise mathematical representation of the evolving interactions in dynamic complex systems. Measuring distances between network snapshots is important for analyzing and understanding evolution processes of dynamic systems. To the best of our knowledge, however, existing network distance measures are designed for static networks. Therefore, when measuring the distance between any two snapshots in dynamic networks, valuable context structure information existing in other snapshots is ignored. To guide the construction of context-aware distance measures, we propose a context-aware distance paradigm, which introduces context information to enrich the connotation of the general definition of network distance measures. A Context-aware Spectral Distance (CSD) is then given as an instance of the paradigm by constructing a context-aware spectral representation to replace the core component of traditional Spectral Distance (SD). In a node-aligned dynamic network, the context effectively helps CSD gain mainly advantages over SD as follows: (1) CSD is not affected by isospectral problems; (2) CSD satisfies all the requirements of a metric, while SD cannot; and (3) CSD is computationally efficient. In order to process large-scale networks, we develop a kCSD that computes top- k eigenvalues to further reduce the computational complexity of CSD. Although kCSD is a pseudo-metric, it retains most of the advantages of CSD. Experimental results in two practical applications, i.e., event detection and network clustering in dynamic networks, show that our context-aware spectral distance performs better than traditional spectral distance in terms of accuracy, stability, and computational efficiency. In addition, context-aware spectral distance outperforms other baseline methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3202343680",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1961659",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Searchers on the Web often aim to find key resources about a topic. Finding such results is called topic distillation. Previous research has shown that the use of sources of evidence such as page indegree and URL structure can significantly improve ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230839775",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1993053",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "LDAP directories have proliferated as the appropriate storage framework for various and heterogeneous data sources, operating under a wide range of applications and services. Due to the increased amount and heterogeneity of the LDAP data, there is a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232131513",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2460383",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The Service-Oriented Architecture (SOA) paradigm is giving rise to a new generation of applications built by dynamically composing loosely coupled autonomous services. Clients (i.e., software agents acting on behalf of human users or service providers) ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234251132",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2560539",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Services are an indispensable component in cloud computing. Web services are particularly important. As an increasing number of Web services provides equivalent functions, one common issue faced by users is the selection of the most appropriate one ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237148633",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2019643",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240771590",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2435215",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Visual complexities (VisComs) of Web pages significantly affect user experience, and automatic evaluation can facilitate a large number of Web-based applications. The construction of a model for measuring the VisComs of Web pages requires the extraction ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243467793",
    "type": "paratext"
  },
  {
    "title": "Classifying search quries using the web as a source of knowledge.",
    "doi": null,
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Evgeniy Gabrilovich; Andrei Broder; Marcus Fontoura; Amruta Joshi; Vanja Josifovski; Lance Riedel; Tong Zhang",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3195958942",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1513876",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a methodology for building a robust query classification system that can identify thousands of query classes, while dealing in real time with the query volume of a commercial Web search engine. We use a pseudo relevance feedback technique: ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232008164",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1734200",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "An ads-portal domain refers to a Web domain that shows only advertisements, served by a third-party advertisement syndication service, in the form of ads listing. We develop a machine-learning-based classifier to identify ads-portal domains, which has ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235193145",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2180861.2180862",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Helen Ashman; Arun Iyengar; Marc Najork",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236137170",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1806916",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Map search engines, such as Google Maps, Yahoo! Maps, and Microsoft Live Maps, allow users to explicitly specify a target geographic location, either in keywords or on the map, and to search businesses, people, and other information of that location. In ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240064806",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1841909",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The rise of the software-as-a-service paradigm has led to the development of a new breed of sophisticated, interactive applications often called Web 2.0. While Web applications have become larger and more complex, Web application developers today have ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241185995",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1541822",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245210009",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2382616",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247461082",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2109205",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "User browsing information, particularly non-search-related activity, reveals important contextual information on the preferences and intents of Web users. In this article, we demonstrate the importance of mining general Web user behavior data to improve ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255645596",
    "type": "paratext"
  },
  {
    "title": "Analyzing the Adoption and Cascading Process of OSN-Based Gifting Applications",
    "doi": "https://doi.org/10.1145/3023871",
    "publication_date": "2017-04-14",
    "publication_year": 2017,
    "authors": "Masoud Rahman; Jinyoung Han; Yong‐Jae Lee; Chen‐Nee Chuah",
    "corresponding_authors": "",
    "abstract": "To achieve growth in the user base of online social networks--(OSN) based applications, word-of-mouth diffusion mechanisms, such as user-to-user invitations, are widely used. This article characterizes the adoption and cascading process of OSN-based applications that grow via user invitations. We analyze a detailed large-scale dataset of a popular Facebook gifting application, iHeart, that contains more than 2 billion entries of user activities generated by 190 million users during a span of 64 weeks. We investigate (1) how users invite their friends to an OSN-based application, (2) how application adoption of an individual user can be predicted, (3) what factors drive the cascading process of application adoptions, and (4) what are the good predictors of the ultimate cascade sizes. We find that sending or receiving a large number of invitations does not necessarily help to recruit new users to iHeart. We also find that the average success ratio of inviters is the most important feature in predicting an adoption of an individual user, which indicates that the effectiveness of inviters has strong predictive power with respect to application adoption. Based on the lessons learned from our analyses, we build and evaluate learning-based models to predict whether a user will adopt iHeart. Our proposed model that utilizes additional activity information of individual users from other similar types of gifting applications can achieve high precision (83%) in predicting adoptions in the target application (i.e., iHeart). We next identify a set of distinctive features that are good predictors of the growth of the application adoptions in terms of final population size. We finally propose a prediction model to infer whether a cascade of application adoption will continue to grow in the future based on observing the initial adoption process. Results show that our proposed model can achieve high precision (over 80%) in predicting large cascades of application adoptions. We believe our work can give an important implication in resource allocation of OSN-based product stakeholders, for example, via targeted marketing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2575671729",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1409220",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "As popular search engines face the sometimes conflicting interests of protecting privacy while retaining query logs for a variety of uses, numerous technical measures have been suggested to both enhance privacy and preserve at least a portion of the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231906259",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3062397",
    "publication_date": "2017-04-10",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The use of second screen devices with social media facilitates conversational interaction concerning broadcast media events, creating what we refer to as the social soundtrack. In this research, we evaluate the change of the Super Bowl XLIX social ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237862530",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3079924",
    "publication_date": "2017-05-12",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "It is often difficult to separate the highly capable “experts” from the average worker in crowdsourced systems. This is especially true for challenge application domains that require extensive domain knowledge. The problem of stock analysis is one such ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240292876",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1281480",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Vulnerability-driven filtering of network data can offer a fast and easy-to-deploy alternative or intermediary to software patching, as exemplified in Shield [Wang et al. 2004]. In this article, we take Shield's vision to a new domain, inspecting and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240837187",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3127338",
    "publication_date": "2017-09-01",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This work aimed to propose LDoW-PaN, a Linked Data presentation and navigation model focused on the average user. The LDoW-PaN model is an extension of the Dexter Hypertext Reference Model. Through the LDoW-PaN model, ordinary people—who have no ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242233267",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1377488",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243261595",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1346337",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Semantic annotations of web services can support the effective and efficient discovery of services, and guide their composition into workflows. At present, however, the practical utility of such annotations is limited by the small number of service ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249482276",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3113174",
    "publication_date": "2017-07-12",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "When users interact with the Web today, they leave sequential digital trails on a massive scale. Examples of such human trails include Web navigation, sequences of online restaurant reviews, or online music play lists. Understanding the factors that ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256194702",
    "type": "paratext"
  },
  {
    "title": "Dynamic Bayesian Contrastive Predictive Coding Model for Personalized Product Search",
    "doi": "https://doi.org/10.1145/3609225",
    "publication_date": "2023-07-13",
    "publication_year": 2023,
    "authors": "Bin Wu; Zaiqiao Meng; Shangsong Liang",
    "corresponding_authors": "",
    "abstract": "In this article, we study the problem of dynamic personalized product search. Due to the data-sparsity problem in the real world, existing methods suffer from the challenge of data inefficiency. We address the challenge by proposing a Dynamic Bayesian Contrastive Predictive Coding model (DBCPC), which aims to capture the rich structured information behind search records to improve data efficiency. Our proposed DBCPC utilizes contrastive predictive learning to jointly learn dynamic embeddings with structure information of entities (i.e., users, products, and words). Specifically, our DBCPC employs structured prediction to tackle the intractability caused by non-linear output space and utilizes the time embedding technique to avoid designing different encoders each time in the Dynamic Bayesian models. In this way, our model jointly learns the underlying embeddings of entities (i.e., users, products, and words) via prediction tasks, which enables the embeddings to focus more on their general attributes and capture the general information during the preference evolution with time. For inferring the dynamic embeddings, we propose an inference algorithm combining the variational objective and the contrastive objectives. Experiments were conducted on an Amazon dataset and the experimental results show that our proposed DBCPC can learn the higher-quality embeddings and outperforms the state-of-the-art non-dynamic and dynamic models for product search.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4384201271",
    "type": "article"
  },
  {
    "title": "Bridging Performance of X (formerly known as Twitter) Users: A Predictor of Subjective Well-Being During the Pandemic",
    "doi": "https://doi.org/10.1145/3635033",
    "publication_date": "2023-11-30",
    "publication_year": 2023,
    "authors": "Ninghan Chen; Xihui Chen; Zhiqiang Zhong; Jun Pang",
    "corresponding_authors": "",
    "abstract": "The outbreak of the COVID-19 pandemic triggered the perils of misinformation over social media. By amplifying the spreading speed and popularity of trustworthy information, influential social media users have been helping overcome the negative impacts of such flooding misinformation. In this article, we use the COVID-19 pandemic as a representative global health crisisand examine the impact of the COVID-19 pandemic on these influential users’ subjective well-being (SWB), one of the most important indicators of mental health. We leverage X (formerly known as Twitter) as a representative social media platform and conduct the analysis with our collection of 37,281,824 tweets spanning almost two years. To identify influential X users, we propose a new measurement called user bridging performance (UBM) to evaluate the speed and wideness gain of information transmission due to their sharing. With our tweet collection, we manage to reveal the more significant mental sufferings of influential users during the COVID-19 pandemic. According to this observation, through comprehensive hierarchical multiple regression analysis, we are the first to discover the strong relationship between individual social users’ subjective well-being and their bridging performance. We proceed to extend bridging performance from individuals to user subgroups. The new measurement allows us to conduct a subgroup analysis according to users’ multilingualism and confirm the bridging role of multilingual users in the COVID-19 information propagation. We also find that multilingual users not only suffer from a much lower SWB in the pandemic, but also experienced a more significant SWB drop.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389196097",
    "type": "article"
  },
  {
    "title": "Unsupervised Domain Ranking in Large-Scale Web Crawls",
    "doi": "https://doi.org/10.1145/3182180",
    "publication_date": "2018-09-27",
    "publication_year": 2018,
    "authors": "Yi Cui; Clint Sparkman; Hsin-Tsang Lee; Dmitri Loguinov",
    "corresponding_authors": "",
    "abstract": "With the proliferation of web spam and infinite autogenerated web content, large-scale web crawlers require low-complexity ranking methods to effectively budget their limited resources and allocate bandwidth to reputable sites. In this work, we assume crawls that produce frontiers orders of magnitude larger than RAM, where sorting of pending URLs is infeasible in real time. Under these constraints, the main objective is to quickly compute domain budgets and decide which of them can be massively crawled. Those ranked at the top of the list receive aggressive crawling allowances, while all other domains are visited at some small default rate. To shed light on Internet-wide spam avoidance, we study topology-based ranking algorithms on domain-level graphs from the two largest academic crawls: a 6.3B-page IRLbot dataset and a 1B-page ClueWeb09 exploration. We first propose a new methodology for comparing the various rankings and then show that in-degree BFS-based techniques decisively outperform classic PageRank-style methods, including TrustRank. However, since BFS requires several orders of magnitude higher overhead and is generally infeasible for real-time use, we propose a fast, accurate, and scalable estimation method called TSE that can achieve much better crawl prioritization in practice. It is especially beneficial in applications with limited hardware resources.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2892827555",
    "type": "article"
  },
  {
    "title": "A Rule-Based Transducer for Querying Incompletely Aligned Datasets",
    "doi": "https://doi.org/10.1145/3228328",
    "publication_date": "2018-09-27",
    "publication_year": 2018,
    "authors": "Ana I. Torre-Bastida; Jesús Bermúdez; Arantza Illarramendi",
    "corresponding_authors": "",
    "abstract": "A growing number of Linked Open Data sources (from diverse provenances and about different domains) that can be freely browsed and searched to find and extract useful information have been made available. However, access to them is difficult for different reasons. This study addresses access issues concerning heterogeneity. It is common for datasets to describe the same or overlapping domains while using different vocabularies. Our study presents a transducer that transforms a SPARQL query suitably expressed in terms of the vocabularies used in a source dataset into another SPARQL query suitably expressed for a target dataset involving different vocabularies. The transformation is based on existing alignments between terms in different datasets. Whenever the transducer is unable to produce a semantically equivalent query because of the scarcity of term alignments, the transducer produces a semantic approximation of the query to avoid returning the empty answer to the user. Transformation across datasets is achieved through the management of a wide range of transformation rules. The feasibility of our proposal has been validated with a prototype implementation that processes queries that appear in well-known benchmarks and SPARQL endpoint logs. Results of the experiments show that the system is quite effective in achieving adequate transformations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2893233607",
    "type": "article"
  },
  {
    "title": "2017 TWEB Reviewers",
    "doi": "https://doi.org/10.1145/3209033",
    "publication_date": "2018-05-27",
    "publication_year": 2018,
    "authors": "Brian D. Davison",
    "corresponding_authors": "Brian D. Davison",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2971896796",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3176641",
    "publication_date": "2018-06-20",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A knowledge graph is a graph with entities of different types as nodes and various relations among them as edges. The construction of knowledge graphs in the past decades facilitates many applications, such as link prediction, web search analysis, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238599225",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3133955",
    "publication_date": "2018-02-05",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "DanMu, an emerging type of user-generated comment, has become increasingly popular in recent years. Many online video platforms such as Tudou.com have provided the DanMu function. Unlike traditional online reviews such as reviews at Youtube.com that are ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238925806",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3372405",
    "publication_date": "2019-12-20",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Free web proxies promise anonymity and censorship circumvention at no cost. Several websites publish lists of free proxies organized by country, anonymity level, and performance. These lists index hundreds of thousands of hosts discovered via automated ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239984427",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3352383",
    "publication_date": "2019-11-18",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Offensive or antagonistic language targeted at individuals and social groups based on their personal characteristics (also known as cyber hate speech or cyberhate) has been frequently posted and widely circulated via the World Wide Web. This can be ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241537810",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3313948",
    "publication_date": "2019-04-12",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Many current web pages include structured data which can directly be processed and used. Search engines, in particular, gather that structured data and provide question answering capabilities over the integrated data with an entity-centric presentation ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241931588",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3281744",
    "publication_date": "2018-11-06",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250686906",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3232925",
    "publication_date": "2018-05-31",
    "publication_year": 2018,
    "authors": "Brian D. Davison",
    "corresponding_authors": "Brian D. Davison",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252799767",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3297729",
    "publication_date": "2019-02-20",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Website privacy policies are often long and difficult to understand. While research shows that Internet users care about their privacy, they do not have the time to understand the policies of every website they visit, and most users hardly ever read ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253555297",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3240924",
    "publication_date": "2018-07-18",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In our prior work, we presented a novel approach to the evaluation of quality in use of corporate web sites based on an original quality model (QM-U) and a related methodology (EQ-EVAL). This article focuses on two research questions. The first one aims ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256487363",
    "type": "paratext"
  },
  {
    "title": "「両世界の最良」 自動AOI検出のためのWebページと視線追跡データ駆動アプローチの統合【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2020-01-01",
    "publication_year": 2020,
    "authors": "Sukru Eraslan; Yeliz Yeşilada; Harper Simon",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3175618199",
    "type": "article"
  },
  {
    "title": "Decoding the Kodi Ecosystem",
    "doi": "https://doi.org/10.1145/3563700",
    "publication_date": "2022-09-15",
    "publication_year": 2022,
    "authors": "Yunming Xiao; Matteo Varvello; Marc Anthony Warrior; Aleksandar Kuzmanovic",
    "corresponding_authors": "",
    "abstract": "Free and open-source media centers are experiencing a boom in popularity for the convenience they offer users seeking to remotely consume digital content. Kodi is today’s most popular home media center, with millions of users worldwide. Kodi’s popularity derives from its ability to centralize the sheer amount of media content available on the Web, both free and copyrighted . Researchers have been hinting at potential security concerns around Kodi, due to add-ons injecting unwanted content as well as user settings linked with security holes. Motivated by these observations, this article conducts the first comprehensive analysis of the Kodi ecosystem: 15,000 Kodi users from 104 countries, 11,000 unique add-ons, and data collected over 9 months. Our work makes three important contributions. Our first contribution is that we build “crawling” software ( de-Kodi ) which can automatically install a Kodi add-on, explore its menu, and locate (video) content. This is challenging for two main reasons. First, Kodi largely relies on visual information and user input which intrinsically complicates automation. Second, the potential sheer size of this ecosystem (i.e., the number of available add-ons) requires a highly scalable crawling solution. Our second contribution is that we develop a solution to discover Kodi add-ons. Our solution combines Web crawling of popular websites where Kodi add-ons are published (LazyKodi and GitHub) and SafeKodi , a Kodi add-on we have developed which leverages the help of Kodi users to learn which add-ons are used in the wild and, in return, offers information about how safe these add-ons are, e.g., do they track user activity or contact sketchy URLs/IP addresses. Our third contribution is a classifier to passively detect Kodi traffic and add-on usage in the wild. Our analysis of the Kodi ecosystem reveals the following findings. We find that most installed add-ons are unofficial but safe to use. Still, 78% of the users have installed at least one unsafe add-on, and even worse, such add-ons are among the most popular. In response to the information offered by SafeKodi, one-third of the users reacted by disabling some of their add-ons. However, the majority of users ignored our warnings for several months attracted by the content such unsafe add-ons have to offer. Last but not least, we show that Kodi’s auto-update, a feature active for 97.6% of SafeKodi users, makes Kodi users easily identifiable by their ISPs. While passively identifying which Kodi add-on is in use is, as expected, much harder, we also find that many unofficial add-ons do not use HTTPS yet, making their passive detection straightforward. 1",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4296131092",
    "type": "article"
  },
  {
    "title": "Welcome Message from the New Editor-in-Chief",
    "doi": "https://doi.org/10.1145/3456294",
    "publication_date": "2021-05-13",
    "publication_year": 2021,
    "authors": "Ryen W. White",
    "corresponding_authors": "Ryen W. White",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3162230302",
    "type": "article"
  },
  {
    "title": "Sequential Learning-based IaaS Composition",
    "doi": "https://doi.org/10.1145/3452332",
    "publication_date": "2021-07-14",
    "publication_year": 2021,
    "authors": "Sajib Mistry; Sheik Mohammad Mostakim Fattah; Athman Bouguettaya",
    "corresponding_authors": "",
    "abstract": "We propose a novel Infrastructure-as-a-Service composition framework that selects an optimal set of consumer requests according to the provider’s qualitative preferences on long-term service provisions. Decision variables are included in the temporal conditional preference networks to represent qualitative preferences for both short-term and long-term consumers. The global preference ranking of a set of requests is computed using a k -d tree indexing-based temporal similarity measure approach. We propose an extended three-dimensional Q-learning approach to maximize the global preference ranking. We design the on-policy-based sequential selection learning approach that applies the length of request to accept or reject requests in a composition. The proposed on-policy-based learning method reuses historical experiences or policies of sequential optimization using an agglomerative clustering approach. Experimental results prove the feasibility of the proposed framework.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3179595431",
    "type": "article"
  }
]