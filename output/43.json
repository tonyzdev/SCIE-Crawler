[
  {
    "title": "<i>L</i> -diversity",
    "doi": "https://doi.org/10.1145/1217299.1217302",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Ashwin Machanavajjhala; Daniel Kifer; Johannes Gehrke; Muthuramakrishnan Venkitasubramaniam",
    "corresponding_authors": "",
    "abstract": "Publishing data about individuals without revealing sensitive information about them is an important problem. In recent years, a new definition of privacy called k -anonymity has gained popularity. In a k -anonymized dataset, each record is indistinguishable from at least k − 1 other records with respect to certain identifying attributes. In this article, we show using two simple attacks that a k -anonymized dataset has some subtle but severe privacy problems. First, an attacker can discover the values of sensitive attributes when there is little diversity in those sensitive attributes. This is a known problem. Second, attackers often have background knowledge, and we show that k -anonymity does not guarantee privacy against attackers using background knowledge. We give a detailed analysis of these two attacks, and we propose a novel and powerful privacy criterion called ℓ-diversity that can defend against such attacks. In addition to building a formal foundation for ℓ-diversity, we show in an experimental evaluation that ℓ-diversity is practical and can be implemented efficiently.",
    "cited_by_count": 3506,
    "openalex_id": "https://openalex.org/W2134167315",
    "type": "article"
  },
  {
    "title": "Graph evolution",
    "doi": "https://doi.org/10.1145/1217299.1217301",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Jure Leskovec; Jon Kleinberg; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "How do real graphs evolve over time? What are normal growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs , identifying properties in a single snapshot of a large network or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time. Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time with the number of edges growing superlinearly in the number of nodes. Second, the average distance between nodes often shrinks over time in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O (log n ) or O (log(log n )). Existing graph generation models do not exhibit these types of behavior even at a qualitative level. We provide a new graph generator, based on a forest fire spreading process that has a simple, intuitive justification, requires very few parameters (like the flammability of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study. We also notice that the forest fire model exhibits a sharp transition between sparse graphs and graphs that are densifying. Graphs with decreasing distance between the nodes are generated around this transition point. Last, we analyze the connection between the temporal evolution of the degree distribution and densification of a graph. We find that the two are fundamentally related. We also observe that real networks exhibit this type of relation between densification and the degree distribution.",
    "cited_by_count": 2337,
    "openalex_id": "https://openalex.org/W2108614537",
    "type": "article"
  },
  {
    "title": "Isolation-Based Anomaly Detection",
    "doi": "https://doi.org/10.1145/2133360.2133363",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Fei Tony Liu; Kai Ming Ting; Zhi‐Hua Zhou",
    "corresponding_authors": "",
    "abstract": "Anomalies are data points that are few and different. As a result of these properties, we show that, anomalies are susceptible to a mechanism called isolation . This article proposes a method called Isolation Forest ( i Forest), which detects anomalies purely based on the concept of isolation without employing any distance or density measure---fundamentally different from all existing methods. As a result, i Forest is able to exploit subsampling (i) to achieve a low linear time-complexity and a small memory-requirement and (ii) to deal with the effects of swamping and masking effectively. Our empirical evaluation shows that i Forest outperforms ORCA, one-class SVM, LOF and Random Forests in terms of AUC, processing time, and it is robust against masking and swamping effects. i Forest also works well in high dimensional problems containing a large number of irrelevant attributes, and when anomalies are not available in training sample.",
    "cited_by_count": 1720,
    "openalex_id": "https://openalex.org/W1995443851",
    "type": "article"
  },
  {
    "title": "Clustering high-dimensional data",
    "doi": "https://doi.org/10.1145/1497577.1497578",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Hans‐Peter Kriegel; Peer Kröger; Arthur Zimek",
    "corresponding_authors": "",
    "abstract": "As a prolific research area in data mining, subspace clustering and related problems induced a vast quantity of proposed solutions. However, many publications compare a new proposition—if at all—with one or two competitors, or even with a so-called “naïve” ad hoc solution, but fail to clarify the exact problem definition. As a consequence, even if two solutions are thoroughly compared experimentally, it will often remain unclear whether both solutions tackle the same problem or, if they do, whether they agree in certain tacit assumptions and how such assumptions may influence the outcome of an algorithm. In this survey, we try to clarify: (i) the different problem definitions related to subspace clustering in general; (ii) the specific difficulties encountered in this field of research; (iii) the varying assumptions, heuristics, and intuitions forming the basis of different approaches; and (iv) how several prominent solutions tackle different problems.",
    "cited_by_count": 1095,
    "openalex_id": "https://openalex.org/W2079361215",
    "type": "article"
  },
  {
    "title": "Clustering aggregation",
    "doi": "https://doi.org/10.1145/1217299.1217303",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Aristides Gionis; Heikki Mannila; Panayiotis Tsaparas",
    "corresponding_authors": "",
    "abstract": "We consider the following problem: given a set of clusterings, find a single clustering that agrees as much as possible with the input clusterings. This problem, clustering aggregation , appears naturally in various contexts. For example, clustering categorical data is an instance of the clustering aggregation problem; each categorical attribute can be viewed as a clustering of the input rows where rows are grouped together if they take the same value on that attribute. Clustering aggregation can also be used as a metaclustering method to improve the robustness of clustering by combining the output of multiple algorithms. Furthermore, the problem formulation does not require a priori information about the number of clusters; it is naturally determined by the optimization function. In this article, we give a formal statement of the clustering aggregation problem, and we propose a number of algorithms. Our algorithms make use of the connection between clustering aggregation and the problem of correlation clustering . Although the problems we consider are NP-hard, for several of our methods, we provide theoretical guarantees on the quality of the solutions. Our work provides the best deterministic approximation algorithm for the variation of the correlation clustering problem we consider. We also show how sampling can be used to scale the algorithms for large datasets. We give an extensive empirical evaluation demonstrating the usefulness of the problem and of the solutions.",
    "cited_by_count": 800,
    "openalex_id": "https://openalex.org/W2295256067",
    "type": "article"
  },
  {
    "title": "Factor in the neighbors",
    "doi": "https://doi.org/10.1145/1644873.1644874",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Yehuda Koren",
    "corresponding_authors": "Yehuda Koren",
    "abstract": "Recommender systems provide users with personalized suggestions for products or services. These systems often rely on collaborating filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The most common approach to CF is based on neighborhood models, which originate from similarities between products or users. In this work we introduce a new neighborhood model with an improved prediction accuracy. Unlike previous approaches that are based on heuristic similarities, we model neighborhood relations by minimizing a global cost function. Further accuracy improvements are achieved by extending the model to exploit both explicit and implicit feedback by the users. Past models were limited by the need to compute all pairwise similarities between items or users, which grow quadratically with input size. In particular, this limitation vastly complicates adopting user similarity models, due to the typical large number of users. Our new model solves these limitations by factoring the neighborhood model, thus making both item-item and user-user implementations scale linearly with the size of the data. The methods are tested on the Netflix data, with encouraging results.",
    "cited_by_count": 720,
    "openalex_id": "https://openalex.org/W2047109571",
    "type": "article"
  },
  {
    "title": "Hierarchical Density Estimates for Data Clustering, Visualization, and Outlier Detection",
    "doi": "https://doi.org/10.1145/2733381",
    "publication_date": "2015-07-22",
    "publication_year": 2015,
    "authors": "Ricardo J. G. B. Campello; Davoud Moulavi; Arthur Zimek; Jörg Sander",
    "corresponding_authors": "",
    "abstract": "An integrated framework for density-based cluster analysis, outlier detection, and data visualization is introduced in this article. The main module consists of an algorithm to compute hierarchical estimates of the level sets of a density, following Hartigan’s classic model of density-contour clusters and trees. Such an algorithm generalizes and improves existing density-based clustering techniques with respect to different aspects. It provides as a result a complete clustering hierarchy composed of all possible density-based clusters following the nonparametric model adopted, for an infinite range of density thresholds. The resulting hierarchy can be easily processed so as to provide multiple ways for data visualization and exploration. It can also be further postprocessed so that: (i) a normalized score of “outlierness” can be assigned to each data object, which unifies both the global and local perspectives of outliers into a single definition; and (ii) a “flat” (i.e., nonhierarchical) clustering solution composed of clusters extracted from local cuts through the cluster tree (possibly corresponding to different density thresholds) can be obtained, either in an unsupervised or in a semisupervised way. In the unsupervised scenario, the algorithm corresponding to this postprocessing module provides a global, optimal solution to the formal problem of maximizing the overall stability of the extracted clusters. If partially labeled objects or instance-level constraints are provided by the user, the algorithm can solve the problem by considering both constraints violations/satisfactions and cluster stability criteria. An asymptotic complexity analysis, both in terms of running time and memory space, is described. Experiments are reported that involve a variety of synthetic and real datasets, including comparisons with state-of-the-art, density-based clustering and (global and local) outlier detection methods.",
    "cited_by_count": 709,
    "openalex_id": "https://openalex.org/W2180566385",
    "type": "article"
  },
  {
    "title": "Collective entity resolution in relational data",
    "doi": "https://doi.org/10.1145/1217299.1217304",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Indrajit Bhattacharya; Lise Getoor",
    "corresponding_authors": "",
    "abstract": "Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution . Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms.",
    "cited_by_count": 584,
    "openalex_id": "https://openalex.org/W2148019918",
    "type": "article"
  },
  {
    "title": "Temporal Link Prediction Using Matrix and Tensor Factorizations",
    "doi": "https://doi.org/10.1145/1921632.1921636",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Daniel Dunlavy; Tamara G. Kolda; Evrim Acar",
    "corresponding_authors": "",
    "abstract": "The data in many disciplines such as social networks, Web analysis, etc. is link-based, and the link structure can be exploited for many different data mining tasks. In this article, we consider the problem of temporal link prediction: Given link data for times 1 through T , can we predict the links at time T + 1? If our data has underlying periodic structure, can we predict out even further in time, i.e., links at time T + 2, T + 3, etc.? In this article, we consider bipartite graphs that evolve over time and consider matrix- and tensor-based methods for predicting future links. We present a weight-based method for collapsing multiyear data into a single matrix. We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data. Through several numerical experiments, we demonstrate that both matrix- and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem. Additionally, we show that tensor-based techniques are particularly effective for temporal data with varying periodic patterns.",
    "cited_by_count": 532,
    "openalex_id": "https://openalex.org/W3122868618",
    "type": "article"
  },
  {
    "title": "Semantic text similarity using corpus-based word similarity and string similarity",
    "doi": "https://doi.org/10.1145/1376815.1376819",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Aminul Islam; Diana Inkpen",
    "corresponding_authors": "",
    "abstract": "We present a method for measuring the semantic similarity of texts using a corpus-based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Existing methods for computing text similarity have focused mainly on either large documents or individual words. We focus on computing the similarity between two sentences or two short paragraphs. The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery. Evaluation results on two different data sets show that our method outperforms several competing methods.",
    "cited_by_count": 484,
    "openalex_id": "https://openalex.org/W2045929671",
    "type": "article"
  },
  {
    "title": "Leakage in data mining",
    "doi": "https://doi.org/10.1145/2382577.2382579",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Shachar Kaufman; Saharon Rosset; Claudia Perlich; Ori Stitelman",
    "corresponding_authors": "",
    "abstract": "Deemed “one of the top ten data mining mistakes”, leakage is the introduction of information about the data mining target that should not be legitimately available to mine from. In addition to our own industry experience with real-life projects, controversies around several major public data mining competitions held recently such as the INFORMS 2010 Data Mining Challenge and the IJCNN 2011 Social Network Challenge are evidence that this issue is as relevant today as it has ever been. While acknowledging the importance and prevalence of leakage in both synthetic competitions and real-life data mining projects, existing literature has largely left this idea unexplored. What little has been said turns out not to be broad enough to cover more complex cases of leakage, such as those where the classical independently and identically distributed (i.i.d.) assumption is violated, that have been recently documented. In our new approach, these cases and others are explained by explicitly defining modeling goals and analyzing the broader framework of the data mining problem. The resulting definition enables us to derive general methodology for dealing with the issue. We show that it is possible to avoid leakage with a simple specific approach to data management followed by what we call a learn-predict separation, and present several ways of detecting leakage when the modeler has no control over how the data have been collected. We also offer an alternative point of view on leakage that is based on causal graph modeling concepts.",
    "cited_by_count": 429,
    "openalex_id": "https://openalex.org/W2736287575",
    "type": "article"
  },
  {
    "title": "Inferring Networks of Diffusion and Influence",
    "doi": "https://doi.org/10.1145/2086737.2086741",
    "publication_date": "2012-01-31",
    "publication_year": 2012,
    "authors": "Manuel Gomez-Rodriguez; Jure Leskovec; Andreas Krause",
    "corresponding_authors": "",
    "abstract": "Information diffusion and virus propagation are fundamental processes taking place in networks. While it is often possible to directly observe when nodes become infected with a virus or publish the information, observing individual transmissions (who infects whom, or who influences whom) is typically very difficult. Furthermore, in many applications, the underlying network over which the diffusions and propagations spread is actually unobserved. We tackle these challenges by developing a method for tracing paths of diffusion and influence through networks and inferring the networks over which contagions propagate. Given the times when nodes adopt pieces of information or become infected, we identify the optimal network that best explains the observed infection times. Since the optimization problem is NP-hard to solve exactly, we develop an efficient approximation algorithm that scales to large datasets and finds provably near-optimal networks. We demonstrate the effectiveness of our approach by tracing information diffusion in a set of 170 million blogs and news articles over a one year period to infer how information flows through the online media space. We find that the diffusion network of news for the top 1,000 media sites and blogs tends to have a core-periphery structure with a small set of core media sites that diffuse information to the rest of the Web. These sites tend to have stable circles of influence with more general news media sites acting as connectors between them.",
    "cited_by_count": 426,
    "openalex_id": "https://openalex.org/W2569283211",
    "type": "article"
  },
  {
    "title": "Knowledge Graph Embedding for Link Prediction",
    "doi": "https://doi.org/10.1145/3424672",
    "publication_date": "2021-01-04",
    "publication_year": 2021,
    "authors": "Andrea Rossi; Denilson Barbosa; Donatella Firmani; Antonio Matinata; Paolo Merialdo",
    "corresponding_authors": "",
    "abstract": "Knowledge Graphs (KGs) have found many applications in industry and academic settings, which in turn, have motivated considerable research efforts towards large-scale information extraction from a variety of sources. Despite such efforts, it is well known that even state-of-the-art KGs suffer from incompleteness. Link Prediction (LP), the task of predicting missing facts among entities already a KG, is a promising and widely studied task aimed at addressing KG incompleteness. Among the recent LP techniques, those based on KG embeddings have achieved very promising performances in some benchmarks. Despite the fast growing literature in the subject, insufficient attention has been paid to the effect of the various design choices in those methods. Moreover, the standard practice in this area is to report accuracy by aggregating over a large number of test facts in which some entities are over-represented; this allows LP methods to exhibit good performance by just attending to structural properties that include such entities, while ignoring the remaining majority of the KG. This analysis provides a comprehensive comparison of embedding-based LP methods, extending the dimensions of analysis beyond what is commonly available in the literature. We experimentally compare effectiveness and efficiency of 16 state-of-the-art methods, consider a rule-based baseline, and report detailed analysis over the most popular benchmarks in the literature.",
    "cited_by_count": 395,
    "openalex_id": "https://openalex.org/W3120491054",
    "type": "article"
  },
  {
    "title": "Discovering social circles in ego networks",
    "doi": "https://doi.org/10.1145/2556612",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Julian McAuley; Jure Leskovec",
    "corresponding_authors": "",
    "abstract": "People's personal social networks are big and cluttered, and currently there is no good way to automatically organize them. Social networking sites allow users to manually categorize their friends into social circles (e.g., “circles” on Google+, and “lists” on Facebook and Twitter). However, circles are laborious to construct and must be manually updated whenever a user's network grows. In this article, we study the novel task of automatically identifying users' social circles. We pose this task as a multimembership node clustering problem on a user's ego network, a network of connections between her friends. We develop a model for detecting circles that combines network structure as well as user profile information. For each circle, we learn its members and the circle-specific user profile similarity metric. Modeling node membership to multiple circles allows us to detect overlapping as well as hierarchically nested circles. Experiments show that our model accurately identifies circles on a diverse set of data from Facebook, Google+, and Twitter, for all of which we obtain hand-labeled ground truth.",
    "cited_by_count": 358,
    "openalex_id": "https://openalex.org/W2158908968",
    "type": "article"
  },
  {
    "title": "A Survey on Causal Inference",
    "doi": "https://doi.org/10.1145/3444944",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Liuyi Yao; Zhixuan Chu; Sheng Li; Yaliang Li; Jing Gao; Aidong Zhang",
    "corresponding_authors": "",
    "abstract": "Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine, and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.",
    "cited_by_count": 358,
    "openalex_id": "https://openalex.org/W3160537436",
    "type": "article"
  },
  {
    "title": "Multilabel dimensionality reduction via dependence maximization",
    "doi": "https://doi.org/10.1145/1839490.1839495",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Yin Zhang; Zhi‐Hua Zhou",
    "corresponding_authors": "",
    "abstract": "Multilabel learning deals with data associated with multiple labels simultaneously. Like other data mining and machine learning tasks, multilabel learning also suffers from the curse of dimensionality . Dimensionality reduction has been studied for many years, however, multilabel dimensionality reduction remains almost untouched. In this article, we propose a multilabel dimensionality reduction method, MDDM, with two kinds of projection strategies, attempting to project the original data into a lower-dimensional feature space maximizing the dependence between the original feature description and the associated class labels. Based on the Hilbert-Schmidt Independence Criterion, we derive a eigen-decomposition problem which enables the dimensionality reduction process to be efficient. Experiments validate the performance of MDDM.",
    "cited_by_count": 350,
    "openalex_id": "https://openalex.org/W1972490990",
    "type": "article"
  },
  {
    "title": "Self-Adaptive Particle Swarm Optimization for Large-Scale Feature Selection in Classification",
    "doi": "https://doi.org/10.1145/3340848",
    "publication_date": "2019-09-24",
    "publication_year": 2019,
    "authors": "Yu Xue; Bing Xue; Jun Zhang",
    "corresponding_authors": "",
    "abstract": "Many evolutionary computation (EC) methods have been used to solve feature selection problems and they perform well on most small-scale feature selection problems. However, as the dimensionality of feature selection problems increases, the solution space increases exponentially. Meanwhile, there are more irrelevant features than relevant features in datasets, which leads to many local optima in the huge solution space. Therefore, the existing EC methods still suffer from the problem of stagnation in local optima on large-scale feature selection problems. Furthermore, large-scale feature selection problems with different datasets may have different properties. Thus, it may be of low performance to solve different large-scale feature selection problems with an existing EC method that has only one candidate solution generation strategy (CSGS). In addition, it is time-consuming to find a suitable EC method and corresponding suitable parameter values for a given large-scale feature selection problem if we want to solve it effectively and efficiently. In this article, we propose a self-adaptive particle swarm optimization (SaPSO) algorithm for feature selection, particularly for large-scale feature selection. First, an encoding scheme for the feature selection problem is employed in the SaPSO. Second, three important issues related to self-adaptive algorithms are investigated. After that, the SaPSO algorithm with a typical self-adaptive mechanism is proposed. The experimental results on 12 datasets show that the solution size obtained by the SaPSO algorithm is smaller than its EC counterparts on all datasets. The SaPSO algorithm performs better than its non-EC and EC counterparts in terms of classification accuracy not only on most training sets but also on most test sets. Furthermore, as the dimensionality of the feature selection problem increases, the advantages of SaPSO become more prominent. This highlights that the SaPSO algorithm is suitable for solving feature selection problems, particularly large-scale feature selection problems.",
    "cited_by_count": 314,
    "openalex_id": "https://openalex.org/W2997585558",
    "type": "article"
  },
  {
    "title": "Dynamic Graph Convolutional Recurrent Network for Traffic Prediction: Benchmark and Solution",
    "doi": "https://doi.org/10.1145/3532611",
    "publication_date": "2022-05-17",
    "publication_year": 2022,
    "authors": "Fuxian Li; Jie Feng; Huan Yan; Guangyin Jin; Fan Yang; Funing Sun; Depeng Jin; Yong Li",
    "corresponding_authors": "",
    "abstract": "Traffic prediction is the cornerstone of intelligent transportation system. Accurate traffic forecasting is essential for the applications of smart cities, i.e., intelligent traffic management and urban planning. Although various methods are proposed for spatio-temporal modeling, they ignore the dynamic characteristics of correlations among locations on road network. Meanwhile, most Recurrent Neural Network based works are not efficient enough due to their recurrent operations. Additionally, there is a severe lack of fair comparison among different methods on the same datasets. To address the above challenges, in this article, we propose a novel traffic prediction framework, named Dynamic Graph Convolutional Recurrent Network (DGCRN). In DGCRN, hyper-networks are designed to leverage and extract dynamic characteristics from node attributes, while the parameters of dynamic filters are generated at each time step. We filter the node embeddings and then use them to generate dynamic graph, which is integrated with pre-defined static graph. As far as we know, we are first to employ a generation method to model fine topology of dynamic graph at each time step. Furthermore, to enhance efficiency and performance, we employ a training strategy for DGCRN by restricting the iteration number of decoder during forward and backward propagation. Finally, a reproducible standardized benchmark and a brand new representative traffic dataset are opened for fair comparison and further research. Extensive experiments on three datasets demonstrate that our model outperforms 15 baselines consistently. Source codes are available at https://github.com/tsinghua-fib-lab/Traffic-Benchmark .",
    "cited_by_count": 308,
    "openalex_id": "https://openalex.org/W3158304688",
    "type": "article"
  },
  {
    "title": "Outcome-Oriented Predictive Process Monitoring",
    "doi": "https://doi.org/10.1145/3301300",
    "publication_date": "2019-03-13",
    "publication_year": 2019,
    "authors": "Irene Teinemaa; Marlon Dumas; Marcello La Rosa; Fabrizio Maria Maggi",
    "corresponding_authors": "",
    "abstract": "Predictive business process monitoring refers to the act of making predictions about the future state of ongoing cases of a business process, based on their incomplete execution traces and logs of historical (completed) traces. Motivated by the increasingly pervasive availability of fine-grained event data about business process executions, the problem of predictive process monitoring has received substantial attention in the past years. In particular, a considerable number of methods have been put forward to address the problem of outcome-oriented predictive process monitoring, which refers to classifying each ongoing case of a process according to a given set of possible categorical outcomes—e.g., Will the customer complain or not? Will an order be delivered, canceled, or withdrawn? Unfortunately, different authors have used different datasets, experimental settings, evaluation measures, and baselines to assess their proposals, resulting in poor comparability and an unclear picture of the relative merits and applicability of different methods. To address this gap, this article presents a systematic review and taxonomy of outcome-oriented predictive process monitoring methods, and a comparative experimental evaluation of eleven representative methods using a benchmark covering 24 predictive process monitoring tasks based on nine real-life event logs.",
    "cited_by_count": 299,
    "openalex_id": "https://openalex.org/W2964066696",
    "type": "article"
  },
  {
    "title": "Analyzing communities and their evolutions in dynamic social networks",
    "doi": "https://doi.org/10.1145/1514888.1514891",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Yu‐Ru Lin; Yün Chi; Shenghuo Zhu; Hari Sundaram; Belle L. Tseng",
    "corresponding_authors": "",
    "abstract": "We discover communities from social network data and analyze the community evolution. These communities are inherent characteristics of human interaction in online social networks, as well as paper citation networks. Also, communities may evolve over time, due to changes to individuals' roles and social status in the network as well as changes to individuals' research interests. We present an innovative algorithm that deviates from the traditional two-step approach to analyze community evolutions. In the traditional approach, communities are first detected for each time slice, and then compared to determine correspondences. We argue that this approach is inappropriate in applications with noisy data. In this paper, we propose FacetNet for analyzing communities and their evolutions through a robust unified process. This novel framework will discover communities and capture their evolution with temporal smoothness given by historic community structures. Our approach relies on formulating the problem in terms of maximum a posteriori (MAP) estimation, where the community structure is estimated both by the observed networked data and by the prior distribution given by historic community structures. Then we develop an iterative algorithm, with proven low time complexity, which is guaranteed to converge to an optimal solution. We perform extensive experimental studies, on both synthetic datasets and real datasets, to demonstrate that our method discovers meaningful communities and provides additional insights not directly obtainable from traditional methods.",
    "cited_by_count": 293,
    "openalex_id": "https://openalex.org/W2007516075",
    "type": "article"
  },
  {
    "title": "An event-based framework for characterizing the evolutionary behavior of interaction graphs",
    "doi": "https://doi.org/10.1145/1631162.1631164",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Sitaram Asur; Srinivasan Parthasarathy; Duygu Ucar",
    "corresponding_authors": "",
    "abstract": "Interaction graphs are ubiquitous in many fields such as bioinformatics, sociology and physical sciences. There have been many studies in the literature targeted at studying and mining these graphs. However, almost all of them have studied these graphs from a static point of view. The study of the evolution of these graphs over time can provide tremendous insight on the behavior of entities, communities and the flow of information among them. In this work, we present an event-based characterization of critical behavioral patterns for temporally varying interaction graphs. We use nonoverlapping snapshots of interaction graphs and develop a framework for capturing and identifying interesting events from them. We use these events to characterize complex behavioral patterns of individuals and communities over time. We show how semantic information can be incorporated to reason about community-behavior events. We also demonstrate the application of behavioral patterns for the purposes of modeling evolution, link prediction and influence maximization. Finally, we present a diffusion model for evolving networks, based on our framework.",
    "cited_by_count": 291,
    "openalex_id": "https://openalex.org/W2097876158",
    "type": "article"
  },
  {
    "title": "Author name disambiguation in MEDLINE",
    "doi": "https://doi.org/10.1145/1552303.1552304",
    "publication_date": "2009-07-01",
    "publication_year": 2009,
    "authors": "Vetle I. Torvik; Neil R. Smalheiser",
    "corresponding_authors": "",
    "abstract": "BACKGROUND: We recently described \"Author-ity,\" a model for estimating the probability that two articles in MEDLINE, sharing the same author name, were written by the same individual. Features include shared title words, journal name, coauthors, medical subject headings, language, affiliations, and author name features (middle initial, suffix, and prevalence in MEDLINE). Here we test the hypothesis that the Author-ity model will suffice to disambiguate author names for the vast majority of articles in MEDLINE. METHODS: Enhancements include: (a) incorporating first names and their variants, email addresses, and correlations between specific last names and affiliation words; (b) new methods of generating large unbiased training sets; (c) new methods for estimating the prior probability; (d) a weighted least squares algorithm for correcting transitivity violations; and (e) a maximum likelihood based agglomerative algorithm for computing clusters of articles that represent inferred author-individuals. RESULTS: Pairwise comparisons were computed for all author names on all 15.3 million articles in MEDLINE (2006 baseline), that share last name and first initial, to create Author-ity 2006, a database that has each name on each article assigned to one of 6.7 million inferred author-individual clusters. Recall is estimated at ~98.8%. Lumping (putting two different individuals into the same cluster) affects ~0.5% of clusters, whereas splitting (assigning articles written by the same individual to >1 cluster) affects ~2% of articles. IMPACT: The Author-ity model can be applied generally to other bibliographic databases. Author name disambiguation allows information retrieval and data integration to become person-centered, not just document-centered, setting the stage for new data mining and social network tools that will facilitate the analysis of scholarly publishing and collaboration behavior. AVAILABILITY: The Author-ity 2006 database is available for nonprofit academic research, and can be freely queried via http://arrowsmith.psych.uic.edu.",
    "cited_by_count": 287,
    "openalex_id": "https://openalex.org/W1964879903",
    "type": "article"
  },
  {
    "title": "Uncovering social network Sybils in the wild",
    "doi": "https://doi.org/10.1145/2556609",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Zhi Yang; Christo Wilson; Xiao Wang; Tingting Gao; Ben Y. Zhao; Yafei Dai",
    "corresponding_authors": "",
    "abstract": "Sybil accounts are fake identities created to unfairly increase the power or resources of a single malicious user. Researchers have long known about the existence of Sybil accounts in online communities such as file-sharing systems, but they have not been able to perform large-scale measurements to detect them or measure their activities. In this article, we describe our efforts to detect, characterize, and understand Sybil account activity in the Renren Online Social Network (OSN). We use ground truth provided by Renren Inc. to build measurement-based Sybil detectors and deploy them on Renren to detect more than 100,000 Sybil accounts. Using our full dataset of 650,000 Sybils, we examine several aspects of Sybil behavior. First, we study their link creation behavior and find that contrary to prior conjecture, Sybils in OSNs do not form tight-knit communities. Next, we examine the fine-grained behaviors of Sybils on Renren using clickstream data. Third, we investigate behind-the-scenes collusion between large groups of Sybils. Our results reveal that Sybils with no explicit social ties still act in concert to launch attacks. Finally, we investigate enhanced techniques to identify stealthy Sybils. In summary, our study advances the understanding of Sybil behavior on OSNs and shows that Sybils can effectively avoid existing community-based Sybil detectors. We hope that our results will foster new research on Sybil detection that is based on novel types of Sybil features.",
    "cited_by_count": 287,
    "openalex_id": "https://openalex.org/W2092277251",
    "type": "article"
  },
  {
    "title": "Time Series Classification with HIVE-COTE",
    "doi": "https://doi.org/10.1145/3182382",
    "publication_date": "2018-07-05",
    "publication_year": 2018,
    "authors": "Jason Lines; Sarah Taylor; Anthony Bagnall",
    "corresponding_authors": "",
    "abstract": "A recent experimental evaluation assessed 19 time series classification (TSC) algorithms and found that one was significantly more accurate than all others: the Flat Collective of Transformation-based Ensembles (Flat-COTE). Flat-COTE is an ensemble that combines 35 classifiers over four data representations. However, while comprehensive, the evaluation did not consider deep learning approaches. Convolutional neural networks (CNN) have seen a surge in popularity and are now state of the art in many fields and raises the question of whether CNNs could be equally transformative for TSC. We implement a benchmark CNN for TSC using a common structure and use results from a TSC-specific CNN from the literature. We compare both to Flat-COTE and find that the collective is significantly more accurate than both CNNs. These results are impressive, but Flat-COTE is not without deficiencies. We significantly improve the collective by proposing a new hierarchical structure with probabilistic voting, defining and including two novel ensemble classifiers built in existing feature spaces, and adding further modules to represent two additional transformation domains. The resulting classifier, the Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE), encapsulates classifiers built on five data representations. We demonstrate that HIVE-COTE is significantly more accurate than Flat-COTE (and all other TSC algorithms that we are aware of) over 100 resamples of 85 TSC problems and is the new state of the art for TSC. Further analysis is included through the introduction and evaluation of 3 new case studies and extensive experimentation on 1,000 simulated datasets of 5 different types.",
    "cited_by_count": 287,
    "openalex_id": "https://openalex.org/W2786161686",
    "type": "article"
  },
  {
    "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
    "doi": "https://doi.org/10.1145/3649506",
    "publication_date": "2024-02-28",
    "publication_year": 2024,
    "authors": "Jingfeng Yang; Hongye Jin; Ruixiang Tang; Xiaotian Han; Qizhang Feng; Haoming Jiang; Shaochen Zhong; Bing Yin; Xia Hu",
    "corresponding_authors": "",
    "abstract": "This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at https://github.com/Mooler0410/LLMsPracticalGuide . An LLMs evolutionary tree, editable yet regularly updated, can be found at llmtree.ai .",
    "cited_by_count": 270,
    "openalex_id": "https://openalex.org/W4392240262",
    "type": "article"
  },
  {
    "title": "A Survey of Parallel Sequential Pattern Mining",
    "doi": "https://doi.org/10.1145/3314107",
    "publication_date": "2019-06-07",
    "publication_year": 2019,
    "authors": "Wensheng Gan; Jerry Chun‐Wei Lin; Philippe Fournier‐Viger; Han‐Chieh Chao; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "With the growing popularity of shared resources, large volumes of complex data of different types are collected automatically. Traditional data mining algorithms generally have problems and challenges including huge memory cost, low processing speed, and inadequate hard disk space. As a fundamental task of data mining, sequential pattern mining (SPM) is used in a wide variety of real-life applications. However, it is more complex and challenging than other pattern mining tasks, i.e., frequent itemset mining and association rule mining, and also suffers from the above challenges when handling the large-scale data. To solve these problems, mining sequential patterns in a parallel or distributed computing environment has emerged as an important issue with many applications. In this paper, an in-depth survey of the current status of parallel sequential pattern mining (PSPM) is investigated and provided, including detailed categorization of traditional serial SPM approaches, and state of the art parallel SPM. We review the related work of parallel sequential pattern mining in detail, including partition-based algorithms for PSPM, Apriori-based PSPM, pattern growth based PSPM, and hybrid algorithms for PSPM, and provide deep description (i.e., characteristics, advantages, disadvantages and summarization) of these parallel approaches of PSPM. Some advanced topics for PSPM, including parallel quantitative / weighted / utility sequential pattern mining, PSPM from uncertain data and stream data, hardware acceleration for PSPM, are further reviewed in details. Besides, we review and provide some well-known open-source software of PSPM. Finally, we summarize some challenges and opportunities of PSPM in the big data era.",
    "cited_by_count": 252,
    "openalex_id": "https://openalex.org/W3102927519",
    "type": "article"
  },
  {
    "title": "Attention Models in Graphs",
    "doi": "https://doi.org/10.1145/3363574",
    "publication_date": "2019-11-11",
    "publication_year": 2019,
    "authors": "John Boaz Lee; Ryan A. Rossi; Sungchul Kim; Nesreen K. Ahmed; Eunyee Koh",
    "corresponding_authors": "",
    "abstract": "Graph-structured data arise naturally in many different application domains. By representing data as graphs, we can capture entities (i.e., nodes) as well as their relationships (i.e., edges) with each other. Many useful insights can be derived from graph-structured data as demonstrated by an ever-growing body of work focused on graph mining. However, in the real-world, graphs can be both large—with many complex patterns—and noisy, which can pose a problem for effective graph mining. An effective way to deal with this issue is to incorporate “attention” into graph mining solutions. An attention mechanism allows a method to focus on task-relevant parts of the graph, helping it to make better decisions. In this work, we conduct a comprehensive and focused survey of the literature on the emerging field of graph attention models. We introduce three intuitive taxonomies to group existing work. These are based on problem setting (type of input and output), the type of attention mechanism used, and the task (e.g., graph classification, link prediction). We motivate our taxonomies through detailed examples and use each to survey competing approaches from a unique standpoint. Finally, we highlight several challenges in the area and discuss promising directions for future work.",
    "cited_by_count": 227,
    "openalex_id": "https://openalex.org/W2987119394",
    "type": "article"
  },
  {
    "title": "Robust Manifold Nonnegative Matrix Factorization",
    "doi": "https://doi.org/10.1145/2601434",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Jin Huang; Feiping Nie; Heng Huang; Chris Ding",
    "corresponding_authors": "",
    "abstract": "Nonnegative Matrix Factorization (NMF) has been one of the most widely used clustering techniques for exploratory data analysis. However, since each data point enters the objective function with squared residue error, a few outliers with large errors easily dominate the objective function. In this article, we propose a Robust Manifold Nonnegative Matrix Factorization (RMNMF) method using ℓ 2,1 -norm and integrating NMF and spectral clustering under the same clustering framework. We also point out the solution uniqueness issue for the existing NMF methods and propose an additional orthonormal constraint to address this problem. With the new constraint, the conventional auxiliary function approach no longer works. We tackle this difficult optimization problem via a novel Augmented Lagrangian Method (ALM)--based algorithm and convert the original constrained optimization problem on one variable into a multivariate constrained problem. The new objective function then can be decomposed into several subproblems that each has a closed-form solution. More importantly, we reveal the connection of our method with robust K -means and spectral clustering, and we demonstrate its theoretical significance. Extensive experiments have been conducted on nine benchmark datasets, and all empirical results show the effectiveness of our method.",
    "cited_by_count": 223,
    "openalex_id": "https://openalex.org/W2162316550",
    "type": "article"
  },
  {
    "title": "Network Sampling",
    "doi": "https://doi.org/10.1145/2601438",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Nesreen K. Ahmed; Jennifer Neville; Ramana Rao Kompella",
    "corresponding_authors": "",
    "abstract": "Network sampling is integral to the analysis of social, information, and biological networks. Since many real-world networks are massive in size, continuously evolving, and/or distributed in nature, the network structure is often sampled in order to facilitate study. For these reasons, a more thorough and complete understanding of network sampling is critical to support the field of network science. In this paper, we outline a framework for the general problem of network sampling by highlighting the different objectives, population and units of interest, and classes of network sampling methods. In addition, we propose a spectrum of computational models for network sampling methods, ranging from the traditionally studied model based on the assumption of a static domain to a more challenging model that is appropriate for streaming domains. We design a family of sampling methods based on the concept of graph induction that generalize across the full spectrum of computational models (from static to streaming) while efficiently preserving many of the topological properties of the input graphs. Furthermore, we demonstrate how traditional static sampling algorithms can be modified for graph streams for each of the three main classes of sampling methods: node, edge, and topology-based sampling. Experimental results indicate that our proposed family of sampling methods more accurately preserve the underlying properties of the graph in both static and streaming domains. Finally, we study the impact of network sampling algorithms on the parameter estimation and performance evaluation of relational classification algorithms.",
    "cited_by_count": 219,
    "openalex_id": "https://openalex.org/W2963316155",
    "type": "article"
  },
  {
    "title": "Addressing Big Data Time Series",
    "doi": "https://doi.org/10.1145/2500489",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Thanawin Rakthanmanon; Bilson Campana; Abdullah Mueen; Gustavo Batista; Brandon Westover; Qiang Zhu; Jesin Zakaria; Eamonn Keogh",
    "corresponding_authors": "",
    "abstract": "Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms, including classification, clustering, motif discovery, anomaly detection, and so on. The difficulty of scaling a search to large datasets explains to a great extent why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine massive time series for the first time. We demonstrate the following unintuitive fact: in large datasets we can exactly search under Dynamic Time Warping (DTW) much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. In particular, the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining papers ever published. We explain how our ideas allow us to solve higher-level time series data mining problems such as motif discovery and clustering at scales that would otherwise be untenable. Moreover, we show how our ideas allow us to efficiently support the uniform scaling distance measure, a measure whose utility seems to be underappreciated, but which we demonstrate here. In addition to mining massive datasets with up to one trillion datapoints, we will show that our ideas also have implications for real-time monitoring of data streams, allowing us to handle much faster arrival rates and/or use cheaper and lower powered devices than are currently possible.",
    "cited_by_count": 207,
    "openalex_id": "https://openalex.org/W2012609801",
    "type": "article"
  },
  {
    "title": "Tensor Completion Algorithms in Big Data Analytics",
    "doi": "https://doi.org/10.1145/3278607",
    "publication_date": "2019-01-09",
    "publication_year": 2019,
    "authors": "Qingquan Song; Hancheng Ge; James Caverlee; Xia Hu",
    "corresponding_authors": "",
    "abstract": "Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.",
    "cited_by_count": 200,
    "openalex_id": "https://openalex.org/W2963472624",
    "type": "article"
  },
  {
    "title": "Who Influenced You? Predicting Retweet via Social Influence Locality",
    "doi": "https://doi.org/10.1145/2700398",
    "publication_date": "2015-04-01",
    "publication_year": 2015,
    "authors": "Jing Zhang; Jie Tang; Juanzi Li; Yang Liu; Chunxiao Xing",
    "corresponding_authors": "",
    "abstract": "Social influence occurs when one’s opinions, emotions, or behaviors are affected by others in a social network. However, social influence takes many forms, and its underlying mechanism is still unclear. For example, how is one’s behavior influenced by a group of friends who know each other and by the friends from different ego friend circles? In this article, we study the social influence problem in a large microblogging network. Particularly, we consider users’ (re)tweet behaviors and focus on investigating how friends in one’s ego network influence retweet behaviors. We propose a novel notion of social influence locality and develop two instantiation functions based on pairwise influence and structural diversity. The defined influence locality functions have strong predictive power. Without any additional features, we can obtain an F1-score of 71.65% for predicting users’ retweet behaviors by training a logistic regression classifier based on the defined influence locality functions. We incorporate social influence locality into a factor graph model, which can further leverage the network-based correlation. Our experiments on the large microblogging network show that the model significantly improves the precision of retweet prediction. Our analysis also reveals several intriguing discoveries. For example, if you have six friends retweeting a microblog, the average likelihood that you will also retweet it strongly depends on the structure among the six friends: The likelihood will significantly drop (only ⅙) when the six friends do not know each other, compared with the case when the six friends know each other.",
    "cited_by_count": 194,
    "openalex_id": "https://openalex.org/W2094948131",
    "type": "article"
  },
  {
    "title": "Citywide Traffic Flow Prediction Based on Multiple Gated Spatio-temporal Convolutional Neural Networks",
    "doi": "https://doi.org/10.1145/3385414",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Cen Chen; Kenli Li; Sin G. Teo; Xiaofeng Zou; Keqin Li; Zeng Zeng",
    "corresponding_authors": "",
    "abstract": "Traffic flow prediction is crucial for public safety and traffic management, and remains a big challenge because of many complicated factors, e.g., multiple spatio-temporal dependencies, holidays, and weather. Some work leveraged 2D convolutional neural networks (CNNs) and long short-term memory networks (LSTMs) to explore spatial relations and temporal relations, respectively, which outperformed the classical approaches. However, it is hard for these work to model spatio-temporal relations jointly. To tackle this, some studies utilized LSTMs to connect high-level layers of CNNs, but left the spatio-temporal correlations not fully exploited in low-level layers. In this work, we propose novel spatio-temporal CNNs to extract spatio-temporal features simultaneously from low-level to high-level layers, and propose a novel gated scheme to control the spatio-temporal features that should be propagated through the hierarchy of layers. Based on these, we propose an end-to-end framework, multiple gated spatio-temporal CNNs (MGSTC), for citywide traffic flow prediction. MGSTC can explore multiple spatio-temporal dependencies through multiple gated spatio-temporal CNN branches, and combine the spatio-temporal features with external factors dynamically. Extensive experiments on two real traffic datasets demonstrates that MGSTC outperforms other state-of-the-art baselines.",
    "cited_by_count": 193,
    "openalex_id": "https://openalex.org/W3033688252",
    "type": "article"
  },
  {
    "title": "Rumor Gauge",
    "doi": "https://doi.org/10.1145/3070644",
    "publication_date": "2017-07-14",
    "publication_year": 2017,
    "authors": "Soroush Vosoughi; Mostafa Mohsenvand; Deb Roy",
    "corresponding_authors": "",
    "abstract": "The spread of malicious or accidental misinformation in social media, especially in time-sensitive situations, such as real-world emergencies, can have harmful effects on individuals and society. In this work, we developed models for automated verification of rumors (unverified information) that propagate through Twitter. To predict the veracity of rumors, we identified salient features of rumors by examining three aspects of information spread: linguistic style used to express rumors, characteristics of people involved in propagating information, and network propagation dynamics. The predicted veracity of a time series of these features extracted from a rumor (a collection of tweets) is generated using Hidden Markov Models. The verification algorithm was trained and tested on 209 rumors representing 938,806 tweets collected from real-world events, including the 2013 Boston Marathon bombings, the 2014 Ferguson unrest, and the 2014 Ebola epidemic, and many other rumors about various real-world events reported on popular websites that document public rumors. The algorithm was able to correctly predict the veracity of 75% of the rumors faster than any other public source, including journalists and law enforcement officials. The ability to track rumors and predict their outcomes may have practical applications for news consumers, financial markets, journalists, and emergency services, and more generally to help minimize the impact of false information on Twitter.",
    "cited_by_count": 180,
    "openalex_id": "https://openalex.org/W2735898654",
    "type": "article"
  },
  {
    "title": "A Survey on Deep Hashing Methods",
    "doi": "https://doi.org/10.1145/3532624",
    "publication_date": "2022-04-27",
    "publication_year": 2022,
    "authors": "Xiao Luo; Haixin Wang; Daqing Wu; Chong Chen; Minghua Deng; Jianqiang Huang; Xian‐Sheng Hua",
    "corresponding_authors": "",
    "abstract": "Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W3009806461",
    "type": "article"
  },
  {
    "title": "Multi-Source and Multi-modal Deep Network Embedding for Cross-Network Node Classification",
    "doi": "https://doi.org/10.1145/3653304",
    "publication_date": "2024-03-20",
    "publication_year": 2024,
    "authors": "Hongwei Yang; Hui He; Weizhe Zhang; Yan Wang; Jing Lin",
    "corresponding_authors": "",
    "abstract": "In recent years, to address the issue of networked data sparsity in node classification tasks, cross-network node classification (CNNC) leverages the richer information from a source network to enhance the performance of node classification in the target network, which typically has sparser information. However, in real-world applications, labeled nodes may be collected from multiple sources with multiple modalities (e.g., text, vision, and video). Naive application of single-source and single-modal CNNC methods may result in sub-optimal solutions. To this end, in this article, we propose a model called Multi-source and Multi-modal Cross-network Deep Network Embedding (M 2 CDNE) for cross-network node classification. In M 2 CDNE, we propose a deep multi-modal network embedding approach that combines the extracted deep multi-modal features to make the node vector representations network invariant. In addition, we apply dynamic adversarial adaptation to assess the significance of marginal and conditional probability distributions between each source and target network to make node vector representations label discriminative. Furthermore, we devise to classify nodes in the target network through the related source classifier and aggregate different predictions utilizing respective network weights, corresponding to the discrepancy between each source and target network. Extensive experiments performed on real-world datasets demonstrate that the proposed M 2 CDNE significantly outperforms the state-of-the-art approaches.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W4393001808",
    "type": "article"
  },
  {
    "title": "A Survey on Explainable Anomaly Detection",
    "doi": "https://doi.org/10.1145/3609333",
    "publication_date": "2023-07-15",
    "publication_year": 2023,
    "authors": "Zhong Li; Yuxuan Zhu; Matthijs van Leeuwen",
    "corresponding_authors": "",
    "abstract": "In the past two decades, most research on anomaly detection has focused on improving the accuracy of the detection, while largely ignoring the explainability of the corresponding methods and thus leaving the explanation of outcomes to practitioners. As anomaly detection algorithms are increasingly used in safety-critical domains, providing explanations for the high-stakes decisions made in those domains has become an ethical and regulatory requirement. Therefore, this work provides a comprehensive and structured survey on state-of-the-art explainable anomaly detection techniques. We propose a taxonomy based on the main aspects that characterise each explainable anomaly detection technique, aiming to help practitioners and researchers find the explainable anomaly detection method that best suits their needs.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W4384407578",
    "type": "article"
  },
  {
    "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
    "doi": "https://doi.org/10.1145/3715964",
    "publication_date": "2025-01-31",
    "publication_year": 2025,
    "authors": "Teo Sušnjak; P. Hwang; Napoleon H. Reyes; Andre L. C. Barczak; Timothy R. McIntosh; Surangika Ranathunga",
    "corresponding_authors": "",
    "abstract": "This research pioneers the use of fine-tuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies. Our study employed advanced fine-tuning methodologies on open-sourced LLMs, applying textual data mining techniques to automate the knowledge discovery and synthesis phases of an SLR process, thus demonstrating a practical and efficient approach for extracting and analyzing high-quality information from large academic datasets. The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR. Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. The findings ultimately confirmed the potential of fine-tuned LLMs in streamlining various labour-intensive processes of conducting literature reviews. As a scalable proof-of-concept, this study highlights the broad applicability of our approach across multiple research domains. The potential demonstrated here advocates for updates to PRISMA reporting guidelines, incorporating AI-driven processes to ensure methodological transparency and reliability in future SLRs. This study broadens the appeal of AI-enhanced tools across various academic and research fields, demonstrating how to conduct comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies while maintaining high standards.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4407028609",
    "type": "article"
  },
  {
    "title": "Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis",
    "doi": "https://doi.org/10.1145/3726871",
    "publication_date": "2025-03-28",
    "publication_year": 2025,
    "authors": "Lanling Xu; Junjie Zhang; B. Li; Jinpeng Wang; Sheng Chen; Wayne Xin Zhao; Ji-Rong Wen",
    "corresponding_authors": "",
    "abstract": "Recently, Large Language Models (LLMs) such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompt engineering. We propose a general framework for leveraging LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, i.e., task descriptions, user interest modeling, candidate items construction and prompting strategies. In each section, we first define and categorize concepts in line with the existing literature. Then, we propose inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on recommendation performance. Based on our empirical analysis, we finally summarize promising directions to shed lights on future research.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4408929686",
    "type": "article"
  },
  {
    "title": "Incremental tensor analysis",
    "doi": "https://doi.org/10.1145/1409620.1409621",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Jimeng Sun; Dacheng Tao; Spiros Papadimitriou; Philip S. Yu; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "How do we find patterns in author-keyword associations, evolving over time? Or in data cubes (tensors), with product-branchcustomer sales information? And more generally, how to summarize high-order data cubes (tensors)? How to incrementally update these patterns over time? Matrix decompositions, like principal component analysis (PCA) and variants, are invaluable tools for mining, dimensionality reduction, feature selection, rule identification in numerous settings like streaming data, text, graphs, social networks, and many more settings. However, they have only two orders (i.e., matrices, like author and keyword in the previous example). We propose to envision such higher-order data as tensors, and tap the vast literature on the topic. However, these methods do not necessarily scale up, let alone operate on semi-infinite streams. Thus, we introduce a general framework, incremental tensor analysis (ITA), which efficiently computes a compact summary for high-order and high-dimensional data, and also reveals the hidden correlations. Three variants of ITA are presented: (1) dynamic tensor analysis (DTA); (2) streaming tensor analysis (STA); and (3) window-based tensor analysis (WTA). In paricular, we explore several fundamental design trade-offs such as space efficiency, computational cost, approximation accuracy, time dependency, and model complexity. We implement all our methods and apply them in several real settings, such as network anomaly detection, multiway latent semantic indexing on citation networks, and correlation study on sensor measurements. Our empirical studies show that the proposed methods are fast and accurate and that they find interesting patterns and outliers on the real datasets.",
    "cited_by_count": 256,
    "openalex_id": "https://openalex.org/W1993589664",
    "type": "article"
  },
  {
    "title": "Assessing data mining results via swap randomization",
    "doi": "https://doi.org/10.1145/1297332.1297338",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Aristides Gionis; Heikki Mannila; Taneli Mielikäinen; Panayiotis Tsaparas",
    "corresponding_authors": "",
    "abstract": "The problem of assessing the significance of data mining results on high-dimensional 0--1 datasets has been studied extensively in the literature. For problems such as mining frequent sets and finding correlations, significance testing can be done by standard statistical tests such as chi-square, or other methods. However, the results of such tests depend only on the specific attributes and not on the dataset as a whole. Moreover, the tests are difficult to apply to sets of patterns or other complex results of data mining algorithms. In this article, we consider a simple randomization technique that deals with this shortcoming. The approach consists of producing random datasets that have the same row and column margins as the given dataset, computing the results of interest on the randomized instances and comparing them to the results on the actual data. This randomization technique can be used to assess the results of many different types of data mining algorithms, such as frequent sets, clustering, and spectral analysis. To generate random datasets with given margins, we use variations of a Markov chain approach which is based on a simple swap operation. We give theoretical results on the efficiency of different randomization methods, and apply the swap randomization method to several well-known datasets. Our results indicate that for some datasets the structure discovered by the data mining algorithms is expected, given the row and column margins of the datasets, while for other datasets the discovered structure conveys information that is not captured by the margin counts.",
    "cited_by_count": 220,
    "openalex_id": "https://openalex.org/W1978036582",
    "type": "article"
  },
  {
    "title": "Blocking links to minimize contamination spread in a social network",
    "doi": "https://doi.org/10.1145/1514888.1514892",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Masahiro Kimura; Kazumi Saito; Hiroshi Motoda",
    "corresponding_authors": "",
    "abstract": "We address the problem of minimizing the propagation of undesirable things, such as computer viruses or malicious rumors, by blocking a limited number of links in a network, which is converse to the influence maximization problem in which the most influential nodes for information diffusion is searched in a social network. This minimization problem is more fundamental than the problem of preventing the spread of contamination by removing nodes in a network. We introduce two definitions for the contamination degree of a network, accordingly define two contamination minimization problems, and propose methods for efficiently finding good approximate solutions to these problems on the basis of a naturally greedy strategy. Using large social networks, we experimentally demonstrate that the proposed methods outperform conventional link-removal methods. We also show that unlike the case of blocking a limited number of nodes, the strategy of removing nodes with high out-degrees is not necessarily effective for these problems.",
    "cited_by_count": 207,
    "openalex_id": "https://openalex.org/W2030701825",
    "type": "article"
  },
  {
    "title": "Weighted cluster ensembles",
    "doi": "https://doi.org/10.1145/1460797.1460800",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Carlotta Domeniconi; Muna Al‐Razgan",
    "corresponding_authors": "",
    "abstract": "Cluster ensembles offer a solution to challenges inherent to clustering arising from its ill-posed nature. Cluster ensembles can provide robust and stable solutions by leveraging the consensus across multiple clustering results, while averaging out emergent spurious structures that arise due to the various biases to which each participating algorithm is tuned. In this article, we address the problem of combining multiple weighted clusters that belong to different subspaces of the input space. We leverage the diversity of the input clusterings in order to generate a consensus partition that is superior to the participating ones. Since we are dealing with weighted clusters, our consensus functions make use of the weight vectors associated with the clusters. We demonstrate the effectiveness of our techniques by running experiments with several real datasets, including high-dimensional text data. Furthermore, we investigate in depth the issue of diversity and accuracy for our ensemble methods. Our analysis and experimental results show that the proposed techniques are capable of producing a partition that is as good as or better than the best individual clustering.",
    "cited_by_count": 207,
    "openalex_id": "https://openalex.org/W2144419338",
    "type": "article"
  },
  {
    "title": "A Framework for Computing the Privacy Scores of Users in Online Social Networks",
    "doi": "https://doi.org/10.1145/1870096.1870102",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Kun Liu; Evimaria Terzi",
    "corresponding_authors": "",
    "abstract": "A large body of work has been devoted to address corporate-scale privacy concerns related to social networks. Most of this work focuses on how to share social networks owned by organizations without revealing the identities or the sensitive relationships of the users involved. Not much attention has been given to the privacy risk of users posed by their daily information-sharing activities. In this article, we approach the privacy issues raised in online social networks from the individual users’ viewpoint: we propose a framework to compute the privacy score of a user. This score indicates the user’s potential risk caused by his or her participation in the network. Our definition of privacy score satisfies the following intuitive properties: the more sensitive information a user discloses, the higher his or her privacy risk. Also, the more visible the disclosed information becomes in the network, the higher the privacy risk. We develop mathematical models to estimate both sensitivity and visibility of the information. We apply our methods to synthetic and real-world data and demonstrate their efficacy and practical utility.",
    "cited_by_count": 200,
    "openalex_id": "https://openalex.org/W2107855415",
    "type": "article"
  },
  {
    "title": "Data mining for discrimination discovery",
    "doi": "https://doi.org/10.1145/1754428.1754432",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "Salvatore Ruggieri; Dino Pedreschi; Franco Turini",
    "corresponding_authors": "",
    "abstract": "In the context of civil rights law, discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority, without regard to individual merit. Discrimination in credit, mortgage, insurance, labor market, and education has been investigated by researchers in economics and human sciences. With the advent of automatic decision support systems, such as credit scoring systems, the ease of data collection opens several challenges to data analysts for the fight against discrimination. In this article, we introduce the problem of discovering discrimination through data mining in a dataset of historical decision records, taken by humans or by automatic systems. We formalize the processes of direct and indirect discrimination discovery by modelling protected-by-law groups and contexts where discrimination occurs in a classification rule based syntax. Basically, classification rules extracted from the dataset allow for unveiling contexts of unlawful discrimination, where the degree of burden over protected-by-law groups is formalized by an extension of the lift measure of a classification rule. In direct discrimination, the extracted rules can be directly mined in search of discriminatory contexts. In indirect discrimination, the mining process needs some background knowledge as a further input, for example, census data, that combined with the extracted rules might allow for unveiling contexts of discriminatory decisions. A strategy adopted for combining extracted classification rules with background knowledge is called an inference model. In this article, we propose two inference models and provide automatic procedures for their implementation. An empirical assessment of our results is provided on the German credit dataset and on the PKDD Discovery Challenge 1999 financial dataset.",
    "cited_by_count": 174,
    "openalex_id": "https://openalex.org/W1999380087",
    "type": "article"
  },
  {
    "title": "PathSelClus",
    "doi": "https://doi.org/10.1145/2500492",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Yizhou Sun; Brandon Norick; Jiawei Han; Xifeng Yan; Philip S. Yu; Xiao Yu",
    "corresponding_authors": "",
    "abstract": "Real-world, multiple-typed objects are often interconnected, forming heterogeneous information networks. A major challenge for link-based clustering in such networks is their potential to generate many different results, carrying rather diverse semantic meanings. In order to generate desired clustering, we propose to use meta-path , a path that connects object types via a sequence of relations, to control clustering with distinct semantics. Nevertheless, it is easier for a user to provide a few examples (seeds) than a weighted combination of sophisticated meta-paths to specify her clustering preference. Thus, we propose to integrate meta-path selection with user-guided clustering to cluster objects in networks, where a user first provides a small set of object seeds for each cluster as guidance. Then the system learns the weight for each meta-path that is consistent with the clustering result implied by the guidance, and generates clusters under the learned weights of meta-paths. A probabilistic approach is proposed to solve the problem, and an effective and efficient iterative algorithm, PathSelClus , is proposed to learn the model, where the clustering quality and the meta-path weights mutually enhance each other. Our experiments with several clustering tasks in two real networks and one synthetic network demonstrate the power of the algorithm in comparison with the baselines.",
    "cited_by_count": 169,
    "openalex_id": "https://openalex.org/W2049137142",
    "type": "article"
  },
  {
    "title": "A shared-subspace learning framework for multi-label classification",
    "doi": "https://doi.org/10.1145/1754428.1754431",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "Shuiwang Ji; Lei Tang; Shipeng Yu; Jieping Ye",
    "corresponding_authors": "",
    "abstract": "Multi-label problems arise in various domains such as multi-topic document categorization, protein function prediction, and automatic image annotation. One natural way to deal with such problems is to construct a binary classifier for each label, resulting in a set of independent binary classification problems. Since multiple labels share the same input space, and the semantics conveyed by different labels are usually correlated, it is essential to exploit the correlation information contained in different labels. In this paper, we consider a general framework for extracting shared structures in multi-label classification. In this framework, a common subspace is assumed to be shared among multiple labels. We show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem, though the problem is nonconvex. For high-dimensional problems, direct computation of the solution is expensive, and we develop an efficient algorithm for this case. One appealing feature of the proposed framework is that it includes several well-known algorithms as special cases, thus elucidating their intrinsic relationships. We further show that the proposed framework can be extended to the kernel-induced feature space. We have conducted extensive experiments on multi-topic web page categorization and automatic gene expression pattern image annotation tasks, and results demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithms.",
    "cited_by_count": 163,
    "openalex_id": "https://openalex.org/W2042759724",
    "type": "article"
  },
  {
    "title": "Clustering Large Attributed Graphs",
    "doi": "https://doi.org/10.1145/1921632.1921638",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Hong Cheng; Yang Zhou; Jeffrey Xu Yu",
    "corresponding_authors": "",
    "abstract": "Social networks, sensor networks, biological networks, and many other information networks can be modeled as a large graph. Graph vertices represent entities, and graph edges represent their relationships or interactions. In many large graphs, there is usually one or more attributes associated with every graph vertex to describe its properties. In many application domains, graph clustering techniques are very useful for detecting densely connected groups in a large graph as well as for understanding and visualizing a large graph. The goal of graph clustering is to partition vertices in a large graph into different clusters based on various criteria such as vertex connectivity or neighborhood similarity. Many existing graph clustering methods mainly focus on the topological structure for clustering, but largely ignore the vertex properties, which are often heterogenous. In this article, we propose a novel graph clustering algorithm, SA-Cluster , which achieves a good balance between structural and attribute similarities through a unified distance measure. Our method partitions a large graph associated with attributes into k clusters so that each cluster contains a densely connected subgraph with homogeneous attribute values. An effective method is proposed to automatically learn the degree of contributions of structural similarity and attribute similarity. Theoretical analysis is provided to show that SA-Cluster is converging quickly through iterative cluster refinement. Some optimization techniques on matrix computation are proposed to further improve the efficiency of SA-Cluster on large graphs. Extensive experimental results demonstrate the effectiveness of SA-Cluster through comparisons with the state-of-the-art graph clustering and summarization methods.",
    "cited_by_count": 159,
    "openalex_id": "https://openalex.org/W1993591232",
    "type": "article"
  },
  {
    "title": "Multisource domain adaptation and its application to early detection of fatigue",
    "doi": "https://doi.org/10.1145/2382577.2382582",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Rita Chattopadhyay; Qian Sun; Wei Fan; Ian Davidson; Sethuraman Panchanathan; Jieping Ye",
    "corresponding_authors": "",
    "abstract": "We consider the characterization of muscle fatigue through a noninvasive sensing mechanism such as Surface ElectroMyoGraphy (SEMG). While changes in the properties of SEMG signals with respect to muscle fatigue have been reported in the literature, the large variation in these signals across different individuals makes the task of modeling and classification of SEMG signals challenging. Indeed, the variation in SEMG parameters from subject to subject creates differences in the data distribution. In this article, we propose two transfer learning frameworks based on the multisource domain adaptation methodology for detecting different stages of fatigue using SEMG signals, that addresses the distribution differences. In the proposed frameworks, the SEMG data of a subject represent a domain; data from multiple subjects in the training set form the multiple source domains and the test subject data form the target domain. SEMG signals are predominantly different in conditional probability distribution across subjects. The key feature of the first framework is a novel weighting scheme that addresses the conditional probability distribution differences across multiple domains (subjects) and the key feature of the second framework is a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences (first stage) as well as conditional probability differences (second stage), with the target domain data. The weights for minimizing the marginal probability differences are estimated independently, while the weights for minimizing conditional probability differences are computed simultaneously by exploiting the potential interaction among multiple sources. We also provide a theoretical analysis on the generalization performance of the proposed multisource domain adaptation formulation using the weighted Rademacher complexity measure. We have validated the proposed frameworks on Surface ElectroMyoGram signals collected from 8 people during a fatigue-causing repetitive gripping activity. Comprehensive experiments on the SEMG dataset demonstrate that the proposed method improves the classification accuracy by 20% to 30% over the cases without any domain adaptation method and by 13% to 30% over existing state-of-the-art domain adaptation methods.",
    "cited_by_count": 159,
    "openalex_id": "https://openalex.org/W2735735104",
    "type": "article"
  },
  {
    "title": "A Combination Approach to Web User Profiling",
    "doi": "https://doi.org/10.1145/1870096.1870098",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Jie Tang; Limin Yao; Duo Zhang; Jing Zhang",
    "corresponding_authors": "",
    "abstract": "In this article, we study the problem of Web user profiling, which is aimed at finding, extracting, and fusing the “semantic”-based user profile from the Web. Previously, Web user profiling was often undertaken by creating a list of keywords for the user, which is (sometimes even highly) insufficient for main applications. This article formalizes the profiling problem as several subtasks: profile extraction, profile integration, and user interest discovery. We propose a combination approach to deal with the profiling tasks. Specifically, we employ a classification model to identify relevant documents for a user from the Web and propose a Tree-Structured Conditional Random Fields (TCRF) to extract the profile information from the identified documents; we propose a unified probabilistic model to deal with the name ambiguity problem (several users with the same name) when integrating the profile information extracted from different sources; finally, we use a probabilistic topic model to model the extracted user profiles, and construct the user interest model. Experimental results on an online system show that the combination approach to different profiling tasks clearly outperforms several baseline methods. The extracted profiles have been applied to expert finding, an important application on the Web. Experiments show that the accuracy of expert finding can be improved (ranging from +6% to +26% in terms of MAP) by taking advantage of the profiles.",
    "cited_by_count": 157,
    "openalex_id": "https://openalex.org/W2046083725",
    "type": "article"
  },
  {
    "title": "Density-based clustering of data streams at multiple resolutions",
    "doi": "https://doi.org/10.1145/1552303.1552307",
    "publication_date": "2009-07-01",
    "publication_year": 2009,
    "authors": "Li Wan; Wee Keong Ng; Xuan Hong Dang; Philip S. Yu; Kuan Zhang",
    "corresponding_authors": "",
    "abstract": "In data stream clustering, it is desirable to have algorithms that are able to detect clusters of arbitrary shape, clusters that evolve over time, and clusters with noise. Existing stream data clustering algorithms are generally based on an online-offline approach: The online component captures synopsis information from the data stream (thus, overcoming real-time and memory constraints) and the offline component generates clusters using the stored synopsis. The online-offline approach affects the overall performance of stream data clustering in various ways: the ease of deriving synopsis from streaming data; the complexity of data structure for storing and managing synopsis; and the frequency at which the offline component is used to generate clusters. In this article, we propose an algorithm that (1) computes and updates synopsis information in constant time; (2) allows users to discover clusters at multiple resolutions; (3) determines the right time for users to generate clusters from the synopsis information; (4) generates clusters of higher purity than existing algorithms; and (5) determines the right threshold function for density-based clustering based on the fading model of stream data. To the best of our knowledge, no existing data stream algorithms has all of these features. Experimental results show that our algorithm is able to detect arbitrarily shaped, evolving clusters with high quality.",
    "cited_by_count": 152,
    "openalex_id": "https://openalex.org/W2112482089",
    "type": "article"
  },
  {
    "title": "Privacy-preserving decision trees over vertically partitioned data",
    "doi": "https://doi.org/10.1145/1409620.1409624",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Jaideep Vaidya; Chris Clifton; Murat Kantarcıoğlu; A. Scott Patterson",
    "corresponding_authors": "",
    "abstract": "Privacy and security concerns can prevent sharing of data, derailing data-mining projects. Distributed knowledge discovery, if done correctly, can alleviate this problem. We introduce a generalized privacy-preserving variant of the ID3 algorithm for vertically partitioned data distributed over two or more parties. Along with a proof of security, we discuss what would be necessary to make the protocols completely secure. We also provide experimental results, giving a first demonstration of the practical complexity of secure multiparty computation-based data mining.",
    "cited_by_count": 152,
    "openalex_id": "https://openalex.org/W2137596716",
    "type": "article"
  },
  {
    "title": "On evolutionary spectral clustering",
    "doi": "https://doi.org/10.1145/1631162.1631165",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Yün Chi; Xiaodan Song; Dengyong Zhou; Koji Hino; Belle L. Tseng",
    "corresponding_authors": "",
    "abstract": "Evolutionary clustering is an emerging research area essential to important applications such as clustering dynamic Web and blog contents and clustering data streams. In evolutionary clustering, a good clustering result should fit the current data well, while simultaneously not deviate too dramatically from the recent history. To fulfill this dual purpose, a measure of temporal smoothness is integrated in the overall measure of clustering quality. In this article, we propose two frameworks that incorporate temporal smoothness in evolutionary spectral clustering. For both frameworks, we start with intuitions gained from the well-known k -means clustering problem, and then propose and solve corresponding cost functions for the evolutionary spectral clustering problems. Our solutions to the evolutionary spectral clustering problems provide more stable and consistent clustering results that are less sensitive to short-term noises while at the same time are adaptive to long-term cluster drifts. Furthermore, we demonstrate that our methods provide the optimal solutions to the relaxed versions of the corresponding evolutionary k -means clustering problems. Performance experiments over a number of real and synthetic data sets illustrate our evolutionary spectral clustering methods provide more robust clustering results that are not sensitive to noise and can adapt to data drifts.",
    "cited_by_count": 146,
    "openalex_id": "https://openalex.org/W1998819761",
    "type": "article"
  },
  {
    "title": "Learning Incoherent Sparse and Low-Rank Patterns from Multiple Tasks",
    "doi": "https://doi.org/10.1145/2086737.2086742",
    "publication_date": "2012-01-31",
    "publication_year": 2012,
    "authors": "Jianhui Chen; Ji Liu; Jieping Ye",
    "corresponding_authors": "",
    "abstract": "We consider the problem of learning incoherent sparse and low-rank patterns from multiple tasks. Our approach is based on a linear multi-task learning formulation, in which the sparse and low-rank patterns are induced by a cardinality regularization term and a low-rank constraint, respectively. This formulation is non-convex; we convert it into its convex surrogate, which can be routinely solved via semidefinite programming for small-size problems. We propose to employ the general projected gradient scheme to efficiently solve such a convex surrogate; however, in the optimization formulation, the objective function is non-differentiable and the feasible domain is non-trivial. We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme. The computation of projected gradient involves a constrained optimization problem; we show that the optimal solution to such a problem can be obtained via solving an unconstrained optimization subproblem and an Euclidean projection subproblem. We also present two projected gradient algorithms and analyze their rates of convergence in details. In addition, we illustrate the use of the presented projected gradient algorithms for the proposed multi-task learning formulation using the least squares loss. Experimental results on a collection of real-world data sets demonstrate the effectiveness of the proposed multi-task learning formulation and the efficiency of the proposed projected gradient algorithms.",
    "cited_by_count": 146,
    "openalex_id": "https://openalex.org/W2034295546",
    "type": "article"
  },
  {
    "title": "D <scp>elta</scp> C <scp>on</scp>",
    "doi": "https://doi.org/10.1145/2824443",
    "publication_date": "2016-02-24",
    "publication_year": 2016,
    "authors": "Danai Koutra; Neil Shah; Joshua T Vogelstein; Brian Gallagher; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "How much has a network changed since yesterday? How different is the wiring of Bob’s brain (a left-handed male) and Alice’s brain (a right-handed female), and how is it different? Graph similarity with given node correspondence, i.e., the detection of changes in the connectivity of graphs, arises in numerous settings. In this work, we formally state the axioms and desired properties of the graph similarity functions, and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs. We propose D elta C on , a principled, intuitive, and scalable algorithm that assesses the similarity between two graphs on the same nodes (e.g., employees of a company, customers of a mobile carrier). In conjunction, we propose D elta C on -A ttr , a related approach that enables attribution of change or dissimilarity to responsible nodes and edges. Experiments on various synthetic and real graphs showcase the advantages of our method over existing similarity measures. Finally, we employ D elta C on and D elta C on -A ttr on real applications: (a) we classify people to groups of high and low creativity based on their brain connectivity graphs, (b) do temporal anomaly detection in the who-emails-whom Enron graph and find the top culprits for the changes in the temporal corporate email graph, and (c) recover pairs of test-retest large brain scans ( ∼17M edges, up to 90M edges) for 21 subjects.",
    "cited_by_count": 142,
    "openalex_id": "https://openalex.org/W2339085491",
    "type": "article"
  },
  {
    "title": "Centralized and Distributed Anonymization for High-Dimensional Healthcare Data",
    "doi": "https://doi.org/10.1145/1857947.1857950",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Noman Mohammed; Benjamin C. M. Fung; Patrick C. K. Hung; Cheuk‐Kwong Lee",
    "corresponding_authors": "",
    "abstract": "Sharing healthcare data has become a vital requirement in healthcare system management; however, inappropriate sharing and usage of healthcare data could threaten patients’ privacy. In this article, we study the privacy concerns of sharing patient information between the Hong Kong Red Cross Blood Transfusion Service (BTS) and the public hospitals. We generalize their information and privacy requirements to the problems of centralized anonymization and distributed anonymization , and identify the major challenges that make traditional data anonymization methods not applicable. Furthermore, we propose a new privacy model called LKC-privacy to overcome the challenges and present two anonymization algorithms to achieve LKC-privacy in both the centralized and the distributed scenarios. Experiments on real-life data demonstrate that our anonymization algorithms can effectively retain the essential information in anonymous data for data analysis and is scalable for anonymizing large datasets.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W2038873345",
    "type": "article"
  },
  {
    "title": "Modeling Location-Based User Rating Profiles for Personalized Recommendation",
    "doi": "https://doi.org/10.1145/2663356",
    "publication_date": "2015-04-01",
    "publication_year": 2015,
    "authors": "Hongzhi Yin; Bin Cui; Ling Chen; Zhiting Hu; Chengqi Zhang",
    "corresponding_authors": "",
    "abstract": "This article proposes LA-LDA, a location-aware probabilistic generative model that exploits location-based ratings to model user profiles and produce recommendations. Most of the existing recommendation models do not consider the spatial information of users or items; however, LA-LDA supports three classes of location-based ratings, namely spatial user ratings for nonspatial items, nonspatial user ratings for spatial items, and spatial user ratings for spatial items. LA-LDA consists of two components, ULA-LDA and ILA-LDA, which are designed to take into account user and item location information, respectively. The component ULA-LDA explicitly incorporates and quantifies the influence from local public preferences to produce recommendations by considering user home locations, whereas the component ILA-LDA recommends items that are closer in both taste and travel distance to the querying users by capturing item co-occurrence patterns, as well as item location co-occurrence patterns. The two components of LA-LDA can be applied either separately or collectively, depending on the available types of location-based ratings. To demonstrate the applicability and flexibility of the LA-LDA model, we deploy it to both top- k recommendation and cold start recommendation scenarios. Experimental evidence on large-scale real-world data, including the data from Gowalla (a location-based social network), DoubanEvent (an event-based social network), and MovieLens (a movie recommendation system), reveal that LA-LDA models user profiles more accurately by outperforming existing recommendation models for top- k recommendation and the cold start problem.",
    "cited_by_count": 134,
    "openalex_id": "https://openalex.org/W2032611833",
    "type": "article"
  },
  {
    "title": "HADI",
    "doi": "https://doi.org/10.1145/1921632.1921634",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "U Kang; Charalampos E. Tsourakakis; Ana Paula Appel; Christos Faloutsos; Jure Leskovec",
    "corresponding_authors": "",
    "abstract": "Given large, multimillion-node graphs (e.g., Facebook, Web-crawls, etc.), how do they evolve over time? How are they connected? What are the central nodes and the outliers? In this article we define the Radius plot of a graph and show how it can answer these questions. However, computing the Radius plot is prohibitively expensive for graphs reaching the planetary scale. There are two major contributions in this article: (a) We propose HADI (HAdoop DIameter and radii estimator), a carefully designed and fine-tuned algorithm to compute the radii and the diameter of massive graphs, that runs on the top of the Hadoop / MapReduce system, with excellent scale-up on the number of available machines (b) We run HADI on several real world datasets including YahooWeb (6B edges, 1/8 of a Terabyte), one of the largest public graphs ever analyzed. Thanks to HADI, we report fascinating patterns on large networks, like the surprisingly small effective diameter, the multimodal/bimodal shape of the Radius plot, and its palindrome motion over time.",
    "cited_by_count": 133,
    "openalex_id": "https://openalex.org/W2140030920",
    "type": "article"
  },
  {
    "title": "Scalable and Accurate Online Feature Selection for Big Data",
    "doi": "https://doi.org/10.1145/2976744",
    "publication_date": "2016-12-03",
    "publication_year": 2016,
    "authors": "Kui Yu; Xindong Wu; Wei Ding; Jian Pei",
    "corresponding_authors": "",
    "abstract": "Feature selection is important in many big data applications. Two critical challenges closely associate with big data. First, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Second, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a &lt;underline&gt;S&lt;/underline&gt;calable and &lt;underline&gt;A&lt;/underline&gt;ccurate &lt;underline&gt;O&lt;/underline&gt;n&lt;underline&gt;L&lt;/underline&gt;ine &lt;underline&gt;A&lt;/underline&gt;pproach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintains a parsimonious model over time in an online manner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real datasets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on datasets of extremely high dimensionality and have superior performance over the state-of-the-art feature selection methods.",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W2257550357",
    "type": "article"
  },
  {
    "title": "A Regularization Approach to Learning Task Relationships in Multitask Learning",
    "doi": "https://doi.org/10.1145/2538028",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Yu Zhang; Dit‐Yan Yeung",
    "corresponding_authors": "",
    "abstract": "Multitask learning is a learning paradigm that seeks to improve the generalization performance of a learning task with the help of some other related tasks. In this article, we propose a regularization approach to learning the relationships between tasks in multitask learning. This approach can be viewed as a novel generalization of the regularized formulation for single-task learning. Besides modeling positive task correlation, our approach—multitask relationship learning (MTRL)—can also describe negative task correlation and identify outlier tasks based on the same underlying principle. By utilizing a matrix-variate normal distribution as a prior on the model parameters of all tasks, our MTRL method has a jointly convex objective function. For efficiency, we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks. We study MTRL in the symmetric multitask learning setting and then generalize it to the asymmetric setting as well. We also discuss some variants of the regularization approach to demonstrate the use of other matrix-variate priors for learning task relationships. Moreover, to gain more insight into our model, we also study the relationships between MTRL and some existing multitask learning methods. Experiments conducted on a toy problem as well as several benchmark datasets demonstrate the effectiveness of MTRL as well as its high interpretability revealed by the task covariance matrix.",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W2097451239",
    "type": "article"
  },
  {
    "title": "From Context to Distance",
    "doi": "https://doi.org/10.1145/2133360.2133361",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Dino Ienco; Ruggero G. Pensa; Rosa Meo",
    "corresponding_authors": "",
    "abstract": "Clustering data described by categorical attributes is a challenging task in data mining applications. Unlike numerical attributes, it is difficult to define a distance between pairs of values of a categorical attribute, since the values are not ordered. In this article, we propose a framework to learn a context-based distance for categorical attributes. The key intuition of this work is that the distance between two values of a categorical attribute A i can be determined by the way in which the values of the other attributes A j are distributed in the dataset objects: if they are similarly distributed in the groups of objects in correspondence of the distinct values of A i a low value of distance is obtained. We propose also a solution to the critical point of the choice of the attributes A j . We validate our approach by embedding our distance learning framework in a hierarchical clustering algorithm. We applied it on various real world and synthetic datasets, both low and high-dimensional. Experimental results show that our method is competitive with respect to the state of the art of categorical data clustering approaches. We also show that our approach is scalable and has a low impact on the overall computational time of a clustering task.",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W2143687373",
    "type": "article"
  },
  {
    "title": "Confidence Weighted Mean Reversion Strategy for Online Portfolio Selection",
    "doi": "https://doi.org/10.1145/2435209.2435213",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Bin Li; Steven C. H. Hoi; Peilin Zhao; Vivekanand Gopalkrishnan",
    "corresponding_authors": "",
    "abstract": "Online portfolio selection has been attracting increasing attention from the data mining and machine learning communities. All existing online portfolio selection strategies focus on the first order information of a portfolio vector, though the second order information may also be beneficial to a strategy. Moreover, empirical evidence shows that relative stock prices may follow the mean reversion property, which has not been fully exploited by existing strategies. This article proposes a novel online portfolio selection strategy named Confidence Weighted Mean Reversion (CWMR). Inspired by the mean reversion principle in finance and confidence weighted online learning technique in machine learning, CWMR models the portfolio vector as a Gaussian distribution, and sequentially updates the distribution by following the mean reversion trading principle. CWMR’s closed-form updates clearly reflect the mean reversion trading idea. We also present several variants of CWMR algorithms, including a CWMR mixture algorithm that is theoretical universal. Empirically, CWMR strategy is able to effectively exploit the power of mean reversion for online portfolio selection. Extensive experiments on various real markets show that the proposed strategy is superior to the state-of-the-art techniques. The experimental testbed including source codes and data sets is available online.",
    "cited_by_count": 125,
    "openalex_id": "https://openalex.org/W2158100334",
    "type": "article"
  },
  {
    "title": "Message-Passing Algorithms for Sparse Network Alignment",
    "doi": "https://doi.org/10.1145/2435209.2435212",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Mohsen Bayati; David F. Gleich; Amin Saberi; Ying Wang",
    "corresponding_authors": "",
    "abstract": "Network alignment generalizes and unifies several approaches for forming a matching or alignment between the vertices of two graphs. We study a mathematical programming framework for network alignment problem and a sparse variation of it where only a small number of matches between the vertices of the two graphs are possible. We propose a new message passing algorithm that allows us to compute, very efficiently, approximate solutions to the sparse network alignment problems with graph sizes as large as hundreds of thousands of vertices. We also provide extensive simulations comparing our algorithms with two of the best solvers for network alignment problems on two synthetic matching problems, two bioinformatics problems, and three large ontology alignment problems including a multilingual problem with a known labeled alignment.",
    "cited_by_count": 112,
    "openalex_id": "https://openalex.org/W2017822917",
    "type": "article"
  },
  {
    "title": "Less is More",
    "doi": "https://doi.org/10.1145/2890508",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Shebuti Rayana; Leman Akoglu",
    "corresponding_authors": "",
    "abstract": "Ensemble learning for anomaly detection has been barely studied, due to difficulty in acquiring ground truth and the lack of inherent objective functions. In contrast, ensemble approaches for classification and clustering have been studied and effectively used for long. Our work taps into this gap and builds a new ensemble approach for anomaly detection, with application to event detection in temporal graphs as well as outlier detection in no-graph settings. It handles and combines multiple heterogeneous detectors to yield improved and robust performance. Importantly, trusting results from all the constituent detectors may deteriorate the overall performance of the ensemble, as some detectors could provide inaccurate results depending on the type of data in hand and the underlying assumptions of a detector. This suggests that combining the detectors selectively is key to building effective anomaly ensembles—hence “less is more”. In this paper we propose a novel ensemble approach called SELECT for anomaly detection, which automatically and systematically selects the results from constituent detectors to combine in a fully unsupervised fashion. We apply our method to event detection in temporal graphs and outlier detection in multi-dimensional point data (no-graph), where SELECT successfully utilizes five base detectors and seven consensus methods under a unified ensemble framework. We provide extensive quantitative evaluation of our approach for event detection on five real-world datasets (four with ground truth events), including Enron email communications, RealityMining SMS and phone call records, New York Times news corpus, and World Cup 2014 Twitter news feed. We also provide results for outlier detection on seven real-world multi-dimensional point datasets from UCI Machine Learning Repository. Thanks to its selection mechanism, SELECT yields superior performance compared to the individual detectors alone, the full ensemble (naively combining all results), an existing diversity-based ensemble, and an existing weighted ensemble approach.",
    "cited_by_count": 112,
    "openalex_id": "https://openalex.org/W309312769",
    "type": "article"
  },
  {
    "title": "Network Embedding for Community Detection in Attributed Networks",
    "doi": "https://doi.org/10.1145/3385415",
    "publication_date": "2020-05-13",
    "publication_year": 2020,
    "authors": "Heli Sun; Fang He; Jianbin Huang; Yizhou Sun; Li Yang; Chenyu Wang; Liang He; Zhongbin Sun; Xiaolin Jia",
    "corresponding_authors": "",
    "abstract": "Community detection aims to partition network nodes into a set of clusters, such that nodes are more densely connected to each other within the same cluster than other clusters. For attributed networks, apart from the denseness requirement of topology structure, the attributes of nodes in the same community should also be homogeneous. Network embedding has been proved extremely useful in a variety of tasks, such as node classification, link prediction, and graph visualization, but few works dedicated to unsupervised embedding of node features specified for clustering task, which is vital for community detection and graph clustering. By post-processing with clustering algorithms like k -means, most existing network embedding methods can be applied to clustering tasks. However, the learned embeddings are not designed for clustering task, they only learn topological and attributed information of networks, and no clustering-oriented information is explored. In this article, we propose an algorithm named Network Embedding for node Clustering (NEC) to learn network embedding for node clustering in attributed graphs. Specifically, the presented work introduces a framework that simultaneously learns graph structure-based representations and clustering-oriented representations together. The framework consists of the following three modules: graph convolutional autoencoder module, soft modularity maximization module, and self-clustering module. Graph convolutional autoencoder module learns node embeddings based on topological structure and node attributes. We introduce soft modularity, which can be easily optimized using gradient descent algorithms, to exploit the community structure of networks. By integrating clustering loss and embedding loss, NEC can jointly optimize node cluster labels assignment and learn representations that keep local structure of network. This model can be effectively optimized using stochastic gradient algorithm. Empirical experiments on real-world networks and synthetic networks validate the feasibility and effectiveness of our algorithm on community detection task compared with network embedding based methods and traditional community detection methods.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W3026423472",
    "type": "article"
  },
  {
    "title": "Convex Sparse PCA for Unsupervised Feature Learning",
    "doi": "https://doi.org/10.1145/2910585",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Xiaojun Chang; Feiping Nie; Yi Yang; Chengqi Zhang; Heng Huang",
    "corresponding_authors": "",
    "abstract": "Principal component analysis (PCA) has been widely applied to dimensionality reduction and data pre-processing for different applications in engineering, biology, social science, and the like. Classical PCA and its variants seek for linear projections of the original variables to obtain the low-dimensional feature representations with maximal variance. One limitation is that it is difficult to interpret the results of PCA. Besides, the classical PCA is vulnerable to certain noisy data. In this paper, we propose a Convex Sparse Principal Component Analysis (CSPCA) algorithm and apply it to feature learning. First, we show that PCA can be formulated as a low-rank regression optimization problem. Based on the discussion, the l 2, 1 -normminimization is incorporated into the objective function to make the regression coefficients sparse, thereby robust to the outliers. Also, based on the sparse model used in CSPCA, an optimal weight is assigned to each of the original feature, which in turn provides the output with good interpretability. With the output of our CSPCA, we can effectively analyze the importance of each feature under the PCA criteria. Our new objective function is convex, and we propose an iterative algorithm to optimize it. We apply the CSPCA algorithm to feature selection and conduct extensive experiments on seven benchmark datasets. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art unsupervised feature selection algorithms.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W108615132",
    "type": "article"
  },
  {
    "title": "Catching Synchronized Behaviors in Large Networks",
    "doi": "https://doi.org/10.1145/2746403",
    "publication_date": "2016-06-29",
    "publication_year": 2016,
    "authors": "Meng Jiang; Peng Cui; Alex Beutel; Christos Faloutsos; Shiqiang Yang",
    "corresponding_authors": "",
    "abstract": "Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes judging only from their connectivity patterns? Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. We propose a fast and effective method, C atch S ync , which exploits two of the tell-tale signs left in graphs by fraudsters: (a) synchronized behavior: suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together (such as follow the same user); and (b) rare behavior: their connectivity patterns are very different from the majority. We introduce novel measures to quantify both concepts (“synchronicity” and “normality”) and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots. Thanks to careful design, C atch S ync has the following desirable properties: (a) it is scalable to large datasets, being linear in the graph size; (b) it is parameter free ; and (c) it is side-information-oblivious : it can operate using only the topology, without needing labeled data, nor timing information, and the like., while still capable of using side information if available. We applied C atch S ync on three large, real datasets, 1-billion-edge Twitter social graph, 3-billion-edge, and 12-billion-edge Tencent Weibo social graphs, and several synthetic ones; C atch S ync consistently outperforms existing competitors, both in detection accuracy by 36% on Twitter and 20% on Tencent Weibo, as well as in speed.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2473813716",
    "type": "article"
  },
  {
    "title": "Streaming Social Event Detection and Evolution Discovery in Heterogeneous Information Networks",
    "doi": "https://doi.org/10.1145/3447585",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Hao Peng; Jianxin Li; Yangqiu Song; Renyu Yang; Rajiv Ranjan; Philip S. Yu; Lifang He",
    "corresponding_authors": "",
    "abstract": "Events are happening in real world and real time, which can be planned and organized for occasions, such as social gatherings, festival celebrations, influential meetings, or sports activities. Social media platforms generate a lot of real-time text information regarding public events with different topics. However, mining social events is challenging because events typically exhibit heterogeneous texture and metadata are often ambiguous. In this article, we first design a novel event-based meta-schema to characterize the semantic relatedness of social events and then build an event-based heterogeneous information network (HIN) integrating information from external knowledge base. Second, we propose a novel Pairwise Popularity Graph Convolutional Network, named as PP-GCN, based on weighted meta-path instance similarity and textual semantic representation as inputs, to perform fine-grained social event categorization and learn the optimal weights of meta-paths in different tasks. Third, we propose a streaming social event detection and evolution discovery framework for HINs based on meta-path similarity search, historical information about meta-paths, and heterogeneous DBSCAN clustering method. Comprehensive experiments on real-world streaming social text data are conducted to compare various social event detection and evolution discovery algorithms. Experimental results demonstrate that our proposed framework outperforms other alternative social event detection and evolution discovery techniques.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W3130808434",
    "type": "article"
  },
  {
    "title": "High-Utility Itemset Mining with Effective Pruning Strategies",
    "doi": "https://doi.org/10.1145/3363571",
    "publication_date": "2019-11-11",
    "publication_year": 2019,
    "authors": "Jimmy Ming‐Tai Wu; Jerry Chun‐Wei Lin; Ashish Kumar Tamrakar",
    "corresponding_authors": "",
    "abstract": "High-utility itemset mining is a popular data mining problem that considers utility factors, such as quantity and unit profit of items besides frequency measure from the transactional database. It helps to find the most valuable and profitable products/items that are difficult to track by using only the frequent itemsets. An item might have a high-profit value which is rare in the transactional database and has a tremendous importance. While there are many existing algorithms to find high-utility itemsets (HUIs) that generate comparatively large candidate sets, our main focus is on significantly reducing the computation time with the introduction of new pruning strategies. The designed pruning strategies help to reduce the visitation of unnecessary nodes in the search space, which reduces the time required by the algorithm. In this article, two new stricter upper bounds are designed to reduce the computation time by refraining from visiting unnecessary nodes of an itemset. Thus, the search space of the potential HUIs can be greatly reduced, and the mining procedure of the execution time can be improved. The proposed strategies can also significantly minimize the transaction database generated on each node. Experimental results showed that the designed algorithm with two pruning strategies outperform the state-of-the-art algorithms for mining the required HUIs in terms of runtime and number of revised candidates. The memory usage of the designed algorithm also outperforms the state-of-the-art approach. Moreover, a multi-thread concept is also discussed to further handle the problem of big datasets.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W3003236177",
    "type": "article"
  },
  {
    "title": "Adversarial Attacks on Graph Neural Networks",
    "doi": "https://doi.org/10.1145/3394520",
    "publication_date": "2020-06-21",
    "publication_year": 2020,
    "authors": "Daniel Zügner; Oliver Borchert; Amir Akbarnejad; Stephan Günnemann",
    "corresponding_authors": "",
    "abstract": "Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, little is known about their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g., the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we present a study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node’s features and the graph structure , thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain, we propose an efficient algorithm N ettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given. For the first time, we successfully identify important patterns of adversarial attacks on graph neural networks (GNNs) — a first step towards being able to detect adversarial attacks on GNNs.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W3038342492",
    "type": "article"
  },
  {
    "title": "Neural Networks for Entity Matching: A Survey",
    "doi": "https://doi.org/10.1145/3442200",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Nils Barlaug; Jon Atle Gulla",
    "corresponding_authors": "",
    "abstract": "Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years, we have seen new methods based upon deep learning techniques for natural language processing emerge. In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W3155638005",
    "type": "article"
  },
  {
    "title": "Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series",
    "doi": "https://doi.org/10.1145/3516367",
    "publication_date": "2022-06-24",
    "publication_year": 2022,
    "authors": "Sindhu Tipirneni; Chandan K. Reddy",
    "corresponding_authors": "",
    "abstract": "Multivariate time-series data are frequently observed in critical care settings and are typically characterized by sparsity (missing information) and irregular time intervals. Existing approaches for learning representations in this domain handle these challenges by either aggregation or imputation of values, which in-turn suppresses the fine-grained information and adds undesirable noise/overhead into the machine learning model. To tackle this problem, we propose a S elf-supervised Tra nsformer for T ime- S eries (STraTS) model, which overcomes these pitfalls by treating time-series as a set of observation triplets instead of using the standard dense matrix representation. It employs a novel Continuous Value Embedding technique to encode continuous time and variable values without the need for discretization. It is composed of a Transformer component with multi-head attention layers, which enable it to learn contextual triplet embeddings while avoiding the problems of recurrence and vanishing gradients that occur in recurrent architectures. In addition, to tackle the problem of limited availability of labeled data (which is typically observed in many healthcare applications), STraTS utilizes self-supervision by leveraging unlabeled data to learn better representations by using time-series forecasting as an auxiliary proxy task. Experiments on real-world multivariate clinical time-series benchmark datasets demonstrate that STraTS has better prediction performance than state-of-the-art methods for mortality prediction, especially when labeled data is limited. Finally, we also present an interpretable version of STraTS, which can identify important measurements in the time-series data. Our data preprocessing and model implementation codes are available at https://github.com/sindhura97/STraTS .",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W4283370926",
    "type": "article"
  },
  {
    "title": "Personalized Federated Learning on Non-IID Data via Group-based Meta-learning",
    "doi": "https://doi.org/10.1145/3558005",
    "publication_date": "2022-08-23",
    "publication_year": 2022,
    "authors": "Lei Yang; Jiaming Huang; Wanyu Lin; Jiannong Cao",
    "corresponding_authors": "",
    "abstract": "Personalized federated learning (PFL) has emerged as a paradigm to provide a personalized model that can fit the local data distribution of each client. One natural choice for PFL is to leverage the fast adaptation capability of meta-learning, where it first obtains a single global model, and each client achieves a personalized model by fine-tuning the global one with its local data. However, existing meta-learning-based approaches implicitly assume that the data distribution among different clients is similar, which may not be applicable due to the property of data heterogeneity in federated learning. In this work, we propose a Group-based Federated Meta-Learning framework, called G-FML , which adaptively divides the clients into groups based on the similarity of their data distribution, and the personalized models are obtained with meta-learning within each group. In particular, we develop a simple yet effective grouping mechanism to adaptively partition the clients into multiple groups. Our mechanism ensures that each group is formed by the clients with similar data distribution such that the group-wise meta-model can achieve “personalization” at large. By doing so, our framework can be generalized to a highly heterogeneous environment. We evaluate the effectiveness of our proposed G-FML framework on three heterogeneous benchmarking datasets. The experimental results show that our framework improves the model accuracy by up to 13.15% relative to the state-of-the-art federated meta-learning.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W4292737460",
    "type": "article"
  },
  {
    "title": "In-Processing Modeling Techniques for Machine Learning Fairness: A Survey",
    "doi": "https://doi.org/10.1145/3551390",
    "publication_date": "2022-07-30",
    "publication_year": 2022,
    "authors": "Mingyang Wan; Daochen Zha; Ninghao Liu; Na Zou",
    "corresponding_authors": "",
    "abstract": "Machine learning models are becoming pervasive in high-stakes applications. Despite their clear benefits in terms of performance, the models could show discrimination against minority groups and result in fairness issues in a decision-making process, leading to severe negative impacts on the individuals and the society. In recent years, various techniques have been developed to mitigate the unfairness for machine learning models. Among them, in-processing methods have drawn increasing attention from the community, where fairness is directly taken into consideration during model design to induce intrinsically fair models and fundamentally mitigate fairness issues in outputs and representations. In this survey, we review the current progress of in-processing fairness mitigation techniques. Based on where the fairness is achieved in the model, we categorize them into explicit and implicit methods, where the former directly incorporates fairness metrics in training objectives, and the latter focuses on refining latent representation learning. Finally, we conclude the survey with a discussion of the research challenges in this community to motivate future exploration.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W4288758404",
    "type": "article"
  },
  {
    "title": "Compression of Deep Learning Models for Text: A Survey",
    "doi": "https://doi.org/10.1145/3487045",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Manish Gupta; Puneet Agrawal",
    "corresponding_authors": "",
    "abstract": "In recent years, the fields of natural language processing (NLP) and information retrieval (IR) have made tremendous progress thanks to deep learning models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTMs) networks, and Transformer [ 121 ] based models like Bidirectional Encoder Representations from Transformers (BERT) [ 24 ], Generative Pre-training Transformer (GPT-2) [ 95 ], Multi-task Deep Neural Network (MT-DNN) [ 74 ], Extra-Long Network (XLNet) [ 135 ], Text-to-text transfer transformer (T5) [ 96 ], T-NLG [ 99 ], and GShard [ 64 ]. But these models are humongous in size. On the other hand, real-world applications demand small model size, low response times, and low computational power wattage. In this survey, we discuss six different types of methods (Pruning, Quantization, Knowledge Distillation (KD), Parameter Sharing, Tensor Decomposition, and Sub-quadratic Transformer-based methods) for compression of such models to enable their deployment in real industry NLP projects. Given the critical need of building applications with efficient and small models, and the large amount of recently published work in this area, we believe that this survey organizes the plethora of work done by the “deep learning for NLP” community in the past few years and presents it as a coherent story.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W4205952419",
    "type": "article"
  },
  {
    "title": "A Unified View of Causal and Non-causal Feature Selection",
    "doi": "https://doi.org/10.1145/3436891",
    "publication_date": "2021-04-18",
    "publication_year": 2021,
    "authors": "Kui Yu; Lin Liu; Jiuyong Li",
    "corresponding_authors": "",
    "abstract": "In this article, we aim to develop a unified view of causal and non-causal feature selection methods. The unified view will fill in the gap in the research of the relation between the two types of methods. Based on the Bayesian network framework and information theory, we first show that causal and non-causal feature selection methods share the same objective. That is to find the Markov blanket of a class attribute, the theoretically optimal feature set for classification. We then examine the assumptions made by causal and non-causal feature selection methods when searching for the optimal feature set, and unify the assumptions by mapping them to the restrictions on the structure of the Bayesian network model of the studied problem. We further analyze in detail how the structural assumptions lead to the different levels of approximations employed by the methods in their search, which then result in the approximations in the feature sets found by the methods with respect to the optimal feature set. With the unified view, we can interpret the output of non-causal methods from a causal perspective and derive the error bounds of both types of methods. Finally, we present practical understanding of the relation between causal and non-causal methods using extensive experiments with synthetic data and various types of real-world data.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W3155575086",
    "type": "article"
  },
  {
    "title": "Nested Named Entity Recognition: A Survey",
    "doi": "https://doi.org/10.1145/3522593",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Yu Wang; Hanghang Tong; Ziye Zhu; Yun Li",
    "corresponding_authors": "",
    "abstract": "With the rapid development of text mining, many studies observe that text generally contains a variety of implicit information, and it is important to develop techniques for extracting such information. Named Entity Recognition (NER), the first step of information extraction, mainly identifies names of persons, locations, and organizations in text. Although existing neural-based NER approaches achieve great success in many language domains, most of them normally ignore the nested nature of named entities. Recently, diverse studies focus on the nested NER problem and yield state-of-the-art performance. This survey attempts to provide a comprehensive review on existing approaches for nested NER from the perspectives of the model architecture and the model property, which may help readers have a better understanding of the current research status and ideas. In this survey, we first introduce the background of nested NER, especially the differences between nested NER and traditional (i.e., flat) NER. We then review the existing nested NER approaches from 2002 to 2020 and mainly classify them into five categories according to the model architecture, including early rule-based, layered-based, region-based, hypergraph-based, and transition-based approaches. We also explore in greater depth the impact of key properties unique to nested NER approaches from the model property perspective, namely entity dependency, stage framework, error propagation, and tag scheme. Finally, we summarize the open challenges and point out a few possible future directions in this area. This survey would be useful for three kinds of readers: (i) Newcomers in the field who want to learn about NER, especially for nested NER. (ii) Researchers who want to clarify the relationship and advantages between flat NER and nested NER. (iii) Practitioners who just need to determine which NER technique (i.e., nested or not) works best in their applications.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W4221025428",
    "type": "article"
  },
  {
    "title": "Graph-Enhanced Spatial-Temporal Network for Next POI Recommendation",
    "doi": "https://doi.org/10.1145/3513092",
    "publication_date": "2022-02-24",
    "publication_year": 2022,
    "authors": "Zhaobo Wang; Yanmin Zhu; Qiaomei Zhang; Haobing Liu; Chunyang Wang; Tong Liu",
    "corresponding_authors": "",
    "abstract": "The task of next Point-of-Interest (POI) recommendation aims at recommending a list of POIs for a user to visit at the next timestamp based on his/her previous interactions, which is valuable for both location-based service providers and users. Recent state-of-the-art studies mainly employ recurrent neural network (RNN) based methods to model user check-in behaviors according to user’s historical check-in sequences. However, most of the existing RNN-based methods merely capture geographical influences depending on physical distance or successive relation among POIs. They are insufficient to capture the high-order complex geographical influences among POI networks, which are essential for estimating user preferences. To address this limitation, we propose a novel Graph-based Spatial Dependency modeling (GSD) module, which focuses on explicitly modeling complex geographical influences by leveraging graph embedding. GSD captures two types of geographical influences, i.e., distance-based and transition-based influences from designed POI semantic graphs. Additionally, we propose a novel Graph-enhanced Spatial-Temporal network (GSTN), which incorporates user spatial and temporal dependencies for next POI recommendation. Specifically, GSTN consists of a Long Short-Term Memory (LSTM) network for user-specific temporal dependencies modeling and GSD for user spatial dependencies learning. Finally, we evaluate the proposed model using three real-world datasets. Extensive experiments demonstrate the effectiveness of GSD in capturing various geographical influences and the improvement of GSTN over state-of-the-art methods.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W4213457653",
    "type": "article"
  },
  {
    "title": "Be Causal: De-Biasing Social Network Confounding in Recommendation",
    "doi": "https://doi.org/10.1145/3533725",
    "publication_date": "2022-05-05",
    "publication_year": 2022,
    "authors": "Qian Li; Xiangmeng Wang; Zhichao Wang; Guandong Xu",
    "corresponding_authors": "",
    "abstract": "In recommendation systems, the existence of the missing-not-at-random (MNAR) problem results in the selection bias issue, degrading the recommendation performance ultimately. A common practice to address MNAR is to treat missing entries from the so-called “exposure” perspective, i.e., modeling how an item is exposed (provided) to a user. Most of the existing approaches use heuristic models or re-weighting strategy on observed ratings to mimic the missing-at-random setting. However, little research has been done to reveal how the ratings are missing from a causal perspective. To bridge the gap, we propose an unbiased and robust method called DENC ( De-Bias Network Confounding in Recommendation ), inspired by confounder analysis in causal inference. In general, DENC provides a causal analysis on MNAR from both the inherent factors (e.g., latent user or item factors) and auxiliary network’s perspective. Particularly, the proposed exposure model in DENC can control the social network confounder meanwhile preserve the observed exposure information. We also develop a deconfounding model through the balanced representation learning to retain the primary user and item features, which enables DENC generalize well on the rating prediction. Extensive experiments on three datasets validate that our proposed model outperforms the state-of-the-art baselines.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W3161460925",
    "type": "article"
  },
  {
    "title": "Self-supervised Graph-level Representation Learning with Adversarial Contrastive Learning",
    "doi": "https://doi.org/10.1145/3624018",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Xiao Luo; Wei Ju; Yiyang Gu; Zhengyang Mao; Luchen Liu; Yuhui Yuan; Ming Zhang",
    "corresponding_authors": "",
    "abstract": "The recently developed unsupervised graph representation learning approaches apply contrastive learning into graph-structured data and achieve promising performance. However, these methods mainly focus on graph augmentation for positive samples, while the negative mining strategies for graph contrastive learning are less explored, leading to sub-optimal performance. To tackle this issue, we propose a Graph Adversarial Contrastive Learning (GraphACL) scheme that learns a bank of negative samples for effective self-supervised whole-graph representation learning. Our GraphACL consists of (i) a graph encoding branch that generates the representations of positive samples and (ii) an adversarial generation branch that produces a bank of negative samples. To generate more powerful hard negative samples, our method minimizes the contrastive loss during encoding updating while maximizing the contrastive loss adversarially over the negative samples for providing the challenging contrastive task. Moreover, the quality of representations produced by the adversarial generation branch is enhanced through the regularization of carefully designed bank divergence loss and bank orthogonality loss. We optimize the parameters of the graph encoding branch and adversarial generation branch alternately. Extensive experiments on 14 real-world benchmarks on both graph classification and transfer learning tasks demonstrate the effectiveness of the proposed approach over existing graph self-supervised representation learning methods.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W4386711666",
    "type": "article"
  },
  {
    "title": "Hypergraph Convolution on Nodes-Hyperedges Network for Semi-Supervised Node Classification",
    "doi": "https://doi.org/10.1145/3494567",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Hanrui Wu; Michael K. Ng",
    "corresponding_authors": "",
    "abstract": "Hypergraphs have shown great power in representing high-order relations among entities, and lots of hypergraph-based deep learning methods have been proposed to learn informative data representations for the node classification problem. However, most of these deep learning approaches do not take full consideration of either the hyperedge information or the original relationships among nodes and hyperedges. In this article, we present a simple yet effective semi-supervised node classification method named Hypergraph Convolution on Nodes-Hyperedges network, which performs filtering on both nodes and hyperedges as well as recovers the original hypergraph with the least information loss. Instead of only reducing the cross-entropy loss over the labeled samples as most previous approaches do, we additionally consider the hypergraph reconstruction loss as prior information to improve prediction accuracy. As a result, by taking both the cross-entropy loss on the labeled samples and the hypergraph reconstruction loss into consideration, we are able to achieve discriminative latent data representations for training a classifier. We perform extensive experiments on the semi-supervised node classification problem and compare the proposed method with state-of-the-art algorithms. The promising results demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W4205568429",
    "type": "article"
  },
  {
    "title": "ADATIME: A Benchmarking Suite for Domain Adaptation on Time Series Data",
    "doi": "https://doi.org/10.1145/3587937",
    "publication_date": "2023-03-24",
    "publication_year": 2023,
    "authors": "Mohamed Ragab; Emadeldeen Eldele; Wee Ling Tan; Chuan-Sheng Foo; Zhenghua Chen; Min Wu; Chee Keong Kwoh; Xiaoli Li",
    "corresponding_authors": "",
    "abstract": "Unsupervised domain adaptation methods aim to generalize well on unlabeled test data that may have a different (shifted) distribution from the training data. Such methods are typically developed on image data, and their application to time series data is less explored. Existing works on time series domain adaptation suffer from inconsistencies in evaluation schemes, datasets, and backbone neural network architectures. Moreover, labeled target data are often used for model selection, which violates the fundamental assumption of unsupervised domain adaptation. To address these issues, we develop a benchmarking evaluation suite (AdaTime) to systematically and fairly evaluate different domain adaptation methods on time series data. Specifically, we standardize the backbone neural network architectures and benchmarking datasets, while also exploring more realistic model selection approaches that can work with no labeled data or just a few labeled samples. Our evaluation includes adapting state-of-the-art visual domain adaptation methods to time series data as well as the recent methods specifically developed for time series data. We conduct extensive experiments to evaluate 11 state-of-the-art methods on five representative datasets spanning 50 cross-domain scenarios. Our results suggest that with careful selection of hyper-parameters, visual domain adaptation methods are competitive with methods proposed for time series domain adaptation. In addition, we find that hyper-parameters could be selected based on realistic model selection approaches. Our work unveils practical insights for applying domain adaptation methods on time series data and builds a solid foundation for future works in the field. The code is available at \\href{https://github.com/emadeldeen24/AdaTime}{github.com/emadeldeen24/AdaTime}.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W4221138878",
    "type": "article"
  },
  {
    "title": "A Survey on Influence Maximization: From an ML-Based Combinatorial Optimization",
    "doi": "https://doi.org/10.1145/3604559",
    "publication_date": "2023-06-12",
    "publication_year": 2023,
    "authors": "Yandi Li; Haobo Gao; Yunxuan Gao; Jianxiong Guo; Weili Wu",
    "corresponding_authors": "",
    "abstract": "Influence Maximization (IM) is a classical combinatorial optimization problem, which can be widely used in mobile networks, social computing, and recommendation systems. It aims at selecting a small number of users such that maximizing the influence spread across the online social network. Because of its potential commercial and academic value, there are a lot of researchers focusing on studying the IM problem from different perspectives. The main challenge comes from the NP-hardness of the IM problem and \\#P-hardness of estimating the influence spread, thus traditional algorithms for overcoming them can be categorized into two classes: heuristic algorithms and approximation algorithms. However, there is no theoretical guarantee for heuristic algorithms, and the theoretical design is close to the limit. Therefore, it is almost impossible to further optimize and improve their performance. With the rapid development of artificial intelligence, the technology based on Machine Learning (ML) has achieved remarkable achievements in many fields. In view of this, in recent years, a number of new methods have emerged to solve combinatorial optimization problems by using ML-based techniques. These methods have the advantages of fast solving speed and strong generalization ability to unknown graphs, which provide a brand-new direction for solving combinatorial optimization problems. Therefore, we abandon the traditional algorithms based on iterative search and review the recent development of ML-based methods, especially Deep Reinforcement Learning, to solve the IM problem and other variants in social networks. We focus on summarizing the relevant background knowledge, basic principles, common methods, and applied research. Finally, the challenges that need to be solved urgently in future IM research are pointed out.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W4380324285",
    "type": "article"
  },
  {
    "title": "Enhanced Fuzzy Clustering for Incomplete Instance with Evidence Combination",
    "doi": "https://doi.org/10.1145/3638061",
    "publication_date": "2023-12-19",
    "publication_year": 2023,
    "authors": "Zhe Liu; Sukumar Letchmunan",
    "corresponding_authors": "",
    "abstract": "Clustering incomplete instance is still a challenging task since missing values maybe make the cluster information ambiguous, leading to the uncertainty and imprecision in results. This article investigates an enhanced fuzzy clustering with evidence combination method based on Dempster-Shafer theory (DST) to address this problem. First, the dataset is divided into several subsets, and missing values are imputed by neighbors with different weights in each subset. It aims to model missing values locally to reduce the negative impact of the bad estimations. Second, an objective function of enhanced fuzzy clustering is designed and then optimized until the best membership and reliability matrices are found. Each subset has a membership matrix that contains all sub-instances’ membership to different clusters. The fuzzy reliability matrix is employed to characterize the reliability of each subset on different clusters. Third, an adaptive evidence combination rule based on the DST is developed to combine the discounted subresults (memberships) with different reliability to make the final decision for each instance. The proposed method can characterize uncertainty and imprecision by assigning instances to specific clusters or meta-clusters composed of several specific clusters. Once an instance is assigned to a meta-cluster, the cluster information of this instance is (locally) imprecise. The effectiveness of proposed method is demonstrated on several real-world datasets by comparing with existing techniques.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W4389937721",
    "type": "article"
  },
  {
    "title": "Overlapping Graph Clustering in Attributed Networks via Generalized Cluster Potential Game",
    "doi": "https://doi.org/10.1145/3597436",
    "publication_date": "2023-05-18",
    "publication_year": 2023,
    "authors": "Hui‐Jia Li; Yuhao Feng; Chengyi Xia; Jie Cao",
    "corresponding_authors": "",
    "abstract": "Overlapping graph clustering is essential to understand the nature and behavior of real complex systems including human interactions, technical systems and transportation network. However, in addition of topological structure, many real-world networked systems contain spare factors, i.e., attributes of networks. Despite the considerable efforts that have been made in graph clustering, they only concentrate on the topological structure, which lack a profound understanding of cluster configuration on attributed graphs. To address this great challenge, in this article, we propose a new overlapping graph clustering algorithm by integrating the topological and attributive information into a cluster potential game (CPG). Firstly, a generalized definition of the utility function is provided, which measures the payoff of each node based on different node-to-cluster distance functions. It is worth mentioning that the model we proposed is able to associate with the classic ordinal potential game well. Then, we define the measures of both tightness and the homogeneity in each cluster, and introduce a novel two-way selection mechanism. The goal is to extend the flexibility of the cluster potential game, so that one can achieve a win-win situation between nodes and clusters. Finally, a distributed and heterogeneous multiagent system (DHMAS) is carefully designed based on a fast self-learning algorithm (SLA) for attributed overlapping graph clustering. Two series of experiments are implemented in multi-types datasets and the results verify the effectiveness and the scalability after the comparison with the most advanced approaches of literature.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W4377021937",
    "type": "article"
  },
  {
    "title": "Group-Aware Graph Neural Network for Nationwide City Air Quality Forecasting",
    "doi": "https://doi.org/10.1145/3631713",
    "publication_date": "2023-11-04",
    "publication_year": 2023,
    "authors": "Ling Chen; Jiahui Xu; Binqing Wu; Jianlong Huang",
    "corresponding_authors": "",
    "abstract": "The problem of air pollution threatens public health. Air quality forecasting can provide the air quality index hours or even days later, which can help the public to prevent air pollution in advance. Previous works focus on citywide air quality forecasting and cannot solve nationwide city forecasting problem, whose difficulties lie in capturing the latent dependencies between geographically distant but highly correlated cities. In this paper, we propose the group-aware graph neural network (GAGNN), a hierarchical model for nationwide city air quality forecasting. The model constructs a city graph and a city group graph to model the spatial and latent dependencies between cities, respectively. GAGNN introduces differentiable grouping network to discover the latent dependencies among cities and generate city groups. Based on the generated city groups, a group correlation encoding module is introduced to learn the correlations between them, which can effectively capture the dependencies between city groups. After the graph construction, GAGNN implements message passing mechanism to model the dependencies between cities and city groups. The evaluation experiments on Chinese city air quality dataset indicate that our GAGNN outperforms existing forecasting models.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3197700565",
    "type": "article"
  },
  {
    "title": "Mobile User Traffic Generation Via Multi-Scale Hierarchical GAN",
    "doi": "https://doi.org/10.1145/3664655",
    "publication_date": "2024-05-10",
    "publication_year": 2024,
    "authors": "Tong Li; Shuodi Hui; Shiyuan Zhang; Huandong Wang; Yuheng Zhang; Pan Hui; Depeng Jin; Yong Li",
    "corresponding_authors": "",
    "abstract": "Mobile user traffic facilitates diverse applications, including network planning and optimization, whereas large-scale mobile user traffic is hardly available due to privacy concerns. One alternative solution is to generate mobile user traffic data for downstream applications. However, existing generation models cannot simulate the multi-scale temporal dynamics in mobile user traffic on individual and aggregate levels. In this work, we propose a multi-scale hierarchical generative adversarial network (MSH-GAN) containing multiple generators and a multi-class discriminator. Specifically, the mobile traffic usage behavior exhibits a mixture of multiple behavior patterns, which are called micro-scale behavior patterns and are modeled by different pattern generators in our model. Moreover, the traffic usage behavior of different users exhibits strong clustering characteristics, with the co-existence of users with similar and different traffic usage behaviors. Thus, we model each cluster of users as a class in the discriminator’s output, referred to as macro-scale user clusters. Then, the gap between micro-scale behavior patterns and macro-scale user clusters is bridged by introducing the switch mode generators, which describe the traffic usage behavior in switching between different patterns. All users share the pattern generators. In contrast, the switch mode generators are only shared by a specific cluster of users, which models the multi-scale hierarchical structure of the traffic usage behavior of massive users. Finally, we urge MSH-GAN to learn the multi-scale temporal dynamics via a combined loss function, including adversarial loss, clustering loss, aggregated loss, and regularity terms. Extensive experiment results demonstrate that MSH-GAN outperforms state-of-art baselines by at least 118.17% in critical data fidelity and usability metrics. Moreover, observations show that MSH-GAN can simulate traffic patterns and pattern switch behaviors.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4396796620",
    "type": "article"
  },
  {
    "title": "Bayesian Graph Local Extrema Convolution with Long-tail Strategy for Misinformation Detection",
    "doi": "https://doi.org/10.1145/3639408",
    "publication_date": "2024-01-03",
    "publication_year": 2024,
    "authors": "Guixian Zhang; Shichao Zhang; Guan Yuan",
    "corresponding_authors": "",
    "abstract": "It has become a cardinal task to identify fake information (misinformation) on social media, because it has significantly harmed the government and the public. There are many spam bots maliciously retweeting misinformation. This study proposes an efficient model for detecting misinformation with self-supervised contrastive learning. A B ayesian graph L ocal extrema C onvolution (BLC) is first proposed to aggregate node features in the graph structure. The BLC approach considers unreliable relationships and uncertainties in the propagation structure, and the differences between nodes and neighboring nodes are emphasized in the attributes. Then, a new long-tail strategy for matching long-tail users with the global social network is advocated to avoid over-concentration on high-degree nodes in graph neural networks. Finally, the proposed model is experimentally evaluated with two public Twitter datasets and demonstrates that the proposed long-tail strategy significantly improves the effectiveness of existing graph-based methods in terms of detecting misinformation. The robustness of BLC has also been examined on three graph datasets and demonstrates that it consistently outperforms traditional algorithms when perturbed by 15% of a dataset.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4390547902",
    "type": "article"
  },
  {
    "title": "Automatically Inspecting Thousands of Static Bug Warnings with Large Language Model: How Far Are We?",
    "doi": "https://doi.org/10.1145/3653718",
    "publication_date": "2024-03-26",
    "publication_year": 2024,
    "authors": "Cheng Wen; Yuandao Cai; Bin Zhang; Jie Su; Zhiwu Xu; Dugang Liu; Shengchao Qin; Zhong Ming; Cong Tian",
    "corresponding_authors": "",
    "abstract": "Static analysis tools for capturing bugs and vulnerabilities in software programs are widely employed in practice, as they have the unique advantages of high coverage and independence from the execution environment. However, existing tools for analyzing large codebases often produce a great deal of false warnings over genuine bug reports. As a result, developers are required to manually inspect and confirm each warning, a challenging, time-consuming, and automation-essential task. This article advocates a fast, general, and easily extensible approach called Llm4sa that automatically inspects a sheer volume of static warnings by harnessing (some of) the powers of Large Language Models (LLMs). Our key insight is that LLMs have advanced program understanding capabilities, enabling them to effectively act as human experts in conducting manual inspections on bug warnings with their relevant code snippets. In this spirit, we propose a static analysis to effectively extract the relevant code snippets via program dependence traversal guided by the bug warning reports themselves. Then, by formulating customized questions that are enriched with domain knowledge and representative cases to query LLMs, Llm4sa can remove a great deal of false warnings and facilitate bug discovery significantly. Our experiments demonstrate that Llm4sa is practical in automatically inspecting thousands of static warnings from Juliet benchmark programs and 11 real-world C/C++ projects, showcasing a high precision (81.13%) and a recall rate (94.64%) for a total of 9,547 bug warnings. Our research introduces new opportunities and methodologies for using the LLMs to reduce human labor costs, improve the precision of static analyzers, and ensure software trustworthiness",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4393191843",
    "type": "article"
  },
  {
    "title": "Enhancing Out-of-distribution Generalization on Graphs via Causal Attention Learning",
    "doi": "https://doi.org/10.1145/3644392",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Yongduo Sui; Wenyu Mao; Shuyao Wang; Xiang Wang; Jiancan Wu; Xiangnan He; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "In graph classification, attention- and pooling-based graph neural networks (GNNs) predominate to extract salient features from the input graph and support the prediction. They mostly follow the paradigm of “learning to attend,” which maximizes the mutual information between the attended graph and the ground-truth label. However, this paradigm causes GNN classifiers to indiscriminately absorb all statistical correlations between input features and labels in the training data without distinguishing the causal and noncausal effects of features. Rather than emphasizing causal features, the attended graphs tend to rely on noncausal features as shortcuts to predictions. These shortcut features may easily change outside the training distribution, thereby leading to poor generalization for GNN classifiers. In this article, we take a causal view on GNN modeling. Under our causal assumption, the shortcut feature serves as a confounder between the causal feature and prediction. It misleads the classifier into learning spurious correlations that facilitate prediction in in-distribution (ID) test evaluation while causing significant performance drop in out-of-distribution (OOD) test data. To address this issue, we employ the backdoor adjustment from causal theory—combining each causal feature with various shortcut features, to identify causal patterns and mitigate the confounding effect. Specifically, we employ attention modules to estimate the causal and shortcut features of the input graph. Then, a memory bank collects the estimated shortcut features, enhancing the diversity of shortcut features for combination. Simultaneously, we apply the prototype strategy to improve the consistency of intra-class causal features. We term our method as CAL+, which can promote stable relationships between causal estimation and prediction, regardless of distribution changes. Extensive experiments on synthetic and real-world OOD benchmarks demonstrate our method’s effectiveness in improving OOD generalization. Our codes are released at https://github.com/shuyao-wang/CAL-plus .",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4391568820",
    "type": "article"
  },
  {
    "title": "Concept Drift Adaptation by Exploiting Drift Type",
    "doi": "https://doi.org/10.1145/3638777",
    "publication_date": "2024-01-02",
    "publication_year": 2024,
    "authors": "Jinpeng Li; Hang Yu; Zhenyu Zhang; Xiangfeng Luo; Shaorong Xie",
    "corresponding_authors": "",
    "abstract": "Concept drift is a phenomenon where the distribution of data streams changes over time. When this happens, model predictions become less accurate. Hence, models built in the past need to be re-learned for the current data. Two design questions need to be addressed in designing a strategy to re-learn models: which type of concept drift has occurred, and how to utilize the drift type to improve re-learning performance. Existing drift detection methods are often good at determining when drift has occurred. However, few retrieve information about how the drift came to be present in the stream. Hence, determining the impact of the type of drift on adaptation is difficult. Filling this gap, we designed a framework based on a lazy strategy called Type-Driven Lazy Drift Adaptor (Type-LDA). Type-LDA first retrieves information about both how and when a drift has occurred, then it uses this information to re-learn the new model. To identify the type of drift, a drift type identifier is pre-trained on synthetic data of known drift types. Furthermore, a drift point locator locates the optimal point of drift via a sharing loss. Hence, Type-LDA can select the optimal point, according to the drift type, to re-learn the new model. Experiments validate Type-LDA on both synthetic data and real-world data, and the results show that accurately identifying drift type can improve adaptation accuracy.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4390490763",
    "type": "article"
  },
  {
    "title": "Mixed Graph Contrastive Network for Semi-supervised Node Classification",
    "doi": "https://doi.org/10.1145/3641549",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Xihong Yang; Yiqi Wang; Yue Liu; Yi Wen; Lingyuan Meng; Sihang Zhou; Xinwang Liu; En Zhu",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) have achieved promising performance in semi-supervised node classification in recent years. However, the problem of insufficient supervision, together with representation collapse, largely limits the performance of the GNNs in this field. To alleviate the collapse of node representations in semi-supervised scenario, we propose a novel graph contrastive learning method, termed M ixed G raph C ontrastive N etwork (MGCN). In our method, we improve the discriminative capability of the latent embeddings by an interpolation-based augmentation strategy and a correlation reduction mechanism. Specifically, we first conduct the interpolation-based augmentation in the latent space and then force the prediction model to change linearly between samples. Second, we enable the learned network to tell apart samples across two interpolation-perturbed views through forcing the correlation matrix across views to approximate an identity matrix. By combining the two settings, we extract rich supervision information from both the abundant unlabeled nodes and the rare yet valuable labeled nodes for discriminative representation learning. Extensive experimental results on six datasets demonstrate the effectiveness and the generality of MGCN compared to the existing state-of-the-art methods. The code of MGCN is available at https://github.com/xihongyang1999/MGCN on Github.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4391568866",
    "type": "article"
  },
  {
    "title": "Diffusion Models for Tabular Data Imputation and Synthetic Data Generation",
    "doi": "https://doi.org/10.1145/3742435",
    "publication_date": "2025-06-10",
    "publication_year": 2025,
    "authors": "Mario Villaizán-Vallelado; Matteo Salvatori; Carlos Segura; Ioannis Arapakis",
    "corresponding_authors": "",
    "abstract": "Data imputation and data generation have important applications across many domains where incomplete or missing data can hinder accurate analysis and decision-making. Diffusion models have emerged as powerful generative models capable of capturing complex data distributions across various data modalities such as image, audio, and time series. Recently, they have been also adapted to generate tabular data. In this paper, we propose a diffusion model for tabular data that introduces three key enhancements: (1) a conditioning attention mechanism, (2) an encoder-decoder transformer as the denoising network, and (3) dynamic masking. The conditioning attention mechanism is designed to improve the model's ability to capture the relationship between the condition and synthetic data. The transformer layers help model interactions within the condition (encoder) or synthetic data (decoder), while dynamic masking enables our model to efficiently handle both missing data imputation and synthetic data generation tasks within a unified framework. We conduct a comprehensive evaluation by comparing the performance of diffusion models with transformer conditioning against state-of-the-art techniques such as Variational Autoencoders, Generative Adversarial Networks and Diffusion Models, on benchmark datasets. Our evaluation focuses on the assessment of the generated samples with respect to three important criteria, namely: (1) Machine Learning efficiency, (2) statistical similarity, and (3) privacy risk mitigation. For the task of data imputation, we consider the efficiency of the generated samples across different levels of missing features. The results demonstrates average superior machine learning efficiency and statistical accuracy compared to the baselines, while maintaining privacy risks at a comparable level, particularly showing increased performance in datasets with a large number of features. By conditioning the data generation on a desired target variable, the model can mitigate systemic biases, generate augmented datasets to address data imbalance issues, and improve data quality for subsequent analysis. This has significant implications for domains such as healthcare and finance, where accurate, unbiased, and privacy-preserving data are critical for informed decision-making and fair model outcomes.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4411175984",
    "type": "article"
  },
  {
    "title": "Stream data clustering based on grid density and attraction",
    "doi": "https://doi.org/10.1145/1552303.1552305",
    "publication_date": "2009-07-01",
    "publication_year": 2009,
    "authors": "Li Tu; Yixin Chen",
    "corresponding_authors": "",
    "abstract": "Clustering real-time stream data is an important and challenging problem. Existing algorithms such as CluStream are based on the k -means algorithm. These clustering algorithms have difficulties finding clusters of arbitrary shapes and handling outliers. Further, they require the knowledge of k and user-specified time window. To address these issues, this article proposes D-Stream , a framework for clustering stream data using a density-based approach. Our algorithm uses an online component that maps each input data record into a grid and an offline component that computes the grid density and clusters the grids based on the density. The algorithm adopts a density decaying technique to capture the dynamic changes of a data stream and a attraction-based mechanism to accurately generate cluster boundaries. Exploiting the intricate relationships among the decay factor, attraction, data density, and cluster structure, our algorithm can efficiently and effectively generate and adjust the clusters in real time. Further, a theoretically sound technique is developed to detect and remove sporadic grids mapped by outliers in order to dramatically improve the space and time efficiency of the system. The technique makes high-speed data stream clustering feasible without degrading the clustering quality. The experimental results show that our algorithm has superior quality and efficiency, can find clusters of arbitrary shapes, and can accurately recognize the evolving behaviors of real-time data streams.",
    "cited_by_count": 131,
    "openalex_id": "https://openalex.org/W2055902670",
    "type": "article"
  },
  {
    "title": "Closed patterns meet <i>n</i> -ary relations",
    "doi": "https://doi.org/10.1145/1497577.1497580",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Loïc Cerf; Jérémy Besson; Céline Robardet; Jean‐François Boulicaut",
    "corresponding_authors": "",
    "abstract": "Set pattern discovery from binary relations has been extensively studied during the last decade. In particular, many complete and efficient algorithms for frequent closed set mining are now available. Generalizing such a task to n -ary relations ( n ≥ 2) appears as a timely challenge. It may be important for many applications, for example, when adding the time dimension to the popular objects × features binary case. The generality of the task (no assumption being made on the relation arity or on the size of its attribute domains) makes it computationally challenging. We introduce an algorithm called Data-Peeler. From an n -ary relation, it extracts all closed n -sets satisfying given piecewise (anti) monotonic constraints. This new class of constraints generalizes both monotonic and antimonotonic constraints. Considering the special case of ternary relations, Data-Peeler outperforms the state-of-the-art algorithms CubeMiner and Trias by orders of magnitude. These good performances must be granted to a new clever enumeration strategy allowing to efficiently enforce the closeness property. The relevance of the extracted closed n -sets is assessed on real-life 3-and 4-ary relations. Beyond natural 3-or 4-ary relations, expanding a relation with an additional attribute can help in enforcing rather abstract constraints such as the robustness with respect to binarization. Furthermore, a collection of closed n -sets is shown to be an excellent starting point to compute a tiling of the dataset.",
    "cited_by_count": 124,
    "openalex_id": "https://openalex.org/W2046273500",
    "type": "article"
  },
  {
    "title": "Learning to predict reciprocity and triadic closure in social networks",
    "doi": "https://doi.org/10.1145/2499907.2499908",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Tiancheng Lou; Jie Tang; John E. Hopcroft; Zhanpeng Fang; Xiaowen Ding",
    "corresponding_authors": "",
    "abstract": "We study how links are formed in social networks. In particular, we focus on investigating how a reciprocal (two-way) link, the basic relationship in social networks, is developed from a parasocial (one-way) relationship and how the relationships further develop into triadic closure, one of the fundamental processes of link formation. We first investigate how geographic distance and interactions between users influence the formation of link structure among users. Then we study how social theories including homophily, social balance, and social status are satisfied over networks with parasocial and reciprocal relationships. The study unveils several interesting phenomena. For example, “friend's friend is a friend” indeed exists in the reciprocal relationship network, but does not hold in the parasocial relationship network. We propose a learning framework to formulate the problems of predicting reciprocity and triadic closure into a graphical model. We demonstrate that it is possible to accurately infer 90% of reciprocal relationships in a Twitter network. The proposed model also achieves better performance (+20--30% in terms of F1-measure) than several alternative methods for predicting the triadic closure formation.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W1994618970",
    "type": "article"
  },
  {
    "title": "DOLPHIN",
    "doi": "https://doi.org/10.1145/1497577.1497581",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Fabrizio Angiulli; Fabio Fassetti",
    "corresponding_authors": "",
    "abstract": "In this work a novel distance-based outlier detection algorithm, named DOLPHIN, working on disk-resident datasets and whose I/O cost corresponds to the cost of sequentially reading the input dataset file twice, is presented. It is both theoretically and empirically shown that the main memory usage of DOLPHIN amounts to a small fraction of the dataset and that DOLPHIN has linear time performance with respect to the dataset size. DOLPHIN gains efficiency by naturally merging together in a unified schema three strategies, namely the selection policy of objects to be maintained in main memory, usage of pruning rules, and similarity search techniques. Importantly, similarity search is accomplished by the algorithm without the need of preliminarily indexing the whole dataset, as other methods do. The algorithm is simple to implement and it can be used with any type of data, belonging to either metric or nonmetric spaces. Moreover, a modification to the basic method allows DOLPHIN to deal with the scenario in which the available buffer of main memory is smaller than its standard requirements. DOLPHIN has been compared with state-of-the-art distance-based outlier detection algorithms, showing that it is much more efficient.",
    "cited_by_count": 102,
    "openalex_id": "https://openalex.org/W2007869097",
    "type": "article"
  },
  {
    "title": "Topic taxonomy adaptation for group profiling",
    "doi": "https://doi.org/10.1145/1324172.1324173",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Lei Tang; Huan Liu; Jianping Zhang; Nitin Agarwal; John Salerno",
    "corresponding_authors": "",
    "abstract": "A topic taxonomy is an effective representation that describes salient features of virtual groups or online communities. A topic taxonomy consists of topic nodes. Each internal node is defined by its vertical path (i.e., ancestor and child nodes) and its horizonal list of attributes (or terms). In a text-dominant environment, a topic taxonomy can be used to flexibly describe a group's interests with varying granularity. However, the stagnant nature of a taxonomy may fail to timely capture the dynamic change of a group's interest. This article addresses the problem of how to adapt a topic taxonomy to the accumulated data that reflects the change of a group's interest to achieve dynamic group profiling. We first discuss the issues related to topic taxonomy. We next formulate taxonomy adaptation as an optimization problem to find the taxonomy that best fits the data. We then present a viable algorithm that can efficiently accomplish taxonomy adaptation. We conduct extensive experiments to evaluate our approach's efficacy for group profiling, compare the approach with some alternatives, and study its performance for dynamic group profiling. While pointing out various applications of taxonomy adaption, we suggest some future work that can take advantage of burgeoning Web 2.0 services for online targeted marketing, counterterrorism in connecting dots, and community tracking.",
    "cited_by_count": 99,
    "openalex_id": "https://openalex.org/W2018910044",
    "type": "article"
  },
  {
    "title": "A Modular Machine Learning System for Flow-Level Traffic Classification in Large Networks",
    "doi": "https://doi.org/10.1145/2133360.2133364",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Yu Jin; Nick Duffield; Jeffrey Erman; Patrick Haffner; Subhabrata Sen; Zhili Zhang",
    "corresponding_authors": "",
    "abstract": "The ability to accurately and scalably classify network traffic is of critical importance to a wide range of management tasks of large networks, such as tier-1 ISP networks and global enterprise networks. Guided by the practical constraints and requirements of traffic classification in large networks, in this article, we explore the design of an accurate and scalable machine learning based flow-level traffic classification system, which is trained on a dataset of flow-level data that has been annotated with application protocol labels by a packet-level classifier. Our system employs a lightweight modular architecture , which combines a series of simple linear binary classifiers, each of which can be efficiently implemented and trained on vast amounts of flow data in parallel, and embraces three key innovative mechanisms, weighted threshold sampling, logistic calibration , and intelligent data partitioning , to achieve scalability while attaining high accuracy. Evaluations using real traffic data from multiple locations in a large ISP show that our system accurately reproduces the labels of the packet level classifier when runs on (unlabeled) flow records, while meeting the scalability and stability requirements of large ISP networks. Using training and test datasets that are two months apart and collected from two different locations, the flow error rates are only 3% for TCP flows and 0.4% for UDP flows. We further show that such error rates can be reduced by combining the information of spatial distributions of flows, or collective traffic statistics , during classification. We propose a novel two-step model, which seamlessly integrates these collective traffic statistics into the existing traffic classification system. Experimental results display performance improvement on all traffic classes and an overall error rate reduction by 15%. In addition to a high accuracy, at runtime, our implementation easily scales to classify traffic on 10Gbps links.",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W2027355980",
    "type": "article"
  },
  {
    "title": "Triangle listing in massive networks",
    "doi": "https://doi.org/10.1145/2382577.2382581",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Shumo Chu; James Cheng",
    "corresponding_authors": "",
    "abstract": "Triangle listing is one of the fundamental algorithmic problems whose solution has numerous applications especially in the analysis of complex networks, such as the computation of clustering coefficients, transitivity, triangular connectivity, trusses, etc. Existing algorithms for triangle listing are mainly in-memory algorithms, whose performance cannot scale with the massive volume of today's fast growing networks. When the input graph cannot fit in main memory, triangle listing requires random disk accesses that can incur prohibitively huge I/O cost. Some streaming, semistreaming, and sampling algorithms have been proposed but these are approximation algorithms. We propose an I/O-efficient algorithm for triangle listing. Our algorithm is exact and avoids random disk access. Our results show that our algorithm is scalable and outperforms the state-of-the-art in-memory and local triangle estimation algorithms.",
    "cited_by_count": 95,
    "openalex_id": "https://openalex.org/W2030088585",
    "type": "article"
  },
  {
    "title": "Efficiently Estimating Motif Statistics of Large Networks",
    "doi": "https://doi.org/10.1145/2629564",
    "publication_date": "2014-09-23",
    "publication_year": 2014,
    "authors": "Pinghui Wang; John C. S. Lui; Bruno Ribeiro; Don Towsley; Junzhou Zhao; Xiaohong Guan",
    "corresponding_authors": "",
    "abstract": "Exploring statistics of locally connected subgraph patterns (also known as network motifs) has helped researchers better understand the structure and function of biological and Online Social Networks (OSNs). Nowadays, the massive size of some critical networks—often stored in already overloaded relational databases—effectively limits the rate at which nodes and edges can be explored, making it a challenge to accurately discover subgraph statistics. In this work, we propose sampling methods to accurately estimate subgraph statistics from as few queried nodes as possible. We present sampling algorithms that efficiently and accurately estimate subgraph properties of massive networks. Our algorithms require no precomputation or complete network topology information. At the same time, we provide theoretical guarantees of convergence. We perform experiments using widely known datasets and show that, for the same accuracy, our algorithms require an order of magnitude less queries (samples) than the current state-of-the-art algorithms.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W2089418726",
    "type": "article"
  },
  {
    "title": "GLAD",
    "doi": "https://doi.org/10.1145/2811268",
    "publication_date": "2015-10-26",
    "publication_year": 2015,
    "authors": "Rose Yu; Xinran He; Yan Liu",
    "corresponding_authors": "",
    "abstract": "Traditional anomaly detection on social media mostly focuses on individual point anomalies while anomalous phenomena usually occur in groups. Therefore, it is valuable to study the collective behavior of individuals and detect group anomalies. Existing group anomaly detection approaches rely on the assumption that the groups are known, which can hardly be true in real world social media applications. In this article, we take a generative approach by proposing a hierarchical Bayes model: Group Latent Anomaly Detection (GLAD) model. GLAD takes both pairwise and point-wise data as input, automatically infers the groups and detects group anomalies simultaneously. To account for the dynamic properties of the social media data, we further generalize GLAD to its dynamic extension d-GLAD. We conduct extensive experiments to evaluate our models on both synthetic and real world datasets. The empirical results demonstrate that our approach is effective and robust in discovering latent groups and detecting group anomalies.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W2015172091",
    "type": "article"
  },
  {
    "title": "User Identification Across Social Media",
    "doi": "https://doi.org/10.1145/2747880",
    "publication_date": "2015-10-12",
    "publication_year": 2015,
    "authors": "Reza Zafarani; Lei Tang; Huan Liu",
    "corresponding_authors": "",
    "abstract": "People use various social media sites for different purposes. The information on each site is often partial. When sources of complementary information are integrated, a better profile of a user can be built. This profile can help improve online services such as advertising across sites. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem. We provide evidence on the existence of a mapping among identities of individuals across social media sites, study the feasibility of finding this mapping, and illustrate and develop means for finding this mapping. Our studies show that effective approaches that exploit information redundancies due to users’ unique behavioral patterns can be utilized to find such a mapping. This study paves the way for analysis and mining across social networking sites, and facilitates the creation of novel online services across sites. In particular, recommending friends and advertising across networks, analyzing information diffusion across sites, and studying specific user behavior such as user migration across sites in social media are one of the many areas that can benefit from the results of this study.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2021999013",
    "type": "article"
  },
  {
    "title": "Improving Top-N Recommendation for Cold-Start Users via Cross-Domain Information",
    "doi": "https://doi.org/10.1145/2724720",
    "publication_date": "2015-06-01",
    "publication_year": 2015,
    "authors": "Nima Mirbakhsh; Charles X. Ling",
    "corresponding_authors": "",
    "abstract": "Making accurate recommendations for cold-start users is a challenging yet important problem in recommendation systems. Including more information from other domains is a natural solution to improve the recommendations. However, most previous work in cross-domain recommendations has focused on improving prediction accuracy with several severe limitations. In this article, we extend our previous work on clustering-based matrix factorization in single domains into cross domains. In addition, we utilize recent results on unobserved ratings. Our new method can more effectively utilize data from auxiliary domains to achieve better recommendations, especially for cold-start users. For example, our method improves the recall to 21% on average for cold-start users, whereas previous methods result in only 15% recall in the cross-domain Amazon dataset. We also observe almost the same improvements in the Epinions dataset. Considering that it is often difficult to make even a small improvement in recommendations, for cold- start users in particular, our result is quite significant.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2400248106",
    "type": "article"
  },
  {
    "title": "Local Spectral Clustering for Overlapping Community Detection",
    "doi": "https://doi.org/10.1145/3106370",
    "publication_date": "2018-01-10",
    "publication_year": 2018,
    "authors": "Yixuan Li; Kun He; Kyle Kloster; David Bindel; John E. Hopcroft",
    "corresponding_authors": "",
    "abstract": "Large graphs arise in a number of contexts and understanding their structure and extracting information from them is an important research area. Early algorithms for mining communities have focused on global graph structure, and often run in time proportional to the size of the entire graph. As we explore networks with millions of vertices and find communities of size in the hundreds, it becomes important to shift our attention from macroscopic structure to microscopic structure in large networks. A growing body of work has been adopting local expansion methods in order to identify communities from a few exemplary seed members. In this article, we propose a novel approach for finding overlapping communities called L emon ( L ocal E xpansion via M inimum O ne N orm). Provided with a few known seeds , the algorithm finds the community by performing a local spectral diffusion. The core idea of L emon is to use short random walks to approximate an invariant subspace near a seed set, which we refer to as local spectra . Local spectra can be viewed as the low-dimensional embedding that captures the nodes’ closeness in the local network structure. We show that L emon ’s performance in detecting communities is competitive with state-of-the-art methods. Moreover, the running time scales with the size of the community rather than that of the entire graph. The algorithm is easy to implement and is highly parallelizable. We further provide theoretical analysis of the local spectral properties, bounding the measure of tightness of extracted community using the eigenvalues of graph Laplacian. We thoroughly evaluate our approach using both synthetic and real-world datasets across different domains, and analyze the empirical variations when applying our method to inherently different networks in practice. In addition, the heuristics on how the seed set quality and quantity would affect the performance are provided.",
    "cited_by_count": 82,
    "openalex_id": "https://openalex.org/W2783779423",
    "type": "article"
  },
  {
    "title": "An Influence Propagation View of PageRank",
    "doi": "https://doi.org/10.1145/3046941",
    "publication_date": "2017-03-21",
    "publication_year": 2017,
    "authors": "Qi Liu; Biao Xiang; Nicholas Jing Yuan; Enhong Chen; Hui Xiong; Zheng Yi; Yang Yu",
    "corresponding_authors": "",
    "abstract": "For a long time, PageRank has been widely used for authority computation and has been adopted as a solid baseline for evaluating social influence related applications. However, when measuring the authority of network nodes, the traditional PageRank method does not take the nodes’ prior knowledge into consideration. Also, the connection between PageRank and social influence modeling methods is not clearly established. To that end, this article provides a focused study on understanding PageRank as well as the relationship between PageRank and social influence analysis. Along this line, we first propose a linear social influence model and reveal that this model generalizes the PageRank-based authority computation by introducing some constraints. Then, we show that the authority computation by PageRank can be enhanced if exploiting more reasonable constraints (e.g., from prior knowledge). Next, to deal with the computational challenge of linear model with general constraints, we provide an upper bound for identifying nodes with top authorities. Moreover, we extend the proposed linear model for better measuring the authority of the given node sets, and we also demonstrate the way to quickly identify the top authoritative node sets. Finally, extensive experimental evaluations on four real-world networks validate the effectiveness of the proposed linear model with respect to different constraint settings. The results show that the methods with more reasonable constraints can lead to better ranking and recommendation performance. Meanwhile, the upper bounds formed by PageRank values could be used to quickly locate the nodes and node sets with the highest authorities.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2602756700",
    "type": "article"
  },
  {
    "title": "ABRA",
    "doi": "https://doi.org/10.1145/3208351",
    "publication_date": "2018-07-20",
    "publication_year": 2018,
    "authors": "Matteo Riondato; Eli Upfal",
    "corresponding_authors": "",
    "abstract": "ABPA Ξ A Σ ( ABRAXAS ): Gnostic word of mystic meaning . We present ABRA, a suite of algorithms to compute and maintain probabilistically guaranteed high-quality approximations of the betweenness centrality of all nodes (or edges) on both static and fully dynamic graphs. Our algorithms use progressive random sampling and their analysis rely on Rademacher averages and pseudodimension, fundamental concepts from statistical learning theory. To our knowledge, ABRA is the first application of these concepts to the field of graph analysis. Our experimental results show that ABRA is much faster than exact methods, and vastly outperforms, in both runtime number of samples, and accuracy, state-of-the-art algorithms with the same quality guarantees.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2884593183",
    "type": "article"
  },
  {
    "title": "Graph-Based Fraud Detection in the Face of Camouflage",
    "doi": "https://doi.org/10.1145/3056563",
    "publication_date": "2017-06-29",
    "publication_year": 2017,
    "authors": "Bryan Hooi; Kijung Shin; Hyun Ah Song; Alex Beutel; Neil Shah; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows? Existing fraud detection methods (spectral, etc.) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph. Fraudsters can evade these methods using camouflage , by adding reviews or follows with honest targets so that they look “normal.” Even worse, some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic. Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. We propose FRAUDAR, an algorithm that (a) is camouflage resistant, (b) provides upper bounds on the effectiveness of fraudsters, and (c) is effective in real-world data. Experimental results under various attacks show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud. Additionally, in real-world experiments with a Twitter follower--followee graph of 1.47 billion edges, FRAUDAR successfully detected a subgraph of more than 4, 000 detected accounts, of which a majority had tweets showing that they used follower-buying services.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W2730363704",
    "type": "article"
  },
  {
    "title": "Uncovering Hierarchical and Overlapping Communities with a Local-First Approach",
    "doi": "https://doi.org/10.1145/2629511",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "Michele Coscia; Giulio Rossetti; Fosca Giannotti; Dino Pedreschi",
    "corresponding_authors": "",
    "abstract": "Community discovery in complex networks is the task of organizing a network’s structure by grouping together nodes related to each other. Traditional approaches are based on the assumption that there is a global-level organization in the network. However, in many scenarios, each node is the bearer of complex information and cannot be classified in disjoint clusters. The top-down global view of the partition approach is not designed for this. Here, we represent this complex information as multiple latent labels, and we postulate that edges in the networks are created among nodes carrying similar labels. The latent labels are the communities a node belongs to and we discover them with a simple local-first approach to community discovery. This is achieved by democratically letting each node vote for the communities it sees surrounding it in its limited view of the global system, its ego neighborhood, using a label propagation algorithm, assuming that each node is aware of the label it shares with each of its connections. The local communities are merged hierarchically, unveiling the modular organization of the network at the global level and identifying overlapping groups and groups of groups. We tested this intuition against the state-of-the-art overlapping community discovery and found that our new method advances in the chosen scenarios in the quality of the obtained communities. We perform a test on benchmark and on real-world networks, evaluating the quality of the community coverage by using the extracted communities to predict the metadata attached to the nodes, which we consider external information about the latent labels. We also provide an explanation about why real-world networks contain overlapping communities and how our logic is able to capture them. Finally, we show how our method is deterministic, is incremental, and has a limited time complexity, so that it can be used on real-world scale networks.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W2114532600",
    "type": "article"
  },
  {
    "title": "MDL4BMF",
    "doi": "https://doi.org/10.1145/2601437",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Pauli Miettinen; Jilles Vreeken",
    "corresponding_authors": "",
    "abstract": "Matrix factorizations—where a given data matrix is approximated by a product of two or more factor matrices—are powerful data mining tools. Among other tasks, matrix factorizations are often used to separate global structure from noise. This, however, requires solving the “model order selection problem” of determining the proper rank of the factorization, that is, to answer where fine-grained structure stops, and where noise starts. Boolean Matrix Factorization (BMF)—where data, factors, and matrix product are Boolean—has in recent years received increased attention from the data mining community. The technique has desirable properties, such as high interpretability and natural sparsity. Yet, so far no method for selecting the correct model order for BMF has been available. In this article, we propose the use of the Minimum Description Length (MDL) principle for this task. Besides solving the problem, this well-founded approach has numerous benefits; for example, it is automatic, does not require a likelihood function, is fast, and, as experiments show, is highly accurate. We formulate the description length function for BMF in general—making it applicable for any BMF algorithm. We discuss how to construct an appropriate encoding: starting from a simple and intuitive approach, we arrive at a highly efficient data-to-model--based encoding for BMF. We extend an existing algorithm for BMF to use MDL to identify the best Boolean matrix factorization, analyze the complexity of the problem, and perform an extensive experimental evaluation to study its behavior.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2086325844",
    "type": "article"
  },
  {
    "title": "Systematic Review of Clustering High-Dimensional and Large Datasets",
    "doi": "https://doi.org/10.1145/3132088",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Divya Pandove; Shivan Goel; Rinkl Rani",
    "corresponding_authors": "",
    "abstract": "Technological advancement has enabled us to store and process huge amount of data in relatively short spans of time. The nature of data is rapidly changing, particularly its dimensionality is more commonly multi- and high-dimensional. There is an immediate need to expand our focus to include analysis of high-dimensional and large datasets. Data analysis is becoming a mammoth task, due to incremental increase in data volume and complexity in terms of heterogony of data. It is due to this dynamic computing environment that the existing techniques either need to be modified or discarded to handle new data in multiple high-dimensions. Data clustering is a tool that is used in many disciplines, including data mining, so that meaningful knowledge can be extracted from seemingly unstructured data. The aim of this article is to understand the problem of clustering and various approaches addressing this problem. This article discusses the process of clustering from both microviews (data treating) and macroviews (overall clustering process). Different distance and similarity measures, which form the cornerstone of effective data clustering, are also identified. Further, an in-depth analysis of different clustering approaches focused on data mining, dealing with large-scale datasets is given. These approaches are comprehensively compared to bring out a clear differentiation among them. This article also surveys the problem of high-dimensional data and the existing approaches, that makes it more relevant. It also explores the latest trends in cluster analysis, and the real-life applications of this concept. This survey is exhaustive as it tries to cover all the aspects of clustering in the field of data mining.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2784378868",
    "type": "article"
  },
  {
    "title": "Emerging Trends in Personality Identification Using Online Social Networks—A Literature Survey",
    "doi": "https://doi.org/10.1145/3070645",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Vishal Kaushal; Manasi Patwardhan",
    "corresponding_authors": "",
    "abstract": "Personality is a combination of all the attributes—behavioral, temperamental, emotional, and mental—that characterizes a unique individual. Ability to identify personalities of people has always been of great interest to the researchers due to its importance. It continues to find highly useful applications in many domains. Owing to the increasing popularity of online social networks, researchers have started looking into the possibility of predicting a user's personality from his online social networking profile, which serves as a rich source of textual as well as non-textual content published by users. In the process of creating social networking profiles, users reveal a lot about themselves both in what they share and how they say it. Studies suggest that the online social networking websites are, in fact, a relevant and valid means of communicating personality. In this article, we review these various studies reported in literature toward identification of personality using online social networks. To the best of our knowledge, this is the first reported survey of its kind at the time of submission. We hope that our contribution, especially in summarizing the previous findings and in identifying the directions for future research in this area, would encourage researchers to do more work in this budding area.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2784934784",
    "type": "article"
  },
  {
    "title": "Greedily Improving Our Own Closeness Centrality in a Network",
    "doi": "https://doi.org/10.1145/2953882",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Pierluigi Crescenzi; Gianlorenzo D’Angelo; Lorenzo Severini; Yllka Velaj",
    "corresponding_authors": "",
    "abstract": "The closeness centrality is a well-known measure of importance of a vertex within a given complex network. Having high closeness centrality can have positive impact on the vertex itself: hence, in this paper we consider the optimization problem of determining how much a vertex can increase its centrality by creating a limited amount of new edges incident to it. We will consider both the undirected and the directed graph cases. In both cases, we first prove that the optimization problem does not admit a polynomial-time approximation scheme (unless P = NP ), and then propose a greedy approximation algorithm (with an almost tight approximation ratio), whose performance is then tested on synthetic graphs and real-world networks.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2484873874",
    "type": "article"
  },
  {
    "title": "Taxonomy and Evaluation for Microblog Popularity Prediction",
    "doi": "https://doi.org/10.1145/3301303",
    "publication_date": "2019-03-13",
    "publication_year": 2019,
    "authors": "Xiaofeng Gao; Zhenhao Cao; Sha Li; Bin Yao; Guihai Chen; Shaojie Tang",
    "corresponding_authors": "",
    "abstract": "As social networks become a major source of information, predicting the outcome of information diffusion has appeared intriguing to both researchers and practitioners. By organizing and categorizing the joint efforts of numerous studies on popularity prediction, this article presents a hierarchical taxonomy and helps to establish a systematic overview of popularity prediction methods for microblog. Specifically, we uncover three lines of thoughts: the feature-based approach, time-series modelling, and the collaborative filtering approach and analyse them, respectively. Furthermore, we also categorize prediction methods based on their underlying rationale: whether they attempt to model the motivation of users or monitor the early responses. Finally, we put these prediction methods to test by performing experiments on real-life data collected from popular social networks Twitter and Weibo. We compare the methods in terms of accuracy, efficiency, timeliness, robustness, and bias. As far as we are concerned, there is no precedented survey aimed at microblog popularity prediction at the time of submission. By establishing a taxonomy and evaluation for the first time, we hope to provide an in-depth review of state-of-the-art prediction methods and point out directions for further research. Our evaluations show that time-series modelling has the advantage of high accuracy and the ability to improve over time. The feature-based methods using only temporal features performs nearly as well as using all possible features, producing average results. This suggests that temporal features do have strong predictive power and that power is better exploited with time-series models. On the other hand, this implies that we know little about the future popularity of an item before it is posted, which may be the focus of further research.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2921701073",
    "type": "article"
  },
  {
    "title": "On Proximity and Structural Role-based Embeddings in Networks",
    "doi": "https://doi.org/10.1145/3397191",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Ryan A. Rossi; Di Jin; Sungchul Kim; Nesreen K. Ahmed; Danai Koutra; John Boaz Lee",
    "corresponding_authors": "",
    "abstract": "Structural roles define sets of structurally similar nodes that are more similar to nodes inside the set than outside, whereas communities define sets of nodes with more connections inside the set than outside. Roles based on structural similarity and communities based on proximity are fundamentally different but important complementary notions. Recently, the notion of structural roles has become increasingly important and has gained a lot of attention due to the proliferation of work on learning representations (node/edge embeddings) from graphs that preserve the notion of roles. Unfortunately, recent work has sometimes confused the notion of structural roles and communities (based on proximity) leading to misleading or incorrect claims about the capabilities of network embedding methods. As such, this article seeks to clarify the misconceptions and key differences between structural roles and communities, and formalize the general mechanisms (e.g., random walks and feature diffusion) that give rise to community- or role-based structural embeddings. We theoretically prove that embedding methods based on these mechanisms result in either community- or role-based structural embeddings. These mechanisms are typically easy to identify and can help researchers quickly determine whether a method preserves community- or role-based embeddings. Furthermore, they also serve as a basis for developing new and improved methods for community- or role-based structural embeddings. Finally, we analyze and discuss applications and data characteristics where community- or role-based embeddings are most appropriate.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W4288080007",
    "type": "article"
  },
  {
    "title": "Shop-Type Recommendation Leveraging the Data from Social Media and Location-Based Services",
    "doi": "https://doi.org/10.1145/2930671",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Zhiwen Yu; Miao Tian; Zhu Wang; Bin Guo; Tao Mei",
    "corresponding_authors": "",
    "abstract": "It is an important yet challenging task for investors to determine the most suitable type of shop (e.g., restaurant, fashion) for a newly opened store. Traditional ways are predominantly field surveys and empirical estimation, which are not effective as they lack shop-related data. As social media and location-based services (LBS) are becoming more and more pervasive, user-generated data from these platforms are providing rich information not only about individual consumption experiences, but also about shop attributes. In this paper, we investigate the recommendation of shop types for a given location, by leveraging heterogeneous data that are mainly historical user preferences and location context from social media and LBS. Our goal is to select the most suitable shop type, seeking to maximize the number of customers served from a candidate set of types. We propose a novel bias learning matrix factorization method with feature fusion for shop popularity prediction. Features are defined and extracted from two perspectives: location, where features are closely related to location characteristics, and commercial, where features are about the relationships between shops in the neighborhood. Experimental results show that the proposed method outperforms state-of-the-art solutions.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2479352434",
    "type": "article"
  },
  {
    "title": "Rumor Blocking through Online Link Deletion on Social Networks",
    "doi": "https://doi.org/10.1145/3301302",
    "publication_date": "2019-03-13",
    "publication_year": 2019,
    "authors": "Ruidong Yan; Yi Li; Weili Wu; Deying Li; Yongcai Wang",
    "corresponding_authors": "",
    "abstract": "In recent years, social networks have become important platforms for people to disseminate information. However, we need to take effective measures such as blocking a set of links to control the negative rumors spreading over the network. In this article, we propose a Rumor Spread Minimization (RSM) problem, i.e., we remove an edge set from network such that the rumor spread is minimized. We first prove the objective function of RSM problem is not submodular. Then, we propose both submodular lower-bound and upper-bound of the objective function. Next, we develop a heuristic algorithm to approximate the objective function. Furthermore, we reformulate our objective function as the DS function (the Difference of Submodular functions). Finally, we conduct experiments on real-world datasets to evaluate our proposed method. The experiment results show that the upper and lower bounds are very close, which indicates the good quality of them. And, the proposed method outperforms the comparison methods.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2922034766",
    "type": "article"
  },
  {
    "title": "Aspect Aware Learning for Aspect Category Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3350487",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Peisong Zhu; Zhuang Chen; Haojie Zheng; Tieyun Qian",
    "corresponding_authors": "",
    "abstract": "Aspect category sentiment analysis (ACSA) is an underexploited subtask in aspect level sentiment analysis. It aims to identify the sentiment of predefined aspect categories. The main challenge in ACSA comes from the fact that the aspect category may not occur in the sentence in most of the cases. For example, the review “ they have delicious sandwiches ” positively talks about the aspect category “ food ” in an implicit manner. In this article, we propose a novel aspect aware learning (AAL) framework for ACSA tasks. Our key idea is to exploit the interaction between the aspect category and the contents under the guidance of both sentiment polarity and predefined categories. To this end, we design a two-way memory network for integrating AAL into the framework of sentiment classification. We further present two algorithms to incorporate the potential impacts of aspect categories. One is to capture the correlations between aspect terms and the aspect category like “sandwiches” and “food.” The other is to recognize the aspect category for sentiment representations like “food” for “delicious.” We conduct extensive experiments on four SemEval datasets. The results reveal the essential role of AAL in ACSA by achieving the state-of-the-art performance.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2980958805",
    "type": "article"
  },
  {
    "title": "Story Forest",
    "doi": "https://doi.org/10.1145/3377939",
    "publication_date": "2020-05-13",
    "publication_year": 2020,
    "authors": "Bang Liu; Fred X. Han; Di Niu; Linglong Kong; Kunfeng Lai; Yu Xu",
    "corresponding_authors": "",
    "abstract": "Extracting events accurately from vast news corpora and organize events logically is critical for news apps and search engines, which aim to organize news information collected from the Internet and present it to users in the most sensible forms. Intuitively speaking, an event is a group of news documents that report the same news incident possibly in different ways. In this article, we describe our experience of implementing a news content organization system at Tencent to discover events from vast streams of breaking news and to evolve news story structures in an online fashion. Our real-world system faces unique challenges in contrast to previous studies on topic detection and tracking (TDT) and event timeline or graph generation, in that we (1) need to accurately and quickly extract distinguishable events from massive streams of long text documents, and (2) must develop the structures of event stories in an online manner, in order to guarantee a consistent user viewing experience. In solving these challenges, we propose Story Forest , a set of online schemes that automatically clusters streaming documents into events, while connecting related events in growing trees to tell evolving stories. A core novelty of our Story Forest system is EventX , a semi-supervised scheme to extract events from massive Internet news corpora. EventX relies on a two-layered, graph-based clustering procedure to group documents into fine-grained events. We conducted extensive evaluations based on (1) 60 GB of real-world Chinese news data, (2) a large Chinese Internet news dataset that contains 11,748 news articles with truth event labels, and (3) the 20 News Groups English dataset, through detailed pilot user experience studies. The results demonstrate the superior capabilities of Story Forest to accurately identify events and organize news text into a logical structure that is appealing to human readers.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W3027864066",
    "type": "article"
  },
  {
    "title": "An Improved KNN-Based Efficient Log Anomaly Detection Method with Automatically Labeled Samples",
    "doi": "https://doi.org/10.1145/3441448",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Shi Ying; Bingming Wang; Lu Wang; Qingshan Li; Yishi Zhao; Jianga Shang; Hao Huang; Guoli Cheng; Zhe Yang; Jiangyi Geng",
    "corresponding_authors": "",
    "abstract": "Logs that record system abnormal states (anomaly logs) can be regarded as outliers, and the k-Nearest Neighbor (kNN) algorithm has relatively high accuracy in outlier detection methods. Therefore, we use the kNN algorithm to detect anomalies in the log data. However, there are some problems when using the kNN algorithm to detect anomalies, three of which are: excessive vector dimension leads to inefficient kNN algorithm, unlabeled log data cannot support the kNN algorithm, and the imbalance of the number of log data distorts the classification decision of kNN algorithm. In order to solve these three problems, we propose an efficient log anomaly detection method based on an improved kNN algorithm with an automatically labeled sample set. This method first proposes a log parsing method based on N-gram and frequent pattern mining (FPM) method, which reduces the dimension of the log vector converted with Term frequency.Inverse Document Frequency (TF-IDF) technology. Then we use clustering and self-training method to get labeled log data sample set from historical logs automatically. Finally, we improve the kNN algorithm using average weighting technology, which improves the accuracy of the kNN algorithm on unbalanced samples. The method in this article is validated on six log datasets with different types.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W3152794541",
    "type": "article"
  },
  {
    "title": "Deep Learning-Based Energy Disaggregation and On/Off Detection of Household Appliances",
    "doi": "https://doi.org/10.1145/3441300",
    "publication_date": "2021-05-03",
    "publication_year": 2021,
    "authors": "Jie Jiang; Qiuqiang Kong; Mark D. Plumbley; Nigel Gilbert; Mark Hoogendoorn; Diederik M. Roijers",
    "corresponding_authors": "",
    "abstract": "Energy disaggregation, a.k.a. Non-Intrusive Load Monitoring, aims to separate the energy consumption of individual appliances from the readings of a mains power meter measuring the total energy consumption of, e.g., a whole house. Energy consumption of individual appliances can be useful in many applications, e.g., providing appliance-level feedback to the end users to help them understand their energy consumption and ultimately save energy. Recently, with the availability of large-scale energy consumption datasets, various neural network models such as convolutional neural networks and recurrent neural networks have been investigated to solve the energy disaggregation problem. Neural network models can learn complex patterns from large amounts of data and have been shown to outperform the traditional machine learning methods such as variants of hidden Markov models. However, current neural network methods for energy disaggregation are either computational expensive or are not capable of handling long-term dependencies. In this article, we investigate the application of the recently developed WaveNet models for the task of energy disaggregation. Based on a real-world energy dataset collected from 20 households over 2 years, we show that WaveNet models outperforms the state-of-the-art deep learning methods proposed in the literature for energy disaggregation in terms of both error measures and computational cost. On the basis of energy disaggregation, we then investigate the performance of two deep-learning based frameworks for the task of on/off detection which aims at estimating whether an appliance is in operation or not. The first framework obtains the on/off states of an appliance by binarising the predictions of a regression model trained for energy disaggregation, while the second framework obtains the on/off states of an appliance by directly training a binary classifier with binarised energy readings of the appliance serving as the target values. Based on the same dataset, we show that for the task of on/off detection the second framework, i.e., directly training a binary classifier, achieves better performance in terms of F1 score.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W3165162317",
    "type": "article"
  },
  {
    "title": "Measuring the Network Vulnerability Based on Markov Criticality",
    "doi": "https://doi.org/10.1145/3464390",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Hui‐Jia Li; Lin Wang; Zhan Bu; Jie Cao; Yong Shi",
    "corresponding_authors": "",
    "abstract": "Vulnerability assessment—a critical issue for networks—attempts to foresee unexpected destructive events or hostile attacks in the whole system. In this article, we consider a new Markov global connectivity metric—Kemeny constant, and take its derivative called Markov criticality to identify critical links. Markov criticality allows us to find links that are most influential on the derivative of Kemeny constant. Thus, we can utilize it to identity a critical link ( i , j ) from node i to node j , such that removing it leads to a minimization of networks’ global connectivity, i.e., the Kemeny constant. Furthermore, we also define a novel vulnerability index to measure the average speed by which we can disconnect a specified ratio of links with network decomposition. Our method is of high efficiency, which can be easily employed to calculate the Markov criticality in real-life networks. Comprehensive experiments on several synthetic and real-life networks have demonstrated our method’s better performance by comparing it with state-of-the-art baseline approaches.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W3184640909",
    "type": "article"
  },
  {
    "title": "Mixed Information Flow for Cross-Domain Sequential Recommendations",
    "doi": "https://doi.org/10.1145/3487331",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Muyang Ma; Pengjie Ren; Zhumin Chen; Zhaochun Ren; Lifan Zhao; Peiyu Liu; Jun Ma; Maarten de Rijke",
    "corresponding_authors": "",
    "abstract": "Cross-domain sequential recommendation is the task of predict the next item that the user is most likely to interact with based on past sequential behavior from multiple domains. One of the key challenges in cross-domain sequential recommendation is to grasp and transfer the flow of information from multiple domains so as to promote recommendations in all domains. Previous studies have investigated the flow of behavioral information by exploring the connection between items from different domains. The flow of knowledge (i.e., the connection between knowledge from different domains) has so far been neglected. In this article, we propose a mixed information flow network for cross-domain sequential recommendation to consider both the flow of behavioral information and the flow of knowledge by incorporating a behavior transfer unit and a knowledge transfer unit . The proposed mixed information flow network is able to decide when cross-domain information should be used and, if so, which cross-domain information should be used to enrich the sequence representation according to users’ current preferences. Extensive experiments conducted on four e-commerce datasets demonstrate that the proposed mixed information flow network is able to improve recommendation performance in different domains by modeling mixed information flow. In this article, we focus on the application of mixed information flow network s to a scenario with two domains, but the method can easily be extended to multiple domains.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W3108694672",
    "type": "article"
  },
  {
    "title": "Multi-Label Feature Selection Via Adaptive Label Correlation Estimation",
    "doi": "https://doi.org/10.1145/3604560",
    "publication_date": "2023-06-10",
    "publication_year": 2023,
    "authors": "Zan Zhang; Zhe Zhang; Jialu Yao; Lin Liu; Jiuyong Li; Gongqing Wu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "In multi-label learning, each instance is associated with multiple labels simultaneously. Multi-label data often have noisy, irrelevant, and redundant features of high dimensionality. Multi-label feature selection has received considerable attention as an effective means for dealing with high-dimensional multi-label data. Many multi-label feature selection methods exploit label correlations to help select features. However, finding label correlations and selecting features in existing multi-label feature selection methods are often two separate processes, the existence of noises and outliers in training data makes the label correlations exploited from label space less reliable. Therefore, the learned label correlations may mislead the feature selection process and result in the selection of less informative features. This article proposes a novel algorithm named ROAD, i.e., multi-label featuRe selectiOn via ADaptive label correlation estimation. ROAD jointly performs adaptive label correlation exploration and feature selection with alternating optimization to obtain reliable estimation of label correlations, which can more effectively reveal the intrinsic manifold structure among labels and lead to the selection of a more proper feature subset. Comprehensive experiments on several frequently used datasets validate the superiority of ROAD against the state-of-the-art multi-label feature selection algorithms.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W4380152815",
    "type": "article"
  },
  {
    "title": "A Generalized Deep Learning Clustering Algorithm Based on Non-Negative Matrix Factorization",
    "doi": "https://doi.org/10.1145/3584862",
    "publication_date": "2023-02-20",
    "publication_year": 2023,
    "authors": "Dexian Wang; Tianrui Li; Ping Deng; Fan Zhang; Wei Huang; Pengfei Zhang; Jia Liu",
    "corresponding_authors": "",
    "abstract": "Clustering is a popular research topic in the field of data mining, in which the clustering method based on non-negative matrix factorization (NMF) has been widely employed. However, in the update process of NMF, there is no learning rate to guide the update as well as the update depends on the data itself, which leads to slow convergence and low clustering accuracy. To solve these problems, a generalized deep learning clustering (GDLC) algorithm based on NMF is proposed in this article. Firstly, a nonlinear constrained NMF (NNMF) algorithm is constructed to achieve sequential updates of the elements in the matrix guided by the learning rate. Then, the gradient values corresponding to the element update are transformed into generalized weights and generalized biases, by inputting the elements as well as their corresponding generalized weights and generalized biases into the nonlinear activation function to construct the GDLC algorithm. In addition, for improving the understanding of the GDLC algorithm, its detailed inference procedure and algorithm design are provided. Finally, the experimental results on eight datasets show that the GDLC algorithm has efficient performance.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4321372616",
    "type": "article"
  },
  {
    "title": "Distributed Cooperative Coevolution of Data Publishing Privacy and Transparency",
    "doi": "https://doi.org/10.1145/3613962",
    "publication_date": "2023-08-07",
    "publication_year": 2023,
    "authors": "Yong-Feng Ge; Elisa Bertino; Hua Wang; Jinli Cao; Yanchun Zhang",
    "corresponding_authors": "",
    "abstract": "Data transparency is beneficial to data participants’ awareness, users’ fairness, and research work’s reproducibility. However, when addressing transparency requirements, we cannot ignore data privacy. This article defines the multi-objective data publishing (MODP) problem, optimizing data privacy and transparency at the same time. Accordingly, we propose a distributed cooperative coevolutionary genetic algorithm (DCCGA) to optimize the MODP problem. In the population of DCCGA, each individual represents an anonymization solution to MODP. Three modules in DCCGA, i.e., grouping module, cooperative coevolutionary module, and evolving module, are proposed for distributed sub-population update and evaluation, improving DCCGA’s optimization performance and parallel efficiency. Moreover, a matrix-based crossover operator and a matrix-based mutation operator are designed to exchange and adjust anonymization information in the individuals efficiently. Experimental results demonstrate that the proposed DCCGA outperforms the competitors with respect to solution accuracy, convergence speed, and scalability. Besides, we verify the effectiveness of all the proposed components in DCCGA.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4385616961",
    "type": "article"
  },
  {
    "title": "Review of Clustering Methods for Functional Data",
    "doi": "https://doi.org/10.1145/3581789",
    "publication_date": "2023-01-24",
    "publication_year": 2023,
    "authors": "Mimi Zhang; Andrew Parnell",
    "corresponding_authors": "",
    "abstract": "Functional data clustering is to identify heterogeneous morphological patterns in the continuous functions underlying the discrete measurements/observations. Application of functional data clustering has appeared in many publications across various fields of sciences, including but not limited to biology, (bio)chemistry, engineering, environmental science, medical science, psychology, social science, and so on. The phenomenal growth of the application of functional data clustering indicates the urgent need for a systematic approach to develop efficient clustering methods and scalable algorithmic implementations. On the other hand, there is abundant literature on the cluster analysis of time series, trajectory data, spatio-temporal data, and so on, which are all related to functional data. Therefore, an overarching structure of existing functional data clustering methods will enable the cross-pollination of ideas across various research fields. We here conduct a comprehensive review of original clustering methods for functional data. We propose a systematic taxonomy that explores the connections and differences among the existing functional data clustering methods and relates them to the conventional multivariate clustering methods. The structure of the taxonomy is built on three main attributes of a functional data clustering method and therefore is more reliable than existing categorizations. The review aims to bridge the gap between the functional data analysis community and the clustering community and to generate new principles for functional data clustering.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4317831475",
    "type": "article"
  },
  {
    "title": "StructCoder: Structure-Aware Transformer for Code Generation",
    "doi": "https://doi.org/10.1145/3636430",
    "publication_date": "2023-12-07",
    "publication_year": 2023,
    "authors": "Sindhu Tipirneni; Ming Zhu; Chandan K. Reddy",
    "corresponding_authors": "",
    "abstract": "There has been a recent surge of interest in automating software engineering tasks using deep learning. This article addresses the problem of code generation, in which the goal is to generate target code given source code in a different language or a natural language description. Most state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model in which both the encoder and decoder are explicitly trained to recognize the syntax and dataflow in the source and target codes, respectively. We not only make the encoder structure aware by leveraging the source code’s syntax tree and dataflow graph, but we also support the decoder in preserving the syntax and dataflow of the target code by introducing two novel auxiliary tasks: Abstract Syntax Tree (AST) path prediction and dataflow prediction. To the best of our knowledge, this is the first work to introduce a structure-aware Transformer decoder that models both syntax and dataflow to enhance the quality of generated code. The proposed StructCoder model achieves state-of-the-art performance on code translation and text-to-code generation tasks in the CodeXGLUE benchmark and improves over baselines of similar size on the APPS code generation benchmark. Our code is publicly available at https://github.com/reddy-lab-code-research/StructCoder/ .",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4389438938",
    "type": "article"
  },
  {
    "title": "Enhancing Heterogeneous Knowledge Graph Completion with a Novel GAT-based Approach",
    "doi": "https://doi.org/10.1145/3639472",
    "publication_date": "2024-01-03",
    "publication_year": 2024,
    "authors": "Wanxu Wei; Yitong Song; Bin Yao",
    "corresponding_authors": "",
    "abstract": "Knowledge graphs (KGs) play a vital role in enhancing search results and recommendation systems. With the rapid increase in the size of the KGs, they are becoming inaccuracy and incomplete. This problem can be solved by the knowledge graph completion methods, of which graph attention network (GAT)-based methods stand out since their superior performance. However, existing GAT-based knowledge graph completion methods often suffer from overfitting issues when dealing with heterogeneous knowledge graphs, primarily due to the unbalanced number of samples. Additionally, these methods demonstrate poor performance in predicting the tail (head) entity that shares the same relation and head (tail) entity with others. To solve these problems, we propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH incorporates two separate attention network modules that work synergistically to predict the missing entities. We also introduce novel encoding and feature transformation approaches, enabling the robust performance of GATH in scenarios with imbalanced samples. Comprehensive experiments are conducted to evaluate the GATH's performance. Compared with the existing SOTA GAT-based model on Hits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the FB15K-237 dataset, and by 4.5% and 14.6% on the WN18RR dataset, respectively.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4390532736",
    "type": "article"
  },
  {
    "title": "TaSPM: Targeted Sequential Pattern Mining",
    "doi": "https://doi.org/10.1145/3639827",
    "publication_date": "2024-01-19",
    "publication_year": 2024,
    "authors": "Gengsen Huang; Wensheng Gan; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Sequential pattern mining (SPM) is an important technique in the field of pattern mining, which has many applications in reality. Although many efficient SPM algorithms have been proposed, there are few studies that can focus on targeted tasks. Targeted querying of the concerned sequential patterns can not only reduce the number of patterns generated, but also increase the efficiency of users in performing related analysis. The current algorithms available for targeted sequence querying are based on specific scenarios and can not be extended to other applications. In this article, we formulate the problem of targeted sequential pattern mining and propose a generic algorithm, namely TaSPM. What is more, to improve the efficiency of TaSPM on large-scale datasets and multiple-item-based sequence datasets, we propose several pruning strategies to reduce meaningless operations in the mining process. Totally four pruning strategies are designed in TaSPM, and hence TaSPM can terminate unnecessary pattern extensions quickly and achieve better performance. Finally, we conducted extensive experiments on different datasets to compare the baseline SPM algorithm with TaSPM. Experiments show that the novel targeted mining algorithm TaSPM can achieve faster running time and less memory consumption.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4391026416",
    "type": "article"
  },
  {
    "title": "FiFrauD: Unsupervised Financial Fraud Detection in Dynamic Graph Streams",
    "doi": "https://doi.org/10.1145/3641857",
    "publication_date": "2024-01-27",
    "publication_year": 2024,
    "authors": "Samira Khodabandehlou; Alireza Hashemi Golpayegani",
    "corresponding_authors": "",
    "abstract": "Given a stream of financial transactions between traders in an e-market, how can we accurately detect fraudulent traders and suspicious behaviors in real time? Despite the efforts made in detecting these fraudsters, this field still faces serious challenges, including the ineffectiveness of existing methods for the complex and streaming environment of e-markets. As a result, it is still difficult to quickly and accurately detect suspected traders and behavior patterns in real-time transactions, and it is still considered an open problem. To solve this problem and alleviate the existing challenges, in this article, we propose FiFrauD, which is an unsupervised, scalable approach that depicts the behavior of manipulators in a transaction stream. In this approach, real-time transactions between traders are converted into a stream of graphs and, instead of using supervised and semi-supervised learning methods, fraudulent traders are detected precisely by exploiting density signals in graphs. Specifically, we reveal the traits of fraudulent traders in the market and propose a novel metric from this perspective, i.e., graph topology, time, and behavior. Then, we search for suspicious blocks by greedily optimizing the proposed metric. Theoretical analysis demonstrates upper bounds for FiFrauD's effectiveness in catching suspicious trades. Extensive experiments on five real-world datasets with both actual and synthetic labels demonstrate that FiFrauD achieves significant accuracy improvements compared with state-of-the-art fraud detection methods. Also, it can find various suspicious behavior patterns in a linear runtime and provide interpretable results. Furthermore, FiFrauD is resistant to the camouflage tactics used by fraudulent traders.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4391277897",
    "type": "article"
  },
  {
    "title": "Anomaly Detection in Dynamic Graphs: A Comprehensive Survey",
    "doi": "https://doi.org/10.1145/3669906",
    "publication_date": "2024-05-29",
    "publication_year": 2024,
    "authors": "Ocheme Anthony Ekle; William Eberle",
    "corresponding_authors": "",
    "abstract": "This survey paper presents a comprehensive and conceptual overview of anomaly detection using dynamic graphs. We focus on existing graph-based anomaly detection (AD) techniques and their applications to dynamic networks. The contributions of this survey paper include the following: i) a comparative study of existing surveys on anomaly detection; ii) a Dynamic Graph-based Anomaly Detection (DGAD) review framework in which approaches for detecting anomalies in dynamic graphs are grouped based on traditional machine-learning models, matrix transformations, probabilistic approaches, and deep-learning approaches; iii) a discussion of graphically representing both discrete and dynamic networks; and iv) a discussion of the advantages of graph-based techniques for capturing the relational structure and complex interactions in dynamic graph data. Finally, this work identifies the potential challenges and future directions for detecting anomalies in dynamic networks. This DGAD survey approach aims to provide a valuable resource for researchers and practitioners by summarizing the strengths and limitations of each approach, highlighting current research trends, and identifying open challenges. In doing so, it can guide future research efforts and promote advancements in anomaly detection in dynamic graphs. Keywords: Graphs, Anomaly Detection, dynamic networks,Graph Neural Networks (GNN), Node anomaly, Graph mining.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4399121544",
    "type": "article"
  },
  {
    "title": "A Compact Vulnerability Knowledge Graph for Risk Assessment",
    "doi": "https://doi.org/10.1145/3671005",
    "publication_date": "2024-06-05",
    "publication_year": 2024,
    "authors": "Jiao Yin; Wei Hong; Hua Wang; Jinli Cao; Yuan Miao; Yanchun Zhang",
    "corresponding_authors": "",
    "abstract": "Software vulnerabilities, also known as flaws, bugs or weaknesses, are common in modern information systems, putting critical data of organizations and individuals at cyber risk. Due to the scarcity of resources, initial risk assessment is becoming a necessary step to prioritize vulnerabilities and make better decisions on remediation, mitigation, and patching. Datasets containing historical vulnerability information are crucial digital assets to enable AI-based risk assessments. However, existing datasets focus on collecting information on individual vulnerabilities while simply storing them in relational databases, disregarding their structural connections. This article constructs a compact vulnerability knowledge graph, VulKG, containing over 276 K nodes and 1 M relationships to represent the connections between vulnerabilities, exploits, affected products, vendors, referred domain names, and more. We provide a detailed analysis of VulKG modeling and construction, demonstrating VulKG-based query and reasoning, and providing a use case of applying VulKG to a vulnerability risk assessment task, i.e., co-exploitation behavior discovery. Experimental results demonstrate the value of graph connections in vulnerability risk assessment tasks. VulKG offers exciting opportunities for more novel and significant research in areas related to vulnerability risk assessment. The data and codes of this article are available at https://github.com/happyResearcher/VulKG.git .",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4399353588",
    "type": "article"
  },
  {
    "title": "DyExplainer: Self-explainable Dynamic Graph Neural Network with Sparse Attentions",
    "doi": "https://doi.org/10.1145/3729173",
    "publication_date": "2025-04-12",
    "publication_year": 2025,
    "authors": "Tianchun Wang; Dongsheng Luo; Wei Cheng; Haifeng Chen; X. D. Zhang",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) resurge as a trending research subject owing to their impressive ability to capture representations from graph-structured data. However, the black-box nature of GNNs presents a significant challenge in terms of comprehending and trusting these models, thereby limiting their practical applications in mission-critical scenarios. Although there has been substantial progress in the field of explaining GNNs in recent years, the majority of these studies are centered on static graphs, leaving the explanation of dynamic GNNs less explored. Dynamic GNNs, with their ever-evolving graph structures, pose a unique challenge and require additional efforts to effectively capture temporal dependencies and structural relationships. To address this challenge, we present DyExplainer, a novel approach to explaining dynamic GNNs on the fly. DyExplainer trains a dynamic GNN backbone to extract representations of the graph at each snapshot, while simultaneously exploring structural relationships and temporal dependencies through a sparse attention technique. To preserve the desired properties of the explanation, such as structural consistency and temporal continuity, we augment our approach with contrastive learning techniques to provide priori-guided regularization. To model longer-term temporal dependencies, we develop a buffer-based live-updating scheme for training. The results of our extensive experiments on various datasets demonstrate the superiority of DyExplainer, not only providing faithful explainability of the model predictions but also significantly improving the model prediction accuracy, as evidenced in the link prediction task.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4409390844",
    "type": "article"
  },
  {
    "title": "Mining periodic patterns with gap requirement from sequences",
    "doi": "https://doi.org/10.1145/1267066.1267068",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Minghua Zhang; Ben Kao; David W. Cheung; Kevin Y. Yip",
    "corresponding_authors": "",
    "abstract": "We study a problem of mining frequently occurring periodic patterns with a gap requirement from sequences. Given a character sequence S of length L and a pattern P of length l , we consider P a frequently occurring pattern in S if the probability of observing P given a randomly picked length- l subsequence of S exceeds a certain threshold. In many applications, particularly those related to bioinformatics, interesting patterns are periodic with a gap requirement . That is to say, the characters in P should match subsequences of S in such a way that the matching characters in S are separated by gaps of more or less the same size. We show the complexity of the mining problem and discuss why traditional mining algorithms are computationally infeasible. We propose practical algorithms for solving the problem and study their characteristics. We also present a case study in which we apply our algorithms on some DNA sequences. We discuss some interesting patterns obtained from the case study.",
    "cited_by_count": 95,
    "openalex_id": "https://openalex.org/W1998959019",
    "type": "article"
  },
  {
    "title": "Mining frequent cross-graph quasi-cliques",
    "doi": "https://doi.org/10.1145/1460797.1460799",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Daxin Jiang; Jian Pei",
    "corresponding_authors": "",
    "abstract": "Joint mining of multiple datasets can often discover interesting, novel, and reliable patterns which cannot be obtained solely from any single source. For example, in bioinformatics, jointly mining multiple gene expression datasets obtained by different labs or during various biological processes may overcome the heavy noise in the data. Moreover, by joint mining of gene expression data and protein-protein interaction data, we may discover clusters of genes which show coherent expression patterns and also produce interacting proteins. Such clusters may be potential pathways. In this article, we investigate a novel data mining problem, mining frequent cross-graph quasi-cliques , which is generalized from several interesting applications in bioinformatics, cross-market customer segmentation, social network analysis, and Web mining. In a graph, a set of vertices S is a γ-quasi-clique (0 &lt; γ ≤ 1) if each vertex v in S directly connects to at least γ ⋅ (| S | − 1) other vertices in S . Given a set of graphs G 1 , …, G n and parameter min_sup (0 &lt; min_sup ≤ 1), a set of vertices S is a frequent cross-graph quasi-clique if S is a γ-quasi-clique in at least min_sup ⋅ n graphs, and there does not exist a proper superset of S having the property. We build a general model, show why the complete set of frequent cross-graph quasi-cliques cannot be found by previous data mining methods, and study the complexity of the problem. While the problem is difficult, we develop practical algorithms which exploit several interesting and effective techniques and heuristics to efficaciously mine frequent cross-graph quasi-cliques. A systematic performance study is reported on both synthetic and real data sets. We demonstrate some interesting and meaningful frequent cross-graph quasi-cliques in bioinformatics. The experimental results also show that our algorithms are efficient and scalable.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W2096281132",
    "type": "article"
  },
  {
    "title": "Finding hierarchical heavy hitters in streaming data",
    "doi": "https://doi.org/10.1145/1324172.1324174",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Graham Cormode; Flip Korn; S. Muthukrishnan; Divesh Srivastava",
    "corresponding_authors": "",
    "abstract": "Data items that arrive online as streams typically have attributes which take values from one or more hierarchies (time and geographic location, source and destination IP addresses, etc.). Providing an aggregate view of such data is important for summarization, visualization, and analysis. We develop an aggregate view based on certain organized sets of large-valued regions (“heavy hitters”) corresponding to hierarchically discounted frequency counts. We formally define the notion of hierarchical heavy hitters (HHHs). We first consider computing (approximate) HHHs over a data stream drawn from a single hierarchical attribute. We formalize the problem and give deterministic algorithms to find them in a single pass over the input. In order to analyze a wider range of realistic data streams (e.g., from IP traffic-monitoring applications), we generalize this problem to multiple dimensions. Here, the semantics of HHHs are more complex, since a “child” node can have multiple “parent” nodes. We present online algorithms that find approximate HHHs in one pass, with provable accuracy guarantees. The product of hierarchical dimensions forms a mathematical lattice structure. Our algorithms exploit this structure, and so are able to track approximate HHHs using only a small, fixed number of statistics per stored item, regardless of the number of dimensions. We show experimentally, using real data, that our proposed algorithms yields outputs which are very similar (virtually identical, in many cases) to offline computations of the exact solutions, whereas straightforward heavy-hitters-based approaches give significantly inferior answer quality. Furthermore, the proposed algorithms result in an order of magnitude savings in data structure size while performing competitively.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W2130873644",
    "type": "article"
  },
  {
    "title": "Introduction",
    "doi": "https://doi.org/10.1145/1217299.1217300",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Han Jiawei",
    "corresponding_authors": "Han Jiawei",
    "abstract": "No abstract available.",
    "cited_by_count": 82,
    "openalex_id": "https://openalex.org/W2623600023",
    "type": "article"
  },
  {
    "title": "Privacy-preserving classification of vertically partitioned data via random kernels",
    "doi": "https://doi.org/10.1145/1409620.1409622",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "O. L. Mangasarian; Edward W. Wild; Glenn Fung",
    "corresponding_authors": "",
    "abstract": "We propose a novel privacy-preserving support vector machine (SVM) classifier for a data matrix A whose input feature columns are divided into groups belonging to different entities. Each entity is unwilling to share its group of columns or make it public. Our classifier is based on the concept of a reduced kernel K ( A , B ′), where B ′ is the transpose of a random matrix B . The column blocks of B corresponding to the different entities are privately generated by each entity and never made public. The proposed linear or nonlinear SVM classifier, which is public but does not reveal any of the privately held data, has accuracy comparable to that of an ordinary SVM classifier that uses the entire set of input features directly.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2076826889",
    "type": "article"
  },
  {
    "title": "Efficient algorithms for large-scale local triangle counting",
    "doi": "https://doi.org/10.1145/1839490.1839494",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Luca Becchetti; Paolo Boldi; Carlos Castillo; Aristides Gionis",
    "corresponding_authors": "",
    "abstract": "In this article, we study the problem of approximate local triangle counting in large graphs. Namely, given a large graph G =( V,E ) we want to estimate as accurately as possible the number of triangles incident to every node v ∈ V in the graph. We consider the question both for undirected and directed graphs. The problem of computing the global number of triangles in a graph has been considered before, but to our knowledge this is the first contribution that addresses the problem of approximate local triangle counting with a focus on the efficiency issues arising in massive graphs and that also considers the directed case. The distribution of the local number of triangles and the related local clustering coefficient can be used in many interesting applications. For example, we show that the measures we compute can help detect the presence of spamming activity in large-scale Web graphs, as well as to provide useful features for content quality assessment in social networks. For computing the local number of triangles (undirected and directed), we propose two approximation algorithms, which are based on the idea of min-wise independent permutations [Broder et al. 1998]. Our algorithms operate in a semi-streaming fashion, using O (| V |) space in main memory and performing O (log | V |) sequential scans over the edges of the graph. The first algorithm we describe in this article also uses O (| E |) space of external memory during computation, while the second algorithm uses only main memory. We present the theoretical analysis as well as experimental results on large graphs, demonstrating the practical efficiency of our approach.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2094631006",
    "type": "article"
  },
  {
    "title": "Self-sufficient itemsets",
    "doi": "https://doi.org/10.1145/1644873.1644876",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Geoffrey I. Webb",
    "corresponding_authors": "Geoffrey I. Webb",
    "abstract": "Self-sufficient itemsets are those whose frequency cannot be explained solely by the frequency of either their subsets or of their supersets. We argue that itemsets that are not self-sufficient will often be of little interest to the data analyst, as their frequency should be expected once that of the itemsets on which their frequency depends is known. We present tests for statistically sound discovery of self-sufficient itemsets, and computational techniques that allow those tests to be applied as a post-processing step for any itemset discovery algorithm. We also present a measure for assessing the degree of potential interest in an itemset that complements these statistical measures.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2064668866",
    "type": "article"
  },
  {
    "title": "Can the Utility of Anonymized Data be Used for Privacy Breaches?",
    "doi": "https://doi.org/10.1145/1993077.1993080",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Raymond Chi-Wing Wong; Ada Wai-Chee Fu; Ke Wang; Philip S. Yu; Jian Pei",
    "corresponding_authors": "",
    "abstract": "Group based anonymization is the most widely studied approach for privacy-preserving data publishing. Privacy models/definitions using group based anonymization includes k -anonymity, l -diversity, and t -closeness, to name a few. The goal of this article is to raise a fundamental issue regarding the privacy exposure of the approaches using group based anonymization. This has been overlooked in the past. The group based anonymization approach by bucketization basically hides each individual record behind a group to preserve data privacy. If not properly anonymized, patterns can actually be derived from the published data and be used by an adversary to breach individual privacy. For example, from the medical records released, if patterns such as that people from certain countries rarely suffer from some disease can be derived, then the information can be used to imply linkage of other people in an anonymized group with this disease with higher likelihood. We call the derived patterns from the published data the foreground knowledge. This is in contrast to the background knowledge that the adversary may obtain from other channels, as studied in some previous work. Finally, our experimental results show such an attack is realistic in the privacy benchmark dataset under the traditional group based anonymization approach.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2069857456",
    "type": "article"
  },
  {
    "title": "Integrating Document Clustering and Multidocument Summarization",
    "doi": "https://doi.org/10.1145/1993077.1993078",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Dingding Wang; Shenghuo Zhu; Tao Li; Yün Chi; Yihong Gong",
    "corresponding_authors": "",
    "abstract": "Document understanding techniques such as document clustering and multidocument summarization have been receiving much attention recently. Current document clustering methods usually represent the given collection of documents as a document-term matrix and then conduct the clustering process. Although many of these clustering methods can group the documents effectively, it is still hard for people to capture the meaning of the documents since there is no satisfactory interpretation for each document cluster. A straightforward solution is to first cluster the documents and then summarize each document cluster using summarization methods. However, most of the current summarization methods are solely based on the sentence-term matrix and ignore the context dependence of the sentences. As a result, the generated summaries lack guidance from the document clusters. In this article, we propose a new language model to simultaneously cluster and summarize documents by making use of both the document-term and sentence-term matrices. By utilizing the mutual influence of document clustering and summarization, our method makes; (1) a better document clustering method with more meaningful interpretation; and (2) an effective document summarization method with guidance from document clustering. Experimental results on various document datasets show the effectiveness of our proposed method and the high interpretability of the generated summaries.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2119706777",
    "type": "article"
  },
  {
    "title": "MARGIN",
    "doi": "https://doi.org/10.1145/1839490.1839491",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Lini Thomas; Satyanarayana R. Valluri; Kamalakar Karlapalem",
    "corresponding_authors": "",
    "abstract": "The exponential number of possible subgraphs makes the problem of frequent subgraph mining a challenge. The set of maximal frequent subgraphs is much smaller to that of the set of frequent subgraphs providing ample scope for pruning. MARGIN is a maximal subgraph mining algorithm that moves among promising nodes of the search space along the “border” of the infrequent and frequent subgraphs. This drastically reduces the number of candidate patterns in the search space. The proof of correctness of the algorithm is presented. Experimental results validate the efficiency and utility of the technique proposed.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W1985147554",
    "type": "article"
  },
  {
    "title": "Large Linear Classification When Data Cannot Fit in Memory",
    "doi": "https://doi.org/10.1145/2086737.2086743",
    "publication_date": "2012-01-31",
    "publication_year": 2012,
    "authors": "Hsiang‐Fu Yu; Cho‐Jui Hsieh; Kai‐Wei Chang; Chih‐Jen Lin",
    "corresponding_authors": "",
    "abstract": "Recent advances in linear classification have shown that for applications such as document classification, the training process can be extremely efficient. However, most of the existing training methods are designed by assuming that data can be stored in the computer memory. These methods cannot be easily applied to data larger than the memory capacity due to the random access to the disk. We propose and analyze a block minimization framework for data larger than the memory size. At each step a block of data is loaded from the disk and handled by certain learning methods. We investigate two implementations of the proposed framework for primal and dual SVMs, respectively. Because data cannot fit in memory, many design considerations are very different from those for traditional algorithms. We discuss and compare with existing approaches that are able to handle data larger than memory. Experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2146068631",
    "type": "article"
  },
  {
    "title": "Mining multidimensional and multilevel sequential patterns",
    "doi": "https://doi.org/10.1145/1644873.1644877",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Marc Plantevit; Anne Laurent; Dominique Laurent; Maguelonne Teisseire; Yeow Wei Choong",
    "corresponding_authors": "",
    "abstract": "Multidimensional databases have been designed to provide decision makers with the necessary tools to help them understand their data. This framework is different from transactional data as the datasets contain huge volumes of historicized and aggregated data defined over a set of dimensions that can be arranged through multiple levels of granularities. Many tools have been proposed to query the data and navigate through the levels of granularity. However, automatic tools are still missing to mine this type of data in order to discover regular specific patterns. In this article, we present a method for mining sequential patterns from multidimensional databases, at the same time taking advantage of the different dimensions and levels of granularity, which is original compared to existing work. The necessary definitions and algorithms are extended from regular sequential patterns to this particular case. Experiments are reported, showing the significance of this approach.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W1964815362",
    "type": "article"
  },
  {
    "title": "GBAGC",
    "doi": "https://doi.org/10.1145/2629616",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "Zhiqiang Xu; Yiping Ke; Yi Wang; Hong Cheng; James Cheng",
    "corresponding_authors": "",
    "abstract": "Graph clustering, also known as community detection, is a long-standing problem in data mining. In recent years, with the proliferation of rich attribute information available for objects in real-world graphs, how to leverage not only structural but also attribute information for clustering attributed graphs becomes a new challenge. Most existing works took a distance-based approach. They proposed various distance measures to fuse structural and attribute information and then applied standard techniques for graph clustering based on these distance measures. In this article, we take an alternative view and propose a novel Bayesian framework for attributed graph clustering. Our framework provides a general and principled solution to modeling both the structural and the attribute aspects of a graph. It avoids the artificial design of a distance measure in existing methods and, furthermore, can seamlessly handle graphs with different types of edges and vertex attributes. We develop an efficient variational method for graph clustering under this framework and derive two concrete algorithms for clustering unweighted and weighted attributed graphs. Experimental results on large real-world datasets show that our algorithms significantly outperform the state-of-the-art distance-based method, in terms of both effectiveness and efficiency.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2044400205",
    "type": "article"
  },
  {
    "title": "Batch Mode Active Sampling Based on Marginal Probability Distribution Matching",
    "doi": "https://doi.org/10.1145/2513092.2513094",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Rita Chattopadhyay; Zheng Wang; Wei Fan; Ian Davidson; Sethuraman Panchanathan; Jieping Ye",
    "corresponding_authors": "",
    "abstract": "Active Learning is a machine learning and data mining technique that selects the most informative samples for labeling and uses them as training data; it is especially useful when there are large amount of unlabeled data and labeling them is expensive. Recently, batch-mode active learning, where a set of samples are selected concurrently for labeling, based on their collective merit, has attracted a lot of attention. The objective of batch-mode active learning is to select a set of informative samples so that a classifier learned on these samples has good generalization performance on the unlabeled data. Most of the existing batch-mode active learning methodologies try to achieve this by selecting samples based on certain criteria. In this article we propose a novel criterion which achieves good generalization performance of a classifier by specifically selecting a set of query samples that minimize the difference in distribution between the labeled and the unlabeled data, after annotation. We explicitly measure this difference based on all candidate subsets of the unlabeled data and select the best subset. The proposed objective is an NP-hard integer programming optimization problem. We provide two optimization techniques to solve this problem. In the first one, the problem is transformed into a convex quadratic programming problem and in the second method the problem is transformed into a linear programming problem. Our empirical studies using publicly available UCI datasets and two biomedical image databases demonstrate the effectiveness of the proposed approach in comparison with the state-of-the-art batch-mode active learning methods. We also present two extensions of the proposed approach, which incorporate uncertainty of the predicted labels of the unlabeled data and transfer learning in the proposed formulation. In addition, we present a joint optimization framework for performing both transfer and active learning simultaneously unlike the existing approaches of learning in two separate stages, that is, typically, transfer learning followed by active learning. We specifically minimize a common objective of reducing distribution difference between the domain adapted source, the queried and labeled samples and the rest of the unlabeled target domain data. Our empirical studies on two biomedical image databases and on a publicly available 20 Newsgroups dataset show that incorporation of uncertainty information and transfer learning further improves the performance of the proposed active learning based classifier. Our empirical studies also show that the proposed transfer-active method based on the joint optimization framework performs significantly better than a framework which implements transfer and active learning in two separate stages.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2735257517",
    "type": "article"
  },
  {
    "title": "Addressing Big Data Time Series",
    "doi": "https://doi.org/10.1145/2513092.2500489",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Thanawin Rakthanmanon; Bilson Campana; Abdullah Mueen; Gustavo Batista; Brandon Westover; Qiang Zhu; Jesin Zakaria; Eamonn Keogh",
    "corresponding_authors": "",
    "abstract": "Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms, including classification, clustering, motif discovery, anomaly detection, and so on. The difficulty of scaling a search to large datasets explains to a great extent why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine massive time series for the first time. We demonstrate the following unintuitive fact: in large datasets we can exactly search under Dynamic Time Warping (DTW) much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. In particular, the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining papers ever published. We explain how our ideas allow us to solve higher-level time series data mining problems such as motif discovery and clustering at scales that would otherwise be untenable. Moreover, we show how our ideas allow us to efficiently support the uniform scaling distance measure, a measure whose utility seems to be underappreciated, but which we demonstrate here. In addition to mining massive datasets with up to one trillion datapoints, we will show that our ideas also have implications for real-time monitoring of data streams, allowing us to handle much faster arrival rates and/or use cheaper and lower powered devices than are currently possible.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W4255556848",
    "type": "article"
  },
  {
    "title": "Optimizing Text Quantifiers for Multivariate Loss Functions",
    "doi": "https://doi.org/10.1145/2700406",
    "publication_date": "2015-06-01",
    "publication_year": 2015,
    "authors": "Andrea Esuli; Fabrizio Sebastiani",
    "corresponding_authors": "",
    "abstract": "We address the problem of \\emph{quantification}, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or \\emph{prevalence}) of the class in a dataset of unlabelled items. Quantification has several applications in data and text mining, such as estimating the prevalence of positive reviews in a set of reviews of a given product, or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabelled items which have been assigned the class, and tuning the obtained counts according to some heuristics. In this paper we depart from the tradition of using general-purpose classifiers, and use instead a supervised learning model for \\emph{structured prediction}, capable of generating classifiers directly optimized for the (multivariate and non-linear) function used for evaluating quantification accuracy. The experiments that we have run on 5500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing, state-of-the-art quantification methods.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W1919365417",
    "type": "article"
  },
  {
    "title": "Fast Algorithms for Approximating the Singular Value Decomposition",
    "doi": "https://doi.org/10.1145/1921632.1921639",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Aditya Krishna Menon; Charles Elkan",
    "corresponding_authors": "",
    "abstract": "A low-rank approximation to a matrix A is a matrix with significantly smaller rank than A , and which is close to A according to some norm. Many practical applications involving the use of large matrices focus on low-rank approximations. By reducing the rank or dimensionality of the data, we reduce the complexity of analyzing the data. The singular value decomposition is the most popular low-rank matrix approximation. However, due to its expensive computational requirements, it has often been considered intractable for practical applications involving massive data. Recent developments have tried to address this problem, with several methods proposed to approximate the decomposition with better asymptotic runtime. We present an empirical study of these techniques on a variety of dense and sparse datasets. We find that a sampling approach of Drineas, Kannan and Mahoney is often, but not always, the best performing method. This method gives solutions with high accuracy much faster than classical SVD algorithms, on large sparse datasets in particular. Other modern methods, such as a recent algorithm by Rokhlin and Tygert, also offer savings compared to classical SVD algorithms. The older sampling methods of Achlioptas and McSherry are shown to sometimes take longer than classical SVD.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W1997120656",
    "type": "article"
  },
  {
    "title": "Efficient Mining of Gap-Constrained Subsequences and Its Various Applications",
    "doi": "https://doi.org/10.1145/2133360.2133362",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Chun Li; Qingyan Yang; Jianyong Wang; Ming Li",
    "corresponding_authors": "",
    "abstract": "Mining frequent subsequence patterns is a typical data-mining problem and various efficient sequential pattern mining algorithms have been proposed. In many application domains (e.g., biology), the frequent subsequences confined by the predefined gap requirements are more meaningful than the general sequential patterns. In this article, we propose two algorithms, Gap-BIDE for mining closed gap-constrained subsequences from a set of input sequences, and Gap-Connect for mining repetitive gap-constrained subsequences from a single input sequence. Inspired by some state-of-the-art closed or constrained sequential pattern mining algorithms, the Gap-BIDE algorithm adopts an efficient approach to finding the complete set of closed sequential patterns with gap constraints, while the Gap-Connect algorithm efficiently mines an approximate set of long patterns by connecting short patterns. We also present several methods for feature selection from the set of gap-constrained patterns for the purpose of classification and clustering. Our extensive performance study shows that our approaches are very efficient in mining frequent subsequences with gap constraints, and the gap-constrained pattern based classification/clustering approaches can achieve high-quality results.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2022798908",
    "type": "article"
  },
  {
    "title": "Multilabel relationship learning",
    "doi": "https://doi.org/10.1145/2499907.2499910",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Yu Zhang; Dit‐Yan Yeung",
    "corresponding_authors": "",
    "abstract": "Multilabel learning problems are commonly found in many applications. A characteristic shared by many multilabel learning problems is that some labels have significant correlations between them. In this article, we propose a novel multilabel learning method, called MultiLabel Relationship Learning (MLRL), which extends the conventional support vector machine by explicitly learning and utilizing the relationships between labels. Specifically, we model the label relationships using a label covariance matrix and use it to define a new regularization term for the optimization problem. MLRL learns the model parameters and the label covariance matrix simultaneously based on a unified convex formulation. To solve the convex optimization problem, we use an alternating method in which each subproblem can be solved efficiently. The relationship between MLRL and two widely used maximum margin methods for multilabel learning is investigated. Moreover, we also propose a semisupervised extension of MLRL, called SSMLRL, to demonstrate how to make use of unlabeled data to help learn the label covariance matrix. Through experiments conducted on some multilabel applications, we find that MLRL not only gives higher classification accuracy but also has better interpretability as revealed by the label covariance matrix.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2092887517",
    "type": "article"
  },
  {
    "title": "Multi-View Low-Rank Analysis with Applications to Outlier Detection",
    "doi": "https://doi.org/10.1145/3168363",
    "publication_date": "2018-03-23",
    "publication_year": 2018,
    "authors": "Sheng Li; Ming Shao; Yun Fu",
    "corresponding_authors": "",
    "abstract": "Detecting outliers or anomalies is a fundamental problem in various machine learning and data mining applications. Conventional outlier detection algorithms are mainly designed for single-view data. Nowadays, data can be easily collected from multiple views, and many learning tasks such as clustering and classification have benefited from multi-view data. However, outlier detection from multi-view data is still a very challenging problem, as the data in multiple views usually have more complicated distributions and exhibit inconsistent behaviors. To address this problem, we propose a multi-view low-rank analysis (MLRA) framework for outlier detection in this article. MLRA pursuits outliers from a new perspective, robust data representation. It contains two major components. First, the cross-view low-rank coding is performed to reveal the intrinsic structures of data. In particular, we formulate a regularized rank-minimization problem, which is solved by an efficient optimization algorithm. Second, the outliers are identified through an outlier score estimation procedure. Different from the existing multi-view outlier detection methods, MLRA is able to detect two different types of outliers from multiple views simultaneously. To this end, we design a criterion to estimate the outlier scores by analyzing the obtained representation coefficients. Moreover, we extend MLRA to tackle the multi-view group outlier detection problem. Extensive evaluations on seven UCI datasets, the MovieLens, the USPS-MNIST, and the WebKB datasets demon strate that our approach outperforms several state-of-the-art outlier detection methods.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2791255512",
    "type": "article"
  },
  {
    "title": "TRIÈST",
    "doi": "https://doi.org/10.1145/3059194",
    "publication_date": "2017-06-29",
    "publication_year": 2017,
    "authors": "Lorenzo De Stefani; Alessandro Epasto; Matteo Riondato; Eli Upfal",
    "corresponding_authors": "",
    "abstract": "“Ogni lassada xe persa.” 1 -- Proverb from Trieste, Italy. We present trièst , a suite of one-pass streaming algorithms to compute unbiased, low-variance, high-quality approximations of the global and local (i.e., incident to each vertex) number of triangles in a fully dynamic graph represented as an adversarial stream of edge insertions and deletions. Our algorithms use reservoir sampling and its variants to exploit the user-specified memory space at all times. This is in contrast with previous approaches, which require hard-to-choose parameters (e.g., a fixed sampling probability) and offer no guarantees on the amount of memory they use. We analyze the variance of the estimations and show novel concentration bounds for these quantities. Our experimental results on very large graphs demonstrate that trièst outperforms state-of-the-art approaches in accuracy and exhibits a small update time.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W4248364277",
    "type": "article"
  },
  {
    "title": "A Space-Efficient Streaming Algorithm for Estimating Transitivity and Triangle Counts Using the Birthday Paradox",
    "doi": "https://doi.org/10.1145/2700395",
    "publication_date": "2015-02-17",
    "publication_year": 2015,
    "authors": "Madhav Jha; C. Seshadhri; Ali Pınar",
    "corresponding_authors": "",
    "abstract": "We design a space-efficient algorithm that approximates the transitivity (global clustering coefficient) and total triangle count with only a single pass through a graph given as a stream of edges. Our procedure is based on the classic probabilistic result, the birthday paradox . When the transitivity is constant and there are more edges than wedges (common properties for social networks), we can prove that our algorithm requires O (√ n ) space ( n is the number of vertices) to provide accurate estimates. We run a detailed set of experiments on a variety of real graphs and demonstrate that the memory requirement of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 40,000 edges to give accurate results. Being a single pass streaming algorithm, our procedure also maintains a real-time estimate of the transitivity/number of triangles of a graph by storing a minuscule fraction of edges.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2093063455",
    "type": "article"
  },
  {
    "title": "Unsupervised Rare Pattern Mining",
    "doi": "https://doi.org/10.1145/2898359",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Yun Sing Koh; Sri Devi Ravana",
    "corresponding_authors": "",
    "abstract": "Association rule mining was first introduced to examine patterns among frequent items. The original motivation for seeking these rules arose from need to examine customer purchasing behaviour in supermarket transaction data. It seeks to identify combinations of items or itemsets, whose presence in a transaction affects the likelihood of the presence of another specific item or itemsets. In recent years, there has been an increasing demand for rare association rule mining. Detecting rare patterns in data is a vital task, with numerous high-impact applications including medical, finance, and security. This survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods for rare pattern mining. We investigate the problems in finding rare rules using traditional association rule mining. As rare association rule mining has not been well explored, there is still specific groundwork that needs to be established. We will discuss some of the major issues in rare association rule mining and also look at current algorithms. As a contribution, we give a general framework for categorizing algorithms: Apriori and Tree based. We highlight the differences between these methods. Finally, we present several real-world application using rare pattern mining in diverse domains. We conclude our survey with a discussion on open and practical challenges in the field.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2394978971",
    "type": "article"
  },
  {
    "title": "GrammarViz 3.0",
    "doi": "https://doi.org/10.1145/3051126",
    "publication_date": "2018-02-13",
    "publication_year": 2018,
    "authors": "Pavel Senin; Jessica Lin; Xing Wang; Tim Oates; Sunil Gandhi; Arnold P. Boedihardjo; Crystal Chen; Susan Frankenstein",
    "corresponding_authors": "",
    "abstract": "The problems of recurrent and anomalous pattern discovery in time series, e.g., motifs and discords, respectively, have received a lot of attention from researchers in the past decade. However, since the pattern search space is usually intractable, most existing detection algorithms require that the patterns have discriminative characteristics and have its length known in advance and provided as input, which is an unreasonable requirement for many real-world problems. In addition, patterns of similar structure, but of different lengths may co-exist in a time series. Addressing these issues, we have developed algorithms for variable-length time series pattern discovery that are based on symbolic discretization and grammar inference—two techniques whose combination enables the structured reduction of the search space and discovery of the candidate patterns in linear time. In this work, we present GrammarViz 3.0—a software package that provides implementations of proposed algorithms and graphical user interface for interactive variable-length time series pattern discovery. The current version of the software provides an alternative grammar inference algorithm that improves the time series motif discovery workflow, and introduces an experimental procedure for automated discretization parameter selection that builds upon the minimum cardinality maximum cover principle and aids the time series recurrent and anomalous pattern discovery.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2785441506",
    "type": "article"
  },
  {
    "title": "Refining Social Graph Connectivity via Shortcut Edge Addition",
    "doi": "https://doi.org/10.1145/2757281",
    "publication_date": "2015-10-12",
    "publication_year": 2015,
    "authors": "Manos Papagelis",
    "corresponding_authors": "Manos Papagelis",
    "abstract": "Small changes on the structure of a graph can have a dramatic effect on its connectivity. While in the traditional graph theory, the focus is on well-defined properties of graph connectivity, such as biconnectivity, in the context of a social graph , connectivity is typically manifested by its ability to carry on social processes . In this paper, we consider the problem of adding a small set of nonexisting edges ( shortcuts ) in a social graph with the main objective of minimizing its characteristic path length . This property determines the average distance between pairs of vertices and essentially controls how broadly information can propagate through a network. We formally define the problem of interest, characterize its hardness and propose a novel method, path screening , which quickly identifies important shortcuts to guide the augmentation of the graph. We devise a sampling-based variant of our method that can scale up the computation in larger graphs. The claims of our methods are formally validated. Through experiments on real and synthetic data, we demonstrate that our methods are a multitude of times faster than standard approaches, their accuracy outperforms sensible baselines and they can ease the spread of information in a network, for a varying range of conditions.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W1973522621",
    "type": "article"
  },
  {
    "title": "Adaptive Model Rules From High-Speed Data Streams",
    "doi": "https://doi.org/10.1145/2829955",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "João Duarte; João Gama; Albert Bifet",
    "corresponding_authors": "",
    "abstract": "Decision rules are one of the most expressive and interpretable models for machine learning. In this article, we present Adaptive Model Rules (AMRules), the first stream rule learning algorithm for regression problems. In AMRules, the antecedent of a rule is a conjunction of conditions on the attribute values, and the consequent is a linear combination of the attributes. In order to maintain a regression model compatible with the most recent state of the process generating data, each rule uses a Page-Hinkley test to detect changes in this process and react to changes by pruning the rule set. Online learning might be strongly affected by outliers. AMRules is also equipped with outliers detection mechanisms to avoid model adaption using anomalous examples. In the experimental section, we report the results of AMRules on benchmark regression problems, and compare the performance of our system with other streaming regression algorithms.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2266593536",
    "type": "article"
  },
  {
    "title": "Learning Multiple Diagnosis Codes for ICU Patients with Local Disease Correlation Mining",
    "doi": "https://doi.org/10.1145/3003729",
    "publication_date": "2017-03-06",
    "publication_year": 2017,
    "authors": "Sen Wang; Xue Li; Xiaojun Chang; Lina Yao; Quan Z. Sheng; Guodong Long",
    "corresponding_authors": "",
    "abstract": "In the era of big data, a mechanism that can automatically annotate disease codes to patients’ records in the medical information system is in demand. The purpose of this work is to propose a framework that automatically annotates the disease labels of multi-source patient data in Intensive Care Units (ICUs). We extract features from two main sources, medical charts and notes. The Bag-of-Words model is used to encode the features. Unlike most of the existing multi-label learning algorithms that globally consider correlations between diseases, our model learns disease correlation locally in the patient data. To achieve this, we derive a local disease correlation representation to enrich the discriminant power of each patient data. This representation is embedded into a unified multi-label learning framework. We develop an alternating algorithm to iteratively optimize the objective function. Extensive experiments have been conducted on a real-world ICU database. We have compared our algorithm with representative multi-label learning algorithms. Evaluation results have shown that our proposed method has state-of-the-art performance in the annotation of multiple diagnostic codes for ICU patients. This study suggests that problems in the automated diagnosis code annotation can be reliably addressed by using a multi-label learning model that exploits disease correlation. The findings of this study will greatly benefit health care and management in ICU considering that the automated diagnosis code annotation can significantly improve the quality and management of health care for both patients and caregivers.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2592222102",
    "type": "article"
  },
  {
    "title": "Eigen-Optimization on Large Graphs by Edge Manipulation",
    "doi": "https://doi.org/10.1145/2903148",
    "publication_date": "2016-06-14",
    "publication_year": 2016,
    "authors": "Chen Chen; Hanghang Tong; B. Aditya Prakash; Tina Eliassi‐Rad; Michalis Faloutsos; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "Large graphs are prevalent in many applications and enable a variety of information dissemination processes, e.g., meme, virus, and influence propagation. How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way (e.g., stop a virus propagation, facilitate the propagation of a piece of good idea, etc)? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models. In this paper, we study the problem of how to optimally place a set of edges (e.g., edge deletion and edge addition) to optimize the leading eigenvalue of the underlying graph, so that we can guide the dissemination process in a desired way. We propose effective, scalable algorithms for edge deletion and edge addition, respectively. In addition, we reveal the intrinsic relationship between edge deletion and node deletion problems. Experimental results validate the effectiveness and efficiency of the proposed algorithms.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2431497588",
    "type": "article"
  },
  {
    "title": "Robust Spectral Ensemble Clustering via Rank Minimization",
    "doi": "https://doi.org/10.1145/3278606",
    "publication_date": "2019-01-09",
    "publication_year": 2019,
    "authors": "Zhiqiang Tao; Hongfu Liu; Sheng Li; Zhengming Ding; Yun Fu",
    "corresponding_authors": "",
    "abstract": "Ensemble Clustering (EC) is an important topic for data cluster analysis. It targets to integrate multiple Basic Partitions (BPs) of a particular dataset into a consensus partition. Among previous works, one promising and effective way is to transform EC as a graph partitioning problem on the co-association matrix, which is a pair-wise similarity matrix summarized by all the BPs in essence. However, most existing EC methods directly utilize the co-association matrix, yet without considering various noises (e.g., the disagreement between different BPs and the outliers) that may exist in it. These noises can impair the cluster structure of a co-association matrix, and thus mislead the final graph partitioning process. To address this challenge, we propose a novel Robust Spectral Ensemble Clustering (RSEC) algorithm in this article. Specifically, we learn low-rank representation (LRR) for the co-association matrix to uncover its cluster structure and handle the noises, and meanwhile, we perform spectral clustering with the learned representation to seek for a consensus partition. These two steps are jointly proceeded within a unified optimization framework. In particular, during the optimizing process, we leverage consensus partition to iteratively enhance the block-diagonal structure of LRR, in order to assist the graph partitioning. To solve RSEC, we first formulate it by using nuclear norm as a convex proxy to the rank function. Then, motivated by the recent advances in non-convex rank minimization, we further develop a non-convex model for RSEC and provide it a solution by the majorization--minimization Augmented Lagrange Multiplier algorithm. Experiments on 18 real-world datasets demonstrate the effectiveness of our algorithm compared with state-of-the-art methods. Moreover, several impact factors on the clustering performance of our approach are also explored extensively.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2908931523",
    "type": "article"
  },
  {
    "title": "Bursty Event Detection in Twitter Streams",
    "doi": "https://doi.org/10.1145/3332185",
    "publication_date": "2019-08-08",
    "publication_year": 2019,
    "authors": "Carmela Comito; Agostino Forestiero; Clara Pizzuti",
    "corresponding_authors": "",
    "abstract": "Social media, in recent years, have become an invaluable source of information for both public and private organizations to enhance the comprehension of people interests and the onset of new events. Twitter, especially, allows a fast spread of news and events happening real time that can contribute to situation awareness during emergency situations, but also to understand trending topics of a period. The article proposes an online algorithm that incrementally groups tweet streams into clusters. The approach summarizes the examined tweets into the cluster centroid by maintaining a number of textual and temporal features that allow the method to effectively discover groups of interest on particular themes. Experiments on messages posted by users addressing different issues, and a comparison with state-of-the-art approaches show that the method is capable to detect discussions regarding topics of interest, but also to distinguish bursty events revealed by a sudden spreading of attention on messages published by users.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2966969968",
    "type": "article"
  },
  {
    "title": "Adaptive Local Linear Discriminant Analysis",
    "doi": "https://doi.org/10.1145/3369870",
    "publication_date": "2020-02-03",
    "publication_year": 2020,
    "authors": "Feiping Nie; Zheng Wang; Rong Wang; Zhen Wang; Xuelong Li",
    "corresponding_authors": "",
    "abstract": "Dimensionality reduction plays a significant role in high-dimensional data processing, and Linear Discriminant Analysis (LDA) is a widely used supervised dimensionality reduction approach. However, a major drawback of LDA is that it is incapable of extracting the local structure information, which is crucial for handling multimodal data. In this article, we propose a novel supervised dimensionality reduction method named Adaptive Local Linear Discriminant Analysis (ALLDA), which adaptively learns a k -nearest neighbors graph from data themselves to extract the local connectivity of data. Furthermore, the original high-dimensional data usually contains noisy and redundant features, which has a negative impact on the evaluation of neighborships and degrades the subsequent classification performance. To address this issue, our method learns the similarity matrix and updates the subspace simultaneously so that the neighborships can be evaluated in the optimal subspaces where the noises have been removed. Through the optimal graph embedding, the underlying sub-manifolds of data in intra-class can be extracted precisely. Meanwhile, an efficient iterative optimization algorithm is proposed to solve the minimization problem. Promising experimental results on synthetic and real-world datasets are provided to evaluate the effectiveness of proposed method.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W3004545648",
    "type": "article"
  },
  {
    "title": "Sequential Feature Explanations for Anomaly Detection",
    "doi": "https://doi.org/10.1145/3230666",
    "publication_date": "2019-01-08",
    "publication_year": 2019,
    "authors": "Md Amran Siddiqui; Alan Fern; Thomas G. Dietterich; Weng‐Keen Wong",
    "corresponding_authors": "",
    "abstract": "In many applications, an anomaly detection system presents the most anomalous data instance to a human analyst, who then must determine whether the instance is truly of interest (e.g., a threat in a security setting). Unfortunately, most anomaly detectors provide no explanation about why an instance was considered anomalous, leaving the analyst with no guidance about where to begin the investigation. To address this issue, we study the problems of computing and evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE of an anomaly is a sequence of features, which are presented to the analyst one at a time (in order) until the information contained in the highlighted features is enough for the analyst to make a confident judgement about the anomaly. Since analyst effort is related to the amount of information that they consider in an investigation, an explanation’s quality is related to the number of features that must be revealed to attain confidence. In this article, we first formulate the problem of optimizing SFEs for a particular density-based anomaly detector. We then present both greedy algorithms and an optimal algorithm, based on branch-and-bound search, for optimizing SFEs. Finally, we provide a large scale quantitative evaluation of these algorithms using a novel framework for evaluating explanations. The results show that our algorithms are quite effective and that our best greedy algorithm is competitive with optimal solutions.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2963338867",
    "type": "article"
  },
  {
    "title": "Local Overlapping Community Detection",
    "doi": "https://doi.org/10.1145/3361739",
    "publication_date": "2019-12-13",
    "publication_year": 2019,
    "authors": "Ni Li; Wenjian Luo; Wenjie Zhu; Bei Hua",
    "corresponding_authors": "",
    "abstract": "Local community detection refers to finding the community that contains the given node based on local information, which becomes very meaningful when global information about the network is unavailable or expensive to acquire. Most studies on local community detection focus on finding non-overlapping communities. However, many real-world networks contain overlapping communities like social networks. Given an overlapping node that belongs to multiple communities, the problem is to find communities to which it belongs according to local information. We propose a framework for local overlapping community detection. The framework has three steps. First, find nodes in multiple communities to which the given node belongs. Second, select representative nodes from nodes obtained above, which tends to be in different communities. Third, discover the communities to which these representative nodes belong. In addition, to demonstrate the effectiveness of the framework, we implement six versions of this framework. Experimental results demonstrate that the six implementation versions outperform the other algorithms.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W3004718422",
    "type": "article"
  },
  {
    "title": "Unbiased Measurement of Feature Importance in Tree-Based Methods",
    "doi": "https://doi.org/10.1145/3429445",
    "publication_date": "2021-01-04",
    "publication_year": 2021,
    "authors": "Zhengze Zhou; Giles Hooker",
    "corresponding_authors": "",
    "abstract": "We propose a modification that corrects for split-improvement variable importance measures in Random Forests and other tree-based methods. These methods have been shown to be biased towards increasing the importance of features with more potential splits. We show that by appropriately incorporating split-improvement as measured on out of sample data, this bias can be corrected yielding better summaries and screening tools.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W3118773135",
    "type": "article"
  },
  {
    "title": "Community Detection by Motif-Aware Label Propagation",
    "doi": "https://doi.org/10.1145/3378537",
    "publication_date": "2020-02-09",
    "publication_year": 2020,
    "authors": "Peizhen Li; Ling Huang; Chang‐Dong Wang; Jianhuang Lai; Dong Huang",
    "corresponding_authors": "",
    "abstract": "Community detection (or graph clustering) is crucial for unraveling the structural properties of complex networks. As an important technique in community detection, label propagation has shown the advantage of finding a good community structure with nearly linear time complexity. However, despite the progress that has been made, there are still several important issues that have not been properly addressed. First, the label propagation typically proceeds over the lower order structure of the network and only the direct one-hop connections between nodes are taken into consideration. Unfortunately, the higher order structure that may encode design principle of the network and be crucial for community detection is neglected under this regime. Second, the stability of the identified community structure may also be seriously affected by the inherent randomness in the label propagation process. To tackle the above issues, this article proposes a Motif-Aware Weighted Label Propagation method for community detection. We focus on triangles within the network, but our technique extends to other kinds of motifs as well. Specifically, the motif-based higher order structure mining is conducted to capture structural characteristics of the network. First, the motif of interest (locally meaningful pattern) is identified, and then, the motif-based hypergraph can be constructed to encode the higher order connections. To further utilize the structural information of the network, a re-weighted network is designed, which unifies both the higher order structure and the original lower order structure. Accordingly, a novel voting strategy termed NaS (considering both &lt;underline&gt;N&lt;/underline&gt;umber &lt;underline&gt;a&lt;/underline&gt;nd &lt;underline&gt;S&lt;/underline&gt;trength of connections) is proposed to update node labels during the label propagation process. In this way, the random label selection can be effectively eliminated, yielding more stable community structures. Experimental results on multiple real-world datasets have shown the superiority of the proposed method.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W3008313157",
    "type": "article"
  },
  {
    "title": "Trajectory Outlier Detection",
    "doi": "https://doi.org/10.1145/3425867",
    "publication_date": "2021-02-10",
    "publication_year": 2021,
    "authors": "Youcef Djenouri; Djamel Djenouri; Jerry Chun‐Wei Lin",
    "corresponding_authors": "",
    "abstract": "This article introduces two new problems related to trajectory outlier detection: (1) group trajectory outlier (GTO) detection and (2) deviation point detection for both individual and group of trajectory outliers. Five algorithms are proposed for the first problem by adapting DBSCAN , k nearest neighbors (kNN) , and feature selection (FS) . DBSCAN-GTO first applies DBSCAN to derive the micro clusters , which are considered as potential candidates. A pruning strategy based on density computation measure is then suggested to find the group of trajectory outliers. kNN-GTO recursively derives the trajectory candidates from the individual trajectory outliers and prunes them based on their density. The overall process is repeated for all individual trajectory outliers. FS-GTO considers the set of individual trajectory outliers as the set of all features, while the FS process is used to retrieve the group of trajectory outliers. The proposed algorithms are improved by incorporating ensemble learning and high-performance computing during the detection process. Moreover, we propose a general two-phase-based algorithm for detecting the deviation points, as well as a version for graphic processing units implementation using sliding windows. Experiments on a real trajectory dataset have been carried out to demonstrate the performance of the proposed approaches. The results show that they can efficiently identify useful patterns represented by group of trajectory outliers, deviation points, and that they outperform the baseline group detection algorithms.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W3130499959",
    "type": "article"
  },
  {
    "title": "Core Decomposition in Multilayer Networks",
    "doi": "https://doi.org/10.1145/3369872",
    "publication_date": "2020-02-03",
    "publication_year": 2020,
    "authors": "Edoardo Galimberti; Francesco Bonchi; Francesco Gullo; Tommaso Lanciano",
    "corresponding_authors": "",
    "abstract": "Multilayer networks are a powerful paradigm to model complex systems, where multiple relations occur between the same entities. Despite the keen interest in a variety of tasks, algorithms, and analyses in this type of network, the problem of extracting dense subgraphs has remained largely unexplored so far. As a first step in this direction, in this work, we study the problem of core decomposition of a multilayer network . Unlike the single-layer counterpart in which cores are all nested into one another and can be computed in linear time, the multilayer context is much more challenging as no total order exists among multilayer cores; rather, they form a lattice whose size is exponential in the number of layers. In this setting, we devise three algorithms, which differ in the way they visit the core lattice and in their pruning techniques. We assess time and space efficiency of the three algorithms on a large variety of real-world multilayer networks. We then move a step forward and study the problem of extracting the inner-most (also known as maximal ) cores, i.e., the cores that are not dominated by any other core in terms of their core index in all the layers. inner-most cores are typically orders of magnitude less than all the cores. Motivated by this, we devise an algorithm that effectively exploits the maximality property and extracts inner-most cores directly, without first computing a complete decomposition. This allows for a consistent speed up over a naïve method that simply filters out non-inner-most ones from all the cores. Finally, we showcase the multilayer core-decomposition tool in a variety of scenarios and problems. We start by considering the problem of densest-subgraph extraction in multilayer networks . We introduce a definition of multilayer densest subgraph that tradesoff between high density and number of layers in which the high density holds, and exploit multilayer core decomposition to approximate this problem with quality guarantees. As further applications, we show how to utilize multilayer core decomposition to speed-up the extraction of frequent cross-graph quasi-cliques and to generalize the community-search problem to the multilayer setting.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W3004777475",
    "type": "article"
  },
  {
    "title": "Deep Graph Matching and Searching for Semantic Code Retrieval",
    "doi": "https://doi.org/10.1145/3447571",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Xiang Ling; Lingfei Wu; Saizhuo Wang; Gaoning Pan; Tengfei Ma; Fangli Xu; Alex X. Liu; Chunming Wu; Shouling Ji",
    "corresponding_authors": "",
    "abstract": "Code retrieval is to find the code snippet from a large corpus of source code repositories that highly matches the query of natural language description. Recent work mainly uses natural language processing techniques to process both query texts (i.e., human natural language) and code snippets (i.e., machine programming language), however neglecting the deep structured features of query texts and source codes, both of which contain rich semantic information. In this paper, we propose an end-to-end deep graph matching and searching (DGMS) model based on graph neural networks for the task of semantic code retrieval. To this end, we first represent both natural language query texts and programming language code snippets with the unified graph-structured data, and then use the proposed graph matching and searching model to retrieve the best matching code snippet. In particular, DGMS not only captures more structural information for individual query texts or code snippets but also learns the fine-grained similarity between them by cross-attention based semantic matching operations. We evaluate the proposed DGMS model on two public code retrieval datasets with two representative programming languages (i.e., Java and Python). Experiment results demonstrate that DGMS significantly outperforms state-of-the-art baseline models by a large margin on both datasets. Moreover, our extensive ablation studies systematically investigate and illustrate the impact of each part of DGMS.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W3093604544",
    "type": "article"
  },
  {
    "title": "Variable-lag Granger Causality and Transfer Entropy for Time Series Analysis",
    "doi": "https://doi.org/10.1145/3441452",
    "publication_date": "2021-05-08",
    "publication_year": 2021,
    "authors": "Chainarong Amornbunchornvej; Elena Zheleva; Tanya Berger‐Wolf",
    "corresponding_authors": "",
    "abstract": "Granger causality is a fundamental technique for causal inference in time series data, commonly used in the social and biological sciences. Typical operationalizations of Granger causality make a strong assumption that every time point of the effect time series is influenced by a combination of other time series with a fixed time delay. The assumption of fixed time delay also exists in Transfer Entropy, which is considered to be a non-linear version of Granger causality. However, the assumption of the fixed time delay does not hold in many applications, such as collective behavior, financial markets, and many natural phenomena. To address this issue, we develop Variable-lag Granger causality and Variable-lag Transfer Entropy, generalizations of both Granger causality and Transfer Entropy that relax the assumption of the fixed time delay and allow causes to influence effects with arbitrary time delays. In addition, we propose methods for inferring both Variable-lag Granger causality and Transfer Entropy relations. In our approaches, we utilize an optimal warping path of Dynamic Time Warping to infer variable-lag causal relations. We demonstrate our approaches on an application for studying coordinated collective behavior and other real-world casual-inference datasets and show that our proposed approaches perform better than several existing methods in both simulated and real-world datasets. Our approaches can be applied in any domain of time series analysis. The software of this work is available in the R-CRAN package: VLTimeCausality.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W3161692194",
    "type": "article"
  },
  {
    "title": "MULFE: Multi-Label Learning via Label-Specific Feature Space Ensemble",
    "doi": "https://doi.org/10.1145/3451392",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Yaojin Lin; Qinghua Hu; Jinghua Liu; Xingquan Zhu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "In multi-label learning, label correlations commonly exist in the data. Such correlation not only provides useful information, but also imposes significant challenges for multi-label learning. Recently, label-specific feature embedding has been proposed to explore label-specific features from the training data, and uses feature highly customized to the multi-label set for learning. While such feature embedding methods have demonstrated good performance, the creation of the feature embedding space is only based on a single label, without considering label correlations in the data. In this article, we propose to combine multiple label-specific feature spaces, using label correlation, for multi-label learning. The proposed algorithm, mu lti- l abel-specific f eature space e nsemble (MULFE), takes consideration label-specific features, label correlation, and weighted ensemble principle to form a learning framework. By conducting clustering analysis on each label’s negative and positive instances, MULFE first creates features customized to each label. After that, MULFE utilizes the label correlation to optimize the margin distribution of the base classifiers which are induced by the related label-specific feature spaces. By combining multiple label-specific features, label correlation based weighting, and ensemble learning, MULFE achieves maximum margin multi-label classification goal through the underlying optimization framework. Empirical studies on 10 public data sets manifest the effectiveness of MULFE.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W3184825405",
    "type": "article"
  },
  {
    "title": "Modeling Temporal Patterns with Dilated Convolutions for Time-Series Forecasting",
    "doi": "https://doi.org/10.1145/3453724",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Yangfan Li; Kenli Li; Cen Chen; Xu Zhou; Zeng Zeng; Keqin Li",
    "corresponding_authors": "",
    "abstract": "Time-series forecasting is an important problem across a wide range of domains. Designing accurate and prompt forecasting algorithms is a non-trivial task, as temporal data that arise in real applications often involve both non-linear dynamics and linear dependencies, and always have some mixtures of sequential and periodic patterns, such as daily, weekly repetitions, and so on. At this point, however, most recent deep models often use Recurrent Neural Networks (RNNs) to capture these temporal patterns, which is hard to parallelize and not fast enough for real-world applications especially when a huge amount of user requests are coming. Recently, CNNs have demonstrated significant advantages for sequence modeling tasks over the de-facto RNNs, while providing high computational efficiency due to the inherent parallelism. In this work, we propose HyDCNN, a novel hybrid framework based on fully Dilated CNN for time-series forecasting tasks. The core component in HyDCNN is a proposed hybrid module, in which our proposed position-aware dilated CNNs are utilized to capture the sequential non-linear dynamics and an autoregressive model is leveraged to capture the sequential linear dependencies. To further capture the periodic temporal patterns, a novel hop scheme is introduced in the hybrid module. HyDCNN is then composed of multiple hybrid modules to capture the sequential and periodic patterns. Each of these hybrid modules targets on either the sequential pattern or one kind of periodic patterns. Extensive experiments on five real-world datasets have shown that the proposed HyDCNN is better compared with state-of-the-art baselines and is at least 200% better than RNN baselines. The datasets and source code will be published in Github to facilitate more future work.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W3185846556",
    "type": "article"
  },
  {
    "title": "3DGCN: 3-Dimensional Dynamic Graph Convolutional Network for Citywide Crowd Flow Prediction",
    "doi": "https://doi.org/10.1145/3451394",
    "publication_date": "2021-06-28",
    "publication_year": 2021,
    "authors": "Xia Tong; Junjie Lin; Yong Li; Jie Feng; Pan Hui; Funing Sun; Diansheng Guo; Depeng Jin",
    "corresponding_authors": "",
    "abstract": "Crowd flow prediction is an essential task benefiting a wide range of applications for the transportation system and public safety. However, it is a challenging problem due to the complex spatio-temporal dependence and the complicated impact of urban structure on the crowd flow patterns. In this article, we propose a novel framework, 3- D imensional G raph C onvolution N etwork (3DGCN), to predict citywide crowd flow. We first model it as a dynamic spatio-temporal graph prediction problem, where each node represents a region with time-varying flows, and each edge represents the origin–destination (OD) flow between its corresponding regions. As such, OD flows among regions are treated as a proxy for the spatial interactions among regions. To tackle the complex spatio-temporal dependence, our proposed 3DGCN can model the correlation among graph spatial and temporal neighbors simultaneously. To learn and incorporate urban structures in crowd flow prediction, we design the GCN aggregator to be learned from both crowd flow prediction and region function inference at the same time. Extensive experiments with real-world datasets in two cities demonstrate that our model outperforms state-of-the-art baselines by 9.6%∼19.5% for the next-time-interval prediction.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3176279440",
    "type": "article"
  },
  {
    "title": "Graph Neural News Recommendation with User Existing and Potential Interest Modeling",
    "doi": "https://doi.org/10.1145/3511708",
    "publication_date": "2022-02-04",
    "publication_year": 2022,
    "authors": "Zhaopeng Qiu; Yunfan Hu; Xian Wu",
    "corresponding_authors": "",
    "abstract": "Personalized news recommendations can alleviate the information overload problem. To enable personalized recommendation, one critical step is to learn a comprehensive user representation to model her/his interests. Many existing works learn user representations from the historical clicked news articles, which reflect their existing interests. However, these approaches ignore users’ potential interests and pay less attention to news that may interest the users in the future. To address this problem, we propose a novel G raph neural news R ecommendation model with user E xisting and P otential interest modeling, named GREP. Different from existing works, GREP introduces three modules to jointly model users’ existing and potential interests: (1) Existing Interest Encoding module mines user historical clicked news and applies the multi-head self-attention mechanism to capture the relatedness among the news; (2) Potential Interest Encoding module leverages the graph neural network to explore the user potential interests on the knowledge graph; and (3) Bi-directional Interaction module dynamically builds a news-entity bipartite graph to further enrich two interest representations. Finally, GREP combines the existing and potential interest representations to represent the user and leverages a prediction layer to estimate the clicking probability of the candidate news. Experiments on two real-world large-scale datasets demonstrate the state-of-the-art performance of GREP.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W4210509099",
    "type": "article"
  },
  {
    "title": "STAD-GAN: Unsupervised Anomaly Detection on Multivariate Time Series with Self-training Generative Adversarial Networks",
    "doi": "https://doi.org/10.1145/3572780",
    "publication_date": "2022-11-23",
    "publication_year": 2022,
    "authors": "Zhijie Zhang; Wenzhong Li; Wangxiang Ding; Linming Zhang; Qingning Lu; Peng Hu; Tong Gui; Sanglu Lu",
    "corresponding_authors": "",
    "abstract": "Anomaly detection on multivariate time series (MTS) is an important research topic in data mining, which has a wide range of applications in information technology, financial management, manufacturing system, and so on. However, the state-of-the-art unsupervised deep learning models for MTS anomaly detection are vulnerable to noise and have poor performance on the training data containing anomalies. In this article, we propose a novel Self-Training based Anomaly Detection with Generative Adversarial Network (GAN) model called STAD-GAN to address the practical challenge. The STAD-GAN model consists of a generator-discriminator structure for adversarial learning and a neural network classifier for anomaly classification. The generator is learned to capture the normal data distribution, and the discriminator is learned to amplify the reconstruction error of abnormal data for better recognition. The proposed model is optimized with a self-training teacher-student framework, where a teacher model generates reliable high-quality pseudo-labels to train a student model iteratively with a refined dataset so that the performance of the anomaly classifier can be gradually improved. Extensive experiments based on six open MTS datasets show that STAD-GAN is robust to noise and achieves significant performance improvement compared to the state-of-the-art.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4309765030",
    "type": "article"
  },
  {
    "title": "A Semantics-enhanced Topic Modelling Technique: Semantic-LDA",
    "doi": "https://doi.org/10.1145/3639409",
    "publication_date": "2024-01-02",
    "publication_year": 2024,
    "authors": "Dakshi Kapugama Geeganage; Yue Xu; Yuefeng Li",
    "corresponding_authors": "",
    "abstract": "Topic modelling is a beneficial technique used to discover latent topics in text collections. But to correctly understand the text content and generate a meaningful topic list, semantics are important. By ignoring semantics, that is, not attempting to grasp the meaning of the words, most of the existing topic modelling approaches can generate some meaningless topic words. Even existing semantic-based approaches usually interpret the meanings of words without considering the context and related words. In this article, we introduce a semantic-based topic model called semantic-LDA that captures the semantics of words in a text collection using concepts from an external ontology. A new method is introduced to identify and quantify the concept–word relationships based on matching words from the input text collection with concepts from an ontology without using pre-calculated values from the ontology that quantify the relationships between the words and concepts. These pre-calculated values may not reflect the actual relationships between words and concepts for the input collection, because they are derived from datasets used to build the ontology rather than from the input collection itself. Instead, quantifying the relationship based on the word distribution in the input collection is more realistic and beneficial in the semantic capture process. Furthermore, an ambiguity handling mechanism is introduced to interpret the unmatched words, that is, words for which there are no matching concepts in the ontology. Thus, this article makes a significant contribution by introducing a semantic-based topic model that calculates the word–concept relationships directly from the input text collection. The proposed semantic-based topic model and an enhanced version with the disambiguation mechanism were evaluated against a set of state-of-the-art systems, and our approaches outperformed the baseline systems in both topic quality and information filtering evaluations.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4390490760",
    "type": "article"
  },
  {
    "title": "Fair Feature Selection: A Causal Perspective",
    "doi": "https://doi.org/10.1145/3643890",
    "publication_date": "2024-02-03",
    "publication_year": 2024,
    "authors": "Zhaolong Ling; Enqi Xu; Peng Zhou; Liang Du; Kui Yu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Fair feature selection for classification decision tasks has recently garnered significant attention from researchers. However, existing fair feature selection algorithms fall short of providing a full explanation of the causal relationship between features and sensitive attributes, potentially impacting the accuracy of fair feature identification. To address this issue, we propose a fair causal feature selection algorithm, called FairCFS . Specifically, FairCFS constructs a localized causal graph that identifies the Markov blankets of class and sensitive variables, to block the transmission of sensitive information for selecting fair causal features. Extensive experiments on seven public real-world datasets validate that FairCFS has accuracy comparable to eight state-of-the-art feature selection algorithms while presenting more superior fairness.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4391514782",
    "type": "article"
  },
  {
    "title": "VAE*: A Novel Variational Autoencoder via Revisiting Positive and Negative Samples for Top- <i>N</i> Recommendation",
    "doi": "https://doi.org/10.1145/3680552",
    "publication_date": "2024-08-10",
    "publication_year": 2024,
    "authors": "Wei Liu; Leong Hou U; Shangsong Liang; Huaijie Zhu; Jianxing Yu; Y Liu; Jian Yin",
    "corresponding_authors": "",
    "abstract": "Due to the easy access, implicit feedback is often used for recommender systems. Compared with point-wise learning and pair-wise learning methods, list-wise rank learning methods have superior performance for top- \\(N\\) recommendation. Recent solutions, especially the list-wise methods, simply treat all interacted items of a user as equally important positives and annotate all no-interaction items of a user as negatives. For the list-wise approaches, we argue that this annotation scheme of implicit feedback is over-simplified due to the sparsity and missing fine-grained labels of the feedback data. To overcome this issue, we revisit the so-called positive and negative samples. First, considering the loss function of list-wise ranking, we analyze the impact of false positives and negatives theoretically. Second, based on the observation, we propose a self-adjusting credibility weight mechanism to re-weigh the positive samples and exploit the higher-order relation based on item–item matrix to sample the critical negative samples. In order to prevent the introduction of noise, we design a pruning strategy for critical negatives. Besides, to combine the reconstruction loss function for the positive samples and critical negative samples, we develop a simple yet effective VAEs framework with linear structure, which abandons the complex non-linear structure. Extensive experiments are conducted on six public real-world datasets. The results demonstrate that, our VAE* outperforms other VAE-based models by a large margin. Besides, we also verify the effect of denoising positives and exploring critical negatives by ablation study.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4401482781",
    "type": "article"
  },
  {
    "title": "Stochastic Block Models for Complex Network Analysis: A Survey",
    "doi": "https://doi.org/10.1145/3713076",
    "publication_date": "2025-01-22",
    "publication_year": 2025,
    "authors": "Xueyan Liu; Wenzhuo Song; Katarzyna Musiał; Yang Li; Xuehua Zhao; Bo Yang",
    "corresponding_authors": "",
    "abstract": "Complex networks enable to represent and characterize the interactions between entities in various complex systems which widely exist in the real world and usually generate vast amounts of data about all the elements, their behaviors and interactions over time. The studies concentrating on new network analysis approaches and methodologies are vital because of the diversity and ubiquity of complex networks. The stochastic block model (SBM), based on Bayesian theory, is a statistical network model. SBMs are essential tools for analyzing complex networks since SBMs have the advantages of interpretability, expressiveness, flexibility and generalization. Thus, designing diverse SBMs and their learning algorithms for various networks has become an intensively researched topic in network analysis and data mining. In this paper, we review, in a comprehensive and in-depth manner, SBMs for different types of networks (i.e., model extensions), existing methods (including parameter estimation and model selection) for learning optimal SBMs for given networks and SBMs combined with deep learning. Finally, we provide an outlook on the future research directions of SBMs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406708380",
    "type": "article"
  },
  {
    "title": "An Interpretable Deep Learning-based Model for Decision-making through Piecewise Linear Approximation",
    "doi": "https://doi.org/10.1145/3715150",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Mengzhuo Guo; Qingpeng Zhang; Daniel Zeng",
    "corresponding_authors": "",
    "abstract": "Full complexity machine learning models, such as the deep neural network, are non-traceable black-box, whereas the classic interpretable models, such as linear regression models, are often over-simplified, leading to lower accuracy. Model interpretability limits the application of machine learning models in management problems, which requires high prediction performance, as well as the understanding of individual features’ contributions to the model outcome. To enhance model interpretability while preserving good prediction performance, we propose a hybrid interpretable model that combines a piecewise linear component and a nonlinear component. The first component describes the explicit feature contributions by piecewise linear approximation to increase the expressiveness of the model. The other component uses a multi-layer perceptron to increase the prediction performance by capturing the high-order interactions between features and their complex nonlinear transformations. The interpretability is obtained once the model is learned in the form of shape functions for the main effects. We also provide a variant to explore the higher-order interactions among features. Experiments are conducted on synthetic and real-world data sets to demonstrate that the proposed models can achieve good interpretability by explicitly describing the main effects and the interaction effects of the features while maintaining state-of-the-art accuracy.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406798094",
    "type": "article"
  },
  {
    "title": "Mining Repetitive Negative Sequential Patterns With Gap Constraints",
    "doi": "https://doi.org/10.1145/3716390",
    "publication_date": "2025-02-07",
    "publication_year": 2025,
    "authors": "Yan Li; Zhulin Wang; Jing Liu; Lei Guo; Philippe Fournier‐Viger; Youxi Wu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Sequential pattern mining (SPM) with gap constraints (or repetitive SPM or tandem repeat discovery in bioinformatics) can find frequent repetitive subsequences satisfying gap constraints, which are called positive sequential patterns with gap constraints (PSPGs). However, classical SPM with gap constraints cannot find the frequent missing items in the PSPGs. To tackle this issue, this paper explores negative sequential patterns with gap constraints (NSPGs). We propose an efficient NSPG-Miner algorithm that can mine both frequent PSPGs and NSPGs simultaneously. To effectively reduce candidate patterns, we propose a pattern join strategy with negative patterns which can generate both positive and negative candidate patterns at the same time. To calculate the support (frequency of occurrence) of a pattern in each sequence, we explore a NegPair algorithm that employs a key-value pair array structure to deal with the gap constraints and the negative items simultaneously and can avoid redundant rescanning of the original sequence, thus improving the efficiency of the algorithm. To report the performance of NSPG-Miner, 11 competitive algorithms and 11 datasets are employed. The experimental results not only validate the effectiveness of the strategies adopted by NSPG-Miner, but also verify that NSPG-Miner can discover more valuable information than the state-of-the-art algorithms. Algorithms and datasets can be downloaded from https://github.com/wuc567/Pattern-Mining/tree/master/NSPG-Miner .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4407240150",
    "type": "article"
  },
  {
    "title": "Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting",
    "doi": "https://doi.org/10.1145/3727622",
    "publication_date": "2025-04-02",
    "publication_year": 2025,
    "authors": "Zhanyu Liu; Guanjie Zheng; Yanwei Yu",
    "corresponding_authors": "",
    "abstract": "Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called M ulti-scale T raffic P attern B ank ( MTPB ). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, the traffic data of the data-scarce target city could query the traffic pattern bank, facilitating the aggregation of meta-knowledge. This meta-knowledge, in turn, assumes a pivotal role as a robust guide in subsequent processes involving graph reconstruction and forecasting. Empirical assessments conducted on real-world traffic datasets affirm the superior performance of MTPB, surpassing existing methods across various categories and exhibiting numerous attributes conducive to the advancement of cross-city few-shot forecasting methodologies. The code is available in https://github.com/zhyliu00/MTPB .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4409095137",
    "type": "article"
  },
  {
    "title": "MagiNet: Mask-Aware Graph Imputation Network for Incomplete Traffic Data",
    "doi": "https://doi.org/10.1145/3743141",
    "publication_date": "2025-06-06",
    "publication_year": 2025,
    "authors": "Jianping Zhou; Bin Lü; Zhanyu Liu; Siyu Pan; Xuejun Feng; Hua Wei; Guanjie Zheng; Xinbing Wang; Chenghu Zhou",
    "corresponding_authors": "",
    "abstract": "Due to detector malfunctions and communication failures, missing data is ubiquitous during the collection of traffic data. Therefore, it is of vital importance to impute the missing values to facilitate data analysis and decision-making for Intelligent Transportation System (ITS). However, existing imputation methods generally perform zero pre-filling techniques to initialize missing values, introducing inevitable noise. Moreover, we observe prevalent over-smoothed interpolations, falling short in revealing the intrinsic spatio-temporal correlations of incomplete traffic data. To this end, we propose M ask- a ware g raph i mputation Net work : MagiNet . Our method designs an adaptive mask spatio-temporal encoder to learn the latent representations of incomplete data, eliminating the reliance on pre-filling missing values. Furthermore, we devise a spatio-temporal decoder that stacks multiple blocks to capture the inherent spatial and temporal dependencies within incomplete traffic data, alleviating over-smoothed imputation. Extensive experiments demonstrate that our method outperforms state-of-the-art imputation methods on five real-world traffic datasets, yielding an average improvement of 4.31% in RMSE and 3.72% in MAPE under Missing Completely At Random (MCAR) pattern. Code is available at https://github.com/JeremyChou28/MagiNet .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4411088760",
    "type": "article"
  },
  {
    "title": "Constructing comprehensive summaries of large event sequences",
    "doi": "https://doi.org/10.1145/1631162.1631169",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Jerry Kiernan; Evimaria Terzi",
    "corresponding_authors": "",
    "abstract": "Event sequences capture system and user activity over time. Prior research on sequence mining has mostly focused on discovering local patterns appearing in a sequence. While interesting, these patterns do not give a comprehensive summary of the entire event sequence. Moreover, the number of patterns discovered can be large. In this article, we take an alternative approach and build short summaries that describe an entire sequence, and discover local dependencies between event types. We formally define the summarization problem as an optimization problem that balances shortness of the summary with accuracy of the data description. We show that this problem can be solved optimally in polynomial time by using a combination of two dynamic-programming algorithms. We also explore more efficient greedy alternatives and demonstrate that they work well on large datasets. Experiments on both synthetic and real datasets illustrate that our algorithms are efficient and produce high-quality results, and reveal interesting local structures in the data.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2031915852",
    "type": "article"
  },
  {
    "title": "Comparative document summarization via discriminative sentence selection",
    "doi": "https://doi.org/10.1145/2362383.2362386",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Dingding Wang; Shenghuo Zhu; Tao Li; Yihong Gong",
    "corresponding_authors": "",
    "abstract": "Given a collection of document groups, a natural question is to identify the differences among them. Although traditional document summarization techniques can summarize the content of the document groups one by one, there exists a great necessity to generate a summary of the differences among the document groups. In this article, we study a novel problem, that of summarizing the differences between document groups. A discriminative sentence selection method is proposed to extract the most discriminative sentences which represent the specific characteristics of each document group. Experiments and case studies on real-world data sets demonstrate the effectiveness of our proposed method.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2021579266",
    "type": "article"
  },
  {
    "title": "Summarizing data succinctly with the most informative itemsets",
    "doi": "https://doi.org/10.1145/2382577.2382580",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Michael Mampaey; Jilles Vreeken; Nikolaj Tatti",
    "corresponding_authors": "",
    "abstract": "Knowledge discovery from data is an inherently iterative process. That is, what we know about the data greatly determines our expectations, and therefore, what results we would find interesting and/or surprising. Given new knowledge about the data, our expectations will change. Hence, in order to avoid redundant results, knowledge discovery algorithms ideally should follow such an iterative updating procedure. With this in mind, we introduce a well-founded approach for succinctly summarizing data with the most informative itemsets; using a probabilistic maximum entropy model, we iteratively find the itemset that provides us the most novel information—that is, for which the frequency in the data surprises us the most—and in turn we update our model accordingly. As we use the maximum entropy principle to obtain unbiased probabilistic models, and only include those itemsets that are most informative with regard to the current model, the summaries we construct are guaranteed to be both descriptive and nonredundant. The algorithm that we present, called mtv, can either discover the top- k most informative itemsets, or we can employ either the Bayesian Information Criterion (bic) or the Minimum Description Length (mdl) principle to automatically identify the set of itemsets that together summarize the data well. In other words, our method will “tell you what you need to know” about the data. Importantly, it is a one-phase algorithm: rather than picking itemsets from a user-provided candidate set, itemsets and their supports are mined on-the-fly. To further its applicability, we provide an efficient method to compute the maximum entropy distribution using Quick Inclusion-Exclusion. Experiments on our method, using synthetic, benchmark, and real data, show that the discovered summaries are succinct, and correctly identify the key patterns in the data. The models they form attain high likelihoods, and inspection shows that they summarize the data well with increasingly specific, yet nonredundant itemsets.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W3121180423",
    "type": "article"
  },
  {
    "title": "Socializing by Gaming",
    "doi": "https://doi.org/10.1145/2736698",
    "publication_date": "2015-10-12",
    "publication_year": 2015,
    "authors": "Adele Lu Jia; Siqi Shen; Ruud van de Bovenkamp; Alexandru Iosup; Fernando Kuipers; Dick Epema",
    "corresponding_authors": "",
    "abstract": "Multiplayer Online Games (MOGs) like Defense of the Ancients and StarCraft II have attracted hundreds of millions of users who communicate, interact, and socialize with each other through gaming. In MOGs, rich social relationships emerge and can be used to improve gaming services such as match recommendation and game population retention, which are important for the user experience and the commercial value of the companies who run these MOGs. In this work, we focus on understanding social relationships in MOGs. We propose a graph model that is able to capture social relationships of a variety of types and strengths. We apply our model to real-world data collected from three MOGs that contain in total over ten years of behavioral history for millions of players and matches. We compare social relationships in MOGs across different game genres and with regular online social networks like Facebook. Taking match recommendation as an example application of our model, we propose SAMRA, a Socially Aware Match Recommendation Algorithm that takes social relationships into account. We show that our model not only improves the precision of traditional link prediction approaches, but also potentially helps players enjoy games to a higher extent.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W1996700789",
    "type": "article"
  },
  {
    "title": "Data-Aware Vaccine Allocation Over Large Networks",
    "doi": "https://doi.org/10.1145/2803176",
    "publication_date": "2015-10-12",
    "publication_year": 2015,
    "authors": "Yao Zhang; B. Aditya Prakash",
    "corresponding_authors": "",
    "abstract": "Given a graph, like a social/computer network or the blogosphere, in which an infection (or meme or virus) has been spreading for some time, how to select the k best nodes for immunization/quarantining immediately? Most previous works for controlling propagation (say via immunization) have concentrated on developing strategies for vaccination preemptively before the start of the epidemic. While very useful to provide insights in to which baseline policies can best control an infection, they may not be ideal to make real-time decisions as the infection is progressing. In this paper, we study how to immunize healthy nodes, in the presence of already infected nodes. Efficient algorithms for such a problem can help public-health experts make more informed choices, tailoring their decisions to the actual distribution of the epidemic on the ground. First we formulate the Data-Aware Vaccination problem, and prove it is NP-hard and also that it is hard to approximate. Secondly, we propose three effective polynomial-time heuristics DAVA, DAVA-prune and DAVA-fast, of varying degrees of efficiency and performance. Finally, we also demonstrate the scalability and effectiveness of our algorithms through extensive experiments on multiple real networks including large epidemiology datasets (containing millions of interactions). Our algorithms show substantial gains of up to ten times more healthy nodes at the end against many other intuitive and nontrivial competitors.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2085419924",
    "type": "article"
  },
  {
    "title": "Nearest Neighbor-Based Classification of Uncertain Data",
    "doi": "https://doi.org/10.1145/2435209.2435210",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Fabrizio Angiulli; Fabio Fassetti",
    "corresponding_authors": "",
    "abstract": "This work deals with the problem of classifying uncertain data. With this aim we introduce the Uncertain Nearest Neighbor (UNN) rule, which represents the generalization of the deterministic nearest neighbor rule to the case in which uncertain objects are available. The UNN rule relies on the concept of nearest neighbor class, rather than on that of nearest neighbor object. The nearest neighbor class of a test object is the class that maximizes the probability of providing its nearest neighbor. The evidence is that the former concept is much more powerful than the latter in the presence of uncertainty, in that it correctly models the right semantics of the nearest neighbor decision rule when applied to the uncertain scenario. An effective and efficient algorithm to perform uncertain nearest neighbor classification of a generic (un)certain test object is designed, based on properties that greatly reduce the temporal cost associated with nearest neighbor class probability computation. Experimental results are presented, showing that the UNN rule is effective and efficient in classifying uncertain data.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W1993659579",
    "type": "article"
  },
  {
    "title": "Efficient Discovery of Association Rules and Frequent Itemsets through Sampling with Tight Performance Guarantees",
    "doi": "https://doi.org/10.1145/2629586",
    "publication_date": "2014-08-29",
    "publication_year": 2014,
    "authors": "Matteo Riondato; Eli Upfal",
    "corresponding_authors": "",
    "abstract": "The tasks of extracting (top- K ) Frequent Itemsets (FIs) and Association Rules (ARs) are fundamental primitives in data mining and database applications. Exact algorithms for these problems exist and are widely used, but their running time is hindered by the need of scanning the entire dataset, possibly multiple times. High-quality approximations of FIs and ARs are sufficient for most practical uses. Sampling techniques can be used for fast discovery of approximate solutions, but works exploring this technique did not provide satisfactory performance guarantees on the quality of the approximation due to the difficulty of bounding the probability of under- or oversampling any one of an unknown number of frequent itemsets. We circumvent this issue by applying the statistical concept of Vapnik-Chervonenkis (VC) dimension to develop a novel technique for providing tight bounds on the sample size that guarantees approximation of the (top- K ) FIs and ARs within user-specified parameters. The resulting sample size is linearly dependent on the VC-dimension of a range space associated with the dataset. We analyze the VC-dimension of this range space and show that it is upper bounded by an easy-to-compute characteristic quantity of the dataset, the d-index , namely, the maximum integer d such that the dataset contains at least d transactions of length at least d such that no one of them is a superset of or equal to another. We show that this bound is tight for a large class of datasets. The resulting sample size is a significant improvement over previous known results. We present an extensive experimental evaluation of our technique on real and artificial datasets, demonstrating the practicality of our methods, and showing that they achieve even higher quality approximations than what is guaranteed by the analysis.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2129501055",
    "type": "article"
  },
  {
    "title": "Mining Event-Oriented Topics in Microblog Stream with Unsupervised Multi-View Hierarchical Embedding",
    "doi": "https://doi.org/10.1145/3173044",
    "publication_date": "2018-04-11",
    "publication_year": 2018,
    "authors": "Min Peng; Jiahui Zhu; Hua Wang; Xu‐Hui Li; Yanchun Zhang; Xiuzhen Zhang; Gang Tian",
    "corresponding_authors": "",
    "abstract": "This article presents an unsupervised multi-view hierarchical embedding (UMHE) framework to sufficiently reveal the intrinsic topical knowledge in social events. Event-oriented topics are highly related to such events as it can provide explicit descriptions of what have happened in social community. In many real-world cases, however, it is difficult to include all attributes of microblogs, more often, textual aspects only are available. Traditional topic modelling methods have failed to generate event-oriented topics with the textual aspects, since the inherent relations between topics are often overlooked in these methods. Meanwhile, the metrics in original word vocabulary space might not effectively capture semantic distances. Our UMHE framework overcomes the severe information deficiency and poor feature representation. The UMHE first develops a multi-view Bayesian rose tree to preliminarily generate prior knowledge for latent topics and their relations. With such prior knowledge, we design an unsupervised translation-based hierarchical embedding method to make a better representation of these latent topics. By applying self-adaptive spectral clustering on the embedding space and the original space concomitantly, we eventually extract event-oriented topics in word distributions to express social events. Our framework is purely data-driven and unsupervised, without any external knowledge. Experimental results on TREC Tweets2011 dataset and Sina Weibo dataset demonstrate that the UMHE framework can construct hierarchical structure with high fitness, but also yield topic embeddings with salient semantics; therefore, it can derive event-oriented topics with meaningful descriptions.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2798001063",
    "type": "article"
  },
  {
    "title": "Motif Counting Beyond Five Nodes",
    "doi": "https://doi.org/10.1145/3186586",
    "publication_date": "2018-04-16",
    "publication_year": 2018,
    "authors": "Marco Bressan; Flavio Chierichetti; Ravi Kumar; Stefano Leucci; Alessandro Panconesi",
    "corresponding_authors": "",
    "abstract": "Counting graphlets is a well-studied problem in graph mining and social network analysis. Recently, several papers explored very simple and natural algorithms based on Monte Carlo sampling of Markov Chains (MC), and reported encouraging results. We show, perhaps surprisingly, that such algorithms are outperformed by color coding (CC) [2], a sophisticated algorithmic technique that we extend to the case of graphlet sampling and for which we prove strong statistical guarantees. Our computational experiments on graphs with millions of nodes show CC to be more accurate than MC; furthermore, we formally show that the mixing time of the MC approach is too high in general, even when the input graph has high conductance. All this comes at a price however. While MC is very efficient in terms of space, CC’s memory requirements become demanding when the size of the input graph and that of the graphlets grow. And yet, our experiments show that CC can push the limits of the state-of-the-art, both in terms of the size of the input graph and of that of the graphlets.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2802109861",
    "type": "article"
  },
  {
    "title": "Entity-Based Query Recommendation for Long-Tail Queries",
    "doi": "https://doi.org/10.1145/3233186",
    "publication_date": "2018-08-22",
    "publication_year": 2018,
    "authors": "Zhipeng Huang; Bogdan Cautis; Reynold Cheng; Yudian Zheng; Nikos Mamoulis; Jing Nathan Yan",
    "corresponding_authors": "",
    "abstract": "Query recommendation, which suggests related queries to search engine users, has attracted a lot of attention in recent years. Most of the existing solutions, which perform analysis of users’ search history (or query logs ), are often insufficient for long-tail queries that rarely appear in query logs. To handle such queries, we study the use of entities found in queries to provide recommendations. Specifically, we extract entities from a query, and use these entities to explore new ones by consulting an information source. The discovered entities are then used to suggest new queries to the user. In this article, we examine two information sources: (1) a knowledge base (or KB), such as YAGO and Freebase; and (2) a click log, which contains the URLs accessed by a query user. We study how to use these sources to find new entities useful for query recommendation. We further study a hybrid framework that integrates different query recommendation methods effectively. As shown in the experiments, our proposed approaches provide better recommendations than existing solutions for long-tail queries. In addition, our query recommendation process takes less than 100ms to complete. Thus, our solution is suitable for providing online query recommendation services for search engines.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2888021824",
    "type": "article"
  },
  {
    "title": "Road Network Construction with Complex Intersections Based on Sparsely Sampled Private Car Trajectory Data",
    "doi": "https://doi.org/10.1145/3326060",
    "publication_date": "2019-06-20",
    "publication_year": 2019,
    "authors": "Huang You-rong; Zhu Xiao; Xiaoyou Yu; Dong Wang; Vincent Havyarimana; Jing Bai",
    "corresponding_authors": "",
    "abstract": "A road network is a critical aspect of both urban planning and route recommendation. This article proposes an efficient approach to build a fine-grained road network based on sparsely sampled private car trajectory data under complex urban environment. In order to resolve difficulties introduced by low sampling rate trajectory data, we concentrate sample points around intersections by utilizing the turning characteristics from the large-scale trajectory data to ensure the accuracy of the detection of intersections and road segments. In front of complex road networks including many complex intersections, such as the overpasses and underpasses, we first layer intersections into major and minor one, and then propose a simplified representation of intersections and corresponding computable model based on the features of roads, which can significantly improve the accuracy of detected road networks, especially for the complex intersections. In order to construct fine-grained road networks, we distinguish various types of intersections using direction information and detected turning limit. To the best of our knowledge, our road network building method is the first time to give fine-grained road networks based on low-sampling rate private car trajectory data, especially able to infer the location of complex intersections and its connections to other intersections. Last but not the least, we propose an effective parameter selection process for the Density-Based Spatial Clustering of Applications with Noise based clustering algorithm, which is used to implement the reliable intersection detection. Extensive evaluations are conducted based on a real-world trajectory dataset from 1,345 private cars in Futian district, Shenzhen city of China. The results demonstrate the effectiveness of the proposed method. The constructed road network matches close to the one from a public editing map OpenStreetMap, especially the location of the road intersections and road segments, which achieves 92.2% intersections within 20m and 91.6% road segments within 8m.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2950268711",
    "type": "article"
  },
  {
    "title": "Interactive Recommendation with User-Specific Deep Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3359554",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Yu Lei; Wenjie Li",
    "corresponding_authors": "",
    "abstract": "In this article, we study a multi-step interactive recommendation problem for explicit-feedback recommender systems. Different from the existing works, we propose a novel user-specific deep reinforcement learning approach to the problem. Specifically, we first formulate the problem of interactive recommendation for each target user as a Markov decision process (MDP). We then derive a multi-MDP reinforcement learning task for all involved users. To model the possible relationships (including similarities and differences) between different users’ MDPs, we construct user-specific latent states by using matrix factorization. After that, we propose a user-specific deep Q-learning (UDQN) method to estimate optimal policies based on the constructed user-specific latent states. Furthermore, we propose Biased UDQN (BUDQN) to explicitly model user-specific information by employing an additional bias parameter when estimating the Q-values for different users. Finally, we validate the effectiveness of our approach by comprehensive experimental results and analysis.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2981211936",
    "type": "article"
  },
  {
    "title": "Collaborative Filtering with Topic and Social Latent Factors Incorporating Implicit Feedback",
    "doi": "https://doi.org/10.1145/3127873",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Guangneng Hu; Xinyu Dai; Fengyu Qiu; Rui Xia; Tao Li; Shujian Huang; Jiajun Chen",
    "corresponding_authors": "",
    "abstract": "Recommender systems (RSs) provide an effective way of alleviating the information overload problem by selecting personalized items for different users. Latent factors-based collaborative filtering (CF) has become the popular approaches for RSs due to its accuracy and scalability. Recently, online social networks and user-generated content provide diverse sources for recommendation beyond ratings. Although social matrix factorization (Social MF) and topic matrix factorization (Topic MF) successfully exploit social relations and item reviews, respectively; both of them ignore some useful information. In this article, we investigate the effective data fusion by combining the aforementioned approaches. First, we propose a novel model MR3 to jointly model three sources of information (i.e., ratings, item reviews, and social relations) effectively for rating prediction by aligning the latent factors and hidden topics. Second, we incorporate the implicit feedback from ratings into the proposed model to enhance its capability and to demonstrate its flexibility. We achieve more accurate rating prediction on real-life datasets over various state-of-the-art methods. Furthermore, we measure the contribution from each of the three data sources and the impact of implicit feedback from ratings, followed by the sensitivity analysis of hyperparameters. Empirical studies demonstrate the effectiveness and efficacy of our proposed model and its extension.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W3123065220",
    "type": "article"
  },
  {
    "title": "Moving Destination Prediction Using Sparse Dataset",
    "doi": "https://doi.org/10.1145/3051128",
    "publication_date": "2017-04-14",
    "publication_year": 2017,
    "authors": "Liang Wang; Zhiwen Yu; Bin Guo; Tao Ku; Fei Yi",
    "corresponding_authors": "",
    "abstract": "Moving destination prediction offers an important category of location-based applications and provides essential intelligence to business and governments. In existing studies, a common approach to destination prediction is to match the given query trajectory with massive recorded trajectories by similarity calculation. Unfortunately, due to privacy concerns, budget constraints, and many other factors, in most circumstances, we can only obtain a sparse trajectory dataset. In sparse dataset, the available moving trajectories are far from enough to cover all possible query trajectories; thus the predictability of the matching-based approach will decrease remarkably. Toward destination prediction with sparse dataset, instead of searching similar trajectories over the sparse records, we alternatively examine the changes of distances from sampling locations to final destination on query trajectory. The underlying idea is intuitive: It is directly motivated by travel purpose, people always get closer to the final destination during the movement. By borrowing the conception of gradient descent in optimization theory, we propose a novel moving destination prediction approach, namely MGDPre. Building upon the mobility gradient descent, MGDPre only investigates the behavior characteristics of query trajectory itself without matching historical trajectories, and thus is applicable for sparse dataset. We evaluate our approach based on extensive experiments, using GPS trajectories generated by a sample of taxis over a 10-day period in Shenzhen city, China. The results demonstrate that the effectiveness, efficiency, and scalability of our approach outperform state-of-the-art baseline methods.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2606476371",
    "type": "article"
  },
  {
    "title": "Algorithms for Online Influencer Marketing",
    "doi": "https://doi.org/10.1145/3274670",
    "publication_date": "2018-12-19",
    "publication_year": 2018,
    "authors": "Paul Lagrée; Olivier Cappé; Bogdan Cautis; Silviu Maniu",
    "corresponding_authors": "",
    "abstract": "Influence maximization is the problem of finding influential users, or nodes, in a graph so as to maximize the spread of information. It has many applications in advertising and marketing on social networks. In this article, we study a highly generic version of influence maximization, one of optimizing influence campaigns by sequentially selecting “spread seeds” from a set of influencers , a small subset of the node population, under the hypothesis that, in a given campaign, previously activated nodes remain persistently active. This problem is in particular relevant for an important form of online marketing, known as influencer marketing , in which the marketers target a sub-population of influential people, instead of the entire base of potential buyers. Importantly, we make no assumptions on the underlying diffusion model, and we work in a setting where neither a diffusion network nor historical activation data are available. We call this problem online influencer marketing with persistence (in short, OIMP). We first discuss motivating scenarios and present our general approach. We introduce an estimator on the influencers’ remaining potential – the expected number of nodes that can still be reached from a given influencer – and justify its strength to rapidly estimate the desired value, relying on real data gathered from Twitter. We then describe a novel algorithm, GT-UCB, relying on probabilistic upper confidence bounds on the remaining potential. We show that our approach leads to high-quality spreads on both simulated and real datasets. Importantly, it is orders of magnitude faster than state-of-the-art influence maximization methods, making it possible to deal with large-scale online scenarios.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2765477361",
    "type": "article"
  },
  {
    "title": "Will Triadic Closure Strengthen Ties in Social Networks?",
    "doi": "https://doi.org/10.1145/3154399",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Hong Huang; Yuxiao Dong; Jie Tang; Hongxia Yang; Nitesh V. Chawla; Xiaoming Fu",
    "corresponding_authors": "",
    "abstract": "The social triad—a group of three people—is one of the simplest and most fundamental social groups. Extensive network and social theories have been developed to understand its structure, such as triadic closure and social balance. Over the course of a triadic closure—the transition from two ties to three among three users, the strength dynamics of its social ties, however, are much less well understood. Using two dynamic networks from social media and mobile communication, we examine how the formation of the third tie in a triad affects the strength of the existing two ties. Surprisingly, we find that in about 80% social triads, the strength of the first two ties is weakened although averagely the tie strength in the two networks maintains an increasing or stable trend. We discover that (1) the decrease in tie strength among three males is more sharply than that among females, and (2) the tie strength between celebrities is more likely to be weakened as the closure of a triad than those between ordinary people. Furthermore, we formalize a triadic tie strength dynamics prediction problem to infer whether social ties of a triad will become weakened after its closure. We propose a TRIST method—a kernel density estimation (KDE)-based graphical model—to solve the problem by incorporating user demographics, temporal effects, and structural information. Extensive experiments demonstrate that TRIST offers a greater than 82% potential predictability for inferring triadic tie strength dynamics in both networks. The leveraging of the KDE and structural correlations enables TRIST to outperform baselines by up to 30% in terms of F1-score.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2785221547",
    "type": "article"
  },
  {
    "title": "Put Three and Three Together",
    "doi": "https://doi.org/10.1145/2775108",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "Arnau Prat-Pèrez; David Domínguez-Sal; Josep-M. Brunat; Josep-L. Larriba-Pey",
    "corresponding_authors": "",
    "abstract": "Community detection has arisen as one of the most relevant topics in the field of graph data mining due to its applications in many fields such as biology, social networks, or network traffic analysis. Although the existing metrics used to quantify the quality of a community work well in general, under some circumstances, they fail at correctly capturing such notion. The main reason is that these metrics consider the internal community edges as a set, but ignore how these actually connect the vertices of the community. We propose the Weighted Community Clustering ( WCC ), which is a new community metric that takes the triangle instead of the edge as the minimal structural motif indicating the presence of a strong relation in a graph. We theoretically analyse WCC in depth and formally prove, by means of a set of properties, that the maximization of WCC guarantees communities with cohesion and structure. In addition, we propose Scalable Community Detection (SCD) , a community detection algorithm based on WCC , which is designed to be fast and scalable on SMP machines, showing experimentally that WCC correctly captures the concept of community in social networks using real datasets. Finally, using ground-truth data, we show that SCD provides better quality than the best disjoint community detection algorithms of the state of the art while performing faster.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2338082078",
    "type": "article"
  },
  {
    "title": "Twitter Geolocation",
    "doi": "https://doi.org/10.1145/3178112",
    "publication_date": "2018-03-23",
    "publication_year": 2018,
    "authors": "Jordan Bakerman; Karl Pazdernik; Alyson G. Wilson; Geoffrey Fairchild; Rian Bahran",
    "corresponding_authors": "",
    "abstract": "Geotagging Twitter messages is an important tool for event detection and enrichment. Despite the availability of both social media content and user network information, these two features are generally utilized separately in the methodology. In this article, we create a hybrid method that uses Twitter content and network information jointly as model features. We use Gaussian mixture models to map the raw spatial distribution of the model features to a predicted field. This approach is scalable to large datasets and provides a natural representation of model confidence. Our method is tested against other approaches and we achieve greater prediction accuracy. The model also improves both precision and coverage.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2793483901",
    "type": "article"
  },
  {
    "title": "Robust Drift Characterization from Event Streams of Business Processes",
    "doi": "https://doi.org/10.1145/3375398",
    "publication_date": "2020-03-13",
    "publication_year": 2020,
    "authors": "Alireza Ostovar; Sander J. J. Leemans; Marcello La Rosa",
    "corresponding_authors": "",
    "abstract": "Process workers may vary the normal execution of a business process to adjust to changes in their operational environment, e.g., changes in workload, season, or regulations. Changes may be simple, such as skipping an individual activity, or complex, such as replacing an entire procedure with another. Over time, these changes may negatively affect process performance; hence, it is important to identify and understand them early on. As such, a number of techniques have been developed to detect process drifts , i.e., statistically significant changes in process behavior, from process event logs (offline) or event streams (online). However, detecting a drift without characterizing it, i.e., without providing explanations on its nature, is not enough to help analysts understand and rectify root causes for process performance issues. Existing approaches for drift characterization are limited to simple changes that affect individual activities. This article contributes an efficient, accurate, and noise-tolerant automated method for characterizing complex drifts affecting entire process fragments. The method, which works both offline and online, relies on two cornerstone techniques, one to automatically discover process trees from event streams (logs) and the other to transform process trees using a minimum number of change operations. The operations identified are then translated into natural language statements to explain the change behind a drift. The method has been extensively evaluated on artificial and real-life datasets, and against a state-of-the-art baseline method. The results from one of the real-life datasets have also been validated with a process stakeholder.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2901934595",
    "type": "article"
  },
  {
    "title": "A Deep Multi-task Contextual Attention Framework for Multi-modal Affect Analysis",
    "doi": "https://doi.org/10.1145/3380744",
    "publication_date": "2020-05-13",
    "publication_year": 2020,
    "authors": "Md Shad Akhtar; Dushyant Singh Chauhan; Asif Ekbal",
    "corresponding_authors": "",
    "abstract": "Multi-modal affect analysis (e.g., sentiment and emotion analysis) is an interdisciplinary study and has been an emerging and prominent field in Natural Language Processing and Computer Vision. The effective fusion of multiple modalities (e.g., text , acoustic, or visual frames ) is a non-trivial task, as these modalities, often, carry distinct and diverse information, and do not contribute equally. The issue further escalates when these data contain noise. In this article, we study the concept of multi-task learning for multi-modal affect analysis and explore a contextual inter-modal attention framework that aims to leverage the association among the neighboring utterances and their multi-modal information. In general, sentiments and emotions have inter-dependence on each other (e.g., anger → negative or happy → positive ). In our current work, we exploit the relatedness among the participating tasks in the multi-task framework. We define three different multi-task setups, each having two tasks, i.e., sentiment 8 emotion classification, sentiment classification 8 sentiment intensity prediction, and emotion classificati on 8 emotion intensity prediction. Our evaluation of the proposed system on the CMU-Multi-modal Opinion Sentiment and Emotion Intensity benchmark dataset suggests that, in comparison with the single-task learning framework, our multi-task framework yields better performance for the inter-related participating tasks. Further, comparative studies show that our proposed approach attains state-of-the-art performance for most of the cases.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W3028041000",
    "type": "article"
  },
  {
    "title": "Self-weighted Multi-view Fuzzy Clustering",
    "doi": "https://doi.org/10.1145/3396238",
    "publication_date": "2020-06-22",
    "publication_year": 2020,
    "authors": "Xiaofeng Zhu; Shichao Zhang; Yonghua Zhu; Wei Zheng; Yang Yang",
    "corresponding_authors": "",
    "abstract": "Since the data in each view may contain distinct information different from other views as well as has common information for all views in multi-view learning, many multi-view clustering methods have been designed to use these information (including the distinct information for each view and the common information for all views) to improve the clustering performance. However, previous multi-view clustering methods cannot effectively detect these information so that difficultly outputting reliable clustering models. In this article, we propose a fuzzy, sparse, and robust multi-view clustering method to consider all kinds of relations among the data (such as view importance, view stability, and view diversity), which can effectively extract both distinct information and common information as well as balance these two kinds of information. Moreover, we devise an alternating optimization algorithm to solve the resulting objective function as well as prove that our proposed algorithm achieves fast convergence. It is noteworthy that existing multi-view clustering methods only consider a part of the relations, and thus are a special case of our proposed framework. Experimental results on synthetic datasets and real datasets show that our proposed method outperforms the state-of-the-art clustering methods in terms of evaluation metrics of clustering such as clustering accuracy, normalized mutual information, purity, and adjusted rand index.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W3036015371",
    "type": "article"
  },
  {
    "title": "Edge2vec",
    "doi": "https://doi.org/10.1145/3391298",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Changping Wang; Chaokun Wang; Zheng Wang; Xiaojun Ye; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Graph embedding, also known as network embedding and network representation learning, is a useful technique which helps researchers analyze information networks through embedding a network into a low-dimensional space. However, existing graph embedding methods are all node-based, which means they can just directly map the nodes of a network to low-dimensional vectors while the edges could only be mapped to vectors indirectly. One important reason is the computational cost, because the number of edges is always far greater than the number of nodes. In this article, considering an important property of social networks, i.e., the network is sparse, and hence the average degree of nodes is bounded, we propose an edge-based graph embedding ( edge2vec ) method to map the edges in social networks directly to low-dimensional vectors. Edge2vec takes both the local and the global structure information of edges into consideration to preserve structure information of embedded edges as much as possible. To achieve this goal, edge2vec first ingeniously combines the deep autoencoder and Skip-gram model through a well-designed deep neural network. The experimental results on different datasets show edge2vec benefits from the direct mapping in preserving the structure information of edges.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3033917965",
    "type": "article"
  },
  {
    "title": "NTP-Miner: Nonoverlapping Three-Way Sequential Pattern Mining",
    "doi": "https://doi.org/10.1145/3480245",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Youxi Wu; Lanfang Luo; Yan Li; Lei Guo; Philippe Fournier‐Viger; Xingquan Zhu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Nonoverlapping sequential pattern mining is an important type of sequential pattern mining (SPM) with gap constraints, which not only can reveal interesting patterns to users but also can effectively reduce the search space using the Apriori (anti-monotonicity) property. However, the existing algorithms do not focus on attributes of interest to users, meaning that existing methods may discover many frequent patterns that are redundant. To solve this problem, this article proposes a task called nonoverlapping three-way sequential pattern (NTP) mining, where attributes are categorized according to three levels of interest: strong, medium, and weak interest. NTP mining can effectively avoid mining redundant patterns since the NTPs are composed of strong and medium interest items. Moreover, NTPs can avoid serious deviations (the occurrence is significantly different from its pattern) since gap constraints cannot match with strong interest patterns. To mine NTPs, an effective algorithm is put forward, called NTP-Miner, which applies two main steps: support (frequency occurrence) calculation and candidate pattern generation. To calculate the support of an NTP, depth-first and backtracking strategies are adopted, which do not require creating a whole Nettree structure, meaning that many redundant nodes and parent–child relationships do not need to be created. Hence, time and space efficiency is improved. To generate candidate patterns while reducing their number, NTP-Miner employs a pattern join strategy and only mines patterns of strong and medium interest. Experimental results on stock market and protein datasets show that NTP-Miner not only is more efficient than other competitive approaches but can also help users find more valuable patterns. More importantly, NTP mining has achieved better performance than other competitive methods in clustering tasks. Algorithms and data are available at: https://github.com/wuc567/Pattern-Mining/tree/master/NTP-Miner .",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3210621838",
    "type": "article"
  },
  {
    "title": "Graph Neural Networks for Fast Node Ranking Approximation",
    "doi": "https://doi.org/10.1145/3446217",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Sunil Kumar Maurya; Xin Liu; Tsuyoshi Murata",
    "corresponding_authors": "",
    "abstract": "Graphs arise naturally in numerous situations, including social graphs, transportation graphs, web graphs, protein graphs, etc. One of the important problems in these settings is to identify which nodes are important in the graph and how they affect the graph structure as a whole. Betweenness centrality and closeness centrality are two commonly used node ranking measures to find out influential nodes in the graphs in terms of information spread and connectivity. Both of these are considered as shortest path based measures as the calculations require the assumption that the information flows between the nodes via the shortest paths. However, exact calculations of these centrality measures are computationally expensive and prohibitive, especially for large graphs. Although researchers have proposed approximation methods, they are either less efficient or suboptimal or both. We propose the first graph neural network (GNN) based model to approximate betweenness and closeness centrality. In GNN, each node aggregates features of the nodes in multihop neighborhood. We use this feature aggregation scheme to model paths and learn how many nodes are reachable to a specific node. We demonstrate that our approach significantly outperforms current techniques while taking less amount of time through extensive experiments on a series of synthetic and real-world datasets. A benefit of our approach is that the model is inductive, which means it can be trained on one set of graphs and evaluated on another set of graphs with varying structures. Thus, the model is useful for both static graphs and dynamic graphs. Source code is available at https://github.com/sunilkmaurya/GNN_Ranking",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W3161944108",
    "type": "article"
  },
  {
    "title": "High-Order Structure Exploration on Massive Graphs",
    "doi": "https://doi.org/10.1145/3425637",
    "publication_date": "2021-01-09",
    "publication_year": 2021,
    "authors": "Dawei Zhou; Si Zhang; Mehmet Yigit Yildirim; Scott Alcorn; Hanghang Tong; Hasan Davulcu; Jingrui He",
    "corresponding_authors": "",
    "abstract": "Modeling and exploring high-order connectivity patterns, also called network motifs, are essential for understanding the fundamental structures that control and mediate the behavior of many complex systems. For example, in social networks, triangles have been proven to play the fundamental role in understanding social network communities; in online transaction networks, detecting directed looped transactions helps identify money laundering activities; in personally identifiable information networks, the star-shaped structures may correspond to a set of synthetic identities. Despite the ubiquity of such high-order structures, many existing graph clustering methods are either not designed for the high-order connectivity patterns, or suffer from the prohibitive computational cost when modeling high-order structures in the large-scale networks. This article generalizes the challenges in multiple dimensions. First ( Model ), we introduce the notion of high-order conductance, and define the high-order diffusion core, which is based on a high-order random walk induced by the user-specified high-order network structure. Second ( Algorithm ), we propose a novel high-order structure-preserving graph clustering framework named HOSGRAP , which partitions the graph into structure-rich clusters in polylogarithmic time with respect to the number of edges in the graph. Third ( Generalization ), we generalize our proposed algorithm to solve the real-world problems on various types of graphs, such as signed graphs, bipartite graphs, and multi-partite graphs. Experimental results on both synthetic and real graphs demonstrate the effectiveness and efficiency of the proposed algorithms.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3119582991",
    "type": "article"
  },
  {
    "title": "User Embedding for Expert Finding in Community Question Answering",
    "doi": "https://doi.org/10.1145/3441302",
    "publication_date": "2021-03-26",
    "publication_year": 2021,
    "authors": "Negin Ghasemi; Ramin Fatourechi; Saeedeh Momtazi",
    "corresponding_authors": "",
    "abstract": "The number of users who have the appropriate knowledge to answer asked questions in community question answering is lower than those who ask questions. Therefore, finding expert users who can answer the questions is very crucial and useful. In this article, we propose a framework to find experts for given questions and assign them the related questions. The proposed model benefits from users’ relations in a community along with the lexical and semantic similarities between new question and existing answers. Node embedding is applied to the community graph to find similar users. Our experiments on four different Stack Exchange datasets show that adding community relations improves the performance of expert finding models.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W3150452003",
    "type": "article"
  },
  {
    "title": "Learning Graph Neural Networks with Positive and Unlabeled Nodes",
    "doi": "https://doi.org/10.1145/3450316",
    "publication_date": "2021-06-28",
    "publication_year": 2021,
    "authors": "Man Wu; Shirui Pan; Lan Du; Xingquan Zhu",
    "corresponding_authors": "",
    "abstract": "Graph neural networks (GNNs) are important tools for transductive learning tasks, such as node classification in graphs, due to their expressive power in capturing complex interdependency between nodes. To enable GNN learning, existing works typically assume that labeled nodes, from two or multiple classes, are provided, so that a discriminative classifier can be learned from the labeled data. In reality, this assumption might be too restrictive for applications, as users may only provide labels of interest in a single class for a small number of nodes. In addition, most GNN models only aggregate information from short distances ( e.g. , 1-hop neighbors) in each round, and fail to capture long-distance relationship in graphs. In this article, we propose a novel GNN framework, long-short distance aggregation networks, to overcome these limitations. By generating multiple graphs at different distance levels, based on the adjacency matrix, we develop a long-short distance attention model to model these graphs. The direct neighbors are captured via a short-distance attention mechanism, and neighbors with long distance are captured by a long-distance attention mechanism. Two novel risk estimators are further employed to aggregate long-short-distance networks, for PU learning and the loss is back-propagated for model learning. Experimental results on real-world datasets demonstrate the effectiveness of our algorithm.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W3176943960",
    "type": "article"
  },
  {
    "title": "Graph-Based Stock Recommendation by Time-Aware Relational Attention Network",
    "doi": "https://doi.org/10.1145/3451397",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Jianliang Gao; Xiaoting Ying; Cong Xu; Jianxin Wang; Shichao Zhang; Zhao Li",
    "corresponding_authors": "",
    "abstract": "The stock market investors aim at maximizing their investment returns. Stock recommendation task is to recommend stocks with higher return ratios for the investors. Most stock prediction methods study the historical sequence patterns to predict stock trend or price in the near future. In fact, the future price of a stock is correlated not only with its historical price, but also with other stocks. In this article, we take into account the relationships between stocks (corporations) by stock relation graph. Furthermore, we propose a Time-aware Relational Attention Network (TRAN) for graph-based stock recommendation according to return ratio ranking. In TRAN, the time-aware relational attention mechanism is designed to capture time-varying correlation strengths between stocks by the interaction of historical sequences and stock description documents. With the dynamic strengths, the nodes of the stock relation graph aggregate the features of neighbor stock nodes by graph convolution operation. For a given group of stocks, the proposed TRAN model can output the ranking results of stocks according to their return ratios. The experimental results on several real-world datasets demonstrate the effectiveness of our TRAN for stock recommendation.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W3184572086",
    "type": "article"
  },
  {
    "title": "Improved Customer Lifetime Value Prediction With Sequence-To-Sequence Learning and Feature-Based Models",
    "doi": "https://doi.org/10.1145/3441444",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Josef G. Bauer; Dietmar Jannach",
    "corresponding_authors": "",
    "abstract": "The prediction of the Customer Lifetime Value (CLV) is an important asset for tool-supported marketing by customer relationship managers. Since standard methods based on purchase recency, frequency, and past profit and revenue statistics often have limited predictive power, advanced machine learning (ML) techniques were applied to this task in recent years. However, existing approaches are often not fully capable of modeling certain temporal patterns that can be commonly found in practice, such as periodic purchasing behavior of customers. To address these shortcomings, we propose a novel method for CLV prediction based on a combination of several ML techniques. At its core, our method consists of a tailored deep learning approach based on encoder–decoder sequence-to-sequence recurrent neural networks with augmented temporal convolutions. This model is then combined with gradient boosting machines (GBMs) and a set of novel features in a hybrid framework. Empirical evaluations based on real-world data from a larger e-commerce company and a public dataset from the domain of online retail show that already the sequence-based model leads to competitive performance results. Stacking it with the GBM model is synergistic and further improves accuracy, indicating that the two models capture different patterns in the data.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3162650135",
    "type": "article"
  },
  {
    "title": "MBN: Towards Multi-Behavior Sequence Modeling for Next Basket Recommendation",
    "doi": "https://doi.org/10.1145/3497748",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Yanyan Shen; Baoyuan Ou; Ranzhen Li",
    "corresponding_authors": "",
    "abstract": "Next basket recommendation aims at predicting the next set of items that a user would likely purchase together, which plays an important role in e-commerce platforms. Unlike conventional item recommendation, the next basket recommendation focuses on capturing item correlations among baskets and learning the user’s temporal interest from the past purchasing basket sequence. In practice, most users interact with items in various kinds of behaviors. The multi-behavior data sheds light on user’s potential purchasing intention and resolves noisy signals from accidentally purchased items. In this article, we conduct an empirical study on real datasets to exploit the characteristics of multi-behavior data and confirm its positive effects on next basket recommendation. We develop a novel Multi-Behavior Network (MBN) model that captures item correlations and acquires meta-knowledge from multi-behavior basket sequences effectively. MBN employs the meta multi-behavior sequence encoder to model temporal dependencies of each individual behavior and extract meta-knowledge across different behaviors. Furthermore, we design the recurring-item-aware predictor in MBN to realize the high degree of the repeated occurrences of items, leading to better recommendation performance. We conduct extensive experiments to evaluate the performance of our proposed MBN model using real-world multi-behavior data. The results demonstrate the superior recommendation performance of MBN compared with various state-of-the-art methods.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4220729518",
    "type": "article"
  },
  {
    "title": "Hypergraph Transformer Neural Networks",
    "doi": "https://doi.org/10.1145/3565028",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Mengran Li; Yong Zhang; Xiaoyong Li; Yuchen Zhang; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Graph neural networks (GNNs) have been widely used for graph structure learning and achieved excellent performance in tasks such as node classification and link prediction. Real-world graph networks imply complex and various semantic information and are often referred to as heterogeneous information networks (HINs). Previous GNNs have laboriously modeled heterogeneous graph networks with pairwise relations, in which the semantic information representation for learning is incomplete and severely hinders node embedded learning. Therefore, the conventional graph structure cannot satisfy the demand for information discovery in HINs. In this article, we propose an end-to-end hypergraph transformer neural network (HGTN) that exploits the communication abilities between different types of nodes and hyperedges to learn higher-order relations and discover semantic information. Specifically, attention mechanisms weigh the importance of semantic information hidden in original HINs to generate useful meta-paths. Meanwhile, our method develops a multi-scale attention module to aggregate node embeddings in higher-order neighborhoods. We evaluate the proposed model with node classification tasks on six datasets: DBLP, ACM, IBDM, Reuters, STUD-BJUT, and Citeseer. Experiments on a large number of benchmarks show the advantages of HGTN.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4297379527",
    "type": "article"
  },
  {
    "title": "Auto IV: Counterfactual Prediction via Automatic Instrumental Variable Decomposition",
    "doi": "https://doi.org/10.1145/3494568",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Junkun Yuan; Anpeng Wu; Kun Kuang; Bo Li; Runze Wu; Fei Wu; Lanfen Lin",
    "corresponding_authors": "",
    "abstract": "Instrumental variables (IVs), sources of treatment randomization that are conditionally independent of the outcome, play an important role in causal inference with unobserved confounders. However, the existing IV-based counterfactual prediction methods need well-predefined IVs, while it is an art rather than science to find valid IVs in many real-world scenes. Moreover, the predefined hand-made IVs could be weak or erroneous by violating the conditions of valid IVs. These thorny facts hinder the application of the IV-based counterfactual prediction methods. In this paper, we propose a novel Automatic Instrumental Variable decomposition (AutoIV) algorithm to automatically generate representations serving the role of IVs from observed variables (IV candidates). Specifically, we let the learned IV representations satisfy the relevance condition with the treatment and exclusion condition with the outcome via mutual information maximization and minimization constraints, respectively. We also learn confounder representations by encouraging them to be relevant to both the treatment and the outcome. The IV and confounder representations compete for the information with their constraints in an adversarial game, which allows us to get valid IV representations for IV-based counterfactual prediction. Extensive experiments demonstrate that our method generates valid IV representations for accurate IV-based counterfactual prediction.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3187851437",
    "type": "article"
  },
  {
    "title": "Self-paced Adaptive Bipartite Graph Learning for Consensus Clustering",
    "doi": "https://doi.org/10.1145/3564701",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Peng Zhou; Xinwang Liu; Liang Du; Xuejun Li",
    "corresponding_authors": "",
    "abstract": "Consensus clustering provides an elegant framework to aggregate multiple weak clustering results to learn a consensus one that is more robust and stable than a single result. However, most of the existing methods usually use all data for consensus learning, whereas ignoring the side effects caused by some unreliable or difficult data. To address this issue, in this article, we propose a novel self-paced consensus clustering method with adaptive bipartite graph learning to gradually involve data from more reliable to less reliable ones in consensus learning. At first, we construct an initial bipartite graph from the base results, where the nodes represent the clusters and instances, and the edges indicate that an instance belongs to a cluster. Then, we adaptively learn a structured bipartite graph from this initial one by self-paced learning, i.e., we automatically determine the reliability of each edge with adaptive cluster similarity measuring and involve the edges in bipartite graph learning in order of their reliability. At last, we obtain the final consensus result from the learned structured bipartite graph. We conduct extensive experiments on both toy and benchmark datasets, and the results show the effectiveness and superiority of our method. The codes of this article are released in http://Doctor-Nobody.github.io/codes/code_SCCABG.zip.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4297172927",
    "type": "article"
  },
  {
    "title": "Trip Reinforcement Recommendation with Graph-based Representation Learning",
    "doi": "https://doi.org/10.1145/3564609",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Lei Chen; Jie Cao; Haicheng Tao; Jia Wu",
    "corresponding_authors": "",
    "abstract": "Tourism is an important industry and a popular leisure activity involving billions of tourists per annum. One challenging problem tourists face is identifying attractive Places-of-Interest (POIs) and planning the personalized trip with time constraints. Most of the existing trip recommendation methods mainly consider POI popularity and user preferences, and focus on the last visited POI when choosing the next POI. However, the visit patterns and their asymmetry property have not been fully exploited. To this end, in this article, we present a GRM-RTrip (short for G raph-based R epresentation M ethod for R einforce Trip Recommendation) framework. GRM-RTrip learns POI representations from incoming and outgoing views to obtain asymmetric POI-POI transition probability via POI-POI graph networks, and then fuses the trained POI representation into a user-POI graph network to estimate user preferences. Finally, after formulating the personalized trip recommendation as a Markov Decision Process (MDP), we utilize a reinforcement learning algorithm for generating a personalized trip with maximal user travel experience. Extensive experiments are performed on the public datasets and the results demonstrate the superiority of GRM-RTrip compared with the state-of-the-art trip recommendation methods.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4297377240",
    "type": "article"
  },
  {
    "title": "ONP-Miner: One-off Negative Sequential Pattern Mining",
    "doi": "https://doi.org/10.1145/3549940",
    "publication_date": "2022-08-04",
    "publication_year": 2022,
    "authors": "Youxi Wu; Mingjie Chen; Yan Li; Jing Liu; Zhao Li; Jinyan Li; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Negative sequential pattern mining (SPM) is an important SPM research topic. Unlike positive SPM, negative SPM can discover events that should have occurred but have not occurred, and it can be used for financial risk management and fraud detection. However, existing methods generally ignore the repetitions of the pattern and do not consider gap constraints, which can lead to mining results containing a large number of patterns that users are not interested in. To solve this problem, this article discovers frequent one-off negative sequential patterns (ONPs). This problem has the following two characteristics. First, the support is calculated under the one-off condition, which means that any character in the sequence can only be used once at most. Second, the gap constraint can be given by the user. To efficiently mine patterns, this article proposes the ONP-Miner algorithm, which employs depth-first and backtracking strategies to calculate the support. Therefore, ONP-Miner can effectively avoid creating redundant nodes and parent-child relationships. Moreover, to effectively reduce the number of candidate patterns, ONP-Miner uses pattern join and pruning strategies to generate and further prune the candidate patterns, respectively. Experimental results show that ONP-Miner not only improves the mining efficiency but also has better mining performance than the state-of-the-art algorithms. More importantly, ONP mining can find more interesting patterns in traffic volume data to predict future traffic.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4289766427",
    "type": "article"
  },
  {
    "title": "Graph Domain Adaptation: A Generative View",
    "doi": "https://doi.org/10.1145/3631712",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Ruichu Cai; Fengzhu Wu; Zijian Li; Pengfei Wei; Lingling Yi; Kun Zhang",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed tremendous interest in deep learning on graph-structured data. Due to the high cost of collecting labeled graph-structured data, domain adaptation is important to supervised graph learning tasks with limited samples. However, current graph domain adaptation methods are generally adopted from traditional domain adaptation tasks, and the properties of graph-structured data are not well utilized. For example, the observed social networks on different platforms are controlled not only by the different crowd or communities but also by the domain-specific policies and the background noise. Based on these properties in graph-structured data, we first assume that the graph-structured data generation process is controlled by three independent types of latent variables, i.e., the semantic latent variables, the domain latent variables, and the random latent variables. Based on this assumption, we propose a disentanglement-based unsupervised domain adaptation method for the graph-structured data, which applies variational graph auto-encoders to recover these latent variables and disentangles them via three supervised learning modules. Extensive experimental results on two real-world datasets in the graph classification task reveal that our method not only significantly outperforms the traditional domain adaptation methods and the disentangled-based domain adaptation methods but also outperforms the state-of-the-art graph domain adaptation algorithms.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3168650020",
    "type": "article"
  },
  {
    "title": "Fairness in Recommender Systems: Evaluation Approaches and Assurance Strategies",
    "doi": "https://doi.org/10.1145/3604558",
    "publication_date": "2023-06-12",
    "publication_year": 2023,
    "authors": "Yao Wu; Jian Cao; Guandong Xu",
    "corresponding_authors": "",
    "abstract": "With the wide application of recommender systems, the potential impacts of recommender systems on customers, item providers and other parties have attracted increasing attention. Fairness, which is the quality of treating people equally, is also becoming important in recommender system evaluation and algorithm design. Therefore, in the past years, there has been a growing interest in fairness measurement and assurance in recommender systems. Although there are several reviews on related topics, such as fairness in machine learning and debias in recommender systems, they do not present a systematic view on fairness in recommender systems, which is context aware and has a multi-sided meaning. Therefore, in this review, the concept of fairness is discussed in detail in the various contexts of recommender systems. Specifically, a comprehensive framework to classify fairness metrics is proposed from four dimensions, i.e., Fairness for Whom , Demographic Unit , Time Frame , and Quantification Method . Then the strategies for eliminating unfairness in recommendations, fairness in different recommendation tasks and datasets are reviewed and summarized. Finally, the challenges and future work are discussed.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4380323660",
    "type": "article"
  },
  {
    "title": "Causal Disentanglement for Implicit Recommendations with Network Information",
    "doi": "https://doi.org/10.1145/3582435",
    "publication_date": "2023-02-01",
    "publication_year": 2023,
    "authors": "Paras Sheth; Ruocheng Guo; Lu Cheng; Huan Liu; K. Selçuk Candan",
    "corresponding_authors": "",
    "abstract": "Online user engagement is highly influenced by various machine learning models, such as recommender systems. These systems recommend new items to the user based on the user’s historical interactions. Implicit recommender systems reflect a binary setting showing whether a user interacted (e.g., clicked on) with an item or not. However, the observed clicks may be due to various causes such as user’s interest, item’s popularity, and social influence factors. Traditional recommender systems consider these causes under a unified representation, which may lead to the emergence and amplification of various biases in recommendations. However, recent work indicates that by disentangling the unified representations, one can mitigate bias (e.g., popularity bias) in recommender systems and help improve recommendation performance. Yet, prior work in causal disentanglement in recommendations does not consider a crucial factor, that is, social influence. Social theories such as homophily and social influence provide evidence that a user’s decision can be highly influenced by the user’s social relations. Thus, accounting for the social relations while disentangling leads to less biased recommendations. To this end, we identify three separate causes behind an effect (e.g., clicks): (a) user’s interest, (b) item’s popularity, and (c) user’s social influence. Our approach seeks to causally disentangle the user and item latent features to mitigate popularity bias in implicit feedback–based social recommender systems. To achieve this goal, we draw from causal inference theories and social network theories and propose a causality-aware disentanglement method that leverages both the user–item interaction network and auxiliary social network information. Experiments on real-world datasets against various state-of-the-art baselines validate the effectiveness of the proposed model for mitigating popularity bias and generating de-biased recommendations.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4318831962",
    "type": "article"
  },
  {
    "title": "Combining Diverse Meta-Features to Accurately Identify Recurring Concept Drift in Data Streams",
    "doi": "https://doi.org/10.1145/3587098",
    "publication_date": "2023-03-07",
    "publication_year": 2023,
    "authors": "Ben Halstead; Yun Sing Koh; Patricia Riddle; Mykola Pechenizkiy; Albert Bifet",
    "corresponding_authors": "",
    "abstract": "Learning from streaming data is challenging as the distribution of incoming data may change over time, a phenomenon known as concept drift. The predictive patterns, or experience learned under one distribution may become irrelevant as conditions change under concept drift, but may become relevant once again when conditions reoccur. Adaptive learning methods adapt a classifier to concept drift by identifying which distribution, or concept , is currently present in order to determine which experience is relevant. Identifying a concept requires some representation to be stored for comparison, with the quality of the representation being key to accurate identification. Existing concept representations are based on meta-features, efficient univariate summaries of a concept. However, no single meta-feature can fully represent a concept, leading to severe accuracy loss when existing representations cannot describe concept drift. To avoid these failure cases, we propose the first general framework for combining a diverse range of meta-features into a single representation. We solve two main challenges, first presenting a method of efficiently computing, storing, and querying an arbitrary set of meta-features as a single representation, showing that a combination of meta-features may successfully avoid failure cases seen with existing methods. Second, we present the first method for dynamically learning which meta-features distinguish concepts in any given dataset, significantly improving performance. Our proposed approach enables state-of-the-art feature selection methods, such as mutual information, to be applied to concept representation meta-features for the first time. We investigate tradeoffs between memory budget and classification performance, observing accuracy increases of up to 16% by dynamically weighting the contribution of each meta-feature.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4323361239",
    "type": "article"
  },
  {
    "title": "Adaptive Neighbor Graph Aggregated Graph Attention Network for Heterogeneous Graph Embedding",
    "doi": "https://doi.org/10.1145/3616377",
    "publication_date": "2023-08-19",
    "publication_year": 2023,
    "authors": "Kai-Biao Lin; Jin-Po Chen; Chen Ruicong; Fan Yang; Zhuorang Yang; Lin Min; Lu Ping",
    "corresponding_authors": "",
    "abstract": "Graph attention network can generate effective feature embedding by specifying different weights to different nodes. The key of the research on heterogeneous graph embedding is the way to combine its rich structural information with semantic relations to aggregate the neighborhood information. Most of the existing heterogeneous graph representation learning methods guide the selection of neighbors by defining various meta-paths on heterogeneous graphs. However, these models only consider the information contained in the nodes under different paths and ignore the potential semantic relationships of nodes in different neighbor graph structures, which leads to the underutilization of graph structure information. In this article, we propose a novel adaptive framework named Neighbor Graph Aggregated Graph Attention Network (NGGAN) to fully exploit graph topological details in heterogeneous graph, and aggregates their information to obtain an effective embedding. The key idea is to use different levels of sampling methods to define neighborhood, and use neighbor graphs to represent the complex structural interaction between nodes. In this way, the high-order relationship between nodes and the latent semantics of neighbor graphs can be fully explored. Afterward, a hierarchical attention mechanism is applied to adaptively learn the importance of different objects, including node information, path information, and neighbor graph information. Multiple downstream tasks are performed on four real-world heterogeneous graph datasets, and the experimental results demonstrate the effectiveness of NGGAN.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4386001640",
    "type": "article"
  },
  {
    "title": "Graph Time-series Modeling in Deep Learning: A Survey",
    "doi": "https://doi.org/10.1145/3638534",
    "publication_date": "2023-12-23",
    "publication_year": 2023,
    "authors": "Hongjie Chen; Hoda Eldardiry",
    "corresponding_authors": "",
    "abstract": "Time-series and graphs have been extensively studied for their ubiquitous existence in numerous domains. Both topics have been separately explored in the field of deep learning. For time-series modeling, recurrent neural networks or convolutional neural networks model the relations between values across timesteps, while for graph modeling, graph neural networks model the inter-relations between nodes. Recent research in deep learning requires simultaneous modeling for time-series and graphs when both representations are present. For example, both types of modeling are necessary for time-series classification, regression, and anomaly detection in graphs. This article aims to provide a comprehensive summary of these models, which we call graph time-series models. To the best of our knowledge, this is the first survey article that provides a picture of related models from the perspective of deep graph time-series modeling to address a range of time-series tasks, including regression, classification, and anomaly detection. Graph time-series models are split into two categories: (a) graph recurrent/convolutional neural networks and (b) graph attention neural networks. Under each category, we further categorize models based on their properties. Additionally, we compare representative models and discuss how distinctive model characteristics are utilized with respect to various model components and data challenges. Pointers to commonly used datasets and code are included to facilitate access for further research. In the end, we discuss potential directions for future research.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4390143397",
    "type": "article"
  },
  {
    "title": "Prerequisite-Enhanced Category-Aware Graph Neural Networks for Course Recommendation",
    "doi": "https://doi.org/10.1145/3643644",
    "publication_date": "2024-01-29",
    "publication_year": 2024,
    "authors": "Jianshan Sun; Suyuan Mei; Kun Yuan; Yuanchun Jiang; Jie Cao",
    "corresponding_authors": "",
    "abstract": "The rapid development of Massive Open Online Courses (MOOCs) platforms has created an urgent need for an efficient personalized course recommender system that can assist learners of all backgrounds and levels of knowledge in selecting appropriate courses. Currently, most existing methods utilize a sequential recommendation paradigm that captures the user’s learning interests from their learning history, typically through recurrent or graph neural networks. However, fewer studies have explored how to incorporate principles of human learning at both the course and category levels to enhance course recommendations. In this article, we aim at addressing this gap by introducing a novel model, named Prerequisite-Enhanced Catory-Aware Graph Neural Network (PCGNN), for course recommendation. Specifically, we first construct a course prerequisite graph that reflects the human learning principles and further pre-train the course prerequisite relationships as the base embeddings for courses and categories. Then, to capture the user’s complex learning patterns, we build an item graph and a category graph from the user’s historical learning records, respectively: (1) the item graph reflects the course-level local learning transition patterns and (2) the category graph provides insight into the user’s long-term learning interest. Correspondingly, we propose a user interest encoder that employs a gated graph neural network to learn the course-level user interest embedding and design a category transition pattern encoder that utilizes GRU to yield the category-level user interest embedding. Finally, the two fine-grained user interest embeddings are fused to achieve precise course prediction. Extensive experiments on two real-world datasets demonstrate the effectiveness of PCGNN compared with other state-of-the-art methods.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4391315853",
    "type": "article"
  },
  {
    "title": "TOMGPT: Reliable Text-Only Training Approach for Cost-Effective Multi-modal Large Language Model",
    "doi": "https://doi.org/10.1145/3654674",
    "publication_date": "2024-03-28",
    "publication_year": 2024,
    "authors": "Yunkai Chen; Qimeng Wang; Shiwei Wu; Yan Gao; Tong Xu; Yao Hu",
    "corresponding_authors": "",
    "abstract": "Multi-modal large language models (MLLMs), such as GPT-4, exhibit great comprehension capabilities on human instruction, as well as zero-shot ability on new downstream multi-modal tasks. To integrate the different modalities within a unified embedding space, previous MLLMs attempted to conduct visual instruction tuning with massive and high-quality image-text pair data, which requires substantial costs in data collection and training resources. In this article, we propose TOMGPT (Text-Only training Multi-modal GPT), a cost-effective MLLM tuned solely on easily accessible text data with much fewer resources. Along with pre-trained visual-linguistic coupled modality space (e.g., CLIP and ALIGN model), a text-only training strategy is devised to further project the aligned multi-modal latent space to that of LLM, endowing the LLM with visual comprehension capabilities in an efficient manner. Instead of enormous image-text training data required by previous MLLMs, we find that TOMGPT can be well-tuned with fewer yet diverse GPT-generated free-form text data, as we establish the semantic connection between LLM and pre-trained vision-language model. A quantitative evaluation is conducted on both MME and LVLM, which are recently released and extensively utilized MLLM benchmarks. The experiments reveal that TOMGPT achieved reliable performance compared to numerous models trained on a large amount of image-text pair data. Case studies are also presented, demonstrating TOMGPT’s broad understanding and dialogue capabilities across diverse image categories.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4393259344",
    "type": "article"
  },
  {
    "title": "Detecting Process Duration Drift Using Gamma Mixture Models in a Left-Truncated and Right-Censored Environment",
    "doi": "https://doi.org/10.1145/3669942",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Lingkai Yang; Sally McClean; Mark Donnelly; Kashaf Khan; Kevin Burke",
    "corresponding_authors": "",
    "abstract": "Within the realm of business context, process duration signifies time spent by customers between successive activities. This temporal perspective offers important insight to customer behavior, highlighting potential bottlenecks, and influencing business management decisions. The distribution of these process duration often changes over time due to factors such as seasonality, emerging legislation, changes to supply chains, and customer demand. Referred to as concept drift, these variations pose challenges for robust process modeling, understanding, and refinement. Subsequently, gamma mixture models are widely employed to model durations. These source data can, however, become left-truncated and right-censored within any specific observation window thereby necessitating a (well-known) modification to the likelihood function. The approach reported in this article leveraged this adapted likelihood across a series of observation windows, applying the likelihood ratio test to identify duration changes/concept drift. Due to its flexibility in modelling any duration distribution, the gamma mixture model was used with Nelder–Mead optimized likelihood for the left-truncated and right-censored data. The number of gamma components was determined by the Bayesian information criterion. The proposed framework underwent validation through simulated exponential samples, leading to recommendations for its practical application. Subsequently, we applied the methodology to three real-life event logs exhibiting diverse characteristics. Experimental results showcase the effectiveness of our approach in terms of data fitting, as compared to Kaplan–Meier curves, and in detecting instances of drift. This comprehensive validation underscores the practical utility and reliability of our framework for dynamic business scenarios.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4399576866",
    "type": "article"
  },
  {
    "title": "Neural-Symbolic Methods for Knowledge Graph Reasoning: A Survey",
    "doi": "https://doi.org/10.1145/3686806",
    "publication_date": "2024-08-12",
    "publication_year": 2024,
    "authors": "Kewei Cheng; Nesreen K. Ahmed; Ryan A. Rossi; Theodore L. Willke; Yizhou Sun",
    "corresponding_authors": "",
    "abstract": "Neural symbolic knowledge graph (KG) reasoning offers a promising approach that combines the expressive power of symbolic reasoning with the learning capabilities inherent in neural networks. This survey provides a comprehensive overview of advancements, techniques, and challenges in the field of neural symbolic KG reasoning. The survey introduces the fundamental concepts of KGs and symbolic logic, followed by an exploration of three significant KG reasoning tasks: KG completion, complex query answering, and logical rule learning. For each task, we thoroughly discuss three distinct categories of methods: pure symbolic methods, pure neural approaches, and the integration of neural networks and symbolic reasoning methods known as neural-symbolic. We carefully analyze and compare the strengths and limitations of each category of methods to provide a comprehensive understanding. By synthesizing recent research contributions and identifying open research directions, this survey aims to equip researchers and practitioners with a comprehensive understanding of the state-of-the-art in neural symbolic KG reasoning, fostering future advancements in this interdisciplinary domain.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4401513697",
    "type": "article"
  },
  {
    "title": "A Survey on the Role of Crowds in Combating Online Misinformation: Annotators, Evaluators, and Creators",
    "doi": "https://doi.org/10.1145/3694980",
    "publication_date": "2024-09-11",
    "publication_year": 2024,
    "authors": "Bing He; Yibo Hu; Yeon-Chang Lee; Soyoung Oh; Gaurav Verma; Srijan Kumar",
    "corresponding_authors": "",
    "abstract": "Online misinformation poses a global risk with significant real-world consequences. To combat misinformation, current research relies on professionals like journalists and fact-checkers for annotating and debunking false information, while also developing automated machine learning methods for detecting misinformation. Complementary to these approaches, recent research has increasingly concentrated on utilizing the power of ordinary social media users, a.k.a. “the crowd”, who act as eyes-on-the-ground proactively questioning and countering misinformation. Notably, recent studies show that 96% of counter-misinformation responses originate from them. Acknowledging their prominent role, we present the first systematic and comprehensive survey of research papers that actively leverage the crowds to combat misinformation. In this survey, we first identify 88 papers related to crowd-based efforts 1 , following a meticulous annotation process adhering to the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses). We then present key statistics related to misinformation, counter-misinformation, and crowd input in different formats and topics. Upon holistic analysis of the papers, we introduce a novel taxonomy of the roles played by the crowds in combating misinformation: (i) crowds as annotators who actively identify misinformation; (ii) crowds as evaluators who assess counter-misinformation effectiveness; (iii) crowds as creators who create counter-misinformation. This taxonomy explores the crowd's capabilities in misinformation detection, identifies the prerequisites for effective counter-misinformation, and analyzes crowd-generated counter-misinformation. In each assigned role, we conduct a detailed analysis to categorize the specific utilization of the crowd. Particularly, we delve into (i) distinguishing individual, collaborative, and machine-assisted labeling for annotators; (ii) analyzing the effectiveness of counter-misinformation through surveys, interviews, and in-lab experiments for evaluators; and (iii) characterizing creation patterns and creator profiles for creators. Finally, we conclude this survey by outlining potential avenues for future research in this field.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4402441570",
    "type": "article"
  },
  {
    "title": "Intricate Spatiotemporal Dependency Learning for Temporal Knowledge Graph Reasoning",
    "doi": "https://doi.org/10.1145/3648366",
    "publication_date": "2024-02-16",
    "publication_year": 2024,
    "authors": "Xuefei Li; Huiwei Zhou; Weihong Yao; Wenchu Li; Bao-Jie Liu; Yingyu Lin",
    "corresponding_authors": "",
    "abstract": "Knowledge Graph (KG) reasoning has been an interesting topic in recent decades. Most current researches focus on predicting the missing facts for incomplete KG. Nevertheless, Temporal KG (TKG) reasoning, which is to forecast future facts, still faces with a dilemma due to the complex interactions between entities over time. This article proposes a novel intricate Spatiotemporal Dependency learning Network (STDN) based on Graph Convolutional Network (GCN) to capture the underlying correlations of an entity at different timestamps. Specifically, we first learn an adaptive adjacency matrix to depict the direct dependencies from the temporally adjacent facts of an entity, obtaining its previous context embedding. Then, a Spatiotemporal feature Encoding GCN (STE-GCN) is proposed to capture the latent spatiotemporal dependencies of the entity, getting the spatiotemporal embedding. Finally, a time gate unit is used to integrate the previous context embedding and the spatiotemporal embedding at the current timestamp to update the entity evolutional embedding for predicting future facts. STDN could generate the more expressive embeddings for capturing the intricate spatiotemporal dependencies in TKG. Extensive experiments on WIKI, ICEWS14, and ICEWS18 datasets prove our STDN has the advantage over state-of-the-art baselines for the temporal reasoning task.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4391883834",
    "type": "article"
  },
  {
    "title": "Spatio-Temporal Parallel Transformer based model for Traffic Prediction",
    "doi": "https://doi.org/10.1145/3679017",
    "publication_date": "2024-07-19",
    "publication_year": 2024,
    "authors": "Rahul Kumar; João Mendes‐Moreira; Joydeep Chandra",
    "corresponding_authors": "",
    "abstract": "Traffic forecasting problems involve jointly modeling the non-linear spatio-temporal dependencies at different scales. While graph neural network models have been effectively used to capture the non-linear spatial dependencies, capturing the dynamic spatial dependencies between the locations remains a major challenge. The errors in capturing such dependencies propagate in modeling the temporal dependencies between the locations, thereby severely affecting the performance of long-term predictions. While transformer-based mechanisms have been recently proposed for capturing the dynamic spatial dependencies, these methods are susceptible to fluctuations in data brought on by unforeseen events like traffic congestion and accidents. To mitigate these issues we propose an improvised spatio-temporal parallel transformer (STPT) based model for traffic prediction that uses multiple adjacency graphs passed through a pair of coupled graph transformer-convolution network units, operating in parallel, to generate more noise-resilient embeddings. We conduct extensive experiments on 4 real-world traffic datasets and compare the performance of STPT with several state-of-the-art baselines, in terms of measures like RMSE, MAE, and MAPE. We find that using STPT improves the performance by around \\(10-34\\%\\) as compared to the baselines. We also investigate the applicability of the model on other spatio-temporal data in other domains. We use a Covid-19 dataset to predict the number of future occurrences in different regions from a given set of historical occurrences. The results demonstrate the superiority of our model for such datasets.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4400813762",
    "type": "article"
  },
  {
    "title": "A Spatial-Temporal Aggregated Graph Neural Network for Docked Bike-sharing Demand Forecasting",
    "doi": "https://doi.org/10.1145/3690388",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Jiahui Feng; Hefu Liu; Jingmei Zhou; Yang Zhou",
    "corresponding_authors": "",
    "abstract": "Predicting the number of rented and returned bikes at each station is crucial for operators to proactively manage shared bike relocation. Although existing research has proposed spatial-temporal prediction models that significantly advance traffic prediction, these models often neglect the unique characteristics of shared bike systems (BSS). Spatially, the entire bike-sharing system (BSS) experiences peak activity during morning and evening rush hours, whereas, during other periods, activity is localized to local stations, with some recording no rides, highlighting the need to distinguish between global and local spatial information across different times. Temporally, the historical riding records for each station exhibit non-stationary patterns, necessitating the analysis of both global trends and local fluctuations. Existing Graph Neural Network (GNN) approaches to predicting shared bike demand primarily capture static spatial-temporal data and fail to account for the dynamic nature of bike flows. Moreover, these studies focus on global spatial-temporal information without considering local nuances, making it challenging to capture spatiotemporal dynamics in fluctuating BSS. To address these challenges, we introduce the Spatial-Temporal Aggregated Graph Neural Network (STAGNN). Our model first constructs a dynamic adjacent matrix to describe the evolving connections between stations, followed by local and global information layers to capture spatial-temporal information from large-scale shared bike networks accurately. Our methodology has been validated through experiments on four real-world datasets, comparing it against benchmark models to demonstrate superior prediction accuracy. Additionally, we conduct extended experiments on four datasets during the morning and evening rush hours, and the results also affirm the efficacy of the STAGNN in enhancing prediction performance.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4401944111",
    "type": "article"
  },
  {
    "title": "Modeling information-seeker satisfaction in community question answering",
    "doi": "https://doi.org/10.1145/1514888.1514893",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Eugene Agichtein; Yandong Liu; Jiang Bian",
    "corresponding_authors": "",
    "abstract": "Question Answering Communities such as Naver, Baidu Knows, and Yahoo! Answers have emerged as popular, and often effective, means of information seeking on the web. By posting questions for other participants to answer, information seekers can obtain specific answers to their questions. Users of CQA portals have already contributed millions of questions, and received hundreds of millions of answers from other participants. However, CQA is not always effective: in some cases, a user may obtain a perfect answer within minutes, and in others it may require hours—and sometimes days—until a satisfactory answer is contributed. We investigate the problem of predicting information seeker satisfaction in collaborative question answering communities, where we attempt to predict whether a question author will be satisfied with the answers submitted by the community participants. We present a general prediction model, and develop a variety of content, structure, and community-focused features for this task. Our experimental results, obtained from a large-scale evaluation over thousands of real questions and user ratings, demonstrate the feasibility of modeling and predicting asker satisfaction. We complement our results with a thorough investigation of the interactions and information seeking patterns in question answering communities that correlate with information seeker satisfaction. We also explore personalized models of asker satisfaction, and show that when sufficient interaction history exists, personalization can significantly improve prediction accuracy over a “one-size-fits-all” model. Our models and predictions could be useful for a variety of applications, such as user intent inference, answer ranking, interface design, and query suggestion and routing.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2006150671",
    "type": "article"
  },
  {
    "title": "Measuring and extracting proximity graphs in networks",
    "doi": "https://doi.org/10.1145/1297332.1297336",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Yehuda Koren; Stephen C. North; Chris Volinsky",
    "corresponding_authors": "",
    "abstract": "Measuring distance or some other form of proximity between objects is a standard data mining tool. Connection subgraphs were recently proposed as a way to demonstrate proximity between nodes in networks. We propose a new way of measuring and extracting proximity in networks called “cycle-free effective conductance” (CFEC). Importantly, the measured proximity is accompanied with a proximity subgraph which allows assessing and understanding measured values. Our proximity calculation can handle more than two endpoints, directed edges, is statistically well behaved, and produces an effectiveness score for the computed subgraphs. We provide an efficient algorithm to measure and extract proximity. Also, we report experimental results and show examples for four large network datasets: a telecommunications calling graph, the IMDB actors graph, an academic coauthorship network, and a movie recommendation system.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2056285697",
    "type": "article"
  },
  {
    "title": "Efficient online learning for multitask feature selection",
    "doi": "https://doi.org/10.1145/2499907.2499909",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Haiqin Yang; Michael R. Lyu; Irwin King",
    "corresponding_authors": "",
    "abstract": "Learning explanatory features across multiple related tasks, or MultiTask Feature Selection (MTFS), is an important problem in the applications of data mining, machine learning, and bioinformatics. Previous MTFS methods fulfill this task by batch-mode training. This makes them inefficient when data come sequentially or when the number of training data is so large that they cannot be loaded into the memory simultaneously. In order to tackle these problems, we propose a novel online learning framework to solve the MTFS problem. A main advantage of the online algorithm is its efficiency in both time complexity and memory cost. The weights of the MTFS models at each iteration can be updated by closed-form solutions based on the average of previous subgradients. This yields the worst-case bounds of the time complexity and memory cost at each iteration, both in the order of O ( d × Q ), where d is the number of feature dimensions and Q is the number of tasks. Moreover, we provide theoretical analysis for the average regret of the online learning algorithms, which also guarantees the convergence rate of the algorithms. Finally, we conduct detailed experiments to show the characteristics and merits of the online learning algorithms in solving several MTFS problems.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2039044448",
    "type": "article"
  },
  {
    "title": "Community Discovery via Metagraph Factorization",
    "doi": "https://doi.org/10.1145/1993077.1993081",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Yu‐Ru Lin; Jimeng Sun; Hari Sundaram; Aisling Kelliher; Paul Castro; Ravi Konuru",
    "corresponding_authors": "",
    "abstract": "This work aims at discovering community structure in rich media social networks through analysis of time-varying, multirelational data. Community structure represents the latent social context of user actions. It has important applications such as search and recommendation. The problem is particularly useful in the enterprise domain, where extracting emergent community structure on enterprise social media can help in forming new collaborative teams, in expertise discovery, and in the long term reorganization of enterprises based on collaboration patterns. There are several unique challenges: (a) In social media, the context of user actions is constantly changing and coevolving; hence the social context contains time-evolving multidimensional relations. (b) The social context is determined by the available system features and is unique in each social media platform; hence the analysis of such data needs to flexibly incorporate various system features. In this article we propose MetaFac (MetaGraph Factorization), a framework that extracts community structures from dynamic, multidimensional social contexts and interactions. Our work has three key contributions: (1) metagraph, a novel relational hypergraph representation for modeling multirelational and multidimensional social data; (2) an efficient multirelational factorization method for community extraction on a given metagraph; (3) an online method to handle time-varying relations through incremental metagraph factorization. Extensive experiments on real-world social data collected from an enterprise and the public Digg social media Web site suggest that our technique is scalable and is able to extract meaningful communities from social media contexts. We illustrate the usefulness of our framework through two prediction tasks: (1) in the enterprise dataset, the task is to predict users’ future interests on tag usage, and (2) in the Digg dataset, the task is to predict users’ future interests in voting and commenting on Digg stories. Our prediction significantly outperforms baseline methods (including aspect model and tensor analysis), indicating the promising direction of using metagraphs for handling time-varying social relational contexts.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2083329513",
    "type": "article"
  },
  {
    "title": "Random Projections for Linear Support Vector Machines",
    "doi": "https://doi.org/10.1145/2641760",
    "publication_date": "2014-08-29",
    "publication_year": 2014,
    "authors": "Saurabh Paul; Christos Boutsidis; Malik Magdon‐Ismail; Petros Drineas",
    "corresponding_authors": "",
    "abstract": "Let X be a data matrix of rank ρ, whose rows represent n points in d -dimensional space. The linear support vector machine constructs a hyperplane separator that maximizes the 1-norm soft margin. We develop a new oblivious dimension reduction technique that is precomputed and can be applied to any input matrix X . We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within ϵ-relative error, ensuring comparable generalization as in the original space in the case of classification. For regression, we show that the margin is preserved to ϵ-relative error with high probability. We present extensive experiments with real and synthetic data to support our theory.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W1980738513",
    "type": "article"
  },
  {
    "title": "Large-Scale Cross-Language Web Page Classification via Dual Knowledge Transfer Using Fast Nonnegative Matrix Trifactorization",
    "doi": "https://doi.org/10.1145/2710021",
    "publication_date": "2015-07-27",
    "publication_year": 2015,
    "authors": "Hua Wang; Feiping Nie; Heng Huang",
    "corresponding_authors": "",
    "abstract": "With the rapid growth of modern technologies, Internet has reached almost every corner of the world. As a result, it becomes more and more important to manage and mine information contained in Web pages in different languages. Traditional supervised learning methods usually require a large amount of training data to obtain accurate and robust classification models. However, labeled Web pages did not increase as fast as the growth of Internet. The lack of sufficient training Web pages in many languages, especially for those in uncommonly used languages, makes it a challenge for traditional classification algorithms to achieve satisfactory performance. To address this, we observe that Web pages for a same topic from different languages usually share some common semantic patterns, though in different representation forms. In addition, we also observe that the associations between word clusters and Web page classes are another type of reliable carriers to transfer knowledge across languages. With these recognitions, in this article we propose a novel joint nonnegative matrix trifactorization (NMTF) based Dual Knowledge Transfer (DKT) approach for cross-language Web page classification. Our approach transfers knowledge from the auxiliary language, in which abundant labeled Web pages are available, to the target languages, in which we want to classify Web pages, through two different paths: word cluster approximation and the associations between word clusters and Web page classes. With the reinforcement between these two different knowledge transfer paths, our approach can achieve better classification accuracy. In order to deal with the large-scale real world data, we further develop the proposed DKT approach by constraining the factor matrices of NMTF to be cluster indicator matrices. Due to the nature of cluster indicator matrices, we can decouple the proposed optimization objective and the resulted subproblems are of much smaller sizes involving much less matrix multiplications, which make our new approach much more computationally efficient. We evaluate the proposed approach in extensive experiments using a real world cross-language Web page data set. Promising results have demonstrated the effectiveness of our approach that are consistent with our theoretical analyses.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2204517102",
    "type": "article"
  },
  {
    "title": "P <scp>ar</scp> C <scp>ube</scp>",
    "doi": "https://doi.org/10.1145/2729980",
    "publication_date": "2015-07-22",
    "publication_year": 2015,
    "authors": "Evangelos E. Papalexakis; Christos Faloutsos; Nicholas D. Sidiropoulos",
    "corresponding_authors": "",
    "abstract": "How can we efficiently decompose a tensor into sparse factors, when the data do not fit in memory? Tensor decompositions have gained a steadily increasing popularity in data-mining applications; however, the current state-of-art decomposition algorithms operate on main memory and do not scale to truly large datasets. In this work, we propose P ar C ube , a new and highly parallelizable method for speeding up tensor decompositions that is well suited to produce sparse approximations. Experiments with even moderately large data indicate over 90% sparser outputs and 14 times faster execution, with approximation error close to the current state of the art irrespective of computation and memory requirements. We provide theoretical guarantees for the algorithm’s correctness and we experimentally validate our claims through extensive experiments, including four different real world datasets (E nron , L bnl , F acebook and N ell ), demonstrating its effectiveness for data-mining practitioners. In particular, we are the first to analyze the very large N ell dataset using a sparse tensor decomposition, demonstrating that P ar C ube enables us to handle effectively and efficiently very large datasets. Finally, we make our highly scalable parallel implementation publicly available, enabling reproducibility of our work.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2220516362",
    "type": "article"
  },
  {
    "title": "Large-Scale Online Feature Selection for Ultra-High Dimensional Sparse Data",
    "doi": "https://doi.org/10.1145/3070646",
    "publication_date": "2017-06-29",
    "publication_year": 2017,
    "authors": "Yue Wu; Steven C. H. Hoi; Tao Mei; Nenghai Yu",
    "corresponding_authors": "",
    "abstract": "Feature selection (FS) is an important technique in machine learning and data mining, especially for large-scale high-dimensional data. Most existing studies have been restricted to batch learning, which is often inefficient and poorly scalable when handling big data in real world. As real data may arrive sequentially and continuously, batch learning has to retrain the model for the new coming data, which is very computationally intensive. Online feature selection (OFS) is a promising new paradigm that is more efficient and scalable than batch learning algorithms. However, existing online algorithms usually fall short in their inferior efficacy. In this article, we present a novel second-order OFS algorithm that is simple yet effective, very fast and extremely scalable to deal with large-scale ultra-high dimensional sparse data streams. The basic idea is to exploit the second-order information to choose the subset of important features with high confidence weights. Unlike existing OFS methods that often suffer from extra high computational cost, we devise a novel algorithm with a MaxHeap-based approach, which is not only more effective than the existing first-order algorithms, but also significantly more efficient and scalable. Our extensive experiments validated that the proposed technique achieves highly competitive accuracy as compared with state-of-the-art batch FS methods, meanwhile it consumes significantly less computational cost that is orders of magnitude lower. Impressively, on a billion-scale synthetic dataset (1-billion dimensions, 1-billion non-zero features, and 1-million samples), the proposed algorithm takes less than 3 minutes to run on a single PC.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2179489622",
    "type": "article"
  },
  {
    "title": "Measuring Temporal Patterns in Dynamic Social Networks",
    "doi": "https://doi.org/10.1145/2749465",
    "publication_date": "2015-07-22",
    "publication_year": 2015,
    "authors": "Wei Wei; Kathleen M. Carley",
    "corresponding_authors": "",
    "abstract": "Given social networks over time, how can we measure network activities across different timesteps with a limited number of metrics? We propose two classes of dynamic metrics for assessing temporal evolution patterns of agents in terms of persistency and emergence. For each class of dynamic metrics, we implement it using three different temporal aggregation models ranging from the most commonly used Average Aggregation Model to more the complex models such as the Exponential Aggregation Model. We argue that the problem of measuring temporal patterns can be formulated using Recency and Primacy effect, which is a concept used to characterize human cognitive processes. Experimental results show that the way metrics model Recency--Primacy effect is closely related to their abilities to measure temporal patterns. Furthermore, our results indicate that future network agent activities can be predicted based on history information using dynamic metrics. By conducting multiple experiments, we are also able to find an optimal length of history information that is most relevant to future activities. This optimal length is highly consistent within a dataset and can be used as an intrinsic metric to evaluate a dynamic social network.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2199717152",
    "type": "article"
  },
  {
    "title": "Process Discovery under Precedence Constraints",
    "doi": "https://doi.org/10.1145/2710020",
    "publication_date": "2015-06-01",
    "publication_year": 2015,
    "authors": "Gianluigi Greco; Antonella Guzzo; Francesco Lupia; Luigi Pontieri",
    "corresponding_authors": "",
    "abstract": "Process discovery has emerged as a powerful approach to support the analysis and the design of complex processes. It consists of analyzing a set of traces registering the sequence of tasks performed along several enactments of a transactional system, in order to build a process model that can explain all the episodes recorded over them. An approach to accomplish this task is presented that can benefit from the background knowledge that, in many cases, is available to the analysts taking care of the process (re-)design. The approach is based on encoding the information gathered from the log and the (possibly) given background knowledge in terms of precedence constraints , that is, of constraints over the topology of the resulting process models. Mining algorithms are eventually formulated in terms of reasoning problems over precedence constraints, and the computational complexity of such problems is thoroughly analyzed by tracing their tractability frontier. Solution algorithms are proposed and their properties analyzed. These algorithms have been implemented in a prototype system, and results of a thorough experimental activity are discussed.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2204866765",
    "type": "article"
  },
  {
    "title": "Modeling Buying Motives for Personalized Product Bundle Recommendation",
    "doi": "https://doi.org/10.1145/3022185",
    "publication_date": "2017-03-06",
    "publication_year": 2017,
    "authors": "Guannan Liu; Yanjie Fu; Guoqing Chen; Hui Xiong; Can Chen",
    "corresponding_authors": "",
    "abstract": "Product bundling is a marketing strategy that offers several products/items for sale as one bundle. While the bundling strategy has been widely used, less efforts have been made to understand how items should be bundled with respect to consumers’ preferences and buying motives for product bundles. This article investigates the relationships between the items that are bought together within a product bundle. To that end, each purchased product bundle is formulated as a bundle graph with items as nodes and the associations between pairs of items in the bundle as edges. The relationships between items can be analyzed by the formation of edges in bundle graphs, which can be attributed to the associations of feature aspects. Then, a probabilistic model BPM (Bundle Purchases with Motives) is proposed to capture the composition of each bundle graph, with two latent factors node-type and edge-type introduced to describe the feature aspects and relationships respectively. Furthermore, based on the preferences inferred from the model, an approach for recommending items to form product bundles is developed by estimating the probability that a consumer would buy an associative item together with the item already bought in the shopping cart. Finally, experimental results on real-world transaction data collected from well-known shopping sites show the effectiveness advantages of the proposed approach over other baseline methods. Moreover, the experiments also show that the proposed model can explain consumers’ buying motives for product bundles in terms of different node-types and edge-types .",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2591707791",
    "type": "article"
  },
  {
    "title": "Robust Graph Regularized Nonnegative Matrix Factorization for Clustering",
    "doi": "https://doi.org/10.1145/3003730",
    "publication_date": "2017-03-06",
    "publication_year": 2017,
    "authors": "Chong Peng; Zhao Kang; Yunhong Hu; Jie Cheng; Qiang Cheng",
    "corresponding_authors": "",
    "abstract": "Matrix factorization is often used for data representation in many data mining and machine-learning problems. In particular, for a dataset without any negative entries, nonnegative matrix factorization (NMF) is often used to find a low-rank approximation by the product of two nonnegative matrices. With reduced dimensions, these matrices can be effectively used for many applications such as clustering. The existing methods of NMF are often afflicted with their sensitivity to outliers and noise in the data. To mitigate this drawback, in this paper, we consider integrating NMF into a robust principal component model, and design a robust formulation that effectively captures noise and outliers in the approximation while incorporating essential nonlinear structures. A set of comprehensive empirical evaluations in clustering applications demonstrates that the proposed method has strong robustness to gross errors and superior performance to current state-of-the-art methods.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2592859786",
    "type": "article"
  },
  {
    "title": "Mining Community Structures in Multidimensional Networks",
    "doi": "https://doi.org/10.1145/3080574",
    "publication_date": "2017-06-29",
    "publication_year": 2017,
    "authors": "Oualid Boutemine; Mohamed Bouguessa",
    "corresponding_authors": "",
    "abstract": "We investigate the problem of community detection in multidimensional networks, that is, networks where entities engage in various interaction types (dimensions) simultaneously. While some approaches have been proposed to identify community structures in multidimensional networks, there are a number of problems still to solve. In fact, the majority of the proposed approaches suffer from one or even more of the following limitations: (1) difficulty detecting communities in networks characterized by the presence of many irrelevant dimensions, (2) lack of systematic procedures to explicitly identify the relevant dimensions of each community, and (3) dependence on a set of user-supplied parameters, including the number of communities, that require a proper tuning. Most of the existing approaches are inadequate for dealing with these three issues in a unified framework. In this paper, we develop a novel approach that is capable of addressing the aforementioned limitations in a single framework. The proposed approach allows automated identification of communities and their sub-dimensional spaces using a novel objective function and a constrained label propagation-based optimization strategy. By leveraging the relevance of dimensions at the node level, the strategy aims to maximize the number of relevant within-community links while keeping track of the most relevant dimensions. A notable feature of the proposed approach is that it is able to automatically identify low dimensional community structures embedded in a high dimensional space. Experiments on synthetic and real multidimensional networks illustrate the suitability of the new method.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2724995883",
    "type": "article"
  },
  {
    "title": "Discovering Communities and Anomalies in Attributed Graphs",
    "doi": "https://doi.org/10.1145/3139241",
    "publication_date": "2018-01-10",
    "publication_year": 2018,
    "authors": "Bryan Perozzi; Leman Akoglu",
    "corresponding_authors": "",
    "abstract": "Given a network with node attributes, how can we identify communities and spot anomalies? How can we characterize, describe, or summarize the network in a succinct way? Community extraction requires a measure of quality for connected subgraphs (e.g., social circles). Existing subgraph measures, however, either consider only the connectedness of nodes inside the community and ignore the cross-edges at the boundary (e.g., density) or only quantify the structure of the community and ignore the node attributes (e.g., conductance). In this work, we focus on node-attributed networks and introduce: (1) a new measure of subgraph quality for attributed communities called normality, (2) a community extraction algorithm that uses normality to extract communities and a few characterizing attributes per community, and (3) a summarization and interactive visualization approach for attributed graph exploration. More specifically, (1) we first introduce a new measure to quantify the normality of an attributed subgraph. Our normality measure carefully utilizes structure and attributes together to quantify both the internal consistency and external separability. We then formulate an objective function to automatically infer a few attributes (called the “focus”) and respective attribute weights, so as to maximize the normality score of a given subgraph. Most notably, unlike many other approaches, our measure allows for many cross-edges as long as they can be “exonerated;” i.e., either (i) are expected under a null graph model, and/or (ii) their boundary nodes do not exhibit the focus attributes. Next, (2) we propose AMEN (for Attributed Mining of Entity Networks), an algorithm that simultaneously discovers the communities and their respective focus in a given graph, with a goal to maximize the total normality. Communities for which a focus that yields high normality cannot be found are considered low quality or anomalous. Last, (3) we formulate a summarization task with a multi-criteria objective, which selects a subset of the communities that (i) cover the entire graph well, are (ii) high quality and (iii) diverse in their focus attributes. We further design an interactive visualization interface that presents the communities to a user in an interpretable, user-friendly fashion. The user can explore all the communities, analyze various algorithm-generated summaries, as well as devise their own summaries interactively to characterize the network in a succinct way. As the experiments on real-world attributed graphs show, our proposed approaches effectively find anomalous communities and outperform several existing measures and methods, such as conductance, density, OddBall, and SODA. We also conduct extensive user studies to measure the capability and efficiency that our approach provides to the users toward network summarization, exploration, and sensemaking.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2784052497",
    "type": "article"
  },
  {
    "title": "Structural Analysis of User Choices for Mobile App Recommendation",
    "doi": "https://doi.org/10.1145/2983533",
    "publication_date": "2016-11-15",
    "publication_year": 2016,
    "authors": "Bin Liu; Yao Wu; Neil Zhenqiang Gong; Junjie Wu; Hui Xiong; Martin Ester",
    "corresponding_authors": "",
    "abstract": "Advances in smartphone technology have promoted the rapid development of mobile apps. However, the availability of a huge number of mobile apps in application stores has imposed the challenge of finding the right apps to meet the user needs. Indeed, there is a critical demand for personalized app recommendations. Along this line, there are opportunities and challenges posed by two unique characteristics of mobile apps. First, app markets have organized apps in a hierarchical taxonomy. Second, apps with similar functionalities are competing with each other. Although there are a variety of approaches for mobile app recommendations, these approaches do not have a focus on dealing with these opportunities and challenges. To this end, in this article, we provide a systematic study for addressing these challenges. Specifically, we develop a structural user choice model (SUCM) to learn fine-grained user preferences by exploiting the hierarchical taxonomy of apps as well as the competitive relationships among apps. Moreover, we design an efficient learning algorithm to estimate the parameters for the SUCM model. Finally, we perform extensive experiments on a large app adoption dataset collected from Google Play. The results show that SUCM consistently outperforms state-of-the-art Top-N recommendation methods by a significant margin.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2405494619",
    "type": "article"
  },
  {
    "title": "Scalable and Efficient Flow-Based Community Detection for Large-Scale Graph Analysis",
    "doi": "https://doi.org/10.1145/2992785",
    "publication_date": "2017-03-06",
    "publication_year": 2017,
    "authors": "Seunghee Bae; Daniel Halperin; Jevin D. West; Martin Rosvall; Bill Howe",
    "corresponding_authors": "",
    "abstract": "Community detection is an increasingly popular approach to uncover important structures in large networks. Flow-based community detection methods rely on communication patterns of the network rather than structural properties to determine communities. The Infomap algorithm in particular optimizes a novel objective function called the map equation and has been shown to outperform other approaches in third-party benchmarks. However, Infomap and its variants are inherently sequential, limiting their use for large-scale graphs. In this article, we propose a novel algorithm to optimize the map equation called RelaxMap. RelaxMap provides two important improvements over Infomap: parallelization, so that the map equation can be optimized over much larger graphs, and prioritization, so that the most important work occurs first, iterations take less time, and the algorithm converges faster. We implement these techniques using OpenMP on shared-memory multicore systems, and evaluate our approach on a variety of graphs from standard graph clustering benchmarks as well as real graph datasets. Our evaluation shows that both techniques are effective: RelaxMap achieves 70% parallel efficiency on eight cores, and prioritization improves algorithm performance by an additional 20--50% on average, depending on the graph properties. Additionally, RelaxMap converges in the similar number of iterations and provides solutions of equivalent quality as the serial Infomap implementation.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2591888683",
    "type": "article"
  },
  {
    "title": "Chromatic Correlation Clustering",
    "doi": "https://doi.org/10.1145/2728170",
    "publication_date": "2015-06-01",
    "publication_year": 2015,
    "authors": "Francesco Bonchi; Aristides Gionis; Francesco Gullo; Charalampos E. Tsourakakis; Antti Ukkonen",
    "corresponding_authors": "",
    "abstract": "We study a novel clustering problem in which the pairwise relations between objects are categorical . This problem can be viewed as clustering the vertices of a graph whose edges are of different types ( colors ). We introduce an objective function that ensures the edges within each cluster have, as much as possible, the same color. We show that the problem is NP -hard and propose a randomized algorithm with approximation guarantee proportional to the maximum degree of the input graph. The algorithm iteratively picks a random edge as a pivot, builds a cluster around it, and removes the cluster from the graph. Although being fast, easy to implement, and parameter-free, this algorithm tends to produce a relatively large number of clusters. To overcome this issue we introduce a variant algorithm, which modifies how the pivot is chosen and how the cluster is built around the pivot. Finally, to address the case where a fixed number of output clusters is required, we devise a third algorithm that directly optimizes the objective function based on the alternating-minimization paradigm. We also extend our objective function to handle cases where object’s relations are described by multiple labels. We modify our randomized approximation algorithm to optimize such an extended objective function and show that its approximation guarantee remains proportional to the maximum degree of the graph. We test our algorithms on synthetic and real data from the domains of social media, protein-interaction networks, and bibliometrics. Results reveal that our algorithms outperform a baseline algorithm both in the task of reconstructing a ground-truth clustering and in terms of objective-function value.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2204093302",
    "type": "article"
  },
  {
    "title": "Mining Product Adopter Information from Online Reviews for Improving Product Recommendation",
    "doi": "https://doi.org/10.1145/2842629",
    "publication_date": "2016-02-09",
    "publication_year": 2016,
    "authors": "Wayne Xin Zhao; Jinpeng Wang; Yulan He; Ji-Rong Wen; Edward Yi Chang; Xiaoming Li",
    "corresponding_authors": "",
    "abstract": "We present in this article an automated framework that extracts product adopter information from online reviews and incorporates the extracted information into feature-based matrix factorization for more effective product recommendation. In specific, we propose a bootstrapping approach for the extraction of product adopters from review text and categorize them into a number of different demographic categories. The aggregated demographic information of many product adopters can be used to characterize both products and users in the form of distributions over different demographic categories. We further propose a graph-based method to iteratively update user- and product-related distributions more reliably in a heterogeneous user--product graph and incorporate them as features into the matrix factorization approach for product recommendation. Our experimental results on a large dataset crawled from J ing D ong , the largest B2C e-commerce website in China, show that our proposed framework outperforms a number of competitive baselines for product recommendation.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2263643212",
    "type": "article"
  },
  {
    "title": "Adaptive Cluster Tendency Visualization and Anomaly Detection for Streaming Data",
    "doi": "https://doi.org/10.1145/2997656",
    "publication_date": "2016-12-03",
    "publication_year": 2016,
    "authors": "Dheeraj Kumar; James C. Bezdek; Sutharshan Rajasegarar; Marimuthu Palaniswami; Christopher Leckie; Jeffrey Chan; Jayavardhana Gubbi",
    "corresponding_authors": "",
    "abstract": "The growth in pervasive network infrastructure called the Internet of Things (IoT) enables a wide range of physical objects and environments to be monitored in fine spatial and temporal detail. The detailed, dynamic data that are collected in large quantities from sensor devices provide the basis for a variety of applications. Automatic interpretation of these evolving large data is required for timely detection of interesting events. This article develops and exemplifies two new relatives of the visual assessment of tendency (VAT) and improved visual assessment of tendency (iVAT) models, which uses cluster heat maps to visualize structure in static datasets. One new model is initialized with a static VAT/iVAT image, and then incrementally (hence inc-VAT/inc-iVAT) updates the current minimal spanning tree (MST) used by VAT with an efficient edge insertion scheme. Similarly, dec-VAT/dec-iVAT efficiently removes a node from the current VAT MST. A sequence of inc-iVAT/dec-iVAT images can be used for (visual) anomaly detection in evolving data streams and for sliding window based cluster assessment for time series data. The method is illustrated with four real datasets (three of them being smart city IoT data). The evaluation demonstrates the algorithms’ ability to successfully isolate anomalies and visualize changing cluster structure in the streaming data.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2560792524",
    "type": "article"
  },
  {
    "title": "Consensus Guided Multi-View Clustering",
    "doi": "https://doi.org/10.1145/3182384",
    "publication_date": "2018-04-23",
    "publication_year": 2018,
    "authors": "Hongfu Liu; Yun Fu",
    "corresponding_authors": "",
    "abstract": "In recent decades, tremendous emerging techniques thrive the artificial intelligence field due to the increasing collected data captured from multiple sensors. These multi-view data provide more rich information than traditional single-view data. Fusing heterogeneous information for certain tasks is a core part of multi-view learning, especially for multi-view clustering. Although numerous multi-view clustering algorithms have been proposed, most scholars focus on finding the common space of different views, but unfortunately ignore the benefits from partition level by ensemble clustering. For ensemble clustering, however, there is no interaction between individual partitions from each view and the final consensus one. To fill the gap, we propose a Consensus Guided Multi-View Clustering (CMVC) framework, which incorporates the generation of basic partitions from each view and fusion of consensus clustering in an interactive way, i.e., the consensus clustering guides the generation of basic partitions, and high quality basic partitions positively contribute to the consensus clustering as well. We design a non-trivial optimization solution to formulate CMVC into two iterative k -means clusterings by an approximate calculation. In addition, the generalization of CMVC provides a rich feasibility for different scenarios, and the extension of CMVC with incomplete multi-view clustering further validates the effectiveness for real-world applications. Extensive experiments demonstrate the advantages of CMVC over other widely used multi-view clustering methods in terms of cluster validity, and the robustness of CMVC to some important parameters and incomplete multi-view data.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2801999749",
    "type": "article"
  },
  {
    "title": "Large Scale Online Multiple Kernel Regression with Application to Time-Series Prediction",
    "doi": "https://doi.org/10.1145/3299875",
    "publication_date": "2019-01-23",
    "publication_year": 2019,
    "authors": "Doyen Sahoo; Steven C. H. Hoi; Bin Li",
    "corresponding_authors": "",
    "abstract": "Kernel-based regression represents an important family of learning techniques for solving challenging regression tasks with non-linear patterns. Despite being studied extensively, most of the existing work suffers from two major drawbacks as follows: (i) they are often designed for solving regression tasks in a batch learning setting, making them not only computationally inefficient and but also poorly scalable in real-world applications where data arrives sequentially; and (ii) they usually assume that a fixed kernel function is given prior to the learning task, which could result in poor performance if the chosen kernel is inappropriate. To overcome these drawbacks, this work presents a novel scheme of Online Multiple Kernel Regression (OMKR), which sequentially learns the kernel-based regressor in an online and scalable fashion, and dynamically explore a pool of multiple diverse kernels to avoid suffering from a single fixed poor kernel so as to remedy the drawback of manual/heuristic kernel selection. The OMKR problem is more challenging than regular kernel-based regression tasks since we have to on-the-fly determine both the optimal kernel-based regressor for each individual kernel and the best combination of the multiple kernel regressors. We propose a family of OMKR algorithms for regression and discuss their application to time series prediction tasks including application to AR, ARMA, and ARIMA time series. We develop novel approaches to make OMKR scalable for large datasets, to counter the problems arising from an unbounded number of support vectors. We also explore the effect of kernel combination at prediction level and at the representation level. Finally, we conduct extensive experiments to evaluate the empirical performance on both real-world regression and times series prediction tasks.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2913100396",
    "type": "article"
  },
  {
    "title": "Multi-Label Punitive kNN with Self-Adjusting Memory for Drifting Data Streams",
    "doi": "https://doi.org/10.1145/3363573",
    "publication_date": "2019-11-11",
    "publication_year": 2019,
    "authors": "Martha Roseberry; Bartosz Krawczyk; Alberto Cano",
    "corresponding_authors": "",
    "abstract": "In multi-label learning, data may simultaneously belong to more than one class. When multi-label data arrives as a stream, the challenges associated with multi-label learning are joined by those of data stream mining, including the need for algorithms that are fast and flexible, able to match both the speed and evolving nature of the stream. This article presents a punitive k nearest neighbors algorithm with a self-adjusting memory (MLSAMPkNN) for multi-label, drifting data streams. The memory adjusts in size to contain only the current concept and a novel punitive system identifies and penalizes errant data examples early, removing them from the window. By retaining and using only data that are both current and beneficial, MLSAMPkNN is able to adapt quickly and efficiently to changes within the data stream while still maintaining a low computational complexity. Additionally, the punitive removal mechanism offers increased robustness to various data-level difficulties present in data streams, such as class imbalance and noise. The experimental study compares the proposal to 24 algorithms using 30 real-world and 15 artificial multi-label data streams on six multi-label metrics, evaluation time, and memory consumption. The superior performance of the proposed method is validated through non-parametric statistical analysis, proving both high accuracy and low time complexity. MLSAMPkNN is a versatile classifier, capable of returning excellent performance in diverse stream scenarios.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2987491803",
    "type": "article"
  },
  {
    "title": "SemRe-Rank",
    "doi": "https://doi.org/10.1145/3201408",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Ziqi Zhang; Jie Gao; Fabio Ciravegna",
    "corresponding_authors": "",
    "abstract": "Automatic Term Extraction (ATE) deals with the extraction of terminology from a domain specific corpus, and has long been an established research area in data and knowledge acquisition. ATE remains a challenging task as it is known that there is no existing ATE methods that can consistently outperform others in any domain. This work adopts a refreshed perspective to this problem: instead of searching for such a ‘one-size-fit-all’ solution that may never exist, we propose to develop generic methods to ‘enhance’ existing ATE methods. We introduce SemRe-Rank, the first method based on this principle, to incorporate semantic relatedness—an often overlooked venue—into an existing ATE method to further improve its performance. SemRe-Rank incorporates word embeddings into a personalised PageRank process to compute ‘semantic importance’ scores for candidate terms from a graph of semantically related words (nodes), which are then used to revise the scores of candidate terms computed by a base ATE algorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on four datasets of diverse nature, it is shown to have achieved widespread improvement over all base methods and across all datasets, with up to 15 percentage points when measured by the Precision in the top ranked K candidate terms (the average for a set of K ’s), or up to 28 percentage points in F1 measured at a K that equals to the expected real terms in the candidates (F1 in short). Compared to an alternative approach built on the well-known TextRank algorithm, SemRe-Rank can potentially outperform by up to 8 points in Precision at top K , or up to 17 points in F1.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2796219422",
    "type": "article"
  },
  {
    "title": "GOOWE",
    "doi": "https://doi.org/10.1145/3139240",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Hamed Bonab; Fazlı Can",
    "corresponding_authors": "",
    "abstract": "Designing adaptive classifiers for an evolving data stream is a challenging task due to the data size and its dynamically changing nature. Combining individual classifiers in an online setting, the ensemble approach, is a well-known solution. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem, which is not yet fully addressed in online evolving environments. We propose a novel data stream ensemble classifier, called Geometrically Optimum and Online-Weighted Ensemble (GOOWE), which assigns optimum weights to the component classifiers using a sliding window containing the most recent data instances. We map vote scores of individual classifiers and true class labels into a spatial environment. Based on the Euclidean distance between vote scores and ideal-points, and using the linear least squares (LSQ) solution, we present a novel, dynamic, and online weighting approach. While LSQ is used for batch mode ensemble classifiers, it is the first time that we adapt and use it for online environments by providing a spatial modeling of online ensembles. In order to show the robustness of the proposed algorithm, we use real-world datasets and synthetic data generators using the Massive Online Analysis (MOA) libraries. First, we analyze the impact of our weighting system on prediction accuracy through two scenarios. Second, we compare GOOWE with eight state-of-the-art ensemble classifiers in a comprehensive experimental environment. Our experiments show that GOOWE provides improved reactions to different types of concept drift compared to our baselines. The statistical tests indicate a significant improvement in accuracy, with conservative time and memory requirements.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2962805619",
    "type": "article"
  },
  {
    "title": "Fully Dynamic Approximate k-Core Decomposition in Hypergraphs",
    "doi": "https://doi.org/10.1145/3385416",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Bintao Sun; T-H. Hubert Chan; Mauro Sozio",
    "corresponding_authors": "",
    "abstract": "In this article, we design algorithms to maintain approximate core values in dynamic hypergraphs. This notion has been well studied for normal graphs in both static and dynamic setting. We generalize the problem to hypergraphs when edges can be inserted or deleted by an adversary. We consider two dynamic scenarios. In the first case, there are only insertions; and in the second case, there can be both insertions and deletions. In either case, the update time is poly-logarithmic in the number of nodes, with the insertion-only case boasting a better approximation ratio. We also perform extensive experiments on large real-world datasets, which demonstrate the accuracy and efficiency of our algorithms.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W3033544197",
    "type": "article"
  },
  {
    "title": "Leveraging Label-Specific Discriminant Mapping Features for Multi-Label Learning",
    "doi": "https://doi.org/10.1145/3319911",
    "publication_date": "2019-04-26",
    "publication_year": 2019,
    "authors": "Yumeng Guo; Fu-Lai Chung; Guozheng Li; Jiancong Wang; James C. Gee",
    "corresponding_authors": "",
    "abstract": "As an important machine learning task, multi-label learning deals with the problem where each sample instance (feature vector) is associated with multiple labels simultaneously. Most existing approaches focus on manipulating the label space, such as exploiting correlations between labels and reducing label space dimension, with identical feature space in the process of classification. One potential drawback of this traditional strategy is that each label might have its own specific characteristics and using identical features for all label cannot lead to optimized performance. In this article, we propose an effective algorithm named LSDM, i.e., leveraging label-specific discriminant mapping features for multi-label learning , to overcome the drawback. LSDM sets diverse ratio parameter values to conduct cluster analysis on the positive and negative instances of identical label. It reconstructs label-specific feature space which includes distance information and spatial topology information. Our experimental results show that combining these two parts of information in the new feature representation can better exploit the clustering results in the learning process. Due to the problem of diverse combinations for identical label, we employ simplified linear discriminant analysis to efficiently excavate optimal one for each label and perform classification by querying the corresponding results. Comparison with the state-of-the-art algorithms on a total of 20 benchmark datasets clearly manifests the competitiveness of LSDM.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2942759156",
    "type": "article"
  },
  {
    "title": "Robust Tensor Recovery with Fiber Outliers for Traffic Events",
    "doi": "https://doi.org/10.1145/3417337",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Yue Hu; Daniel B. Work",
    "corresponding_authors": "",
    "abstract": "Event detection is gaining increasing attention in smart cities research. Large-scale mobility data serves as an important tool to uncover the dynamics of urban transportation systems, and more often than not the dataset is incomplete. In this article, we develop a method to detect extreme events in large traffic datasets, and to impute missing data during regular conditions. Specifically, we propose a robust tensor recovery problem to recover low-rank tensors under fiber-sparse corruptions with partial observations, and use it to identify events, and impute missing data under typical conditions. Our approach is scalable to large urban areas, taking full advantage of the spatio-temporal correlations in traffic patterns. We develop an efficient algorithm to solve the tensor recovery problem based on the alternating direction method of multipliers (ADMM) framework. Compared with existing l 1 norm regularized tensor decomposition methods, our algorithm can exactly recover the values of uncorrupted fibers of a low-rank tensor and find the positions of corrupted fibers under mild conditions. Numerical experiments illustrate that our algorithm can achieve exact recovery and outlier detection even with missing data rates as high as 40% under 5% gross corruption, depending on the tensor size and the Tucker rank of the low rank tensor. Finally, we apply our method on a real traffic dataset corresponding to downtown Nashville, TN and successfully detect the events like severe car crashes, construction lane closures, and other large events that cause significant traffic disruptions.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W3117280498",
    "type": "article"
  },
  {
    "title": "Probability Ordinal-Preserving Semantic Hashing for Large-Scale Image Retrieval",
    "doi": "https://doi.org/10.1145/3442204",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Zheng Zhang; Xiaofeng Zhu; Guangming Lu; Yudong Zhang",
    "corresponding_authors": "",
    "abstract": "Semantic hashing enables computation and memory-efficient image retrieval through learning similarity-preserving binary representations. Most existing hashing methods mainly focus on preserving the piecewise class information or pairwise correlations of samples into the learned binary codes while failing to capture the mutual triplet-level ordinal structure in similarity preservation. In this article, we propose a novel Probability Ordinal-preserving Semantic Hashing (POSH) framework, which for the first time defines the ordinal-preserving hashing concept under a non-parametric Bayesian theory. Specifically, we derive the whole learning framework of the ordinal similarity-preserving hashing based on the maximum posteriori estimation, where the probabilistic ordinal similarity preservation, probabilistic quantization function, and probabilistic semantic-preserving function are jointly considered into one unified learning framework. In particular, the proposed triplet-ordering correlation preservation scheme can effectively improve the interpretation of the learned hash codes under an economical anchor-induced asymmetric graph learning model. Moreover, the sparsity-guided selective quantization function is designed to minimize the loss of space transformation, and the regressive semantic function is explored to promote the flexibility of the formulated semantics in hash code learning. The final joint learning objective is formulated to concurrently preserve the ordinal locality of original data and explore potentials of semantics for producing discriminative hash codes. Importantly, an efficient alternating optimization algorithm with the strictly proof convergence guarantee is developed to solve the resulting objective problem. Extensive experiments on several large-scale datasets validate the superiority of the proposed method against state-of-the-art hashing-based retrieval methods.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3154992183",
    "type": "article"
  },
  {
    "title": "Benchmarking Unsupervised Outlier Detection with Realistic Synthetic Data",
    "doi": "https://doi.org/10.1145/3441453",
    "publication_date": "2021-04-18",
    "publication_year": 2021,
    "authors": "Georg Steinbuß; Klemens Böhm",
    "corresponding_authors": "",
    "abstract": "Benchmarking unsupervised outlier detection is difficult. Outliers are rare, and existing benchmark data contains outliers with various and unknown characteristics. Fully synthetic data usually consists of outliers and regular instance with clear characteristics and thus allows for a more meaningful evaluation of detection methods in principle. Nonetheless, there have only been few attempts to include synthetic data in benchmarks for outlier detection. This might be due to the imprecise notion of outliers or to the difficulty to arrive at a good coverage of different domains with synthetic data. In this work we propose a generic process for the generation of data sets for such benchmarking. The core idea is to reconstruct regular instances from existing real-world benchmark data while generating outliers so that they exhibit insightful characteristics. This allows both for a good coverage of domains and for helpful interpretations of results. We also describe three instantiations of the generic process that generate outliers with specific characteristics, like local outliers. A benchmark with state-of-the-art detection methods confirms that our generic process is indeed practical.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3153838899",
    "type": "article"
  },
  {
    "title": "The Pulse of Urban Transport: Exploring the Co-evolving Pattern for Spatio-temporal Forecasting",
    "doi": "https://doi.org/10.1145/3450528",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Jinliang Deng; Xiusi Chen; Zipei Fan; Renhe Jiang; Xuan Song; Ivor W. Tsang",
    "corresponding_authors": "",
    "abstract": "Transportation demand forecasting is a topic of large practical value. However, the model that fits the demand of one transportation by only considering the historical data of its own could be vulnerable since random fluctuations could easily impact the modeling. On the other hand, common factors like time and region attribute, drive the evolution demand of different transportation, leading to a co-evolving intrinsic property between different kinds of transportation. In this work, we focus on exploring the co-evolution between different modes of transport, e.g., taxi demand and shared-bike demand. Two significant challenges impede the discovery of the co-evolving pattern: (1) diversity of the co-evolving correlation, which varies from region to region and time to time. (2) Multi-modal data fusion. Taxi demand and shared-bike demand are time-series data, which have different representations with the external factors. Moreover, the distribution of taxi demand and bike demand are not identical. To overcome these challenges, we propose a novel method, known as co-evolving spatial temporal neural network (CEST). CEST learns a multi-view demand representation for each mode of transport, extracts the co-evolving pattern, then predicts the demand for the target transportation based on multi-scale representation, which includes fine-scale demand information and coarse-scale pattern information. We conduct extensive experiments to validate the superiority of our model over the state-of-art models.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3161335532",
    "type": "article"
  },
  {
    "title": "Adaptive Influence Maximization",
    "doi": "https://doi.org/10.1145/3447396",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Jianxiong Guo; Weili Wu",
    "corresponding_authors": "",
    "abstract": "Influence maximization problem attempts to find a small subset of nodes that makes the expected influence spread maximized, which has been researched intensively before. They all assumed that each user in the seed set we select is activated successfully and then spread the influence. However, in the real scenario, not all users in the seed set are willing to be an influencer. Based on that, we consider each user associated with a probability with which we can activate her as a seed, and we can attempt to activate her many times. In this paper, we study the adaptive influence maximization with multiple activations (Adaptive-IMMA) problem, where we select a node in each iteration, observe whether she accepts to be a seed, if yes, wait to observe the influence diffusion process; If no, we can attempt to activate her again with a higher cost or select another node as a seed. We model the multiple activations mathematically and define it on the domain of integer lattice. We propose a new concept, adaptive dr-submodularity, and show our Adaptive-IMMA is the problem that maximizing an adaptive monotone and dr-submodular function under the expected knapsack constraint. Adaptive dr-submodular maximization problem is never covered by any existing studies. Thus, we summarize its properties and study its approximability comprehensively, which is a non-trivial generalization of existing analysis about adaptive submodularity. Besides, to overcome the difficulty to estimate the expected influence spread, we combine our adaptive greedy policy with sampling techniques without losing the approximation ratio but reducing the time complexity. Finally, we conduct experiments on several real datasets to evaluate the effectiveness and efficiency of our proposed policies.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3162400915",
    "type": "article"
  },
  {
    "title": "Traffic Flow Forecasting in the COVID-19: A Deep Spatial-temporal Model Based on Discrete Wavelet Transformation",
    "doi": "https://doi.org/10.1145/3564753",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Haoran Li; Zhiqiang Lv; Jianbo Li; Zhihao Xu; Yue Wang; Haokai Sun; Zhaoyu Sheng",
    "corresponding_authors": "",
    "abstract": "Traffic flow prediction has always been the focus of research in the field of Intelligent Transportation Systems, which is conducive to the more reasonable allocation of basic transportation resources and formulation of transportation policies. The spread of COVID-19 has seriously affected the normal order in the transportation sector. With the increase in the number of infected people and the government's anti-epidemic policy, human outgoing activities have gradually decreased, resulting in increasingly obvious discreteness and irregularities in traffic flow data. This article proposes a deep-space time traffic flow prediction model based on discrete wavelet transform (DSTM-DWT) to overcome the highly discrete and irregular nature of the new crown epidemic. First, DSTM-DWT decomposes traffic flow into discrete attributes, such as flow trend, discrete amplitude, and discrete baseline. Second, we design the spatial relationship of the transportation network as a graph and integrate the new crown pneumonia epidemic data into the characteristics of each transportation node. Then, we use the graph convolutional network to calculate the spatial correlation of each node, and the temporal convolutional network to calculate the temporal correlation of the data. In order to solve the problem of high discreteness of traffic flow data during the epidemic, this article proposes a graph memory network (GMN), which is used to convert discrete magnitudes separated by discrete wavelet transform into high-dimensional discrete features. Finally, use DWT to segment the predicted traffic data, and then perform the inverse discrete wavelet transform between the newly segmented traffic trend and discrete baseline and the discrete model predicted by GMN to obtain the final traffic flow prediction result. In simulation experiments, this work was compared with the existing advanced baselines to verify the superiority of DSTM-DWT.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4297380734",
    "type": "article"
  },
  {
    "title": "Real-Time Anomaly Detection in Edge Streams",
    "doi": "https://doi.org/10.1145/3494564",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Siddharth Bhatia; Rui Liu; Bryan Hooi; Minji Yoon; Kijung Shin; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? Existing approaches aim to detect individually surprising edges. In this work, we propose MIDAS, which focuses on detecting microcluster anomalies, or suddenly arriving groups of suspiciously similar edges, such as lockstep behavior, including denial of service attacks in network traffic data. We further propose MIDAS-F, to solve the problem by which anomalies are incorporated into the algorithm's internal states, creating a `poisoning' effect that can allow future anomalies to slip through undetected. MIDAS-F introduces two modifications: 1) We modify the anomaly scoring function, aiming to reduce the `poisoning' effect of newly arriving edges; 2) We introduce a conditional merge step, which updates the algorithm's data structures after each time tick, but only if the anomaly score is below a threshold value, also to reduce the `poisoning' effect. Experiments show that MIDAS-F has significantly higher accuracy than MIDAS. MIDAS has the following properties: (a) it detects microcluster anomalies while providing theoretical guarantees about its false positive probability; (b) it is online, thus processing each edge in constant time and constant memory, and also processes the data orders-of-magnitude faster than state-of-the-art approaches; (c) it provides up to 62% higher ROC-AUC than state-of-the-art approaches.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4205329374",
    "type": "article"
  },
  {
    "title": "ARIS: A Noise Insensitive Data Pre-Processing Scheme for Data Reduction Using Influence Space",
    "doi": "https://doi.org/10.1145/3522592",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Jianghui Cai; Yuqing Yang; Haifeng Yang; Xujun Zhao; Jing Hao",
    "corresponding_authors": "",
    "abstract": "The extensive growth of data quantity has posed many challenges to data analysis and retrieval. Noise and redundancy are typical representatives of the above-mentioned challenges, which may reduce the reliability of analysis and retrieval results and increase storage and computing overhead. To solve the above problems, a two-stage data pre-processing framework for noise identification and data reduction, called ARIS, is proposed in this article. The first stage identifies and removes noises by the following steps: First, the influence space (IS) is introduced to elaborate data distribution. Second, a ranking factor (RF) is defined to describe the possibility that the points are regarded as noises, then, the definition of noise is given based on RF. Third, a clean dataset (CD) is obtained by removing noise from the original dataset. The second stage learns representative data and realizes data reduction. In this process, CD is divided into multiple small regions by IS. Then the reduced dataset is formed by collecting the representations of each region. The performance of ARIS is verified by experiments on artificial and real datasets. Experimental results show that ARIS effectively weakens the impact of noise and reduces the amount of data and significantly improves the accuracy of data analysis within a reasonable time cost range.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4220798605",
    "type": "article"
  },
  {
    "title": "Structure Diversity-Induced Anchor Graph Fusion for Multi-View Clustering",
    "doi": "https://doi.org/10.1145/3534931",
    "publication_date": "2022-05-05",
    "publication_year": 2022,
    "authors": "Xun Lu; Songhe Feng",
    "corresponding_authors": "",
    "abstract": "The anchor graph structure has been widely used to speed up large-scale multi-view clustering and exhibited promising performance. How to effectively integrate the anchor graphs on multiple views to achieve enhanced clustering performance still remains a challenging task. Existing fusing strategies ignore the structure diversity among anchor graphs and restrict the anchor generation to be same on different views, which degenerates the representation ability of corresponding fused consensus graph. To overcome these drawbacks, we propose a novel structural fusion framework to integrate the multi-view anchor graphs for clustering. Different from traditional integration strategies, we merge the anchors and edges of all the view-specific anchor graphs into a single graph for the structural optimal graph learning. Benefiting from the structural fusion strategy, the anchor generation of each view is not forced to be same, which greatly improves the representation capability of the target structural optimal graph, since the anchors of each view capture the diverse structure of different views. By leveraging the potential structural consistency among each anchor graph, a connectivity constraint is imposed on the target graph to indicate clusters directly without any post-processing such as k -means in classical spectral clustering. Substantial experiments on real-world datasets are conducted to verify the superiority of the proposed method, as compared with the state-of-the-arts over the clustering performance and time expenditure.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4229034616",
    "type": "article"
  },
  {
    "title": "ABLE: Meta-Path Prediction in Heterogeneous Information Networks",
    "doi": "https://doi.org/10.1145/3494558",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Chenji Huang; Yixiang Fang; Xuemin Lin; Xin Cao; Wenjie Zhang",
    "corresponding_authors": "",
    "abstract": "Given a heterogeneous information network (HIN) H, a head node h , a meta-path P, and a tail node t , the meta-path prediction aims at predicting whether h can be linked to t by an instance of P. Most existing solutions either require predefined meta-paths, which limits their scalability to schema-rich HINs and long meta-paths, or do not aim at predicting the existence of an instance of P. To address these issues, in this article, we propose a novel prediction model, called ABLE, by exploiting the A ttention mechanism and B i L STM for E mbedding. Particularly, we present a concatenation node embedding method by considering the node types and a dynamic meta-path embedding method that carefully considers the importance and positions of edge types in the meta-paths by the Attention mechanism and BiLSTM model, respectively. A triplet embedding is then derived to complete the prediction. We conduct extensive experiments on four real datasets. The empirical results show that ABLE outperforms the state-of-the-art methods by up to 20% and 22% of improvement of AUC and AP scores, respectively.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4205646925",
    "type": "article"
  },
  {
    "title": "Dynamic Multi-View Graph Neural Networks for Citywide Traffic Inference",
    "doi": "https://doi.org/10.1145/3564754",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Shaojie Dai; Jinshuai Wang; Chao Huang; Yanwei Yu; Junyu Dong",
    "corresponding_authors": "",
    "abstract": "Accurate citywide traffic inference is critical for improving intelligent transportation systems with smart city applications. However, this task is very challenging given the limited training data, due to the high cost of sensor installment and maintenance across the entire urban space. A more practical scenario to study the citywide traffic inference is effectively modeling the spatial and temporal traffic patterns with limited historical traffic observations. In this work, we propose a dynamic multi-view graph neural network for citywide traffic inference with the method CTVI+. Specifically, for the temporal dimension, we propose a temporal self-attention mechanism that is capable of learning the dynamics of traffic data with the time-evolving traffic volume variations. For spatial dimension, we build a multi-view graph neural network, employing the road-wise message passing scheme to capture the region dependencies. With the designed spatial-temporal learning paradigms, we enable our traffic inference model to encode the dynamism from both spatial and temporal traffic patterns, which is reflective of intra- and inter-road traffic correlations. In our evaluation, CTVI+ achieves consistent better performance compared with different baselines on real-world traffic volume datasets. Further ablation study validates the effectiveness of key components in CTVI+. We release the model implementation at https://github.com/dsj96/TKDD.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4297380497",
    "type": "article"
  },
  {
    "title": "DMGF-Net: An Efficient Dynamic Multi-Graph Fusion Network for Traffic Prediction",
    "doi": "https://doi.org/10.1145/3586164",
    "publication_date": "2023-03-03",
    "publication_year": 2023,
    "authors": "He Li; Duo Jin; Xuejiao Li; Jianbin Huang; Xiaoke Ma; Jiangtao Cui; De-Shuang Huang; Shaojie Qiao; Jaesoo Yoo",
    "corresponding_authors": "",
    "abstract": "Traffic prediction is the core task of intelligent transportation system (ITS) and accurate traffic prediction can greatly improve the utilization of public resources. Dynamic interaction of multiple spatial relationships will influence the accuracy of traffic prediction. However, many existing methods only consider static spatial relationships, which restricts the accuracy of the prediction. To address the above problem, in this article, we propose the Dynamic Multi-Graph Fusion Network (DMGF-Net) to model the spatial-temporal correlations in traffic network. In the DMGF-Net, the fusion graph is designed to leverage and extract the various spatial correlations between different regions by fusing spatial graph, semantic graph, and spatial-semantic graph. Further, to dynamically learn the importance of different neighbors, we design the Dynamic Spatial-Temporal Unit (DSTU), which can adjust the aggregation weights of different neighbors by combining the convolution operation and the attention mechanism. It can selectively aggregate spatial-temporal features from different neighbors. Extensive experiments on three datasets demonstrate that effectiveness of our model, especially on PEMS08, our model achieves an increase of about 8.55% and 7.55% in terms of MAE and RMSE than the static model STGCN.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4323042646",
    "type": "article"
  },
  {
    "title": "Neighborhood Weighted Voting-Based Noise Correction for Crowdsourcing",
    "doi": "https://doi.org/10.1145/3586998",
    "publication_date": "2023-03-11",
    "publication_year": 2023,
    "authors": "Huiru Li; Liangxiao Jiang; Siqing Xue",
    "corresponding_authors": "",
    "abstract": "In crowdsourcing scenarios, we can obtain each instance’s multiple noisy labels set from different crowd workers and then use a ground truth inference algorithm to infer its integrated label. Despite the effectiveness of ground truth inference algorithms, a certain level of noise still remains in the integrated labels. To reduce the impact of noise, many noise correction algorithms have been proposed in recent years. To the best of our knowledge, however, nearly all existing noise correction algorithms only exploit each instance’s own multiple noisy label sets but ignore the multiple noisy label sets of its neighbors. Here neighbors refer to the nearest instances found in the feature space based on the distance metric learning. In this article, we propose neighborhood weighted voting-based noise correction (NWVNC). In NWVNC, we at first take advantage of the multiple noisy label sets of each instance’s neighbors (including itself) to estimate the probability that it belongs to its integrated label. Then, we use the estimated probability to identify and filter noise instances and thus obtain a clean set and a noise set. Finally, we train three heterogeneous classifiers on the clean set and correct the noise instances by the consensus voting of three trained classifiers. The experimental results on 34 simulated and two real-world crowdsourced datasets show that NWVNC significantly outperforms all the other state-of-the-art noise correction algorithms used for comparison.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4323925706",
    "type": "article"
  },
  {
    "title": "TDAN: Transferable Domain Adversarial Network for Link Prediction in Heterogeneous Social Networks",
    "doi": "https://doi.org/10.1145/3610229",
    "publication_date": "2023-07-18",
    "publication_year": 2023,
    "authors": "Huan Wang; Guoquan Liu; Po Hu",
    "corresponding_authors": "",
    "abstract": "Link prediction has received increased attention in social network analysis. One of the unique challenges in heterogeneous social networks is link prediction in new link types without verified link information, such as recommending products to new overseas groups. Existing link prediction models tend to learn type-specific knowledge on specific link types and predict missing or future links on the same link types. However, because of the uncertainty of new link types in the evolving process of social networks, it is difficult to collect sufficient verified link information in new link types. Therefore, we propose the Transferable Domain Adversarial Network ( TDAN ) based on transfer learning to handle the challenge. TDAN exploits transferable type-shared knowledge in historical link types to help predict the unobserved links in new link types. TDAN mainly comprises a structural encoder, a domain discriminator, and an optimization decoder. The structural encoder learns the link representations in a heterogeneous social network. Subsequently, to learn transferable type-shared knowledge, the domain discriminator distinguishes link representations into different link types while minimizing the differences between type-specific knowledge in adversarial training. Inspired by the denoising auto-encoder, the optimization decoder reconstructs the learned type-shared knowledge to eliminate the noise generated during the adversarial training. Extensive experiments on Facebook and YouTube show that TDAN can outperform the state-of-the-art models.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4384696429",
    "type": "article"
  },
  {
    "title": "Contrastive Learning Based Graph Convolution Network for Social Recommendation",
    "doi": "https://doi.org/10.1145/3587268",
    "publication_date": "2023-03-11",
    "publication_year": 2023,
    "authors": "Jiabo Zhuang; Shunmei Meng; Jing Zhang; Victor S. Sheng",
    "corresponding_authors": "",
    "abstract": "Exploiting social networks is expected to enhance the performance of recommender systems when interaction information is sparse. Existing social recommendation models focus on modeling multi-graph structures and then aggregating the information from these multiple graphs to learn potential user preferences. However, these methods often employ complex models and redundant parameters to get a slight performance improvement. Contrastive learning has been widely researched as an effective paradigm in the area of recommendation. Most existing contrastive learning-based models usually focus on constructing multi-graph structures to perform graph augmentation for contrastive learning. However, the effect of graph augmentation on contrastive learning is inconclusive. In view of these challenges, in this work, we propose a contrastive learning based graph convolution network for social recommendation (CLSR), which integrates information from both the social graph and the interaction graph. First, we propose a fusion-simplified method to combine the social graph and the interaction graph. Technically, on the basis of exploring users’ interests by interaction graph, we further exploit social connections to alleviate data sparsity. By combining the user embeddings learned through two graphs in a certain proportion, we can obtain user representation at a finer granularity. Meanwhile, we introduce a contrastive learning framework for multi-graph network modeling, where we explore the feasibility of constructing positive and negative samples of contrastive learning by conducting data augmentation on embedding representations. Extensive experiments verify the superiority of CLSR’s contrastive learning framework and fusion-simplified method of integrating social relations.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4323923995",
    "type": "article"
  },
  {
    "title": "Learning the Explainable Semantic Relations via Unified Graph Topic-Disentangled Neural Networks",
    "doi": "https://doi.org/10.1145/3589964",
    "publication_date": "2023-04-03",
    "publication_year": 2023,
    "authors": "Likang Wu; Hongke Zhao; Zhi Li; Zhenya Huang; Qi Liu; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) such as Graph Convolutional Networks (GCNs) can effectively learn node representations via aggregating neighbors based on the relation graph. However, despite a few exceptions, most of the previous work in this line does not consider the topical semantics underlying the edges, making the node representations less effective and the learned relation between nodes hard to explain. For instance, the current GNNs make us usually don’t know what is the reason for the connection of network nodes, such as the specific research topics cited in this article and the concerns among friends on social platforms. Some methods have begun to explore the extraction of relation semantics in recent related literature, but existing studies generally face two bottlenecks, i.e., either being unable to explain the mined latent relations to ensure their reasonableness and independence, or demanding the textual content of edges which is unavailable in most real-world datasets. Actually, these two issues are both crucial in practical use. In our work, we propose a novel Topic-Disentangled Graph Neural Network (TDG) to address the above two issues at the same time, which explores the relation topics from the perspective of node contents. We design an optimized graph topic module to handle node features to construct independent and explainable semantic subspaces, then the reasonable relation topics that correspond to these subspaces are assigned to each graph relation via a neighborhood routing mechanism. Our proposed model can be easily combined with related graph tasks to form an end-to-end model, to avoid the risk of deviation between node representation space and task space. To evaluate the efficiency of our model, sufficient node-related tasks are conducted on three public datasets in the experimental section. The results show the obvious superiority of TDG compared with the state-of-the-art models.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4362578480",
    "type": "article"
  },
  {
    "title": "Modeling Long- and Short-Term User Preferences via Self-Supervised Learning for Next POI Recommendation",
    "doi": "https://doi.org/10.1145/3597211",
    "publication_date": "2023-05-12",
    "publication_year": 2023,
    "authors": "Shaowei Jiang; Wei He; Lizhen Cui; Yonghui Xu; Lei Liu",
    "corresponding_authors": "",
    "abstract": "With the accumulation of check-in data from location-based services, next Point-of-Interest (POI) recommendations are gaining increasing attention. It is well known that the spatio-temporal contextual information of user check-in behavior plays a crucial role in handling vital and inherent challenges in next POI recommendation, including capture of user dynamic preferences and the sparsity problem of check-in data. However, many studies either ignore or simply stack the context features with the embedding of POIs while relying only on POI recommendation loss to optimize the entire model, therefore failing to take full advantage of the potential information in contexts. Additionally, users’ interests are usually unstable and evolve over time, and accordingly recent studies have proposed various approaches to predict users’ next POIs by incorporating contextual information and modeling both their long- and short-term preferences, respectively. Yet many studies overemphasize the final POI recommendation performance, and the association between POI sequences and contextual information is not well embodied in data representations. In this article, we focus on the preceding problems and propose a unified attention framework for next POI recommendation by modeling users’ Long- and Short-term Preferences via Self-supervised Learning (LSPSL). Specifically, based on the self-attention network and two self-supervised optimization objectives, LSPSL first deeply exploits the intrinsic correlations between POI sequences and contextual information through pre-training, which strengthens data representations. Then, supported by pre-trained contextualized embeddings, LSPSL models and fuses users’ complex long- and short-term preferences in a unified way. Extensive experiments on real-world datasets demonstrate the superiority of our model compared with other state-of-the-art approaches.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4376277369",
    "type": "article"
  },
  {
    "title": "Multifaceted Relation-aware Meta-learning with Dual Customization for User Cold-start Recommendation",
    "doi": "https://doi.org/10.1145/3597458",
    "publication_date": "2023-05-16",
    "publication_year": 2023,
    "authors": "Chunyang Wang; Yanmin Zhu; Haobing Liu; Tianzi Zang; Ke Wang; Jiadi Yu",
    "corresponding_authors": "",
    "abstract": "User cold-start scenarios pose great challenges to recommendation systems in accurately capturing user preferences with sparse interaction records. Besides incorporating auxiliary information to enrich user/item representations, recent studies under the schema of meta-learning focus on quickly adapting personalized recommendation models based on cold-start users’ scarce interactions. The majority of meta-learning based recommendation methods follow a bi-level optimization paradigm and learn globally shared initialization across all cold-start recommendation tasks. In addition, to further facilitate the ability of fast adaptation, existing methods have made efforts to tailor task-specific prior knowledge by identifying the individual characteristics of each task. However, we argue that multi-view commonalities between existing users and cold-start users are also essential for precisely distinguishing new tasks, but not comprehensively modeled in previous studies. In this article, we propose a multifaceted relation-aware meta-learning approach namely MeCM for user cold-start recommendation, which enhances task-adaptive initialization customization by extracting multiple views of task relevance. We design a dual customization framework consisting of two successive phases including cluster-level customization and task-level customization. Specifically, MeCM first extracts multifaceted semantic relations between tasks and refines task commonalities into task clusters maintained with memory networks (MNs). Globally learned fast weights corresponding to task clusters are queried to perform cluster-level customization. Then task-level customization is triggered based on contextual information of the target task via interaction-wise encoding. Extensive experiments on real-world datasets demonstrate the superior performance of our model over state-of-the-art meta-learning-based recommendation methods.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4376643762",
    "type": "article"
  },
  {
    "title": "Multi-View Graph Convolutional Networks with Differentiable Node Selection",
    "doi": "https://doi.org/10.1145/3608954",
    "publication_date": "2023-07-11",
    "publication_year": 2023,
    "authors": "Zhaoliang Chen; Lele Fu; Shunxin Xiao; Shiping Wang; Claudia Plant; Wenzhong Guo",
    "corresponding_authors": "",
    "abstract": "Multi-view data containing complementary and consensus information can facilitate representation learning by exploiting the intact integration of multi-view features. Because most objects in the real world often have underlying connections, organizing multi-view data as heterogeneous graphs is beneficial to extracting latent information among different objects. Due to the powerful capability to gather information of neighborhood nodes, in this article, we apply Graph Convolutional Network (GCN) to cope with heterogeneous graph data originating from multi-view data, which is still under-explored in the field of GCN. In order to improve the quality of network topology and alleviate the interference of noises yielded by graph fusion, some methods undertake sorting operations before the graph convolution procedure. These GCN-based methods generally sort and select the most confident neighborhood nodes for each vertex, such as picking the top- k nodes according to pre-defined confidence values. Nonetheless, this is problematic due to the non-differentiable sorting operators and inflexible graph embedding learning, which may result in blocked gradient computations and undesired performance. To cope with these issues, we propose a joint framework dubbed Multi-view Graph Convolutional Network with Differentiable Node Selection (MGCN-DNS), which is constituted of an adaptive graph fusion layer, a graph learning module, and a differentiable node selection schema. MGCN-DNS accepts multi-channel graph-structural data as inputs and aims to learn more robust graph fusion through a differentiable neural network. The effectiveness of the proposed method is verified by rigorous comparisons with considerable state-of-the-art approaches in terms of multi-view semi-supervised classification tasks, and the experimental results indicate that MGCN-DNS achieves pleasurable performance on several benchmark multi-view datasets.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4383981973",
    "type": "article"
  },
  {
    "title": "FedEgo: Privacy-preserving Personalized Federated Graph Learning with Ego-graphs",
    "doi": "https://doi.org/10.1145/3624017",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Taolin Zhang; Chengyuan Mai; Yaomin Chang; Chuan Chen; Lin Shu; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "As special information carriers containing both structure and feature information, graphs are widely used in graph mining, e.g., Graph Neural Networks (GNNs). However, graph data are stored separately in multiple distributed parties in some practical scenarios, which may not be directly shared due to conflicts of interest. Hence, federated graph neural networks are proposed to address such data silo issues while preserving each party’s privacy (or client). Nevertheless, different graph data distributions of various parties, which is known as the statistical heterogeneity, may degrade the performance of naive federated learning algorithms like FedAvg. In this article, we propose FedEgo, a federated graph learning framework based on ego-graphs to tackle the challenges above, in which each client will train their local models while also contributing to the training of a global model. FedEgo applies GraphSAGE over ego-graphs to make full use of the structure information and utilizes Mixup for privacy concerns. To deal with the statistical heterogeneity, we integrate personalization into learning and propose an adaptive mixing coefficient strategy that enables clients to achieve their optimal personalization. Extensive experimental results and in-depth analysis demonstrate the effectiveness of FedEgo.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4386896750",
    "type": "article"
  },
  {
    "title": "Fairness-Aware Graph Neural Networks: A Survey",
    "doi": "https://doi.org/10.1145/3649142",
    "publication_date": "2024-02-24",
    "publication_year": 2024,
    "authors": "April Chen; Ryan A. Rossi; Namyong Park; Puja Trivedi; Yu Wang; Tong Yu; Sungchul Kim; Franck Dernoncourt; Nesreen K. Ahmed",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) have become increasingly important due to their representational power and state-of-the-art predictive performance on many fundamental learning tasks. Despite this success, GNNs suffer from fairness issues that arise as a result of the underlying graph data and the fundamental aggregation mechanism that lies at the heart of the large class of GNN models. In this article, we examine and categorize fairness techniques for improving the fairness of GNNs. We categorize these techniques by whether they focus on improving fairness in the pre-processing, in-processing (during training), or post-processing phases. We discuss how such techniques can be used together whenever appropriate and highlight the advantages and intuition as well. We also introduce an intuitive taxonomy for fairness evaluation metrics, including graph-level fairness, neighborhood-level fairness, embedding-level fairness, and prediction-level fairness metrics. In addition, graph datasets that are useful for benchmarking the fairness of GNN models are summarized succinctly. Finally, we highlight key open problems and challenges that remain to be addressed.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4392131519",
    "type": "article"
  },
  {
    "title": "A Survey on AutoML Methods and Systems for Clustering",
    "doi": "https://doi.org/10.1145/3643564",
    "publication_date": "2024-01-26",
    "publication_year": 2024,
    "authors": "Yannis Poulakis; Christos Doulkeridis; Dimosthenis Kyriazis",
    "corresponding_authors": "",
    "abstract": "Automated Machine Learning (AutoML) aims to identify the best-performing machine learning algorithm along with its input parameters for a given dataset and a specific machine learning task. This is a challenging problem, as the process of finding the best model and tuning it for a particular problem at hand is both time-consuming for a data scientist and computationally expensive. In this survey, we focus on unsupervised learning, and we turn our attention on AutoML methods for clustering. We present a systematic review that includes many recent research works for automated clustering. Furthermore, we provide a taxonomy for the classification of existing works, and we perform a qualitative comparison. As a result, this survey provides a comprehensive overview of the field of AutoML for clustering. Moreover, we identify open challenges for future research in this field.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4391262438",
    "type": "article"
  },
  {
    "title": "FairGAT: Fairness-Aware Graph Attention Networks",
    "doi": "https://doi.org/10.1145/3645096",
    "publication_date": "2024-02-12",
    "publication_year": 2024,
    "authors": "Öykü Deniz Köse; Yanning Shen",
    "corresponding_authors": "",
    "abstract": "Graphs can facilitate modeling various complex systems such as gene networks and power grids as well as analyzing the underlying relations within them. Learning over graphs has recently attracted increasing attention, particularly graph neural network (GNN)–based solutions, among which graph attention networks (GATs) have become one of the most widely utilized neural network structures for graph-based tasks. Although it is shown that the use of graph structures in learning results in the amplification of algorithmic bias, the influence of the attention design in GATs on algorithmic bias has not been investigated. Motivated by this, the present study first carries out a theoretical analysis in order to demonstrate the sources of algorithmic bias in GAT-based learning for node classification. Then, a novel algorithm, FairGAT, which leverages a fairness-aware attention design, is developed based on the theoretical findings. Experimental results on real-world networks demonstrate that FairGAT improves group fairness measures while also providing comparable utility to the fairness-aware baselines for node classification and link prediction.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4391753841",
    "type": "article"
  },
  {
    "title": "ProtoMGAE: Prototype-Aware Masked Graph Auto-Encoder for Graph Representation Learning",
    "doi": "https://doi.org/10.1145/3649143",
    "publication_date": "2024-02-20",
    "publication_year": 2024,
    "authors": "Yimei Zheng; Caiyan Jia",
    "corresponding_authors": "",
    "abstract": "Graph self-supervised representation learning has gained considerable attention and demonstrated remarkable efficacy in extracting meaningful representations from graphs, particularly in the absence of labeled data. Two representative methods in this domain are graph auto-encoding and graph contrastive learning. However, the former methods primarily focus on global structures, potentially overlooking some fine-grained information during reconstruction. The latter methods emphasize node similarity across correlated views in the embedding space, potentially neglecting the inherent global graph information in the original input space. Moreover, handling incomplete graphs in real-world scenarios, where original features are unavailable for certain nodes, poses challenges for both types of methods. To alleviate these limitations, we integrate masked graph auto-encoding and prototype-aware graph contrastive learning into a unified model to learn node representations in graphs. In our method, we begin by masking a portion of node features and utilize a specific decoding strategy to reconstruct the masked information. This process facilitates the recovery of graphs from a global or macro level and enables handling incomplete graphs easily. Moreover, we treat the masked graph and the original one as a pair of contrasting views, enforcing the alignment and uniformity between their corresponding node representations at a local or micro level. Last, to capture cluster structures from a meso level and learn more discriminative representations, we introduce a prototype-aware clustering consistency loss that is jointly optimized with the preceding two complementary objectives. Extensive experiments conducted on several datasets demonstrate that the proposed method achieves significantly better or competitive performance on downstream tasks, especially for graph clustering, compared with the state-of-the-art methods, showcasing its superiority in enhancing graph representation learning.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4391957172",
    "type": "article"
  },
  {
    "title": "Toward Few-Label Vertical Federated Learning",
    "doi": "https://doi.org/10.1145/3656344",
    "publication_date": "2024-04-09",
    "publication_year": 2024,
    "authors": "Lei Zhang; Lele Fu; Chen Liu; Yang Zhao; Jing‐Hua Yang; Zibin Zheng; Chuan Chen",
    "corresponding_authors": "",
    "abstract": "Federated Learning (FL) provides a novel paradigm for privacy-preserving machine learning, enabling multiple clients to collaborate on model training without sharing private data. To handle multi-source heterogeneous data, Vertical Federated Learning (VFL) has been extensively investigated. However, in the context of VFL, the label information tends to be kept in one authoritative client and is very limited. This poses two challenges for model training in the VFL scenario. On the one hand, a small number of labels cannot guarantee to train a well VFL model with informative network parameters, resulting in unclear boundaries for classification decisions. On the other hand, the large amount of unlabeled data is dominant and should not be discounted, and it is worthwhile to focus on how to leverage them to improve representation modeling capabilities. To address the preceding two challenges, we first introduce supervised contrastive loss to enhance the intra-class aggregation and inter-class estrangement, which is to deeply explore label information and improve the effectiveness of downstream classification tasks. Then, for unlabeled data, we introduce a pseudo-label-guided consistency mechanism to induce the classification results coherent across clients, which allows the representations learned by local networks to absorb the knowledge from other clients, and alleviates the disagreement between different clients for classification tasks. We conduct sufficient experiments on four commonly used datasets, and the experimental results demonstrate that our method is superior to the state-of-the-art methods, especially in the low-label rate scenario, and the improvement becomes more significant.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4394606312",
    "type": "article"
  },
  {
    "title": "LMACL: Improving Graph Collaborative Filtering with Learnable Model Augmentation Contrastive Learning",
    "doi": "https://doi.org/10.1145/3657302",
    "publication_date": "2024-04-12",
    "publication_year": 2024,
    "authors": "Xinru Liu; Yongjing Hao; Lei Zhao; Guanfeng Liu; Victor S. Sheng; Pengpeng Zhao",
    "corresponding_authors": "",
    "abstract": "Graph collaborative filtering (GCF) has achieved exciting recommendation performance with its ability to aggregate high-order graph structure information. Recently, contrastive learning (CL) has been incorporated into GCF to alleviate data sparsity and noise issues. However, most of the existing methods employ random or manual augmentation to produce contrastive views that may destroy the original topology and amplify the noisy effects. We argue that such augmentation is insufficient to produce the optimal contrastive view, leading to suboptimal recommendation results. In this article, we proposed a L earnable M odel A ugmentation C ontrastive L earning (LMACL) framework for recommendation, which effectively combines graph-level and node-level collaborative relations to enhance the expressiveness of collaborative filtering (CF) paradigm. Specifically, we first use the graph convolution network (GCN) as a backbone encoder to incorporate multi-hop neighbors into graph-level original node representations by leveraging the high-order connectivity in user-item interaction graphs. At the same time, we treat the multi-head graph attention network (GAT) as an augmentation view generator to adaptively generate high-quality node-level augmented views. Finally, joint learning endows the end-to-end training fashion. In this case, the mutual supervision and collaborative cooperation of GCN and GAT achieves learnable model augmentation. Extensive experiments on several benchmark datasets demonstrate that LMACL provides a significant improvement over the strongest baseline in terms of Recall and NDCG by 2.5%–3.8% and 1.6%–4.0%, respectively. Our model implementation code is available at https://github.com/LiuHsinx/LMACL .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4394734565",
    "type": "article"
  },
  {
    "title": "A Survey of Trustworthy Representation Learning Across Domains",
    "doi": "https://doi.org/10.1145/3657301",
    "publication_date": "2024-04-12",
    "publication_year": 2024,
    "authors": "Ronghang Zhu; Dongliang Guo; Daiqing Qi; Zhixuan Chu; Xiang Yu; Sheng Li",
    "corresponding_authors": "",
    "abstract": "As AI systems have obtained significant performance to be deployed widely in our daily lives and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts of AI systems, and representation learning is the fundamental technology in machine learning. How to make representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4394779996",
    "type": "article"
  },
  {
    "title": "Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation",
    "doi": "https://doi.org/10.1145/3671149",
    "publication_date": "2024-06-08",
    "publication_year": 2024,
    "authors": "Meng Xiao; Min Wu; Ziyue Qiao; Yanjie Fu; Zhiyuan Ning; Yi Du; Yuanchun Zhou",
    "corresponding_authors": "",
    "abstract": "The objective of topic inference in research proposals aims to obtain the most suitable disciplinary division from the discipline system defined by a funding agency. The agency will subsequently find appropriate peer review experts from their database based on this division. Automated topic inference can reduce human errors caused by manual topic filling, bridge the knowledge gap between funding agencies and project applicants, and improve system efficiency. Existing methods focus on modeling this as a hierarchical multi-label classification problem, using generative models to iteratively infer the most appropriate topic information. However, these methods overlook the gap in scale between interdisciplinary research proposals and non-interdisciplinary ones, leading to an unjust phenomenon where the automated inference system categorizes interdisciplinary proposals as non-interdisciplinary, causing unfairness during the expert assignment. How can we address this data imbalance issue under a complex discipline system and hence resolve this unfairness? In this paper, we implement a topic label inference system based on a Transformer encoder-decoder architecture. Furthermore, we utilize interpolation techniques to create a series of pseudo-interdisciplinary proposals from non-interdisciplinary ones during training based on non-parametric indicators such as cross-topic probabilities and topic occurrence probabilities. This approach aims to reduce the bias of the system during model training. Finally, we conduct extensive experiments on a real-world dataset to verify the effectiveness of the proposed method. The experimental results demonstrate that our training strategy can significantly mitigate the unfairness generated in the topic inference task. To improve the reproducibility of our research, we have released accompanying code by Dropbox. 1 .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4399461512",
    "type": "article"
  },
  {
    "title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese",
    "doi": "https://doi.org/10.1145/3686807",
    "publication_date": "2024-08-06",
    "publication_year": 2024,
    "authors": "Haochun Wang; Sendong Zhao; Zewen Qiang; Zijian Li; Chi Liu; Nuwa Xi; Yanrui Du; Bing Qin; Ting Liu",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts. To address this challenge, we propose knowledge-tuning, which leverages structured medical knowledge bases for the LLMs to grasp domain knowledge efficiently and facilitate trustworthy response generation. We also release cMedKnowQA, a Chinese medical knowledge question-answering dataset constructed from medical knowledge bases to assess the medical knowledge proficiency of LLMs. Experimental results show that the LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of accuracy in response generation compared with vanilla instruction-tuning and offer a new trustworthy way for the domain adaptation of LLMs. We release our code and data at https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4401379766",
    "type": "article"
  },
  {
    "title": "<i>C</i> <sup>3</sup> -GAN+: <u>C</u> omplex- <u>C</u> ondition- <u>C</u> ontrolled <u>G</u> enerative <u>A</u> dversarial <u>N</u> etworks with Enhanced Embedding",
    "doi": "https://doi.org/10.1145/3712264",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Yingxue Zhang; Yanhua Li; Xun Zhou; Zhenming Liu; Jun Luo",
    "corresponding_authors": "",
    "abstract": "Given historical traffic distributions and associated urban conditions observed in a city, the conditional urban traffic estimation problem aims at estimating realistic future projections of the traffic under a set of new urban conditions, e.g. , new bus routes, rainfall intensity and travel demands. The problem is important in reducing traffic congestion, improving public transportation efficiency, and facilitating urban planning. However, solving this problem is challenging due to the strong spatial dependencies of traffic patterns and the complex relations between the traffic and urban conditions. Recently, we proposed a C omplex- C ondition- C ontrolled G enerative A dversarial N etwork ( \\(C^{3}\\) -GAN), which tackles both of the challenges and solves the urban traffic estimation problem under various complex conditions by adding a fixed embedding network and an inference network on top of the standard conditional GAN model. The randomly chosen embedding network transforms the complex conditions to latent vectors, and the inference network enhances the connections between the embedded vectors and the traffic data. However, a randomly chosen embedding network cannot always successfully extract features of complex urban conditions, which indicates \\(C^{3}\\) -GAN is unable to uniquely map different urban conditions to proper latent distributions. Thus, \\(C^{3}\\) -GAN would fail in certain traffic estimation tasks. Besides, \\(C^{3}\\) -GAN is hard to train due to vanishing gradients and mode collapse problems. To address these issues, in this paper, we extend our prior work by introducing a new deep generative model, namely, \\(C^{3}\\) -GAN+, which significantly improves the estimation performance and model stability. \\(C^{3}\\) -GAN+ has new objective, architecture and training algorithm. The new objective applies Wasserstein loss to the conditional generation case to encourage stable training. Shared convolutional layers between the discriminator and the inference network help to capture spatial dependencies of traffic more efficiently, part of the shared convolutional layers are used to update the embedding network periodically aiming to encourage good representation and avoid model divergence. Extensive experiments on real-world datasets demonstrate that our \\(C^{3}\\) -GAN+ produces high-quality traffic estimations and outperforms state-of-the-art baseline methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406423271",
    "type": "article"
  },
  {
    "title": "Dual-View Learning from Crowds",
    "doi": "https://doi.org/10.1145/3712605",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Huan Zhang; Liangxiao Jiang; Wenjun Zhang; Geoffrey I. Webb",
    "corresponding_authors": "",
    "abstract": "Crowdsourcing services provide a fast and cheap way to obtain substantial labeled data by employing crowd workers on the Internet. In crowdsourcing learning, two-stage methods have been widely used, which first infer the integrated label for each instance and then build a learning model using instances with their integrated labels. However, existing two-stage methods mainly focus on how to infer more accurate integrated labels, after that, most of them directly regard the integrated labels as class labels to build a learning model, which loses the detailed worker labeling information in multiple noisy labels and thus results in suboptimal model accuracy. To solve this problem, in this study, we take the multiple noisy labels of each instance as its attribute value vector to construct another view in addition to the original attribute view, and propose a novel two-stage method called dual-view learning from crowds (DVLFC). In DVLFC, we first pick out workers with sufficient number of labels and augment the multiple noisy label set for each instance, then we build a supervised learning model in each view and at last we fuse their class-membership probabilities to get the final classification result. Extensive experiments on both real-world and artificial crowdsourced datasets prove the effectiveness of DVLFC.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406530548",
    "type": "article"
  },
  {
    "title": "Multi-Modal Twitter Data Analysis for Identifying Offensive Posts Using a Deep Cross Attention based Transformer Framework",
    "doi": "https://doi.org/10.1145/3713077",
    "publication_date": "2025-01-21",
    "publication_year": 2025,
    "authors": "Jayanta Paul; Siddhartha Mallick; Atindra K. Mitra; Anuska Roy; Jaya Sil",
    "corresponding_authors": "",
    "abstract": "In today’s society dissemination of information among the individuals occur very rapidly due to the widespread usage of social media platforms like Twitter (now-a-days acclaimed as X). However, information may pose challenges to maintaining a healthy online environment because often it contains harmful content. This paper presents a novel approach to identify different categories of offensive posts such as hate speech, profanity, targeted insult, and derogatory commentary by analyzing multi-modal image and text data, collected from Twitter. We propose a comprehensive deep learning framework, “Value Mixed Cross Attention Transformer” (VMCA-Trans) that leverage a combination of computer vision and natural language processing methodologies to effectively classify the posts into four classes with binary labels. We have created an in-house dataset (OffenTweet) comprising of Twitter posts having textual content, accompanying with images to build the proposed model. The dataset is carefully annotated by several experts with offensive labels such as hate speech, profanity, targeted insult, and derogatory commentary. VMCA-Trans utilizes fine-tuned state-of-the-art transformer based backbones such as ViT, BERT, RoBERTA etc. The combined representation of image and text embeddings obtained by these fine-tuned transformer encoders, is fed into a classifier to categorize the posts into offensive and non-offensive classes. To assess its effectiveness, we extensively evaluate the VMCA-Trans model using various performance metrics. The results indicate that the proposed multi-modal approach, achieves superior performance compared to traditional unimodal methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406686519",
    "type": "article"
  },
  {
    "title": "Evaluation-Free Time-Series Forecasting Model Selection via Meta-Learning",
    "doi": "https://doi.org/10.1145/3715149",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Mustafa Abdallah; Ryan A. Rossi; Kanak Mahadik; Sungchul Kim; Handong Zhao; Saurabh Bagchi",
    "corresponding_authors": "",
    "abstract": "Time-series forecasting models are invariably used in a variety of domains for crucial decision-making. Traditionally these models are constructed by experts with considerable manual effort. Unfortunately, this approach has poor scalability while generating accurate forecasts for new datasets belonging to diverse applications. Without access to skilled domain-knowledge, one approach is to train all the models on the new time-series data and then select the best one. However, this approach is nonviable in practice. In this work, we develop techniques for fast automatic selection of the best forecasting model for a new unseen time-series dataset, without having to first train (or evaluate) all the models on the new time-series data to select the best one. In particular, we develop a forecasting meta-learning approach called AutoForecast that allows for the quick inference of the best time-series forecasting model for an unseen dataset. Our approach learns both forecasting models performances over time horizon of the same dataset and task similarity across different datasets. The experiments demonstrate the effectiveness of the approach over state-of-the-art (SOTA) single and ensemble methods and several SOTA meta-learners (adapted to our problem) in terms of selecting better forecasting models (i.e., 2X gain) for unseen tasks for univariate and multivariate testbeds. AutoForecast has also significant reduction in inference time compared to the naïve approach (doing inference using all possible models and then selecting the best one), with median of 42X across the two testbeds. We release our meta-learning database corpus (348 datasets), performances of the 322 forecasting models on the database corpus, meta-features, and source codes for the community to access them for forecasting model selection and to build on them with new datasets and models which can help advance automating time-series forecasting problem. In our released database corpus, we unveil new traces of Adobe computing cluster usage for production workloads.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406798103",
    "type": "article"
  },
  {
    "title": "Disentangled Multi-Graph Convolution for Cross-Domain Recommendation",
    "doi": "https://doi.org/10.1145/3715151",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Yibo Gao; Zhen Liu; Xinxin Yang; Sibo Lu; Y. Yuan",
    "corresponding_authors": "",
    "abstract": "Data sparsity poses a significant challenge for recommendation systems, prompting the research of cross-domain recommendation ( CDR ). CDR aims to leverage more user-item interaction information from source domains to improve the recommendation performance in the target domain. However, a major challenge in CDR is the identification of transferable features. Traditional CDR methods struggle to distinguish between the various features of users, including domain-invariant features that are effective for feature transfer and domain-specific features that are detrimental to cross-domain information transfer. In this paper, we aim to disentangle domain-invariant features and domain-specific features and effectively utilize these different features. This enables effective domain-to-domain information transfer by only transferring domain-invariant features while still considering the role of domain-specific features within their respective domains. Based on the superiority of graph structural feature learning and disentangled represent learning, we propose \\(\\mathbf{DMGCDR}\\) —a model that learns D isentangled user feature representations and constructs a M ulti- G raph network for bidirectional knowledge transfer of shared features for CDR . Specifically, we designed two regularization terms to disentangle domain-invariant features and domain-specific features. Subsequently, we established a multi-graph convolutional network to enhance domain-specific features within single-domain graphs and transfer domain-invariant features across cross-domain graphs. Our approach also includes designing feature constraints to enhance the combination of features derived from different graphs and to uncover potential correlations among them. Extensive experiments on real-world datasets have demonstrated that our model significantly outperforms state-of-the-art cross-domain recommendation approaches.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406798303",
    "type": "article"
  },
  {
    "title": "Fuzzy Weighted Principal Component Analysis for Anomaly Detection",
    "doi": "https://doi.org/10.1145/3715148",
    "publication_date": "2025-01-29",
    "publication_year": 2025,
    "authors": "Sisi Wang; Feiping Nie; Zheng Wang; Rong Wang; Xuelong Li",
    "corresponding_authors": "",
    "abstract": "Principal Component Analysis (PCA) is one of the most famous unsupervised dimensionality reduction algorithms and has been widely used in many fields. However, it is very sensitive to outliers, which reduces the robustness of the algorithm.. In recent years, many studies have tried to employ \\(\\ell_{1}\\) -norm to improve the robustness of PCA, but they all lack rotation invariance or the solution is expensive. In this paper, we propose a novel robust principal component analysis, namely, Fuzzy Weighted Principal Component Analysis (FWPCA), which still uses squared \\(\\ell_{2}\\) -norm to minimize reconstruction error and maintains rotation invariance of PCA. The biggest bright spot is that the contribution of data is restricted by fuzzy weights, so that the contribution of normal samples are much greater than noise or abnormal data, and realizes anomaly detection. Besides, a more reasonable data center can be obtained by solving the optimal mean to make projection matrix more accurate. Subsequently, an effective iterative optimization algorithm is developed to solve this problem, and its convergence is strictly proved. Extensive experimental results on face datasets and RGB anomaly detection datasets show the superiority of our proposed method.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406956957",
    "type": "article"
  },
  {
    "title": "Online Stable Streaming Feature Selection via Feature Aggregation",
    "doi": "https://doi.org/10.1145/3715918",
    "publication_date": "2025-01-31",
    "publication_year": 2025,
    "authors": "Peng Zhou; Yanyan Wang; Yunyun Zhang; Zhaolong Ling; Shu Zhao; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Feature selection is an essential pre-process component in data mining that aims to select the most relevant features from the target dataset. Datasets are always dynamic in real-world applications, and features may exist in stream mode. Then, online streaming feature selection methods are proposed, which deal with streaming features arriving continuously in real-time. However, most existing algorithms prioritize high accuracy and low time consumption but overlook the stability of the selected features. Stable feature selection results are crucial for users in practice. For instance, in the medical field, unstable feature selection results can make it challenging for experts to identify the main causative factors of a disease. Motivated by this, this paper proposes a new Online Stable Streaming Feature Selection method via feature aggregation named OSSFS. Specifically, inspired by the cohesive MeanShift approach, OSSFS applies an incremental aggregation strategy to partition the streaming features into multiple hyperellipsoids. Then, we incrementally update and merge these hyperellipsoids with new streaming features. Finally, we select representative features from each hyperellipsoid as the final selected feature subset. Extensive experiments are conducted on several real-world datasets to compare our new method with state-of-the-art competing algorithms in cases of stability and predictive accuracy. Experimental results indicate that OSSFS achieves optimal stability without losing prediction accuracy.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407028966",
    "type": "article"
  },
  {
    "title": "ORIC: Feature Interaction Detection through Online Random Interaction Chains for Click-Through Rate Prediction",
    "doi": "https://doi.org/10.1145/3717070",
    "publication_date": "2025-02-13",
    "publication_year": 2025,
    "authors": "Yannian Kou; Qiuqiang Lin; Chuanhou Gao",
    "corresponding_authors": "",
    "abstract": "Click-Through Rate prediction aims to predict the ratio of clicks to impressions of a specific link, which is challenging due to (1) extremely high-dimensional categorical features; (2) both important original features and their interactions; and (3) reliance on different features and interactions in different time periods. To overcome these difficulties, we propose a new feature interaction detection method based on the idea of frequent itemset mining, named Online Random Intersection Chains (ORIC), which detects informative feature interactions with high interpretability. ORIC can be updated by controlling the importance of the historical and latest data with a tuning parameter, which saves computational burden and makes full use of historical information. Further, Streaming Integrated Model (SIM) is developed to feed the time-varying feature interactions into CTR prediction models. Empirical results on three benchmark datasets show that SIM achieves better performance than many CTR prediction models, as well as the efficiency, consistency and interpretability of ORIC.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407507041",
    "type": "article"
  },
  {
    "title": "Momentum-accelerated and Biased Unconstrained Non-negative Latent Factor Model for Handling High-Dimensional and Incomplete Data",
    "doi": "https://doi.org/10.1145/3717069",
    "publication_date": "2025-02-13",
    "publication_year": 2025,
    "authors": "Ming‐Wei Lin; Hengshuo Yang; Xiuqin Xu; Ling Lin; Zeshui Xu; Xin Luo",
    "corresponding_authors": "",
    "abstract": "High-dimensional and incomplete (HDI) data are involved frequently in big data-related industrial applications. Latent factor (LF) analysis aims at extracting the knowledge of great value from such extremely sparse HDI data efficiently. Non-negative latent factor models based on the single latent factor-dependent, non-negative and multiplicative update rules exactly are the representative of LF analysis. However, these models face low generalization dilemma due to incompatible with general unconstrained optimization techniques. To address this issue, this paper proposes a novel momentum-accelerated and biased unconstrained non-negative latent factor (MBUNLF) model, which matches with unconstrained optimization techniques. The proposed MBUNLF model is built on three main ideas: a) Improving the generalization through a non-negative mapping function; b) Capturing information among different entities through linear biases; c) Accelerating convergence during the training process through generalized momentum method. Empirical studies on six datasets from industrial applications indicate that the proposed MBUNLF model outperforms nine state-of-the-art models when processing HDI data, reducing the root mean square error by 19.47% on average. It demonstrates the validity of the MBUNLF model in extracting non-negative LFs from HDI data.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407507065",
    "type": "article"
  },
  {
    "title": "Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation Systems: A Survey",
    "doi": "https://doi.org/10.1145/3708325",
    "publication_date": "2025-02-20",
    "publication_year": 2025,
    "authors": "Zefang Zong; Jingwei Wang; Tao Feng; Xia Tong; Yong Li",
    "corresponding_authors": "",
    "abstract": "Recent technology development brings the boom of numerous new Demand-Driven Services (DDS) into urban lives, including ridesharing, on-demand delivery, express systems and warehousing. In DDS, a service loop is an elemental structure, including its service worker, the service providers and corresponding service targets. The service workers should transport either people or parcels from the providers to the target locations. Various planning tasks within DDS can thus be classified into two individual stages: 1) Dispatching, which is to form service loops from demand/supply distributions, and 2) Routing, which is to decide specific serving orders within the constructed loops. Generating high-quality strategies in both stages is important to develop DDS but faces several challenges. Meanwhile, deep reinforcement learning (DRL) has been developed rapidly in recent years. It is a powerful tool to solve these problems since DRL can learn a parametric model without relying on too many problem-based assumptions and optimize long-term effects by learning sequential decisions. In this survey, we first define DDS, then highlight common applications and important decision/control problems within. For each problem, we comprehensively introduce the existing DRL solutions. We also introduce open simulation environments for development and evaluation of DDS applications. Finally, we analyze remaining challenges and discuss further research opportunities in DRL solutions for DDS.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407765419",
    "type": "article"
  },
  {
    "title": "Attention-Mechanism-Based Neural Latent-Factorization-of-Tensors Model",
    "doi": "https://doi.org/10.1145/3719295",
    "publication_date": "2025-02-25",
    "publication_year": 2025,
    "authors": "Xiuqin Xu; Ming‐Wei Lin; Zeshui Xu; Xin Luo",
    "corresponding_authors": "",
    "abstract": "High-Dimensional and Incomplete (HDI) tensors contain a wealth of knowledge and patterns, which are typically utilized to characterize complex relationships between entities in a variety of industrial applications. Currently, the neural network-based tensor factorization model has shown superiority when handling the missing data in HDI tensors. However, it only uses the outer product of latent factors of entities and neglects the interactions between the latent factor (LF) vectors. In addition, the simple linear operation does not consider the nonlinear structure of the HDI tensor. To overcome the aforementioned issues, an Attention-mechanism-based Neural Latent-Factorization-of-Tensors (ANLFT) model is provided in this paper. It encompasses three primary ideas: a) incorporating the theory of neural networks with the latent factorization of tensor to construct the nonlinear structure in the HDI tensor effectively; b) adopting the attention mechanism of the LF vectors to depict the interactions between entities; c) using the position-transitional particle swarm optimization backward propagation learning ( \\(\\rm{P}^{2}\\) BP) scheme to train the ANLFT model efficiently. The experimental results on eight HDI datasets show that the ANLFT model can obtain higher estimation performance gain than state-of-the-art models. The convergence performance of the proposed model is also competitive with that of state-of-the-art models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407922777",
    "type": "article"
  },
  {
    "title": "Temporal Structural Preserving with Subtree Attention in Dynamic Graph Transformers",
    "doi": "https://doi.org/10.1145/3720549",
    "publication_date": "2025-02-27",
    "publication_year": 2025,
    "authors": "Minh D. N. Nguyen; Viet Cuong Ta",
    "corresponding_authors": "",
    "abstract": "Dynamic graph learning is a rapidly developing area of research due to its widespread application in various real-world networks. Most existing works combine graph neural networks and sequential models to exploit the graph topology and the temporal information of dynamic graphs. However, these methods exhibit certain limitations in extracting local and global information and capturing fine-grained temporal structure in dynamic graphs. In this paper, we present our novel framework, Dynamic Graph Subtree Attention, which is centralized by a learnable temporal edge sampling module and a lightweight attention operator to address the aforementioned issues. Our approach first constructs a temporal union graph for each time step using an adaptive edge sampling module, which preserves relevant interactions for our graph encoder to directly exploit fine-gained interactions across different times. Based on the temporal union graph, we further propose a subtree attention module that leverages the multi-hop representation and the self-attention mechanism to properly extract the local and global information from first- to high-order neighborhoods. To further reduce the computation complexity, the subtree module is equipped with a kernelized attention operation, which scales linearly with respect to the number of edges. By performing extensive experiments, we demonstrate the superiority of our proposed model in dynamic graph representation learning, as it consistently outperforms existing methods in future link prediction tasks. The code is publicly available at: https://github.com/minhduc1122002/DySubTree .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408024623",
    "type": "article"
  },
  {
    "title": "DHFM: Diversity Enhanced Hypergraph Factorization Machines for Feature Interaction Modeling",
    "doi": "https://doi.org/10.1145/3721982",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Hongyu Shi; Ling Chen; Xing Tang; Dandan Lyu",
    "corresponding_authors": "",
    "abstract": "Feature interaction modeling, which exploits interactive information between features, has been widely explored in various applications. Recently, many graph neural networks (GNNs) based models are proposed to model feature interactions by predicting the existence of edges between pairwise nodes that represent features. However, these models can only directly model two-order feature interactions. Although stacking multiple GNN layers can implicitly capture the arbitrary high-order feature interactions, it may lead to the over-smoothing problem. To this end, we propose DHFM, D iversity enhanced H ypergraph F actorization M achines that incorporate hypergraphs into feature interaction modeling, which can model the diverse feature interactions of different orders explicitly. Specifically, order-wise hyperedge predictors are proposed to discover beneficial feature interactions and explicitly model the feature interactions of different orders. In addition, diversity measures are introduced in hyperedge predictors and in the results of feature interactions to make discovered feature interactions as diverse as possible and avoid generating correlated errors. Extensive experiments on four real-world datasets demonstrate the superiority of the proposed model. In addition, the case study is conducted to further justify the effectiveness of the proposed model.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408220836",
    "type": "article"
  },
  {
    "title": "VGA: Vision and Graph Fused Attention Network for Rumor Detection",
    "doi": "https://doi.org/10.1145/3722225",
    "publication_date": "2025-03-11",
    "publication_year": 2025,
    "authors": "Lin Bai; Caiyan Jia; Ziying Song; Chaoqun Cui",
    "corresponding_authors": "",
    "abstract": "With the development of social media, rumors have been spread broadly on social media platforms, causing great harm to society. Beside textual information, many rumors also use manipulated images or conceal textual information within images to deceive people and avoid being detected, making multimodal rumor detection be a critical problem. The majority of multimodal rumor detection methods mainly concentrate on extracting features of source claims and their corresponding images, while ignoring the comments of rumors and their propagation structures. These comments and structures imply the wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these methods usually focus on basic visual feature extraction and rarely take into account image tampering or textual information. Therefore, in this study, we propose a novel Vision and Graph Fused Attention Network (VGA) for rumor detection to utilize propagation structures among posts so as to obtain the crowd opinions and further explore visual tampering features, as well as the textual information hidden in images. We conduct extensive experiments on three datasets, demonstrating that VGA can effectively detect multimodal rumors and outperform state-of-the-art methods significantly.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408338194",
    "type": "article"
  },
  {
    "title": "Have Our Cake and Eat It: Augmentation Diversity and Semantic Consistency Balanced Graph Contrastive Learning",
    "doi": "https://doi.org/10.1145/3728646",
    "publication_date": "2025-04-09",
    "publication_year": 2025,
    "authors": "Hao Yan; Senzhang Wang; Chaozhuo Li; Jun Yin; Philip S. Yu; Jianxin Wang",
    "corresponding_authors": "",
    "abstract": "Self-supervised learning on graph neural networks is receiving increasing attention due to the difficulty of obtaining graph labels in many real applications. Graph contrastive learning (GCL), a recently popular method for self-supervised learning on graphs, has achieved great success in many tasks. The key to the effectiveness of GCL is the construction of suitable contrasting pairs to capture important attributes of the data through the data augmentation modules. However, most of the existing approaches fail to fully consider both data diversity and the semantic consistency when conducting data augmentation. To fill this gap, we propose an augmentation diversity and semantic consistency balanced graph contrastive learning model (ADSCB for short), which enhances the representation ability of the CL model through richer contrasting objectives. In particular, we first introduce a semantic consistency module to extract the subgraph from the original graph through optimizing a carefully designed semantic consistency loss. Then, we introduce an augmentation diversity module and perform data augmentation and cross-scale mix-up operations on the original graph and the extracted semantic preserved subgraph to generate more diverse contrasting pairs. With the above two modules, our model ultimately achieves two contrasting objectives: diversity contrasting and semantic contrasting. The trade-off between these two contrasting objectives allows our model to benefit from both the augmentation diversity and the semantic consistency. We evaluate ADSCB for graph classification in unsupervised, semi-supervised, and transfer learning settings using standard graph contrastive learning benchmarks. The results demonstrate the superiority of our method against several state-of-the-art baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409292193",
    "type": "article"
  },
  {
    "title": "Clustering Categorical Data via Multiple Hypothesis Testing",
    "doi": "https://doi.org/10.1145/3735977",
    "publication_date": "2025-05-15",
    "publication_year": 2025,
    "authors": "Lianyu Hu; Mudi Jiang; Yan Liu; Quan Zou; Zengyou He",
    "corresponding_authors": "",
    "abstract": "Categorical data clustering is a fundamental data mining problem, which has been extensively studied during the past decades. To date, many effective clustering algorithms for categorical data are available in the literature. However, almost all existing categorical data clustering algorithms did not address the issue of the statistical significance of detected clusters. In particular, how to assess the statistical significance of a set of non-overlapping categorical clusters still remains unaddressed. In this paper, we formulate the categorical data clustering problem as a multiple hypothesis testing problem, where the null hypothesis is that each attribute is independent of the given partition of clusters. Then, all individual \\(p\\) -values from different attributes are integrated to obtain a consensus \\(p\\) -value through statistical meta-analysis. Thereafter, a significance-based clustering algorithm is proposed in which the combined \\(p\\) -value is efficiently optimized in an indirectly and incremental manner. Experimental results on 25 real-world data sets demonstrate that our method is capable of achieving comparable performance to state-of-the-art categorical data clustering algorithms. Furthermore, our method has a good capability of determining whether there really exists a clustering structure and assessing whether a given set of clusters is statistically significant.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410398663",
    "type": "article"
  },
  {
    "title": "An Efficient Framework for Epidemiological Parameter Estimation via Graph Reduction and Graph Neural Networks",
    "doi": "https://doi.org/10.1145/3736727",
    "publication_date": "2025-05-22",
    "publication_year": 2025,
    "authors": "Muhammad Alfas; Manoj Kumar; Shaurya Shriyam; Sandeep Kumar",
    "corresponding_authors": "",
    "abstract": "We propose an epidemiological parameter estimation framework based on contact networks and graph neural networks (GNNs). Contact network-based epidemiological models allow us to capture heterogeneity and individual-level details more effectively. Parameter estimation involves fitting real-world disease data to mathematical models. Traditionally, several likelihood-based methods that focus on compartment-based simulation models have been widely used to perform parameter estimation. However, these methods suffer from making assumptions such as homogeneous traits among the individuals of the population under consideration, which may cause them to fail in handling the complexity and diversity of real-world data. Our proposed framework estimates epidemiological parameters based on the availability of contact network data and individual-level disease time series data. We use supervised as well as self-supervised GNN architectures to incorporate the contact network information into the model. We also employed graph reduction methods such as sampling and coarsening to study scaling behavior and computational efficiency. We formulated the parameter estimation in two ways to study the predictive behavior better: classification and inference problems. We experimentally confirm improvements over the baselines chosen in this paper. We also conducted ablation studies, explainability quantification, and scalability experiments to generate further insights into the GNN models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410610005",
    "type": "article"
  },
  {
    "title": "Corrigendum: Diffusion Models for Tabular Data Imputation and Synthetic Data Generation",
    "doi": "https://doi.org/10.1145/3761939",
    "publication_date": "2025-09-25",
    "publication_year": 2025,
    "authors": "Mario Villaizán-Vallelado; Matteo Salvatori; Carlos Segura; Ioannis Arapakis",
    "corresponding_authors": "",
    "abstract": "This is a corrigendum for the article “Diffusion Models for Tabular Data Imputation and Synthetic Data Generation” published in ACM Trans. Knowl. Discov. Data 19(6): 125:1-125:32 (2025).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4414511309",
    "type": "erratum"
  },
  {
    "title": "Joint cluster analysis of attribute data and relationship data",
    "doi": "https://doi.org/10.1145/1376815.1376816",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Rong Ge; Martin Ester; Byron J. Gao; Zengjian Hu; Binay Bhattacharya; Boaz Ben‐Moshe",
    "corresponding_authors": "",
    "abstract": "Attribute data and relationship data are two principal types of data, representing the intrinsic and extrinsic properties of entities. While attribute data have been the main source of data for cluster analysis, relationship data such as social networks or metabolic networks are becoming increasingly available. It is also common to observe both data types carry complementary information such as in market segmentation and community identification, which calls for a joint cluster analysis of both data types so as to achieve better results. In this article, we introduce the novel Connected k -Center ( CkC ) problem, a clustering model taking into account attribute data as well as relationship data. We analyze the complexity of the problem and prove its NP-hardness. Therefore, we analyze the approximability of the problem and also present a constant factor approximation algorithm. For the special case of the CkC problem where the relationship data form a tree structure, we propose a dynamic programming method giving an optimal solution in polynomial time. We further present NetScan, a heuristic algorithm that is efficient and effective for large real databases. Our extensive experimental evaluation on real datasets demonstrates the meaningfulness and accuracy of the NetScan results.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W1965792576",
    "type": "article"
  },
  {
    "title": "Learning to detect events with Markov-modulated poisson processes",
    "doi": "https://doi.org/10.1145/1297332.1297337",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Alexander Ihler; Jon Hutchins; Padhraic Smyth",
    "corresponding_authors": "",
    "abstract": "Time-series of count data occur in many different contexts, including Internet navigation logs, freeway traffic monitoring, and security logs associated with buildings. In this article we describe a framework for detecting anomalous events in such data using an unsupervised learning approach. Normal periodic behavior is modeled via a time-varying Poisson process model, which in turn is modulated by a hidden Markov process that accounts for bursty events. We outline a Bayesian framework for learning the parameters of this model from count time-series. Two large real-world datasets of time-series counts are used as testbeds to validate the approach, consisting of freeway traffic data and logs of people entering and exiting a building. We show that the proposed model is significantly more accurate at detecting known events than a more traditional threshold-based technique. We also describe how the model can be used to investigate different degrees of periodicity in the data, including systematic day-of-week and time-of-day effects, and to make inferences about different aspects of events such as number of vehicles or people involved. The results indicate that the Markov-modulated Poisson framework provides a robust and accurate framework for adaptively and autonomously learning how to separate unusual bursty events from traces of normal human activity.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2039738096",
    "type": "article"
  },
  {
    "title": "Tree model guided candidate generation for mining frequent subtrees from XML documents",
    "doi": "https://doi.org/10.1145/1376815.1376818",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Henry Tan; Fedja Hadzic; Tharam S. Dillon; Elizabeth Chang; Ling Feng",
    "corresponding_authors": "",
    "abstract": "Due to the inherent flexibilities in both structure and semantics, XML association rules mining faces few challenges, such as: a more complicated hierarchical data structure and ordered data context. Mining frequent patterns from XML documents can be recast as mining frequent tree structures from a database of XML documents. In this study, we model a database of XML documents as a database of rooted labeled ordered subtrees. In particular, we are mainly concerned with mining frequent induced and embedded ordered subtrees. Our main contributions are as follows. We describe our unique embedding list representation of the tree structure, which enables efficient implementation of our Tree Model Guided ( TMG ) candidate generation. TMG is an optimal, nonredundant enumeration strategy that enumerates all the valid candidates that conform to the structural aspects of the data. We show through a mathematical model and experiments that TMG has better complexity compared to the commonly used join approach. In this article, we propose two algorithms, MB3-Miner and iMB3-Miner. MB3-Miner mines embedded subtrees. iMB3-Miner mines induced and/or embedded subtrees by using the maximum level of embedding constraint . Our experiments with both synthetic and real datasets against two well-known algorithms for mining induced and embedded subtrees, demonstrate the effectiveness and the efficiency of the proposed techniques.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2068395804",
    "type": "article"
  },
  {
    "title": "Tree model guided candidate generation for mining frequent subtrees from XML",
    "doi": null,
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Henry Tan; Fedja Hadzic; Tharam S. Dillon; Elizabeth Chang; Ling Feng; Lin Feng",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2992163282",
    "type": "article"
  },
  {
    "title": "VOGUE",
    "doi": "https://doi.org/10.1145/1644873.1644878",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Mohammed J. Zaki; Christopher D. Carothers; Bolesław K. Szymański",
    "corresponding_authors": "",
    "abstract": "We present VOGUE, a novel, variable order hidden Markov model with state durations, that combines two separate techniques for modeling complex patterns in sequential data: pattern mining and data modeling. VOGUE relies on a variable gap sequence mining method to extract frequent patterns with different lengths and gaps between elements. It then uses these mined sequences to build a variable order hidden Markov model (HMM), that explicitly models the gaps. The gaps implicitly model the order of the HMM, and they explicitly model the duration of each state. We apply VOGUE to a variety of real sequence data taken from domains such as protein sequence classification, Web usage logs, intrusion detection, and spelling correction. We show that VOGUE has superior classification accuracy compared to regular HMMs, higher-order HMMs, and even special purpose HMMs like HMMER, which is a state-of-the-art method for protein classification. The VOGUE implementation and the datasets used in this article are available as open-source. 1",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2002038995",
    "type": "article"
  },
  {
    "title": "Link spam target detection using page farms",
    "doi": "https://doi.org/10.1145/1552303.1552306",
    "publication_date": "2009-07-01",
    "publication_year": 2009,
    "authors": "Bin Zhou; Jian Pei",
    "corresponding_authors": "",
    "abstract": "Currently, most popular Web search engines adopt some link-based ranking methods such as PageRank. Driven by the huge potential benefit of improving rankings of Web pages, many tricks have been attempted to boost page rankings. The most common way, which is known as link spam, is to make up some artificially designed link structures. Detecting link spam effectively is a big challenge. In this article, we develop novel and effective detection methods for link spam target pages using page farms. The essential idea is intuitive: whether a page is the beneficiary of link spam is reflected by how it collects its PageRank score. Technically, how a target page collects its PageRank score is modeled by a page farm, which consists of pages contributing a major portion of the PageRank score of the target page. We propose two spamicity measures based on page farms. They can be used as an effective measure to check whether the pages are link spam target pages. An empirical study using a newly available real dataset strongly suggests that our method is effective. It outperforms the state-of-the-art methods like SpamRank and SpamMass in both precision and recall.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2020791430",
    "type": "article"
  },
  {
    "title": "SCOAL",
    "doi": "https://doi.org/10.1145/1839490.1839492",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Meghana Deodhar; Joydeep Ghosh",
    "corresponding_authors": "",
    "abstract": "For difficult classification or regression problems, practitioners often segment the data into relatively homogeneous groups and then build a predictive model for each group. This two-step procedure usually results in simpler, more interpretable and actionable models without any loss in accuracy. In this work, we consider problems such as predicting customer behavior across products, where the independent variables can be naturally partitioned into two sets, that is, the data is dyadic in nature. A pivoting operation now results in the dependent variable showing up as entries in a “customer by product” data matrix. We present the Simultaneous CO-clustering And Learning (SCOAL) framework, based on the key idea of interleaving co-clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models. This algorithm provably converges to a local minimum of a suitable cost function. The framework not only generalizes co-clustering and collaborative filtering to model-based co-clustering, but can also be viewed as simultaneous co-segmentation and classification or regression, which is typically better than independently clustering the data first and then building models. Moreover, it applies to a wide range of bi-modal or multimodal data, and can be easily specialized to address classification and regression problems. We demonstrate the effectiveness of our approach on both these problems through experimentation on a variety of datasets.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2083001254",
    "type": "article"
  },
  {
    "title": "User behavior learning and transfer in composite social networks",
    "doi": "https://doi.org/10.1145/2556613",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Erheng Zhong; Wei Fan; Qiang Yang",
    "corresponding_authors": "",
    "abstract": "Accurate prediction of user behaviors is important for many social media applications, including social marketing, personalization, and recommendation. A major challenge lies in that although many previous works model user behavior from only historical behavior logs, the available user behavior data or interactions between users and items in a given social network are usually very limited and sparse (e.g., ⩾ 99.9% empty), which makes models overfit the rare observations and fail to provide accurate predictions. We observe that many people are members of several social networks in the same time, such as Facebook, Twitter, and Tencent’s QQ. Importantly, users’ behaviors and interests in different networks influence one another. This provides an opportunity to leverage the knowledge of user behaviors in different networks by considering the overlapping users in different networks as bridges, in order to alleviate the data sparsity problem, and enhance the predictive performance of user behavior modeling. Combining different networks “simply and naively” does not work well. In this article, we formulate the problem to model multiple networks as “adaptive composite transfer” and propose a framework called ComSoc . ComSoc first selects the most suitable networks inside a composite social network via a hierarchical Bayesian model, parameterized for individual users. It then builds topic models for user behavior prediction using both the relationships in the selected networks and related behavior data. With different relational regularization, we introduce different implementations, corresponding to different ways to transfer knowledge from composite social relations. To handle big data, we have implemented the algorithm using Map/Reduce. We demonstrate that the proposed composite network-based user behavior models significantly improve the predictive accuracy over a number of existing approaches on several real-world applications, including a very large social networking dataset from Tencent Inc.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W1991332482",
    "type": "article"
  },
  {
    "title": "Connecting Two (or Less) Dots",
    "doi": "https://doi.org/10.1145/2086737.2086744",
    "publication_date": "2012-01-31",
    "publication_year": 2012,
    "authors": "Dafna Shahaf; Carlos Guestrin",
    "corresponding_authors": "",
    "abstract": "Finding information is becoming a major part of our daily life. Entire sectors, from Web users to scientists and intelligence analysts, are increasingly struggling to keep up with the larger and larger amounts of content published every day. With this much data, it is often easy to miss the big picture. In this article, we investigate methods for automatically connecting the dots---providing a structured, easy way to navigate within a new topic and discover hidden connections. We focus on the news domain: given two news articles, our system automatically finds a coherent chain linking them together. For example, it can recover the chain of events starting with the decline of home prices (January 2007), and ending with the health care debate (2009). We formalize the characteristics of a good chain and provide a fast search-driven algorithm to connect two fixed endpoints. We incorporate user feedback into our framework, allowing the stories to be refined and personalized. We also provide a method to handle partially-specified endpoints, for users who do not know both ends of a story. Finally, we evaluate our algorithm over real news data. Our user studies demonstrate that the objective we propose captures the users’ intuitive notion of coherence, and that our algorithm effectively helps users understand the news.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W1996894838",
    "type": "article"
  },
  {
    "title": "Efficient Discovery of the Most Interesting Associations",
    "doi": "https://doi.org/10.1145/2601433",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Geoffrey I. Webb; Jilles Vreeken",
    "corresponding_authors": "",
    "abstract": "Self-sufficient itemsets have been proposed as an effective approach to summarizing the key associations in data. However, their computation appears highly demanding, as assessing whether an itemset is self-sufficient requires consideration of all pairwise partitions of the itemset into pairs of subsets as well as consideration of all supersets. This article presents the first published algorithm for efficiently discovering self-sufficient itemsets. This branch-and-bound algorithm deploys two powerful pruning mechanisms based on upper bounds on itemset value and statistical significance level. It demonstrates that finding top- k productive and nonredundant itemsets, with postprocessing to identify those that are not independently productive, can efficiently identify small sets of key associations. We present extensive evaluation of the strengths and limitations of the technique, including comparisons with alternative approaches to finding the most interesting associations.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2123572748",
    "type": "article"
  },
  {
    "title": "Hierarchical Bayesian Inference and Recursive Regularization for Large-Scale Classification",
    "doi": "https://doi.org/10.1145/2629585",
    "publication_date": "2015-04-13",
    "publication_year": 2015,
    "authors": "Siddharth Gopal; Yiming Yang",
    "corresponding_authors": "",
    "abstract": "In this article, we address open challenges in large-scale classification, focusing on how to effectively leverage the dependency structures (hierarchical or graphical) among class labels, and how to make the inference scalable in jointly optimizing all model parameters. We propose two main approaches, namely the hierarchical Bayesian inference framework and the recursive regularization scheme. The key idea in both approaches is to reinforce the similarity among parameter across the nodes in a hierarchy or network based on the proximity and connectivity of the nodes. For scalability, we develop hierarchical variational inference algorithms and fast dual coordinate descent training procedures with parallelization. In our experiments for classification problems with hundreds of thousands of classes and millions of training instances with terabytes of parameters, the proposed methods show consistent and statistically significant improvements over other competing approaches, and the best results on multiple benchmark datasets for large-scale classification.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W1997530783",
    "type": "article"
  },
  {
    "title": "A Framework for Hierarchical Ensemble Clustering",
    "doi": "https://doi.org/10.1145/2611380",
    "publication_date": "2014-09-23",
    "publication_year": 2014,
    "authors": "Li Zheng; Tao Li; Chris Ding",
    "corresponding_authors": "",
    "abstract": "Ensemble clustering, as an important extension of the clustering problem, refers to the problem of combining different (input) clusterings of a given dataset to generate a final (consensus) clustering that is a better fit in some sense than existing clusterings. Over the past few years, many ensemble clustering approaches have been developed. However, most of them are designed for partitional clustering methods, and few research efforts have been reported for ensemble hierarchical clustering methods. In this article, a hierarchical ensemble clustering framework that can naturally combine both partitional clustering and hierarchical clustering results is proposed. In addition, a novel method for learning the ultra-metric distance from the aggregated distance matrices and generating final hierarchical clustering with enhanced cluster separation is developed based on the ultra-metric distance for hierarchical clustering. We study three important problems: dendrogram description, dendrogram combination, and dendrogram selection. We develop two approaches for dendrogram selection based on tree distances, and we investigate various dendrogram distances for representing dendrograms. We provide a systematic empirical study of the ensemble hierarchical clustering problem. Experimental results demonstrate the effectiveness of our proposed approaches.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2150969552",
    "type": "article"
  },
  {
    "title": "Social Influence Based Clustering and Optimization over Heterogeneous Information Networks",
    "doi": "https://doi.org/10.1145/2717314",
    "publication_date": "2015-07-22",
    "publication_year": 2015,
    "authors": "Yang Zhou; Ling Liu",
    "corresponding_authors": "",
    "abstract": "Social influence analysis has shown great potential for strategic marketing decision. It is well known that people influence one another based on both their social connections and the social activities that they have engaged in the past. In this article, we develop an innovative and high-performance social influence based graph clustering framework with four unique features. First, we explicitly distinguish social connection based influence (self-influence) and social activity based influence (co-influence). We compute the self-influence similarity between two members based on their social connections within a single collaboration network, and compute the co-influence similarity by taking into account not only the set of activities that people participate but also the semantic association between these activities. Second, we define the concept of influence-based similarity by introducing a unified influence-based similarity matrix that employs an iterative weight update method to integrate self-influence and co-influence similarities. Third, we design a dynamic learning algorithm, called SI-C luster , for social influence based graph clustering. It iteratively partitions a large social collaboration network into K clusters based on both the social network itself and the multiple associated activity information networks, each representing a category of activities that people have engaged. To make the SI-C luster algorithm converge fast, we transform sophisticated nonlinear fractional programming problem with respect to multiple weights into a straightforward nonlinear parametric programming problem of single variable. Finally, we develop an optimization technique of diagonalizable-matrix approximation to speed up the computation of self-influence similarity and co-influence similarities. Our SI-Cluster-Opt significantly improves the efficiency of SI-Cluster on large graphs while maintaining high quality of clustering results. Extensive experimental evaluation on three real-world graphs shows that, compared to existing representative graph clustering algorithms, our SI-C luster -O pt approach not only achieves a very good balance between self-influence and co-influence similarities but also scales extremely well for clustering large graphs in terms of time complexity while meeting the guarantee of high density, low entropy and low Davies--Bouldin Index.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2254361149",
    "type": "article"
  },
  {
    "title": "Real-Time Large-Scale Map Matching Using Mobile Phone Data",
    "doi": "https://doi.org/10.1145/3046945",
    "publication_date": "2017-07-14",
    "publication_year": 2017,
    "authors": "Essam Algizawy; Tetsuji Ogawa; Ahmed El-Mahdy",
    "corresponding_authors": "",
    "abstract": "With the wide spread use of mobile phones, cellular mobile big data is becoming an important resource that provides a wealth of information with almost no cost. However, the data generally suffers from relatively high spatial granularity, limiting the scope of its application. In this article, we consider, for the first time, the utility of actual mobile big data for map matching allowing for “microscopic” level traffic analysis. The state-of-the-art in map matching generally targets GPS data, which provides far denser sampling and higher location resolution than the mobile data. Our approach extends the typical Hidden-Markov model used in map matching to accommodate for highly sparse location trajectories, exploit the large mobile data volume to learn the model parameters, and exploit the sparsity of the data to provide for real-time Viterbi processing. We study an actual, anonymised mobile trajectories data set of the city of Dakar, Senegal, spanning a year, and generate a corresponding road-level traffic density, at an hourly granularity, for each mobile trajectory. We observed a relatively high correlation between the generated traffic intensities and corresponding values obtained by the gravity and equilibrium models typically used in mobility analysis, indicating the utility of the approach as an alternative means for traffic analysis.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2735039195",
    "type": "article"
  },
  {
    "title": "Memory-Efficient and Accurate Sampling for Counting Local Triangles in Graph Streams",
    "doi": "https://doi.org/10.1145/3022186",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Yongsub Lim; Minsoo Jung; U Kang",
    "corresponding_authors": "",
    "abstract": "How can we estimate local triangle counts accurately in a graph stream without storing the whole graph? How to handle duplicated edges in local triangle counting for graph stream? Local triangle counting, which computes the number of triangles attached to each node in a graph, is a very important problem with wide applications in social network analysis, anomaly detection, web mining, and the like. In this article, we propose algorithms for local triangle counting in a graph stream based on edge sampling: M ascot for a simple graph, and M ulti BM ascot and M ulti WM ascot for a multigraph. To develop M ascot , we first present two naive local triangle counting algorithms in a graph stream, called M ascot -C and M ascot -A. M ascot -C is based on constant edge sampling, and M ascot -A improves its accuracy by utilizing more memory spaces. M ascot achieves both accuracy and memory-efficiency of the two algorithms by unconditional triangle counting for a new edge, regardless of whether it is sampled or not. Extending the idea to a multigraph, we develop two algorithms M ulti BM ascot and M ulti WM ascot . M ulti BM ascot enables local triangle counting on the corresponding simple graph of a streamed multigraph without explicit graph conversion; M ulti WM ascot considers repeated occurrences of an edge as its weight and counts each triangle as the product of its three edge weights. In contrast to the existing algorithm that requires prior knowledge on the target graph and appropriately set parameters, our proposed algorithms require only one parameter of edge sampling probability. Through extensive experiments, we show that for the same number of edges sampled, M ascot provides the best accuracy compared to the existing algorithm as well as M ascot -C and M ascot -A. We also demonstrate that M ulti BM ascot on a multigraph is comparable to M ascot -C on the counterpart simple graph, and M ulti WM ascot becomes more accurate for higher degree nodes. Thanks to M ascot , we also discover interesting anomalous patterns in real graphs, including core-peripheries in the web, a bimodal call pattern in a phone call history, and intensive collaboration in DBLP.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2786220088",
    "type": "article"
  },
  {
    "title": "Stability and Robustness in Influence Maximization",
    "doi": "https://doi.org/10.1145/3233227",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Xinran He; David Kempe",
    "corresponding_authors": "",
    "abstract": "In the well-studied Influence Maximization problem, the goal is to identify a set of k nodes in a social network whose joint influence on the network is maximized. A large body of recent work has justified research on Influence Maximization models and algorithms with their potential to create societal or economic value. However, in order to live up to this potential, the algorithms must be robust to large amounts of noise, for they require quantitative estimates of the influence, which individuals exert on each other; ground truth for such quantities is inaccessible, and even decent estimates are very difficult to obtain. We begin to address this concern formally. First, we exhibit simple inputs on which even very small estimation errors may mislead every algorithm into highly suboptimal solutions. Motivated by this observation, we propose the Perturbation Interval model as a framework to characterize the stability of Influence Maximization against noise in the inferred diffusion network. Analyzing the susceptibility of specific instances to estimation errors leads to a clean algorithmic question, which we term the Influence Difference Maximization problem. However, the objective function of Influence Difference Maximization is NP-hard to approximate within a factor of O ( n 1−ϵ) for any ϵ &gt; 0. Given the infeasibility of diagnosing instability algorithmically, we focus on finding influential users robustly across multiple diffusion settings. We define a Robust Influence Maximization framework wherein an algorithm is presented with a set of influence functions. The algorithm’s goal is to identify a set of k nodes who are simultaneously influential for all influence functions, compared to the (function-specific) optimum solutions. We show strong approximation hardness results for this problem unless the algorithm gets to select at least a logarithmic factor more seeds than the optimum solution. However, when enough extra seeds may be selected, we show that techniques of Krause et al. can be used to approximate the optimum robust influence to within a factor of 1−1/ e . We evaluate this bicriteria approximation algorithm against natural heuristics on several real-world datasets. Our experiments indicate that the worst-case hardness does not necessarily translate into bad performance on real-world datasets; all algorithms perform fairly well.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2889208719",
    "type": "article"
  },
  {
    "title": "Graph Manipulations for Fast Centrality Computation",
    "doi": "https://doi.org/10.1145/3022668",
    "publication_date": "2017-03-10",
    "publication_year": 2017,
    "authors": "Ahmet Erdem Sarıyüce; Kamer Kaya; Érik Saule; Ümit V. Çatalyürek",
    "corresponding_authors": "",
    "abstract": "The betweenness and closeness metrics are widely used metrics in many network analysis applications. Yet, they are expensive to compute. For that reason, making the betweenness and closeness centrality computations faster is an important and well-studied problem. In this work, we propose the framework BADIOS that manipulates the graph by compressing it and splitting into pieces so that the centrality computation can be handled independently for each piece. Experimental results show that the proposed techniques can be a great arsenal to reduce the centrality computation time for various types and sizes of networks. In particular, it reduces the betweenness centrality computation time of a 4.6 million edges graph from more than 5 days to less than 16 hours. For the same graph, the closeness computation time is decreased from more than 3 days to 6 hours (12.7x speedup).",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W87276267",
    "type": "article"
  },
  {
    "title": "Classification with Streaming Features: An Emerging-Pattern Mining Approach",
    "doi": "https://doi.org/10.1145/2700409",
    "publication_date": "2015-06-01",
    "publication_year": 2015,
    "authors": "Kui Yu; Wei Ding; Dan A. Simovici; Hao Wang; Jian Pei; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Many datasets from real-world applications have very high-dimensional or increasing feature space. It is a new research problem to learn and maintain a classifier to deal with very high dimensionality or streaming features. In this article, we adapt the well-known emerging-pattern--based classification models and propose a semi-streaming approach. For streaming features, it is computationally expensive or even prohibitive to mine long-emerging patterns, and it is nontrivial to integrate emerging-pattern mining with feature selection. We present an online feature selection step, which is capable of selecting and maintaining a pool of effective features from a feature stream. Then, in our offline step, separated from the online step, we periodically compute and update emerging patterns from the pool of selected features from the online step. We evaluate the effectiveness and efficiency of the proposed method using a series of benchmark datasets and a real-world case study on Mars crater detection. Our proposed method yields classification performance comparable to the state-of-art static classification methods. Most important, the proposed method is significantly faster and can efficiently handle datasets with streaming features.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2214080290",
    "type": "article"
  },
  {
    "title": "Detecting and Assessing Anomalous Evolutionary Behaviors of Nodes in Evolving Social Networks",
    "doi": "https://doi.org/10.1145/3299886",
    "publication_date": "2019-01-23",
    "publication_year": 2019,
    "authors": "Huan Wang; Jia Wu; Wenbin Hu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Based on the performance of entire social networks, anomaly analysis for evolving social networks generally ignores the otherness of the evolutionary behaviors of different nodes, such that it is difficult to precisely identify the anomalous evolutionary behaviors of nodes ( AEBN ). Assuming that a node's evolutionary behavior that generates and removes edges normally follows stable evolutionary mechanisms, this study focuses on detecting and assessing AEBN, whose evolutionary mechanisms deviate from their past mechanisms, and proposes a link prediction detection ( LPD ) method and a matrix perturbation assessment ( MPA ) method. LPD describes a node's evolutionary behavior by fitting its evolutionary mechanism, and designs indexes for edge generation and removal to evaluate the extent to which the evolutionary mechanism of a node's evolutionary behavior can be fitted by a link prediction algorithm. Furthermore, it detects AEBN by quantifying the differences among behavior vectors that characterize the node's evolutionary behaviors in different periods. In addition, MPA considers AEBN as a perturbation of the social network structure, and quantifies the effect of AEBN on the social network structure based on matrix perturbation analysis. Extensive experiments on eight disparate real-world networks demonstrate that analyzing AEBN from the perspective of evolutionary mechanisms is important and beneficial.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2912974153",
    "type": "article"
  },
  {
    "title": "Use of Local Group Information to Identify Communities in Networks",
    "doi": "https://doi.org/10.1145/2700404",
    "publication_date": "2015-04-01",
    "publication_year": 2015,
    "authors": "Sucheta Soundarajan; John E. Hopcroft",
    "corresponding_authors": "",
    "abstract": "The recent interest in networks has inspired a broad range of work on algorithms and techniques to characterize, identify, and extract communities from networks. Such efforts are complicated by a lack of consensus on what a “community” truly is, and these disagreements have led to a wide variety of mathematical formulations for describing communities. Often, these mathematical formulations, such as modularity and conductance, have been founded in the general principle that communities, like a G ( n , p ) graph, are “round,” with connections throughout the entire community, and so algorithms were developed to optimize such mathematical measures. More recently, a variety of algorithms have been developed that, rather than expecting connectivity through the entire community, seek out very small groups of well-connected nodes and then connect these groups into larger communities. In this article, we examine seven real networks, each containing external annotation that allows us to identify “annotated communities.” A study of these annotated communities gives insight into why the second category of community detection algorithms may be more successful than the first category. We then present a flexible algorithm template that is based on the idea of joining together small sets of nodes. In this template, we first identify very small, tightly connected “subcommunities” of nodes, each corresponding to a single node’s “perception” of the network around it. We then create a new network in which each node represents such a subcommunity, and then identify communities in this new network. Because each node can appear in multiple subcommunities, this method allows us to detect overlapping communities. When evaluated on real data, we show that our template outperforms many other state-of-the-art algorithms.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2041681655",
    "type": "article"
  },
  {
    "title": "Finding Dynamic Dense Subgraphs",
    "doi": "https://doi.org/10.1145/3046791",
    "publication_date": "2017-03-10",
    "publication_year": 2017,
    "authors": "Polina Rozenshtein; Nikolaj Tatti; Aristides Gionis",
    "corresponding_authors": "",
    "abstract": "Online social networks are often defined by considering interactions of entities at an aggregate level. For example, a call graph is formed among individuals who have called each other at least once; or at least k times. Similarly, in social-media platforms, we consider implicit social networks among users who have interacted in some way, e.g., have made a conversation, have commented to the content of each other, and so on. Such definitions have been used widely in the literature and they have offered significant insights regarding the structure of social networks. However, it is obvious that they suffer from a severe limitation: They neglect the precise time that interactions among the network entities occur. In this article, we consider interaction networks , where the data description contains not only information about the underlying topology of the social network, but also the exact time instances that network entities interact. In an interaction network, an edge is associated with a timestamp, and multiple edges may occur for the same pair of entities. Consequently, interaction networks offer a more fine-grained representation, which can be leveraged to reveal otherwise hidden dynamic phenomena. In the setting of interaction networks, we study the problem of discovering dynamic dense subgraphs whose edges occur in short time intervals . We view such subgraphs as fingerprints of dynamic activity occurring within network communities. Such communities represent groups of individuals who interact with each other in specific time instances, for example, a group of employees who work on a project and whose interaction intensifies before certain project milestones. We prove that the problem we define is NP -hard, and we provide efficient algorithms by adapting techniques for finding dense subgraphs. We also show how to speed-up the proposed methods by exploiting concavity properties of our objective function and by the means of fractional programming . We perform extensive evaluation of the proposed methods on synthetic and real datasets, which demonstrates the validity of our approach and shows that our algorithms can be used to obtain high-quality results.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2596774062",
    "type": "article"
  },
  {
    "title": "Query-Driven Learning for Predictive Analytics of Data Subspace Cardinality",
    "doi": "https://doi.org/10.1145/3059177",
    "publication_date": "2017-06-29",
    "publication_year": 2017,
    "authors": "Christos Anagnostopoulos; Peter Triantafillou",
    "corresponding_authors": "",
    "abstract": "Fundamental to many predictive analytics tasks is the ability to estimate the cardinality (number of data items) of multi-dimensional data subspaces, defined by query selections over datasets. This is crucial for data analysts dealing with, e.g., interactive data subspace explorations, data subspace visualizations, and in query processing optimization. However, in many modern data systems, predictive analytics may be (i) too costly money-wise, e.g., in clouds, (ii) unreliable, e.g., in modern Big Data query engines, where accurate statistics are difficult to obtain/maintain, or (iii) infeasible, e.g., for privacy issues. We contribute a novel, query-driven, function estimation model of analyst-defined data subspace cardinality. The proposed estimation model is highly accurate in terms of prediction and accommodating the well-known selection queries: multi-dimensional range and distance-nearest neighbors (radius) queries. Our function estimation model: (i) quantizes the vectorial query space, by learning the analysts’ access patterns over a data space, (ii) associates query vectors with their corresponding cardinalities of the analyst-defined data subspaces, (iii) abstracts and employs query vectorial similarity to predict the cardinality of an unseen/unexplored data subspace, and (iv) identifies and adapts to possible changes of the query subspaces based on the theory of optimal stopping. The proposed model is decentralized, facilitating the scaling-out of such predictive analytics queries. The research significance of the model lies in that (i) it is an attractive solution when data-driven statistical techniques are undesirable or infeasible, (ii) it offers a scale-out, decentralized training solution, (iii) it is applicable to different selection query types, and (iv) it offers a performance that is superior to that of data-driven approaches.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2598177319",
    "type": "article"
  },
  {
    "title": "Community Detection Using Diffusion Information",
    "doi": "https://doi.org/10.1145/3110215",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Maryam Ramezani; Ali Khodadadi; Hamid R. Rabiee",
    "corresponding_authors": "",
    "abstract": "Community detection in social networks has become a popular topic of research during the last decade. There exist a variety of algorithms for modularizing the network graph into different communities. However, they mostly assume that partial or complete information of the network graphs are available that is not feasible in many cases. In this article, we focus on detecting communities by exploiting their diffusion information. To this end, we utilize the Conditional Random Fields (CRF) to discover the community structures. The proposed method, community diffusion (CoDi), does not require any prior knowledge about the network structure or specific properties of communities. Furthermore, in contrast to the structure-based community detection methods, this method is able to identify the hidden communities. The experimental results indicate considerable improvements in detecting communities based on accuracy, scalability, and real cascade information measures.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2785078427",
    "type": "article"
  },
  {
    "title": "Krylov Subspace Approximation for Local Community Detection in Large Networks",
    "doi": "https://doi.org/10.1145/3340708",
    "publication_date": "2019-09-24",
    "publication_year": 2019,
    "authors": "Kun He; Shi Pan; David Bindel; John E. Hopcroft",
    "corresponding_authors": "",
    "abstract": "Community detection is an important information mining task to uncover modular structures in large networks. For increasingly common large network datasets, global community detection is prohibitively expensive, and attention has shifted to methods that mine local communities, i.e., identifying all latent members of a particular community from a few labeled seed members. To address such semi-supervised mining task, we systematically develop a local spectral (LOSP) subspace-based community detection method, called LOSP. We define a family of LOSP subspaces based on Krylov subspaces, and seek a sparse indicator for the target community via an ℓ 1 norm minimization over the Krylov subspace. Variants of LOSP depend on type of random walks with different diffusion speeds, type of random walks, dimension of the LOSP subspace, and step of diffusions. The effectiveness of the proposed LOSP approach is theoretically analyzed based on Rayleigh quotients, and it is experimentally verified on a wide variety of real-world networks across social, production, and biological domains, as well as on an extensive set of synthetic LFR benchmark datasets.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2976688833",
    "type": "article"
  },
  {
    "title": "Cluster’s Quality Evaluation and Selective Clustering Ensemble",
    "doi": "https://doi.org/10.1145/3211872",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Feijiang Li; Yuhua Qian; Jieting Wang; Chuangyin Dang; Bing Liu",
    "corresponding_authors": "",
    "abstract": "Clustering ensemble has drawn much attention in recent years due to its ability to generate a high quality and robust partition result. Weighted clustering ensemble and selective clustering ensemble are two general ways to further improve the performance of a clustering ensemble method. Existing weighted clustering ensemble methods assign the same weight to each cluster in a partition of the ensemble. Since the qualities of the clusters in a partition are different, the clusters should be weighted differently. To address this issue, this article proposes a new measure to calculate the similarity between a cluster and a partition. Theoretically, this measure is effective in handling two problems in measuring the quality of a cluster, which are defined as the symmetric problem and the context meaning problem. In addition, some properties of the proposed measure are analyzed. This measure can be easily expanded to a clustering performance measure that calculates the similarity between two partitions. As a result of this measure, we propose a novel selective clustering ensemble framework, which considers the differences between the objective of the ensemble selection stage and the object of the ensemble integration stage in the selective clustering ensemble. To verify the performance of the new measure, we compare the performance of the measure with the two existing measures in weighting clusters. The experiments show that the proposed measure is more effective. To verify the performance of the novel framework, four existing state-of-the-art selective clustering ensemble frameworks are employed as references. The experiments show that the proposed framework is statistically better than the others on 17 UCI benchmark datasets, 8 document datasets, and the Olivetti Face Database.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2811210411",
    "type": "article"
  },
  {
    "title": "Chameleon 2",
    "doi": "https://doi.org/10.1145/3299876",
    "publication_date": "2019-01-29",
    "publication_year": 2019,
    "authors": "Tomáš Bartoň; Tomas Bruna; Pavel Kordík",
    "corresponding_authors": "",
    "abstract": "Traditional clustering algorithms fail to produce human-like results when confronted with data of variable density, complex distributions, or in the presence of noise. We propose an improved graph-based clustering algorithm called Chameleon 2, which overcomes several drawbacks of state-of-the-art clustering approaches. We modified the internal cluster quality measure and added an extra step to ensure algorithm robustness. Our results reveal a significant positive impact on the clustering quality measured by Normalized Mutual Information on 32 artificial datasets used in the clustering literature. This significant improvement is also confirmed on real-world datasets. The performance of clustering algorithms such as DBSCAN is extremely parameter sensitive, and exhaustive manual parameter tuning is necessary to obtain a meaningful result. All hierarchical clustering methods are very sensitive to cutoff selection, and a human expert is often required to find the true cutoff for each clustering result. We present an automated cutoff selection method that enables the Chameleon 2 algorithm to generate high-quality clustering in autonomous mode.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2912567961",
    "type": "article"
  },
  {
    "title": "A Pipeline Computing Method of SpTV for Three-Order Tensors on CPU and GPU",
    "doi": "https://doi.org/10.1145/3363575",
    "publication_date": "2019-11-11",
    "publication_year": 2019,
    "authors": "Wangdong Yang; Kenli Li; Keqin Li",
    "corresponding_authors": "",
    "abstract": "Tensors have drawn a growing attention in many applications, such as physics, engineering science, social networks, recommended systems. Tensor decomposition is the key to explore the inherent intrinsic data relationship of tensor. There are many sparse tensor and vector multiplications (SpTV) in tensor decomposition. We analyze a variety of storage formats of sparse tensors and develop a piecewise compression strategy to improve the storage efficiency of large sparse tensors. This compression strategy can avoid storing a large number of empty slices and empty fibers in sparse tensors, and thus the storage space is significantly reduced. A parallel algorithm for the SpTV based on the high-order compressed format based on slices is designed to greatly improve its computing performance on graphics processing unit. Each tensor is cut into multiple slices to form a series of sparse matrix and vector multiplications, which form the pipelined parallelism. The transmission time of the slices can be hidden through pipelined parallel to further optimize the performance of the SpTV.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W3003516456",
    "type": "article"
  },
  {
    "title": "Internal Evaluation of Unsupervised Outlier Detection",
    "doi": "https://doi.org/10.1145/3394053",
    "publication_date": "2020-06-26",
    "publication_year": 2020,
    "authors": "Henrique O. Marques; Ricardo J. G. B. Campello; Jörg Sander; Arthur Zimek",
    "corresponding_authors": "",
    "abstract": "Although there is a large and growing literature that tackles the unsupervised outlier detection problem, the unsupervised evaluation of outlier detection results is still virtually untouched in the literature. The so-called internal evaluation, based solely on the data and the assessed solutions themselves, is required if one wants to statistically validate (in absolute terms) or just compare (in relative terms) the solutions provided by different algorithms or by different parameterizations of a given algorithm in the absence of labeled data. However, in contrast to unsupervised cluster analysis, where indexes for internal evaluation and validation of clustering solutions have been conceived and shown to be very useful, in the outlier detection domain, this problem has been notably overlooked. Here we discuss this problem and provide a solution for the internal evaluation of outlier detection results. Specifically, we describe an index called Internal, Relative Evaluation of Outlier Solutions (IREOS) that can evaluate and compare different candidate outlier detection solutions. Initially, the index is designed to evaluate binary solutions only, referred to as top - n outlier detection results. We then extend IREOS to the general case of non-binary solutions, consisting of outlier detection scorings. We also statistically adjust IREOS for chance and extensively evaluate it in several experiments involving different collections of synthetic and real datasets.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W3039137796",
    "type": "article"
  },
  {
    "title": "Fast, Accurate and Provable Triangle Counting in Fully Dynamic Graph Streams",
    "doi": "https://doi.org/10.1145/3375392",
    "publication_date": "2020-02-09",
    "publication_year": 2020,
    "authors": "Kijung Shin; Sejoon Oh; Jisu Kim; Bryan Hooi; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "Given a stream of edge additions and deletions, how can we estimate the count of triangles in it? If we can store only a subset of the edges, how can we obtain unbiased estimates with small variances? Counting triangles (i.e., cliques of size three) in a graph is a classical problem with applications in a wide range of research areas, including social network analysis, data mining, and databases. Recently, streaming algorithms for triangle counting have been extensively studied since they can naturally be used for large dynamic graphs. However, existing algorithms cannot handle edge deletions or suffer from low accuracy. Can we handle edge deletions while achieving high accuracy? We propose T hink D, which accurately estimates the counts of global triangles (i.e., all triangles) and local triangles associated with each node in a fully dynamic graph stream with additions and deletions of edges. Compared to its best competitors, T hink D is (a) Accurate: up to 4.3 × more accurate within the same memory budget, (b) Fast: up to 2.2 × faster for the same accuracy requirements, and (c) Theoretically sound: always maintaining estimates with zero bias (i.e., the difference between the true triangle count and the expected value of its estimate) and small variance. As an application, we use T hink D to detect suddenly emerging dense subgraphs, and we show its advantages over state-of-the-art methods.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3006060129",
    "type": "article"
  },
  {
    "title": "Probabilistic Topic Modeling for Comparative Analysis of Document Collections",
    "doi": "https://doi.org/10.1145/3369873",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Ting Hua; Chang‐Tien Lu; Jaegul Choo; Chandan K. Reddy",
    "corresponding_authors": "",
    "abstract": "Probabilistic topic models, which can discover hidden patterns in documents, have been extensively studied. However, rather than learning from a single document collection, numerous real-world applications demand a comprehensive understanding of the relationships among various document sets. To address such needs, this article proposes a new model that can identify the common and discriminative aspects of multiple datasets. Specifically, our proposed method is a Bayesian approach that represents each document as a combination of common topics (shared across all document sets) and distinctive topics (distributions over words that are exclusive to a particular dataset). Through extensive experiments, we demonstrate the effectiveness of our method compared with state-of-the-art models. The proposed model can be useful for “comparative thinking” analysis in real-world document collections.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3009379225",
    "type": "article"
  },
  {
    "title": "Pop Music Generation",
    "doi": "https://doi.org/10.1145/3374915",
    "publication_date": "2020-07-06",
    "publication_year": 2020,
    "authors": "Hongyuan Zhu; Qi Liu; Nicholas Jing Yuan; Kun Zhang; Guang Zhou; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "Music plays an important role in our daily life. With the development of deep learning and modern generation techniques, researchers have done plenty of works on automatic music generation. However, due to the special requirements of both melody and arrangement, most of these methods have limitations when applying to multi-track music generation. Some critical factors related to the quality of music are not well addressed, such as chord progression, rhythm pattern, and musical style. In order to tackle the problems and ensure the harmony of multi-track music, in this article, we propose an end-to-end melody and arrangement generation framework to generate a melody track with several accompany tracks played by some different instruments. To be specific, we first develop a novel Chord based Rhythm and Melody Cross-Generation Model to generate melody with a chord progression. Then, we propose a Multi-Instrument Co-Arrangement Model based on multi-task learning for multi-track music arrangement. Furthermore, to control the musical style of arrangement, we design a Multi-Style Multi-Instrument Co-Arrangement Model to learn the musical style with adversarial training. Therefore, we can not only maintain the harmony of the generated music but also control the musical style for better utilization. Extensive experiments on a real-world dataset demonstrate the superiority and effectiveness of our proposed models.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3040218036",
    "type": "article"
  },
  {
    "title": "Efficient Mining of Outlying Sequence Patterns for Analyzing Outlierness of Sequence Data",
    "doi": "https://doi.org/10.1145/3399671",
    "publication_date": "2020-08-05",
    "publication_year": 2020,
    "authors": "Tingting Wang; Lei Duan; Guozhu Dong; Zhifeng Bao",
    "corresponding_authors": "",
    "abstract": "Recently, a lot of research work has been proposed in different domains to detect outliers and analyze the outlierness of outliers for relational data. However, while sequence data is ubiquitous in real life, analyzing the outlierness for sequence data has not received enough attention. In this article, we study the problem of mining outlying sequence patterns in sequence data addressing the question: given a query sequence s in a sequence dataset D , the objective is to discover sequence patterns that will indicate the most unusualness (i.e., outlierness) of s compared against other sequences. Technically, we use the rank defined by the average probabilistic strength ( aps ) of a sequence pattern in a sequence to measure the outlierness of the sequence. Then a minimal sequence pattern where the query sequence is ranked the highest is defined as an outlying sequence pattern. To address the above problem, we present OSPMiner, a heuristic method that computes aps by incorporating several pruning techniques. Our empirical study using both real and synthetic data demonstrates that OSPMiner is effective and efficient.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3046961612",
    "type": "article"
  },
  {
    "title": "Multi-objective Cuckoo Search-based Streaming Feature Selection for Multi-label Dataset",
    "doi": "https://doi.org/10.1145/3447586",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Dipanjyoti Paul; Rahul Kumar; Sriparna Saha; Jimson Mathew",
    "corresponding_authors": "",
    "abstract": "The feature selection method is the process of selecting only relevant features by removing irrelevant or redundant features amongst the large number of features that are used to represent data. Nowadays, many application domains especially social media networks, generate new features continuously at different time stamps. In such a scenario, when the features are arriving in an online fashion, to cope up with the continuous arrival of features, the selection task must also have to be a continuous process. Therefore, the streaming feature selection based approach has to be incorporated, i.e., every time a new feature or a group of features arrives, the feature selection process has to be invoked. Again, in recent years, there are many application domains that generate data where samples may belong to more than one classes called multi-label dataset. The multiple labels that the instances are being associated with, may have some dependencies amongst themselves. Finding the co-relation amongst the class labels helps to select the discriminative features across multiple labels. In this article, we develop streaming feature selection methods for multi-label data where the multiple labels are reduced to a lower-dimensional space. The similar labels are grouped together before performing the selection method to improve the selection quality and to make the model time efficient. The multi-objective version of the cuckoo search-based approach is used to select the optimal feature set. The proposed method develops two versions of the streaming feature selection method: ) when the features arrive individually and ) when the features arrive in the form of a batch. Various multi-label datasets from various domains such as text, biology, and audio have been used to test the developed streaming feature selection methods. The proposed methods are compared with many previous feature selection methods and from the comparison, the superiority of using multiple objectives and label co-relation in the feature selection process can be established.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W3161516854",
    "type": "article"
  },
  {
    "title": "An Instance Space Analysis of Regression Problems",
    "doi": "https://doi.org/10.1145/3436893",
    "publication_date": "2021-03-27",
    "publication_year": 2021,
    "authors": "Mario Andrés Muñoz; Tao Yan; Matheus Rodrigues Leal; Kate Smith‐Miles; Ana Carolina Lorena; Gisele L. Pappa; Rômulo Madureira Rodrigues",
    "corresponding_authors": "",
    "abstract": "The quest for greater insights into algorithm strengths and weaknesses, as revealed when studying algorithm performance on large collections of test problems, is supported by interactive visual analytics tools. A recent advance is Instance Space Analysis, which presents a visualization of the space occupied by the test datasets, and the performance of algorithms across the instance space. The strengths and weaknesses of algorithms can be visually assessed, and the adequacy of the test datasets can be scrutinized through visual analytics. This article presents the first Instance Space Analysis of regression problems in Machine Learning, considering the performance of 14 popular algorithms on 4,855 test datasets from a variety of sources. The two-dimensional instance space is defined by measurable characteristics of regression problems, selected from over 26 candidate features. It enables the similarities and differences between test instances to be visualized, along with the predictive performance of regression algorithms across the entire instance space. The purpose of creating this framework for visual analysis of an instance space is twofold: one may assess the capability and suitability of various regression techniques; meanwhile the bias, diversity, and level of difficulty of the regression problems popularly used by the community can be visually revealed. This article shows the applicability of the created regression instance space to provide insights into the strengths and weaknesses of regression algorithms, and the opportunities to diversify the benchmark test instances to support greater insights.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3146229669",
    "type": "article"
  },
  {
    "title": "Trust Prediction for Online Social Networks with Integrated Time-Aware Similarity",
    "doi": "https://doi.org/10.1145/3447682",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Xiaofeng Gao; Wenyi Xu; Mingding Liao; Guihai Chen",
    "corresponding_authors": "",
    "abstract": "Online social networks gain increasing popularity in recent years. In online social networks, trust prediction is significant for recommendations of high reputation users as well as in many other applications. In the literature, trust prediction problem can be solved by several strategies, such as matrix factorization, trust propagation, and -NN search. However, most of the existing works have not considered the possible complementarity among these mainstream strategies to optimize their effectiveness and efficiency. In this article, we propose a novel trust prediction approach named iSim : an integrated time-aware similarity-based collaborative filtering approach leveraging on user similarity, which integrates three kinds of factors to measure user similarity, including vector space similarity, time-aware matrix factorization, and propagated trust. This article is the first work in the literature employing time-aware matrix factorization and propagated trust in the study of similarity. Additionally, we use several methods like adding inverted index to reduce the time complexity of iSim , and provide its theoretical time bound. Moreover, we also provide the detailed overview and theoretical analysis of the existing works. Finally, the extensive experiments with real-world datasets show that iSim achieves great improvement for both efficiency and effectiveness over the state-of-the-art approaches.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3163666463",
    "type": "article"
  },
  {
    "title": "BiLabel-Specific Features for Multi-Label Classification",
    "doi": "https://doi.org/10.1145/3458283",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Min-Ling Zhang; Jun-Peng Fang; Yibo Wang",
    "corresponding_authors": "",
    "abstract": "In multi-label classification, the task is to induce predictive models which can assign a set of relevant labels for the unseen instance. The strategy of label-specific features has been widely employed in learning from multi-label examples, where the classification model for predicting the relevancy of each class label is induced based on its tailored features rather than the original features. Existing approaches work by generating a group of tailored features for each class label independently, where label correlations are not fully considered in the label-specific features generation process. In this article, we extend existing strategy by proposing a simple yet effective approach based on BiLabel-specific features. Specifically, a group of tailored features is generated for a pair of class labels with heuristic prototype selection and embedding. Thereafter, predictions of classifiers induced by BiLabel-specific features are ensembled to determine the relevancy of each class label for unseen instance. To thoroughly evaluate the BiLabel-specific features strategy, extensive experiments are conducted over a total of 35 benchmark datasets. Comparative studies against state-of-the-art label-specific features techniques clearly validate the superiority of utilizing BiLabel-specific features to yield stronger generalization performance for multi-label classification.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3183680505",
    "type": "article"
  },
  {
    "title": "Community Detection in Partially Observable Social Networks",
    "doi": "https://doi.org/10.1145/3461339",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Cong Tran; Won-Yong Shin; Andreas Spitz",
    "corresponding_authors": "",
    "abstract": "The discovery of community structures in social networks has gained significant attention since it is a fundamental problem in understanding the networks’ topology and functions. However, most social network data are collected from partially observable networks with both missing nodes and edges . In this article, we address a new problem of detecting overlapping community structures in the context of such an incomplete network, where communities in the network are allowed to overlap since nodes belong to multiple communities at once. To solve this problem, we introduce KroMFac , a new framework that conducts community detection via regularized nonnegative matrix factorization (NMF) based on the Kronecker graph model. Specifically, from an inferred Kronecker generative parameter matrix, we first estimate the missing part of the network. As our major contribution to the proposed framework, to improve community detection accuracy, we then characterize and select influential nodes (which tend to have high degrees) by ranking, and add them to the existing graph. Finally, we uncover the community structures by solving the regularized NMF-aided optimization problem in terms of maximizing the likelihood of the underlying graph. Furthermore, adopting normalized mutual information (NMI), we empirically show superiority of our KroMFac approach over two baseline schemes by using both synthetic and real-world networks.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3185484565",
    "type": "article"
  },
  {
    "title": "Scalable Mining of High-Utility Sequential Patterns With Three-Tier MapReduce Model",
    "doi": "https://doi.org/10.1145/3487046",
    "publication_date": "2021-11-15",
    "publication_year": 2021,
    "authors": "Jerry Chun‐Wei Lin; Youcef Djenouri; Gautam Srivastava; Yuanfa Li; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "High-utility sequential pattern mining (HUSPM) is a hot research topic in recent decades since it combines both sequential and utility properties to reveal more information and knowledge rather than the traditional frequent itemset mining or sequential pattern mining. Several works of HUSPM have been presented but most of them are based on main memory to speed up mining performance. However, this assumption is not realistic and not suitable in large-scale environments since in real industry, the size of the collected data is very huge and it is impossible to fit the data into the main memory of a single machine. In this article, we first develop a parallel and distributed three-stage MapReduce model for mining high-utility sequential patterns based on large-scale databases. Two properties are then developed to hold the correctness and completeness of the discovered patterns in the developed framework. In addition, two data structures called sidset and utility-linked list are utilized in the developed framework to accelerate the computation for mining the required patterns. From the results, we can observe that the designed model has good performance in large-scale datasets in terms of runtime, memory, efficiency of the number of distributed nodes, and scalability compared to the serial HUSP-Span approach.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3212326024",
    "type": "article"
  },
  {
    "title": "Mobile App Cross-Domain Recommendation with Multi-Graph Neural Network",
    "doi": "https://doi.org/10.1145/3442201",
    "publication_date": "2021-04-18",
    "publication_year": 2021,
    "authors": "Yi Ouyang; Bin Guo; Xing Tang; Xiuqiang He; Jian Xiong; Zhiwen Yu",
    "corresponding_authors": "",
    "abstract": "With the rapid development of mobile app ecosystem, mobile apps have grown greatly popular. The explosive growth of apps makes it difficult for users to find apps that meet their interests. Therefore, it is necessary to recommend user with a personalized set of apps. However, one of the challenges is data sparsity, as users’ historical behavior data are usually insufficient. In fact, user’s behaviors from different domains in app store regarding the same apps are usually relevant. Therefore, we can alleviate the sparsity using complementary information from correlated domains. It is intuitive to model users’ behaviors using graph, and graph neural networks have shown the great power for representation learning. In this article, we propose a novel model, Deep Multi-Graph Embedding (DMGE), to learn cross-domain app embedding. Specifically, we first construct a multi-graph based on users’ behaviors from different domains, and then propose a multi-graph neural network to learn cross-domain app embedding. Particularly, we present an adaptive method to balance the weight of each domain and efficiently train the model. Finally, we achieve cross-domain app recommendation based on the learned app embedding. Extensive experiments on real-world datasets show that DMGE outperforms other state-of-art embedding methods.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3156028762",
    "type": "article"
  },
  {
    "title": "A Method for Mining Granger Causality Relationship on Atmospheric Visibility",
    "doi": "https://doi.org/10.1145/3447681",
    "publication_date": "2021-05-29",
    "publication_year": 2021,
    "authors": "Bo Liu; Xi He; Mingdong Song; Jiangqiang Li; Guangzhi Qu; Jianlei Lang; Rentao Gu",
    "corresponding_authors": "",
    "abstract": "Atmospheric visibility is an indicator of atmospheric transparency and its range directly reflects the quality of the atmospheric environment. With the acceleration of industrialization and urbanization, the natural environment has suffered some damages. In recent decades, the level of atmospheric visibility shows an overall downward trend. A decrease in atmospheric visibility will lead to a higher frequency of haze, which will seriously affect people's normal life, and also have a significant negative economic impact. The causal relationship mining of atmospheric visibility can reveal the potential relation between visibility and other influencing factors, which is very important in environmental management, air pollution control and haze control. However, causality mining based on statistical methods and traditional machine learning techniques usually achieve qualitative results that are hard to measure the degree of causality accurately. This article proposed the seq2seq-LSTM Granger causality analysis method for mining the causality relationship between atmospheric visibility and its influencing factors. In the experimental part, by comparing with methods such as linear regression, random forest, gradient boosting decision tree, light gradient boosting machine, and extreme gradient boosting, it turns out that the visibility prediction accuracy based on the seq2seq-LSTM model is about 10% higher than traditional machine learning methods. Therefore, the causal relationship mining based on this method can deeply reveal the implicit relationship between them and provide theoretical support for air pollution control.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3166337766",
    "type": "article"
  },
  {
    "title": "sGrapp: Butterfly Approximation in Streaming Graphs",
    "doi": "https://doi.org/10.1145/3495011",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Aida Sheshbolouki; M. TAMER ÖZSU",
    "corresponding_authors": "",
    "abstract": "We study the fundamental problem of butterfly (i.e., (2,2)-bicliques) counting in bipartite streaming graphs. Similar to triangles in unipartite graphs, enumerating butterflies is crucial in understanding the structure of bipartite graphs. This benefits many applications where studying the cohesion in a graph shaped data is of particular interest. Examples include investigating the structure of computational graphs or input graphs to the algorithms, as well as dynamic phenomena and analytic tasks over complex real graphs. Butterfly counting is computationally expensive, and known techniques do not scale to large graphs; the problem is even harder in streaming graphs. In this article, following a data-driven methodology, we first conduct an empirical analysis to uncover temporal organizing principles of butterflies in real streaming graphs and then we introduce an approximate adaptive window-based algorithm, sGrapp, for counting butterflies as well as its optimized version sGrapp-x. sGrapp is designed to operate efficiently and effectively over any graph stream with any temporal behavior. Experimental studies of sGrapp and sGrapp-x show superior performance in terms of both accuracy and efficiency.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4205969642",
    "type": "article"
  },
  {
    "title": "Causal Feature Selection with Missing Data",
    "doi": "https://doi.org/10.1145/3488055",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Kui Yu; Yajing Yang; Wei Ding",
    "corresponding_authors": "",
    "abstract": "Causal feature selection aims at learning the Markov blanket (MB) of a class variable for feature selection. The MB of a class variable implies the local causal structure among the class variable and its MB and all other features are probabilistically independent of the class variable conditioning on its MB, this enables causal feature selection to identify potential causal features for feature selection for building robust and physically meaningful prediction models. Missing data, ubiquitous in many real-world applications, remain an open research problem in causal feature selection due to its technical complexity. In this article, we discuss a novel multiple imputation MB (MimMB) framework for causal feature selection with missing data. MimMB integrates Data Imputation with MB Learning in a unified framework to enable the two key components to engage with each other. MB Learning enables Data Imputation in a potentially causal feature space for achieving accurate data imputation, while accurate Data Imputation helps MB Learning identify a reliable MB of the class variable in turn. Then, we further design an enhanced kNN estimator for imputing missing values and instantiate the MimMB. In our comprehensively experimental evaluation, our new approach can effectively learn the MB of a given variable in a Bayesian network and outperforms other rival algorithms using synthetic and real-world datasets.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4206306045",
    "type": "article"
  },
  {
    "title": "Multi-Concept Representation Learning for Knowledge Graph Completion",
    "doi": "https://doi.org/10.1145/3533017",
    "publication_date": "2022-04-30",
    "publication_year": 2022,
    "authors": "Jiapu Wang; Boyue Wang; Junbin Gao; Yongli Hu; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Knowledge Graph Completion (KGC) aims at inferring missing entities or relations by embedding them in a low-dimensional space. However, most existing KGC methods generally fail to handle the complex concepts hidden in triplets, so the learned embeddings of entities or relations may deviate from the true situation. In this article, we propose a novel M ulti- c oncept R epresentation L earning (McRL) method for the KGC task, which mainly consists of a multi-concept representation module, a deep residual attention module, and an interaction embedding module. Specifically, instead of the single-feature representation, the multi-concept representation module projects each entity or relation to multiple vectors to capture the complex conceptual information hidden in them. The deep residual attention module simultaneously explores the inter- and intra-connection between entities and relations to enhance the entity and relation embeddings corresponding to the current contextual situation. Moreover, the interaction embedding module further weakens the noise and ambiguity to obtain the optimal and robust embeddings. We conduct the link prediction experiment to evaluate the proposed method on several standard datasets, and experimental results show that the proposed method outperforms existing state-of-the-art KGC methods.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4225157436",
    "type": "article"
  },
  {
    "title": "Explainability-Based Mix-Up Approach for Text Data Augmentation",
    "doi": "https://doi.org/10.1145/3533048",
    "publication_date": "2022-04-27",
    "publication_year": 2022,
    "authors": "Soonki Kwon; Younghoon Lee",
    "corresponding_authors": "",
    "abstract": "Text augmentation is a strategy for increasing the diversity of training examples without explicitly collecting new data. Owing to the efficiency and effectiveness of text augmentation, numerous augmentation methodologies have been proposed. Among them, the method based on modification, particularly the mix-up method of swapping words between two or more sentences, is widely used because it can be applied simply and shows good levels of performance. However, the existing mix-up approaches are limited; they do not reflect the importance of the manipulated word. That is, even if a word that has a critical effect on the classification result is manipulated, it is not considered significant in labeling the augmented data. Therefore, in this study, we propose an effective text augmentation technique that explicitly derives the importance of manipulated words and reflects this importance in the labeling of augmented data. The importance of each word, in other words, explainability, is calculated, and this is explicitly reflected in the labeling process of the augmented data. The results of the experiment confirmed that when the importance of the manipulated word was reflected in the labeling, the performance was significantly higher than that of the existing methods.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4293187584",
    "type": "article"
  },
  {
    "title": "Online Scalable Streaming Feature Selection via Dynamic Decision",
    "doi": "https://doi.org/10.1145/3502737",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Peng Zhou; Shu Zhao; Yuanting Yan; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Feature selection is one of the core concepts in machine learning, which hugely impacts the model’s performance. For some real-world applications, features may exist in a stream mode that arrives one by one over time, while we cannot know the exact number of features before learning. Online streaming feature selection aims at selecting optimal stream features at each timestamp on the fly. Without the global information of the entire feature space, most of the existing methods select stream features in terms of individual feature information or the comparison of features in pairs. This article proposes a new online scalable streaming feature selection framework from the dynamic decision perspective that is scalable on running time and selected features by dynamic threshold adjustment. Regarding the philosophy of “Thinking-in-Threes”, we classify each new arrival feature as selecting, discarding, or delaying, aiming at minimizing the overall decision risks. With the dynamic updating of global statistical information, we add the selecting features into the candidate feature subset, ignore the discarding features, cache the delaying features into the undetermined feature subset, and wait for more information. Meanwhile, we perform the redundancy analysis for the candidate features and uncertainty analysis for the undetermined features. Extensive experiments on eleven real-world datasets demonstrate the efficiency and scalability of our new framework compared with state-of-the-art algorithms.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4221117710",
    "type": "article"
  },
  {
    "title": "Disambiguation Enabled Linear Discriminant Analysis for Partial Label Dimensionality Reduction",
    "doi": "https://doi.org/10.1145/3494565",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Min-Ling Zhang; Jinghan Wu; Wei-Xuan Bao",
    "corresponding_authors": "",
    "abstract": "As an emerging weakly supervised learning framework, partial label learning considers inaccurate supervision where each training example is associated with multiple candidate labels among which only one is valid. In this article, a first attempt toward employing dimensionality reduction to help improve the generalization performance of partial label learning system is investigated. Specifically, the popular linear discriminant analysis (LDA) techniques are endowed with the ability of dealing with partial label training examples. To tackle the challenge of unknown ground-truth labeling information, a novel learning approach named Delin is proposed which alternates between LDA dimensionality reduction and candidate label disambiguation based on estimated labeling confidences over candidate labels. On one hand, the (kernelized) projection matrix of LDA is optimized by utilizing disambiguation-guided labeling confidences. On the other hand, the labeling confidences are disambiguated by resorting to k NN aggregation in the LDA-induced feature space. Extensive experiments over a broad range of partial label datasets clearly validate the effectiveness of Delin in improving the generalization performance of well-established partial label learning algorithms.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4226450994",
    "type": "article"
  },
  {
    "title": "HW-Forest: Deep Forest with Hashing Screening and Window Screening",
    "doi": "https://doi.org/10.1145/3532193",
    "publication_date": "2022-05-04",
    "publication_year": 2022,
    "authors": "Pengfei Ma; Youxi Wu; Yan Li; Lei Guo; He Jiang; Xingquan Zhu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "As a novel deep learning model, gcForest has been widely used in various applications. However, current multi-grained scanning of gcForest produces many redundant feature vectors, and this increases the time cost of the model. To screen out redundant feature vectors, we introduce a hashing screening mechanism for multi-grained scanning and propose a model called HW-Forest which adopts two strategies: hashing screening and window screening. HW-Forest employs perceptual hashing algorithm to calculate the similarity between feature vectors in hashing screening strategy, which is used to remove the redundant feature vectors produced by multi-grained scanning and can significantly decrease the time cost and memory consumption. Furthermore, we adopt a self-adaptive instance screening strategy called window screening to improve the performance of our approach, which can achieve higher accuracy without hyperparameter tuning on different datasets. Our experimental results show that HW-Forest has higher accuracy than other models, and the time cost is also reduced.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4229068720",
    "type": "article"
  },
  {
    "title": "Integrating Global and Local Feature Selection for Multi-Label Learning",
    "doi": "https://doi.org/10.1145/3532190",
    "publication_date": "2022-05-10",
    "publication_year": 2022,
    "authors": "Zan Zhang; Lin Liu; Jiuyong Li; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Multi-label learning deals with the problem where an instance is associated with multiple labels simultaneously. Multi-label data is often of high dimensionality and has many noisy, irrelevant, and redundant features. As an important machine learning task, multi-label feature selection has received considerable attention in recent years due to its promising performance in dealing with high-dimensional multi-label data. Existing multi-label feature selection methods typically select the global features which are shared by all instances in a dataset. However, these multi-label feature selection methods may be suboptimal since they do not consider the specific characteristics of instances. In this paper, we propose a novel algorithm that integrates Global and Local Feature Selection (GLFS) to exploit both the global features and a subset of discriminative features shared only locally by a subgroup of instances in a multi-label dataset. Specifically, GLFS employs linear regression and ℓ 2,1 -norm on the regression parameters to achieve simultaneous global and local feature selection. Moreover, the proposed algorithm has an effective mechanism for utilizing label correlations to improve the feature selection. Experiments on real-world multi-label datasets show the superiority of GLFS over the state-of-the-art multi-label feature selection methods.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4280602456",
    "type": "article"
  },
  {
    "title": "L2MM: Learning to Map Matching with Deep Models for Low-Quality GPS Trajectory Data",
    "doi": "https://doi.org/10.1145/3550486",
    "publication_date": "2022-07-26",
    "publication_year": 2022,
    "authors": "Linli Jiang; Chaoxiong Chen; Chao Chen",
    "corresponding_authors": "",
    "abstract": "Map matching is a fundamental research topic with the objective of aligning GPS trajectories to paths on the road network. However, existing models fail to achieve satisfactory performance for low-quality (i.e., noisy, low-frequency, and non-uniform) trajectory data. To this end, we propose a general and robust deep learning-based model, L2MM , to tackle these issues at all. First, high-quality representations of low-quality trajectories are learned by two representation enhancement methods, i.e., enhancement with high-frequency trajectories and enhancement with the data distribution . The former employs high-frequency trajectories to enhance the expressive capability of representations, while the latter regularizes the representation distribution over the latent space to improve the generalization ability of representations. Secondly, to embrace more heuristic clues, typical mobility patterns are recognized in the latent space and further incorporated into the map matching task. Finally, based on the available representations and patterns, a mapping from trajectories to corresponding paths is constructed through a joint optimization method. Extensive experiments are conducted based on a range of datasets, which demonstrate the superiority of L2MM and validate the significance of high-quality representations as well as mobility patterns.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4287982086",
    "type": "article"
  },
  {
    "title": "Dual Subgraph-Based Graph Neural Network for Friendship Prediction in Location-Based Social Networks",
    "doi": "https://doi.org/10.1145/3554981",
    "publication_date": "2022-08-16",
    "publication_year": 2022,
    "authors": "Xuemei Wei; Yezheng Liu; Jianshan Sun; Yuanchun Jiang; Qifeng Tang; Kun Yuan",
    "corresponding_authors": "",
    "abstract": "With the wide use of Location-Based Social Networks (LBSNs), predicting user friendship from online social relations and offline trajectory data is of great value to improve the platform service quality and user satisfaction. Existing methods mainly focus on some hand-crafted features or graph embedding models based on the user-location bipartite graph, which cannot precisely capture the latent mobility similarity for the majority of users who have no explicit co-visit behaviors and also fail to balance the tradeoff between social features and mobility features for friendship prediction. In this regard, we propose a dual subgraph-based pairwise graph neural network (DSGNN) for friendship prediction in LBSNs, which extracts a pairwise social subgraph and a trajectory subgraph to model the social proximity and mobility similarity, respectively. Specifically, to overcome the co-visit data sparsity, we design an entropy-based random walk to construct a location graph that captures the high-level correlation between locations. Based on this, we characterize the pairwise mobility similarity from trajectory level instead of location level, which is modeled by a graph neural network (GNN) on a labeled trajectory subgraph composed of the two trajectories of the target user pair. Besides, we also utilize another GNN to extract social proximity based on social subgraph of the target user pair. Finally, we propose a gate layer to adaptively balance the fusion of the social and mobility features for friendship prediction. We conduct extensive experiments on the real-world datasets and demonstrate the superiority of our approach, which outperforms other state-of-the-art methods. In particular, the comparative experiments on the trajectory level mobility similarity further validate the effectiveness of the designed trajectory subgraph-based method, which can extract predictive mobility features.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4291972642",
    "type": "article"
  },
  {
    "title": "A Self-Representation Method with Local Similarity Preserving for Fast Multi-View Outlier Detection",
    "doi": "https://doi.org/10.1145/3532191",
    "publication_date": "2022-04-27",
    "publication_year": 2022,
    "authors": "Yu Wang; Chuan Chen; Jinrong Lai; Lele Fu; Yuren Zhou; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "With the rapidly growing attention to multi-view data in recent years, multi-view outlier detection has become a rising field with intense research. These researches have made some success, but still exist some issues that need to be solved. First, many multi-view outlier detection methods can only handle datasets that conform to the cluster structure but are powerless for complex data distributions such as manifold structures. This overly restrictive data assumption limits the applicability of these methods. In addition, almost the majority of multi-view outlier detection algorithms cannot solve the online detection problem of multi-view outliers. To address these issues, we propose a new detection method based on the local similarity relation and data reconstruction, i.e., the Self-Representation Method with Local Similarity Preserving for fast multi-view outlier detection (SRLSP). By using the local similarity structure, the proposed method fully utilizes the characteristics of outliers and detects outliers with an applicable objective function. Besides, a well-designed optimization algorithm is proposed, which completes each iteration with linear time complexity and can calculate each instance parallelly. Also, the optimization algorithm can be easily extended to the online version, which is more suitable for practical production environments. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed method on both performance and time complexity.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4293187572",
    "type": "article"
  },
  {
    "title": "ONION: Joint Unsupervised Feature Selection and Robust Subspace Extraction for Graph-based Multi-View Clustering",
    "doi": "https://doi.org/10.1145/3568684",
    "publication_date": "2022-10-25",
    "publication_year": 2022,
    "authors": "Zhibin Gu; Songhe Feng; Ruiting Hu; Gengyu Lyu",
    "corresponding_authors": "",
    "abstract": "Graph-based Multi-View Clustering (GMVC) has received extensive attention due to its ability to capture the neighborhood relationship among data points from diverse views. However, most existing approaches construct similarity graphs from the original multi-view data, the accuracy of which heavily and implicitly relies on the quality of the original multiple features. Moreover, previous methods either focus on mining the multi-view commonality or emphasize on exploring the multi-view individuality, making the rich information contained in multiple features cannot be effectively exploited. In this work, we design a novel GMVC framework via c O mmo N ality and I ndividuality disc O vering in late N t subspace ( ONION ), seeking for a robust and discriminative subspace representation compatible across multiple features for GMVC. To be specific, our method simultaneously formulates the unsupervised sparse feature selection and the robust subspace extraction, as well as the target graph learning in a unified optimization model, which can help the learning of the discriminative subspace representation and the target graph in a mutual reinforcement manner. Meanwhile, we manipulate the target graph by an explicit structural penalty, rendering the connected components in the graph directly reveal clusters. Experimental results on seven benchmark datasets demonstrate the effectiveness of our proposed method.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4307380435",
    "type": "article"
  },
  {
    "title": "TechPat: Technical Phrase Extraction for Patent Mining",
    "doi": "https://doi.org/10.1145/3596603",
    "publication_date": "2023-05-13",
    "publication_year": 2023,
    "authors": "Ye Liu; Han Wu; Zhenya Huang; Hao Wang; Yuting Ning; Jianhui Ma; Qi Liu; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "In recent years, due to the explosive growth of patent applications, patent mining has drawn extensive attention and interest. An important issue of patent mining is that of recognizing the technologies contained in patents, which serves as a fundamental preparation for deeper analysis. To this end, in this article, we make a focused study on constructing a technology portrait for each patent, i.e., to recognize technical phrases concerned in it, which can summarize and represent patents from a technical perspective. Along this line, a critical challenge is how to analyze the unique characteristics of technical phrases and illustrate them with definite descriptions. Therefore, we first generate the detailed descriptions about the technical phrases existing in extensive patents based on different criteria, including various previous works, practical experience, and statistical analyses. Then, considering the unique characteristics of technical phrases and the complex structure of patent documents, such as multi-aspect semantics and multi-level relevances, we further propose a novel unsupervised model, namely TechPat, which can not only automatically recognize technical phrases from massive patents but also avoid the need for expensive human labeling. After that, we evaluate the extraction results from various aspects. Specifically, we propose a novel evaluation metric called Information Retrieval Efficiency (IRE) to quantify the performance of extracted technical phrases from a new perspective. Extensive experiments on real-world patent data demonstrate that the TechPat model can effectively discriminate technical phrases in patents and greatly outperform existing methods. We further apply extracted technical phrases to two practical application tasks, namely patent search and patent classification, where the experimental results confirm the wide application prospects of technical phrases. Finally, we discuss the generalization ability of our proposed methods.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4376505489",
    "type": "article"
  },
  {
    "title": "HUSP-SP: Faster Utility Mining on Sequence Data",
    "doi": "https://doi.org/10.1145/3597935",
    "publication_date": "2023-05-22",
    "publication_year": 2023,
    "authors": "Chunkai Zhang; Yuting Yang; Zilin Du; Wensheng Gan; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "High-utility sequential pattern mining (HUSPM) has emerged as an important topic due to its wide application and considerable popularity. However, due to the combinatorial explosion of the search space when the HUSPM problem encounters a low-utility threshold or large-scale data, it may be time-consuming and memory-costly to address the HUSPM problem. Several algorithms have been proposed for addressing this problem, but they still cost a lot in terms of running time and memory usage. In this article, to further solve this problem efficiently, we design a compact structure called sequence projection (seqPro) and propose an efficient algorithm, namely, discovering high-utility sequential patterns with the seqPro structure (HUSP-SP). HUSP-SP utilizes the compact seq-array to store the necessary information in a sequence database. The seqPro structure is designed to efficiently calculate candidate patterns’ utilities and upper-bound values. Furthermore, a new upper bound on utility, namely, tighter reduced sequence utility and two pruning strategies in search space, are utilized to improve the mining performance of HUSP-SP. Experimental results on both synthetic and real-life datasets show that HUSP-SP can significantly outperform the state-of-the-art algorithms in terms of running time, memory usage, search space pruning efficiency, and scalability.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4377226991",
    "type": "article"
  },
  {
    "title": "TTS-Norm: Forecasting Tensor Time Series via Multi-Way Normalization",
    "doi": "https://doi.org/10.1145/3605894",
    "publication_date": "2023-06-26",
    "publication_year": 2023,
    "authors": "Jiewen Deng; Jinliang Deng; Du Yin; Renhe Jiang; Xuan Song",
    "corresponding_authors": "",
    "abstract": "Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world applications. Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. However, properly coping with the TTS is a much more challenging task, due to its high-dimensional and complex inner structure. In this article, we start by revealing the structure of TTS data from afn statistical view of point. Then, in line with this analysis, we perform T ensor T ime S eries forecasting via a proposed Multi-way Norm alization ( TTS-Norm ), which effectively disentangles multiple heterogeneous low-dimensional substructures from the original high-dimensional structure. Finally, we design a novel objective function for TTS forecasting, accounting for the numerical heterogeneity among different low-dimensional subspaces of TTS. Extensive experiments on two real-world datasets verify the superior performance of our proposed model. 1",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4382024637",
    "type": "article"
  },
  {
    "title": "A Survey on Bid Optimization in Real-Time Bidding Display Advertising",
    "doi": "https://doi.org/10.1145/3628603",
    "publication_date": "2023-10-18",
    "publication_year": 2023,
    "authors": "Weitong Ou; Bo Chen; Xinyi Dai; Weinan Zhang; Weiwen Liu; Ruiming Tang; Yong Yu",
    "corresponding_authors": "",
    "abstract": "Real-Time Bidding (RTB) is one of the most important forms of online advertising, where an auction is hosted in real time to sell the individual ad impression. How to design an automated bidding strategy in response to the dynamic auction environment is crucial for improving user experience, protecting the interests of advertisers, and promoting the long-term development of the advertising platform. As an exciting topic in the real-world industry, it has attracted great research interest from several disciplines, most notably data science. There have been abundant studies on bidding strategy design which are based on the large volume of historical ad requests. Despite its popularity and significance, few works provide a summary for bid optimization. In this survey, we present the latest overview of the recent works to shed light on the optimization techniques where most of them are validated in practice. We first explore the optimization problem in different works, explaining how these different settings affect the bidding strategy designs. Then, some forms of bidding functions and specific optimization techniques are illustrated. Further, we specifically discuss a new trend about bidding in first-price auctions, which have gradually become popular in recent years. From this survey, both practitioners and researchers can gain insights of the challenges and future prospects of bid optimization in RTB.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4387735217",
    "type": "article"
  },
  {
    "title": "Dual Homogeneity Hypergraph Motifs with Cross-view Contrastive Learning for Multiple Social Recommendations",
    "doi": "https://doi.org/10.1145/3653976",
    "publication_date": "2024-03-26",
    "publication_year": 2024,
    "authors": "Jiadi Han; Yufei Tang; Qian Tao; Yuhan Xia; Liming Zhang",
    "corresponding_authors": "",
    "abstract": "Social relations are often used as auxiliary information to address data sparsity and cold-start issues in social recommendations. In the real world, social relations among users are complex and diverse. Widely used graph neural networks (GNNs) can only model pairwise node relationships and are not conducive to exploring higher-order connectivity, while hypergraph provides a natural way to model high-order relations between nodes. However, recent studies show that social recommendations still face the following challenges: 1) a majority of social recommendations ignore the impact of multifaceted social relationships on user preferences; 2) the item homogeneity is often neglected, mainly referring to items with similar static attributes have similar attractiveness when exposed to users that indicating hidden links between items; and 3) directly combining the representations learned from different independent views cannot fully exploit the potential connections between different views. To address these challenges, in this article, we propose a novel method DH-HGCN++ for multiple social recommendations. Specifically, dual homogeneity (i.e., social homogeneity and item homogeneity) is introduced to mine the impact of diverse social relations on user preferences and enrich item representations. Hypergraph convolution networks with motifs are further exploited to model the high-order relations between nodes. Finally, cross-view contrastive learning is proposed as an auxiliary task to jointly optimize the DH-HGCN++. Real-world datasets are used to validate the effectiveness of the proposed model, where we use sentiment analysis to extract comment relations and employ the k-means clustering algorithm to construct the item-item correlation graph. Experiment results demonstrate that our proposed method consistently outperforms the state-of-the-art baselines on Top-N recommendations.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4393191804",
    "type": "article"
  },
  {
    "title": "A clustering framework based on subjective and objective validity criteria",
    "doi": "https://doi.org/10.1145/1324172.1324176",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Maria Halkidi; Dimitrios Gunopulos; M. Vazirgiannis; Naveen Kumar; Carlotta Domeniconi",
    "corresponding_authors": "",
    "abstract": "Clustering, as an unsupervised learning process is a challenging problem, especially in cases of high-dimensional datasets. Clustering result quality can benefit from user constraints and objective validity assessment. In this article, we propose a semisupervised framework for learning the weighted Euclidean subspace, where the best clustering can be achieved. Our approach capitalizes on: (i) user constraints; and (ii) the quality of intermediate clustering results in terms of their structural properties. The proposed framework uses the clustering algorithm and the validity measure as its parameters. We develop and discuss algorithms for learning and tuning the weights of contributing dimensions and defining the “best” clustering obtained by satisfying user constraints. Experimental results on benchmark datasets demonstrate the superiority of the proposed approach in terms of improved clustering accuracy.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2156305067",
    "type": "article"
  },
  {
    "title": "Motif discovery in physiological datasets",
    "doi": "https://doi.org/10.1145/1644873.1644875",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Zeeshan Syed; Collin M. Stultz; Manolis Kellis; Piotr Indyk; John V. Guttag",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a methodology for identifying predictive physiological patterns in the absence of prior knowledge. We use the principle of conservation to identify activity that consistently precedes an outcome in patients, and describe a two-stage process that allows us to efficiently search for such patterns in large datasets. This involves first transforming continuous physiological signals from patients into symbolic sequences, and then searching for patterns in these reduced representations that are strongly associated with an outcome. Our strategy of identifying conserved activity that is unlikely to have occurred purely by chance in symbolic data is analogous to the discovery of regulatory motifs in genomic datasets. We build upon existing work in this area, generalizing the notion of a regulatory motif and enhancing current techniques to operate robustly on non-genomic data. We also address two significant considerations associated with motif discovery in general: computational efficiency and robustness in the presence of degeneracy and noise. To deal with these issues, we introduce the concept of active regions and new subset-based techniques such as a two-layer Gibbs sampling algorithm. These extensions allow for a framework for information inference, where precursors are identified as approximately conserved activity of arbitrary complexity preceding multiple occurrences of an event. We evaluated our solution on a population of patients who experienced sudden cardiac death and attempted to discover electrocardiographic activity that may be associated with the endpoint of death. To assess the predictive patterns discovered, we compared likelihood scores for motifs in the sudden death population against control populations of normal individuals and those with non-fatal supraventricular arrhythmias. Our results suggest that predictive motif discovery may be able to identify clinically relevant information even in the absence of significant prior knowledge.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2123292855",
    "type": "article"
  },
  {
    "title": "Robust Record Linkage Blocking Using Suffix Arrays and Bloom Filters",
    "doi": "https://doi.org/10.1145/1921632.1921635",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Timothy de Vries; Hui Ke; Sanjay Chawla; Peter Christen",
    "corresponding_authors": "",
    "abstract": "Record linkage is an important data integration task that has many practical uses for matching, merging and duplicate removal in large and diverse databases. However, quadratic scalability for the brute force approach of comparing all possible pairs of records necessitates the design of appropriate indexing or blocking techniques. The aim of these techniques is to cheaply remove candidate record pairs that are unlikely to match. We design and evaluate an efficient and highly scalable blocking approach based on suffix arrays. Our suffix grouping technique exploits the ordering used by the index to merge similar blocks at marginal extra cost, resulting in a much higher accuracy while retaining the high scalability of the base suffix array method. Efficiently grouping similar suffixes is carried out with the use of a sliding window technique. We carry out an in-depth analysis of our method and show results from experiments using real and synthetic data, which highlight the importance of using efficient indexing and blocking in real-world applications where datasets contain millions of records. We extend our disk-based methods with the capability to utilise main memory based storage to construct Bloom filters, which we have found to cause significant speedup by reducing the number of costly database queries by up to 70% in real data. We give practical implementation details and show how Bloom filters can be easily applied to Suffix Array based indexing.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2027393969",
    "type": "article"
  },
  {
    "title": "Discriminative Topic Modeling Based on Manifold Learning",
    "doi": "https://doi.org/10.1145/2086737.2086740",
    "publication_date": "2012-01-31",
    "publication_year": 2012,
    "authors": "Seungil Huh; Stephen E. Fienberg",
    "corresponding_authors": "",
    "abstract": "Topic modeling has become a popular method used for data analysis in various domains including text documents. Previous topic model approaches, such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA), have shown impressive success in discovering low-rank hidden structures for modeling text documents. These approaches, however do not take into account the manifold structure of the data, which is generally informative for nonlinear dimensionality reduction mapping. More recent topic model approaches, Laplacian PLSI (LapPLSI) and Locally-consistent Topic Model (LTM), have incorporated the local manifold structure into topic models and have shown resulting benefits. But they fall short of achieving full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs. In this article, we propose a new approach, Discriminative Topic Model (DTM), which separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together, thereby preserving the global manifold structure as well as improving local consistency. We also present a novel model-fitting algorithm based on the generalized EM algorithm and the concept of Pareto improvement. We empirically demonstrate the success of DTM in terms of unsupervised clustering and semisupervised classification accuracies on text corpora and robustness to parameters compared to state-of-the-art techniques.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W1977830790",
    "type": "article"
  },
  {
    "title": "PathSelClus",
    "doi": "https://doi.org/10.1145/2513092.2500492",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Yizhou Sun; Brandon Norick; Jiawei Han; Xifeng Yan; Philip S. Yu; Xiao Yu",
    "corresponding_authors": "",
    "abstract": "Real-world, multiple-typed objects are often interconnected, forming heterogeneous information networks. A major challenge for link-based clustering in such networks is their potential to generate many different results, carrying rather diverse semantic meanings. In order to generate desired clustering, we propose to use meta-path, a path that connects object types via a sequence of relations, to control clustering with distinct semantics. Nevertheless, it is easier for a user to provide a few examples (seeds) than a weighted combination of sophisticated meta-paths to specify her clustering preference. Thus, we propose to integrate meta-path selection with user-guided clustering to cluster objects in networks, where a user first provides a small set of object seeds for each cluster as guidance. Then the system learns the weight for each meta-path that is consistent with the clustering result implied by the guidance, and generates clusters under the learned weights of meta-paths. A probabilistic approach is proposed to solve the problem, and an effective and efficient iterative algorithm, PathSelClus, is proposed to learn the model, where the clustering quality and the meta-path weights mutually enhance each other. Our experiments with several clustering tasks in two real networks and one synthetic network demonstrate the power of the algorithm in comparison with the baselines.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4238059854",
    "type": "article"
  },
  {
    "title": "Scalable and axiomatic ranking of network role similarity",
    "doi": "https://doi.org/10.1145/2518176",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Ruoming Jin; Victor E. Lee; Longjie Li",
    "corresponding_authors": "",
    "abstract": "A key task in analyzing social networks and other complex networks is role analysis: describing and categorizing nodes according to how they interact with other nodes. Two nodes have the same role if they interact with equivalent sets of neighbors. The most fundamental role equivalence is automorphic equivalence. Unfortunately, the fastest algorithms known for graph automorphism are nonpolynomial. Moreover, since exact equivalence is rare, a more meaningful task is measuring the role similarity between any two nodes. This task is closely related to the structural or link-based similarity problem that SimRank addresses. However, SimRank and other existing similarity measures are not sufficient because they do not guarantee to recognize automorphically or structurally equivalent nodes. This article makes two contributions. First, we present and justify several axiomatic properties necessary for a role similarity measure or metric. Second, we present RoleSim, a new similarity metric that satisfies these axioms and can be computed with a simple iterative algorithm. We rigorously prove that RoleSim satisfies all of these axiomatic properties. We also introduce Iceberg RoleSim, a scalable algorithm that discovers all pairs with RoleSim scores above a user-defined threshold θ. We demonstrate the interpretative power of RoleSim on both both synthetic and real datasets.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2064127653",
    "type": "article"
  },
  {
    "title": "Discovering Conditional Matching Rules",
    "doi": "https://doi.org/10.1145/3070647",
    "publication_date": "2017-06-29",
    "publication_year": 2017,
    "authors": "Yihan Wang; Shaoxu Song; Lei Chen; Jeffrey Xu Yu; Hong Cheng",
    "corresponding_authors": "",
    "abstract": "Matching dependencies (MDs) have recently been proposed to make data dependencies tolerant to various information representations, and found useful in data quality applications such as record matching. Instead of the strict equality function used in traditional dependency syntax (e.g., functional dependencies), MDs specify constraints based on similarity and identification. However, in practice, MDs may still be too strict and applicable only in a subset of tuples in a relation. Thereby, we study the conditional matching dependencies (CMDs), which bind matching dependencies only in a certain part of a table, i.e., MDs conditionally applicable in a subset of tuples. Compared to MDs, CMDs have more expressive power that enables them to satisfy wider application needs. In this article, we study several important theoretical and practical issues of CMDs, including irreducible CMDs with respect to the implication, discovery of CMDs from data, reliable CMDs agreed most by a relation, approximate CMDs almost satisfied in a relation, and finally applications of CMDs in record matching and missing value repairing. Through an extensive experimental evaluation in real data sets, we demonstrate the efficiency of proposed CMDs discovery algorithms and effectiveness of CMDs in real applications.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2732517469",
    "type": "article"
  },
  {
    "title": "The Latent Maximum Entropy Principle",
    "doi": "https://doi.org/10.1145/2297456.2297460",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Shaojun Wang; Dale Schuurmans; Yunxin Zhao",
    "corresponding_authors": "",
    "abstract": "We present an extension to Jaynes’ maximum entropy principle that incorporates latent variables. The principle of latent maximum entropy we propose is different from both Jaynes’ maximum entropy principle and maximum likelihood estimation, but can yield better estimates in the presence of hidden variables and limited training data. We first show that solving for a latent maximum entropy model poses a hard nonlinear constrained optimization problem in general. However, we then show that feasible solutions to this problem can be obtained efficiently for the special case of log-linear models---which forms the basis for an efficient approximation to the latent maximum entropy principle. We derive an algorithm that combines expectation-maximization with iterative scaling to produce feasible log-linear solutions. This algorithm can be interpreted as an alternating minimization algorithm in the information divergence, and reveals an intimate connection between the latent maximum entropy and maximum likelihood principles. To select a final model, we generate a series of feasible candidates, calculate the entropy of each, and choose the model that attains the highest entropy. Our experimental results show that estimation based on the latent maximum entropy principle generally gives better results than maximum likelihood when estimating latent variable models on small observed data samples.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2621752948",
    "type": "article"
  },
  {
    "title": "Modeling Alzheimer’s Disease Progression with Fused Laplacian Sparse Group Lasso",
    "doi": "https://doi.org/10.1145/3230668",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Xiaoli Liu; Peng Cao; André Gonçalves; Dazhe Zhao; Arindam Banerjee",
    "corresponding_authors": "",
    "abstract": "Alzheimer’s disease (AD), the most common type of dementia, not only imposes a huge financial burden on the health care system, but also a psychological and emotional burden on patients and their families. There is thus an urgent need to infer trajectories of cognitive performance over time and identify biomarkers predictive of the progression. In this article, we propose the multi-task learning with fused Laplacian sparse group lasso model, which can identify biomarkers closely related to cognitive measures due to its sparsity-inducing property, and model the disease progression with a general weighted (undirected) dependency graphs among the tasks. An efficient alternative directions method of multipliers based optimization algorithm is derived to solve the proposed non-smooth objective formulation. The effectiveness of the proposed model is demonstrated by its superior prediction performance over multiple state-of-the-art methods and accurate identification of compact sets of cognition-relevant imaging biomarkers that are consistent with prior medical studies.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2888918486",
    "type": "article"
  },
  {
    "title": "An Optimization Framework for Combining Ensembles of Classifiers and Clusterers with Applications to Nontransductive Semisupervised Learning and Transfer Learning",
    "doi": "https://doi.org/10.1145/2601435",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "Ayan Acharya; Eduardo R. Hruschka; Joydeep Ghosh; Sreangsu Acharyya",
    "corresponding_authors": "",
    "abstract": "Unsupervised models can provide supplementary soft constraints to help classify new “target” data because similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This article describes a general optimization framework that takes as input class membership estimates from existing classifiers learned on previously encountered “source” (or training) data, as well as a similarity matrix from a cluster ensemble operating solely on the target (or test) data to be classified, and yields a consensus labeling of the target data. More precisely, the application settings considered are nontransductive semisupervised and transfer learning scenarios where the training data are used only to build an ensemble of classifiers and are subsequently discarded before classifying the target data. The framework admits a wide range of loss functions and classification/clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by naïvely applying classifiers learned on the original task to the target data. In addition, we show that the proposed approach, even not being conceptually transductive, can provide better results compared to some popular transductive learning techniques.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2069445814",
    "type": "article"
  },
  {
    "title": "Latent Time-Series Motifs",
    "doi": "https://doi.org/10.1145/2940329",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Josif Grabocka; Nicolas Schilling; Lars Schmidt-Thieme",
    "corresponding_authors": "",
    "abstract": "Motifs are the most repetitive/frequent patterns of a time-series. The discovery of motifs is crucial for practitioners in order to understand and interpret the phenomena occurring in sequential data. Currently, motifs are searched among series sub-sequences, aiming at selecting the most frequently occurring ones. Search-based methods, which try out series sub-sequence as motif candidates, are currently believed to be the best methods in finding the most frequent patterns. However, this paper proposes an entirely new perspective in finding motifs. We demonstrate that searching is non-optimal since the domain of motifs is restricted, and instead we propose a principled optimization approach able to find optimal motifs. We treat the occurrence frequency as a function and time-series motifs as its parameters, therefore we learn the optimal motifs that maximize the frequency function. In contrast to searching, our method is able to discover the most repetitive patterns (hence optimal), even in cases where they do not explicitly occur as sub-sequences. Experiments on several real-life time-series datasets show that the motifs found by our method are highly more frequent than the ones found through searching, for exactly the same distance threshold.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2496293167",
    "type": "article"
  },
  {
    "title": "G-RoI",
    "doi": "https://doi.org/10.1145/3154411",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Loris Belcastro; Fabrizio Marozzo; Domenico Talia; Paolo Trunfio",
    "corresponding_authors": "",
    "abstract": "Geotagged data gathered from social media can be used to discover interesting locations visited by users called Places-of-Interest (PoIs). Since a PoI is generally identified by the geographical coordinates of a single point, it is hard to match it with user trajectories. Therefore, it is useful to define an area, called Region-of-Interest ( RoI ), to represent the boundaries of the PoI’s area. RoI mining techniques are aimed at discovering ROIs from PoIs and other data. Existing RoI mining techniques are based on three main approaches: predefined shapes, density-based clustering, and grid-based aggregation. This article proposes G-RoI , a novel RoI mining technique that exploits the indications contained in geotagged social media items to discover RoIs with a high accuracy. Experiments performed over a set of PoIs in Rome and Paris using social media geotagged data, demonstrate that G-RoI in most cases achieves better results than existing techniques. In particular, the mean F 1 score is 0.34 higher than that obtained with the well-known DBSCAN algorithm in Rome RoIs and 0.23 higher in Paris RoIs.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2785057145",
    "type": "article"
  },
  {
    "title": "Evasion-Robust Classification on Binary Domains",
    "doi": "https://doi.org/10.1145/3186282",
    "publication_date": "2018-06-08",
    "publication_year": 2018,
    "authors": "Bo Li; Yevgeniy Vorobeychik",
    "corresponding_authors": "",
    "abstract": "The success of classification learning has led to numerous attempts to apply it in adversarial settings such as spam and malware detection. The core challenge in this class of applications is that adversaries are not static, but make a deliberate effort to evade the classifiers. We investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries. We first present a general approach based on mixed-integer linear programming (MILP) with constraint generation. This approach is the first to compute an optimal solution to adversarial loss minimization for two general classes of adversarial evasion models in the context of binary feature spaces. To further improve scalability and significantly generalize the scope of the MILP-based method, we propose a principled iterative retraining framework, which can be used with arbitrary classifiers and essentially arbitrary attack models. We show that the retraining approach, when it converges, minimizes an upper bound on adversarial loss. Extensive experiments demonstrate that the mixed-integer programming approach significantly outperforms several state-of-the-art adversarial learning alternatives. Moreover, the retraining framework performs nearly as well, but scales significantly better. Finally, we show that our approach is robust to misspecifications of the adversarial model.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2808213427",
    "type": "article"
  },
  {
    "title": "Computing top- <i>k</i> Closeness Centrality Faster in Unweighted Graphs",
    "doi": "https://doi.org/10.1145/3344719",
    "publication_date": "2019-09-24",
    "publication_year": 2019,
    "authors": "Elisabetta Bergamini; Michele Borassi; Pierluigi Crescenzi; Andrea Marino; Henning Meyerhenke",
    "corresponding_authors": "",
    "abstract": "Given a connected graph G =( V , E ), where V denotes the set of nodes and E the set of edges of the graph, the length (that is, the number of edges) of the shortest path between two nodes v and w is denoted by d ( v , w ). The closeness centrality of a vertex v is then defined as n =1/Σ w ∈ V d ( v , w ), where n =| V |. This measure is widely used in the analysis of real-world complex networks, and the problem of selecting the k most central vertices has been deeply analyzed in the last decade. However, this problem is computationally not easy, especially for large networks: in the first part of the article, we prove that it is not solvable in time O (| E | 2=ϵ ) on directed graphs, for any constant ϵ &gt; 0, under reasonable complexity assumptions. Furthermore, we propose a new algorithm for selecting the k most central nodes in a graph: we experimentally show that this algorithm improves significantly both the textbook algorithm, which is based on computing the distance between all pairs of vertices, and the state of the art. For example, we are able to compute the top k nodes in few dozens of seconds in real-world networks with millions of nodes and edges. Finally, as a case study, we compute the 10 most central actors in the Internet Movie Database (IMDB) collaboration network, where two actors are linked if they played together in a movie, and in the Wikipedia citation network, which contains a directed edge from a page p to a page q if p contains a link to q .",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2976821708",
    "type": "article"
  },
  {
    "title": "A Review on OLAP Technologies Applied to Information Networks",
    "doi": "https://doi.org/10.1145/3370912",
    "publication_date": "2019-12-13",
    "publication_year": 2019,
    "authors": "Paulo Orlando Queiroz-Sousa; Ana Carolina Salgado",
    "corresponding_authors": "",
    "abstract": "Many real systems produce network data or highly interconnected data, which can be called information networks. These information networks form a critical component in modern information infrastructure, constituting a large graph data volume. The analysis of information network data covers several technological areas, among them OLAP technologies. OLAP is a technology that enables multi-dimensional and multi-level analysis on a large volume of data, providing aggregated data visualizations with different perspectives. This article presents a literature review on the main applications of OLAP technology in the analysis of information network data. To achieve such goal, it shows a systematic review to list the works that apply OLAP technologies in graph data. It defines seven comparison criteria (Materialization, Network, Selection, Aggregation, Model, OLAP Operations, Analytics) to qualify the works found based on their functionalities. The works are analyzed according to each criterion and discussed to identify trends and challenges in the application of OLAP in the information network.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3008488863",
    "type": "review"
  },
  {
    "title": "Density-Friendly Graph Decomposition",
    "doi": "https://doi.org/10.1145/3344210",
    "publication_date": "2019-09-24",
    "publication_year": 2019,
    "authors": "Nikolaj Tatti",
    "corresponding_authors": "Nikolaj Tatti",
    "abstract": "Decomposing a graph into a hierarchical structure via k -core analysis is a standard operation in any modern graph-mining toolkit. k -core decomposition is a simple and efficient method that allows to analyze a graph beyond its mere degree distribution. More specifically, it is used to identify areas in the graph of increasing centrality and connectedness, and it allows to reveal the structural organization of the graph. Despite the fact that k -core analysis relies on vertex degrees, k -cores do not satisfy a certain, rather natural, density property. Simply put, the most central k -core is not necessarily the densest subgraph. This inconsistency between k -cores and graph density provides the basis of our study. We start by defining what it means for a subgraph to be locally dense , and we show that our definition entails a nested chain decomposition of the graph, similar to the one given by k -cores, but in this case the components are arranged in order of increasing density. We show that such a locally dense decomposition for a graph G =( V , E ) can be computed in polynomial time. The running time of the exact decomposition algorithm is O (| V | 2 | E |) but is significantly faster in practice. In addition, we develop a linear-time algorithm that provides a factor-2 approximation to the optimal locally dense decomposition. Furthermore, we show that the k -core decomposition is also a factor-2 approximation, however, as demonstrated by our experimental evaluation, in practice k -cores have different structure than locally dense subgraphs, and as predicted by the theory, k -cores are not always well-aligned with graph density.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3121632873",
    "type": "article"
  },
  {
    "title": "Feature Selection for Social Media Data",
    "doi": "https://doi.org/10.1145/2629587",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Jiliang Tang; Huan Liu",
    "corresponding_authors": "",
    "abstract": "Feature selection is widely used in preparing high-dimensional data for effective data mining. The explosive popularity of social media produces massive and high-dimensional data at an unprecedented rate, presenting new challenges to feature selection. Social media data consists of (1) traditional high-dimensional, attribute-value data such as posts, tweets, comments, and images, and (2) linked data that provides social context for posts and describes the relationships between social media users as well as who generates the posts, and so on. The nature of social media also determines that its data is massive, noisy, and incomplete, which exacerbates the already challenging problem of feature selection. In this article, we study a novel feature selection problem of selecting features for social media data with its social context. In detail, we illustrate the differences between attribute-value data and social media data, investigate if linked data can be exploited in a new feature selection framework by taking advantage of social science theories. We design and conduct experiments on datasets from real-world social media Web sites, and the empirical results demonstrate that the proposed framework can significantly improve the performance of feature selection. Further experiments are conducted to evaluate the effects of user--user and user--post relationships manifested in linked data on feature selection, and research issues for future work will be discussed.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2036732842",
    "type": "article"
  },
  {
    "title": "Behavior2Vec",
    "doi": "https://doi.org/10.1145/3184454",
    "publication_date": "2018-04-16",
    "publication_year": 2018,
    "authors": "Hung‐Hsuan Chen",
    "corresponding_authors": "Hung‐Hsuan Chen",
    "abstract": "Most studies on recommender systems target at increasing the click through rate, and hope that the number of orders will increase as well. We argue that clicking and purchasing an item are different behaviors. Thus, we should probably apply different strategies for different objectives, e.g., increase the click through rate, or increase the order rate. In this article, we propose to generate the distributed representations of users’ viewing and purchasing behaviors on an e-commerce website. By leveraging on the cosine distance between the distributed representations of the behaviors on items under different contexts, we can predict a user’s next clicking or purchasing item more precisely, compared to several baseline methods. Perhaps more importantly, we found that the distributed representations may help discover interesting analogies among the products. We may utilize such analogies to explain how two products are related, and eventually apply different recommendation strategies under different scenarios. We developed the Behavior2Vec library for demonstration. The library can be accessed at https://github.com/ncu-dart/behavior2vec/.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2803068702",
    "type": "article"
  },
  {
    "title": "Continuous-Time Relationship Prediction in Dynamic Heterogeneous Information Networks",
    "doi": "https://doi.org/10.1145/3333028",
    "publication_date": "2019-07-29",
    "publication_year": 2019,
    "authors": "Sina Sajadmanesh; Sogol Bazargani; Jiawei Zhang; Hamid R. Rabiee",
    "corresponding_authors": "",
    "abstract": "Online social networks, World Wide Web, media and technological networks, and other types of so-called information networks are ubiquitous nowadays. These information networks are inherently heterogeneous and dynamic. They are heterogeneous as they consist of multi-typed objects and relations, and they are dynamic as they are constantly evolving over time. One of the challenging issues in such heterogeneous and dynamic environments is to forecast those relationships in the network that will appear in the future. In this paper, we try to solve the problem of continuous-time relationship prediction in dynamic and heterogeneous information networks. This implies predicting the time it takes for a relationship to appear in the future, given its features that have been extracted by considering both heterogeneity and temporal dynamics of the underlying network. To this end, we first introduce a feature extraction framework that combines the power of meta-path-based modeling and recurrent neural networks to effectively extract features suitable for relationship prediction regarding heterogeneity and dynamicity of the networks. Next, we propose a supervised non-parametric approach, called Non-Parametric Generalized Linear Model (NP-GLM), which infers the hidden underlying probability distribution of the relationship building time given its features. We then present a learning algorithm to train NP-GLM and an inference method to answer time-related queries. Extensive experiments conducted on synthetic data and three real-world datasets, namely Delicious, MovieLens, and DBLP, demonstrate the effectiveness of NP-GLM in solving continuous-time relationship prediction problem vis-a-vis competitive baselines",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3122931143",
    "type": "article"
  },
  {
    "title": "Querying Discriminative and Representative Samples for Batch Mode Active Learning",
    "doi": "https://doi.org/10.1145/2700408",
    "publication_date": "2015-02-17",
    "publication_year": 2015,
    "authors": "Zheng Wang; Jieping Ye",
    "corresponding_authors": "",
    "abstract": "Empirical risk minimization (ERM) provides a principled guideline for many machine learning and data mining algorithms. Under the ERM principle, one minimizes an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, ERM requires that the training data are i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where one selects the most informative samples to label, and these data may not follow the source distribution. In this article, we generalize the ERM principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound, we develop a practical batch mode active learning method. The proposed formulation involves a nonconvex integer programming optimization problem. We solve it efficiently by an alternating optimization method. Our method is shown to query the most informative samples while preserving the source distribution as much as possible, thus identifying the most uncertain and representative queries. We further extend our method to multiclass active learning by introducing novel pseudolabels in the multiclass case and developing an efficient algorithm. Experiments on benchmark datasets and real-world applications demonstrate the superior performance of our proposed method compared to state-of-the-art methods.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4376601268",
    "type": "article"
  },
  {
    "title": "Mining Dual Networks",
    "doi": "https://doi.org/10.1145/2785970",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Yubao Wu; Xiaofeng Zhu; Li Li; Wei Fan; Ruoming Jin; Xiang Zhang",
    "corresponding_authors": "",
    "abstract": "Finding the densest subgraph in a single graph is a fundamental problem that has been extensively studied. In many emerging applications, there exist dual networks. For example, in genetics, it is important to use protein interactions to interpret genetic interactions. In this application, one network represents physical interactions among nodes, for example, protein--protein interactions, and another network represents conceptual interactions, for example, genetic interactions. Edges in the conceptual network are usually derived based on certain correlation measure or statistical test measuring the strength of the interaction. Two nodes with strong conceptual interaction may not have direct physical interaction. In this article, we propose the novel dual-network model and investigate the problem of finding the densest connected subgraph (DCS), which has the largest density in the conceptual network and is also connected in the physical network. Density in the conceptual network represents the average strength of the measured interacting signals among the set of nodes. Connectivity in the physical network shows how they interact physically. Such pattern cannot be identified using the existing algorithms for a single network. We show that even though finding the densest subgraph in a single network is polynomial time solvable, the DCS problem is NP-hard. We develop a two-step approach to solve the DCS problem. In the first step, we effectively prune the dual networks, while guarantee that the optimal solution is contained in the remaining networks. For the second step, we develop two efficient greedy methods based on different search strategies to find the DCS. Different variations of the DCS problem are also studied. We perform extensive experiments on a variety of real and synthetic dual networks to evaluate the effectiveness and efficiency of the developed methods.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2341328930",
    "type": "article"
  },
  {
    "title": "Mining Influencers Using Information Flows in Social Streams",
    "doi": "https://doi.org/10.1145/2815625",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "Karthik Subbian; Charų C. Aggarwal; Jaideep Srivastava",
    "corresponding_authors": "",
    "abstract": "The problem of discovering information flow trends in social networks has become increasingly relevant due to the increasing amount of content in online social networks, and its relevance as a tool for research into the content trends analysis in the network. An important part of this analysis is to determine the key patterns of flow in the underlying network. Almost all the work in this area has focused on fixed models of the network structure, and edge-based transmission between nodes. In this article, we propose a fully content-centered model of flow analysis in networks, in which the analysis is based on actual content transmissions in the underlying social stream, rather than a static model of transmission on the edges. First, we introduce the problem of influence analysis in the context of information flow in networks. We then propose a novel algorithm InFlowMine to discover the information flow patterns in the network and demonstrate the effectiveness of the discovered information flows using an influence mining application. This application illustrates the flexibility and effectiveness of our information flow model to find topic- or network-specific influencers, or their combinations. We empirically show that our information flow mining approach is effective and efficient than the existing methods on a number of different measures.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2344550499",
    "type": "article"
  },
  {
    "title": "Fast, Accurate, and Flexible Algorithms for Dense Subtensor Mining",
    "doi": "https://doi.org/10.1145/3154414",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Kijung Shin; Bryan Hooi; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "Given a large-scale and high-order tensor, how can we detect dense subtensors in it? Can we spot them in near-linear time but with quality guarantees? Extensive previous work has shown that dense subtensors, as well as dense subgraphs, indicate anomalous or fraudulent behavior (e.g., lockstep behavior in social networks). However, available algorithms for detecting dense subtensors are not satisfactory in terms of speed, accuracy, and flexibility. In this work, we propose two algorithms, called M-Z oom and M-B iz , for fast and accurate dense-subtensor detection with various density measures. M-Z oom gives a lower bound on the density of detected subtensors, while M-B iz guarantees the local optimality of detected subtensors. M-Z oom and M-B iz can be combined, giving the following advantages: (1) Scalable: scale near-linearly with all aspects of tensors and are up to 114× faster than state-of-the-art methods with similar accuracy, (2) Provably accurate : provide a guarantee on the lowest density and local optimality of the subtensors they find, (3) Flexible: support multi-subtensor detection and size bounds as well as diverse density measures, and (4) Effective: successfully detected edit wars and bot activities in Wikipedia, and spotted network attacks from a TCP dump with near-perfect accuracy (AUC = 0.98).",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2784511520",
    "type": "article"
  },
  {
    "title": "De-anonymizing Clustered Social Networks by Percolation Graph Matching",
    "doi": "https://doi.org/10.1145/3127876",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Carla Fabiana Chiasserini; Michel Garetto; Emili Leonardi",
    "corresponding_authors": "",
    "abstract": "Online social networks offer the opportunity to collect a huge amount of valuable information about billions of users. The analysis of this data by service providers and unintended third parties are posing serious treats to user privacy. In particular, recent work has shown that users participating in more than one online social network can be identified based only on the structure of their links to other users. An effective tool to de-anonymize social network users is represented by graph matching algorithms. Indeed, by exploiting a sufficiently large set of seed nodes, a percolation process can correctly match almost all nodes across the different social networks. In this article, we show the crucial role of clustering, which is a relevant feature of social network graphs (and many other systems). Clustering has both the effect of making matching algorithms more prone to errors, and the potential to greatly reduce the number of seeds needed to trigger percolation. We show these facts by considering a fairly general class of random geometric graphs with variable clustering level. We assume that seeds can be identified in particular sub-regions of the network graph, while no a priori knowledge about the location of the other nodes is required. Under these conditions, we show how clever algorithms can achieve surprisingly good performance while limiting the number of matching errors.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2963456718",
    "type": "article"
  },
  {
    "title": "Influence Maximization",
    "doi": "https://doi.org/10.1145/3399661",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Jianxiong Guo; Weili Wu",
    "corresponding_authors": "",
    "abstract": "Influence maximization problem attempts to find a small subset of nodes in a social network that makes the expected influence maximized, which has been researched intensively before. Most of the existing literature focus only on maximizing total influence, but it ignores whether the influential distribution is balanced through the network. Even though the total influence is maximized, but gathered in a certain area of social network. Sometimes, this is not advisable. In this article, we propose a novel seeding strategy based on community structure, and formulate the Influence Maximization with Community Budget (IMCB) problem. In this problem, the number of seed nodes in each community is under the cardinality constraint, which can be classified as the problem of monotone submodular maximization under the matroid constraint. To give a satisfactory solution for IMCB problem under the triggering model, we propose the IMCB-Framework, which is inspired by the idea of continuous greedy process and pipage rounding, and derive the best approximation ratio for this problem. In IMCB-Framework, we adopt sampling techniques to overcome the high complexity of continuous greedy. Then, we propose a simplified pipage rounding algorithm, which reduces the complexity of IMCB-Framework further. Finally, we conduct experiments on three real-world datasets to evaluate the correctness and effectiveness of our proposed algorithms, as well as the advantage of IMCB-Framework against classical greedy method.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W3089713807",
    "type": "article"
  },
  {
    "title": "Biomedical Ontology Quality Assurance Using a Big Data Approach",
    "doi": "https://doi.org/10.1145/2768830",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Licong Cui; Shiqiang Tao; Guo‐Qiang Zhang",
    "corresponding_authors": "",
    "abstract": "This article presents recent progresses made in using scalable cloud computing environment, Hadoop and MapReduce, to perform ontology quality assurance (OQA), and points to areas of future opportunity. The standard sequential approach used for implementing OQA methods can take weeks if not months for exhaustive analyses for large biomedical ontological systems. With OQA methods newly implemented using massively parallel algorithms in the MapReduce framework, several orders of magnitude in speed-up can be achieved (e.g., from three months to three hours). Such dramatically reduced time makes it feasible not only to perform exhaustive structural analysis of large ontological hierarchies, but also to systematically track structural changes between versions for evolutional analysis. As an exemplar, progress is reported in using MapReduce to perform evolutional analysis and visualization on the Systemized Nomenclature of Medicine—Clinical Terms (SNOMED CT), a prominent clinical terminology system. Future opportunities in three areas are described: one is to extend the scope of MapReduce-based approach to existing OQA methods, especially for automated exhaustive structural analysis. The second is to apply our proposed MapReduce Pipeline for Lattice-based Evaluation (MaPLE) approach, demonstrated as an exemplar method for SNOMED CT, to other biomedical ontologies. The third area is to develop interfaces for reviewing results obtained by OQA methods and for visualizing ontological alignment and evolution, which can also take advantage of cloud computing technology to systematically pre-compute computationally intensive jobs in order to increase performance during user interactions with the visualization interface. Advances in these directions are expected to better support the ontological engineering lifecycle.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2401811235",
    "type": "article"
  },
  {
    "title": "Differentiating Regularization Weights -- A Simple Mechanism to Alleviate Cold Start in Recommender Systems",
    "doi": "https://doi.org/10.1145/3285954",
    "publication_date": "2019-01-09",
    "publication_year": 2019,
    "authors": "Hung‐Hsuan Chen; Pu Chen",
    "corresponding_authors": "",
    "abstract": "Matrix factorization (MF) and its extended methodologies have been studied extensively in the community of recommender systems in the last decade. Essentially, MF attempts to search for low-ranked matrices that can (1) best approximate the known rating scores, and (2) maintain low Frobenius norm for the low-ranked matrices to prevent overfitting. Since the two objectives conflict with each other, the common practice is to assign the relative importance weights as the hyper-parameters to these objectives. The two low-ranked matrices returned by MF are often interpreted as the latent factors of a user and the latent factors of an item that would affect the rating of the user on the item. As a result, it is typical that, in the loss function, we assign a regularization weight λ p on the norms of the latent factors for all users, and another regularization weight λ q on the norms of the latent factors for all the items. We argue that such a methodology probably over-simplifies the scenario. Alternatively, we probably should assign lower constraints to the latent factors associated with the items or users that reveal more information, and set higher constraints to the others. In this article, we systematically study this topic. We found that such a simple technique can improve the prediction results of the MF-based approaches based on several public datasets. Specifically, we applied the proposed methodology on three baseline models -- SVD, SVD++, and the NMF models. We found that this technique improves the prediction accuracy for all these baseline models. Perhaps more importantly, this technique better predicts the ratings on the long-tail items, i.e., the items that were rated/viewed/purchased by few users. This suggests that this approach may partially remedy the cold-start issue. The proposed method is very general and can be easily applied on various recommendation models, such as Factorization Machines, Field-aware Factorization Machines, Factorizing Personalized Markov Chains, Prod2Vec, Behavior2Vec, and so on. We release the code for reproducibility. We implemented a Python package that integrates the proposed regularization technique with the SVD, SVD++, and the NMF model. The package can be accessed at https://github.com/ncu-dart/rdf.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2908770365",
    "type": "article"
  },
  {
    "title": "Clustering Users by Their Mobility Behavioral Patterns",
    "doi": "https://doi.org/10.1145/3322126",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "Irad Ben‐Gal; Shahar Weinstock; Gonen Singer; Nicholas Bambos",
    "corresponding_authors": "",
    "abstract": "The immense stream of data from mobile devices during recent years enables one to learn more about human behavior and provide mobile phone users with personalized services. In this work, we identify clusters of users who share similar mobility behavioral patterns. We analyze trajectories of semantic locations to find users who have similar mobility “lifestyle,” even when they live in different areas. For this task, we propose a new grouping scheme that is called Lifestyle-Based Clustering (LBC). We represent the mobility movement of each user by a Markov model and calculate the Jensen–Shannon distances among pairs of users. The pairwise distances are represented by a similarity matrix, which is used for the clustering. To validate the unsupervised clustering task, we develop an entropy-based clustering measure, namely, an index that measures the homogeneity of mobility patterns within clusters of users. The analysis is validated on a real-world dataset that contains location-movements of 50,000 cellular phone users that were analyzed over a two-month period.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2969573262",
    "type": "article"
  },
  {
    "title": "Neural Serendipity Recommendation",
    "doi": "https://doi.org/10.1145/3396607",
    "publication_date": "2020-06-16",
    "publication_year": 2020,
    "authors": "Yuanbo Xu; Yongjian Yang; En Wang; Jiayu Han; Fuzhen Zhuang; Zhiwen Yu; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "Recommender systems have been playing an important role in providing personalized information to users. However, there is always a trade-off between accuracy and novelty in recommender systems. Usually, many users are suffering from redundant or inaccurate recommendation results. To this end, in this article, we put efforts into exploring the hidden knowledge of observed ratings to alleviate this recommendation dilemma. Specifically, we utilize some basic concepts to define a concept, Serendipity , which is characterized by high-satisfaction and low-initial-interest. Based on this concept, we propose a two-phase recommendation problem which aims to strike a balance between accuracy and novelty achieved by serendipity prediction and personalized recommendation. Along this line, a Neural Serendipity Recommendation (NSR) method is first developed by combining Muti-Layer Percetron and Matrix Factorization for serendipity prediction. Then, a weighted candidate filtering method is designed for personalized recommendation. Finally, extensive experiments on real-world data demonstrate that NSR can achieve a superior serendipity by a 12% improvement in average while maintaining stable accuracy compared with state-of-the-art methods.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3047590354",
    "type": "article"
  },
  {
    "title": "Jointly Modeling Spatio–Temporal Dependencies and Daily Flow Correlations for Crowd Flow Prediction",
    "doi": "https://doi.org/10.1145/3439346",
    "publication_date": "2021-03-26",
    "publication_year": 2021,
    "authors": "Tianzi Zang; Yanmin Zhu; Yanan Xu; Jiadi Yu",
    "corresponding_authors": "",
    "abstract": "Crowd flow prediction is a vital problem for an intelligent transportation system construction in a smart city. It plays a crucial role in traffic management and behavioral analysis, thus it has raised great attention from many researchers. However, predicting crowd flows timely and accurately is a challenging task that is affected by many complex factors such as the dependencies of adjacent regions or recent crowd flows. Existing models mainly focus on capturing such dependencies in spatial or temporal domains and fail to model relations between crowd flows of distant regions. We notice that each region has a relatively fixed daily flow and some regions (even very far away from each other) may share similar flow patterns which show strong correlations among them. In this article, we propose a novel model named Double-Encoder which follows a general encoder–decoder framework for multi-step citywide crowd flow prediction. The model consists of two encoder modules named ST-Encoder and FR-Encoder to model spatial-temporal dependencies and daily flow correlations, respectively. We conduct extensive experiments on two real-world datasets to evaluate the performance of the proposed model and show that our model consistently outperforms state-of-the-art methods.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3149667390",
    "type": "article"
  },
  {
    "title": "Parallel Greedy Algorithm to Multiple Influence Maximization in Social Network",
    "doi": "https://doi.org/10.1145/3442341",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Guanhao Wu; Xiaofeng Gao; Ge Yan; Guihai Chen",
    "corresponding_authors": "",
    "abstract": "Influence Maximization (IM) problem is to select influential users to maximize the influence spread, which plays an important role in many real-world applications such as product recommendation, epidemic control, and network monitoring. Nowadays multiple kinds of information can propagate in online social networks simultaneously, but current literature seldom discuss about this phenomenon. Accordingly, in this article, we propose Multiple Influence Maximization (MIM) problem where multiple information can propagate in a single network with different propagation probabilities. The goal of MIM problems is to maximize the overall accumulative influence spreads of different information with the limit of seed budget . To solve MIM problems, we first propose a greedy framework to solve MIM problems which maintains an -approximate ratio. We further propose parallel algorithms based on semaphores, an inter-thread communication mechanism, which significantly improves our algorithms efficiency. Then we conduct experiments for our framework using complex social network datasets with 12k, 154k, 317k, and 1.1m nodes, and the experimental results show that our greedy framework outperforms other heuristic algorithms greatly for large influence spread and parallelization of algorithms reduces running time observably with acceptable memory overhead.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3153107927",
    "type": "article"
  },
  {
    "title": "TPmod: A Tendency-Guided Prediction Model for Temporal Knowledge Graph Completion",
    "doi": "https://doi.org/10.1145/3443687",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Luyi Bai; Xiangnan Ma; Mingcheng Zhang; Wenting Yu",
    "corresponding_authors": "",
    "abstract": "Temporal knowledge graphs (TKGs) have become useful resources for numerous Artificial Intelligence applications, but they are far from completeness. Inferring missing events in temporal knowledge graphs is a fundamental and challenging task. However, most existing methods solely focus on entity features or consider the entities and relations in a disjoint manner. They do not integrate the features of entities and relations in their modeling process. In this paper, we propose TPmod, a tendency-guided prediction model, to predict the missing events for TKGs (extrapolation). Differing from existing works, we propose two definitions for TKGs: the Goodness of relations and the Closeness of entity pairs. More importantly, inspired by the attention mechanism, we propose a novel tendency strategy to guide our aggregated process. It integrates the features of entities and relations, and assigns varying weights to different past events. What is more, we select the Gate Recurrent Unit (GRU) as our sequential encoder to model the temporal dependency in TKGs. Besides, the Softmax function is employed to generate the final decreasing group of candidate entities. We evaluate our model on two TKG datasets: GDELT-5 and ICEWS-250. Experimental results show that our method has a significant and consistent improvement compared to state-of-the-art baselines.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3156393901",
    "type": "article"
  },
  {
    "title": "Factor-Bounded Nonnegative Matrix Factorization",
    "doi": "https://doi.org/10.1145/3451395",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Kai Liu; Xiangyu Li; Zhihui Zhu; Lodewijk Brand; Hua Wang",
    "corresponding_authors": "",
    "abstract": "Nonnegative Matrix Factorization (NMF) is broadly used to determine class membership in a variety of clustering applications. From movie recommendations and image clustering to visual feature extractions, NMF has applications to solve a large number of knowledge discovery and data mining problems. Traditional optimization methods, such as the Multiplicative Updating Algorithm (MUA), solves the NMF problem by utilizing an auxiliary function to ensure that the objective monotonically decreases. Although the objective in MUA converges, there exists no proof to show that the learned matrix factors converge as well. Without this rigorous analysis, the clustering performance and stability of the NMF algorithms cannot be guaranteed. To address this knowledge gap, in this article, we study the factor-bounded NMF problem and provide a solution algorithm with proven convergence by rigorous mathematical analysis, which ensures that both the objective and matrix factors converge. In addition, we show the relationship between MUA and our solution followed by an analysis of the convergence of MUA. Experiments on both toy data and real-world datasets validate the correctness of our proposed method and its utility as an effective clustering algorithm.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3161106602",
    "type": "article"
  },
  {
    "title": "On-Shelf Utility Mining of Sequence Data",
    "doi": "https://doi.org/10.1145/3457570",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Chunkai Zhang; Zilin Du; Yuting Yang; Wensheng Gan; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Utility mining has emerged as an important and interesting topic owing to its wide application and considerable popularity. However, conventional utility mining methods have a bias toward items that have longer on-shelf time as they have a greater chance to generate a high utility. To eliminate the bias, the problem of on-shelf utility mining (OSUM) is introduced. In this article, we focus on the task of OSUM of sequence data, where the sequential database is divided into several partitions according to time periods and items are associated with utilities and several on-shelf time periods. To address the problem, we propose two methods, OSUM of sequence data (OSUMS) and OSUMS + , to extract on-shelf high-utility sequential patterns. For further efficiency, we also design several strategies to reduce the search space and avoid redundant calculation with two upper bounds time prefix extension utility ( TPEU ) and time reduced sequence utility ( TRSU ). In addition, two novel data structures are developed for facilitating the calculation of upper bounds and utilities. Substantial experimental results on certain real and synthetic datasets show that the two methods outperform the state-of-the-art algorithm. In conclusion, OSUMS may consume a large amount of memory and is unsuitable for cases with limited memory, while OSUMS + has wider real-life applications owing to its high efficiency.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3185014092",
    "type": "article"
  },
  {
    "title": "Anomaly Detection With Kernel Preserving Embedding",
    "doi": "https://doi.org/10.1145/3447684",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Huawen Liu; En-Hui Li; Xinwang Liu; Kaile Su; Shichao Zhang",
    "corresponding_authors": "",
    "abstract": "Similarity representation plays a central role in increasingly popular anomaly detection techniques, which have been successfully applied in various realistic scenes. Until now, many low-rank representation techniques have been introduced to measure the similarity relations of data; yet, they only concern to minimize reconstruction errors, without involving the structural information of data. Besides, the traditional low-rank representation methods often take nuclear norm as their low-rank constraints, easily yielding a suboptimal solution. To address the problems above, in this article, we propose a novel anomaly detection method, which exploits kernel preserving embedding, as well as the double nuclear norm, to explore the similarity relations of data. Based on the similarity relations, a kind of probability transition matrix is derived, and a tailored random walk is further adopted to reveal anomalies. The proposed method can not only preserve the manifold structural properties of the data, but also alleviate the suboptimal problem. To validate the superiority of our method, extensive experiments with eight popular anomaly detection algorithms were conducted on 12 widely used datasets. The experimental results show that our detection method outperformed the state-of-the-art anomaly detection algorithms in most cases.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3160696717",
    "type": "article"
  },
  {
    "title": "DACHA: A Dual Graph Convolution Based Temporal Knowledge Graph Representation Learning Method Using Historical Relation",
    "doi": "https://doi.org/10.1145/3477051",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Ling Chen; Xing Tang; Weiqi Chen; Yuntao Qian; Yansheng Li; Yongjun Zhang",
    "corresponding_authors": "",
    "abstract": "Temporal knowledge graph (TKG) representation learning embeds relations and entities into a continuous low-dimensional vector space by incorporating temporal information. Latest studies mainly aim at learning entity representations by modeling entity interactions from the neighbor structure of the graph. However, the interactions of relations from the neighbor structure of the graph are neglected, which are also of significance for learning informative representations. In addition, there still lacks an effective historical relation encoder to model the multi-range temporal dependencies. In this article, we propose a d ual gr a ph c onvolution network based TKG representation learning method using h istorical rel a tions (DACHA). Specifically, we first construct the primal graph according to historical relations, as well as the edge graph by regarding historical relations as nodes. Then, we employ the dual graph convolution network to capture the interactions of both entities and historical relations from the neighbor structure of the graph. In addition, the temporal self-attentive historical relation encoder is proposed to explicitly model both local and global temporal dependencies. Extensive experiments on two event based TKG datasets demonstrate that DACHA achieves the state-of-the-art results.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3211214398",
    "type": "article"
  },
  {
    "title": "Core Interest Network for Click-Through Rate Prediction",
    "doi": "https://doi.org/10.1145/3428079",
    "publication_date": "2021-01-04",
    "publication_year": 2021,
    "authors": "En Xu; Zhiwen Yu; Bin Guo; Helei Cui",
    "corresponding_authors": "",
    "abstract": "In modern online advertising systems, the click-through rate (CTR) is an important index to measure the popularity of an item. It refers to the ratio of users who click on a specific advertisement to the number of total users who view it. Predicting the CTR of an item in advance can improve the accuracy of the advertisement recommendation. And it is commonly calculated based on users’ interests. Thus, extracting users’ interests is of great importance in CTR prediction tasks. In the literature, a lot of studies treat the interaction between users and items as sequential data and apply the recurrent neural network (RNN) model to extract users’ interests. However, these solutions cannot handle the case when the sequence length is relatively long, e.g., over 100. This is because of the vanishing gradient problem of RNN, i.e., the model cannot learn a users’ previous behaviors that are too far away from the current moment. To address this problem, we propose a new Core Interest Network (CIN) model to mitigate the problem of a long sequence in the CTR prediction task with sequential data. In brief, we first extract the core interests of users and then use the refined data as the input of subsequent learning tasks. Extensive evaluations on real dataset show that our CIN model can outperform the state-of-the-art solutions in terms of prediction accuracy.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3118447047",
    "type": "article"
  },
  {
    "title": "A Stochastic Algorithm Based on Reverse Sampling Technique to Fight Against the Cyberbullying",
    "doi": "https://doi.org/10.1145/3441455",
    "publication_date": "2021-03-26",
    "publication_year": 2021,
    "authors": "Ruidong Yan; Yi Li; Deying Li; Yongcai Wang; Yuqing Zhu; Weili Wu",
    "corresponding_authors": "",
    "abstract": "Cyberbullying has caused serious consequences especially for social network users in recent years. However, the challenge is how to fight against the cyberbullying effectively from the algorithmic perspective. In this article, we study the fighting against the cyberbullying problem, i.e., identify an initial witness set with a budget to spread the positive influence to protect the users in a specific target set such that the number of cybervictim users in the target set being activated by the seed set of cyberbullying is minimized. We first formulate this problem and show its NP-hardness. We further prove that the objective function is submodular with respect to the size of witnesses set when we convert the original problem into the maximal version. Then we propose a stochastic approach to solve this maximal version problem based on the Reverse Sampling Technique with a constant factor guarantee. In addition, we provide theoretical analysis and discuss the relationship between the optimal value and the value returned by the proposed algorithm. To evaluate the proposed approach, we implement extensive experiments on synthetic and real datasets. The experimental results show our approach is superior to the comparison methods.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3151405513",
    "type": "article"
  },
  {
    "title": "Predicting Influential Users in Online Social Network Groups",
    "doi": "https://doi.org/10.1145/3441447",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Andrea De Salve; Paolo Mori; Barbara Guidi; Laura Ricci; Roberto Di Pietro",
    "corresponding_authors": "",
    "abstract": "The widespread adoption of Online Social Networks (OSNs), the ever-increasing amount of information produced by their users, and the corresponding capacity to influence markets, politics, and society, have led both industrial and academic researchers to focus on how such systems could be influenced . While previous work has mainly focused on measuring current influential users, contents, or pages on the overall OSNs, the problem of predicting influencers in OSNs has remained relatively unexplored from a research perspective. Indeed, one of the main characteristics of OSNs is the ability of users to create different groups types, as well as to join groups defined by other users, in order to share information and opinions. In this article, we formulate the Influencers Prediction problem in the context of groups created in OSNs, and we define a general framework and an effective methodology to predict which users will be able to influence the behavior of the other ones in a future time period, based on historical interactions that occurred within the group. Our contribution, while rooted in solid rationale and established analytical tools, is also supported by an extensive experimental campaign. We investigate the accuracy of the predictions collecting data concerning the interactions among about 800,000 users from 18 Facebook groups belonging to different categories (i.e., News, Education, Sport, Entertainment, and Work). The achieved results show the quality and viability of our approach. For instance, we are able to predict, on average, for each group, around a third of what an ex-post analysis will show being the 10 most influential members of that group. While our contribution is interesting on its own and—to the best of our knowledge—unique, it is worth noticing that it also paves the way for further research in this field.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3157013114",
    "type": "article"
  },
  {
    "title": "Utility Mining Across Multi-Dimensional Sequences",
    "doi": "https://doi.org/10.1145/3446938",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Wensheng Gan; Jerry Chun‐Wei Lin; Jiexiong Zhang; Hongzhi Yin; Philippe Fournier‐Viger; Han‐Chieh Chao; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Knowledge extraction from database is the fundamental task in database and data mining community, which has been applied to a wide range of real-world applications and situations. Different from the support-based mining models, the utility-oriented mining framework integrates the utility theory to provide more informative and useful patterns. Time-dependent sequence data are commonly seen in real life. Sequence data have been widely utilized in many applications, such as analyzing sequential user behavior on the Web, influence maximization, route planning, and targeted marketing. Unfortunately, all the existing algorithms lose sight of the fact that the processed data not only contain rich features (e.g., occur quantity, risk, and profit), but also may be associated with multi-dimensional auxiliary information, e.g., transaction sequence can be associated with purchaser profile information. In this article, we first formulate the problem of utility mining across multi-dimensional sequences, and propose a novel framework named MDUS to extract &lt;underline&gt;M&lt;/underline&gt;ulti-&lt;underline&gt;D&lt;/underline&gt;imensional &lt;underline&gt;U&lt;/underline&gt;tility-oriented &lt;underline&gt;S&lt;/underline&gt;equential useful patterns. To the best of our knowledge, this is the first study that incorporates the time-dependent sequence-order, quantitative information, utility factor, and auxiliary dimension. Two algorithms respectively named MDUS EM and MDUS SD are presented to address the formulated problem. The former algorithm is based on database transformation, and the later one performs pattern joins and a searching method to identify desired patterns across multi-dimensional sequences. Extensive experiments are carried on six real-life datasets and one synthetic dataset to show that the proposed algorithms can effectively and efficiently discover the useful knowledge from multi-dimensional sequential databases. Moreover, the MDUS framework can provide better insight, and it is more adaptable to real-life situations than the current existing models.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3161798535",
    "type": "article"
  },
  {
    "title": "Exploring BCI Control in Smart Environments",
    "doi": "https://doi.org/10.1145/3450449",
    "publication_date": "2021-05-29",
    "publication_year": 2021,
    "authors": "Lin Yue; Hao Shen; Sen Wang; Robert Boots; Guodong Long; Weitong Chen; Xiaowei Zhao",
    "corresponding_authors": "",
    "abstract": "The brain–computer interface (BCI) control technology that utilizes motor imagery to perform the desired action instead of manual operation will be widely used in smart environments. However, most of the research lacks robust feature representation of multi-channel EEG series, resulting in low intention recognition accuracy. This article proposes an EEG2Image based Denoised-ConvNets (called EID) to enhance feature representation of the intention recognition task. Specifically, we perform signal decomposition, slicing, and image mapping to decrease the noise from the irrelevant frequency bands. After that, we construct the Denoised-ConvNets structure to learn the colorspace and spatial variations of image objects without cropping new training images precisely. Toward further utilizing the color and spatial transformation layers, the colorspace and colored area of image objects have been enhanced and enlarged, respectively. In the multi-classification scenario, extensive experiments on publicly available EEG datasets confirm that the proposed method has better performance than state-of-the-art methods.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3171638912",
    "type": "article"
  },
  {
    "title": "Learning Sentence-to-Hashtags Semantic Mapping for Hashtag Recommendation on Microblogs",
    "doi": "https://doi.org/10.1145/3466876",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Riccardo Cantini; Fabrizio Marozzo; Giovanni Bruno; Paolo Trunfio",
    "corresponding_authors": "",
    "abstract": "The growing use of microblogging platforms is generating a huge amount of posts that need effective methods to be classified and searched. In Twitter and other social media platforms, hashtags are exploited by users to facilitate the search, categorization, and spread of posts. Choosing the appropriate hashtags for a post is not always easy for users, and therefore posts are often published without hashtags or with hashtags not well defined. To deal with this issue, we propose a new model, called HASHET ( HAshtag recommendation using Sentence-to-Hashtag Embedding Translation ), aimed at suggesting a relevant set of hashtags for a given post. HASHET is based on two independent latent spaces for embedding the text of a post and the hashtags it contains. A mapping process based on a multi-layer perceptron is then used for learning a translation from the semantic features of the text to the latent representation of its hashtags. We evaluated the effectiveness of two language representation models for sentence embedding and tested different search strategies for semantic expansion, finding out that the combined use of BERT ( Bidirectional Encoder Representation from Transformer ) and a global expansion strategy leads to the best recommendation results. HASHET has been evaluated on two real-world case studies related to the 2016 United States presidential election and COVID-19 pandemic. The results reveal the effectiveness of HASHET in predicting one or more correct hashtags, with an average F -score up to 0.82 and a recommendation hit-rate up to 0.92. Our approach has been compared to the most relevant techniques used in the literature ( generative models , unsupervised models, and attention-based supervised models ) by achieving up to 15% improvement in F -score for the hashtag recommendation task and 9% for the topic discovery task.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3196471407",
    "type": "article"
  },
  {
    "title": "Toward Understanding and Evaluating Structural Node Embeddings",
    "doi": "https://doi.org/10.1145/3481639",
    "publication_date": "2021-11-15",
    "publication_year": 2021,
    "authors": "Junchen Jin; Mark Heimann; Di Jin; Danai Koutra",
    "corresponding_authors": "",
    "abstract": "While most network embedding techniques model the proximity between nodes in a network, recently there has been significant interest in structural embeddings that are based on node equivalences , a notion rooted in sociology: equivalences or positions are collections of nodes that have similar roles—i.e., similar functions, ties or interactions with nodes in other positions—irrespective of their distance or reachability in the network. Unlike the proximity-based methods that are rigorously evaluated in the literature, the evaluation of structural embeddings is less mature. It relies on small synthetic or real networks with labels that are not perfectly defined, and its connection to sociological equivalences has hitherto been vague and tenuous. With new node embedding methods being developed at a breakneck pace, proper evaluation, and systematic characterization of existing approaches will be essential to progress. To fill in this gap, we set out to understand what types of equivalences structural embeddings capture. We are the first to contribute rigorous intrinsic and extrinsic evaluation methodology for structural embeddings, along with carefully-designed, diverse datasets of varying sizes. We observe a number of different evaluation variables that can lead to different results (e.g., choice of similarity measure, classifier, and label definitions). We find that degree distributions within nodes’ local neighborhoods can lead to simple yet effective baselines in their own right and guide the future development of structural embedding. We hope that our findings can influence the design of further node embedding methods and also pave the way for more comprehensive and fair evaluation of structural embedding methods.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3212258001",
    "type": "article"
  },
  {
    "title": "Domain-Specific Keyword Extraction Using Joint Modeling of Local and Global Contextual Semantics",
    "doi": "https://doi.org/10.1145/3494560",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Muhammad Abulaish; Mohd Fazil; Mohammed J. Zaki",
    "corresponding_authors": "",
    "abstract": "Domain-specific keyword extraction is a vital task in the field of text mining. There are various research tasks, such as spam e-mail classification, abusive language detection, sentiment analysis, and emotion mining, where a set of domain-specific keywords (aka lexicon) is highly effective. Existing works for keyword extraction list all keywords rather than domain-specific keywords from a document corpus. Moreover, most of the existing approaches perform well on formal document corpuses but fail on noisy and informal user-generated content in online social media. In this article, we present a hybrid approach by jointly modeling the local and global contextual semantics of words, utilizing the strength of distributional word representation and contrasting-domain corpus for domain-specific keyword extraction. Starting with a seed set of a few domain-specific keywords, we model the text corpus as a weighted word-graph. In this graph, the initial weight of a node (word) represents its semantic association with the target domain calculated as a linear combination of three semantic association metrics, and the weight of an edge connecting a pair of nodes represents the co-occurrence count of the respective words. Thereafter, a modified PageRank method is applied to the word-graph to identify the most relevant words for expanding the initial set of domain-specific keywords. We evaluate our method over both formal and informal text corpuses (comprising six datasets), and show that it performs significantly better in comparison to state-of-the-art methods. Furthermore, we generalize our approach to handle the language-agnostic case, and show that it outperforms existing language-agnostic approaches.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4205528052",
    "type": "article"
  },
  {
    "title": "Exploiting Higher Order Multi-dimensional Relationships with Self-attention for Author Name Disambiguation",
    "doi": "https://doi.org/10.1145/3502730",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "K Pooja; Samrat Mondal; Joydeep Chandra",
    "corresponding_authors": "",
    "abstract": "Name ambiguity is a prevalent problem in scholarly publications due to the unprecedented growth of digital libraries and number of researchers. An author is identified by their name in the absence of a unique identifier. The documents of an author are mistakenly assigned due to underlying ambiguity, which may lead to an improper assessment of the author. Various efforts have been made in the literature to solve the name disambiguation problem with supervised and unsupervised approaches. The unsupervised approaches for author name disambiguation are preferred due to the availability of a large amount of unlabeled data. Bibliographic data contain heterogeneous features, thus recently, representation learning-based techniques have been used in literature to embed heterogeneous features in common space. Documents of a scholar are connected by multiple relations. Recently, research has shifted from a single homogeneous relation to multi-dimensional (heterogeneous) relations for the latent representation of document. Connections in graphs are sparse, and higher order links between documents give an additional clue. Therefore, we have used multiple neighborhoods in different relation types in heterogeneous graph for representation of documents. However, different order neighborhood in each relation type has different importance which we have empirically validated also. Therefore, to properly utilize the different neighborhoods in relation type and importance of each relation type in the heterogeneous graph, we propose attention-based multi-dimensional multi-hop neighborhood-based graph convolution network for embedding that uses the two levels of an attention, namely, (i) relation level and (ii) neighborhood level, in each relation. A significant improvement over existing state-of-the-art methods in terms of various evaluation matrices has been obtained by the proposed approach.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4220733515",
    "type": "article"
  },
  {
    "title": "Multi-relation Graph Summarization",
    "doi": "https://doi.org/10.1145/3494561",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Xiangyu Ke; Arijit Khan; Francesco Bonchi",
    "corresponding_authors": "",
    "abstract": "Graph summarization is beneficial in a wide range of applications, such as visualization, interactive and exploratory analysis, approximate query processing, reducing the on-disk storage footprint, and graph processing in modern hardware. However, the bulk of the literature on graph summarization surprisingly overlooks the possibility of having edges of different types. In this article, we study the novel problem of producing summaries of multi-relation networks, i.e., graphs where multiple edges of different types may exist between any pair of nodes. Multi-relation graphs are an expressive model of real-world activities, in which a relation can be a topic in social networks, an interaction type in genetic networks, or a snapshot in temporal graphs. The first approach that we consider for multi-relation graph summarization is a two-step method based on summarizing each relation in isolation, and then aggregating the resulting summaries in some clever way to produce a final unique summary. In doing this, as a side contribution, we provide the first polynomial-time approximation algorithm based on the k -Median clustering for the classic problem of lossless single-relation graph summarization. Then, we demonstrate the shortcomings of these two-step methods, and propose holistic approaches, both approximate and heuristic algorithms, to compute a summary directly for multi-relation graphs. In particular, we prove that the approximation bound of k -Median clustering for the single relation solution can be maintained in a multi-relation graph with proper aggregation operation over adjacency matrices corresponding to its multiple relations. Experimental results and case studies (on co-authorship networks and brain networks) validate the effectiveness and efficiency of the proposed algorithms.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4220897773",
    "type": "article"
  },
  {
    "title": "Stratification of Children with Autism Spectrum Disorder Through Fusion of Temporal Information in Eye-gaze Scan-Paths",
    "doi": "https://doi.org/10.1145/3539226",
    "publication_date": "2022-06-03",
    "publication_year": 2022,
    "authors": "Adham Atyabi; Frederick Shic; Jiajun Jiang; Claire Foster; Erin Barney; Minah Kim; Beibin Li; Pamela Ventola; Chung‐Hao Chen",
    "corresponding_authors": "",
    "abstract": "Background: Looking pattern differences are shown to separate individuals with Autism Spectrum Disorder (ASD) and Typically Developing (TD) controls. Recent studies have shown that, in children with ASD, these patterns change with intellectual and social impairments, suggesting that patterns of social attention provide indices of clinically meaningful variation in ASD. Method: We conducted a naturalistic study of children with ASD (n = 55) and typical development (TD, n = 32). A battery of eye-tracking video stimuli was used in the study, including Activity Monitoring (AM), Social Referencing (SR), Theory of Mind (ToM), and Dyadic Bid (DB) tasks. This work reports on the feasibility of spatial and spatiotemporal scanpaths generated from eye-gaze patterns of these paradigms in stratifying ASD and TD groups. Algorithm: This article presents an approach for automatically identifying clinically meaningful information contained within the raw eye-tracking data of children with ASD and TD. The proposed mechanism utilizes combinations of eye-gaze scan-paths (spatial information), fused with temporal information and pupil velocity data and Convolutional Neural Network (CNN) for stratification of diagnosis (ASD or TD). Results: Spatial eye-gaze representations in the form of scanpaths in stratifying ASD and TD (ASD vs. TD: DNN: 74.4%) are feasible. These spatial eye-gaze features, e.g., scan-paths, are shown to be sensitive to factors mediating heterogeneity in ASD: age (ASD: 2–4 y/old vs. 10–17 y/old CNN: 80.5%), gender (Male vs. Female ASD: DNN: 78.0%) and the mixture of age and gender (5–9 y/old Male vs. 5–9 y/old Female ASD: DNN:98.8%). Limiting scan-path representations temporally increased variance in stratification performance, attesting to the importance of the temporal dimension of eye-gaze data. Spatio-Temporal scan-paths that incorporate velocity of eye movement in their images of eye-gaze are shown to outperform other feature representation methods achieving classification accuracy of 80.25%. Conclusion: The results indicate the feasibility of scan-path images to stratify ASD and TD diagnosis in children of varying ages and gender. Infusion of temporal information and velocity data improves the classification performance of our deep learning models. Such novel velocity fused spatio-temporal scan-path features are shown to be able to capture eye gaze patterns that reflect age, gender, and the mixed effect of age and gender, factors that are associated with heterogeneity in ASD and difficulty in identifying robust biomarkers for ASD.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4281956407",
    "type": "article"
  },
  {
    "title": "Static and Streaming Tucker Decomposition for Dense Tensors",
    "doi": "https://doi.org/10.1145/3568682",
    "publication_date": "2022-10-20",
    "publication_year": 2022,
    "authors": "Jun-Gi Jang; U Kang",
    "corresponding_authors": "",
    "abstract": "Given a dense tensor, how can we efficiently discover hidden relations and patterns in static and online streaming settings? Tucker decomposition is a fundamental tool to analyze multidimensional arrays in the form of tensors. However, existing Tucker decomposition methods in both static and online streaming settings have limitations of efficiency since they directly deal with large dense tensors for the result of Tucker decomposition. In a static setting, although few static methods have tried to reduce their time cost by sampling tensors, sketching tensors, and efficient matrix operations, there remains a need for an efficient method. Moreover, streaming versions of Tucker decomposition are still time-consuming to deal with newly arrived tensors. We propose D-Tucker and D-TuckerO, efficient Tucker decomposition methods for large dense tensors in static and online streaming settings, respectively. By decomposing a given large dense tensor with randomized singular value decomposition, avoiding the reconstruction from SVD results, and carefully determining the order of operations, D-Tucker and D-TuckerO efficiently obtain factor matrices and core tensor. Experimental results show that D-Tucker achieves up to 38.4 × faster running times, and requires up to 17.2 × less space than existing methods while having similar accuracy. Furthermore, D-TuckerO is up to 6.1× faster than existing streaming methods for each newly arrived tensor while its running time is proportional to the size of the newly arrived tensor, not the accumulated tensor.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4306873621",
    "type": "article"
  },
  {
    "title": "Auto-STGCN: Autonomous Spatial-Temporal Graph Convolutional Network Search",
    "doi": "https://doi.org/10.1145/3571285",
    "publication_date": "2022-12-01",
    "publication_year": 2022,
    "authors": "Chunnan Wang; Kaixin Zhang; Hongzhi Wang; Bozhou Chen",
    "corresponding_authors": "",
    "abstract": "In recent years, many spatial-temporal graph convolutional network (STGCN) models are proposed to deal with the spatial-temporal network data forecasting problem. These STGCN models have their own advantages, i.e., each of them puts forward many effective operations and achieves good prediction results in the real applications. If users can effectively utilize and combine these excellent operations integrating the advantages of existing models, then they may obtain more effective STGCN models thus create greater value using existing work. However, they fail to do so due to the lack of domain knowledge, and there is lack of automated system to help users to achieve this goal. In this article, we fill this gap and propose Auto-STGCN algorithm, which makes use of existing models to automatically explore high-performance STGCN model for specific scenarios. Specifically, we design Unified-STGCN framework, which summarizes the operations of existing architectures, and use parameters to control the usage and characteristic attributes of each operation, so as to realize the parameterized representation of the STGCN architecture and the reorganization and fusion of advantages. Then, we present Auto-STGCN, an optimization method based on reinforcement learning, to quickly search the parameter search space provided by Unified-STGCN, and generate optimal STGCN models automatically. Extensive experiments on real-world benchmark datasets show that our Auto-STGCN can find STGCN models superior to existing STGCN models used for search space construction, which demonstrates the effectiveness of our proposed method.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4311057991",
    "type": "article"
  },
  {
    "title": "Edge-enhanced Global Disentangled Graph Neural Network for Sequential Recommendation",
    "doi": "https://doi.org/10.1145/3577928",
    "publication_date": "2023-02-06",
    "publication_year": 2023,
    "authors": "Yunyi Li; Yongjing Hao; Pengpeng Zhao; Guanfeng Liu; Yanchi Liu; Victor S. Sheng; Xiaofang Zhou",
    "corresponding_authors": "",
    "abstract": "Sequential recommendation has been a widely popular topic of recommender systems. Existing works have contributed to enhancing the prediction ability of sequential recommendation systems based on various methods, such as recurrent networks and self-attention mechanisms. However, they fail to discover and distinguish various relationships between items, which could be underlying factors which motivate user behaviors. In this article, we propose an Edge-Enhanced Global Disentangled Graph Neural Network (EGD-GNN) model to capture the relation information between items for global item representation and local user intention learning. At the global level, we build a global-link graph over all sequences to model item relationships. Then a channel-aware disentangled learning layer is designed to decompose edge information into different channels, which can be aggregated to represent the target item from its neighbors. At the local level, we apply a variational auto-encoder framework to learn user intention over the current sequence. We evaluate our proposed method on three real-world datasets. Experimental results show that our model can get a crucial improvement over state-of-the-art baselines and is able to distinguish item features.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4319315274",
    "type": "article"
  },
  {
    "title": "Road Network Representation Learning: A Dual Graph-based Approach",
    "doi": "https://doi.org/10.1145/3592859",
    "publication_date": "2023-04-14",
    "publication_year": 2023,
    "authors": "Liang Zhang; Cheng Long",
    "corresponding_authors": "",
    "abstract": "Road network is a critical infrastructure powering many applications including transportation, mobility and logistics in real life. To leverage the input of a road network across these different applications, it is necessary to learn the representations of the roads in the form of vectors, which is named road network representation learning (RNRL). While several models have been proposed for RNRL, they capture the pairwise relationships/connections among roads only (i.e., as a simple graph), and fail to capture among roads the high-order relationships (e.g., those roads that jointly form a local region usually have similar features such as speed limit) and long-range relationships (e.g., some roads that are far apart may have similar semantics such as being roads in residential areas). Motivated by this, we propose to construct a hypergraph , where each hyperedge corresponds to a set of multiple roads forming a region. The constructed hypergraph would naturally capture the high-order relationships among roads with hyperedges. We then allow information propagation via both the edges in the simple graph and the hyperedges in the hypergraph in a graph neural network context. In addition, we introduce different pretext tasks based on both the simple graph (i.e., graph reconstruction) and the hypergraph (including hypergraph reconstruction and hyperedge classification) for optimizing the representations of roads. The graph reconstruction and hypergraph reconstruction tasks are conventional ones and can capture structural information. The hyperedge classification task can capture long-range relationships between pairs of roads that belong to hyperedges with the same label. We call the resulting model HyperRoad . We further extend HyperRoad to problem settings when additional inputs of road attributes and/or trajectories that are generated on the roads are available. We conduct extensive experiments on two real datasets, for five downstream tasks, and under four problem settings, which demonstrate that our model achieves impressive improvements compared with existing baselines across datasets, tasks, problem settings, and performance metrics. CCS Concepts: • Information systems → Data mining ; • Urban computing ; • Spatial-temporal systems ;",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4365509842",
    "type": "article"
  },
  {
    "title": "Towards a Better Tradeoff between Quality and Efficiency of Community Detection: An Inductive Embedding Method across Graphs",
    "doi": "https://doi.org/10.1145/3596605",
    "publication_date": "2023-05-08",
    "publication_year": 2023,
    "authors": "Meng Qin; Chaorui Zhang; Bo Bai; Gong Zhang; Dit‐Yan Yeung",
    "corresponding_authors": "",
    "abstract": "Many network applications can be formulated as NP-hard combinatorial optimization problems of community detection (CD) that partitions nodes of a graph into several groups with dense linkage. Most existing CD methods are transductive , which independently optimized their models for each single graph, and can only ensure either high quality or efficiency of CD by respectively using advanced machine learning techniques or fast heuristic approximation. In this study, we consider the CD task and aims to alleviate its NP-hard challenge. Motivated by the efficient inductive inference of graph neural networks (GNNs), we explore the possibility to achieve a better tradeoff between the quality and efficiency of CD via an inductive embedding scheme across multiple graphs of a system and propose a novel inductive community detection (ICD) method. Concretely, ICD first conducts the offline training of an adversarial dual GNN structure on historical graphs to capture key properties of a system. The trained model is then directly generalized to new graphs of the same system for online CD without additional optimization, where a better tradeoff between quality and efficiency can be achieved. Compared with existing inductive approaches, we develop a novel feature extraction module based on graph coarsening, which can efficiently extract informative feature inputs for GNNs. Moreover, our original designs of adversarial dual GNN and clustering regularization loss further enable ICD to capture permutation-invariant community labels in the offline training and help derive community-preserved embedding to support the high-quality online CD. Experiments on a set of benchmarks demonstrate that ICD can achieve a significant tradeoff between quality and efficiency over various baselines.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4375861821",
    "type": "article"
  },
  {
    "title": "A Dynamic Attributes-driven Graph Attention Network Modeling on Behavioral Finance for Stock Prediction",
    "doi": "https://doi.org/10.1145/3611311",
    "publication_date": "2023-07-28",
    "publication_year": 2023,
    "authors": "Qiuyue Zhang; Yunfeng Zhang; Xunxiang Yao; Shilong Li; Caiming Zhang; Пэйдэ Лю",
    "corresponding_authors": "",
    "abstract": "Stock prediction is a challenging task due to multiple influencing factors and complex market dependencies. Traditional solutions are based on a single type of information. With the success of multi-source information in different fields, the combination of different types of information such as numerical and textual information has become a promising option. Although multi-source information provides rich multi-view information, how to mine and construct structured relationships from them is a difficult problem. Specifically, most existing methods usually extract features from commonly used multi-source information as predictive information sources, without further pre-constructing stock relationship graphs with dependencies using broader information. More importantly, they typically treat each stock as an isolated forecasting, or employ stock market correlations based on a fixed predefined graph structure, but current methods are not sensitive enough to aggregate the attribute features extracted from multi-source information and stock relationship graph, to obtain the dynamic update of market relations and relationship strength. The stock market is highly temporally, and the attributes of nodes are affected by the time perception of other attributes, which is not fully considered. To address these problems, we propose a novel dynamic attributes-driven graph attention networks incorporating sentiment (DGATS) information, transaction data, and text data. Inspired by behavioral finance, we separately extract sentiment information as a factor of technical indicators, and further realize the early fusion of technical indicators and textual data through Kronecker product-based tensor fusion. In particular, by LSTM and temporal attention network, the short-term and long-term transition features are gradually grasped from the local composition of the fused stock trading sequence. Furthermore, real-time intra-market dependencies and key attributes information are captured with graph networks, enabling dynamic updates of relationships and relationship strengths in predefined graphs. Experiments on the real datasets show that the architecture can outperform the previous methods in prediction performance.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4385349668",
    "type": "article"
  },
  {
    "title": "Pre-training Question Embeddings for Improving Knowledge Tracing with Self-supervised Bi-graph Co-contrastive Learning",
    "doi": "https://doi.org/10.1145/3638055",
    "publication_date": "2023-12-19",
    "publication_year": 2023,
    "authors": "Wentao Wang; Huifang Ma; Yan Zhao; Zhixin Li",
    "corresponding_authors": "",
    "abstract": "Learning high-quality vector representations (aka. embeddings) of educational questions lies at the core of knowledge tracing (KT), which defines a task of estimating students’ knowledge states by predicting the probability that they correctly answer questions. Although existing KT efforts have leveraged question information to achieve remarkable improvements, most of them learn question embeddings by following the supervised learning paradigm. In this article, we propose a novel question embedding pre-training method for improving knowledge tracing with self-supervised Bi -graph Co -contrastive learning ( BiCo ). Technically, on the basis of self-supervised learning paradigm, we first select two similar but distinct views (i.e., representing objective and subjective semantic perspectives) as the semantic source of question embeddings. Then, we design a primary task (structure recovery) together with two auxiliary tasks (question difficulty recovery and contrastive learning) to further enhance the representativeness of questions. Finally, extensive experiments conducted on two real-world datasets show BiCo has a higher expressive power that enables KT methods to effectively predict students’ performances.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4389950236",
    "type": "article"
  },
  {
    "title": "Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach",
    "doi": "https://doi.org/10.1145/3643821",
    "publication_date": "2024-01-30",
    "publication_year": 2024,
    "authors": "Wentao Hu; Hui Fang",
    "corresponding_authors": "",
    "abstract": "With increasing frequency of high-profile privacy breaches in various online platforms, users are becoming more concerned about their privacy. And recommender system is the core component of online platforms for providing personalized service, consequently, its privacy preservation has attracted great attention. As the gold standard of privacy protection, differential privacy has been widely adopted to preserve privacy in recommender systems. However, existing differentially private recommender systems only consider static and independent interactions, so they cannot apply to sequential recommendation where behaviors are dynamic and dependent. Meanwhile, little attention has been paid on the privacy risk of sensitive user features, most of them only protect user feedbacks. In this work, we propose a novel DIfferentially Private Sequential recommendation framework with a noisy Graph Neural Network approach (denoted as DIPSGNN) to address these limitations. To the best of our knowledge, we are the first to achieve differential privacy in sequential recommendation with dependent interactions. Specifically, in DIPSGNN, we first leverage piecewise mechanism to protect sensitive user features. Then, we innovatively add calibrated noise into aggregation step of graph neural network based on aggregation perturbation mechanism. And, this noisy graph neural network can protect sequentially dependent interactions and capture user preferences simultaneously. Extensive experiments demonstrate the superiority of our method over state-of-the-art differentially private recommender systems in terms of better balance between privacy and accuracy.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391361120",
    "type": "article"
  },
  {
    "title": "Semi-supervised Multi-view Clustering based on NMF with Fusion Regularization",
    "doi": "https://doi.org/10.1145/3653022",
    "publication_date": "2024-03-18",
    "publication_year": 2024,
    "authors": "Guosheng Cui; Ruxin Wang; Dan Wu; Ye Li",
    "corresponding_authors": "",
    "abstract": "Multi-view clustering has attracted significant attention and application. Nonnegative matrix factorization is one popular feature of learning technology in pattern recognition. In recent years, many semi-supervised nonnegative matrix factorization algorithms were proposed by considering label information, which has achieved outstanding performance for multi-view clustering. However, most of these existing methods have either failed to consider discriminative information effectively or included too much hyper-parameters. Addressing these issues, a semi-supervised multi-view nonnegative matrix factorization with a novel fusion regularization (FRSMNMF) is developed in this article. In this work, we uniformly constrain alignment of multiple views and discriminative information among clusters with designed fusion regularization. Meanwhile, to align the multiple views effectively, two kinds of compensating matrices are used to normalize the feature scales of different views. Additionally, we preserve the geometry structure information of labeled and unlabeled samples by introducing the graph regularization simultaneously. Due to the proposed methods, two effective optimization strategies based on multiplicative update rules are designed. Experiments implemented on six real-world datasets have demonstrated the effectiveness of our FRSMNMF comparing with several state-of-the-art unsupervised and semi-supervised approaches.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4392915601",
    "type": "article"
  },
  {
    "title": "NOODLE: Joint Cross-View Discrepancy Discovery and High-Order Correlation Detection for Multi-View Subspace Clustering",
    "doi": "https://doi.org/10.1145/3653305",
    "publication_date": "2024-03-20",
    "publication_year": 2024,
    "authors": "Zhibin Gu; Songhe Feng; Zhendong Li; Jiazheng Yuan; Jun Liu",
    "corresponding_authors": "",
    "abstract": "Benefiting from the effective exploration of the valuable topological pair-wise relationship of data points across multiple views, multi-view subspace clustering (MVSC) has received increasing attention in recent years. However, we observe that existing MVSC approaches still suffer from two limitations that need to be further improved to enhance the clustering effectiveness. Firstly, previous MVSC approaches mainly prioritize extracting multi-view consistency, often neglecting the cross-view discrepancy that may arise from noise, outliers, and view-inherent properties. Secondly, existing techniques are constrained by their reliance on pair-wise sample correlation and pair-wise view correlation, failing to capture the high-order correlations that are enclosed within multiple views. To address these issues, we propose a novel MVSC framework via joi N t cr O ss-view discrepancy disc O very an D high-order corre L ation d E tection ( NOODLE ), seeking an informative target subspace representation compatible across multiple features to facilitate the downstream clustering task. Specifically, we first exploit the self-representation mechanism to learn multiple view-specific affinity matrices, which are further decomposed into cohesive factors and incongruous factors to fit the multi-view consistency and discrepancy, respectively. Additionally, an explicit cross-view sparse regularization is applied to incoherent parts, ensuring the consistency and discrepancy to be precisely separated from the initial subspace representations. Meanwhile, the multiple cohesive parts are stacked into a three-dimensional tensor associated with a tensor-Singular Value Decomposition (t-SVD) based weighted tensor nuclear norm constraint, enabling effective detection of the high-order correlations implicit in multi-view data. Our proposed method outperforms state-of-the-art methods for multi-view clustering on six benchmark datasets, demonstrating its effectiveness.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4393001925",
    "type": "article"
  },
  {
    "title": "Hierarchical Convolutional Neural Network with Knowledge Complementation for Long-Tailed Classification",
    "doi": "https://doi.org/10.1145/3653717",
    "publication_date": "2024-03-22",
    "publication_year": 2024,
    "authors": "Hong Zhao; Zhengyu Li; Wenwei He; Zhao Yan",
    "corresponding_authors": "",
    "abstract": "Existing methods based on transfer learning leverage auxiliary information to help tail generalization and improve the performance of the tail classes. However, they cannot fully exploit the relationships between auxiliary information and tail classes and bring irrelevant knowledge to the tail classes. To solve this problem, we propose a hierarchical CNN with knowledge complementation, which regards hierarchical relationships as auxiliary information and transfers relevant knowledge to tail classes. First, we integrate semantics and clustering relationships as hierarchical knowledge into the CNN to guide feature learning. Then, we design a complementary strategy to jointly exploit the two types of knowledge, where semantic knowledge acts as a prior dependence and clustering knowledge reduces the negative information caused by excessive semantic dependence (i.e., semantic gaps). In this way, the CNN facilitates the utilization of the two complementary hierarchical relationships and transfers useful knowledge to tail data to improve long-tailed classification accuracy. Experimental results on public benchmarks show that the proposed model outperforms existing methods. In particular, our model improves accuracy by 3.46% compared with the second-best method on the long-tailed tieredImageNet dataset.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4393085415",
    "type": "article"
  },
  {
    "title": "A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models",
    "doi": "https://doi.org/10.1145/3674840",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "H. Zhang; Zitao Liu; Chenming Shang; D. Li; Yong Jiang",
    "corresponding_authors": "",
    "abstract": "Knowledge tracing (KT) plays a crucial role in predicting students’ future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in modeling the individual question information. This is crucial because students’ knowledge acquisition on questions that share the same set of knowledge components (KCs) may vary significantly. However, due to the large question bank, the average number of interactions per question may not be sufficient. This limitation can potentially result in overfitting of the question embedding and inaccurate question knowledge acquisition state that relies on its corresponding question representation. Furthermore, there is a considerable portion of questions receiving relatively less interaction from students in comparison to the majority of questions. This can further increase the risk of overfitting and lower the accuracy of the obtained question knowledge acquisition state. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model’s prediction results in a manner that teachers find interpretable. This makes teachers accept the rationale behind the prediction results and utilize them to design teaching activities and tailored learning strategies for students. However, the inherent black-box nature of deep learning techniques often poses a hurdle for teachers to fully embrace the model’s prediction results. To address these challenges, we propose a Question-centric Multi-experts Contrastive Learning framework for KT called Q-MCKT. This framework explicitly models students’ knowledge acquisition state at both the question and concept levels. It leverages the mixture of experts technique to capture a more robust and accurate knowledge acquisition state in both question and concept levels for prediction. Additionally, a fine-grained question-centric contrastive learning task is introduced to enhance the representations of less interactive questions and improve the accuracy of their corresponding question knowledge acquisition states. Moreover, Q-MCKT utilizes an item response theory-based prediction layer to generate interpretable prediction results based on the knowledge acquisition states obtained from the question and concept knowledge acquisition modules. We evaluate the proposed Q-MCKT framework on four public real-world educational datasets. The experimental results demonstrate that our approach outperforms a wide range of deep learning-based KT models in terms of prediction accuracy while maintaining better model interpretability. To ensure reproducibility, we have provided all the datasets and code on our website at https://github.com/rattlesnakey/Q-MCKT .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4400112764",
    "type": "article"
  },
  {
    "title": "Toward Cross-Lingual Social Event Detection with Hybrid Knowledge Distillation",
    "doi": "https://doi.org/10.1145/3689948",
    "publication_date": "2024-08-27",
    "publication_year": 2024,
    "authors": "Jiaqian Ren; Hao Peng; Lei Jiang; Zhifeng Hao; Jia Wu; Shengxiang Gao; Zhengtao Yu; Qiang Yang",
    "corresponding_authors": "",
    "abstract": "Recently published graph neural networks (GNNs) show promising performance at social event detection tasks. However, most studies are oriented toward monolingual data in languages with abundant training samples. This has left the common lesser-spoken languages relatively unexplored. Thus, in this work, we present a GNN-based framework that integrates cross-lingual word embeddings into the process of graph knowledge distillation for detecting events in low-resource language data streams. To achieve this, a novel cross-lingual knowledge distillation framework, called CLKD, exploits prior knowledge learned from similar threads in English to make up for the paucity of annotated data. Specifically, to extract sufficient useful knowledge, we propose a hybrid distillation method that consists of both feature-wise and relation-wise information. To transfer both kinds of knowledge in an effective way, we add a cross-lingual module in the feature-wise distillation to eliminate the language gap and selectively choose beneficial relations in the relation-wise distillation to avoid distraction caused by teachers’ misjudgments. Our proposed CLKD framework also adopts different configurations to suit both offline and online situations. Experiments on real-world datasets show that the framework is highly effective at detection in languages where training samples are scarce.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4401913767",
    "type": "article"
  },
  {
    "title": "Dual Perspective of Label-Specific Feature Learning for Multi-Label Classification",
    "doi": "https://doi.org/10.1145/3705006",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "Jun-Yi Hang; Min-Ling Zhang",
    "corresponding_authors": "",
    "abstract": "Label-specific features work as an effective supervised feature manipulation strategy to account for distinct discriminative properties of each class label in multi-label classification. Existing approaches implement this strategy in its primal form, i.e. finding the most pertinent features specific to each class label and directly inducing classifiers on these features. Instead of such a straightforward implementation, a dual perspective for label-specific feature learning is investigated in this paper. As a dual problem of existing primal one, we consider label-specific discriminative properties by identifying non-informative features for each class label and making the discrimination process immutable to variations of identified features. Accordingly, a perturbation-based approach Dela is presented, which endows classifiers with immutability on simultaneously identified non-informative features by solving a probabilistically-relaxed expected risk minimization problem. Furthermore, we touch the realistic issue of label-specific feature learning in a weakly-supervised scenario via extending Dela to accommodate to multi-label data with missing labels. Comprehensive experiments show that our approach outperforms the state-of-the-art counterparts.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4404588996",
    "type": "article"
  },
  {
    "title": "On domination game analysis for microeconomic data mining",
    "doi": "https://doi.org/10.1145/1460797.1460801",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Zhenjie Zhang; Laks V. S. Lakshmanan; Anthony K. H. Tung",
    "corresponding_authors": "",
    "abstract": "Game theory is a powerful tool for analyzing the competitions among manufacturers in a market. In this article, we present a study on combining game theory and data mining by introducing the concept of domination game analysis. We present a multidimensional market model, where every dimension represents one attribute of a commodity. Every product or customer is represented by a point in the multidimensional space, and a product is said to “dominate” a customer if all of its attributes can satisfy the requirements of the customer. The expected market share of a product is measured by the expected number of the buyers in the customers, all of which are equally likely to buy any product dominating him. A Nash equilibrium is a configuration of the products achieving stable expected market shares for all products. We prove that Nash equilibrium in such a model can be computed in polynomial time if every manufacturer tries to modify its product in a round robin manner. To further improve the efficiency of the computation, we also design two algorithms for the manufacturers to efficiently find their best response to other products in the market.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2055028639",
    "type": "article"
  },
  {
    "title": "ACM TKDD Special Issue on Knowledge Discovery for Web Intelligence",
    "doi": "https://doi.org/10.1145/1870096.1870097",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Ning Zhong; Gregory Piatetsky-Shapiro; Yiyu Yao; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1990461944",
    "type": "article"
  },
  {
    "title": "Forecasting in the NBA and other team sports",
    "doi": "https://doi.org/10.1145/2362383.2362387",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Pedro O. S. Vaz de Melo; Virgı́lio Almeida; Antônio A. F. Loureiro; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "The multi-million sports-betting market is based on the fact that the task of predicting the outcome of a sports event is very hard. Even with the aid of an uncountable number of descriptive statistics and background information, only a few can correctly guess the outcome of a game or a league. In this work, our approach is to move away from the traditional way of predicting sports events, and instead to model sports leagues as networks of players and teams where the only information available is the work relationships among them. We propose two network-based models to predict the behavior of teams in sports leagues. These models are parameter-free, that is, they do not have a single parameter, and moreover are sport-agnostic: they can be applied directly to any team sports league. First, we view a sports league as a network in evolution, and we infer the implicit feedback behind network changes and properties over the years. Then, we use this knowledge to construct the network-based prediction models, which can, with a significantly high probability, indicate how well a team will perform over a season. We compare our proposed models with other prediction models in two of the most popular sports leagues: the National Basketball Association (NBA) and the Major League Baseball (MLB). Our model shows consistently good results in comparison with the other models and, relying upon the network properties of the teams, we achieved a ≈ 14% rank prediction accuracy improvement over our best competitor.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2069473687",
    "type": "article"
  },
  {
    "title": "Social trust prediction using heterogeneous networks",
    "doi": "https://doi.org/10.1145/2541268.2541270",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Jin Huang; Feiping Nie; Heng Huang; Yi-Cheng Tu; Lei Yu",
    "corresponding_authors": "",
    "abstract": "Along with increasing popularity of social websites, online users rely more on the trustworthiness information to make decisions, extract and filter information, and tag and build connections with other users. However, such social network data often suffer from severe data sparsity and are not able to provide users with enough information. Therefore, trust prediction has emerged as an important topic in social network research. Traditional approaches are primarily based on exploring trust graph topology itself. However, research in sociology and our life experience suggest that people who are in the same social circle often exhibit similar behaviors and tastes. To take advantage of the ancillary information for trust prediction, the challenge then becomes what to transfer and how to transfer. In this article, we address this problem by aggregating heterogeneous social networks and propose a novel joint social networks mining (JSNM) method. Our new joint learning model explores the user-group-level similarity between correlated graphs and simultaneously learns the individual graph structure; therefore, the shared structures and patterns from multiple social networks can be utilized to enhance the prediction tasks. As a result, we not only improve the trust prediction in the target graph but also facilitate other information retrieval tasks in the auxiliary graphs. To optimize the proposed objective function, we use the alternative technique to break down the objective function into several manageable subproblems. We further introduce the auxiliary function to solve the optimization problems with rigorously proved convergence. The extensive experiments have been conducted on both synthetic and real- world data. All empirical results demonstrate the effectiveness of our method.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1972300965",
    "type": "article"
  },
  {
    "title": "A Generic Multilabel Learning-Based Classification Algorithm Recommendation Method",
    "doi": "https://doi.org/10.1145/2629474",
    "publication_date": "2014-10-09",
    "publication_year": 2014,
    "authors": "Guangtao Wang; Qinbao Song; Xueying Zhang; Kaiyuan Zhang",
    "corresponding_authors": "",
    "abstract": "As more and more classification algorithms continue to be developed, recommending appropriate algorithms to a given classification problem is increasingly important. This article first distinguishes the algorithm recommendation methods by two dimensions: (1) meta-features, which are a set of measures used to characterize the learning problems, and (2) meta-target, which represents the relative performance of the classification algorithms on the learning problem. In contrast to the existing algorithm recommendation methods whose meta-target is usually in the form of either the ranking of candidate algorithms or a single algorithm, this article proposes a new and natural multilabel form to describe the meta-target. This is due to the fact that there would be multiple algorithms being appropriate for a given problem in practice. Furthermore, a novel multilabel learning-based generic algorithm recommendation method is proposed, which views the algorithm recommendation as a multilabel learning problem and solves the problem by the mature multilabel learning algorithms. To evaluate the proposed multilabel learning-based recommendation method, extensive experiments with 13 well-known classification algorithms, two kinds of meta-targets such as algorithm ranking and single algorithm, and five different kinds of meta-features are conducted on 1,090 benchmark learning problems. The results show the effectiveness of our proposed multilabel learning-based recommendation method.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2027308928",
    "type": "article"
  },
  {
    "title": "Anomaly Detection from Incomplete Data",
    "doi": "https://doi.org/10.1145/2629668",
    "publication_date": "2014-09-23",
    "publication_year": 2014,
    "authors": "Siyuan Liu; Lei Chen; Lionel M. Ni",
    "corresponding_authors": "",
    "abstract": "Anomaly detection (a.k.a., outlier or burst detection) is a well-motivated problem and a major data mining and knowledge discovery task. In this article, we study the problem of population anomaly detection, one of the key issues related to event monitoring and population management within a city. Through studying detected population anomalies, we can trace and analyze these anomalies, which could help to model city traffic design and event impact analysis and prediction. Although a significant and interesting issue, it is very hard to detect population anomalies and retrieve anomaly trajectories, especially given that it is difficult to get actual and sufficient population data. To address the difficulties of a lack of real population data, we take advantage of mobile phone networks, which offer enormous spatial and temporal communication data on persons. More importantly, we claim that we can utilize these mobile phone data to infer and approximate population data. Thus, we can study the population anomaly detection problem by taking advantages of unique features hidden in mobile phone data. In this article, we present a system to conduct Population Anomaly Detection (PAD). First, we propose an effective clustering method, correlation-based clustering , to cluster the incomplete location information from mobile phone data (i.e., from mobile call volume distribution to population density distribution). Then, we design an adaptive parameter-free detection method, R-scan , to capture the distributed dynamic anomalies. Finally, we devise an efficient algorithm, BT-miner , to retrieve anomaly trajectories . The experimental results from real-life mobile phone data confirm the effectiveness and efficiency of the proposed algorithms. Finally, the proposed methods are realized as a pilot system in a city in China.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2014211872",
    "type": "article"
  },
  {
    "title": "Occupancy-Based Frequent Pattern Mining <sup>*</sup>",
    "doi": "https://doi.org/10.1145/2753765",
    "publication_date": "2015-10-12",
    "publication_year": 2015,
    "authors": "Lei Zhang; Ping Luo; Linpeng Tang; Enhong Chen; Qi Liu; Min Wang; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "Frequent pattern mining is an important data mining problem with many broad applications. Most studies in this field use support (frequency) to measure the popularity of a pattern, namely the fraction of transactions or sequences that include the pattern in a data set. In this study, we introduce a new interesting measure, namely occupancy, to measure the completeness of a pattern in its supporting transactions or sequences. This is motivated by some real-world pattern recommendation applications in which an interesting pattern should not only be frequent, but also occupies a large portion of its supporting transactions or sequences. With the definition of occupancy we call a pattern dominant if its occupancy value is above a user-specified threshold. Then, our task is to identify the qualified patterns which are both dominant and frequent. Also, we formulate the problem of mining top-k qualified patterns , that is, finding k qualified patterns with maximum values on a user-defined function of support and occupancy, for example, weighted sum of support and occupancy. The challenge to these tasks is that the value of occupancy does not change monotonically when more items are appended to a given pattern. Therefore, we propose a general algorithm called DOFRA (DOminant and FRequent pattern mining Algorithm) for mining these qualified patterns, which explores the upper bound properties on occupancy to drastically reduce the search process. Finally, we show the effectiveness of DOFRA in two real-world applications and also demonstrate the efficiency of DOFRA on several real and large synthetic datasets.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2059303863",
    "type": "article"
  },
  {
    "title": "Partitioning Networks with Node Attributes by Compressing Information Flow",
    "doi": "https://doi.org/10.1145/2968451",
    "publication_date": "2016-11-19",
    "publication_year": 2016,
    "authors": "Laura Mazzoli Smith; Linhong Zhu; Kristina Lerman; Allon G. Percus",
    "corresponding_authors": "",
    "abstract": "Real-world networks are often organized as modules or communities of similar nodes that serve as functional units. These networks are also rich in content, with nodes having distinguished features or attributes. In order to discover a network’s modular structure, it is necessary to take into account not only its links but also node attributes. We describe an information-theoretic method that identifies modules by compressing descriptions of information flow on a network. Our formulation introduces node content into the description of information flow, which we then minimize to discover groups of nodes with similar attributes that also tend to trap the flow of information. The method is conceptually simple and does not require ad-hoc parameters to specify the number of modules or to control the relative contribution of links and node attributes to network structure. We apply the proposed method to partition real-world networks with known community structure. We demonstrate that adding node attributes helps recover the underlying community structure in content-rich networks more effectively than using links alone. In addition, we show that our method is faster and more accurate than alternative state-of-the-art algorithms.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1863793842",
    "type": "article"
  },
  {
    "title": "Toward Personalized Context Recognition for Mobile Users",
    "doi": "https://doi.org/10.1145/2629504",
    "publication_date": "2014-09-23",
    "publication_year": 2014,
    "authors": "Baoxing Huai; Enhong Chen; Hengshu Zhu; Hui Xiong; Tengfei Bao; Qi Liu; Jilei Tian",
    "corresponding_authors": "",
    "abstract": "The problem of mobile context recognition targets the identification of semantic meaning of context in a mobile environment. This plays an important role in understanding mobile user behaviors and thus provides the opportunity for the development of better intelligent context-aware services. A key step of context recognition is to model the personalized contextual information of mobile users. Although many studies have been devoted to mobile context modeling, limited efforts have been made on the exploitation of the sequential and dependency characteristics of mobile contextual information. Also, the latent semantics behind mobile context are often ambiguous and poorly understood. Indeed, a promising direction is to incorporate some domain knowledge of common contexts, such as “waiting for a bus” or “having dinner,” by modeling both labeled and unlabeled context data from mobile users because there are often few labeled contexts available in practice. To this end, in this article, we propose a sequence-based semisupervised approach to modeling personalized context for mobile users. Specifically, we first exploit the Bayesian Hidden Markov Model (B-HMM) for modeling context in the form of probabilistic distributions and transitions of raw context data. Also, we propose a sequential model by extending B-HMM with the prior knowledge of contextual features to model context more accurately. Then, to efficiently learn the parameters and initial values of the proposed models, we develop a novel approach for parameter estimation by integrating the Dirichlet Process Mixture (DPM) model and the Mixture Unigram (MU) model. Furthermore, by incorporating both user-labeled and unlabeled data, we propose a semisupervised learning-based algorithm to identify and model the latent semantics of context. Finally, experimental results on real-world data clearly validate both the efficiency and effectiveness of the proposed approaches for recognizing personalized context of mobile users.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2034549917",
    "type": "article"
  },
  {
    "title": "ASCOS++",
    "doi": "https://doi.org/10.1145/2776894",
    "publication_date": "2015-10-12",
    "publication_year": 2015,
    "authors": "Hung‐Hsuan Chen; C. Lee Giles",
    "corresponding_authors": "",
    "abstract": "In this article, we explore the relationships among digital objects in terms of their similarity based on vertex similarity measures. We argue that SimRank—a famous similarity measure—and its families, such as P-Rank and SimRank++, fail to capture similar node pairs in certain conditions, especially when two nodes can only reach each other through paths of odd lengths. We present new similarity measures ASCOS and ASCOS++ to address the problem. ASCOS outputs a more complete similarity score than SimRank and SimRank’s families. ASCOS++ enriches ASCOS to include edge weight into the measure, giving all edges and network weights an opportunity to make their contribution. We show that both ASCOS++ and ASCOS can be reformulated and applied on a distributed environment for parallel contribution. Experimental results show that ASCOS++ reports a better score than SimRank and several famous similarity measures. Finally, we re-examine previous use cases of SimRank, and explain appropriate and inappropriate use cases. We suggest future SimRank users following the rules proposed here before naïvely applying it. We also discuss the relationship between ASCOS++ and PageRank.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2089056989",
    "type": "article"
  },
  {
    "title": "Smart Multitask Bregman Clustering and Multitask Kernel Clustering",
    "doi": "https://doi.org/10.1145/2747879",
    "publication_date": "2015-07-22",
    "publication_year": 2015,
    "authors": "Xianchao Zhang; Xiaotong Zhang; Han Liu",
    "corresponding_authors": "",
    "abstract": "Traditional clustering algorithms deal with a single clustering task on a single dataset. However, there are many related tasks in the real world, which motivates multitask clustering. Recently some multitask clustering algorithms have been proposed, and among them multitask Bregman clustering (MBC) is a very applicable method. MBC alternatively updates clusters and learns relationships between clusters of different tasks, and the two phases boost each other. However, the boosting does not always have positive effects on improving the clustering performance, it may also cause negative effects. Another issue of MBC is that it cannot deal with nonlinear separable data. In this article, we show that in MBC, the process of using cluster relationship to boost the cluster updating phase may cause negative effects, that is, cluster centroids may be skewed under some conditions. We propose a smart multitask Bregman clustering (S-MBC) algorithm which can identify the negative effects of the boosting and avoid the negative effects if they occur. We then propose a multitask kernel clustering (MKC) framework for nonlinear separable data by using a similar framework like MBC in the kernel space. We also propose a specific optimization method, which is quite different from that of MBC, to implement the MKC framework. Since MKC can also cause negative effects like MBC, we further extend the framework of MKC to a smart multitask kernel clustering (S-MKC) framework in a similar way that S-MBC is extended from MBC. We conduct experiments on 10 real world multitask clustering datasets to evaluate the performance of S-MBC and S-MKC. The results on clustering accuracy show that: (1) compared with the original MBC algorithm MBC, S-MBC and S-MKC perform much better; (2) compared with the convex discriminative multitask relationship clustering (DMTRC) algorithms DMTRC-L and DMTRC-R which also avoid negative transfer, S-MBC and S-MKC perform worse in the (ideal) case in which different tasks have the same cluster number and the empirical label marginal distribution in each task distributes evenly, but better or comparable in other (more general) cases. Moreover, S-MBC and S-MKC can work on the datasets in which different tasks have different number of clusters, violating the assumptions of DMTRC-L and DMTRC-R. The results on efficiency show that S-MBC and S-MKC consume more computational time than MBC and less computational time than DMTRC-L and DMTRC-R. Overall S-MBC and S-MKC are competitive compared with the state-of-the-art multitask clustering algorithms in synthetical terms of accuracy, efficiency and applicability.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2198858367",
    "type": "article"
  },
  {
    "title": "Joint Representation Learning for Location-Based Social Networks with Multi-Grained Sequential Contexts",
    "doi": "https://doi.org/10.1145/3127875",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Wayne Xin Zhao; Feifan Fan; Ji-Rong Wen; Edward Yi Chang",
    "corresponding_authors": "",
    "abstract": "This article studies the problem of learning effective representations for Location-Based Social Networks (LBSN), which is useful in many tasks such as location recommendation and link prediction. Existing network embedding methods mainly focus on capturing topology patterns reflected in social connections, while check-in sequences, the most important data type in LBSNs, are not directly modeled by these models. In this article, we propose a representation learning method for LBSNs called as JRLM++, which models check-in sequences together with social connections. To capture sequential relatedness, JRLM++ characterizes two levels of sequential contexts, namely fine-grained and coarse-grained contexts. We present a learning algorithm tailored to the hierarchical architecture of the proposed model. We conduct extensive experiments on two important applications using real-world datasets. The experimental results demonstrate the superiority of our model. The proposed model can generate effective representations for both users and locations in the same embedding space, which can be further utilized to improve multiple LBSN tasks.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2784528539",
    "type": "article"
  },
  {
    "title": "Reconstructing Graphs from Neighborhood Data",
    "doi": "https://doi.org/10.1145/2641761",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "Dóra Erdös; Rainer Gemulla; Evimaria Terzi",
    "corresponding_authors": "",
    "abstract": "Consider a social network and suppose that we are only given the number of common friends between each pair of users. Can we reconstruct the underlying network? Similarly, consider a set of documents and the words that appear in them. If we only know the number of common words for every pair of documents, as well as the number of common documents for every pair of words, can we infer which words appear in which documents? In this article, we develop a general methodology for answering questions like these. We formalize these questions in what we call the R econstruct problem: given information about the common neighbors of nodes in a network, our goal is to reconstruct the hidden binary matrix that indicates the presence or absence of relationships between individual nodes. In fact, we propose two different variants of this problem: one where the number of connections of every node (i.e., the degree of every node) is known and a second one where it is unknown. We call these variants the degree-aware and the degree-oblivious versions of the R econstruct problem, respectively. Our algorithms for both variants exploit the properties of the singular value decomposition of the hidden binary matrix. More specifically, we show that using the available neighborhood information, we can reconstruct the hidden matrix by finding the components of its singular value decomposition and then combining them appropriately. Our extensive experimental study suggests that our methods are able to reconstruct binary matrices of different characteristics with up to 100% accuracy.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2062435795",
    "type": "article"
  },
  {
    "title": "A separability framework for analyzing community structure",
    "doi": "https://doi.org/10.1145/2527231",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Bruno Abrahão; Sucheta Soundarajan; John E. Hopcroft; Robert Kleinberg",
    "corresponding_authors": "",
    "abstract": "Four major factors govern the intricacies of community extraction in networks: (1) the literature offers a multitude of disparate community detection algorithms whose output exhibits high structural variability across the collection, (2) communities identified by algorithms may differ structurally from real communities that arise in practice, (3) there is no consensus characterizing how to discriminate communities from noncommunities, and (4) the application domain includes a wide variety of networks of fundamentally different natures. In this article, we present a class separability framework to tackle these challenges through a comprehensive analysis of community properties. Our approach enables the assessment of the structural dissimilarity among the output of multiple community detection algorithms and between the output of algorithms and communities that arise in practice. In addition, our method provides us with a way to organize the vast collection of community detection algorithms by grouping those that behave similarly. Finally, we identify the most discriminative graph-theoretical properties of community signature and the small subset of properties that account for most of the biases of the different community detection algorithms. We illustrate our approach with an experimental analysis, which reveals nuances of the structure of real and extracted communities. In our experiments, we furnish our framework with the output of 10 different community detection procedures, representative of categories of popular algorithms available in the literature, applied to a diverse collection of large-scale real network datasets whose domains span biology, online shopping, and social systems. We also analyze communities identified by annotations that accompany the data, which reflect exemplar communities in various domain. We characterize these communities using a broad spectrum of community properties to produce the different structural classes. As our experiments show that community structure is not a universal concept, our framework enables an informed choice of the most suitable community detection method for identifying communities of a specific type in a given network and allows for a comparison of existing community detection algorithms while guiding the design of new ones.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2079149066",
    "type": "article"
  },
  {
    "title": "Rationality Analytics from Trajectories",
    "doi": "https://doi.org/10.1145/2735634",
    "publication_date": "2015-07-22",
    "publication_year": 2015,
    "authors": "Siyuan Liu; Qiang Qu; Shuhui Wang",
    "corresponding_authors": "",
    "abstract": "The availability of trajectories tracking the geographical locations of people as a function of time offers an opportunity to study human behaviors. In this article, we study rationality from the perspective of user decision on visiting a point of interest (POI) which is represented as a trajectory. However, the analysis of rationality is challenged by a number of issues, for example, how to model a trajectory in terms of complex user decision processes? and how to detect hidden factors that have significant impact on the rational decision making? In this study, we propose Rationality Analysis Model (RAM) to analyze rationality from trajectories in terms of a set of impact factors. In order to automatically identify hidden factors, we propose a method, Collective Hidden Factor Retrieval (CHFR), which can also be generalized to parse multiple trajectories at the same time or parse individual trajectories of different time periods. Extensive experimental study is conducted on three large-scale real-life datasets (i.e., taxi trajectories, user shopping trajectories, and visiting trajectories in a theme park). The results show that the proposed methods are efficient, effective, and scalable. We also deploy a system in a large theme park to conduct a field study. Interesting findings and user feedback of the field study are provided to support other applications in user behavior mining and analysis, such as business intelligence and user management for marketing purposes.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2264718882",
    "type": "article"
  },
  {
    "title": "Spatial-Proximity Optimization for Rapid Task Group Deployment",
    "doi": "https://doi.org/10.1145/2818714",
    "publication_date": "2016-06-14",
    "publication_year": 2016,
    "authors": "Chih-Ya Shen; De-Nian Yang; Wang-Chien Lee; Ming-Syan Chen⋆",
    "corresponding_authors": "",
    "abstract": "Spatial proximity is one of the most important factors for the quick deployment of the task groups in various time-sensitive missions. This article proposes a new spatial query, Spatio-Social Team Query (SSTQ) , that forms a strong task group by considering (1) the group’s spatial distance (i.e., transportation time), (2) skills of the candidate group members, and (3) social rapport among the candidates. Efficient processing of SSTQ is very challenging, because the aforementioned spatial, skill, and social factors need to be carefully examined. In this article, therefore, we first formulate two subproblems of SSTQ, namely Hop-Constrained Team Problem (HCTP) and Connection-Oriented Team Query (COTQ) . HCTP is a decision problem that considers only social and skill dimensions. We prove that HCTP is NP-Complete. Moreover, based on the hardness of HCTP, we prove that SSTQ is NP-Hard and inapproximable within any factor . On the other hand, COTQ is a special case of SSTQ that relaxes the social constraint. We prove that COTQ is NP-Hard and propose an approximation algorithm for COTQ, namely COTprox . Furthermore, based on the observations on COTprox, we devise an approximation algorithm, SSTprox , with a guaranteed error bound for SSTQ. Finally, to efficiently obtain the optimal solution to SSTQ for small instances, we design two efficient algorithms, SpatialFirst and SkillFirst , with different scenarios in mind. These two algorithms incorporate various effective ordering and pruning techniques to reduce the search space for answering SSTQ. Experimental results on real datasets indicate that the proposed algorithms can efficiently answer SSTQ under various parameter settings.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2426056643",
    "type": "article"
  },
  {
    "title": "Enhancing Reputation via Price Discounts in E-Commerce Systems",
    "doi": "https://doi.org/10.1145/3154417",
    "publication_date": "2018-01-10",
    "publication_year": 2018,
    "authors": "Hong Xie; T. B. Richard; John C. S. Lui",
    "corresponding_authors": "",
    "abstract": "Reputation systems have become an indispensable component of modern E-commerce systems, as they help buyers make informed decisions in choosing trustworthy sellers. To attract buyers and increase the transaction volume, sellers need to earn reasonably high reputation scores. This process usually takes a substantial amount of time. To accelerate this process, sellers can provide price discounts to attract users, but the underlying difficulty is that sellers have no prior knowledge on buyers’ preferences over price discounts. In this article, we develop an online algorithm to infer the optimal discount rate from data. We first formulate an optimization framework to select the optimal discount rate given buyers’ discount preferences, which is a tradeoff between the short-term profit and the ramp-up time (for reputation). We then derive the closed-form optimal discount rate, which gives us key insights in applying a stochastic bandits framework to infer the optimal discount rate from the transaction data with regret upper bounds. We show that the computational complexity of evaluating the performance metrics is infeasibly high, and therefore, we develop efficient randomized algorithms with guaranteed performance to approximate them. Finally, we conduct experiments on a dataset crawled from eBay. Experimental results show that our framework can trade 60% of the short-term profit for reducing the ramp-up time by 40%. This reduction in the ramp-up time can increase the long-term profit of a seller by at least 20%.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2782747517",
    "type": "article"
  },
  {
    "title": "Mining Graphlet Counts in Online Social Networks",
    "doi": "https://doi.org/10.1145/3182392",
    "publication_date": "2018-04-16",
    "publication_year": 2018,
    "authors": "Xiaowei Chen; John C. S. Lui",
    "corresponding_authors": "",
    "abstract": "Counting subgraphs is a fundamental analysis task for online social networks (OSNs). Given the sheer size and restricted access of OSN, efficient computation of subgraph counts is highly challenging. Although a number of algorithms have been proposed to estimate the relative counts of subgraphs in OSNs with restricted access, there are only few works which try to solve a more general problem, i.e., counting subgraph frequencies. In this article, we propose an efficient random walk-based framework to estimate the subgraph counts. Our framework generates samples by leveraging consecutive steps of the random walk as well as by observing neighbors of visited nodes. Using the importance sampling technique, we derive unbiased estimators of the subgraph counts. To make better use of the degree information of visited nodes, we also design improved estimators, which increases the accuracy of the estimation with no additional cost. We conduct extensive experimental evaluation on real-world OSNs to confirm our theoretical claims. The experiment results show that our estimators are unbiased, accurate, efficient, and better than the state-of-the-art algorithms. For the Weibo graph with more than 58 million nodes, our method produces estimate of triangle count with an error less than 5% using only 20,000 sampled nodes. Detailed comparison with the state-of-the-art methods demonstrates that our algorithm is 2--10 times more accurate.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2799870013",
    "type": "article"
  },
  {
    "title": "Information Diffusion Prediction with Network Regularized Role-based User Representation Learning",
    "doi": "https://doi.org/10.1145/3314106",
    "publication_date": "2019-05-07",
    "publication_year": 2019,
    "authors": "Zhitao Wang; Chengyao Chen; Wenjie Li",
    "corresponding_authors": "",
    "abstract": "In this article, we aim at developing a user representation learning model to solve the information diffusion prediction problem in social media. The main idea is to project the diffusion users into a continuous latent space as the role-based (sender and receiver) representations, which capture unique diffusion characteristics of users. The model learns the role-based representations based on a cascade modeling objective that aims at maximizing the likelihood of observed cascades, and employs the matrix factorization objective of reconstructing structural proximities as a regularization on representations. By jointly embedding the information of cascades and network, the learned representations are robust on different diffusion data. We evaluate the proposed model on three real-world datasets. The experimental results demonstrate the better performance of the proposed model than state-of-the-art diffusion embedding and network embedding models and other popular graph-based methods.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2943977421",
    "type": "article"
  },
  {
    "title": "A Unified Framework with Multi-source Data for Predicting Passenger Demands of Ride Services",
    "doi": "https://doi.org/10.1145/3355563",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Yuandong Wang; Xuelian Lin; Hua Wei; Tianyu Wo; Zhou Huang; Yong Zhang; Jie Xu",
    "corresponding_authors": "",
    "abstract": "Ride-hailing applications have been offering convenient ride services for people in need. However, such applications still suffer from the issue of supply-demand disequilibrium, which is a typical problem for traditional taxi services. With effective predictions on passenger demands, we can alleviate the disequilibrium by pre-dispatching, dynamic pricing or avoiding dispatching cars to zero-demand areas. Existing studies of demand predictions mainly utilize limited data sources, trajectory data, or orders of ride services or both of them, which also lacks a multi-perspective consideration. In this article, we present a unified framework with a new combined model and a road-network-based spatial partition to leverage multi-source data and model the passenger demands from temporal, spatial, and zero-demand-area perspectives. In addition, our framework realizes offline training and online predicting, which can satisfy the real-time requirement more easily. We analyze and evaluate the performance of our combined model using the actual operational data from UCAR. The experimental results indicate that our model outperforms baselines on both Mean Absolute Error and Root Mean Square Error on average.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2997169798",
    "type": "article"
  },
  {
    "title": "Efficient Approaches to k Representative G-Skyline Queries",
    "doi": "https://doi.org/10.1145/3397503",
    "publication_date": "2020-07-06",
    "publication_year": 2020,
    "authors": "Xu Zhou; Kenli Li; Zhibang Yang; Yunjun Gao; Keqin Li",
    "corresponding_authors": "",
    "abstract": "The G-Skyline (GSky) query is a powerful tool to analyze optimal groups in decision support. Compared with other group skyline queries, it releases users from providing an aggregate function. Besides, it can get much comprehensive results without overlooking some important results containing non-skylines. However, it is hard for the users to make sensible choices when facing so many results the GSky query returns, especially over a large, high-dimensional dataset or with a large group size. In this article, we investigate k representative G-Skyline ( k GSky) queries to obtain a manageable size of optimal groups. The k GSky query can also inherit the advantage of the GSky query; its results are representative and diversified. Next, we propose three exact algorithms with novel techniques including an upper bound pruning, a grouping strategy, a layered optimum strategy, and a hybrid strategy to efficiently process the k GSky query. Consider these exact algorithms have high time complexity and the precise results are not necessary in many applications. We further develop two approximate algorithms to trade off some accuracy for efficiency. Extensive experiments on both real and synthetic datasets demonstrate the efficiency, scalability, and accuracy of the proposed algorithms.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3038787356",
    "type": "article"
  },
  {
    "title": "Featuring, Detecting, and Visualizing Human Sentiment in Chinese Micro-Blog",
    "doi": "https://doi.org/10.1145/2821513",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Zhiwen Yu; Zhitao Wang; Liming Chen; Bin Guo; Wenjie Li",
    "corresponding_authors": "",
    "abstract": "Micro-blog has been increasingly used for the public to express their opinions, and for organizations to detect public sentiment about social events or public policies. In this article, we examine and identify the key problems of this field, focusing particularly on the characteristics of innovative words, multi-media elements, and hierarchical structure of Chinese “Weibo.” Based on the analysis, we propose a novel approach and develop associated theoretical and technological methods to address these problems. These include a new sentiment word mining method based on three wording metrics and point-wise information, a rule set model for analyzing sentiment features of different linguistic components, and the corresponding methodology for calculating sentiment on multi-granularity considering emoticon elements as auxiliary affective factors. We evaluate our new word discovery and sentiment detection methods on a real-life Chinese micro-blog dataset. Initial results show that our new diction can improve sentiment detection, and they demonstrate that our multi-level rule set method is more effective, with the average accuracy being 10.2% and 1.5% higher than two existing methods for Chinese micro-blog sentiment analysis. In addition, we exploit visualization techniques to study the relationships between online sentiment and real life. The visualization of detected sentiment can help depict temporal patterns and spatial discrepancy.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2402994219",
    "type": "article"
  },
  {
    "title": "Scalable Clustering by Iterative Partitioning and Point Attractor Representation",
    "doi": "https://doi.org/10.1145/2934688",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Junming Shao; Qinli Yang; Hoang-Vu Dang; Bertil Schmidt; Stefan Krämer",
    "corresponding_authors": "",
    "abstract": "Clustering very large datasets while preserving cluster quality remains a challenging data-mining task to date. In this paper, we propose an effective scalable clustering algorithm for large datasets that builds upon the concept of synchronization. Inherited from the powerful concept of synchronization, the proposed algorithm, CIPA (Clustering by Iterative Partitioning and Point Attractor Representations), is capable of handling very large datasets by iteratively partitioning them into thousands of subsets and clustering each subset separately. Using dynamic clustering by synchronization, each subset is then represented by a set of point attractors and outliers. Finally, CIPA identifies the cluster structure of the original dataset by clustering the newly generated dataset consisting of points attractors and outliers from all subsets. We demonstrate that our new scalable clustering approach has several attractive benefits: (a) CIPA faithfully captures the cluster structure of the original data by performing clustering on each separate data iteratively instead of using any sampling or statistical summarization technique. (b) It allows clustering very large datasets efficiently with high cluster quality. (c) CIPA is parallelizable and also suitable for distributed data. Extensive experiments demonstrate the effectiveness and efficiency of our approach.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2480295678",
    "type": "article"
  },
  {
    "title": "Balancing Prediction Errors for Robust Sentiment Classification",
    "doi": "https://doi.org/10.1145/3328795",
    "publication_date": "2019-06-20",
    "publication_year": 2019,
    "authors": "Mohsin Iqbal; Asim Karim; Faisal Kamiran",
    "corresponding_authors": "",
    "abstract": "Sentiment classification is a popular text mining task in which textual content (e.g., a message) is assigned a polarity label (typically positive or negative) reflecting the sentiment expressed in it. Sentiment classification is used widely in applications like customer feedback analysis where robustness and correctness of results are critical. In this article, we highlight that prediction accuracy alone is not sufficient for assessing the performance of a sentiment classifier; it is also important that the classifier is not biased toward positive or negative polarity, thus distorting the distribution of positive and negative messages in the predictions. We propose a measure, called Polarity Bias Rate, for quantifying this bias in a sentiment classifier. Second, we present two methods for removing this bias in the predictions of unsupervised and supervised sentiment classifiers. Our first method, called Bias-Aware Thresholding (BAT), shifts the decision boundary to control the bias in the predictions. Motivated from cost-sensitive learning, BAT is easily applicable to both lexicon-based unsupervised and supervised classifiers. Our second method, called Balanced Logistic Regression (BLR) introduces a bias-remover constraint into the standard logistic regression model. BLR is an automatic bias-free supervised sentiment classifier. We evaluate our methods extensively on seven real-world datasets. The experiments involve two lexicon-based and two supervised sentiment classifiers and include evaluation on multiple train-test data sizes. The results show that bias is controlled effectively in predictions. Furthermore, prediction accuracy is also increased in many cases, thus enhancing the robustness of sentiment classification.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2952418650",
    "type": "article"
  },
  {
    "title": "The Gene of Scientific Success",
    "doi": "https://doi.org/10.1145/3385530",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Xiangjie Kong; Jun Zhang; Da Zhang; Yi Bu; Ying Ding; Feng Xia",
    "corresponding_authors": "",
    "abstract": "This article elaborates how to identify and evaluate causal factors to improve scientific impact. Currently, analyzing scientific impact can be beneficial to various academic activities including funding application, mentor recommendation, discovering potential cooperators, and the like. It is universally acknowledged that high-impact scholars often have more opportunities to receive awards as an encouragement for their hard work. Therefore, scholars spend great efforts in making scientific achievements and improving scientific impact during their academic life. However, what are the determinate factors that control scholars’ academic success? The answer to this question can help scholars conduct their research more efficiently. Under this consideration, our article presents and analyzes the causal factors that are crucial for scholars’ academic success. We first propose five major factors including article-centered factors, author-centered factors, venue-centered factors, institution-centered factors, and temporal factors. Then, we apply recent advanced machine learning algorithms and jackknife method to assess the importance of each causal factor. Our empirical results show that author-centered and article-centered factors have the highest relevancy to scholars’ future success in the computer science area. Additionally, we discover an interesting phenomenon that the h -index of scholars within the same institution or university are actually very close to each other.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3037119610",
    "type": "article"
  },
  {
    "title": "SIR-GN: A Fast Structural Iterative Representation Learning Approach For Graph Nodes",
    "doi": "https://doi.org/10.1145/3450315",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Mikel Joaristi; Edoardo Serra",
    "corresponding_authors": "",
    "abstract": "Graph representation learning methods have attracted an increasing amount of attention in recent years. These methods focus on learning a numerical representation of the nodes in a graph. Learning these representations is a powerful instrument for tasks such as graph mining, visualization, and hashing. They are of particular interest because they facilitate the direct use of standard machine learning models on graphs. Graph representation learning methods can be divided into two main categories: methods preserving the connectivity information of the nodes and methods preserving nodes’ structural information. Connectivity-based methods focus on encoding relationships between nodes, with connected nodes being closer together in the resulting latent space. While methods preserving structure generate a latent space where nodes serving a similar structural function in the network are encoded close to each other, independently of them being connected or even close to each other in the graph. While there are a lot of works that focus on preserving node connectivity, only a few works focus on preserving nodes’ structure. Properly encoding nodes’ structural information is fundamental for many real-world applications as it has been demonstrated that this information can be leveraged to successfully solve many tasks where connectivity-based methods usually fail. A typical example is the task of node classification, i.e., the assignment or prediction of a particular label for a node. Current limitations of structural representation methods are their scalability, representation meaning, and no formal proof that guaranteed the preservation of structural properties. We propose a new graph representation learning method, called Structural Iterative Representation learning approach for Graph Nodes ( SIR-GN ). In this work, we propose two variations ( SIR-GN: GMM and SIR-GN: K-Means ) and show how our best variation SIR-GN: K-Means : (1) theoretically guarantees the preservation of graph structural similarities, (2) provides a clear meaning about its representation and a way to interpret it with a specifically designed attribution procedure, and (3) is scalable and fast to compute. In addition, from our experiment, we show that SIR-GN: K-Means is often better or, in the worst-case comparable than the existing structural graph representation learning methods present in the literature. Also, we empirically show its superior scalability and computational performance when compared to other existing approaches.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3160144061",
    "type": "article"
  },
  {
    "title": "Cross-domain Recommendation with Bridge-Item Embeddings",
    "doi": "https://doi.org/10.1145/3447683",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Chen Gao; Yong Li; Fuli Feng; Xiangning Chen; Kai Zhao; Xiangnan He; Depeng Jin",
    "corresponding_authors": "",
    "abstract": "Web systems that provide the same functionality usually share a certain amount of items. This makes it possible to combine data from different websites to improve recommendation quality, known as the cross-domain recommendation task. Despite many research efforts on this task, the main drawback is that they largely assume the data of different systems can be fully shared . Such an assumption is unrealistic different systems are typically operated by different companies, and it may violate business privacy policy to directly share user behavior data since it is highly sensitive. In this work, we consider a more practical scenario to perform cross-domain recommendation. To avoid the leak of user privacy during the data sharing process, we consider sharing only the information of the item side, rather than user behavior data. Specifically, we transfer the item embeddings across domains, making it easier for two companies to reach a consensus (e.g., legal policy) on data sharing since the data to be shared is user-irrelevant and has no explicit semantics. To distill useful signals from transferred item embeddings, we rely on the strong representation power of neural networks and develop a new method named as NATR (short for N eural A ttentive T ransfer R ecommendation ). We perform extensive experiments on two real-world datasets, demonstrating that NATR achieves similar or even better performance than traditional cross-domain recommendation methods that directly share user-relevant data. Further insights are provided on the efficacy of NATR in using the transferred item embeddings to alleviate the data sparsity issue.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3185865766",
    "type": "article"
  },
  {
    "title": "3E-LDA",
    "doi": "https://doi.org/10.1145/3442347",
    "publication_date": "2021-03-26",
    "publication_year": 2021,
    "authors": "Yanni Li; Bing Liu; Yongbo Yu; Hui Li; Jiacan Sun; Jiangtao Cui",
    "corresponding_authors": "",
    "abstract": "Linear discriminant analysis (LDA) is one of the important techniques for dimensionality reduction, machine learning, and pattern recognition. However, in many applications, applying the classical LDA often faces the following problems: (1) sensitivity to outliers, (2) absence of local geometric information, and (3) small sample size or matrix singularity that can result in weak robustness and efficiency. Although several researchers have attempted to address one or more of the problems, little work has been done to address all of them together to produce a more effective and efficient LDA algorithm. This article proposes 3E-LDA, an enhanced LDA algorithm, that deals with all three problems as an attempt to further improve LDA. It proposes to learn a weighted median rather than the mean of the samples to deal with (1), to embed both between-class and within-class local geometric information to deal with (2), and to calculate the projection vectors in the null space of the matrix to deal with (3). Experiments on six benchmark datasets show that these three enhancements enable 3E-LDA to markedly outperform state-of-the-art LDA baselines in both accuracy and efficiency.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3148632146",
    "type": "article"
  },
  {
    "title": "Profile Decomposition Based Hybrid Transfer Learning for Cold-Start Data Anomaly Detection",
    "doi": "https://doi.org/10.1145/3530990",
    "publication_date": "2022-04-25",
    "publication_year": 2022,
    "authors": "Ziyue Li; Hao Yan; Fugee Tsung; Ke Zhang",
    "corresponding_authors": "",
    "abstract": "Anomaly detection is an essential task for quality management in smart manufacturing. An accurate data-driven detection method usually needs enough data and labels. However, in practice, there commonly exist newly set-up processes in manufacturing, and they only have quite limited data available for analysis. Borrowing the name from the recommender system, we call this process a cold-start process. The sparsity of anomaly, the deviation of the profile, and noise aggravate the detection difficulty. Transfer learning could help to detect anomalies for cold-start processes by transferring the knowledge from more experienced processes to the new processes. However, the existing transfer learning and multi-task learning frameworks are established on task- or domain-level relatedness. We observe instead, within a domain, some components (background and anomaly) share more commonality, others (profile deviation and noise) not. To this end, we propose a more delicate component-level transfer learning scheme, i.e., decomposition-based hybrid transfer learning ( DHTL ): It first decomposes a domain (e.g., a data source containing profiles) into different components (smooth background, profile deviation, anomaly, and noise); then, each component’s transferability is analyzed by expert knowledge; Lastly, different transfer learning techniques could be tailored accordingly. We adopted the Bayesian probabilistic hierarchical model to formulate parameter transfer for the background, and “ L 2,1 + L 1 ”-norm to formulate low dimension feature-representation transfer for the anomaly. An efficient algorithm based on Block Coordinate Descend is proposed to learn the parameters. A case study based on glass coating pressure profiles demonstrates the improved accuracy and completeness of detected anomaly, and a simulation demonstrates the fidelity of the decomposition results.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4224312222",
    "type": "article"
  },
  {
    "title": "GRACE: A General Graph Convolution Framework for Attributed Graph Clustering",
    "doi": "https://doi.org/10.1145/3544977",
    "publication_date": "2022-06-24",
    "publication_year": 2022,
    "authors": "Barakeel Fanseu Kamhoua; Lin Zhang; Kaili Ma; James Cheng; Bo Li; Bo Han",
    "corresponding_authors": "",
    "abstract": "Attributed graph clustering (AGC) is an important problem in graph mining as more and more complex data in real-world have been represented in graphs with attributed nodes. While it is a common practice to leverage both attribute and structure information for improved clustering performance, most existing AGC algorithms consider only a specific type of relations, which hinders their applicability to integrate various complex relations into node attributes for AGC. In this article, we propose GRACE, an extended graph convolution framework for AGC tasks. Our framework provides a general and interpretative solution for clustering many different types of attributed graphs, including undirected, directed, heterogeneous and hyper attributed graphs. By building suitable graph Laplacians for each of the aforementioned graph types, GRACE can seamlessly perform graph convolution on node attributes to fuse all available information for clustering. We conduct extensive experiments on 14 real-world datasets of four different graph types. The experimental results show that GRACE outperforms the state-of-the-art AGC methods on the different graph types in terms of clustering quality, time, and memory usage.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4283390813",
    "type": "article"
  },
  {
    "title": "Modeling Cross-session Information with Multi-interest Graph Neural Networks for the Next-item Recommendation",
    "doi": "https://doi.org/10.1145/3532192",
    "publication_date": "2022-04-27",
    "publication_year": 2022,
    "authors": "Ting-Yun Wang; Chiao-Ting Chen; Ju-Chun Huang; Szu-Hao Huang",
    "corresponding_authors": "",
    "abstract": "Next-item recommendation involves predicting the next item of interest of a given user from their past behavior. Users tend to browse and purchase various items on e-commerce websites according to their varied interests and needs, as reflected in their purchasing history. Most existing next-item recommendation methods aim at extracting the main point of interest in each browsing session and encapsulate it in a single representation. However, past behavior sequences reflect the multiple interests of a single user, which cannot be captured by methods that focus on single-interest contexts. Indeed, multiple interests cannot be captured in a single representation, and doing so results in missing information. Therefore, we propose a model with a multi-interest structure for capturing the various interests of users from their behavior sequence. Moreover, we adopted a method based on a graph neural network to construct interest graphs based on the historical and current behavior sequences of users. These graphs can capture complex item transition patterns related to different interests. In experiments, the proposed method outperforms state-of-the-art session-based recommendation systems on three real-world datasets, achieving 4% improvement of Recall over the SOTAs on Jdata dataset.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4293237954",
    "type": "article"
  },
  {
    "title": "Nonnegative Matrix Factorization Based on Node Centrality for Community Detection",
    "doi": "https://doi.org/10.1145/3578520",
    "publication_date": "2022-12-27",
    "publication_year": 2022,
    "authors": "Sixing Su; Jiewen Guan; Bilian Chen; Xin Huang",
    "corresponding_authors": "",
    "abstract": "Community detection is an important topic in network analysis, and recently many community detection methods have been developed on top of the Nonnegative Matrix Factorization (NMF) technique. Most NMF-based community detection methods only utilize the first-order proximity information in the adjacency matrix, which has some limitations. Besides, many NMF-based community detection methods involve sparse regularizations to promote clearer community memberships. However, in most of these regularizations, different nodes are treated equally, which seems unreasonable. To dismiss the above limitations, this article proposes a community detection method based on node centrality under the framework of NMF. Specifically, we design a new similarity measure which considers the proximity of higher-order neighbors to form a more informative graph regularization mechanism, so as to better refine the detected communities. Besides, we introduce the node centrality and Gini impurity to measure the importance of nodes and sparseness of the community memberships, respectively. Then, we propose a novel sparse regularization mechanism which forces nodes with higher node centrality to have smaller Gini impurity. Extensive experimental results on a variety of real-world networks show the superior performance of the proposed method over thirteen state-of-the-art methods.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4312204318",
    "type": "article"
  },
  {
    "title": "Multiple Graphs and Low-Rank Embedding for Multi-Source Heterogeneous Domain Adaptation",
    "doi": "https://doi.org/10.1145/3492804",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Hanrui Wu; Michael K. Ng",
    "corresponding_authors": "",
    "abstract": "Multi-source domain adaptation is a challenging topic in transfer learning, especially when the data of each domain are represented by different kinds of features, i.e., Multi-source Heterogeneous Domain Adaptation (MHDA). It is important to take advantage of the knowledge extracted from multiple sources as well as bridge the heterogeneous spaces for handling the MHDA paradigm. This article proposes a novel method named Multiple Graphs and Low-rank Embedding (MGLE), which models the local structure information of multiple domains using multiple graphs and learns the low-rank embedding of the target domain. Then, MGLE augments the learned embedding with the original target data. Specifically, we introduce the modules of both domain discrepancy and domain relevance into the multiple graphs and low-rank embedding learning procedure. Subsequently, we develop an iterative optimization algorithm to solve the resulting problem. We evaluate the effectiveness of the proposed method on several real-world datasets. Promising results show that the performance of MGLE is better than that of the baseline methods in terms of several metrics, such as AUC, MAE, accuracy, precision, F1 score, and MCC, demonstrating the effectiveness of the proposed method.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4205661033",
    "type": "article"
  },
  {
    "title": "Quality-Informed Process Mining: A Case for Standardised Data Quality Annotations",
    "doi": "https://doi.org/10.1145/3511707",
    "publication_date": "2022-02-04",
    "publication_year": 2022,
    "authors": "Kanika Goel; Sander J. J. Leemans; Niels Martin; Moe Thandar Wynn",
    "corresponding_authors": "",
    "abstract": "Real-life event logs, reflecting the actual executions of complex business processes, are faced with numerous data quality issues. Extensive data sanity checks and pre-processing are usually needed before historical data can be used as input to obtain reliable data-driven insights. However, most of the existing algorithms in process mining, a field focusing on data-driven process analysis, do not take any data quality issues or the potential effects of data pre-processing into account explicitly. This can result in erroneous process mining results, leading to inaccurate, or misleading conclusions about the process under investigation. To address this gap, we propose data quality annotations for event logs, which can be used by process mining algorithms to generate quality-informed insights. Using a design science approach, requirements are formulated, which are leveraged to propose data quality annotations. Moreover, we present the “Quality-Informed visual Miner” plug-in to demonstrate the potential utility and impact of data quality annotations. Our experimental results, utilising both synthetic and real-life event logs, show how the use of data quality annotations by process mining techniques can assist in increasing the reliability of performance analysis results.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4210382442",
    "type": "article"
  },
  {
    "title": "Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry",
    "doi": "https://doi.org/10.1145/3494566",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Ms Promila Sharma; Uma Meena; Girish Sharma",
    "corresponding_authors": "",
    "abstract": "Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4220813721",
    "type": "article"
  },
  {
    "title": "TRACE: Travel Reinforcement Recommendation Based on Location-Aware Context Extraction",
    "doi": "https://doi.org/10.1145/3487047",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Zhe Fu; Li Yu; Xi Niu",
    "corresponding_authors": "",
    "abstract": "As the popularity of online travel platforms increases, users tend to make ad-hoc decisions on places to visit rather than preparing the detailed tour plans in advance. Under the situation of timeliness and uncertainty of users’ demand, how to integrate real-time context into dynamic and personalized recommendations have become a key issue in travel recommender system. In this article, by integrating the users’ historical preferences and real-time context, a location-aware recommender system called TRACE ( T ravel R einforcement Recommendations Based on Location- A ware C ontext E xtraction) is proposed. It captures users’ features based on location-aware context learning model, and makes dynamic recommendations based on reinforcement learning. Specifically, this research: (1) designs a travel reinforcing recommender system based on an Actor-Critic framework, which can dynamically track the user preference shifts and optimize the recommender system performance; (2) proposes a location-aware context learning model, which aims at extracting user context from real-time location and then calculating the impacts of nearby attractions on users’ preferences; and (3) conducts both offline and online experiments. Our proposed model achieves the best performance in both of the two experiments, which demonstrates that tracking the users’ preference shifts based on real-time location is valuable for improving the recommendation results.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4225889707",
    "type": "article"
  },
  {
    "title": "DNformer: Temporal Link Prediction with Transfer Learning in Dynamic Networks",
    "doi": "https://doi.org/10.1145/3551892",
    "publication_date": "2022-08-02",
    "publication_year": 2022,
    "authors": "Xin Jiang; Zhengxin Yu; Chao Hai; Hongbo Liu; Xindong Wu; Tomás Ward",
    "corresponding_authors": "",
    "abstract": "Temporal link prediction (TLP) is among the most important graph learning tasks, capable of predicting dynamic, time-varying links within networks. The key problem of TLP is how to explore potential link-evolving tendency from the increasing number of links over time. There exist three major challenges toward solving this problem: temporal nonlinear sparsity, weak serial correlation, and discontinuous structural dynamics. In this article, we propose a novel transfer learning model, called DNformer, to predict temporal link sequence in dynamic networks. The structural dynamic evolution is sequenced into consecutive links one by one over time to inhibit temporal nonlinear sparsity. The self-attention of the model is used to capture the serial correlation between the input and output link sequences. Moreover, our structural encoding is designed to obtain changing structures from the consecutive links and to learn the mapping between link sequences. This structural encoding consists of two parts: the node clustering encoding of each link and the link similarity encoding between links. These encodings enable the model to perceive the importance and correlation of links. Furthermore, we introduce a measurement of structural similarity in the loss function for the structural differences of link sequences. The experimental results demonstrate that our model outperforms other state-of-the-art TLP methods such as Transformer, TGAT, and EvolveGCN. It achieves the three highest AUC and four highest precision scores in five different representative dynamic networks problems.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4289519006",
    "type": "article"
  },
  {
    "title": "Individuality Meets Commonality: A Unified Graph Learning Framework for Multi-View Clustering",
    "doi": "https://doi.org/10.1145/3532612",
    "publication_date": "2022-04-27",
    "publication_year": 2022,
    "authors": "Zhibin Gu; Songhe Feng",
    "corresponding_authors": "",
    "abstract": "Multi-view clustering, which aims at boosting the clustering performance by leveraging the individual information and the common information of multi-view data, has gained extensive consideration in recent years. However, most existing multi-view clustering algorithms either focus on extracting the multi-view individuality or emphasize on exploring the multi-view commonality, neither of which can fully utilize the comprehensive information from multiple views. To this end, we propose a novel algorithm named V iew-specific and C onsensus G raph A lignment (VCGA) for multi-view clustering, which simultaneously formulates the multi-view individuality and the multi-view commonality into a unified framework to effectively partition data points. To be specific, the VCGA model constructs the view-specific graphs and the shared graph from original multi-view data and hidden latent representation, respectively. Furthermore, the view-specific graphs of different views and the consensus graph are aligned into an informative target graph, which is employed as a crucial input to the standard spectral clustering method for clustering. Extensive experimental results on six benchmark datasets demonstrate the superiority of our method against other state-of-the-art clustering algorithms.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4293187595",
    "type": "article"
  },
  {
    "title": "Learning Aspect-Aware High-Order Representations from Ratings and Reviews for Recommendation",
    "doi": "https://doi.org/10.1145/3532188",
    "publication_date": "2022-04-27",
    "publication_year": 2022,
    "authors": "Ke Wang; Yanmin Zhu; Haobing Liu; Tianzi Zang; Chunyang Wang",
    "corresponding_authors": "",
    "abstract": "Textual reviews contain rich semantic information that is useful for making better recommendation, as such semantic information may indicate more fine-grained preferences of users. Recent efforts make considerable improvement on recommendation by integrating textual reviews in rating-based recommendations. However, there still exist major challenges on integrating textual reviews for recommendation. On the one hand, most existing works focus on learning a single representation from reviews but ignoring complex relations between users (or items) and reviews, which may fail to capture user preferences and item attributes together. On the other hand, these works independently learn latent representations from ratings and reviews while omitting correlations between rating-based features and review-based features, which may harm recommendation performance. In this article, we capture the aspect-aware relations by constructing heterogeneous graphs from reviews. Furthermore, we propose a new recommendation model, namely AHOR, to jointly distill rating-based features and review-based features, which are derived from ratings and reviews, respectively. To explore the multi-hop connectivity information between users, items, and aspects, a novel graph neural network is introduced to learn aspect-aware high-order representations. Experiments based on public datasets show that our approach outperforms state-of-the-art methods. We also provide detailed analysis on the high-order signals and the aspect importance to show the interpretability of our proposed model.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4293187604",
    "type": "article"
  },
  {
    "title": "Privacy-Preserving Personalized Fitness Recommender System <i> P <sup>3</sup> FitRec </i> : A Multi-level Deep Learning Approach",
    "doi": "https://doi.org/10.1145/3572899",
    "publication_date": "2023-01-14",
    "publication_year": 2023,
    "authors": "Xiao Liu; Bonan Gao; Basem Suleiman; Han You; Zisu Ma; Yu Liu; Ali Anaissi",
    "corresponding_authors": "",
    "abstract": "Recommender systems have been successfully used in many domains with the help of machine learning algorithms. However, such applications tend to use multi-dimensional user data, which has raised widespread concerns about the breach of users’ privacy. Meanwhile, wearable technologies have enabled users to collect fitness-related data through embedded sensors to monitor their conditions or achieve personalized fitness goals. In this article, we propose a novel privacy-aware personalized fitness recommender system. We introduce a multi-level deep learning framework that learns important features from a large-scale real fitness dataset that is collected from wearable Internet of Things (IoT) devices to derive intelligent fitness recommendations. Unlike most existing approaches, our approach achieves personalization by inferring the fitness characteristics of users from sensory data, minimizing the need for explicitly collecting user identity or biometric information, such as name, age, height, and weight. Our proposed models and algorithms predict (a) personalized exercise distance recommendations to help users to achieve target calories, (b) personalized speed sequence recommendations to adjust exercise speed given the nature of the exercise and the chosen route, and (c) personalized heart rate sequence to guide the user of the potential health status for future exercises. Our experimental evaluation on a real-world Fitbit dataset demonstrated high accuracy in predicting exercise distance, speed sequence, and heart rate sequence compared with similar studies. 1 Furthermore, our approach is novel compared with existing studies, as it does not require collecting and using users’ sensitive information. Thus, it preserves the users’ privacy.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4315977629",
    "type": "article"
  },
  {
    "title": "Revisiting the Role of Heterophily in Graph Representation Learning: An Edge Classification Perspective",
    "doi": "https://doi.org/10.1145/3603378",
    "publication_date": "2023-06-02",
    "publication_year": 2023,
    "authors": "Jincheng Huang; Ping Li; Rui Huang; Na Chen; Acong Zhang",
    "corresponding_authors": "",
    "abstract": "Graph representation learning aims at integrating node contents with graph structure to learn nodes/graph representations. Nevertheless, it is found that many existing graph learning methods do not work well on data with high heterophily level that accounts for a large proportion of edges between different class labels. Recent efforts to this problem focus on improving the message passing mechanism. However, it remains unclear whether heterophily truly does harm to the performance of graph neural networks (GNNs). The key is to unfold the relationship between a node and its immediate neighbors, e.g., are they heterophilous or homophilious? From this perspective, here we study the role of heterophily in graph representation learning before/after the relationships between connected nodes are disclosed. In particular, we propose an end-to-end framework that both learns the type of edges (i.e., heterophilous/homophilious) and leverage edge type information to improve the expressiveness of graph neural networks. We implement this framework in two different ways. Specifically, to avoid messages passing through heterophilous edges, we can optimize the graph structure to be homophilious by dropping heterophilous edges identified by an edge classifier. Alternatively, it is possible to exploit the information about the presence of heterophilous neighbors for feature learning, so a hybrid message passing approach is devised to aggregate homophilious neighbors and diversify heterophilous neighbors based on edge classification. Extensive experiments demonstrate the remarkable performance improvement of GNNs with the proposed framework on multiple datasets across the full spectrum of homophily level.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4379160762",
    "type": "article"
  },
  {
    "title": "Server-Client Collaborative Distillation for Federated Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3604939",
    "publication_date": "2023-06-19",
    "publication_year": 2023,
    "authors": "Weiming Mai; Jiangchao Yao; Chen Gong; Ya Zhang; Yiu‐ming Cheung; Bo Han",
    "corresponding_authors": "",
    "abstract": "Federated Learning (FL) learns a global model in a distributional manner, which does not require local clients to share private data. Such merit has drawn lots of attention in the interaction scenarios, where Federated Reinforcement Learning (FRL) emerges as a cross-field research direction focusing on the robust training of agents. Different from FL, the heterogeneity problem in FRL is more challenging because the data depends on the policy of agents and the environment dynamics. FRL learns to interact under the non-stationary environment feedback, while the typical FL methods aim at handling the constant data heterogeneity. In this article, we are among the first attempts to analyze the heterogeneity problem in FRL and propose an off-policy FRL framework. Specifically, a student–teacher–student model learning and fusion method, termed as Server-Client Collaborative Distillation (SCCD), is introduced. Unlike the traditional FL, we distill all local models on the server side for model fusion. To reduce the variance of the training, a local distillation is also conducted every time the agent receives the global model. Experimentally, we compare SCCD with a range of straightforward combinations between FL methods and RL. The results demonstrate that SCCD has a superior performance in four classical continuous control tasks with non-IID environments.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4381164013",
    "type": "article"
  },
  {
    "title": "Graph Mining for Cybersecurity: A Survey",
    "doi": "https://doi.org/10.1145/3610228",
    "publication_date": "2023-07-19",
    "publication_year": 2023,
    "authors": "Bo Yan; Cheng Yang; Chuan Shi; Yong Fang; Qi Li; Yanfang Ye; Junping Du",
    "corresponding_authors": "",
    "abstract": "The explosive growth of cyber attacks today, such as malware, spam, and intrusions, has caused severe consequences on society. Securing cyberspace has become a great concern for organizations and governments. Traditional machine learning based methods are extensively used in detecting cyber threats, but they hardly model the correlations between real-world cyber entities. In recent years, with the proliferation of graph mining techniques, many researchers have investigated these techniques for capturing correlations between cyber entities and achieving high performance. It is imperative to summarize existing graph-based cybersecurity solutions to provide a guide for future studies. Therefore, as a key contribution of this work, we provide a comprehensive review of graph mining for cybersecurity, including an overview of cybersecurity tasks, the typical graph mining techniques, and the general process of applying them to cybersecurity, as well as various solutions for different cybersecurity tasks. For each task, we probe into relevant methods and highlight the graph types, graph approaches, and task levels in their modeling. Furthermore, we collect open datasets and toolkits for graph-based cybersecurity. Finally, we present an outlook on the potential directions of this field for future research.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4384822835",
    "type": "article"
  },
  {
    "title": "Discovering Interesting Patterns from Hypergraphs",
    "doi": "https://doi.org/10.1145/3622940",
    "publication_date": "2023-09-07",
    "publication_year": 2023,
    "authors": "Md. Tanvir Alam; Chowdhury Farhan Ahmed; Md. Samiullah; Carson K. Leung",
    "corresponding_authors": "",
    "abstract": "A hypergraph is a complex data structure capable of expressing associations among any number of data entities. Overcoming the limitations of traditional graphs, hypergraphs are useful to model real-life problems. Frequent pattern mining is one of the most popular problems in data mining with a lot of applications. To the best of our knowledge, there exists no flexible pattern mining framework for hypergraph databases decomposing associations among data entities. In this article, we propose a flexible and complete framework for mining frequent patterns from a collection of hypergraphs. To discover more interesting patterns beyond the traditional frequent patterns, we propose frameworks for weighted and uncertain hypergraph mining also. We develop three algorithms for mining frequent, weighted, and uncertain hypergraph patterns efficiently by introducing a canonical labeling technique for isomorphic hypergraphs. Extensive experiments have been conducted on real-life hypergraph databases to show both the effectiveness and efficiency of our proposed frameworks and algorithms.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4386497976",
    "type": "article"
  },
  {
    "title": "Graph-based Text Classification by Contrastive Learning with Text-level Graph Augmentation",
    "doi": "https://doi.org/10.1145/3638353",
    "publication_date": "2023-12-22",
    "publication_year": 2023,
    "authors": "Ximing Li; Bing Wang; Yang Wang; Meng Wang",
    "corresponding_authors": "",
    "abstract": "Text Classification (TC) is a fundamental task in the information retrieval community. Nowadays, the mainstay TC methods are built on the deep neural networks, which can learn much more discriminative text features than the traditional shallow learning methods. Among existing deep TC methods, the ones based on Graph Neural Network (GNN) have attracted more attention due to the superior performance. Technically, the GNN-based TC methods mainly transform the full training dataset to a graph of texts; however, they often neglect the dependency between words, so as to miss potential semantic information of texts, which may be significant to exactly represent them. To solve the aforementioned problem, we generate graphs of words instead, so as to capture the dependency information of words. Specifically, each text is translated into a graph of words, where neighboring words are linked. We learn the node features of words by a GNN-like procedure and then aggregate them as the graph feature to represent the current text. To further improve the text representations, we suggest a contrastive learning regularization term. Specifically, we generate two augmented text graphs for each original text graph, we constrain the representations of the two augmented graphs from the same text close and the ones from different texts far away. We propose various techniques to generate the augmented graphs. Upon those ideas, we develop a novel deep TC model, namely Text-level Graph Networks with Contrastive Learning (TGN cl ). We conduct a number of experiments to evaluate the proposed TGN cl model. The empirical results demonstrate that TGN cl can outperform the existing state-of-the-art TC models.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4390134993",
    "type": "article"
  },
  {
    "title": "STA-TCN: Spatial-temporal Attention over Temporal Convolutional Network for Next Point-of-interest Recommendation",
    "doi": "https://doi.org/10.1145/3596497",
    "publication_date": "2023-05-10",
    "publication_year": 2023,
    "authors": "Junjie Ou; Haiming Jin; Xiaocheng Wang; Hao Jiang; Xinbing Wang; Chenghu Zhou",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed a vastly increasing popularity of location-based social networks (LBSNs), which facilitates studies on the next Point-of-Interest (POI) recommendation problem. A user’s POI visiting behavior shows the sequential transition correlation with previous successive check-ins and the global spatial-temporal correlation with those check-ins that happened a long time ago at a similar time of day and in geographically close areas. Although previous POI recommendation methods attempted to capture these two correlations, several limitations remain to be solved: (1) RNNs are widely adopted to capture the sequential transition correlation, whereas training an RNN is rather time-consuming given the long input check-in sequence. (2) The pairwise proximities on time of day and geographical area of check-ins are crucial for global spatial-temporal correlation learning, but have not been comprehensively considered by previous methods. To tackle these issues, we propose a novel next POI recommendation framework named STA-TCN. Specifically, instead of RNNs, STA-TCN augments the Temporal Convolutional Network with gated input injection to learn sequential transition correlation. Furthermore, STA-TCN fuses two novel grid-difference and time-sensitivity learning mechanisms with attention network to learn the pairwise spatial-temporal proximities among a user’s check-ins. Extensive experiments are conducted on two large-scale real-world LBSN datasets, and the results show that STA-TCN outperforms the best state-of-the-art baseline with an average improvement of 9.71% and 7.88% on hit rate and normalized discounted cumulative gain, respectively.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4376114316",
    "type": "article"
  },
  {
    "title": "Networked Time-series Prediction with Incomplete Data via Generative Adversarial Network",
    "doi": "https://doi.org/10.1145/3643822",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Yichen Zhu; Bo Jiang; Haiming Jin; Mengtian Zhang; Feng Gao; Jianqiang Huang; Tao Lin; Xinbing Wang",
    "corresponding_authors": "",
    "abstract": "A networked time series (NETS) is a family of time series on a given graph, one for each node. It has a wide range of applications from intelligent transportation to environment monitoring to smart grid management. An important task in such applications is to predict the future values of a NETS based on its historical values and the underlying graph. Most existing methods require complete data for training. However, in real-world scenarios, it is not uncommon to have missing data due to sensor malfunction, incomplete sensing coverage, and so on. In this article, we study the problem of NETS prediction with incomplete data . We propose networked time series Imputation Generative Adversarial Network (NETS-ImpGAN), a novel deep learning framework that can be trained on incomplete data with missing values in both history and future. Furthermore, we propose Graph Temporal Attention Networks , which incorporate the attention mechanism to capture both inter-time series and temporal correlations. We conduct extensive experiments on four real-world datasets under different missing patterns and missing rates. The experimental results show that NETS-ImpGAN outperforms existing methods, reducing the Mean Absolute Error by up to 25%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391568803",
    "type": "article"
  },
  {
    "title": "Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness",
    "doi": "https://doi.org/10.1145/3648684",
    "publication_date": "2024-02-20",
    "publication_year": 2024,
    "authors": "Chen Zhao; Feng Mi; Xintao Wu; Kai Jiang; Latifur Khan; Feng Chen",
    "corresponding_authors": "",
    "abstract": "The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner’s objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it’s crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this article, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework. Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML. This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy. The problem is framed as a bi-level convex-concave optimization, considering both the model’s primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively. Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints. Our experimental evaluation of various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391957379",
    "type": "article"
  },
  {
    "title": "Adaptive Content-Aware Influence Maximization via Online Learning to Rank",
    "doi": "https://doi.org/10.1145/3651987",
    "publication_date": "2024-03-08",
    "publication_year": 2024,
    "authors": "Konstantinos Theocharidis; Panagiotis Karras; Manolis Terrovitis; Spiros Skiadopoulos; Hady W. Lauw",
    "corresponding_authors": "",
    "abstract": "How can we adapt the composition of a post over a series of rounds to make it more appealing in a social network? Techniques that progressively learn how to make a fixed post more influential over rounds have been studied in the context of the Influence Maximization (IM) problem, which seeks a set of seed users that maximize a post’s influence. However, there is no work on progressively learning how a post’s features affect its influence. In this article, we propose and study the problem of Adaptive Content-Aware Influence Maximization (ACAIM), which calls to find k features to form a post in each round so as to maximize the cumulative influence of those posts over all rounds. We solve ACAIM by applying, for the first time, an Online Learning to Rank (OLR) framework for IM purposes. We introduce the CATRID propagation model , which expresses how posts disseminate in a social network using click probabilities and post visibility criteria and develop a simulator that runs CATRID via a training-testing scheme based on real posts of the VK social network, so as to realistically represent the learning environment. We deploy three learners that solve ACAIM in an online (real-time) manner. We experimentally prove the practical suitability of our solutions via exhaustive experiments on multiple brands (operating as different case studies ) and several VK datasets; the best learner is evaluated on 45 separate case studies yielding convincing results.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4392590332",
    "type": "article"
  },
  {
    "title": "Deconfounding User Preference in Recommendation Systems through Implicit and Explicit Feedback",
    "doi": "https://doi.org/10.1145/3673762",
    "publication_date": "2024-06-18",
    "publication_year": 2024,
    "authors": "Yuliang Liang; Enneng Yang; Guibing Guo; Wei Cai; Linying Jiang; Xingwei Wang",
    "corresponding_authors": "",
    "abstract": "Recommender systems are influenced by many confounding factors (i.e., confounders) which result in various biases (e.g., popularity biases) and inaccurate user preference. Existing approaches try to eliminate these biases by inference with causal graphs. However, they assume all confounding factors can be observed and no hidden confounders exist. We argue that many confounding factors (e.g., season) may not be observable from user–item interaction data, resulting inaccurate user preference. In this article, we propose a deconfounded recommender considering unobservable confounders. Specifically, we propose a new causal graph with explicit and implicit feedback, which can better model user preference. Then, we realize a deconfounded estimator by the front-door adjustment, which is able to eliminate the effect of unobserved confounders. Finally, we conduct a series of experiments on two real-world datasets, and the results show that our approach performs better than other counterparts in terms of recommendation accuracy.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4399785759",
    "type": "article"
  },
  {
    "title": "Mobility Prediction via Rule-Enhanced Knowledge Graph",
    "doi": "https://doi.org/10.1145/3677019",
    "publication_date": "2024-07-24",
    "publication_year": 2024,
    "authors": "Qiaohong Yu; Huandong Wang; Yu Liu; Depeng Jin; Yong Li; Lin Zhu; Junlan Feng",
    "corresponding_authors": "",
    "abstract": "With the rapid development of location acquisition technologies, massive mobile trajectories have been collected and made available to us, which support a fantastic way of understanding and modeling individuals’ mobility. However, existing data-driven methods either fail to capture the long-range dependency or suffer from a high computational cost. To overcome these issues, we propose a knowledge-driven framework for mobility prediction, which leverages knowledge graphs (KG) to formulate the mobility prediction task into the KG completion problem through integrating the structured “knowledge” from the mobility data. However, most related mobility prediction works only focus on the structured information encoded in existing triples, which ignores the rich semantic information of relation paths composed of multiple relation triples. In this article, we apply a dedicated module to extract the supplementary semantic structure of paths in KG, which contributes to the interpretability and accuracy of our model. Specifically, the extracted rules are applied to capture the dependencies between relational facts. Moreover, by incorporating user information in the entity-relation space with the corresponding hyperplane, our method could capture diverse user mobility patterns and model the personal characteristics of users to improve the accuracy of mobility prediction. Extensive evaluations illustrate that our proposed model beats state-of-the-art mobility prediction algorithms, which verifies the superiority of utilizing logical rules and user hyperplanes. Our implementation code is available at https://github.com/tsinghua-fib-lab/RulekG-MobiPre.git",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4400949864",
    "type": "article"
  },
  {
    "title": "SPORT: A Subgraph Perspective on Graph Classification with Label Noise",
    "doi": "https://doi.org/10.1145/3687468",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Nan Yin; Li Shen; Chong Chen; Xian‐Sheng Hua; Xiao Luo",
    "corresponding_authors": "",
    "abstract": "Graph neural networks (GNNs) have achieved great success recently on graph classification tasks using supervised end-to-end training. Unfortunately, extensive noisy graph labels could exist in the real world because of the complicated processes of manual graph data annotations, which may significantly degrade the performance of GNNs. Therefore, we investigate the problem of graph classification with label noise, which is demanding because of the complex graph representation learning issue and serious memorization of noisy samples. In this work, we present a novel approach called S ubgra p h Set Netw or k with Sample Selection and Consis t ency Learning (SPORT) for this problem. To release the overfitting of GNNs, SPORT proposes to characterize each graph as a set of subgraphs generated by certain predefined stratagems, which can be viewed as samples from its underlying semantic distribution in graph space. Then we develop an equivariant network to encode the subgraph set with the consideration of the symmetry group. To further release the influences of noisy examples, we leverage the predictions of subgraphs to measure the likelihood of a sample being clean or noisy, followed by effective label updating. In addition, we propose a joint loss to advance the model generalizability by introducing consistency regularization. Comprehensive experiments on a wide range of graph classification datasets demonstrate the effectiveness of our SPORT. Specifically, SPORT outperforms the most competing baseline by up to 6.4%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4401944154",
    "type": "article"
  },
  {
    "title": "Towards Prototype-Based Self-Explainable Graph Neural Network",
    "doi": "https://doi.org/10.1145/3689647",
    "publication_date": "2024-08-30",
    "publication_year": 2024,
    "authors": "Enyan Dai; Suhang Wang",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) have shown great ability in modeling graph-structured data for various domains. However, GNNs are known as black-box models that lack interpretability. Without understanding their inner working, we cannot fully trust them, which largely limits their adoption in high-stake scenarios. Though some initial efforts have been taken to interpret the predictions of GNNs, they mainly focus on providing post-hoc explanations using an additional explainer, which could misrepresent the true inner working mechanism of the target GNN. The works on self-explainable GNNs are rather limited. Therefore, we study a novel problem of learning prototype-based self-explainable GNNs that can simultaneously give accurate predictions and prototype-based explanations on predictions. We design a framework which can learn prototype graphs that capture representative patterns of each class as class-level explanations. The learned prototypes are also used to simultaneously make prediction for for a test instance and provide instance-level explanation. Extensive experiments on real-world and synthetic datasets show the effectiveness of the proposed framework for both prediction accuracy and explanation quality.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4402047514",
    "type": "article"
  },
  {
    "title": "A Simple Data Augmentation for Graph Classification: A Perspective of Equivariance and Invariance",
    "doi": "https://doi.org/10.1145/3706062",
    "publication_date": "2024-11-28",
    "publication_year": 2024,
    "authors": "Yongduo Sui; Shuyao Wang; Jie Sun; Zhiyuan Liu; Qing Cui; Longfei Li; Jun Zhou; Xiang Wang; Xiangnan He",
    "corresponding_authors": "",
    "abstract": "In graph classification, the out-of-distribution (OOD) issue is attracting great attention. To address this issue, a prevailing idea is to learn stable features, on the assumption that they are substructures causally determining the label and that their relationship with the label is stable to the distributional uncertainty. In contrast, the complementary parts termed environmental features, fail to determine the label solely and hold varying relationships with the label, thus ascribed to the possible reason for the distribution shift. Existing generalization efforts mainly encourage the model's insensitivity to environmental features. While the sensitivity to stable features is promising to distinguish the crucial clues from the distributional uncertainty but largely unexplored. A paradigm of simultaneously exploring the sensitivity to stable features and insensitivity to environmental features is until-now lacking to achieve the generalizable graph classification, to the best of our knowledge. In this work, we conjecture that generalizable models should be sensitive to stable features and insensitive to environmental features. To this end, we propose a simple yet effective augmentation strategy for graph classification: E quivariant and I nvariant C ross- D ata A ugmentation (EI-CDA). By employing equivariance, given a pair of input graphs, we first estimate their stable and environmental features via masks. Then we linearly mix the estimated stable features of two graphs and encourage the model predictions faithfully reflect their mixed semantics. Meanwhile, by using invariance, we swap the estimated environmental features of two graphs and keep the predictions invariant. This simple yet effective strategy endows the models with both sensitivity to stable features and insensitivity to environmental features. Extensive experiments show that EI-CDA significantly improves performance and outperforms leading baselines. Our codes are available at: https://github.com/yongduosui/EI-GNN.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4404811035",
    "type": "article"
  },
  {
    "title": "Clustering Aggregation (long version)",
    "doi": null,
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Aristides Gionis; Heikki Mannila; Panayiotis Tsaparas",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2707138032",
    "type": "article"
  },
  {
    "title": "Twain",
    "doi": "https://doi.org/10.1145/1267066.1267069",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Jen-Wei Huang; Bi-Ru Dai; Ming-Syan Chen⋆",
    "corresponding_authors": "",
    "abstract": "We investigate the general model of mining associations in a temporal database, where the exhibition periods of items are allowed to be different from one to another. The database is divided into partitions according to the time granularity imposed. Such temporal association rules allow us to observe short-term but interesting patterns that are absent when the whole range of the database is evaluated altogether. Prior work may omit some temporal association rules and thus have limited practicability. To remedy this and to give more precise frequent exhibition periods of frequent temporal itemsets, we devise an efficient algorithm Twain (standing for TWo end AssocIation miNer .) Twain not only generates frequent patterns with more precise frequent exhibition periods, but also discovers more interesting frequent patterns. Twain employs Start time and End time of each item to provide precise frequent exhibition period while progressively handling itemsets from one partition to another. Along with one scan of the database, Twain can generate frequent 2-itemsets directly according to the cumulative filtering threshold. Then, Twain adopts the scan reduction technique to generate all frequent k -itemsets ( k &gt; 2) from the generated frequent 2-itemsets. Theoretical properties of Twain are derived as well in this article. The experimental results show that Twain outperforms the prior works in the quality of frequent patterns, execution time, I/O cost, CPU overhead and scalability.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W1966582207",
    "type": "article"
  },
  {
    "title": "Sequential Modeling of Topic Dynamics with Multiple Timescales",
    "doi": "https://doi.org/10.1145/2086737.2086739",
    "publication_date": "2012-01-31",
    "publication_year": 2012,
    "authors": "Tomoharu Iwata; Takeshi Yamada; Yasushi Sakurai; Naonori Ueda",
    "corresponding_authors": "",
    "abstract": "We propose an online topic model for sequentially analyzing the time evolution of topics in document collections. Topics naturally evolve with multiple timescales. For example, some words may be used consistently over one hundred years, while other words emerge and disappear over periods of a few days. Thus, in the proposed model, current topic-specific distributions over words are assumed to be generated based on the multiscale word distributions of the previous epoch. Considering both the long- and short-timescale dependency yields a more robust model. We derive efficient online inference procedures based on a stochastic EM algorithm, in which the model is sequentially updated using newly obtained data; this means that past data are not required to make the inference. We demonstrate the effectiveness of the proposed method in terms of predictive performance and computational efficiency by examining collections of real documents with timestamps.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2042437708",
    "type": "article"
  },
  {
    "title": "Probabilistic Reframing for Cost-Sensitive Regression",
    "doi": "https://doi.org/10.1145/2641758",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "José Hernández‐Orallo",
    "corresponding_authors": "José Hernández‐Orallo",
    "abstract": "Common-day applications of predictive models usually involve the full use of the available contextual information. When the operating context changes, one may fine-tune the by-default (incontextual) prediction or may even abstain from predicting a value (a reject). Global reframing solutions, where the same function is applied to adapt the estimated outputs to a new cost context, are possible solutions here. An alternative approach, which has not been studied in a comprehensive way for regression in the knowledge discovery and data mining literature, is the use of a local (e.g., probabilistic) reframing approach, where decisions are made according to the estimated output and a reliability, confidence, or probability estimation. In this article, we advocate for a simple two-parameter (mean and variance) approach, working with a normal conditional probability density. Given the conditional mean produced by any regression technique, we develop lightweight “enrichment” methods that produce good estimates of the conditional variance, which are used by the probabilistic (local) reframing methods. We apply these methods to some very common families of cost-sensitive problems, such as optimal predictions in (auction) bids, asymmetric loss scenarios, and rejection rules.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1975749451",
    "type": "article"
  },
  {
    "title": "Recommending Users and Communities in Social Media",
    "doi": "https://doi.org/10.1145/2757282",
    "publication_date": "2015-10-26",
    "publication_year": 2015,
    "authors": "Li Lei; Wei Peng; Saurabh Kataria; Tong Sun; Li Tao",
    "corresponding_authors": "",
    "abstract": "Social media has become increasingly prevalent in the last few years, not only enabling people to connect with each other by social links, but also providing platforms for people to share information and interact over diverse topics. Rich user-generated information, for example, users’ relationships and daily posts, are often available in most social media service websites. Given such information, a challenging problem is to provide reasonable user and community recommendation for a target user, and consequently, help the target user engage in the daily discussions and activities with his/her friends or like-minded people. In this article, we propose a unified framework of recommending users and communities that utilizes the information in social media. Given a user’s profile or a set of keywords as input, our framework is capable of recommending influential users and topic-cohesive interactive communities that are most relevant to the given user or keywords. With the proposed framework, users can find other individuals or communities sharing similar interests, and then have more interaction with these users or within the communities. We present a generative topic model to discover user-oriented and community-oriented topics simultaneously, which enables us to capture the exact topical interests of users, as well as the focuses of communities. Extensive experimental evaluation and case studies on a dataset collected from Twitter demonstrate the effectiveness of our proposed framework compared with other probabilistic-topic-model-based recommendation methods.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1986668507",
    "type": "article"
  },
  {
    "title": "Predicting and Identifying Missing Node Information in Social Networks",
    "doi": "https://doi.org/10.1145/2536775",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Ron Eyal; Avi Rosenfeld; Sigal Sina; Sarit Kraus",
    "corresponding_authors": "",
    "abstract": "In recent years, social networks have surged in popularity. One key aspect of social network research is identifying important missing information that is not explicitly represented in the network, or is not visible to all. To date, this line of research typically focused on finding the connections that are missing between nodes, a challenge typically termed as the link prediction problem . This article introduces the missing node identification problem, where missing members in the social network structure must be identified. In this problem, indications of missing nodes are assumed to exist. Given these indications and a partial network, we must assess which indications originate from the same missing node and determine the full network structure. Toward solving this problem, we present the missing node identification by spectral clustering algorithm (MISC), an approach based on a spectral clustering algorithm, combined with nodes’ pairwise affinity measures that were adopted from link prediction research. We evaluate the performance of our approach in different problem settings and scenarios, using real-life data from Facebook. The results show that our approach has beneficial results and can be effective in solving the missing node identification problem. In addition, this article also presents R-MISC, which uses a sparse matrix representation, efficient algorithms for calculating the nodes’ pairwise affinity, and a proprietary dimension reduction technique to enable scaling the MISC algorithm to large networks of more than 100,000 nodes. Last, we consider problem settings where some of the indications are unknown. Two algorithms are suggested for this problem: speculative MISC, based on MISC, and missing link completion, based on classical link prediction literature. We show that speculative MISC outperforms missing link completion.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2051475803",
    "type": "article"
  },
  {
    "title": "User Vulnerability and Its Reduction on a Social Networking Site",
    "doi": "https://doi.org/10.1145/2630421",
    "publication_date": "2014-09-23",
    "publication_year": 2014,
    "authors": "Pritam Gundecha; Geoffrey Barbier; Jiliang Tang; Huan Liu",
    "corresponding_authors": "",
    "abstract": "Privacy and security are major concerns for many users of social media. When users share information (e.g., data and photos) with friends, they can make their friends vulnerable to security and privacy breaches with dire consequences. With the continuous expansion of a user’s social network, privacy settings alone are often inadequate to protect a user’s profile. In this research, we aim to address some critical issues related to privacy protection: (1) How can we measure and assess individual users’ vulnerability? (2) With the diversity of one’s social network friends, how can one figure out an effective approach to maintaining balance between vulnerability and social utility? In this work, first we present a novel way to define vulnerable friends from an individual user’s perspective. User vulnerability is dependent on whether or not the user’s friends’ privacy settings protect the friend and the individual’s network of friends (which includes the user). We show that it is feasible to measure and assess user vulnerability and reduce one’s vulnerability without changing the structure of a social networking site. The approach is to unfriend one’s most vulnerable friends. However, when such a vulnerable friend is also socially important, unfriending him or her would significantly reduce one’s own social status. We formulate this novel problem as vulnerability minimization with social utility constraints. We formally define the optimization problem and provide an approximation algorithm with a proven bound. Finally, we conduct a large-scale evaluation of a new framework using a Facebook dataset. We resort to experiments and observe how much vulnerability an individual user can be decreased by unfriending a vulnerable friend. We compare performance of different unfriending strategies and discuss the security risk of new friend requests. Additionally, by employing different forms of social utility, we confirm that the balance between user vulnerability and social utility can be practically achieved.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2057750974",
    "type": "article"
  },
  {
    "title": "Generating Realistic Synthetic Population Datasets",
    "doi": "https://doi.org/10.1145/3182383",
    "publication_date": "2018-04-16",
    "publication_year": 2018,
    "authors": "Hao Wu; Yue Ning; Prithwish Chakraborty; Jilles Vreeken; Nikolaj Tatti; Naren Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "Modern studies of societal phenomena rely on the availability of large datasets capturing attributes and activities of synthetic, city-level, populations. For instance, in epidemiology, synthetic population datasets are necessary to study disease propagation and intervention measures before implementation. In social science, synthetic population datasets are needed to understand how policy decisions might affect preferences and behaviors of individuals. In public health, synthetic population datasets are necessary to capture diagnostic and procedural characteristics of patient records without violating confidentialities of individuals. To generate such datasets over a large set of categorical variables, we propose the use of the maximum entropy principle to formalize a generative model such that in a statistically well-founded way we can optimally utilize given prior information about the data, and are unbiased otherwise. An efficient inference algorithm is designed to estimate the maximum entropy model, and we demonstrate how our approach is adept at estimating underlying data distributions. We evaluate this approach against both simulated data and US census datasets, and demonstrate its feasibility using an epidemic simulation application.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2280339326",
    "type": "article"
  },
  {
    "title": "Exploring Multiobjective Optimization for Multiview Clustering",
    "doi": "https://doi.org/10.1145/3182181",
    "publication_date": "2018-05-23",
    "publication_year": 2018,
    "authors": "Sriparna Saha; Sayantan Mitra; Stefan Krämer",
    "corresponding_authors": "",
    "abstract": "We present a new multiview clustering approach based on multiobjective optimization. In contrast to existing clustering algorithms based on multiobjective optimization, it is generally applicable to data represented by two or more views and does not require specifying the number of clusters a priori . The approach builds upon the search capability of a multiobjective simulated annealing based technique, AMOSA, as the underlying optimization technique. In the first version of the proposed approach, an internal cluster validity index is used to assess the quality of different partitionings obtained using different views. A new way of checking the compatibility of these different partitionings is also proposed and this is used as another objective function. A new encoding strategy and some new mutation operators are introduced. Finally, a new way of computing a consensus partitioning from multiple individual partitions obtained on multiple views is proposed. As a baseline and for comparison, two multiobjective based ensemble clustering techniques are proposed to combine the outputs of different simple clustering approaches. The efficacy of the proposed clustering methods is shown for partitioning several real-world datasets having multiple views. To show the practical usefulness of the method, we present results on web-search result clustering, where the task is to find a suitable partitioning of web snippets.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2804849150",
    "type": "article"
  },
  {
    "title": "Modeling of Geographic Dependencies for Real Estate Ranking",
    "doi": "https://doi.org/10.1145/2934692",
    "publication_date": "2016-08-27",
    "publication_year": 2016,
    "authors": "Yanjie Fu; Hui Xiong; Yong Ge; Yu Zheng; Zijun Yao; Zhi‐Hua Zhou",
    "corresponding_authors": "",
    "abstract": "It is traditionally a challenge for home buyers to understand, compare, and contrast the investment value of real estate. Although a number of appraisal methods have been developed to value real properties, the performances of these methods have been limited by traditional data sources for real estate appraisal. With the development of new ways of collecting estate-related mobile data, there is a potential to leverage geographic dependencies of real estate for enhancing real estate appraisal. Indeed, the geographic dependencies of the investment value of an estate can be from the characteristics of its own neighborhood (individual), the values of its nearby estates (peer), and the prosperity of the affiliated latent business area (zone). To this end, in this paper, we propose a geographic method, named ClusRanking, for real estate appraisal by leveraging the mutual enforcement of ranking and clustering power. ClusRanking is able to exploit geographic individual, peer, and zone dependencies in a probabilistic ranking model. Specifically, we first extract the geographic utility of estates from geography data, estimate the neighborhood popularity of estates by mining taxicab trajectory data, and model the influence of latent business areas. Also, we fuse these three influential factors and predict real estate investment value. Moreover, we simultaneously consider individual, peer and zone dependencies, and derive an estate-specific ranking likelihood as the objective function. Furthermore, we propose an improved method named CR-ClusRanking by incorporating checkin information as a regularization term which reduces the performance volatility of real estate ranking system. Finally, we conduct a comprehensive evaluation with the real estate-related data of Beijing, and the experimental results demonstrate the effectiveness of our proposed methods.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2461100794",
    "type": "article"
  },
  {
    "title": "Spatio-Temporal Routine Mining on Mobile Phone Data",
    "doi": "https://doi.org/10.1145/3201577",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Tian Qin; Wufan Shangguan; Guojie Song; Jie Tang",
    "corresponding_authors": "",
    "abstract": "Mining human behaviors has always been an important subarea of Data Mining. While it provides empirical evidences to psychological/behavioral studies, it also builds the foundation of various big-data systems, which rely heavily on the prediction of human behaviors. In recent years, the ubiquitous spreading of mobile phones and the massive amount of spatio-temporal data collected from them make it possible to keep track of the daily commute behaviors of mobile subscribers and further conduct routine mining on them. In this article, we propose to model mobile subscribers’ daily commute behaviors by three levels: location trajectory, one-day pattern, and routine pattern. We develop the model Spatio-Temporal Routine Mining Model (STRMM) to characterize the generative process between these three levels. From daily trajectories, the STRMM model unsupervisedly extracts spatio-temporal routine patterns that contain two aspects of information: (1) How people’s typical commute patterns are. (2) How much their commute behaviors vary from day to day. Compared to traditional methods, STRMM takes into account the different degrees of behavioral uncertainty in different timespans of a day, yielding more realistic and intuitive results. To learn model parameters, we adopt Stochastic Expectation Maximization algorithm. Experiments are conducted on two real world datasets, and the empirical results show that the STRMM model can effectively discover hidden routine patterns of human commute behaviors and yields higher accuracy results in trajectory prediction task.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2809783195",
    "type": "article"
  },
  {
    "title": "Fast Parallel Algorithms for Counting and Listing Triangles in Big Graphs",
    "doi": "https://doi.org/10.1145/3365676",
    "publication_date": "2019-12-13",
    "publication_year": 2019,
    "authors": "Shaikh Arifuzzaman; Maleq Khan; Madhav Marathe",
    "corresponding_authors": "",
    "abstract": "Big graphs (networks) arising in numerous application areas pose significant challengesfor graph analysts as these graphs grow to billions of nodes and edges and are prohibitively large to fit in the main memory. Finding the number of triangles in a graph is an important problem in the mining and analysis of graphs. In this article, we present two efficient MPI-based distributed memory parallel algorithms for counting triangles in big graphs. The first algorithm employs overlapping partitioning and efficient load balancing schemes to provide a very fast parallel algorithm. The algorithm scales well to networks with billions of nodes and can compute the exact number of triangles in a network with 10 billion edges in 16 minutes. The second algorithm divides the network into non-overlapping partitions leading to a space-efficient algorithm. Our results on both artificial and real-world networks demonstrate a significant space saving with this algorithm. We also present a novel approach that reduces communication cost drastically leading the algorithm to both a space- and runtime-efficient algorithm. Further, we demonstrate how our algorithms can be used to list all triangles in a graph and compute clustering coefficients of nodes. Our algorithm can also be adapted to a parallel approximation algorithm using an edge sparsification method.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3004718539",
    "type": "article"
  },
  {
    "title": "Discovering Social Circles in Directed Graphs",
    "doi": "https://doi.org/10.1145/2641759",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "Scott H. Burton; Christophe Giraud-Carrier",
    "corresponding_authors": "",
    "abstract": "We examine the problem of identifying social circles, or sets of cohesive and mutually aware nodes surrounding an initial query set, in directed graphs where the complete graph is not known beforehand. This problem differs from local community mining, in that the query set defines the circle of interest. We explicitly handle edge direction, as in many cases relationships are not symmetric, and focus on the local context because many real-world graphs cannot be feasibly known. We outline several issues that are unique to this context, introduce a quality function to measure the value of including a particular node in an emerging social circle, and describe a greedy social circle discovery algorithm. We demonstrate the effectiveness of this approach on artificial benchmarks, large networks with topical community labels, and several real-world case studies.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1987498565",
    "type": "article"
  },
  {
    "title": "Ranking Metric Anomaly in Invariant Networks",
    "doi": "https://doi.org/10.1145/2601436",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Yong Ge; Guofei Jiang; Min Ding; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "The management of large-scale distributed information systems relies on the effective use and modeling of monitoring data collected at various points in the distributed information systems. A traditional approach to model monitoring data is to discover invariant relationships among the monitoring data. Indeed, we can discover all invariant relationships among all pairs of monitoring data and generate invariant networks, where a node is a monitoring data source (metric) and a link indicates an invariant relationship between two monitoring data. Such an invariant network representation can help system experts to localize and diagnose the system faults by examining those broken invariant relationships and their related metrics, since system faults usually propagate among the monitoring data and eventually lead to some broken invariant relationships. However, at one time, there are usually a lot of broken links (invariant relationships) within an invariant network. Without proper guidance, it is difficult for system experts to manually inspect this large number of broken links. To this end, in this article, we propose the problem of ranking metrics according to the anomaly levels for a given invariant network, while this is a nontrivial task due to the uncertainties and the complex nature of invariant networks. Specifically, we propose two types of algorithms for ranking metric anomaly by link analysis in invariant networks. Along this line, we first define two measurements to quantify the anomaly level of each metric, and introduce the m R ank algorithm. Also, we provide a weighted score mechanism and develop the g R ank algorithm, which involves an iterative process to obtain a score to measure the anomaly levels. In addition, some extended algorithms based on m R ank and g R ank algorithms are developed by taking into account the probability of being broken as well as noisy links. Finally, we validate all the proposed algorithms on a large number of real-world and synthetic data sets to illustrate the effectiveness and efficiency of different algorithms.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2073865925",
    "type": "article"
  },
  {
    "title": "Algorithms for Mining the Coevolving Relational Motifs in Dynamic Networks",
    "doi": "https://doi.org/10.1145/2733380",
    "publication_date": "2015-07-22",
    "publication_year": 2015,
    "authors": "Rezwan Ahmed; George Karypis",
    "corresponding_authors": "",
    "abstract": "Computational methods and tools that can efficiently and effectively analyze the temporal changes in dynamic complex relational networks enable us to gain significant insights regarding the entity relations and their evolution. This article introduces a new class of dynamic graph patterns, referred to as coevolving relational motifs (CRMs), which are designed to identify recurring sets of entities whose relations change in a consistent way over time. CRMs can provide evidence to the existence of, possibly unknown, coordination mechanisms by identifying the relational motifs that evolve in a similar and highly conserved fashion. We developed an algorithm to efficiently analyze the frequent relational changes between the entities of the dynamic networks and capture all frequent coevolutions as CRMs. Our algorithm follows a depth-first exploration of the frequent CRM lattice and incorporates canonical labeling for redundancy elimination. Experimental results based on multiple real world dynamic networks show that the method is able to efficiently identify CRMs. In addition, a qualitative analysis of the results shows that the discovered patterns can be used as features to characterize the dynamic network.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2255459394",
    "type": "article"
  },
  {
    "title": "A Novel Bipartite Graph Based Competitiveness Degree Analysis from Query Logs",
    "doi": "https://doi.org/10.1145/2996196",
    "publication_date": "2016-12-03",
    "publication_year": 2016,
    "authors": "Qiang Wei; Dandan Qiao; Jin Zhang; Guoqing Chen; Xunhua Guo",
    "corresponding_authors": "",
    "abstract": "Competitiveness degree analysis is a focal point of business strategy and competitive intelligence, aimed to help managers closely monitor to what extent their rivals are competing with them. This article proposes a novel method, namely BCQ, to measure the competitiveness degree between peers from query logs as an important form of user generated contents, which reflects the “wisdom of crowds” from the search engine users’ perspective. In doing so, a bipartite graph model is developed to capture the competitive relationships through conjoint attributes hidden in query logs, where the notion of competitiveness degree for entity pairs is introduced, and then used to identify the competitive paths mapped in the bipartite graph. Subsequently, extensive experiments are conducted to demonstrate the effectiveness of BCQ to quantify the competitiveness degrees. Experimental results reveal that BCQ can well support competitors ranking, which is helpful for devising competitive strategies and pursuing market performance. In addition, efficiency experiments on synthetic data show a good scalability of BCQ on large scale of query logs.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2560314775",
    "type": "article"
  },
  {
    "title": "The Relationship between Online Social Network Ties and User Attributes",
    "doi": "https://doi.org/10.1145/3314204",
    "publication_date": "2019-05-07",
    "publication_year": 2019,
    "authors": "Amin Mahmoudi; Mohd Ridzwan Yaakub; Azuraliza Abu Bakar",
    "corresponding_authors": "",
    "abstract": "The distance between users has an effect on the formation of social network ties, but it is not the only or even the main factor. Knowing all the features that influence such ties is very important for many related domains such as location-based recommender systems and community and event detection systems for online social networks (OSNs). In recent years, researchers have analyzed the role of user geo-location in OSNs. Researchers have also attempted to determine the probability of friendships being established based on distance, where friendship is not only a function of distance. However, some important features of OSNs remain unknown. In order to comprehensively understand the OSN phenomenon, we also need to analyze users’ attributes. Basically, an OSN functions according to four main user properties: user geo-location, user weight, number of user interactions, and user lifespan. The research presented here sought to determine whether the user mobility pattern can be used to predict users’ interaction behavior. It also investigated whether, in addition to distance, the number of friends (known as user weight) interferes in social network tie formation. To this end, we analyzed the above-stated features in three large-scale OSNs. We found that regardless of a high degree freedom in user mobility, the fraction of the number of outside activities over the inside activity is a significant fraction that helps us to address the user interaction behavior. To the best of our knowledge, research has not been conducted elsewhere on this issue. We also present a high-resolution formula in order to improve the friendship probability function.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2944309452",
    "type": "article"
  },
  {
    "title": "Probabilistic Feature Selection and Classification Vector Machine",
    "doi": "https://doi.org/10.1145/3309541",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Bingbing Jiang; Li Chang; Maarten de Rijke; Xin Yao; Huanhuan Chen",
    "corresponding_authors": "",
    "abstract": "Sparse Bayesian learning is a state-of-the-art supervised learning algorithm that can choose a subset of relevant samples from the input data and make reliable probabilistic predictions. However, in the presence of high-dimensional data with irrelevant features, traditional sparse Bayesian classifiers suffer from performance degradation and low efficiency due to the incapability of eliminating irrelevant features. To tackle this problem, we propose a novel sparse Bayesian embedded feature selection algorithm that adopts truncated Gaussian distributions as both sample and feature priors. The proposed algorithm, called probabilistic feature selection and classification vector machine (PFCVM LP ) is able to simultaneously select relevant features and samples for classification tasks. In order to derive the analytical solutions, Laplace approximation is applied to compute approximate posteriors and marginal likelihoods. Finally, parameters and hyperparameters are optimized by the type-II maximum likelihood method. Experiments on three datasets validate the performance of PFCVM LP along two dimensions: classification performance and effectiveness for feature selection. Finally, we analyze the generalization performance and derive a generalization error bound for PFCVM LP . By tightening the bound, the importance of feature selection is demonstrated.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2964138195",
    "type": "article"
  },
  {
    "title": "CenEEGs",
    "doi": "https://doi.org/10.1145/3371153",
    "publication_date": "2020-02-18",
    "publication_year": 2020,
    "authors": "Chenglong Dai; Dechang Pi; Stefanie I. Becker; Jia Wu; Lin Cui; Blake W. Johnson",
    "corresponding_authors": "",
    "abstract": "This article explores valid brain electroencephalography (EEG) selection for EEG classification with different classifiers, which has been rarely addressed in previous studies and is mostly ignored by existing EEG processing methods and applications. Importantly, traditional selection methods are not able to select valid EEG signals for different classifiers. This article focuses on a source control-based valid EEG selection to reduce the impact of invalid EEG signals and aims to improve EEG-based classification performance for different classifiers. We propose a novel centroid-based EEG selection approach named CenEEGs, which uses a scale-and-shift-invariance similarity metric to measure similarities of EEG signals and then applies a globally optimal centroid strategy to select valid EEG signals with respect to a similarity threshold. A detailed comparison with several state-of-the-art time series selection methods by using standard criteria on 8 EEG datasets demonstrates the efficacy and superiority of CenEEGs for different classifiers.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3008551553",
    "type": "article"
  },
  {
    "title": "Learning Bayesian Networks with the Saiyan Algorithm",
    "doi": "https://doi.org/10.1145/3385655",
    "publication_date": "2020-06-22",
    "publication_year": 2020,
    "authors": "Anthony C. Constantinou",
    "corresponding_authors": "Anthony C. Constantinou",
    "abstract": "Some structure learning algorithms have proven to be effective in reconstructing hypothetical Bayesian Network graphs from synthetic data. However, in their mission to maximise a scoring function, many become conservative and minimise edges discovered. While simplicity is desired, the output is often a graph that consists of multiple independent subgraphs that do not enable full propagation of evidence. While this is not a problem in theory, it can be a problem in practice. This article examines a novel unconventional associational heuristic called Saiyan, which returns a directed acyclic graph that enables full propagation of evidence. Associational heuristics are not expected to perform well relative to sophisticated constraint-based and score-based learning approaches. Moreover, forcing the algorithm to connect all data variables implies that the forced edges will not be correct at the rate of those identified unrestrictedly. Still, synthetic and real-world experiments suggest that such a heuristic can be competitive relative to some of the well-established constraint-based, score-based and hybrid learning algorithms.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3036150266",
    "type": "article"
  },
  {
    "title": "Multi-User Mobile Sequential Recommendation for Route Optimization",
    "doi": "https://doi.org/10.1145/3360048",
    "publication_date": "2020-07-06",
    "publication_year": 2020,
    "authors": "Keli Xiao; Zeyang Ye; Lihao Zhang; Wenjun Zhou; Yong Ge; Yuefan Deng",
    "corresponding_authors": "",
    "abstract": "We enhance the mobile sequential recommendation (MSR) model and address some critical issues in existing formulations by proposing three new forms of the MSR from a multi-user perspective. The multi-user MSR (MMSR) model searches optimal routes for multiple drivers at different locations while disallowing overlapping routes to be recommended. To enrich the properties of pick-up points in the problem formulation, we additionally consider the pick-up capacity as an important feature, leading to the following two modified forms of the MMSR: MMSR-m and MMSR-d. The MMSR-m sets a maximum pick-up capacity for all urban areas, while the MMSR-d allows the pick-up capacity to vary at different locations. We develop a parallel framework based on the simulated annealing to numerically solve the MMSR problem series. Also, a push-point method is introduced to improve our algorithms further for the MMSR-m and the MMSR-d, which can handle the route optimization in more practical ways. Our results on both real-world and synthetic data confirmed the superiority of our problem formulation and solutions under more demanding practical scenarios over several published benchmarks.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3038248204",
    "type": "article"
  },
  {
    "title": "NGUARD+",
    "doi": "https://doi.org/10.1145/3399711",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Jiarong Xu; Yifan Luo; Jianrong Tao; Changjie Fan; Zhou Zhao; Jiangang Lu",
    "corresponding_authors": "",
    "abstract": "Game bots are automated programs that assist cheating users, leading to an imbalance in the game ecosystem and the collapse of user interest. Online games provide immersive gaming experience and attract many loyal fans. However, game bots have proliferated in volume and method, evolving with the real-world detection methods and showing strong diversity, leaving game bot detection efforts extremely difficult. Existing game bot detection techniques mostly rely on handcrafted features or time-series based features instead of fully utilizing player behavior sequences. In this regard, a more reasonable way should be learning user patterns from player behavior sequences when facing the fast-changing nature of game bots. Here we propose a general game bot detection framework for massively multiplayer online role playing games termed NGUARD+ (denoting NetEase Games’ Guard), which captures user patterns in order to identify game bots from player behavior sequences. NGUARD+ mainly employs attention-based methods to automatically differentiate game bots from humans. We provide a combination of supervised and unsupervised methods for game bot detection to detect game bots and new type of game bots even when the labels of game bots are limited. Specifically, we propose the following two variants for attention-based sequence modeling: Attention based Bidirectional Long Short-Term Memory Networks (ABLSTM) and Hierarchical Self-Attention Network (HSAN) as our supervised models. ABLSTM is keen on inducing certain inductive biases which makes learning more reasonable as well as capturing local dependency and global information, while HSAN could handle much longer behavior sequences with less memory and higher computational efficiency. Experiments conducted on a real-world dataset show that NGUARD+ can achieve remarkable performance improvement compared to traditional methods. Moreover, NGUARD+ can reveal outstanding robustness for game bots in mutated patterns and even in completely unseen patterns.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3089394919",
    "type": "article"
  },
  {
    "title": "Bi-Directional Recurrent Attentional Topic Model",
    "doi": "https://doi.org/10.1145/3412371",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Shuangyin Li; Yu Zhang; Rong Pan",
    "corresponding_authors": "",
    "abstract": "In a document, the topic distribution of a sentence depends on both the topics of its neighbored sentences and its own content, and it is usually affected by the topics of the neighbored sentences with different weights. The neighbored sentences of a sentence include the preceding sentences and the subsequent sentences. Meanwhile, it is natural that a document can be treated as a sequence of sentences. Most existing works for Bayesian document modeling do not take these points into consideration. To fill this gap, we propose a bi-Directional Recurrent Attentional Topic Model (bi-RATM) for document embedding. The bi-RATM not only takes advantage of the sequential orders among sentences but also uses the attention mechanism to model the relations among successive sentences. To support to the bi-RATM, we propose a bi-Directional Recurrent Attentional Bayesian Process (bi-RABP) to handle the sequences. Based on the bi-RABP, bi-RATM fully utilizes the bi-directional sequential information of the sentences in a document. Online bi-RATM is proposed to handle large-scale corpus. Experiments on two corpora show that the proposed model outperforms state-of-the-art methods on document modeling and classification.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3091499889",
    "type": "article"
  },
  {
    "title": "Exploiting Viral Marketing for Location Promotion in Location-Based Social Networks",
    "doi": "https://doi.org/10.1145/3001938",
    "publication_date": "2016-11-15",
    "publication_year": 2016,
    "authors": "Wen-Yuan Zhu; Wen-Chih Peng; Ling‐Jyh Chen; Kai Zheng; Xiaofang Zhou",
    "corresponding_authors": "",
    "abstract": "With the explosion of smartphones and social network services, location-based social networks (LBSNs) are increasingly seen as tools for businesses (e.g., restaurants and hotels) to promote their products and services. In this article, we investigate the key techniques that can help businesses promote their locations by advertising wisely through the underlying LBSNs. In order to maximize the benefit of location promotion, we formalize it as an influence maximization problem in an LBSN, i.e., given a target location and an LBSN, a set of k users (called seeds) should be advertised initially such that they can successfully propagate and attract many other users to visit the target location. Existing studies have proposed different ways to calculate the information propagation probability, that is, how likely it is that a user may influence another, in the setting of a static social network. However, it is more challenging to derive the propagation probability in an LBSN since it is heavily affected by the target location and the user mobility, both of which are dynamic and query dependent. This article proposes two user mobility models, namely the Gaussian-based and distance-based mobility models, to capture the check-in behavior of individual LBSN users, based on which location-aware propagation probabilities can be derived. Extensive experiments based on two real LBSN datasets have demonstrated the superior effectiveness of our proposals compared with existing static models of propagation probabilities to truly reflect the information propagation in LBSNs.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2553249525",
    "type": "article"
  },
  {
    "title": "Multi-task Information Bottleneck Co-clustering for Unsupervised Cross-view Human Action Categorization",
    "doi": "https://doi.org/10.1145/3375394",
    "publication_date": "2020-02-09",
    "publication_year": 2020,
    "authors": "Xiaoqiang Yan; Zhengzheng Lou; Shizhe Hu; Yangdong Ye",
    "corresponding_authors": "",
    "abstract": "The widespread adoption of low-cost cameras generates massive amounts of videos recorded from different viewpoints every day. To cope with this vast amount of unlabeled and heterogeneous data, a new multi-task information bottleneck co-clustering (MIBC) approach is proposed to automatically categorize human actions in collections of unlabeled cross-view videos. Our motivation is that, if a learning action category from each view is seen as a single task, it is reasonable to assume that the tasks of learning action patterns from the videos recorded by multiple cameras are dependent and inter-related, since the actions of the same subjects synchronously recorded from different camera viewpoints are complementary to each other. MIBC aims to transfer the shared view knowledge across multiple tasks (i.e., camera viewpoints) to boost the performance of each task. Specifically, MIBC involves the following two parts: (1) extracting action categories for each task by independently maintaining its own relevant information, and (2) allowing the feature representations of all tasks to be compressed into a common feature space, which is utilized to capture the relatedness of multiple tasks and transfer the shared knowledge across different camera viewpoints. These two parts of MIBC work simultaneously and can be solved in a novel co-clustering mechanism. Our experimental evaluation on several cross-view action collections shows that the MIBC algorithm outperforms the existing state-of-the-art baselines.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3007863152",
    "type": "article"
  },
  {
    "title": "An Approach For Concept Drift Detection in a Graph Stream Using Discriminative Subgraphs",
    "doi": "https://doi.org/10.1145/3406243",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Ramesh Paudel; William Eberle",
    "corresponding_authors": "",
    "abstract": "The emergence of mining complex networks like social media, sensor networks, and the world-wide-web has attracted considerable research interest. In a streaming scenario, the concept to be learned can change over time. However, while there has been some research done for detecting concept drift in traditional data streams, little work has been done on addressing concept drift in data represented as a graph . We propose a novel unsupervised concept-drift detection method on graph streams called Discriminative Subgraph-based Drift Detector (DSDD). The methodology starts by discovering discriminative subgraphs for each graph in the stream. We then compute the entropy of the window based on the distribution of discriminative subgraphs with respect to the graphs and then use the direct density-ratio estimation approach for detecting concept drift in the series of entropy values obtained by moving one step forward in the sliding window. The effectiveness of the proposed method is demonstrated through experiments using artificial and real-world datasets and its performance is evaluated by comparing against related baseline methods. Similarly, the usefulness of the proposed concept drift detection approach is studied by incorporating it in a popular graph stream classification algorithm and studying the impact of drift detection in classification accuracy.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3090388987",
    "type": "article"
  },
  {
    "title": "Attributed Network Embedding with Micro-Meso Structure",
    "doi": "https://doi.org/10.1145/3441486",
    "publication_date": "2021-04-18",
    "publication_year": 2021,
    "authors": "Juan-Hui Li; Ling Huang; Chang‐Dong Wang; Dong Huang; Jianhuang Lai; Pei Chen",
    "corresponding_authors": "",
    "abstract": "Recently, network embedding has received a large amount of attention in network analysis. Although some network embedding methods have been developed from different perspectives, on one hand, most of the existing methods only focus on leveraging the plain network structure, ignoring the abundant attribute information of nodes. On the other hand, for some methods integrating the attribute information, only the lower-order proximities (e.g., microscopic proximity structure) are taken into account, which may suffer if there exists the sparsity issue and the attribute information is noisy. To overcome this problem, the attribute information and mesoscopic community structure are utilized. In this article, we propose a novel network embedding method termed Attributed Network Embedding with Micro-Meso structure, which is capable of preserving both the attribute information and the structural information including the microscopic proximity structure and mesoscopic community structure. In particular, both the microscopic proximity structure and node attributes are factorized by Nonnegative Matrix Factorization (NMF), from which the low-dimensional node representations can be obtained. For the mesoscopic community structure, a community membership strength matrix is inferred by a generative model (i.e., BigCLAM) or modularity from the linkage structure, which is then factorized by NMF to obtain the low-dimensional node representations. The three components are jointly correlated by the low-dimensional node representations, from which two objective functions (i.e., ANEM_B and ANEM_M) can be defined. Two efficient alternating optimization schemes are proposed to solve the optimization problems. Extensive experiments have been conducted to confirm the superior performance of the proposed models over the state-of-the-art network embedding methods.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3153182418",
    "type": "article"
  },
  {
    "title": "Side Information Fusion for Recommender Systems over Heterogeneous Information Network",
    "doi": "https://doi.org/10.1145/3441446",
    "publication_date": "2021-06-10",
    "publication_year": 2021,
    "authors": "Huan Zhao; Quanming Yao; Yangqiu Song; James T. Kwok; Dik Lun Lee",
    "corresponding_authors": "",
    "abstract": "Collaborative filtering (CF) has been one of the most important and popular recommendation methods, which aims at predicting users’ preferences (ratings) based on their past behaviors. Recently, various types of side information beyond the explicit ratings users give to items, such as social connections among users and metadata of items, have been introduced into CF and shown to be useful for improving recommendation performance. However, previous works process different types of information separately, thus failing to capture the correlations that might exist across them. To address this problem, in this work, we study the application of heterogeneous information network (HIN), which offers a unifying and flexible representation of different types of side information, to enhance CF-based recommendation methods. However, we face challenging issues in HIN-based recommendation, i.e., how to capture similarities of complex semantics between users and items in a HIN, and how to effectively fuse these similarities to improve final recommendation performance. To address these issues, we apply metagraph to similarity computation and solve the information fusion problem with a “matrix factorization (MF) + factorization machine (FM)” framework. For the MF part, we obtain the user-item similarity matrix from each metagraph and then apply low-rank matrix approximation to obtain latent features for both users and items. For the FM part, we apply FM with Group lasso (FMG) on the features obtained from the MF part to train the recommending model and, at the same time, identify the useful metagraphs. Besides FMG, a two-stage method, we further propose an end-to-end method, hierarchical attention fusing, to fuse metagraph-based similarities for the final recommendation. Experimental results on four large real-world datasets show that the two proposed frameworks significantly outperform existing state-of-the-art methods in terms of recommendation performance.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3171643380",
    "type": "article"
  },
  {
    "title": "HARP",
    "doi": "https://doi.org/10.1145/3424673",
    "publication_date": "2021-01-04",
    "publication_year": 2021,
    "authors": "Yashen Wang; Huanhuan Zhang",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed great advancement of representation learning (RL)-based models for the knowledge graph relation prediction task. However, they generally rely on structure information embedded in the encyclopedic knowledge graph, while the beneficial semantic information provided by lexical knowledge graph is ignored, leading the problem of shallow understanding and coarse-grained analysis for knowledge acquisition. Therefore, this article introduces concept information derived from the lexical knowledge graph (e.g., Probase), and proposes a novel Hierarchical Attention model for Relation Prediction, which consists of entity-level attention mechanism and concept-level attention mechanism, to throughly integrate multiple semantic signals. Experimental results demonstrate the efficiency of the proposed method on two benchmark datasets.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3119483501",
    "type": "article"
  },
  {
    "title": "A Scalable Redefined Stochastic Blockmodel",
    "doi": "https://doi.org/10.1145/3442589",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Xueyan Liu; Bo Yang; Hechang Chen; Katarzyna Musiał; Hongxu Chen; Yang Li; Wanli Zuo",
    "corresponding_authors": "",
    "abstract": "Stochastic blockmodel (SBM) is a widely used statistical network representation model, with good interpretability, expressiveness, generalization, and flexibility, which has become prevalent and important in the field of network science over the last years. However, learning an optimal SBM for a given network is an NP-hard problem. This results in significant limitations when it comes to applications of SBMs in large-scale networks, because of the significant computational overhead of existing SBM models, as well as their learning methods. Reducing the cost of SBM learning and making it scalable for handling large-scale networks, while maintaining the good theoretical properties of SBM, remains an unresolved problem. In this work, we address this challenging task from a novel perspective of model redefinition. We propose a novel redefined SBM with Poisson distribution and its block-wise learning algorithm that can efficiently analyse large-scale networks. Extensive validation conducted on both artificial and real-world data shows that our proposed method significantly outperforms the state-of-the-art methods in terms of a reasonable trade-off between accuracy and scalability. 1",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3155044266",
    "type": "article"
  },
  {
    "title": "Mining Largest Maximal Quasi-Cliques",
    "doi": "https://doi.org/10.1145/3446637",
    "publication_date": "2021-04-15",
    "publication_year": 2021,
    "authors": "Seyed-Vahid Sanei-Mehri; Apurba Das; Hooman Hashemi; Srikanta Tirthapura",
    "corresponding_authors": "",
    "abstract": "Quasi-cliques are dense incomplete subgraphs of a graph that generalize the notion of cliques. Enumerating quasi-cliques from a graph is a robust way to detect densely connected structures with applications in bioinformatics and social network analysis. However, enumerating quasi-cliques in a graph is a challenging problem, even harder than the problem of enumerating cliques. We consider the enumeration of top- k degree-based quasi-cliques and make the following contributions: (1) we show that even the problem of detecting whether a given quasi-clique is maximal (i.e., not contained within another quasi-clique) is NP-hard. (2) We present a novel heuristic algorithm K ernel QC to enumerate the k largest quasi-cliques in a graph. Our method is based on identifying kernels of extremely dense subgraphs within a graph, followed by growing subgraphs around these kernels, to arrive at quasi-cliques with the required densities. (3) Experimental results show that our algorithm accurately enumerates quasi-cliques from a graph, is much faster than current state-of-the-art methods for quasi-clique enumeration (often more than three orders of magnitude faster), and can scale to larger graphs than current methods.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3156116725",
    "type": "article"
  },
  {
    "title": "Context-aware Spatial-Temporal Neural Network for Citywide Crowd Flow Prediction via Modeling Long-range Spatial Dependency",
    "doi": "https://doi.org/10.1145/3477577",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Jie Feng; Yong Li; Ziqian Lin; Can Rong; Funing Sun; Diansheng Guo; Depeng Jin",
    "corresponding_authors": "",
    "abstract": "Crowd flow prediction is of great importance in a wide range of applications from urban planning, traffic control to public safety. It aims at predicting the inflow (the traffic of crowds entering a region in a given time interval) and outflow (the traffic of crowds leaving a region for other places) of each region in the city with knowing the historical flow data. In this article, we propose DeepSTN+, a deep learning-based convolutional model, to predict crowd flows in the metropolis. First, DeepSTN+ employs the ConvPlus structure to model the long-range spatial dependence among crowd flows in different regions. Further, PoI distributions and time factor are combined to express the effect of location attributes to introduce prior knowledge of the crowd movements. Finally, we propose a temporal attention-based fusion mechanism to stabilize the training process, which further improves the performance. Extensive experimental results based on four real-life datasets demonstrate the superiority of our model, i.e., DeepSTN+ reduces the error of the crowd flow prediction by approximately 10%–21% compared with the state-of-the-art baselines.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3208454183",
    "type": "article"
  },
  {
    "title": "Network Embedding via Motifs",
    "doi": "https://doi.org/10.1145/3473911",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Ping Shao; Yang Yang; Shengyao Xu; Chunping Wang",
    "corresponding_authors": "",
    "abstract": "Network embedding has emerged as an effective way to deal with downstream tasks, such as node classification [ 16 , 31 , 42 ]. Most existing methods leverage multi-similarities between nodes such as connectivity, which considers vertices that are closely connected to be similar and structural similarity, which is measured by assessing their relations to neighbors; while these methods only focus on static graphs. In this work, we bridge connectivity and structural similarity in a uniform representation via motifs, and consequently present an algorithm for Learning Embeddings by leveraging Motifs Of Networks (LEMON), which aims to learn embeddings for vertices and various motifs. Moreover, LEMON is inherently capable of dealing with inductive learning tasks for dynamic graphs. To validate the effectiveness and efficiency, we conduct various experiments on two real-world datasets and five public datasets from diverse domains. Through comparison with state-of-the-art baseline models, we find that LEMON achieves significant improvements in downstream tasks. We release our code on Github at https://github.com/larry2020626/LEMON.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3208758912",
    "type": "article"
  },
  {
    "title": "HCBST: An Efficient Hybrid Sampling Technique for Class Imbalance Problems",
    "doi": "https://doi.org/10.1145/3488280",
    "publication_date": "2021-11-15",
    "publication_year": 2021,
    "authors": "Robert A. Sowah; Bernard Kuditchar; Godfrey A. Mills; Amevi Acakpovi; Ralph A. Twum; Gifty Buah; Robert Agboyi",
    "corresponding_authors": "",
    "abstract": "Class imbalance problem is prevalent in many real-world domains. It has become an active area of research. In binary classification problems, imbalance learning refers to learning from a dataset with a high degree of skewness to the negative class. This phenomenon causes classification algorithms to perform woefully when predicting positive classes with new examples. Data resampling, which involves manipulating the training data before applying standard classification techniques, is among the most commonly used techniques to deal with the class imbalance problem. This article presents a new hybrid sampling technique that improves the overall performance of classification algorithms for solving the class imbalance problem significantly. The proposed method called the Hybrid Cluster-Based Undersampling Technique (HCBST) uses a combination of the cluster undersampling technique to under-sample the majority instances and an oversampling technique derived from Sigma Nearest Oversampling based on Convex Combination, to oversample the minority instances to solve the class imbalance problem with a high degree of accuracy and reliability. The performance of the proposed algorithm was tested using 11 datasets from the National Aeronautics and Space Administration Metric Data Program data repository and University of California Irvine Machine Learning data repository with varying degrees of imbalance. Results were compared with classification algorithms such as the K-nearest neighbours, support vector machines, decision tree, random forest, neural network, AdaBoost, naïve Bayes, and quadratic discriminant analysis. Tests results revealed that for the same datasets, the HCBST performed better with average performances of 0.73, 0.67, and 0.35 in terms of performance measures of area under curve, geometric mean, and Matthews Correlation Coefficient, respectively, across all the classifiers used for this study. The HCBST has the potential of improving the performance of the class imbalance problem, which by extension, will improve on the various applications that rely on the concept for a solution.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3212971488",
    "type": "article"
  },
  {
    "title": "TAP: Traffic Accident Profiling via Multi-Task Spatio-Temporal Graph Representation Learning",
    "doi": "https://doi.org/10.1145/3564594",
    "publication_date": "2022-09-22",
    "publication_year": 2022,
    "authors": "Zhi Liu; Chen Yang; Feng Xia; Jixin Bian; Bing Zhu; Guojiang Shen; Xiangjie Kong",
    "corresponding_authors": "",
    "abstract": "Predicting traffic accidents can help traffic management departments respond to sudden traffic situations promptly, improve drivers’ vigilance, and reduce losses caused by traffic accidents. However, the causality of traffic accidents is complex and difficult to analyze. Most existing traffic accident prediction methods do not consider the dynamic spatio-temporal correlation of traffic data, which leads to unsatisfactory prediction accuracy. To address this issue, we propose a multi-task learning framework (TAP) based on the Spatio-temporal Variational Graph Auto-Encoders (ST-VGAE) for traffic accident profiling. We firstly capture the dynamic spatio-temporal correlation of traffic conditions through a spatio-temporal graph convolutional encoder and embed it as a low-latitude vector. Then, we use a multi-task learning scheme to combine external factors to generate the traffic accident profiling. Furthermore, we propose a traffic accident profiling application framework based on edge computing. This method increases the speed of calculation by offloading the calculation of traffic accident profiling to edge nodes. Finally, the experimental results on real datasets demonstrate that TAP outperforms other state-of-the-art baselines.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4296614391",
    "type": "article"
  },
  {
    "title": "Multi-Level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-Tagged Photos",
    "doi": "https://doi.org/10.1145/3582015",
    "publication_date": "2023-01-25",
    "publication_year": 2023,
    "authors": "Ling Chen; Dandan Lyu; Shanshan Yu; Gencai Chen",
    "corresponding_authors": "",
    "abstract": "Geo-tagged photo-based tourist attraction recommendation can discover users’ travel preferences from their taken photos, so as to recommend suitable tourist attractions to them. However, existing visual content-based methods cannot fully exploit the user and tourist attraction information of photos to extract visual features, and do not differentiate the significance of different photos. In this article, we propose multi-level visual similarity-based personalized tourist attraction recommendation using geo-tagged photos (MEAL). MEAL utilizes the visual contents of photos and interaction behavior data to obtain the final embeddings of users and tourist attractions, which are then used to predict the visit probabilities. Specifically, by crossing the user and tourist attraction information of photos, we define four visual similarity levels and introduce a corresponding quintuplet loss to embed the visual contents of photos. In addition, to capture the significance of different photos, we exploit the self-attention mechanism to obtain the visual representations of users and tourist attractions. We conducted experiments on two datasets crawled from Flickr, and the experimental results proved the advantage of this method.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4317935377",
    "type": "article"
  },
  {
    "title": "Efficient and Effective Academic Expert Finding on Heterogeneous Graphs through ( <i>k</i> , 𝒫)-Core based Embedding",
    "doi": "https://doi.org/10.1145/3578365",
    "publication_date": "2023-02-09",
    "publication_year": 2023,
    "authors": "Yuxiang Wang; Jun Liu; Xiaoliang Xu; Xiangyu Ke; Tianxing Wu; Xiaoxuan Gou",
    "corresponding_authors": "",
    "abstract": "Expert finding is crucial for a wealth of applications in both academia and industry. Given a user query and trove of academic papers, expert finding aims at retrieving the most relevant experts for the query, from the academic papers. Existing studies focus on embedding-based solutions that consider academic papers’ textual semantic similarities to a query via document representation and extract the top- n experts from the most similar papers. Beyond implicit textual semantics, however, papers’ explicit relationships (e.g., co-authorship) in a heterogeneous graph (e.g., DBLP) are critical for expert finding, because they help improve the representation quality. Despite their importance, the explicit relationships of papers generally have been ignored in the literature. In this article, we study expert finding on heterogeneous graphs by considering both the explicit relationships and implicit textual semantics of papers in one model. Specifically, we define the cohesive ( k , 𝒫)-core community of papers w.r.t. a meta-path 𝒫 (i.e., relationship) and propose a ( k , 𝒫)-core based document embedding model to enhance the representation quality. Based on this, we design a proximity graph-based index (PG-Index) of papers and present a threshold algorithm (TA)-based method to efficiently extract top- n experts from papers returned by PG-Index. We further optimize our approach in two ways: (1) we boost effectiveness by considering the ( k , 𝒫)-core community of experts and the diversity of experts’ research interests, to achieve high-quality expert representation from paper representation; and (2) we streamline expert finding, going from “extract top- n experts from top- m ( m&gt; n ) semantically similar papers” to “directly return top- n experts”. The process of returning a large number of top- m papers as intermediate data is avoided, thereby improving the efficiency. Extensive experiments using real-world datasets demonstrate our approach’s superiority.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4319736592",
    "type": "article"
  },
  {
    "title": "Improving Node Classification Accuracy of GNN through Input and Output Intervention",
    "doi": "https://doi.org/10.1145/3610535",
    "publication_date": "2023-07-22",
    "publication_year": 2023,
    "authors": "Anjan Chowdhury; Sriram Srinivasan; Animesh Mukherjee; Sanjukta Bhowmick; Kuntal Ghosh",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) are a popular machine learning framework for solving various graph processing applications. This framework exploits both the graph topology and the feature vectors of the nodes. One of the important applications of GNN is in the semi-supervised node classification task. The accuracy of the node classification using GNN depends on (i) the number and (ii) the choice of the training nodes. In this article, we demonstrate that increasing the training nodes by selecting nodes from the same class that are spread out across non-contiguous subgraphs, can significantly improve the accuracy. We accomplish this by presenting a novel input intervention technique that can be used in conjunction with different GNN classification methods to increase the non-contiguous training nodes and, thereby, improve the accuracy. We also present an output intervention technique to identify misclassified nodes and relabel them with their potentially correct labels. We demonstrate on real-world networks that our proposed methods, both individually and collectively, significantly improve the accuracy in comparison to the baseline GNN algorithms. Both our methods are agnostic. Apart from the initial set of training nodes generated by the baseline GNN methods, our techniques do not need any other extra knowledge about the classes of the nodes. Thus, our methods are modular and can be used as pre-and post-processing steps with many of the currently available GNN methods to improve their accuracy.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4385075286",
    "type": "article"
  },
  {
    "title": "Modeling Users’ Curiosity in Recommender Systems",
    "doi": "https://doi.org/10.1145/3617598",
    "publication_date": "2023-08-25",
    "publication_year": 2023,
    "authors": "Zhe Fu; Xi Niu",
    "corresponding_authors": "",
    "abstract": "Today’s recommender systems are criticized for recommending items that are too obvious to arouse users’ interests. Therefore, the research community has advocated some “beyond accuracy” evaluation metrics such as novelty, diversity, and serendipity with the hope of promoting information discovery and sustaining users’ interests over a long period of time. While bringing in new perspectives, most of these evaluation metrics have not considered individual users’ differences in their capacity to experience those “beyond accuracy” items. Open-minded users may embrace a wider range of recommendations than conservative users. In this article, we proposed to use curiosity traits to capture such individual users’ differences. We developed a model to approximate an individual’s curiosity distribution over different stimulus levels. We used an item’s surprise level to estimate the stimulus level and whether such a level is in the range of the user’s appetite for stimulus, called Comfort Zone . We then proposed a recommender system framework that considers both user preference and their Comfort Zone where the curiosity is maximally aroused. Our framework differs from a typical recommender system in that it leverages human’s Comfort Zone for stimuli to promote engagement with the system. A series of evaluation experiments have been conducted to show that our framework is able to rank higher the items with not only high ratings but also high curiosity stimulation. The recommendation list generated by our algorithm has a higher potential of inspiring user curiosity compared to the state-of-the-art deep learning approaches. The personalization factor for assessing the surprise stimulus levels further helps the recommender model achieve smaller (better) inter-user similarity.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4386162431",
    "type": "article"
  },
  {
    "title": "Adaptive Adversarial Contrastive Learning for Cross-Domain Recommendation",
    "doi": "https://doi.org/10.1145/3630259",
    "publication_date": "2023-11-02",
    "publication_year": 2023,
    "authors": "Chi-Wei Hsu; Chiao-Ting Chen; Szu-Hao Huang",
    "corresponding_authors": "",
    "abstract": "Graph-based cross-domain recommendations (CDRs) are useful for suggesting appropriate items because of their promising ability to extract features from user–item interactions and transfer knowledge across domains. Thus, the model can effectively alleviate cold start and data sparsity issues. Although the graph-based CDRs can capture valuable information, they still have some limitations. First, embeddings are highly vulnerable to noisy interactions, because the message aggregation in the graph convolutional network can further enlarge the impact. Second, because of the property of graph-structured data, the influence of high-degree nodes on representation learning is more than that of the long-tail items, and this can cause a poor recommendation performance. In this study, we devised a novel A daptive A dversarial C ontrastive L earning framework for graph-based C ross- D omain R ecommendation ( ACLCDR ). The ACLCDR introduces reinforcement learning to generate adaptive augmented samples for contrastive learning tasks. Then, we leveraged a multitask training strategy to jointly optimize the model with auxiliary tasks. Finally, we verified the effectiveness of the ACLCDR through nine real-world cross-domain tasks adopted from Amazon and Douban. We observed that ACLCDR exceeded the best state-of-the-art baseline by 25%, 42.5%, 16.3%, and 23.8% in terms of HR@ 10 and NDCG@10 for the Music &amp; Movie task from Amazon.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4388210400",
    "type": "article"
  },
  {
    "title": "LSAB: User Behavioral Pattern Modeling in Sequential Recommendation by Learning Self-Attention Bias",
    "doi": "https://doi.org/10.1145/3632625",
    "publication_date": "2023-11-16",
    "publication_year": 2023,
    "authors": "Di Han; Yifan Huang; Junmin Liu; Kai Liao; Kunling Lin",
    "corresponding_authors": "",
    "abstract": "Since the weight of a self-attention model is not affected by the sequence interval, it can more accurately and completely describe the user interests, so it is widely used in processing sequential recommendation. However, the mainstream self-attention model focuses on the similarity between items when calculating the attention weight of user behavioral patterns but fails to reflect the impact of user sudden drift decisions on the model in time. In this article, we introduce a bias strategy in the self-attention module, referred to as Learning Self-Attention Bias (LSAB) to more accurately learn the fast-changing user behavioral patterns. The introduction of LSAB allows for the adjustment of bias resulting from self-attention weights, leading to enhanced prediction performance in sequential recommendation. In addition, this article designs four attention-weight bias types catering to diverse user behavior preferences. After testing on the benchmark datasets, each bias strategy in LSAB is useful for state-of-the-art and can improve the performance of the models by nearly 5% on average. The source code listing is publicly available at https://gitee.com/kyle-liao/lsab .",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4388734012",
    "type": "article"
  },
  {
    "title": "Extrapolation errors in linear model trees",
    "doi": "https://doi.org/10.1145/1267066.1267067",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Wei‐Yin Loh; Chien‐Wei Chen; Wei Zheng",
    "corresponding_authors": "",
    "abstract": "Prediction errors from a linear model tend to be larger when extrapolation is involved, particularly when the model is wrong. This article considers the problem of extrapolation and interpolation errors when a linear model tree is used for prediction. It proposes several ways to curtail the size of the errors, and uses a large collection of real datasets to demonstrate that the solutions are effective in reducing the average mean squared prediction error. The article also provides a proof that, if a linear model is correct, the proposed solutions have no undesirable effects as the training sample size tends to infinity.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1997878053",
    "type": "article"
  },
  {
    "title": "Semantic annotation of frequent patterns",
    "doi": "https://doi.org/10.1145/1297332.1297335",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Qiaozhu Mei; Dong Xin; Hong Cheng; Jiawei Han; ChengXiang Zhai",
    "corresponding_authors": "",
    "abstract": "Using frequent patterns to analyze data has been one of the fundamental approaches in many data mining applications. Research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns, but little attention has been paid to the important next step—interpreting the discovered frequent patterns. Although the compression and summarization of frequent patterns has been studied in some recent work, the proposed techniques there can only annotate a frequent pattern with nonsemantical information (e.g., support), which provides only limited help for a user to understand the patterns. In this article, we study the novel problem of generating semantic annotations for frequent patterns. The goal is to discover the hidden meanings of a frequent pattern by annotating it with in-depth, concise, and structured information. We propose a general approach to generate such an annotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach can well incorporate the user's prior knowledge, and has potentially many applications, such as generating a dictionary-like description for a pattern, finding synonym patterns, discovering semantic relations, and summarizing semantic classes of a set of frequent patterns. Experiments on different datasets show that our approach is effective in generating semantic pattern annotations.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W1999115460",
    "type": "article"
  },
  {
    "title": "RIC",
    "doi": "https://doi.org/10.1145/1297332.1297334",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Christian Böhm; Christos Faloutsos; Jia-Yu Pan; Claudia Plant",
    "corresponding_authors": "",
    "abstract": "How do we find a natural clustering of a real-world point set which contains an unknown number of clusters with different shapes, and which may be contaminated by noise? As most clustering algorithms were designed with certain assumptions (Gaussianity), they often require the user to give input parameters, and are sensitive to noise. In this article, we propose a robust framework for determining a natural clustering of a given dataset, based on the minimum description length (MDL) principle. The proposed framework, robust information-theoretic clustering (RIC) , is orthogonal to any known clustering algorithm: Given a preliminary clustering, RIC purifies these clusters from noise, and adjusts the clusterings such that it simultaneously determines the most natural amount and shape (subspace) of the clusters. Our RIC method can be combined with any clustering technique ranging from K-means and K-medoids to advanced methods such as spectral clustering. In fact, RIC is even able to purify and improve an initial coarse clustering, even if we start with very simple methods. In an extension, we propose a fully automatic stand-alone clustering method and efficiency improvements. RIC scales well with the dataset size. Extensive experiments on synthetic and real-world datasets validate the proposed RIC framework.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2103452682",
    "type": "article"
  },
  {
    "title": "Expanding network communities from representative examples",
    "doi": "https://doi.org/10.1145/1514888.1514890",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Andrew Mehler; Steven Skiena",
    "corresponding_authors": "",
    "abstract": "We present an approach to leverage a small subset of a coherent community within a social network into a much larger, more representative sample. Our problem becomes identifying a small conductance subgraph containing many (but not necessarily all) members of the given seed set. Starting with an initial seed set representing a sample of a community, we seek to discover as much of the full community as possible. We present a general method for network community expansion, demonstrating that our methods work well in expanding communities in real world networks starting from small given seed groups (20 to 400 members). Our approach is marked by incremental expansion from the seeds with retrospective analysis to determine the ultimate boundaries of our community. We demonstrate how to increase the robustness of the general approach through bootstrapping multiple random partitions of the input set into seed and evaluation groups. We go beyond statistical comparisons against gold standards to careful subjective evaluations of our expanded communities. This process explains the causes of most disagreement between our expanded communities and our gold-standards—arguing that our expansion methods provide more reliable communities than can be extracted from reference sources/gazetteers such as Wikipedia.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2019968362",
    "type": "article"
  },
  {
    "title": "Learning multiple nonredundant clusterings",
    "doi": "https://doi.org/10.1145/1839490.1839496",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Ying Cui; Xiaoli Z. Fern; Jennifer Dy",
    "corresponding_authors": "",
    "abstract": "Real-world applications often involve complex data that can be interpreted in many different ways. When clustering such data, there may exist multiple groupings that are reasonable and interesting from different perspectives. This is especially true for high-dimensional data, where different feature subspaces may reveal different structures of the data. However, traditional clustering is restricted to finding only one single clustering of the data. In this article, we propose a new clustering paradigm for exploratory data analysis: find all non-redundant clustering solutions of the data, where data points in the same cluster in one solution can belong to different clusters in other partitioning solutions. We present a framework to solve this problem and suggest two approaches within this framework: (1) orthogonal clustering, and (2) clustering in orthogonal subspaces. In essence, both approaches find alternative ways to partition the data by projecting it to a space that is orthogonal to the current solution. The first approach seeks orthogonality in the cluster space, while the second approach seeks orthogonality in the feature space. We study the relationship between the two approaches. We also combine our framework with techniques for automatically finding the number of clusters in the different solutions, and study stopping criteria for determining when all meaningful solutions are discovered. We test our framework on both synthetic and high-dimensional benchmark data sets, and the results show that indeed our approaches were able to discover varied clustering solutions that are interesting and meaningful.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2026236384",
    "type": "article"
  },
  {
    "title": "Compositional mining of multirelational biological datasets",
    "doi": "https://doi.org/10.1145/1342320.1342322",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Ying Jin; T. M. Murali; Naren Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "High-throughput biological screens are yielding ever-growing streams of information about multiple aspects of cellular activity. As more and more categories of datasets come online, there is a corresponding multitude of ways in which inferences can be chained across them, motivating the need for compositional data mining algorithms. In this article, we argue that such compositional data mining can be effectively realized by functionally cascading redescription mining and biclustering algorithms as primitives. Both these primitives mirror shifts of vocabulary that can be composed in arbitrary ways to create rich chains of inferences. Given a relational database and its schema, we show how the schema can be automatically compiled into a compositional data mining program, and how different domains in the schema can be related through logical sequences of biclustering and redescription invocations. This feature allows us to rapidly prototype new data mining applications, yielding greater understanding of scientific datasets. We describe two applications of compositional data mining: (i) matching terms across categories of the Gene Ontology and (ii) understanding the molecular mechanisms underlying stress response in human cells.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2074329842",
    "type": "article"
  },
  {
    "title": "Analyzing knowledge communities using foreground and background clusters",
    "doi": "https://doi.org/10.1145/1754428.1754430",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "Vasileios Kandylas; S. Phineas Upham; Lyle Ungar",
    "corresponding_authors": "",
    "abstract": "Insight into the growth (or shrinkage) of “knowledge communities” of authors that build on each other's work can be gained by studying the evolution over time of clusters of documents. We cluster documents based on the documents they cite in common using the Streemer clustering method, which finds cohesive foreground clusters (the knowledge communities) embedded in a diffuse background. We build predictive models with features based on the citation structure, the vocabulary of the papers, and the affiliations and prestige of the authors and use these models to study the drivers of community growth and the predictors of how widely a paper will be cited. We find that scientific knowledge communities tend to grow more rapidly if their publications build on diverse information and use narrow vocabulary and that papers that lie on the periphery of a community have the highest impact, while those not in any community have the lowest impact.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2092932618",
    "type": "article"
  },
  {
    "title": "CSNL",
    "doi": "https://doi.org/10.1145/1754428.1754429",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "Sunil Vadera",
    "corresponding_authors": "Sunil Vadera",
    "abstract": "This article presents a new decision tree learning algorithm called CSNL that induces C ost- S ensitive N on- L inear decision trees. The algorithm is based on the hypothesis that nonlinear decision nodes provide a better basis than axis-parallel decision nodes and utilizes discriminant analysis to construct nonlinear decision trees that take account of costs of misclassification. The performance of the algorithm is evaluated by applying it to seventeen datasets and the results are compared with those obtained by two well known cost-sensitive algorithms, ICET and MetaCost, which generate multiple trees to obtain some of the best results to date. The results show that CSNL performs at least as well, if not better than these algorithms, in more than twelve of the datasets and is considerably faster. The use of bagging with CSNL further enhances its performance showing the significant benefits of using nonlinear decision nodes.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1984135268",
    "type": "article"
  },
  {
    "title": "Instance Annotation for Multi-Instance Multi-Label Learning",
    "doi": "https://doi.org/10.1145/2500491",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Forrest Briggs; Xiaoli Z. Fern; Raviv Raich; Qi Lou",
    "corresponding_authors": "",
    "abstract": "Multi-instance multi-label learning (MIML) is a framework for supervised classification where the objects to be classified are bags of instances associated with multiple labels. For example, an image can be represented as a bag of segments and associated with a list of objects it contains. Prior work on MIML has focused on predicting label sets for previously unseen bags. We instead consider the problem of predicting instance labels while learning from data labeled only at the bag level. We propose a regularized rank-loss objective designed for instance annotation, which can be instantiated with different aggregation models connecting instance-level labels with bag-level label sets. The aggregation models that we consider can be factored as a linear function of a “support instance” for each class, which is a single feature vector representing a whole bag. Hence we name our proposed methods rank-loss Support Instance Machines (SIM). We propose two optimization methods for the rank-loss objective, which is nonconvex. One is a heuristic method that alternates between updating support instances, and solving a convex problem in which the support instances are treated as constant. The other is to apply the constrained concave-convex procedure (CCCP), which can also be interpreted as iteratively updating support instances and solving a convex problem. To solve the convex problem, we employ the Pegasos framework of primal subgradient descent, and prove that it finds an ϵ-suboptimal solution in runtime that is linear in the number of bags, instances, and 1/ϵ. Additionally, we suggest a method of extending the linear learning algorithm to nonlinear classification, without increasing the runtime asymptotically. Experiments on artificial and real-world datasets including images and audio show that the proposed methods achieve higher accuracy than other loss functions used in prior work, e.g., Hamming loss, and recent work in ambiguous label classification.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2017818587",
    "type": "article"
  },
  {
    "title": "Modeling Temporal Activity to Detect Anomalous Behavior in Social Media",
    "doi": "https://doi.org/10.1145/3064884",
    "publication_date": "2017-07-14",
    "publication_year": 2017,
    "authors": "Alceu Ferraz Costa; Yuto Yamaguchi; Agma J. M. Traina; Caetano Traina; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "Social media has become a popular and important tool for human communication. However, due to this popularity, spam and the distribution of malicious content by computer-controlled users, known as bots, has become a widespread problem. At the same time, when users use social media, they generate valuable data that can be used to understand the patterns of human communication. In this article, we focus on the following important question: Can we identify and use patterns of human communication to decide whether a human or a bot controls a user? The first contribution of this article is showing that the distribution of inter-arrival times (IATs) between postings is characterized by following four patterns: (i) heavy-tails, (ii) periodic-spikes, (iii) correlation between consecutive values, and (iv) bimodallity. As our second contribution, we propose a mathematical model named Act-M (Activity Model). We show that Act-M can accurately fit the distribution of IATs from social media users. Finally, we use Act-M to develop a method that detects if users are bots based only on the timing of their postings. We validate Act-M using data from over 55 million postings from four social media services: Reddit, Twitter, Stack-Overflow, and Hacker-News. Our experiments show that Act-M provides a more accurate fit to the data than existing models for human dynamics. Additionally, when detecting bots, Act-M provided a precision higher than 93% and 77% with a sensitivity of 70% for the Twitter and Reddit datasets, respectively.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2734923642",
    "type": "article"
  },
  {
    "title": "Mining Overlapping Communities and Inner Role Assignments through Bayesian Mixed-Membership Models of Networks with Context-Dependent Interactions",
    "doi": "https://doi.org/10.1145/3106368",
    "publication_date": "2018-01-10",
    "publication_year": 2018,
    "authors": "Gianni Costa; Riccardo Ortale",
    "corresponding_authors": "",
    "abstract": "Community discovery and role assignment have been recently integrated into an unsupervised approach for the exploratory analysis of overlapping communities and inner roles in networks. However, the formation of ties in these prototypical research efforts is not truly realistic, since it does not account for a fundamental aspect of link establishment in real-world networks, i.e., the explicative reasons that cause interactions among nodes. Such reasons can be interpreted as generic requirements of nodes, that are met by other nodes and essentially pertain both to the nodes themselves and to their interaction contexts (i.e., the respective communities and roles). In this article, we present two new model-based machine-learning approaches, wherein community discovery and role assignment are seamlessly integrated and simultaneously performed through approximate posterior inference in Bayesian mixed-membership models of directed networks. The devised models account for the explicative reasons governing link establishment in terms of node-specific and contextual latent interaction factors. The former are inherently characteristic of nodes, while the latter are characterizations of nodes in the context of the individual communities and roles. The generative process of both models assigns nodes to communities with respective roles and connects them through directed links, which are probabilistically governed by their node-specific and contextual interaction factors. The difference between the proposed models lies in the exploitation of the contextual interaction factors. More precisely, in one model, the contextual interaction factors have the same impact on link generation. In the other model, the contextual interaction factors are weighted by the extent of involvement of the linked nodes in the respective communities and roles. We develop MCMC algorithms implementing approximate posterior inference and parameter estimation within our models. Finally, we conduct an intensive comparative experimentation, which demonstrates their superiority in community compactness and link prediction on various real-world and synthetic networks.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2783216063",
    "type": "article"
  },
  {
    "title": "Online Active Learning with Expert Advice",
    "doi": "https://doi.org/10.1145/3201604",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Shuji Hao; Peiying Hu; Peilin Zhao; Steven C. H. Hoi; Chunyan Miao",
    "corresponding_authors": "",
    "abstract": "In literature, learning with expert advice methods usually assume that a learner always obtain the true label of every incoming training instance at the end of each trial. However, in many real-world applications, acquiring the true labels of all instances can be both costly and time consuming, especially for large-scale problems. For example, in the social media, data stream usually comes in a high speed and volume, and it is nearly impossible and highly costly to label all of the instances. In this article, we address this problem with active learning with expert advice, where the ground truth of an instance is disclosed only when it is requested by the proposed active query strategies. Our goal is to minimize the number of requests while training an online learning model without sacrificing the performance. To address this challenge, we propose a framework of active forecasters, which attempts to extend two fully supervised forecasters, Exponentially Weighted Average Forecaster and Greedy Forecaster, to tackle the task of online active learning (OAL) with expert advice. Specifically, we proposed two OAL with expert advice algorithms, named Active Exponentially Weighted Average Forecaster (AEWAF) and active greedy forecaster (AGF), by considering the difference of expert advices. To further improve the robustness of the proposed AEWAF and AGF algorithms in the noisy scenarios (where noisy experts exist), we also proposed two robust active learning with expert advice algorithms, named Robust Active Exponentially Weighted Average Forecaster and Robust Active Greedy Forecaster. We validate the efficacy of the proposed algorithms by an extensive set of experiments in both normal scenarios (where all of experts are comparably reliable) and noisy scenarios.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2810299612",
    "type": "article"
  },
  {
    "title": "Coordination Event Detection and Initiator Identification in Time Series Data",
    "doi": "https://doi.org/10.1145/3201406",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Chainarong Amornbunchornvej; Ivan Brugere; Ariana Strandburg‐Peshkin; Damien R. Farine; Margaret C. Crofoot; Tanya Berger‐Wolf",
    "corresponding_authors": "",
    "abstract": "Behavior initiation is a form of leadership and is an important aspect of social organization that affects the processes of group formation, dynamics, and decision-making in human societies and other social animal species. In this work, we formalize the \"Coordination Initiator Inference Problem\" and propose a simple yet powerful framework for extracting periods of coordinated activity and determining individuals who initiated this coordination, based solely on the activity of individuals within a group during those periods. The proposed approach, given arbitrary individual time series, automatically (1) identifies times of coordinated group activity, (2) determines the identities of initiators of those activities, and (3) classifies the likely mechanism by which the group coordination occurred, all of which are novel computational tasks. We demonstrate our framework on both simulated and real-world data: trajectories tracking of animals as well as stock market data. Our method is competitive with existing global leadership inference methods but provides the first approaches for local leadership and coordination mechanism classification. Our results are consistent with ground-truthed biological data and the framework finds many known events in financial data which are not otherwise reflected in the aggregate NASDAQ index. Our method is easily generalizable to any coordinated time-series data from interacting entities.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2811104999",
    "type": "article"
  },
  {
    "title": "Solving inverse frequent itemset mining with infrequency constraints via large-scale linear programs",
    "doi": "https://doi.org/10.1145/2541268.2541271",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Antonella Guzzo; Luigi Moccia; Domenico Saccà; Edoardo Serra",
    "corresponding_authors": "",
    "abstract": "Inverse frequent set mining (IFM) is the problem of computing a transaction database D satisfying given support constraints for some itemsets, which are typically the frequent ones. This article proposes a new formulation of IFM, called IFM I (IFM with infrequency constraints ), where the itemsets that are not listed as frequent are constrained to be infrequent; that is, they must have a support less than or equal to a specified unique threshold. An instance of IFM I can be seen as an instance of the original IFM by making explicit the infrequency constraints for the minimal infrequent itemsets, corresponding to the so-called negative generator border defined in the literature. The complexity increase from PSPACE (complexity of IFM) to NEXP (complexity of IFM I ) is caused by the cardinality of the negative generator border, which can be exponential in the original input size. Therefore, the article introduces a specific problem parameter κ that computes an upper bound to this cardinality using a hypergraph interpretation for which minimal infrequent itemsets correspond to minimal transversals. By fixing a constant k , the article formulates a k -bounded definition of the problem, called k -IFM I , that collects all instances for which the value of the parameter κ is less than or equal to k —its complexity is in PSPACE as for IFM. The bounded problem is encoded as an integer linear program with a large number of variables (actually exponential w.r.t. the number of constraints), which is thereafter approximated by relaxing integer constraints—the decision problem of solving the linear program is proven to be in NP. In order to solve the linear program, a column generation technique is used that is a variation of the simplex method designed to solve large-scale linear programs, in particular with a huge number of variables. The method at each step requires the solution of an auxiliary integer linear program, which is proven to be NP hard in this case and for which a greedy heuristic is presented. The resulting overall column generation solution algorithm enjoys very good scaling as evidenced by the intensive experimentation, thereby paving the way for its application in real-life scenarios.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2032579240",
    "type": "article"
  },
  {
    "title": "Discovering Information Propagation Patterns in Microblogging Services",
    "doi": "https://doi.org/10.1145/2742801",
    "publication_date": "2015-07-22",
    "publication_year": 2015,
    "authors": "Zhiwen Yu; Zhu Wang; Huilei He; Jilei Tian; Xinjiang Lu; Bin Guo",
    "corresponding_authors": "",
    "abstract": "During the last decade, microblog has become an important social networking service with billions of users all over the world, acting as a novel and efficient platform for the creation and dissemination of real-time information. Modeling and revealing the information propagation patterns in microblogging services cannot only lead to more accurate understanding of user behaviors and provide insights into the underlying sociology, but also enable useful applications such as trending prediction, recommendation and filtering, spam detection and viral marketing. In this article, we aim to reveal the information propagation patterns in Sina Weibo, the biggest microblogging service in China. First, the cascade of each message is represented as a tree based on its retweeting process. Afterwards, we divide the information propagation pattern into two levels, that is, the macro level and the micro level. On one hand, the macro propagation patterns refer to general propagation modes that are extracted by grouping propagation trees based on hierarchical clustering. On the other hand, the micro propagation patterns are frequent information flow patterns that are discovered using tree-based mining techniques. Experimental results show that several interesting patterns are extracted, such as popular message propagation, artificial propagation, and typical information flows between different types of users.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2221659209",
    "type": "article"
  },
  {
    "title": "Mathematical Modeling and Analysis of Product Rating with Partial Information",
    "doi": "https://doi.org/10.1145/2700386",
    "publication_date": "2015-06-01",
    "publication_year": 2015,
    "authors": "Hong Xie; John C. S. Lui",
    "corresponding_authors": "",
    "abstract": "Many Web services like Amazon, Epinions, and TripAdvisor provide historical product ratings so that users can evaluate the quality of products. Product ratings are important because they affect how well a product will be adopted by the market. The challenge is that we only have partial information on these ratings: each user assigns ratings to only a small subset of products. Under this partial information setting, we explore a number of fundamental questions. What is the minimum number of ratings a product needs so that one can make a reliable evaluation of its quality? How may users’ misbehavior, such as cheating in product rating, affect the evaluation result? To answer these questions, we present a probabilistic model to capture various important factors (e.g., rating aggregation rules, rating behavior) that may influence the product quality assessment under the partial information setting. We derive the minimum number of ratings needed to produce a reliable indicator on the quality of a product. We extend our model to accommodate users’ misbehavior in product rating. We derive the maximum fraction of misbehaving users that a rating aggregation rule can tolerate and the minimum number of ratings needed to compensate. We carry out experiments using both synthetic and real-world data (from Amazon and TripAdvisor). We not only validate our model but also show that the “average rating rule” produces more reliable and robust product quality assessments than the “majority rating rule” and the “median rating rule” in aggregating product ratings. Last, we perform experiments on two movie rating datasets (from Flixster and Netflix) to demonstrate how to apply our framework to improve the applications of recommender systems.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2271406411",
    "type": "article"
  },
  {
    "title": "Discovering Mobile Application Usage Patterns from a Large-Scale Dataset",
    "doi": "https://doi.org/10.1145/3209669",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Fabrício A. Silva; Augusto C. S. A. Domingues; Thais R. M. Braga Silva",
    "corresponding_authors": "",
    "abstract": "The discovering of patterns regarding how, when, and where users interact with mobile applications reveals important insights for mobile service providers. In this work, we exploit for the first time a real and large-scale dataset representing the records of mobile application usage of 5,342 users during 2014. The data was collected by a software agent, installed at the users’ smartphones, which monitors detailed usage of applications. First, we look for general patterns of how users access some of the most popular mobile applications in terms of frequency, duration, diversity, and data traffic. Next, we mine the dataset looking for temporal patterns in terms of when and how often accesses occur. Finally, we exploit the location of each access to detect users’ points of interest and location-based communities. Based on the results, we derive a model to generate synthetic datasets of mobile application usage and evaluate solutions to predict the next application to be launched. We also discuss a series of implications of the findings regarding telecommunication services, mobile advertisements, and smart cities. This is the first time this dataset is used, and we also make it publicly available for other researchers.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2810950405",
    "type": "article"
  },
  {
    "title": "Assessing Human Error Against a Benchmark of Perfection",
    "doi": "https://doi.org/10.1145/3046947",
    "publication_date": "2017-07-27",
    "publication_year": 2017,
    "authors": "Ashton Anderson; Jon Kleinberg; Sendhil Mullainathan",
    "corresponding_authors": "",
    "abstract": "An increasing number of domains are providing us with detailed trace data on human decisions in settings where we can evaluate the quality of these decisions via an algorithm. Motivated by this development, an emerging line of work has begun to consider whether we can characterize and predict the kinds of decisions where people are likely to make errors. To investigate what a general framework for human error prediction might look like, we focus on a model system with a rich history in the behavioral sciences: the decisions made by chess players as they select moves in a game. We carry out our analysis at a large scale, employing datasets with several million recorded games, and using chess tablebases to acquire a form of ground truth for a subset of chess positions that have been completely solved by computers but remain challenging for even the best players in the world. We organize our analysis around three categories of features that we argue are present in most settings where the analysis of human error is applicable: the skill of the decision-maker, the time available to make the decision, and the inherent difficulty of the decision. We identify rich structure in all three of these categories of features, and find strong evidence that in our domain, features describing the inherent difficulty of an instance are significantly more powerful than features based on skill or time.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3004390862",
    "type": "article"
  },
  {
    "title": "Permanence and Community Structure in Complex Networks",
    "doi": "https://doi.org/10.1145/2953883",
    "publication_date": "2016-11-15",
    "publication_year": 2016,
    "authors": "Tanmoy Chakraborty; Sriram Srinivasan; Niloy Ganguly; Animesh Mukherjee; Sanjukta Bhowmick",
    "corresponding_authors": "",
    "abstract": "The goal of community detection algorithms is to identify densely connected units within large networks. An implicit assumption is that all the constituent nodes belong equally to their associated community. However, some nodes are more important in the community than others. To date, efforts have been primarily made to identify communities as a whole, rather than understanding to what extent an individual node belongs to its community. Therefore, most metrics for evaluating communities, for example modularity, are global. These metrics produce a score for each community, not for each individual node. In this article, we argue that the belongingness of nodes in a community is not uniform. We quantify the degree of belongingness of a vertex within a community by a new vertex-based metric called permanence . The central idea of permanence is based on the observation that the strength of membership of a vertex to a community depends upon two factors (i) the extent of connections of the vertex within its community versus outside its community, and (ii) how tightly the vertex is connected internally. We present the formulation of permanence based on these two quantities. We demonstrate that compared to other existing metrics (such as modularity, conductance, and cut-ratio), the change in permanence is more commensurate to the level of perturbation in ground-truth communities. We discuss how permanence can help us understand and utilize the structure and evolution of communities by demonstrating that it can be used to -- (i) measure the persistence of a vertex in a community, (ii) design strategies to strengthen the community structure, (iii) explore the core-periphery structure within a community, and (iv) select suitable initiators for message spreading. We further show that permanence is an excellent metric for identifying communities. We demonstrate that the process of maximizing permanence (abbreviated as MaxPerm ) produces meaningful communities that concur with the ground-truth community structure of the networks more accurately than eight other popular community detection algorithms. Finally, we provide mathematical proofs to demonstrate the correctness of finding communities by maximizing permanence. In particular, we show that the communities obtained by this method are (i) less affected by the changes in vertex ordering, and (ii) more resilient to resolution limit, degeneracy of solutions, and asymptotic growth of values.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2417105593",
    "type": "article"
  },
  {
    "title": "Comparison of Ontology Alignment Systems Across Single Matching Task Via the McNemar’s Test",
    "doi": "https://doi.org/10.1145/3193573",
    "publication_date": "2018-06-08",
    "publication_year": 2018,
    "authors": "Majid Mohammadi; Amir Ahooye Atashin; Wout Hofman; Yao‐Hua Tan",
    "corresponding_authors": "",
    "abstract": "Ontology alignment is widely used to find the correspondences between different ontologies in diverse fields. After discovering the alignments, several performance scores are available to evaluate them. The scores typically require the identified alignment and a reference containing the underlying actual correspondences of the given ontologies. The current trend in the alignment evaluation is to put forward a new score (e.g., precision, weighted precision, semantic precision, etc.) and to compare various alignments by juxtaposing the obtained scores. However, it is substantially provocative to select one measure among others for comparison. On top of that, claiming if one system has a better performance than one another cannot be substantiated solely by comparing two scalars. In this article, we propose the statistical procedures that enable us to theoretically favor one system over one another. The McNemar’s test is the statistical means by which the comparison of two ontology alignment systems over one matching task is drawn. The test applies to a 2 × 2 contingency table, which can be constructed in two different ways based on the alignments, each of which has their own merits/pitfalls. The ways of the contingency table construction and various apposite statistics from the McNemar’s test are elaborated in minute detail. In the case of having more than two alignment systems for comparison, the family wise error rate is expected to happen. Thus, the ways of preventing such an error are also discussed. A directed graph visualizes the outcome of the McNemar’s test in the presence of multiple alignment systems. From this graph, it is readily understood if one system is better than one another or if their differences are imperceptible. The proposed statistical methodologies are applied to the systems participated in the OAEI 2016 anatomy track, and also compares several well-known similarity metrics for the same matching problem.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2604736196",
    "type": "article"
  },
  {
    "title": "ATR-Vis",
    "doi": "https://doi.org/10.1145/3047010",
    "publication_date": "2018-02-06",
    "publication_year": 2018,
    "authors": "Raheleh Makki; Eder José de Carvalho; Axel J. Soto; Stephen Brooks; Maria Cristina Ferreira de Oliveira; Evangelos Milios; Rosane Minghim",
    "corresponding_authors": "",
    "abstract": "The worldwide adoption of Twitter turned it into one of the most popular platforms for content analysis as it serves as a gauge of the public’s feeling and opinion on a variety of topics. This is particularly true of political discussions and lawmakers’ actions and initiatives. Yet, one common but unrealistic assumption is that the data of interest for analysis is readily available in a comprehensive and accurate form. Data need to be retrieved, but due to the brevity and noisy nature of Twitter content, it is difficult to formulate user queries that match relevant posts that use different terminology without introducing a considerable volume of unwanted content. This problem is aggravated when the analysis must contemplate multiple and related topics of interest, for which comments are being concurrently posted. This article presents Active Tweet Retrieval Visualization (ATR-Vis), a user-driven visual approach for the retrieval of Twitter content applicable to this scenario. The method proposes a set of active retrieval strategies to involve an analyst in such a way that a major improvement in retrieval coverage and precision is attained with minimal user effort. ATR-Vis enables non-technical users to benefit from the aforementioned active learning strategies by providing visual aids to facilitate the requested supervision. This supports the exploration of the space of potentially relevant tweets, and affords a better understanding of the retrieval results. We evaluate our approach in scenarios in which the task is to retrieve tweets related to multiple parliamentary debates within a specific time span. We collected two Twitter datasets, one associated with debates in the Canadian House of Commons during a particular week in May 2014, and another associated with debates in the Brazilian Federal Senate during a selected week in May 2015. The two use cases illustrate the effectiveness of ATR-Vis for the retrieval of relevant tweets, while quantitative results show that our approach achieves high retrieval quality with a modest amount of supervision. Finally, we evaluated our tool with three external users who perform searching in social media as part of their professional work.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2786524829",
    "type": "article"
  },
  {
    "title": "Non-Redundant Subspace Clusterings with Nr-Kmeans and Nr-DipMeans",
    "doi": "https://doi.org/10.1145/3385652",
    "publication_date": "2020-06-21",
    "publication_year": 2020,
    "authors": "Dominik Mautz; Wei Ye; Claudia Plant; Christian Böhm",
    "corresponding_authors": "",
    "abstract": "A huge object collection in high-dimensional space can often be clustered in more than one way, for instance, objects could be clustered by their shape or alternatively by their color. Each grouping represents a different view of the dataset. The new research field of non-redundant clustering addresses this class of problems. In this article, we follow the approach that different, non-redundant k -means-like clusterings may exist in different, arbitrarily oriented subspaces of the high-dimensional space. We assume that these subspaces (and optionally a further noise space without any cluster structure) are orthogonal to each other. This assumption enables a particularly rigorous mathematical treatment of the non-redundant clustering problem and thus a particularly efficient algorithm, which we call N r -K means (for non-redundant k -means). The superiority of our algorithm is demonstrated both theoretically, as well as in extensive experiments. Further, we propose an extension of N r -K means that harnesses Hartigan’s dip test to identify the number of clusters for each subspace automatically.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3036780303",
    "type": "article"
  },
  {
    "title": "Class Imbalance and Cost-Sensitive Decision Trees",
    "doi": "https://doi.org/10.1145/3415156",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Michael J. Siers; Md Zahidul Islam",
    "corresponding_authors": "",
    "abstract": "Class imbalance treatment methods and cost-sensitive classification algorithms are typically treated as two independent research areas. However, many of these techniques have properties in common. After providing a background to the two fields of research, this article identifies the fundamental mechanism which is common to both. Using this mechanism, a taxonomy is created which encompasses approaches to both class imbalance treatment and cost-sensitive classification. Through this survey, we aim to bridge the gap between the two fields such that lessons from one field may be applied to the other. Many data mining tasks are naturally both class imbalanced and cost-sensitive. This survey is useful for researchers and practitioners approaching these tasks as it provides a detailed overview of approaches in both fields. Many of the surveyed techniques are classifier independent. However, we chose to focus on techniques which were either decision tree-based or compatible with decision trees. This choice was based on the popularity and novelty of their application to both fields.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3111117709",
    "type": "article"
  },
  {
    "title": "Context-Aware Recommendation Using Role-Based Trust Network",
    "doi": "https://doi.org/10.1145/2751562",
    "publication_date": "2015-10-12",
    "publication_year": 2015,
    "authors": "Liang Hong; Lei Zou; Cheng Zeng; Luming Zhang; Jian Wang; Jilei Tian",
    "corresponding_authors": "",
    "abstract": "Recommender systems have been studied comprehensively in both academic and industrial fields over the past decade. As user interests can be affected by context at any time and any place in mobile scenarios, rich context information becomes more and more important for personalized context-aware recommendations. Although existing context-aware recommender systems can make context-aware recommendations to some extent, they suffer several inherent weaknesses: (1) Users’ context-aware interests are not modeled realistically, which reduces the recommendation quality; (2) Current context-aware recommender systems ignore trust relations among users. Trust relations are actually context-aware and associated with certain aspects (i.e., categories of items) in mobile scenarios. In this article, we define a term role to model common context-aware interests among a group of users. We propose an efficient role mining algorithm to mine roles from a “user-context-behavior” matrix, and a role-based trust model to calculate context-aware trust value between two users. During online recommendation, given a user u in a context c , an efficient weighted set similarity query (WSSQ) algorithm is designed to build u ’s role-based trust network in context c . Finally, we make recommendations to u based on u ’s role-based trust network by considering both context-aware roles and trust relations. Extensive experiments demonstrate that our recommendation approach outperforms the state-of-the-art methods in both effectiveness and efficiency.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1982646697",
    "type": "article"
  },
  {
    "title": "A Bayesian Perspective on Locality Sensitive Hashing with Extensions for Kernel Methods",
    "doi": "https://doi.org/10.1145/2778990",
    "publication_date": "2015-10-12",
    "publication_year": 2015,
    "authors": "Aniket Chakrabarti; Venu Satuluri; Atreya Srivathsan; Srinivasan Parthasarathy",
    "corresponding_authors": "",
    "abstract": "Given a collection of objects and an associated similarity measure, the all-pairs similarity search problem asks us to find all pairs of objects with similarity greater than a certain user-specified threshold. In order to reduce the number of candidates to search, locality-sensitive hashing (LSH) based indexing methods are very effective. However, most such methods only use LSH for the first phase of similarity search—that is, efficient indexing for candidate generation. In this article, we present BayesLSH , a principled Bayesian algorithm for the subsequent phase of similarity search—performing candidate pruning and similarity estimation using LSH. A simpler variant, BayesLSH-Lite , which calculates similarities exactly, is also presented. Our algorithms are able to quickly prune away a large majority of the false positive candidate pairs, leading to significant speedups over baseline approaches. For BayesLSH, we also provide probabilistic guarantees on the quality of the output, both in terms of accuracy and recall. Finally, the quality of BayesLSH’s output can be easily tuned and does not require any manual setting of the number of hashes to use for similarity estimation, unlike standard approaches. For two state-of-the-art candidate generation algorithms, AllPairs and LSH, BayesLSH enables significant speedups, typically in the range 2 × --20 × for a wide variety of datasets. We also extend the BayesLSH algorithm for kernel methods—in which the similarity between two data objects is defined by a kernel function. Since the embedding of data points in the transformed kernel space is unknown, algorithms such as AllPairs which rely on building inverted index structure for fast similarity search do not work with kernel functions. Exhaustive search across all possible pairs is also not an option since the dataset can be huge and computing the kernel values for each pair can be prohibitive. We propose K-BayesLSH an all-pairs similarity search problem for kernel functions. K-BayesLSH leverages a recently proposed idea— kernelized locality sensitive hashing (KLSH)—for hash bit computation and candidate generation, and uses the aforementioned BayesLSH idea for candidate pruning and similarity estimation. We ran a broad spectrum of experiments on a variety of datasets drawn from different domains and with distinct kernels and find a speedup of 2 × --7 × over vanilla KLSH.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2037972594",
    "type": "article"
  },
  {
    "title": "The Convergence Behavior of Naive Bayes on Large Sparse Datasets",
    "doi": "https://doi.org/10.1145/2948068",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Xiang Li; Charles X. Ling; Huaimin Wang",
    "corresponding_authors": "",
    "abstract": "Large and sparse datasets with a lot of missing values are common in the big data era, such as user behaviors over a large number of items. Classification in such datasets is an important topic for machine learning and data mining. Practically, naive Bayes is still a popular classification algorithm for large sparse datasets, as its time and space complexity scales linearly with the size of non-missing values. However, several important questions about the behavior of naive Bayes are yet to be answered. For example, how different mechanisms of data missing, data sparsity, and the number of attributes systematically affect the learning curves and convergence? In this paper, we address several common data missing mechanisms and propose novel data generation methods based on these mechanisms. We generate large and sparse data systematically, and study the entire AUC (Area Under ROC Curve) learning curve and convergence behavior of naive Bayes. We not only have several important experiment observations, but also provide detailed theoretic studies. Finally, we summarize our empirical and theoretic results as an intuitive decision flowchart and a useful guideline for classifying large sparse datasets in practice.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2504431993",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue ACM SIGKDD 2013",
    "doi": "https://doi.org/10.1145/2700993",
    "publication_date": "2015-02-17",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2915220645",
    "type": "article"
  },
  {
    "title": "CFOF",
    "doi": "https://doi.org/10.1145/3362158",
    "publication_date": "2020-02-03",
    "publication_year": 2020,
    "authors": "Fabrizio Angiulli",
    "corresponding_authors": "Fabrizio Angiulli",
    "abstract": "We present a novel notion of outlier, called the Concentration Free Outlier Factor, or CFOF. As a main contribution, we formalize the notion of concentration of outlier scores and theoretically prove that CFOF does not concentrate in the Euclidean space for any arbitrary large dimensionality. To the best of our knowledge, there are no other proposals of data analysis measures related to the Euclidean distance for which it has been provided theoretical evidence that they are immune to the concentration effect. We determine the closed form of the distribution of CFOF scores in arbitrarily large dimensionalities and show that the CFOF score of a point depends on its squared norm standard score and on the kurtosis of the data distribution, thus providing a clear and statistically founded characterization of this notion. Moreover, we leverage this closed form to provide evidence that the definition does not suffer of the hubness problem affecting other measures in high dimensions. We prove that the number of CFOF outliers coming from each cluster is proportional to cluster size and kurtosis, a property that we call semi-locality. We leverage theoretical findings to shed lights on properties of well-known outlier scores. Indeed, we determine that semi-locality characterizes existing reverse nearest neighbor-based outlier definitions, thus clarifying the exact nature of their observed local behavior. We also formally prove that classical distance-based and density-based outliers concentrate both for bounded and unbounded sample sizes and for fixed and variable values of the neighborhood parameter. We introduce the fast-CFOF algorithm for detecting outliers in large high-dimensional dataset. The algorithm has linear cost, supports multi-resolution analysis, and is embarrassingly parallel. Experiments highlight that the technique is able to efficiently process huge datasets and to deal even with large values of the neighborhood parameter, to avoid concentration, and to obtain excellent accuracy.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3004812739",
    "type": "article"
  },
  {
    "title": "Towards an Optimal Outdoor Advertising Placement",
    "doi": "https://doi.org/10.1145/3350488",
    "publication_date": "2020-07-06",
    "publication_year": 2020,
    "authors": "Ping Zhang; Zhifeng Bao; Yuchen Li; Guoliang Li; Yipeng Zhang; Zhiyong Peng",
    "corresponding_authors": "",
    "abstract": "In this article, we propose and study the problem of trajectory-driven influential billboard placement: given a set of billboards U (each with a location and a cost), a database of trajectories T , and a budget L , we find a set of billboards within the budget to influence the largest number of trajectories. One core challenge is to identify and reduce the overlap of the influence from different billboards to the same trajectories, while keeping the budget constraint into consideration. We show that this problem is NP-hard and present an enumeration based algorithm with (1-1/e) approximation ratio. However, the enumeration would be very costly when | U | is large. By exploiting the locality property of billboards’ influence, we propose a partition-based framework PartSel. PartSel partitions U into a set of small clusters, computes the locally influential billboards for each cluster, and merges them to generate the global solution. Since the local solutions can be obtained much more efficiently than the global one, PartSel would reduce the computation cost greatly; meanwhile it achieves a non-trivial approximation ratio guarantee. Then we propose a LazyProbe method to further prune billboards with low marginal influence, while achieving the same approximation ratio as PartSel. Next, we propose a branch-and-bound method to eliminate unnecessary enumerations in both PartSel and LazyProbe, as well as an aggregated index to speed up the computation of marginal influence. Experiments on real datasets verify the efficiency and effectiveness of our methods.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3058390340",
    "type": "article"
  },
  {
    "title": "Multi-View Collaborative Network Embedding",
    "doi": "https://doi.org/10.1145/3441450",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Sezin Kircali Ata; Yuan Fang; Min Wu; Jiaqi Shi; Chee Keong Kwoh; Xiaoli Li",
    "corresponding_authors": "",
    "abstract": "Real-world networks often exist with multiple views, where each view describes one type of interaction among a common set of nodes. For example, on a video-sharing network, while two user nodes are linked, if they have common favorite videos in one view, then they can also be linked in another view if they share common subscribers. Unlike traditional single-view networks, multiple views maintain different semantics to complement each other. In this article, we propose M ulti-view coll A borative N etwork E mbedding (MANE), a multi-view network embedding approach to learn low-dimensional representations. Similar to existing studies, MANE hinges on diversity and collaboration—while diversity enables views to maintain their individual semantics, collaboration enables views to work together. However, we also discover a novel form of second-order collaboration that has not been explored previously, and further unify it into our framework to attain superior node representations. Furthermore, as each view often has varying importance w.r.t. different nodes, we propose MANE , an attention -based extension of MANE, to model node-wise view importance. Finally, we conduct comprehensive experiments on three public, real-world multi-view networks, and the results demonstrate that our models consistently outperform state-of-the-art approaches.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3025739297",
    "type": "article"
  },
  {
    "title": "KRAN: Knowledge Refining Attention Network for Recommendation",
    "doi": "https://doi.org/10.1145/3470783",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Zhenyu Zhang; Lei Zhang; Dingqi Yang; Yang Liu",
    "corresponding_authors": "",
    "abstract": "Recommender algorithms combining knowledge graph and graph convolutional network are becoming more and more popular recently. Specifically, attributes describing the items to be recommended are often used as additional information. These attributes along with items are highly interconnected, intrinsically forming a Knowledge Graph (KG). These algorithms use KGs as an auxiliary data source to alleviate the negative impact of data sparsity. However, these graph convolutional network based algorithms do not distinguish the importance of different neighbors of entities in the KG, and according to Pareto’s principle, the important neighbors only account for a small proportion. These traditional algorithms can not fully mine the useful information in the KG. To fully release the power of KGs for building recommender systems, we propose in this article KRAN, a Knowledge Refining Attention Network, which can subtly capture the characteristics of the KG and thus boost recommendation performance. We first introduce a traditional attention mechanism into the KG processing, making the knowledge extraction more targeted, and then propose a refining mechanism to improve the traditional attention mechanism to extract the knowledge in the KG more effectively. More precisely, KRAN is designed to use our proposed knowledge-refining attention mechanism to aggregate and obtain the representations of the entities (both attributes and items) in the KG. Our knowledge-refining attention mechanism first measures the relevance between an entity and it’s neighbors in the KG by attention coefficients, and then further refines the attention coefficients using a “richer-get-richer” principle, in order to focus on highly relevant neighbors while eliminating less relevant neighbors for noise reduction. In addition, for the item cold start problem, we propose KRAN-CD, a variant of KRAN, which further incorporates pre-trained KG embeddings to handle cold start items. Experiments show that KRAN and KRAN-CD consistently outperform state-of-the-art baselines across different settings.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3197481512",
    "type": "article"
  },
  {
    "title": "Knowledge Distillation with Attention for Deep Transfer Learning of Convolutional Networks",
    "doi": "https://doi.org/10.1145/3473912",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Xingjian Li; Haoyi Xiong; Zeyu Chen; Jun Huan; Ji Liu; Chengzhong Xu; Dejing Dou",
    "corresponding_authors": "",
    "abstract": "Transfer learning through fine-tuning a pre-trained neural network with an extremely large dataset, such as ImageNet, can significantly improve and accelerate training while the accuracy is frequently bottlenecked by the limited dataset size of the new target task. To solve the problem, some regularization methods, constraining the outer layer weights of the target network using the starting point as references (SPAR), have been studied. In this article, we propose a novel regularized transfer learning framework \\operatorname{DELTA} , namely DE ep L earning T ransfer using Feature Map with A ttention . Instead of constraining the weights of neural network, \\operatorname{DELTA} aims at preserving the outer layer outputs of the source network. Specifically, in addition to minimizing the empirical loss, \\operatorname{DELTA} aligns the outer layer outputs of two networks, through constraining a subset of feature maps that are precisely selected by attention that has been learned in a supervised learning manner. We evaluate \\operatorname{DELTA} with the state-of-the-art algorithms, including L^2 and \\emph {L}^2\\text{-}SP . The experiment results show that our method outperforms these baselines with higher accuracy for new tasks. Code has been made publicly available. 1",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3208576249",
    "type": "article"
  },
  {
    "title": "Unsupervised Adversarial Network Alignment with Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3477050",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Yang Zhou; Jiaxiang Ren; Ruoming Jin; Zijie Zhang; Jingyi Zheng; Zhe Jiang; Da Yan; Dejing Dou",
    "corresponding_authors": "",
    "abstract": "Network alignment, which aims at learning a matching between the same entities across multiple information networks, often suffers challenges from feature inconsistency, high-dimensional features, to unstable alignment results. This article presents a novel network alignment framework, Unsupervised Adversarial learning based Network Alignment(UANA), that combines generative adversarial network (GAN) and reinforcement learning (RL) techniques to tackle the above critical challenges. First, we propose a bidirectional adversarial network distribution matching model to perform the bidirectional cross-network alignment translations between two networks, such that the distributions of real and translated networks completely overlap together. In addition, two cross-network alignment translation cycles are constructed for training the unsupervised alignment without the need of prior alignment knowledge. Second, in order to address the feature inconsistency issue, we integrate a dual adversarial autoencoder module with an adversarial binary classification model together to project two copies of the same vertices with high-dimensional inconsistent features into the same low-dimensional embedding space. This facilitates the translations of the distributions of two networks in the adversarial network distribution matching model. Finally, we develop an RL based optimization approach to solve the vertex matching problem in the discrete space of the GAN model, i.e., directly select the vertices in target networks most relevant to the vertices in source networks, without unstable similarity computation that is sensitive to discriminative features and similarity metrics. Extensive evaluation on real-world graph datasets demonstrates the outstanding capability of UANA to address the unsupervised network alignment problem, in terms of both effectiveness and scalability.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3210998975",
    "type": "article"
  },
  {
    "title": "<b>GrOD</b> : Deep Learning with Gradients Orthogonal Decomposition for Knowledge Transfer, Distillation, and Adversarial Training",
    "doi": "https://doi.org/10.1145/3530836",
    "publication_date": "2022-04-18",
    "publication_year": 2022,
    "authors": "Haoyi Xiong; Ruosi Wan; Jian Zhao; Zeyu Chen; Xingjian Li; Zhanxing Zhu; Jun Huan",
    "corresponding_authors": "",
    "abstract": "Regularization that incorporates the linear combination of empirical loss and explicit regularization terms as the loss function has been frequently used for many machine learning tasks. The explicit regularization term is designed in different types, depending on its applications. While regularized learning often boost the performance with higher accuracy and faster convergence, the regularization would sometimes hurt the empirical loss minimization and lead to poor performance. To deal with such issues in this work, we propose a novel strategy, namely Gr adients O rthogonal D ecomposition ( GrOD ), that improves the training procedure of regularized deep learning. Instead of linearly combining gradients of the two terms, GrOD re-estimates a new direction for iteration that does not hurt the empirical loss minimization while preserving the regularization affects, through orthogonal decomposition. We have performed extensive experiments to use GrOD improving the commonly used algorithms of transfer learning [ 2 ], knowledge distillation [ 3 ], and adversarial learning [ 4 ]. The experiment results based on large datasets, including Caltech 256 [ 5 ], MIT indoor 67 [ 6 ], CIFAR-10 [ 7 ], and ImageNet [ 8 ], show significant improvement made by GrOD for all three algorithms in all cases.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4223936353",
    "type": "article"
  },
  {
    "title": "Reinforcement Learning for Practical Express Systems with Mixed Deliveries and Pickups",
    "doi": "https://doi.org/10.1145/3546952",
    "publication_date": "2022-07-07",
    "publication_year": 2022,
    "authors": "Jinwei Chen; Zefang Zong; Yunlin Zhuang; Huan Yan; Depeng Jin; Yong Li",
    "corresponding_authors": "",
    "abstract": "In real-world express systems, couriers need to satisfy not only the delivery demands but also the pick-up demands of customers. Delivery and pickup tasks are usually mixed together within integrated routing plans. Such a mixed routing problem can be abstracted and formulated as Vehicle Routing Problem with Mixed Delivery and Pickup (VRPMDP), which is an NP-hard combinatorial optimization problem. To solve VRPMDP, there are three major challenges as below. (a) Even though successive pickup and delivery tasks are independent to accomplish, the inter-influence between choosing pickup task or delivery task to deal with still exists. (b) Due to the two-way flow of goods between the depot and customers, the loading rate of vehicles leaving the depot affects routing decisions. (c) The proportion of deliveries and pickups will change due to the complex demand situation in real-world scenarios, which requires robustness of the algorithm. To solve the challenges above, we design an encoder-decoder based framework to generate high-quality and robust VRPMDP solutions. First, we consider a VRPMDP instance as a graph and utilize a GNN encoder to extract the feature of the instance effectively. The detailed routing solutions are further decoded as a sequence by the decoder with attention mechanism. Second, we propose a Coordinated Decision of Loading and Routing (CDLR) mechanism to determine the loading rate dynamically after the vehicle returns to the depot, thus avoiding the influence of improper loading rate settings. Finally, the model equipped with a GNN encoder and CDLR simultaneously can adapt to the changes in the proportion of deliveries and pickups. We conduct the experiments to demonstrate the effectiveness of our model. The experiments show that our method achieves desirable results and generalization ability.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4284697663",
    "type": "article"
  },
  {
    "title": "Variational Graph Autoencoder with Adversarial Mutual Information Learning for Network Representation Learning",
    "doi": "https://doi.org/10.1145/3555809",
    "publication_date": "2022-08-22",
    "publication_year": 2022,
    "authors": "Dongjie Li; Dong Li; Guang Lian",
    "corresponding_authors": "",
    "abstract": "With the success of Graph Neural Network (GNN) in network data, some GNN-based representation learning methods for networks have emerged recently. Variational Graph Autoencoder (VGAE) is a basic GNN framework for network representation. Its purpose is to well preserve the topology and node attribute information of the network to learn node representation, but it only reconstructs network topology, and does not consider the reconstruction of node features. This strategy will make node representation can not well reserve node features information, impairing the ability of the VGAE method to learn higher quality representations. To solve this problem, we arise a new network representation method to improve the VGAE method for well retaining both node features and network structure information. The method utilizes adversarial mutual information learning to maximize the mutual information (MI) of node features and node representations during the encoding process of the variational autoencoder, which forces the variational encoder to get the representation containing the most informative node features. The method consists of three parts: a variational graph autoencoder includes a variational encoder (MI generator (G)) and a decoder, a positive MI sample module (maximizing MI module), and an MI discriminator (D). Furthermore, we explain why maximizing MI between node features and node representation can reconstruct node attributes. Finally, we conduct experiments on seven public representative datasets for nodes classification, nodes clustering, and graph visualization tasks. Experimental results demonstrate that the proposed algorithm significantly outperforms current popular network representation algorithms on these tasks. The best improvement is 17.13% than the VGAE method.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4292596021",
    "type": "article"
  },
  {
    "title": "Supervised Contrastive Learning for Interpretable Long-Form Document Matching",
    "doi": "https://doi.org/10.1145/3542822",
    "publication_date": "2022-06-11",
    "publication_year": 2022,
    "authors": "Akshita Jha; Vineeth Rakesh; Jaideep Chandrashekar; Adithya Samavedhi; Chandan K. Reddy",
    "corresponding_authors": "",
    "abstract": "Recent advancements in deep learning techniques have transformed the area of semantic text matching (STM). However, most state-of-the-art models are designed to operate with short documents such as tweets, user reviews, comments, and so on. These models have fundamental limitations when applied to long-form documents such as scientific papers, legal documents, and patents. When handling such long documents, there are three primary challenges: (i) the presence of different contexts for the same word throughout the document, (ii) small sections of contextually similar text between two documents, but dissimilar text in the remaining parts (this defies the basic understanding of “similarity”), and (iii) the coarse nature of a single global similarity measure which fails to capture the heterogeneity of the document content. In this article, we describe CoLDE : Co ntrastive L ong D ocument E ncoder—a transformer-based framework that addresses these challenges and allows for interpretable comparisons of long documents. CoLDE uses unique positional embeddings and a multi-headed chunkwise attention layer in conjunction with a supervised contrastive learning framework to capture similarity at three different levels: (i) high-level similarity scores between a pair of documents, (ii) similarity scores between different sections within and across documents, and (iii) similarity scores between different chunks in the same document and across other documents. These fine-grained similarity scores aid in better interpretability. We evaluate CoLDE on three long document datasets namely, ACL Anthology publications, Wikipedia articles, and USPTO patents. Besides outperforming the state-of-the-art methods on the document matching task, CoLDE is also robust to changes in document length and text perturbations and provides interpretable results. The code for the proposed model is publicly available at https://github.com/InterDigitalInc/CoLDE .",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4293083967",
    "type": "article"
  },
  {
    "title": "STHAN: Transportation Demand Forecasting with Compound Spatio-Temporal Relationships",
    "doi": "https://doi.org/10.1145/3565578",
    "publication_date": "2022-10-04",
    "publication_year": 2022,
    "authors": "Shuai Ling; Zhe Yu; Shaosheng Cao; Haipeng Zhang; Simon Hu",
    "corresponding_authors": "",
    "abstract": "Transportation demand forecasting is a critical precondition of optimal online transportation dispatch, which will greatly reduce drivers’ wasted mileage and customers’ waiting time, contributing to economic and environmental sustainability. Though various methods have been developed, the core spatio-temporal complexity remains challenging from three perspectives: (1) Compound spatial relationships. According to our empirical analysis, these relationships widely exist. Previous studies focus on capturing different spatial relationships using multi-homogeneous graphs. However, the information flow across various spatial relationships is not modeled explicitly. (2) Heterogeneity in spatial relationships. A region’s neighbors under the same spatial relationship may have different weights for this region. Meanwhile, different relationships may also weigh differently. (3) Synchronicity between compound spatial relationships and temporal relationships. Previous research considers synchronous influences from spatial and temporal relationships in a homogeneous fashion while compound spatial relationships are not captured for this synchronicity. To address the aforementioned perspectives, we propose the S patio- T emporal H eterogeneous graph A ttention N etwork (STHAN), where the key intuition is capturing the compound spatial relationships via meta-paths explicitly. We first construct a spatio-temporal heterogeneous graph including multiple spatial relationships and temporal relationships and use meta-paths to depict compound spatial relationships. To capture the heterogeneity, we use hierarchical attention, which contains node level attention and meta-path level attention. The synchronicity between temporal relationships and spatial relationships, including compound ones, is modeled in meta-path-level attention. Our framework outperforms state-of-the-art models by reducing 6.58%, 4.57%, and 4.20% of WMAPE in experiments on three real-world datasets, respectively.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4301395451",
    "type": "article"
  },
  {
    "title": "Multi-view Ensemble Clustering via Low-rank and Sparse Decomposition: From Matrix to Tensor",
    "doi": "https://doi.org/10.1145/3589768",
    "publication_date": "2023-03-30",
    "publication_year": 2023,
    "authors": "Xuanqi Zhang; Qiangqiang Shen; Yongyong Chen; Guokai Zhang; Zhongyun Hua; Jingyong Su",
    "corresponding_authors": "",
    "abstract": "As a significant extension of classical clustering methods, ensemble clustering first generates multiple basic clusterings and then fuses them into one consensus partition by solving a problem concerning graph partition with respect to the co-association matrix. Although the collaborative cluster structure among basic clusterings can be well discovered by ensemble clustering, most advanced ensemble clustering utilizes the self-representation strategy with the constraint of low-rank to explore a shared consensus representation matrix in multiple views. However, they still encounter two challenges: (1) high computational cost caused by both the matrix inversion operation and singular value decomposition of large-scale square matrices; (2) less considerable attention on high-order correlation attributed to the pursue of the two-dimensional pair-wise relationship matrix. In this article, based on low-rank and sparse decomposition from both matrix and tensor perspectives, we propose two novel multi-view ensemble clustering methods, which tangibly decrease computational complexity. Specifically, our first method utilizes low-rank and sparse matrix decomposition to learn one common co-association matrix, while our last method constructs all co-association matrices into one third-order tensor to investigate the high-order correlation among multiple views by low-rank and sparse tensor decomposition. We adopt the alternating direction method of multipliers to solve two convex models by dividing them into several subproblems with closed-form solution. Experimental results on ten real-world datasets prove the effectiveness and efficiency of the proposed two multi-view ensemble clustering methods by comparing them with other advanced ensemble clustering methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4361271902",
    "type": "article"
  },
  {
    "title": "Instrumental Variable-Driven Domain Generalization with Unobserved Confounders",
    "doi": "https://doi.org/10.1145/3595380",
    "publication_date": "2023-04-29",
    "publication_year": 2023,
    "authors": "Junkun Yuan; Xu Ma; Ruoxuan Xiong; Mingming Gong; Xiangyu Liu; Fei Wu; Lanfen Lin; Kun Kuang",
    "corresponding_authors": "",
    "abstract": "Domain generalization (DG) aims to learn from multiple source domains a model that can generalize well on unseen target domains. Existing DG methods mainly learn the representations with invariant marginal distribution of the input features, however, the invariance of the conditional distribution of the labels given the input features is more essential for unknown domain prediction. Meanwhile, the existing of unobserved confounders which affect the input features and labels simultaneously cause spurious correlation and hinder the learning of the invariant relationship contained in the conditional distribution. Interestingly, with a causal view on the data generating process, we find that the input features of one domain are valid instrumental variables for other domains. Inspired by this finding, we propose an instrumental variable-driven DG method (IV-DG) by removing the bias of the unobserved confounders with two-stage learning. In the first stage, it learns the conditional distribution of the input features of one domain given input features of another domain. In the second stage, it estimates the relationship by predicting labels with the learned conditional distribution. Theoretical analyses and simulation experiments show that it accurately captures the invariant relationship. Extensive experiments on real-world datasets demonstrate that IV-DG method yields state-of-the-art results.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4367368028",
    "type": "article"
  },
  {
    "title": "Sequential and Graphical Cross-Domain Recommendations with a Multi-View Hierarchical Transfer Gate",
    "doi": "https://doi.org/10.1145/3604615",
    "publication_date": "2023-06-19",
    "publication_year": 2023,
    "authors": "Huiyuan Li; Li Yu; Xi Niu; Youfang Leng; Qihan Du",
    "corresponding_authors": "",
    "abstract": "Cross-domain recommender systems could potentially improve the recommendation performance by means of transferring abundant knowledge from the auxiliary domain to the target domain. They could help address some key challenges in recommender systems, such as data sparsity and cold start. However, most existing cross-domain recommendation approaches represent the user preferences based on a single kind of user’s feature or behavior and fail to explore the hidden interaction effects of different kinds of features or behaviors. In this article, we propose the S equential and G raphical Cross -Domain Recommendations with a Multi-View Hierarchical Transfer Gate (SGCross) to transfer user representations from multiple perspectives. The SGCross model constructs a user profile by learning the personal preference from a personal view, the dynamic preference from a temporal view, as well as the collaborative preference from a collaborative view. Specifically, a Multi-view Hierarchical Gate (MHG) is designed to transfer the informative representations of user knowledge on different views from the auxiliary domain separately, aiming to enhance the user representations. Furthermore, a two-stage attentive fusion module is designed to integrate transferred information at two levels: the domain level and the view level. Extensive experiments on the Amazon dataset and the Douban dataset have demonstrated that SGCross effectively improves the accuracy of cross-domain recommendations and outperforms the state-of-the-art baseline models.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4381163914",
    "type": "article"
  },
  {
    "title": "Understanding Any Time Series Classifier with a Subsequence-based Explainer",
    "doi": "https://doi.org/10.1145/3624480",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Francesco Spinnato; Riccardo Guidotti; Anna Monreale; Mirco Nanni; Dino Pedreschi; Fosca Giannotti",
    "corresponding_authors": "",
    "abstract": "The growing availability of time series data has increased the usage of classifiers for this data type. Unfortunately, state-of-the-art time series classifiers are black-box models and, therefore, not usable in critical domains such as healthcare or finance, where explainability can be a crucial requirement. This paper presents a framework to explain the predictions of any black-box classifier for univariate and multivariate time series. The provided explanation is composed of three parts. First, a saliency map highlighting the most important parts of the time series for the classification. Second, an instance-based explanation exemplifies the black-box’s decision by providing a set of prototypical and counterfactual time series. Third, a factual and counterfactual rule-based explanation, revealing the reasons for the classification through logical conditions based on subsequences that must, or must not, be contained in the time series. Experiments and benchmarks show that the proposed method provides faithful, meaningful, stable, and interpretable explanations.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4386896779",
    "type": "article"
  },
  {
    "title": "Measuring and Mitigating Gender Bias in Legal Contextualized Language Models",
    "doi": "https://doi.org/10.1145/3628602",
    "publication_date": "2023-10-18",
    "publication_year": 2023,
    "authors": "Mustafa Bozdag; Nurullah Sevim; Aykut Koç",
    "corresponding_authors": "",
    "abstract": "Transformer-based contextualized language models constitute the state-of-the-art in several natural language processing (NLP) tasks and applications. Despite their utility, contextualized models can contain human-like social biases, as their training corpora generally consist of human-generated text. Evaluating and removing social biases in NLP models has been a major research endeavor. In parallel, NLP approaches in the legal domain, namely, legal NLP or computational law, have also been increasing. Eliminating unwanted bias in legal NLP is crucial, since the law has the utmost importance and effect on people. In this work, we focus on the gender bias encoded in BERT-based models. We propose a new template-based bias measurement method with a new bias evaluation corpus using crime words from the FBI database. This method quantifies the gender bias present in BERT-based models for legal applications. Furthermore, we propose a new fine-tuning-based debiasing method using the European Court of Human Rights (ECtHR) corpus to debias legal pre-trained models. We test the debiased models’ language understanding performance on the LexGLUE benchmark to confirm that the underlying semantic vector space is not perturbed during the debiasing process. Finally, we propose a bias penalty for the performance scores to emphasize the effect of gender bias on model performance.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4387735167",
    "type": "article"
  },
  {
    "title": "Traceable Group-Wise Self-Optimizing Feature Transformation Learning: A Dual Optimization Perspective",
    "doi": "https://doi.org/10.1145/3638059",
    "publication_date": "2023-12-20",
    "publication_year": 2023,
    "authors": "Meng Xiao; Dongjie Wang; Min Wu; Kunpeng Liu; Hui Xiong; Yuanchun Zhou; Yanjie Fu",
    "corresponding_authors": "",
    "abstract": "Feature transformation aims to reconstruct an effective representation space by mathematically refining the existing features. It serves as a pivotal approach to combat the curse of dimensionality, enhance model generalization, mitigate data sparsity, and extend the applicability of classical models. Existing research predominantly focuses on domain knowledge-based feature engineering or learning latent representations. However, these methods, while insightful, lack full automation and fail to yield a traceable and optimal representation space. An indispensable question arises: Can we concurrently address these limitations when reconstructing a feature space for a machine learning task? Our initial work took a pioneering step towards this challenge by introducing a novel self-optimizing framework. This framework leverages the power of three cascading reinforced agents to automatically select candidate features and operations for generating improved feature transformation combinations. Despite the impressive strides made, there was room for enhancing its effectiveness and generalization capability. In this extended journal version, we advance our initial work from two distinct yet interconnected perspectives: 1) We propose a refinement of the original framework, which integrates a graph-based state representation method to capture the feature interactions more effectively and develop different Q-learning strategies to alleviate Q-value overestimation further. 2) We utilize a new optimization technique (actor-critic) to train the entire self-optimizing framework in order to accelerate the model convergence and improve the feature transformation performance. Finally, to validate the improved effectiveness and generalization capability of our framework, we perform extensive experiments and conduct comprehensive analyses. These provide empirical evidence of the strides made in this journal version over the initial work, solidifying our framework’s standing as a substantial contribution to the field of automated feature transformation. To improve the reproducibility, we have released the associated code and data by the Github link https://github.com/coco11563/TKDD2023_code.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4390003102",
    "type": "article"
  },
  {
    "title": "X-FSPMiner: A Novel Algorithm for Frequent Similar Pattern Mining",
    "doi": "https://doi.org/10.1145/3643820",
    "publication_date": "2024-01-30",
    "publication_year": 2024,
    "authors": "Ansel Y. Rodríguez‐González; Ramón Aranda; Miguel Á. Álvarez‐Carmona; Ángel Díaz-Pacheco; Rosa María Valdovinos Rosas",
    "corresponding_authors": "",
    "abstract": "Frequent similar pattern mining (FSP mining) allows for finding frequent patterns hidden from the classical approach. However, the use of similarity functions implies more computational effort, necessitating the development of more efficient algorithms for FSP mining. This work aims to improve the efficiency of mining all FSPs when using Boolean and non-increasing monotonic similarity functions. A data structure to condense an object description collection, named FV-Tree , and an algorithm for mining all FSPs from the FV-Tree , named X-FSPMiner , are proposed. The experimental results reveal that the novel algorithm X-FSPMiner vastly outperforms the state-of-the-art algorithms for mining all FSPs using Boolean and non-increasing monotonic similarity functions.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391360724",
    "type": "article"
  },
  {
    "title": "Improving Graph Collaborative Filtering with Directional Behavior Enhanced Contrastive Learning",
    "doi": "https://doi.org/10.1145/3663574",
    "publication_date": "2024-05-02",
    "publication_year": 2024,
    "authors": "Penghang Yu; Bing‐Kun Bao; Zhiyi Tan; Guanming Lu",
    "corresponding_authors": "",
    "abstract": "Graph Collaborative Filtering is a widely adopted approach for recommendation, which captures similar behavior features through Graph Neural Network (GNN). Recently, Contrastive Learning (CL) has been demonstrated as an effective method to enhance the performance of graph collaborative filtering. Typically, CL-based methods first perturb users’ history behavior data (e.g., drop clicked items), then construct a self-discriminating task for behavior representations under different random perturbations. However, for widely existing inactive users, random perturbation makes their sparse behavior information more incomplete, thereby harming the behavior feature extraction. To tackle the above issue, we design a novel directional perturbation-based CL method to improve the graph collaborative filtering performance. The idea is to perturb node representations through directionally enhancing behavior features. To do so, we propose a simple yet effective feedback mechanism, which fuses the representations of nodes based on behavior similarity. Then, to avoid irrelevant behavior preferences introduced by the feedback mechanism, we construct a behavior self-contrast task before and after feedback, to align the node representations between the final output and the first layer of GNN. Different from the widely adopted self-discriminating task, the behavior self-contrast task avoids complex message propagation on different perturbed graphs, which is more efficient than previous methods. Extensive experiments on three public datasets demonstrate that the proposed method has distinct advantages over other CL methods on recommendation accuracy.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4396573711",
    "type": "article"
  },
  {
    "title": "Heterogeneous Network Motif Coding, Counting, and Profiling",
    "doi": "https://doi.org/10.1145/3687465",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Shuo Yu; Feng Xia; Honglong Chen; Ivan Lee; Lianhua Chi; Hanghang Tong",
    "corresponding_authors": "",
    "abstract": "Network motifs, as a fundamental higher-order structure in large-scale networks, have received significant attention over recent years. Particularly in heterogeneous networks, motifs offer a higher capacity to uncover diverse information compared to homogeneous networks. However, the structural complexity and heterogeneity pose challenges in coding, counting, and profiling heterogeneous motifs. This work addresses these challenges by first introducing a novel heterogeneous motif coding method, adaptable to homogeneous motifs as well. Building upon this coding framework, we then propose GIFT, a heterogeneous network motif counting algorithm. GIFT effectively leverages combined structures of heterogeneous motifs through three key procedures: neighborhood searching, motif combination, and redundant motif filtering. We apply GIFT to count three-order and four-order motifs across eight distinct heterogeneous networks. Subsequently, we profile these detected motifs using four classical motif-based indicators. Experimental results demonstrate that by appropriately selecting motifs tailored to specific networks, heterogeneous motifs emerge as significant features in characterizing the underlying network structure.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401944165",
    "type": "article"
  },
  {
    "title": "BISC",
    "doi": "https://doi.org/10.1145/1839490.1839493",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Jinlin Chen; Keli Xiao",
    "corresponding_authors": "",
    "abstract": "The performance of a depth-first frequent itemset (FI) miming algorithm is closely related to the total number of recursions. In previous approaches this is mainly decided by the total number of FIs, which results in poor performance when a large number of FIs are involved. To solve this problem, a three-strategy adaptive algorithm, bitmap itemset support counting (BISC), is presented. The core strategy, BISC1, is used in the innermost steps of the recursion. For a database D with only s frequent items, a depth-first approach need up to s levels of recursions to detect all the FIs (up to 2 s ). BISC1 completely replaces these recursions with a special summation that directly calculates the supports of all the possible 2 s candidate itemsets. With BISC1 the run-time is entirely independent of the database after one database scan, and the per-candidate cost is only s . To offset the exponential growth of cost (both time and space) with BISC1 as s increases, a second strategy, BISC2, is introduced to effectively double the acceptable range of s . BISC2 divides an itemset into prefix and suffix and improves the performance by pruning all the itemsets with infrequent prefixes. If the total number of frequent items in D is high, the classic database projection strategy is used. In this case for the first s items a single run of BISC (1 or 2) is applied. For each of the remaining items, a projected database is created and the mining process proceeds recursively. To achieve optimal performance, BISC adaptively decides which strategy to use based on the dataset and minimum support. Experiments show that BISC outperforms previous approaches in all the datasets tested. Even though this does not guarantee that BISC will always perform the best, the result is impressive given the fact that most existing algorithms are only efficient in some types of datasets. The memory usage of BISC is also comparable to those of other algorithms.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1967168037",
    "type": "article"
  },
  {
    "title": "Bayesian Browsing Model",
    "doi": "https://doi.org/10.1145/1857947.1857951",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Chao Liu; Fan Guo; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "A fundamental challenge in utilizing Web search click data is to infer user-perceived relevance from the search log. Not only is the inference a difficult problem involving statistical reasonings but the bulky size, together with the ever-increasing nature, of the log data imposes extra requirements on scalability. In this paper, we propose the Bayesian Browsing Model (BBM), which performs exact inference of the document relevance, only requires a single pass of the data (i.e., the optimal scalability), and is shown effective. We present two sets of experiments to evaluate the model effectiveness and scalability. On the first set of over 50 million search instances of 1.1 million distinct queries, BBM outperforms the state-of-the-art competitor by 29.2% in log-likelihood while being 57 times faster. On the second click log set, spanning a quarter of petabyte, we showcase the scalability of BBM: we implemented it on a commercial MapReduce cluster, and it took only 3 hours to compute the relevance for 1.15 billion distinct query-URL pairs.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2055052256",
    "type": "article"
  },
  {
    "title": "Reflect and correct",
    "doi": "https://doi.org/10.1145/1631162.1631168",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Mustafa Bilgic; Lise Getoor",
    "corresponding_authors": "",
    "abstract": "Information diffusion, viral marketing, graph-based semi-supervised learning, and collective classification all attempt to model and exploit the relationships among nodes in a network to improve the performance of node labeling algorithms. However, sometimes the advantage of exploiting the relationships can become a disadvantage. Simple models like label propagation and iterative classification can aggravate a misclassification by propagating mistakes in the network, while more complex models that define and optimize a global objective function, such as Markov random fields and graph mincuts, can misclassify a set of nodes jointly. This problem can be mitigated if the classification system is allowed to ask for the correct labels for a few of the nodes during inference. However, determining the optimal set of labels to acquire is intractable under relatively general assumptions, which forces us to resort to approximate and heuristic techniques. We describe three such techniques in this article. The first one is based on directly approximating the value of the objective function of label acquisition and greedily acquiring the label that provides the most improvement. The second technique is a simple technique based on the analogy we draw between viral marketing and label acquisition. Finally, we propose a method, which we refer to as reflect and correct , that can learn and predict when the classification system is likely to make mistakes and suggests acquisitions to correct those mistakes. We empirically show on a variety of synthetic and real-world datasets that the reflect and correct method significantly outperforms the other two techniques, as well as other approaches based on network structural measures such as node degree and network clustering.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2164682059",
    "type": "article"
  },
  {
    "title": "Selecting the Right Correlation Measure for Binary Data",
    "doi": "https://doi.org/10.1145/2637484",
    "publication_date": "2014-09-23",
    "publication_year": 2014,
    "authors": "Lian Duan; W. Nick Street; Yanchi Liu; Songhua Xu; Brook Wu",
    "corresponding_authors": "",
    "abstract": "Finding the most interesting correlations among items is essential for problems in many commercial, medical, and scientific domains. Although there are numerous measures available for evaluating correlations, different correlation measures provide drastically different results. Piatetsky-Shapiro provided three mandatory properties for any reasonable correlation measure, and Tan et al. proposed several properties to categorize correlation measures; however, it is still hard for users to choose the desirable correlation measures according to their needs. In order to solve this problem, we explore the effectiveness problem in three ways. First, we propose two desirable properties and two optional properties for correlation measure selection and study the property satisfaction for different correlation measures. Second, we study different techniques to adjust correlation measures and propose two new correlation measures: the Simplified χ 2 with Continuity Correction and the Simplified χ 2 with Support. Third, we analyze the upper and lower bounds of different measures and categorize them by the bound differences. Combining these three directions, we provide guidelines for users to choose the proper measure according to their needs.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1993130246",
    "type": "article"
  },
  {
    "title": "On the Sample Complexity of Random Fourier Features for Online Learning",
    "doi": "https://doi.org/10.1145/2611378",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Ming Lin; Shifeng Weng; Changshui Zhang",
    "corresponding_authors": "",
    "abstract": "We study the sample complexity of random Fourier features for online kernel learning—that is, the number of random Fourier features required to achieve good generalization performance. We show that when the loss function is strongly convex and smooth, online kernel learning with random Fourier features can achieve an O (log T / T ) bound for the excess risk with only O (1/λ 2 ) random Fourier features, where T is the number of training examples and λ is the modulus of strong convexity. This is a significant improvement compared to the existing result for batch kernel learning that requires O ( T ) random Fourier features to achieve a generalization bound O (1/√T). Our empirical study verifies that online kernel learning with a limited number of random Fourier features can achieve similar generalization performance as online learning using full kernel matrix. We also present an enhanced online learning algorithm with random Fourier features that improves the classification performance by multiple passes of training examples and a partial average.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2028082532",
    "type": "article"
  },
  {
    "title": "Employing Semantic Context for Sparse Information Extraction Assessment",
    "doi": "https://doi.org/10.1145/3201407",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Peipei Li; Haixun Wang; Hongsong Li; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "A huge amount of texts available on the World Wide Web presents an unprecedented opportunity for information extraction (IE). One important assumption in IE is that frequent extractions are more likely to be correct. Sparse IE is hence a challenging task because no matter how big a corpus is, there are extractions supported by only a small amount of evidence in the corpus. However, there is limited research on sparse IE, especially in the assessment of the validity of sparse IEs. Motivated by this, we introduce a lightweight, explicit semantic approach for assessing sparse IE. 1 We first use a large semantic network consisting of millions of concepts, entities, and attributes to explicitly model the context of any semantic relationship. Second, we learn from three semantic contexts using different base classifiers to select an optimal classification model for assessing sparse extractions. Finally, experiments show that as compared with several state-of-the-art approaches, our approach can significantly improve the F -score in the assessment of sparse extractions while maintaining the efficiency.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2809811446",
    "type": "article"
  },
  {
    "title": "Representation Learning for Classification in Heterogeneous Graphs with Application to Social Networks",
    "doi": "https://doi.org/10.1145/3201603",
    "publication_date": "2018-07-20",
    "publication_year": 2018,
    "authors": "Ludovic Dos Santos; Benjamin Piwowarski; Ludovic Denoyer; Patrick Gallinari",
    "corresponding_authors": "",
    "abstract": "We address the task of node classification in heterogeneous networks, where the nodes are of different types, each type having its own set of labels, and the relations between nodes may also be of different types. A typical example is provided by social networks where node types may for example be users, content, or films, and relations friendship , like , authorship . Learning and performing inference on such heterogeneous networks is a recent task requiring new models and algorithms. We propose a model, Labeling Heterogeneous Network (LaHNet) , a transductive approach to classification that learns to project the different types of nodes into a common latent space. This embedding is learned so as to reflect different characteristics of the problem such as the correlation between node labels, as well as the graph topology. The application focus is on social graphs, but the algorithm is general and can be used for other domains. The model is evaluated on five datasets representative of different instances of social data.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2883734673",
    "type": "article"
  },
  {
    "title": "Enumerating Trillion Subgraphs On Distributed Systems",
    "doi": "https://doi.org/10.1145/3237191",
    "publication_date": "2018-10-01",
    "publication_year": 2018,
    "authors": "Ha-Myung Park; Francesco Silvestri; Rasmus Pagh; Chin‐Wan Chung; Sung-Hyon Myaeng; U Kang",
    "corresponding_authors": "",
    "abstract": "How can we find patterns from an enormous graph with billions of vertices and edges? The subgraph enumeration, which is to find patterns from a graph, is an important task for graph data analysis with many applications, including analyzing the social network evolution, measuring the significance of motifs in biological networks, observing the dynamics of Internet, and so on. Especially, the triangle enumeration, a special case of the subgraph enumeration, where the pattern is a triangle, has many applications such as identifying suspicious users in social networks, detecting web spams, and finding communities. However, recent networks are so large that most of the previous algorithms fail to process them. Recently, several MapReduce algorithms have been proposed to address such large networks; however, they suffer from the massive shuffled data resulting in a very long processing time. In this article, we propose scalable methods for enumerating trillion subgraphs on distributed systems. We first propose PTE ( Pre-partitioned Triangle Enumeration ), a new distributed algorithm for enumerating triangles in enormous graphs by resolving the structural inefficiency of the previous MapReduce algorithms. PTE enumerates trillions of triangles in a billion scale graph by decreasing three factors: the amount of shuffled data, total work, and network read. We also propose PSE ( Pre-partitioned Subgraph Enumeration ), a generalized version of PTE for enumerating subgraphs that match an arbitrary query graph. Experimental results show that PTE provides 79 times faster performance than recent distributed algorithms on real-world graphs, and succeeds in enumerating more than 3 trillion triangles on the ClueWeb12 graph with 6.3 billion vertices and 72 billion edges. Furthermore, PSE successfully enumerates 265 trillion clique subgraphs with 4 vertices from a subdomain hyperlink network, showing 47 times faster performance than the state of the art distributed subgraph enumeration algorithm.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2895492593",
    "type": "article"
  },
  {
    "title": "Comparative Document Summarization via Discriminative Sentence Selection",
    "doi": "https://doi.org/10.1145/2435209.2435211",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Dingding Wang; Shenghuo Zhu; Tao Li; Yihong Gong",
    "corresponding_authors": "",
    "abstract": "Given a collection of document groups, a natural question is to identify the differences among these groups. Although traditional document summarization techniques can summarize the content of the document groups one by one, there exists a great necessity to generate a summary of the differences among the document groups. In this article, we study a novel problem of summarizing the differences between document groups. A discriminative sentence selection method is proposed to extract the most discriminative sentences that represent the specific characteristics of each document group. Experiments and case studies on real-world data sets demonstrate the effectiveness of our proposed method.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3174196281",
    "type": "article"
  },
  {
    "title": "Active Sampling for Entity Matching with Guarantees",
    "doi": "https://doi.org/10.1145/2500490",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Kedar Bellare; Suresh Iyengar; Aditya Parameswaran; Vibhor Rastogi",
    "corresponding_authors": "",
    "abstract": "In entity matching, a fundamental issue while training a classifier to label pairs of entities as either duplicates or nonduplicates is the one of selecting informative training examples. Although active learning presents an attractive solution to this problem, previous approaches minimize the misclassification rate (0--1 loss) of the classifier, which is an unsuitable metric for entity matching due to class imbalance (i.e., many more nonduplicate pairs than duplicate pairs). To address this, a recent paper [Arasu et al. 2010] proposes to maximize recall of the classifier under the constraint that its precision should be greater than a specified threshold. However, the proposed technique requires the labels of all n input pairs in the worst case. Our main result is an active learning algorithm that approximately maximizes recall of the classifier while respecting a precision constraint with provably sublinear label complexity (under certain distributional assumptions). Our algorithm uses as a black box any active learning module that minimizes 0--1 loss. We show that label complexity of our algorithm is at most log n times the label complexity of the black box, and also bound the difference in the recall of classifier learnt by our algorithm and the recall of the optimal classifier satisfying the precision constraint. We provide an empirical evaluation of our algorithm on several real-world matching data sets that demonstrates the effectiveness of our approach.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2043772275",
    "type": "article"
  },
  {
    "title": "Recommendations Based on Comprehensively Exploiting the Latent Factors Hidden in Items’ Ratings and Content",
    "doi": "https://doi.org/10.1145/3003728",
    "publication_date": "2017-03-10",
    "publication_year": 2017,
    "authors": "Shanshan Feng; Jian Cao; Jie Wang; Shiyou Qian",
    "corresponding_authors": "",
    "abstract": "To improve the performance of recommender systems in a practical manner, several hybrid approaches have been developed by considering item ratings and content information simultaneously. However, most of these hybrid approaches make recommendations based on aggregating different recommendation techniques using various strategies, rather than considering joint modeling of the item’s ratings and content, and thus fail to detect many latent factors that could potentially improve the performance of the recommender systems. For this reason, these approaches continue to suffer from data sparsity and do not work well for recommending items to individual users. A few studies try to describe a user’s preference by detecting items’ latent features from content-description texts as compensation for the sparse ratings. Unfortunately, most of these methods are still generally unable to accomplish recommendation tasks well for two reasons: (1) they learn latent factors from text descriptions or user--item ratings independently, rather than combining them together; and (2) influences of latent factors hidden in texts and ratings are not fully explored. In this study, we propose a probabilistic approach that we denote as latent random walk (LRW) based on the combination of an integrated latent topic model and random walk (RW) with the restart method, which can be used to rank items according to expected user preferences by detecting both their explicit and implicit correlative information, in order to recommend top-ranked items to potentially interested users. As presented in this article, the goal of this work is to comprehensively discover latent factors hidden in items’ ratings and content in order to alleviate the data sparsity problem and to improve the performance of recommender systems. The proposed topic model provides a generative probabilistic framework that discovers users’ implicit preferences and items’ latent features simultaneously by exploiting both ratings and item content information. On the basis of this probabilistic framework, RW can predict a user’s preference for unrated items by discovering global latent relations. In order to show the efficiency of the proposed approach, we test LRW and other state-of-the-art methods on three real-world datasets, namely, CAMRa2011, Yahoo!, and APP. The experiments indicate that our approach outperforms all comparative methods and, in addition, that it is less sensitive to the data sparsity problem, thus demonstrating the robustness of LRW for recommendation tasks.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2595080281",
    "type": "article"
  },
  {
    "title": "A Session-Based Approach to Fast-But-Approximate Interactive Data Cube Exploration",
    "doi": "https://doi.org/10.1145/3070648",
    "publication_date": "2018-02-13",
    "publication_year": 2018,
    "authors": "Niranjan Kamat; Arnab Nandi",
    "corresponding_authors": "",
    "abstract": "With the proliferation of large datasets, sampling has become pervasive in data analysis. Sampling has numerous benefits—from reducing the computation time and cost to increasing the scope of interactive analysis. A popular task in data science, well-suited toward sampling, is the computation of fast-but-approximate aggregations over sampled data. Aggregation is a foundational block of data analysis, with data cube being its primary construct. We observe that such aggregation queries are typically issued in an ad-hoc, interactive setting. In contrast to one-off queries, a typical query session consists of a series of quick queries, interspersed with the user inspecting the results and formulating the next query. The similarity between session queries opens up opportunities for reusing computation of not just query results, but also error estimates. Error estimates need to be provided alongside sampled results for the results to be meaningful. We propose Sesame , a rewrite and caching framework that accelerates the entire interactive &lt;underline&gt;ses&lt;/underline&gt;sion of aggregation queries over &lt;underline&gt;sam&lt;/underline&gt;pl&lt;underline&gt;e&lt;/underline&gt;d data. We focus on two unique and computationally expensive aspects of this use case: query speculation in the presence of sampling, and error computation, and provide novel strategies for result and error reuse. We demonstrate that our approach outperforms conventional sampled aggregation techniques by at least an order of magnitude, without modifying the underlying database.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2787688218",
    "type": "article"
  },
  {
    "title": "A General Embedding Framework for Heterogeneous Information Learning in Large-Scale Networks",
    "doi": "https://doi.org/10.1145/3241063",
    "publication_date": "2018-10-01",
    "publication_year": 2018,
    "authors": "Xiao Huang; Jundong Li; Na Zou; Xia Hu",
    "corresponding_authors": "",
    "abstract": "Network analysis has been widely applied in many real-world tasks, such as gene analysis and targeted marketing. To extract effective features for these analysis tasks, network embedding automatically learns a low-dimensional vector representation for each node, such that the meaningful topological proximity is well preserved. While the embedding algorithms on pure topological structure have attracted considerable attention, in practice, nodes are often abundantly accompanied with other types of meaningful information, such as node attributes, second-order proximity, and link directionality. A general framework for incorporating the heterogeneous information into network embedding could be potentially helpful in learning better vector representations. However, it remains a challenging task to jointly embed the geometrical structure and a distinct type of information due to the heterogeneity. In addition, the real-world networks often contain a large number of nodes, which put demands on the scalability of the embedding algorithms. To bridge the gap, in this article, we propose a general embedding framework named Heterogeneous Information Learning in Large-scale networks (HILL) to accelerate the joint learning. It enables the simultaneous node proximity assessing process to be done in a distributed manner by decomposing the complex modeling and optimization into many simple and independent sub-problems. We validate the significant correlation between the heterogeneous information and topological structure, and illustrate the generalizability of HILL by applying it to perform attributed network embedding and second-order proximity learning. A variation is proposed for link directionality modeling. Experimental results on real-world networks demonstrate the effectiveness and efficiency of HILL.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2895654512",
    "type": "article"
  },
  {
    "title": "Variant Grassmann Manifolds",
    "doi": "https://doi.org/10.1145/3314203",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Junyuan Hong; Yang Li; Huanhuan Chen",
    "corresponding_authors": "",
    "abstract": "In classification tasks, classifiers trained with finite examples might generalize poorly to new data with unknown variance. For this issue, data augmentation is a successful solution where numerous artificial examples are added to training sets. In this article, we focus on the data augmentation for improving the accuracy of action recognition, where action videos are modeled by linear dynamical systems and approximately represented as linear subspaces. These subspace representations lie in a non-Euclidean space, named Grassmann manifold, containing points as orthonormal matrixes. It is our concern that poor generalization may result from the variance of manifolds when data come from different sources or classes. Thus, we introduce infinitely many variant Grassmann manifolds (VGM) subject to a known distribution, then represent each action video as different Grassmann points leading to augmented representations. Furthermore, a prior based on the stability of subspace bases is introduced, so the manifold distribution can be adaptively determined, balancing discrimination and representation. Experimental results of multi-class and multi-source classification show that VGM softmax classifiers achieve lower test error rates compared to methods with a single manifold.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2944220395",
    "type": "article"
  },
  {
    "title": "Translations Diversification for Expert Finding",
    "doi": "https://doi.org/10.1145/3320489",
    "publication_date": "2019-05-29",
    "publication_year": 2019,
    "authors": "Mahdi Dehghan; Ahmad Ali Abin",
    "corresponding_authors": "",
    "abstract": "Expert finding is the task of retrieving and ranking knowledgeable people in the subject of user’s query. It is a well-studied problem that has attracted the attention of many researchers. The most important challenge in expert finding is to determine the similarity between query words and documents authored by candidate experts. One of the most important challenges in Information Retrieval (IR) community is the issue of vocabulary gap between queries and documents. In this study, a translation model based on words clustering in two query and co-occurrence spaces is proposed to overcome this problem. First, the words that are semantically close, are clustered in a query space and then each cluster in this space are clustered again in a co-occurrence space. Representatives of each cluster in the co-occurrence space are considered as a diverse subset of the parent cluster. By this method, the query translations are expected to be diversified in the query space. Next, a probabilistic model, that is based on the belonging degree of word to cluster and similarity of cluster to query in the query space, is used to consider the problem of vocabulary gap. Finally, the corresponding translations to each query are used in conjunction with a combination model for expert finding. Experiments on Stack Overflow dataset show the effectiveness of the proposed method for expert finding.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2947296954",
    "type": "article"
  },
  {
    "title": "Leveraging Kernel-Incorporated Matrix Factorization for App Recommendation",
    "doi": "https://doi.org/10.1145/3320482",
    "publication_date": "2019-05-29",
    "publication_year": 2019,
    "authors": "Chenyang Liu; Jian Cao; Shanshan Feng",
    "corresponding_authors": "",
    "abstract": "The ever-increasing number of smartphone applications (apps) available on different app markets poses a challenge for personalized app recommendation. Conventional collaborative filtering-based recommendation methods suffer from sparse and binary user-app implicit feedback, which results in poor performance in discriminating user-app preferences. In this article, we first propose two kernel incorporated probabilistic matrix factorization models, which introduce app-categorical information to constrain the user and app latent features to be similar to their neighbors in the latent space. The two models are solved by Stochastic Gradient Descent with a user-oriented negative sampling scheme. To further improve the recommendation performance, we construct pseudo user-app ratings based on user-app usage information, and propose a novel kernelized non-negative matrix factorization by incorporating non-negative constraints on latent factors to predict user-app preferences. This model also leverages user--user and app--app similarities with regard to app-categorical information to mine the latent geometric structure in the pseudo-rating space. Adopting the Karush--Kuhn--Tucker conditions, a Multiplicative Updating Rules based optimization is proposed for model learning, and the convergence is proved by introducing an auxiliary function. The experimental results on a real user-app installation usage dataset show the comparable performance of our models with the state-of-the-art baselines in terms of two ranking-oriented evaluation metrics.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2947767618",
    "type": "article"
  },
  {
    "title": "Identifying Complements and Substitutes of Products",
    "doi": "https://doi.org/10.1145/3320277",
    "publication_date": "2019-06-20",
    "publication_year": 2019,
    "authors": "Mingyue Zhang; Xuan Wei; Xunhua Guo; Guoqing Chen; Qiang Wei",
    "corresponding_authors": "",
    "abstract": "Complements and substitutes are two typical product relationships that deserve consideration in online product recommendation. One of the key objectives of recommender systems is to promote cross-selling, which heavily relies on recommending the appropriate type of products in specific scenarios. Research on consumer behavior has shown that consumers usually prefer substitutes in the browsing stage whereas complements in the purchasing stage. Thus, it is of great importance to identify the complementary and substitutable relationships between products. In this article, we design a neural network based framework that integrates the textual content and non-textual information of online reviews to mine product relationships. For the textual content, we utilize methods such as LDA topic modeling to represent products in a succinct form called “embedding.” To capture the semantics of complementary and substitutable relationships, we design a modeling process that transfers the product embeddings into semantic features and incorporates additional non-textual factors of product reviews. Extensive experiments are conducted to verify the effectiveness of the proposed product relationship mining model. The advantages and robustness of our model are discussed from various perspectives.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2951587203",
    "type": "article"
  },
  {
    "title": "Heterogeneous-Length Text Topic Modeling for Reader-Aware Multi-Document Summarization",
    "doi": "https://doi.org/10.1145/3333030",
    "publication_date": "2019-08-08",
    "publication_year": 2019,
    "authors": "Jipeng Qiang; Ping Chen; Wei Ding; Tong Wang; Fei Xie; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "More and more user comments like Tweets are available, which often contain user concerns. In order to meet the demands of users, a good summary generating from multiple documents should consider reader interests as reflected in reader comments. In this article, we focus on how to generate a summary from multi-document documents by considering reader comments, named as reader-aware multi-document summarization (RA-MDS). We present an innovative topic-based method for RA-MDA, which exploits latent topics to obtain the most salient and lessen redundancy summary from multiple documents. Since finding latent topics for RA-MDS is a crucial step, we also present a Heterogeneous-length Text Topic Modeling (HTTM) to extract topics from the corpus that includes both news reports and user comments, denoted as heterogeneous-length texts. In this case, the latent topics extract by HTTM cover not only important aspects of the event, but also aspects that attract reader interests. Comparisons on summary benchmark datasets also confirm that the proposed RA-MDS method is effective in improving the quality of extracted summaries. In addition, experimental results demonstrate that the proposed topic modeling method outperforms existing topic modeling algorithms.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2967411808",
    "type": "article"
  },
  {
    "title": "Evolutionary Classifier and Cluster Selection Approach for Ensemble Classification",
    "doi": "https://doi.org/10.1145/3366633",
    "publication_date": "2019-12-13",
    "publication_year": 2019,
    "authors": "Zohaib Jan; Brijesh Verma",
    "corresponding_authors": "",
    "abstract": "Ensemble classifiers improve the classification performance by combining several classifiers using a suitable fusion methodology. Many ensemble classifier generation methods have been developed that allowed the training of multiple classifiers on a single dataset. As such random subspace is a common methodology utilized by many state-of-the-art ensemble classifiers that generate random subsamples from the input data and train classifiers on different subsamples. Real-world datasets have randomness and noise in them, therefore not all randomly generated samples are suitable for training. In this article, we propose a novel particle swarm optimization-based approach to optimize the random subspace to generate an ensemble classifier. We first generate a random subspace by incrementally clustering input data and then optimize all generated data clusters. On all optimized data clusters, a set of classifiers is trained and added to the pool. The pool of classifiers is then optimized and an optimized ensemble classifier is generated. The proposed approach is tested on 12 benchmark datasets from the UCI repository and results are compared with current state-of-the-art ensemble classifier approaches. A statistical significance test is also conducted and an analysis is presented.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2998837416",
    "type": "article"
  },
  {
    "title": "Bradykinesia Recognition in Parkinson’s Disease via Single RGB Video",
    "doi": "https://doi.org/10.1145/3369438",
    "publication_date": "2020-02-09",
    "publication_year": 2020,
    "authors": "Bo Lin; Wei Luo; Zhiling Luo; Bo Wang; Shuiguang Deng; Jianwei Yin; MengChu Zhou",
    "corresponding_authors": "",
    "abstract": "Parkinson’s disease is a progressive nervous system disorder afflicting millions of patients. Among its motor symptoms, bradykinesia is one of the cardinal manifestations. Experienced doctors are required for the clinical diagnosis of bradykinesia, but sometimes they also miss subtle changes, especially in early stages of such disease. Therefore, developing auxiliary diagnostic methods that can automatically detect bradykinesia has received more and more attention. In this article, we employ a two-stage framework for bradykinesia recognition based on the video of patient movement. First, convolution neural networks are trained to localize keypoints in each video frame. These time-varying coordinates form motion trajectories that represent the whole movement. From the trajectory, we then propose novel measurements, namely stability , completeness , and self-similarity , to quantify different motor behaviors. We also propose a periodic motion model called PMNet . An encoder--decoder structure is applied to learn a low dimensional representation of a motion process. The compressed motion process and quantified motor behaviors are combined as inputs to a fully-connected neural network. Different from the traditional means, our solution extends the application scenario outside the hospital and can be easily transplanted to conduct similar tasks. A commonly used clinical assessment is served as a case study. Experimental results based on real-world data validate the effectiveness of our approach for bradykinesia recognition.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3006574008",
    "type": "article"
  },
  {
    "title": "Treatment Effect Estimation via Differentiated Confounder Balancing and Regression",
    "doi": "https://doi.org/10.1145/3365677",
    "publication_date": "2019-12-13",
    "publication_year": 2019,
    "authors": "Kun Kuang; Peng Cui; Bo Li; Meng Jiang; Yashen Wang; Fei Wu; Shiqiang Yang",
    "corresponding_authors": "",
    "abstract": "Treatment effect plays an important role on decision making in many fields, such as social marketing, healthcare, and public policy. The key challenge on estimating treatment effect in the wild observational studies is to handle confounding bias induced by imbalance of the confounder distributions between treated and control units. Traditional methods remove confounding bias by re-weighting units with supposedly accurate propensity score estimation under the unconfoundedness assumption. Controlling high-dimensional variables may make the unconfoundedness assumption more plausible, but poses new challenge on accurate propensity score estimation. One strand of recent literature seeks to directly optimize weights to balance confounder distributions, bypassing propensity score estimation. But existing balancing methods fail to do selection and differentiation among the pool of a large number of potential confounders, leading to possible underperformance in many high-dimensional settings. In this article, we propose a data-driven Differentiated Confounder Balancing (DCB) algorithm to jointly select confounders, differentiate weights of confounders and balance confounder distributions for treatment effect estimation in the wild high-dimensional settings. Besides, under some settings with heavy confounding bias, in order to further reduce the bias and variance of estimated treatment effect, we propose a Regression Adjusted Differentiated Confounder Balancing (RA-DCB) algorithm based on our DCB algorithm by incorporating outcome regression adjustment. The synergistic learning algorithms we proposed are more capable of reducing the confounding bias in many observational studies. To validate the effectiveness of our DCB and RA-DCB algorithms, we conduct extensive experiments on both synthetic and real-world datasets. The experimental results clearly demonstrate that our algorithms outperform the state-of-the-art methods. By incorporating regression adjustment, our RA-DCB algorithm achieves more precise estimation on treatment effect than DCB algorithm, especially under the settings with heavy confounding bias. Moreover, we show that the top features ranked by our algorithm generate accurate prediction of online advertising effect.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3009551409",
    "type": "article"
  },
  {
    "title": "Efficient Outlier Detection in Text Corpus Using Rare Frequency and Ranking",
    "doi": "https://doi.org/10.1145/3399712",
    "publication_date": "2020-10-03",
    "publication_year": 2020,
    "authors": "Wathsala Anupama Mohotti; Richi Nayak",
    "corresponding_authors": "",
    "abstract": "Outlier detection in text data collections has become significant due to the need of finding anomalies in the myriad of text data sources. High feature dimensionality, together with the larger size of these document collections, presents a need for developing accurate outlier detection methods with high efficiency. Traditional outlier detection methods face several challenges including data sparseness, distance concentration, and the presence of a larger number of sub-groups when dealing with text data. In this article, we propose to address these issues by developing novel concepts such as presenting documents with the rare document frequency, finding ranking-based neighborhood for similarity computation, and identifying sub-dense local neighborhoods in high dimensions. To improve the proposed primary method based on rare document frequency, we present several novel ensemble approaches using the ranking concept to reduce the false identifications while finding the higher number of true outliers. Extensive empirical analysis shows that the proposed method and its ensemble variations improve the quality of outlier detection in document repositories as well as they are found scalable compared to the relevant benchmarking methods.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3049725889",
    "type": "article"
  },
  {
    "title": "Discovering General Prominent Streaks in Sequence Data",
    "doi": "https://doi.org/10.1145/2601439",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Gensheng Zhang; Xiao Jiang; Ping Luo; Min Wang; Chengkai Li",
    "corresponding_authors": "",
    "abstract": "This article studies the problem of prominent streak discovery in sequence data. Given a sequence of values, a prominent streak is a long consecutive subsequence consisting of only large (small) values, such as consecutive games of outstanding performance in sports, consecutive hours of heavy network traffic, and consecutive days of frequent mentioning of a person in social media. Prominent streak discovery provides insightful data patterns for data analysis in many real-world applications and is an enabling technique for computational journalism. Given its real-world usefulness and complexity, the research on prominent streaks in sequence data opens a spectrum of challenging problems. A baseline approach to finding prominent streaks is a quadratic algorithm that exhaustively enumerates all possible streaks and performs pairwise streak dominance comparison. For more efficient methods, we make the observation that prominent streaks are in fact skyline points in two dimensions—streak interval length and minimum value in the interval. Our solution thus hinges on the idea to separate the two steps in prominent streak discovery: candidate streak generation and skyline operation over candidate streaks. For candidate generation, we propose the concept of local prominent streak (LPS). We prove that prominent streaks are a subset of LPSs and the number of LPSs is less than the length of a data sequence, in comparison with the quadratic number of candidates produced by the brute-force baseline method. We develop efficient algorithms based on the concept of LPS. The nonlinear local prominent streak (NLPS)-based method considers a superset of LPSs as candidates, and the linear local prominent streak (LLPS)-based method further guarantees to consider only LPSs. The proposed properties and algorithms are also extended for discovering general top- k , multisequence, and multidimensional prominent streaks. The results of experiments using multiple real datasets verified the effectiveness of the proposed methods and showed orders of magnitude performance improvement against the baseline method.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2022444209",
    "type": "article"
  },
  {
    "title": "Multiresolution Tensor Decompositions with Mode Hierarchies",
    "doi": "https://doi.org/10.1145/2532169",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Claudio Schifanella; K. Selçuk Candan; Maria Luisa Sapino",
    "corresponding_authors": "",
    "abstract": "Tensors (multidimensional arrays) are widely used for representing high-order dimensional data, in applications ranging from social networks, sensor data, and Internet traffic. Multiway data analysis techniques, in particular tensor decompositions, allow extraction of hidden correlations among multiway data and thus are key components of many data analysis frameworks. Intuitively, these algorithms can be thought of as multiway clustering schemes, which consider multiple facets of the data in identifying clusters, their weights, and contributions of each data element. Unfortunately, algorithms for fitting multiway models are, in general, iterative and very time consuming. In this article, we observe that, in many applications, there is a priori background knowledge (or metadata) about one or more domain dimensions. This metadata is often in the form of a hierarchy that clusters the elements of a given data facet (or mode). We investigate whether such single-mode data hierarchies can be used to boost the efficiency of tensor decomposition process, without significant impact on the final decomposition quality. We consider each domain hierarchy as a guide to help provide higher- or lower-resolution views of the data in the tensor on demand and we rely on these metadata-induced multiresolution tensor representations to develop a multiresolution approach to tensor decomposition. In this article, we focus on an alternating least squares (ALS)--based implementation of the two most important decomposition models such as the PARAllel FACtors (PARAFAC, which decomposes a tensor into a diagonal tensor and a set of factor matrices) and the Tucker (which produces as result a core tensor and a set of dimension-subspaces matrices). Experiment results show that, when the available metadata is used as a rough guide, the proposed multiresolution method helps fit both PARAFAC and Tucker models with consistent (under different parameters settings) savings in execution time and memory consumption, while preserving the quality of the decomposition.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2050703183",
    "type": "article"
  },
  {
    "title": "Information Measures in Statistical Privacy and Data Processing Applications",
    "doi": "https://doi.org/10.1145/2700407",
    "publication_date": "2015-06-01",
    "publication_year": 2015,
    "authors": "Bing-Rong Lin; Daniel Kifer",
    "corresponding_authors": "",
    "abstract": "In statistical privacy, utility refers to two concepts: information preservation, how much statistical information is retained by a sanitizing algorithm, and usability, how (and with how much difficulty) one extracts this information to build statistical models, answer queries, and so forth. Some scenarios incentivize a separation between information preservation and usability, so that the data owner first chooses a sanitizing algorithm to maximize a measure of information preservation, and, afterward, the data consumers process the sanitized output according to their various individual needs [Ghosh et al. 2009; Williams and McSherry 2010]. We analyze the information-preserving properties of utility measures with a combination of two new and three existing utility axioms and study how violations of an axiom can be fixed. We show that the average (over possible outputs of the sanitizer) error of Bayesian decision makers forms the unique class of utility measures that satisfy all of the axioms. The axioms are agnostic to Bayesian concepts such as subjective probabilities and hence strengthen support for Bayesian views in privacy research. In particular, this result connects information preservation to aspects of usability—if the information preservation of a sanitizing algorithm should be measured as the average error of a Bayesian decision maker, shouldn’t Bayesian decision theory be a good choice when it comes to using the sanitized outputs for various purposes? We put this idea to the test in the unattributed histogram problem where our decision-theoretic postprocessing algorithm empirically outperforms previously proposed approaches.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2289484828",
    "type": "article"
  },
  {
    "title": "CGC",
    "doi": "https://doi.org/10.1145/2903147",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Wei Cheng; Zhishan Guo; Xiang Zhang; Wei Wang",
    "corresponding_authors": "",
    "abstract": "Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the same set of instances. Thus instances in different domains can be treated as having strict one-to-one relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this paper, we propose a flexible and robust framework, CGC (Co-regularized Graph Clustering), based on non-negative matrix factorization (NMF), to tackle these challenges. CGC has several advantages over the existing methods. First, it supports many-to-many cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. We develop an efficient optimization method that guarantees to find the global optimal solution with a given confidence requirement. The proposed method can automatically identify noisy domains and assign smaller weights to them. This helps to obtain optimal graph partition for the focused domain. Extensive experimental results on UCI benchmark data sets, newsgroup data sets and biological interaction networks demonstrate the effectiveness of our approach.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2291338622",
    "type": "article"
  },
  {
    "title": "Inferring Dynamic Diffusion Networks in Online Media",
    "doi": "https://doi.org/10.1145/2882968",
    "publication_date": "2016-06-14",
    "publication_year": 2016,
    "authors": "Maryam Tahani; Ali Mohammad Afshin Hemmatyar; Hamid R. Rabiee; Maryam Ramezani",
    "corresponding_authors": "",
    "abstract": "Online media play an important role in information societies by providing a convenient infrastructure for different processes. Information diffusion that is a fundamental process taking place on social and information networks has been investigated in many studies. Research on information diffusion in these networks faces two main challenges: (1) In most cases, diffusion takes place on an underlying network, which is latent and its structure is unknown. (2) This latent network is not fixed and changes over time. In this article, we investigate the diffusion network extraction (DNE) problem when the underlying network is dynamic and latent. We model the diffusion behavior (existence probability) of each edge as a stochastic process and utilize the Hidden Markov Model (HMM) to discover the most probable diffusion links according to the current observation of the diffusion process, which is the infection time of nodes and the past diffusion behavior of links. We evaluate the performance of our Dynamic Diffusion Network Extraction (DDNE) method, on both synthetic and real datasets. Experimental results show that the performance of the proposed method is independent of the cascade transmission model and outperforms the state of art method in terms of F-measure.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2429302915",
    "type": "article"
  },
  {
    "title": "Efficient Ridesharing Framework for Ride-matching via Heterogeneous Network Embedding",
    "doi": "https://doi.org/10.1145/3373839",
    "publication_date": "2020-03-13",
    "publication_year": 2020,
    "authors": "Lei Tang; Zihang Liu; Yaling Zhao; Zongtao Duan; Jingchi Jia",
    "corresponding_authors": "",
    "abstract": "Ridesharing has attracted increasing attention in recent years, and combines the flexibility and speed of private cars with the reduced cost of fixed-line systems to benefit alleviating traffic pressure. A major issue in ridesharing is the accurate assignment of passengers to drivers, and how to maximize the number of rides shared between people being assigned to different drivers has become an increasingly popular research topic. There are two major challenges facing ride-matching: scalability and sparsity. Here, we show that network embedding drives the optimal matches between drivers and riders. Contrary to existing approaches that merely depend on the proximity between passengers and drivers, we employ a heterogeneous network to learn the latent semantics from different choices in two types of ridesharing, and extract features in terms of user trajectories and sentiment. A novel framework for ridesharing, RShareForm, which encodes not only the objects but also a variety of semantic relationships between them, is proposed. This article extends the existing skip-gram model to incorporate meta-paths over a proposed heterogeneous network. It allows diverse features to be used to search for similar participants and then ranks them to improve the quality of ride-matching. Extensive experiments on a large-scale dataset from DiDi in Chengdu, China show that by leveraging heterogeneous network embedding with meta paths, RShareForm can significantly improve the accuracy of identifying the participants for ridesharing over existing methods, including both meta-path guided similarity search methods and variants of embedding methods.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3014137702",
    "type": "article"
  },
  {
    "title": "Social Collaborative Mutual Learning for Item Recommendation",
    "doi": "https://doi.org/10.1145/3387162",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Tianyu Zhu; Guannan Liu; Guoqing Chen",
    "corresponding_authors": "",
    "abstract": "Recommender Systems (RSs) provide users with item choices based on their preferences reflected in past interactions and become important tools to alleviate the information overload problem for users. However, in real-world scenarios, the user–item interaction matrix is generally sparse, leading to the poor performance of recommendation methods. To cope with this problem, social information is introduced into these methods in several ways, such as regularization, ensemble, and sampling. However, these strategies to use social information have their limitations. The regularization and ensemble strategies may suffer from the over-smoothing problem, while the sampling-based strategy may be affected by the overfitting problem. To overcome the limitations of the previous efforts, a novel social recommendation model, namely, Social Collaborative Mutual Learning (SCML), is proposed in this article. SCML combines the item-based CF model with the social CF model by two well-designed mutual regularization strategies. The embedding-level mutual regularization forces the user representations in two models to be close, and the output-level mutual regularization matches the distributions of the predictions in two models. Extensive experiments on three public datasets show that SCML significantly outperforms the baseline methods and the proposed mutual regularization strategies can embrace the advantages of the item-based CF model and the social CF model to improve the recommendation performance.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3032898959",
    "type": "article"
  },
  {
    "title": "Learning Distance Metrics from Probabilistic Information",
    "doi": "https://doi.org/10.1145/3364320",
    "publication_date": "2020-07-06",
    "publication_year": 2020,
    "authors": "Mengdi Huai; Chenglin Miao; Yaliang Li; Qiuling Suo; Lü Su; Aidong Zhang",
    "corresponding_authors": "",
    "abstract": "The goal of metric learning is to learn a good distance metric that can capture the relationships among instances, and its importance has long been recognized in many fields. An implicit assumption in the traditional settings of metric learning is that the associated labels of the instances are deterministic. However, in many real-world applications, the associated labels come naturally with probabilities instead of deterministic values, which makes it difficult for the existing metric-learning methods to work well in these applications. To address this challenge, in this article, we study how to effectively learn the distance metric from datasets that contain probabilistic information, and then propose several novel metric-learning mechanisms for two types of probabilistic labels, i.e., the instance-wise probabilistic label and the group-wise probabilistic label. Compared with the existing metric-learning methods, our proposed mechanisms are capable of learning distance metrics directly from the probabilistic labels with high accuracy. We also theoretically analyze the proposed mechanisms and conduct extensive experiments on real-world datasets to verify the desirable properties of these mechanisms.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3038711713",
    "type": "article"
  },
  {
    "title": "Exploiting Temporal Dynamics in Product Reviews for Dynamic Sentiment Prediction at the Aspect Level",
    "doi": "https://doi.org/10.1145/3441451",
    "publication_date": "2021-04-18",
    "publication_year": 2021,
    "authors": "Peike Xia; Wenjun Jiang; Jie Wu; Surong Xiao; Guojun Wang",
    "corresponding_authors": "",
    "abstract": "Online reviews and ratings play an important role in shaping the purchase decisions of customers in e-commerce. Many researches have been done to make proper recommendations for users, by exploiting reviews, ratings, user profiles, or behaviors. However, the dynamic evolution of user preferences and item properties haven’t been fully exploited. Moreover, it lacks fine-grained studies at the aspect level. To address the above issues, we define two concepts of user maturity and item popularity, to better explore the dynamic changes for users and items. We strive to exploit fine-grained information at the aspect level and the evolution of users and items, for dynamic sentiment prediction. First, we analyze three real datasets from both the overall level and the aspect level, to discover the dynamic changes (i.e., gradual changes and sudden changes) in user aspect preferences and item aspect properties. Next, we propose a novel model of Aspect-based Sentiment Dynamic Prediction (ASDP), to dynamically capture and exploit the change patterns with uniform time intervals. We further propose the improved model ASDP+ with a bin segmentation algorithm to set the time intervals non-uniformly based on the sudden changes. Experimental results on three real-world datasets show that our work leads to significant improvements.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3155244505",
    "type": "article"
  },
  {
    "title": "Dual-Embedding based Deep Latent Factor Models for Recommendation",
    "doi": "https://doi.org/10.1145/3447395",
    "publication_date": "2021-04-15",
    "publication_year": 2021,
    "authors": "Weiyu Cheng; Yanyan Shen; Linpeng Huang; Yanmin Zhu",
    "corresponding_authors": "",
    "abstract": "Among various recommendation methods, latent factor models are usually considered to be state-of-the-art techniques, which aim to learn user and item embeddings for predicting user-item preferences. When applying latent factor models to the recommendation with implicit feedback, the quality of embeddings always suffers from inadequate positive feedback and noisy negative feedback. Inspired by the idea of NSVD that represents users based on their interacted items, this article proposes a dual-embedding based deep latent factor method for recommendation with implicit feedback. In addition to learning a primitive embedding for a user (resp. item), we represent each user (resp. item) with an additional embedding from the perspective of the interacted items (resp. users) and propose attentive neural methods to discriminate the importance of interacted users/items for dual-embedding learning. We design two dual-embedding based deep latent factor models, DELF and DESEQ, for pure collaborative filtering and temporal collaborative filtering (i.e., sequential recommendation), respectively. The novel attempt of the proposed models is to capture each user-item interaction with four deep representations that are subtly fused for preference prediction. We conducted extensive experiments on four real-world datasets. The results verify the effectiveness of user/item dual embeddings and the superior performance of our methods on item recommendation.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3155474951",
    "type": "article"
  },
  {
    "title": "Heterogeneous Graphlets",
    "doi": "https://doi.org/10.1145/3418773",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Ryan A. Rossi; Nesreen K. Ahmed; Aldo Carranza; David Arbour; Anup Rao; Sungchul Kim; Eunyee Koh",
    "corresponding_authors": "",
    "abstract": "In this article, we introduce a generalization of graphlets to heterogeneous networks called typed graphlets . Informally, typed graphlets are small typed induced subgraphs. Typed graphlets generalize graphlets to rich heterogeneous networks as they explicitly capture the higher-order typed connectivity patterns in such networks. To address this problem, we describe a general framework for counting the occurrences of such typed graphlets. The proposed algorithms leverage a number of combinatorial relationships for different typed graphlets. For each edge, we count a few typed graphlets, and with these counts along with the combinatorial relationships, we obtain the exact counts of the other typed graphlets in o (1) constant time. Notably, the worst-case time complexity of the proposed approach matches the time complexity of the best known untyped algorithm. In addition, the approach lends itself to an efficient lock-free and asynchronous parallel implementation. While there are no existing methods for typed graphlets, there has been some work that focused on computing a different and much simpler notion called colored graphlet. The experiments confirm that our proposed approach is orders of magnitude faster and more space-efficient than methods for computing the simpler notion of colored graphlet. Unlike these methods that take hours on small networks, the proposed approach takes only seconds on large networks with millions of edges. Notably, since typed graphlet is more general than colored graphlet (and untyped graphlets), the counts of various typed graphlets can be combined to obtain the counts of the much simpler notion of colored graphlets. The proposed methods give rise to new opportunities and applications for typed graphlets.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3160669444",
    "type": "article"
  },
  {
    "title": "Scholar2vec: Vector Representation of Scholars for Lifetime Collaborator Prediction",
    "doi": "https://doi.org/10.1145/3442199",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Wei Wang; Feng Xia; Jian Wu; Zhiguo Gong; Hanghang Tong; Brian D. Davison",
    "corresponding_authors": "",
    "abstract": "While scientific collaboration is critical for a scholar, some collaborators can be more significant than others, e.g., lifetime collaborators. It has been shown that lifetime collaborators are more influential on a scholar’s academic performance. However, little research has been done on investigating predicting such special relationships in academic networks. To this end, we propose Scholar2vec, a novel neural network embedding for representing scholar profiles. First, our approach creates scholars’ research interest vector from textual information, such as demographics, research, and influence. After bridging research interests with a collaboration network, vector representations of scholars can be gained with graph learning. Meanwhile, since scholars are occupied with various attributes, we propose to incorporate four types of scholar attributes for learning scholar vectors. Finally, the early-stage similarity sequence based on Scholar2vec is used to predict lifetime collaborators with machine learning methods. Extensive experiments on two real-world datasets show that Scholar2vec outperforms state-of-the-art methods in lifetime collaborator prediction. Our work presents a new way to measure the similarity between two scholars by vector representation, which tackles the knowledge between network embedding and academic relationship mining.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3168839214",
    "type": "article"
  },
  {
    "title": "Lost in Transduction: Transductive Transfer Learning in Text Classification",
    "doi": "https://doi.org/10.1145/3453146",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Alejandro Moreo; Andrea Esuli; Fabrizio Sebastiani",
    "corresponding_authors": "",
    "abstract": "Obtaining high-quality labelled data for training a classifier in a new application domain is often costly. Transfer Learning (a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution. Transductive Transfer Learning denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3183441058",
    "type": "article"
  },
  {
    "title": "Spatio-Temporal Event Forecasting Using Incremental Multi-Source Feature Learning",
    "doi": "https://doi.org/10.1145/3464976",
    "publication_date": "2021-09-13",
    "publication_year": 2021,
    "authors": "Liang Zhao; Yuyang Gao; Jieping Ye; Feng Chen; Yanfang Ye; Chang‐Tien Lu; Naren Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "The forecasting of significant societal events such as civil unrest and economic crisis is an interesting and challenging problem which requires both timeliness, precision, and comprehensiveness. Significant societal events are influenced and indicated jointly by multiple aspects of a society, including its economics, politics, and culture. Traditional forecasting methods based on a single data source find it hard to cover all these aspects comprehensively, thus limiting model performance. Multi-source event forecasting has proven promising but still suffers from several challenges, including (1) geographical hierarchies in multi-source data features, (2) hierarchical missing values, (3) characterization of structured feature sparsity, and (4) difficulty in model’s online update with incomplete multiple sources. This article proposes a novel feature learning model that concurrently addresses all the above challenges. Specifically, given multi-source data from different geographical levels, we design a new forecasting model by characterizing the lower-level features’ dependence on higher-level features. To handle the correlations amidst structured feature sets and deal with missing values among the coupled features, we propose a novel feature learning model based on an N th-order strong hierarchy and fused-overlapping group Lasso. An efficient algorithm is developed to optimize model parameters and ensure global optima. More importantly, to enable the model update in real time, the online learning algorithm is formulated and active set techniques are leveraged to resolve the crucial challenge when new patterns of missing features appear in real time. Extensive experiments on 10 datasets in different domains demonstrate the effectiveness and efficiency of the proposed models.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3198967865",
    "type": "article"
  },
  {
    "title": "Exploiting Heterogeneous Graph Neural Networks with Latent Worker/Task Correlation Information for Label Aggregation in Crowdsourcing",
    "doi": "https://doi.org/10.1145/3460865",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Hanlu Wu; Tengfei Ma; Lingfei Wu; Fangli Xu; Shouling Ji",
    "corresponding_authors": "",
    "abstract": "Crowdsourcing has attracted much attention for its convenience to collect labels from non-expert workers instead of experts. However, due to the high level of noise from the non-experts, a label aggregation model that infers the true label from noisy crowdsourced labels is required. In this article, we propose a novel framework based on graph neural networks for aggregating crowd labels. We construct a heterogeneous graph between workers and tasks and derive a new graph neural network to learn the representations of nodes and the true labels. Besides, we exploit the unknown latent interaction between the same type of nodes (workers or tasks) by adding a homogeneous attention layer in the graph neural networks. Experimental results on 13 real-world datasets show superior performance over state-of-the-art models.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3201397997",
    "type": "article"
  },
  {
    "title": "Stacked Convolutional Sparse Auto-Encoders for Representation Learning",
    "doi": "https://doi.org/10.1145/3434767",
    "publication_date": "2021-03-05",
    "publication_year": 2021,
    "authors": "Yi Zhu; Lei Li; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Deep learning seeks to achieve excellent performance for representation learning in image datasets. However, supervised deep learning models such as convolutional neural networks require a large number of labeled image data, which is intractable in applications, while unsupervised deep learning models like stacked denoising auto-encoder cannot employ label information. Meanwhile, the redundancy of image data incurs performance degradation on representation learning for aforementioned models. To address these problems, we propose a semi-supervised deep learning framework called stacked convolutional sparse auto-encoder, which can learn robust and sparse representations from image data with fewer labeled data records. More specifically, the framework is constructed by stacking layers. In each layer, higher layer feature representations are generated by features of lower layers in a convolutional way with kernels learned by a sparse auto-encoder. Meanwhile, to solve the data redundance problem, the algorithm of Reconstruction Independent Component Analysis is designed to train on patches for sphering the input data. The label information is encoded using a Softmax Regression model for semi-supervised learning. With this framework, higher level representations are learned by layers mapping from image data. It can boost the performance of the base subsequent classifiers such as support vector machines. Extensive experiments demonstrate the superior classification performance of our framework compared to several state-of-the-art representation learning methods.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3133979803",
    "type": "article"
  },
  {
    "title": "Time-Aware Graph Embedding: A Temporal Smoothness and Task-Oriented Approach",
    "doi": "https://doi.org/10.1145/3480243",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Yonghui Xu; Shengjie Sun; Huiguo Zhang; Chang’an Yi; Yuan Miao; Dong Yang; Xiaonan Meng; Yi Hu; Ke Wang; Huaqing Min; Hengjie Song; Chuanyan Miao",
    "corresponding_authors": "",
    "abstract": "Knowledge graph embedding, which aims at learning the low-dimensional representations of entities and relationships, has attracted considerable research efforts recently. However, most knowledge graph embedding methods focus on the structural relationships in fixed triples while ignoring the temporal information. Currently, existing time-aware graph embedding methods only focus on the factual plausibility, while ignoring the temporal smoothness, which models the interactions between a fact and its contexts, and thus can capture fine-granularity temporal relationships. This leads to the limited performance of embedding related applications. To solve this problem, this article presents a Robustly Time-aware Graph Embedding (RTGE) method by incorporating temporal smoothness. Two major innovations of our article are presented here. At first, RTGE integrates a measure of temporal smoothness in the learning process of the time-aware graph embedding. Via the proposed additional smoothing factor, RTGE can preserve both structural information and evolutionary patterns of a given graph. Secondly, RTGE provides a general task-oriented negative sampling strategy associated with temporally aware information, which further improves the adaptive ability of the proposed algorithm and plays an essential role in obtaining superior performance in various tasks. Extensive experiments conducted on multiple benchmark tasks show that RTGE can increase performance in entity/relationship/temporal scoping prediction tasks.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3208015520",
    "type": "article"
  },
  {
    "title": "Social Group Query Based on Multi-Fuzzy-Constrained Strong Simulation",
    "doi": "https://doi.org/10.1145/3481640",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Guliu Liu; Lei Li; Guanfeng Liu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Traditional social group analysis mostly uses interaction models, event models, or other social network analysis methods to identify and distinguish groups. This type of method can divide social participants into different groups based on their geographic location, social relationships, and/or related events. However, in some applications, it is necessary to make more specific restrictions on the members and the interactions between members of the group. Generally, Graph Pattern Matching (GPM) technique is used to solve this problem. However, the existing GPM methods rarely consider the rich contextual information of nodes and edges to measure the credibility between members. In this article, first, a social group query problem that needs to consider the trust between members of the group is proposed. Then, to solve this problem, a multi-fuzzy-constrained strong simulation matching model is proposed based on multi-constrained simulation, and a Strong Simulation GPM algorithm (NTSS) based on the exploration of pattern Node Topological ordered sequence is proposed. Aiming at the inefficiency of the NTSS algorithm when pattern graph with multiple nodes with zero in-degree and the problem of repeated calculation of matching edges shared by multiple matching subgraphs, two optimization strategies are proposed. Finally, we conduct verification experiments on the effectiveness and efficiency of the NTSS algorithm and the algorithms with the optimization strategies on four social network datasets in real applications. Experimental results show that the NTSS algorithm is significantly better than the existing multi-constrained GPM algorithm, and the NTSS_Inv_EdgC algorithm, which combines two optimization strategies, greatly improves the efficiency of the NTSS algorithm.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3210940033",
    "type": "article"
  },
  {
    "title": "RHPTree—Risk Hierarchical Pattern Tree for Scalable Long Pattern Mining",
    "doi": "https://doi.org/10.1145/3488380",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Danlu Liu; Li Yu; William Baskett; Dan Lin; Chi‐Ren Shyu",
    "corresponding_authors": "",
    "abstract": "Risk patterns are crucial in biomedical research and have served as an important factor in precision health and disease prevention. Despite recent development in parallel and high-performance computing, existing risk pattern mining methods still struggle with problems caused by large-scale datasets, such as redundant candidate generation, inability to discover long significant patterns, and prolonged post pattern filtering. In this article, we propose a novel dynamic tree structure, Risk Hierarchical Pattern Tree (RHPTree), and a top-down search method, RHPSearch, which are capable of efficiently analyzing a large volume of data and overcoming the limitations of previous works. The dynamic nature of the RHPTree avoids costly tree reconstruction for the iterative search process and dataset updates. We also introduce two specialized search methods, the extended target search (RHPSearch-TS) and the parallel search approach (RHPSearch-SD), to further speed up the retrieval of certain items of interest. Experiments on both UCI machine learning datasets and sampled datasets of the Simons Foundation Autism Research Initiative (SFARI)—Simon’s Simplex Collection (SSC) datasets demonstrate that our method is not only faster but also more effective in identifying comprehensive long risk patterns than existing works. Moreover, the proposed new tree structure is generic and applicable to other pattern mining problems.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4205415213",
    "type": "article"
  },
  {
    "title": "Forecasting the Market with Machine Learning Algorithms: An Application of NMC-BERT-LSTM-DQN-X Algorithm in Quantitative Trading",
    "doi": "https://doi.org/10.1145/3488378",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Chang Liu; Jie Yan; Feiyue Guo; Min Guo",
    "corresponding_authors": "",
    "abstract": "Although machine learning (ML) algorithms have been widely used in forecasting the trend of stock market indices, they failed to consider the following crucial aspects for market forecasting: (1) that investors’ emotions and attitudes toward future market trends have material impacts on market trend forecasting (2) the length of past market data should be dynamically adjusted according to the market status and (3) the transition of market statutes should be considered when forecasting market trends. In this study, we proposed an innovative ML method to forecast China's stock market trends by addressing the three issues above. Specifically, sentimental factors (see Appendix [1] for full trans) were first collected to measure investors’ emotions and attitudes. Then, a non-stationary Markov chain (NMC) model was used to capture dynamic transitions of market statutes. We choose the state-of-the-art (SOTA) method, namely, Bidirectional Encoder Representations from Transformers ( BERT ), to predict the state of the market at time t , and a long short-term memory ( LSTM ) model was used to estimate the varying length of past market data in market trend prediction, where the input of LSTM (the state of the market at time t ) was the output of BERT and probabilities for opening and closing of the gates in the LSTM model were based on outputs of the NMC model. Finally, the optimum parameters of the proposed algorithm were calculated using a reinforced learning-based deep Q-Network. Compared to existing forecasting methods, the proposed algorithm achieves better results with a forecasting accuracy of 61.77%, annualized return of 29.25%, and maximum losses of −8.29%. Furthermore, the proposed model achieved the lowest forecasting error: mean square error (0.095), root mean square error (0.0739), mean absolute error (0.104), and mean absolute percent error (15.1%). As a result, the proposed market forecasting model can help investors obtain more accurate market forecast information.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4206454720",
    "type": "article"
  },
  {
    "title": "MCRapper: Monte-Carlo Rademacher Averages for Poset Families and Approximate Pattern Mining",
    "doi": "https://doi.org/10.1145/3532187",
    "publication_date": "2022-04-25",
    "publication_year": 2022,
    "authors": "Leonardo Pellegrina; Cyrus Cousins; Fabio Vandin; Matteo Riondato",
    "corresponding_authors": "",
    "abstract": "“I’m an MC still as honest” – Eminem, Rap God We present MCRapper , an algorithm for efficient computation of Monte-Carlo Empirical Rademacher Averages (MCERA) for families of functions exhibiting poset (e.g., lattice) structure, such as those that arise in many pattern mining tasks. The MCERA allows us to compute upper bounds to the maximum deviation of sample means from their expectations, thus it can be used to find both (1) statistically-significant functions (i.e., patterns) when the available data is seen as a sample from an unknown distribution, and (2) approximations of collections of high-expectation functions (e.g., frequent patterns) when the available data is a small sample from a large dataset. This flexibility offered by MCRapper is a big advantage over previously proposed solutions, which could only achieve one of the two. MCRapper uses upper bounds to the discrepancy of the functions to efficiently explore and prune the search space, a technique borrowed from pattern mining itself. To show the practical use of MCRapper , we employ it to develop an algorithm TFP-R for the task of True Frequent Pattern (TFP) mining, by appropriately computing approximations of the negative and positive borders of the collection of patterns of interest, which allow an effective pruning of the pattern space and the computation of strong bounds to the supremum deviation. TFP-R gives guarantees on the probability of including any false positives (precision) and exhibits higher statistical power (recall) than existing methods offering the same guarantees. We evaluate MCRapper and TFP-R and show that they outperform the state-of-the-art for their respective tasks.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4224691544",
    "type": "article"
  },
  {
    "title": "A Self-Supervised Representation Learning of Sentence Structure for Authorship Attribution",
    "doi": "https://doi.org/10.1145/3491203",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Fereshteh Jafariakinabad; Kien A. Hua",
    "corresponding_authors": "",
    "abstract": "The syntactic structure of sentences in a document substantially informs about its authorial writing style. Sentence representation learning has been widely explored in recent years and it has been shown that it improves the generalization of different downstream tasks across many domains. Even though utilizing probing methods in several studies suggests that these learned contextual representations implicitly encode some amount of syntax, explicit syntactic information further improves the performance of deep neural models in the domain of authorship attribution. These observations have motivated us to investigate the explicit representation learning of syntactic structure of sentences. In this article, we propose a self-supervised framework for learning structural representations of sentences. The self-supervised network contains two components; a lexical sub-network and a syntactic sub-network which take the sequence of words and their corresponding structural labels as the input, respectively. Due to the n -to-1 mapping of words to their structural labels, each word will be embedded into a vector representation which mainly carries structural information. We evaluate the learned structural representations of sentences using different probing tasks, and subsequently utilize them in the authorship attribution task. Our experimental results indicate that the structural embeddings significantly improve the classification tasks when concatenated with the existing pre-trained word embeddings.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4225788991",
    "type": "article"
  },
  {
    "title": "Crowdsourcing Truth Inference Based on Label Confidence Clustering",
    "doi": "https://doi.org/10.1145/3556545",
    "publication_date": "2022-08-17",
    "publication_year": 2022,
    "authors": "Gongqing Wu; Liangzhu Zhou; Jiazhu Xia; Lei Li; Xianyu Bao; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Truth inference can help solve some difficult problems of data integration in crowdsourcing. Crowdsourced workers are not experts and their labeling ability varies greatly; therefore, in practical applications, it is difficult to determine whether the labels collected from a crowdsourcing platform are correct. This article proposes a novel algorithm called truth inference based on label confidence clustering (TILCC) to improve the quality of integrated labels for the single-choice classification problem in crowdsourcing labeling tasks. We obtain the label confidence via worker reliability, which is calculated from multiple noise labels using a truth discovery method, and then we generate the clustering features and use the K-means algorithm to cluster all the tasks into K different clusters. Each cluster corresponds to a specific class, and the tasks in the cluster are assigned a label. Compared with the performances of six state-of-the-art methods, MV, ZenCrowd, PM, CATD, GLAD, and GTIC, on 12 randomly selected real-world datasets, the performance of our algorithm showed many advantages: no need to set complex parameters, faster running speed, and significantly higher accuracy.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4292092773",
    "type": "article"
  },
  {
    "title": "Adaptive Collaborative Soft Label Learning for Unsupervised Multi-View Feature Selection",
    "doi": "https://doi.org/10.1145/3591467",
    "publication_date": "2023-04-10",
    "publication_year": 2023,
    "authors": "Dan Shi; Lei Zhu; Xiao Dong; Xuemeng Song; Jingjing Li; Zhiyong Cheng",
    "corresponding_authors": "",
    "abstract": "Unsupervised multi-view feature selection aims to select informative features with multi-view features and unsupervised learning. It is a challenging problem due to the absence of explicit semantic supervision. Recently, graph theory and hard pseudo-label learning have been adopted to solve multi-view feature selection problems under the unsupervised learning paradigm. However, graph-based methods are difficult to support large-scale real scenarios due to the high computational complexity of graph construction. Moreover, existing methods based on hard pseudo-label learning generally result in significant information loss. In this article, we propose an Adaptive Collaborative Soft Label Learning (ACSLL) model for unsupervised multi-view feature selection. In this model, collaborative soft label learning and multi-view feature selection are integrated into a unified framework. Specifically, we learn the pseudo soft labels from each view feature by a simple and efficient method and fuse them with an adaptive weighting strategy into a joint soft label matrix. This matrix is further used for guiding the feature selection process to identify valuable features. An effective optimization strategy guaranteed with proven convergence is derived to iteratively solve this problem. Experiments demonstrate the superiority of the proposed method in both feature selection accuracy and efficiency.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4363650390",
    "type": "article"
  },
  {
    "title": "HGV4Risk: Hierarchical Global View-guided Sequence Representation Learning for Risk Prediction",
    "doi": "https://doi.org/10.1145/3605895",
    "publication_date": "2023-06-22",
    "publication_year": 2023,
    "authors": "Youru Li; Zhenfeng Zhu; Xiaobo Guo; Shaoshuai Li; Yuchen Yang; Yao Zhao",
    "corresponding_authors": "",
    "abstract": "Risk prediction, usually achieved by learning representations from patient’s physiological sequence or user’s behavioral sequence data, and has been widely applied in healthcare and finance. Despite that, some recent time-aware deep learning methods have led to superior performances in such sequence representation learning tasks, such improvement is limited due to a lack of guidance from hierarchical global view. To address this issue, we propose a novel end-to-end H ierarchical G lobal V iew-guided (HGV) sequence representation learning framework. Specifically, the Global Graph Embedding (GGE) module is proposed to learn sequential clip-aware representations from temporal correlation graph (TCG) at instance level. Furthermore, following the way of key-query attention, the harmonic β-attention (β-Attn) is also developed for making a global tradeoff between time-aware decay and observation significance at channel level adaptively. Moreover, the hierarchical representations at both instance level and channel level can be coordinated by the heterogeneous information aggregation under the guidance of global view. Experimental results on both healthcare risk prediction benchmark and SMEs credit overdue risk prediction task from the real-world industrial scenario in MYBank, Ant Group, have illustrated that the proposed model can achieve competitive prediction performance compared with other known baselines. The code has been released public available at: https://github.com/LiYouru0228/HGV.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4381612816",
    "type": "article"
  },
  {
    "title": "Exploiting Conversation-Branch-Tweet HyperGraph Structure to Detect Misinformation on Social Media",
    "doi": "https://doi.org/10.1145/3610297",
    "publication_date": "2023-07-28",
    "publication_year": 2023,
    "authors": "Fangfang Li; Zhi Liu; Junwen Duan; Xingliang Mao; Heyuan Shi; Shichao Zhang",
    "corresponding_authors": "",
    "abstract": "The spread of misinformation on social media is a serious issue that can have negative consequences for public health and political stability. While detecting and identifying misinformation can be challenging, many attempts have been made to address this problem. However, traditional models that focus on pairwise relationships on misinformation propagation paths may not be effective in capturing the underlying connections among multiple tweets. To address this limitation, the proposed “Conversation-Branch-Tweet” hypergraph convolutional network (CBT-HGCN) uses a hypergraph to represent the internal structure and content of tweet data, with tweets and their replies viewed as nodes and hyperedges, respectively. The model first pre-processes the tweets of a conversation and then uses a pre-trained model as an encoder to extract node information. Finally, a hypergraph convolution network is used as an information fuser for classification. Experimental results on three benchmark datasets (Twitter15, Twitter16, and Pheme) show that the proposed model outperforms several strong baseline models and achieves state-of-the-art performance. This indicates that the CBT-HGCN approach is effective in detecting and identifying misinformation on social media by capturing the underlying connections among multiple tweets.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4385342710",
    "type": "article"
  },
  {
    "title": "Laplacian-based Cluster-Contractive t-SNE for High-Dimensional Data Visualization",
    "doi": "https://doi.org/10.1145/3612932",
    "publication_date": "2023-08-03",
    "publication_year": 2023,
    "authors": "Yan Sun; Yi Han; Jicong Fan",
    "corresponding_authors": "",
    "abstract": "Dimensionality reduction techniques aim at representing high-dimensional data in low-dimensional spaces to extract hidden and useful information or facilitate visual understanding and interpretation of the data. However, few of them take into consideration the potential cluster information contained implicitly in the high-dimensional data. In this article, we propose Lap tSNE, a new graph-layout nonlinear dimensionality reduction method based on t-SNE, one of the best techniques for visualizing high-dimensional data as 2D scatter plots. Specifically, Lap tSNE leverages the eigenvalue information of the graph Laplacian to shrink the potential clusters in the low-dimensional embedding when learning to preserve the local and global structure from high-dimensional space to low-dimensional space. It is nontrivial to solve the proposed model because the eigenvalues of normalized symmetric Laplacian are functions of the decision variable. We provide a majorization-minimization algorithm with convergence guarantee to solve the optimization problem of Lap tSNE and show how to calculate the gradient analytically, which may be of broad interest when considering optimization with Laplacian-composited objective. We evaluate our method by a formal comparison with state-of-the-art methods on seven benchmark datasets, both visually and via established quantitative measurements. The results demonstrate the superiority of our method over baselines such as t-SNE and UMAP. We also provide out-of-sample extension, large-scale extension, and mini-batch extension for our Lap tSNE to facilitate dimensionality reduction in various scenarios.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4385548936",
    "type": "article"
  },
  {
    "title": "Laplacian Change Point Detection for Single and Multi-view Dynamic Graphs",
    "doi": "https://doi.org/10.1145/3631609",
    "publication_date": "2023-11-06",
    "publication_year": 2023,
    "authors": "Shenyang Huang; Samy Coulombe; Yasmeen Hitti; Reihaneh Rabbany; Guillaume Rabusseau",
    "corresponding_authors": "",
    "abstract": "Dynamic graphs are rich data structures that are used to model complex relationships between entities over time. In particular, anomaly detection in temporal graphs is crucial for many real-world applications such as intrusion identification in network systems, detection of ecosystem disturbances, and detection of epidemic outbreaks. In this article, we focus on change point detection in dynamic graphs and address three main challenges associated with this problem: (i) how to compare graph snapshots across time, (ii) how to capture temporal dependencies, and (iii) how to combine different views of a temporal graph. To solve the above challenges, we first propose Laplacian Anomaly Detection (LAD) which uses the spectrum of graph Laplacian as the low dimensional embedding of the graph structure at each snapshot. LAD explicitly models short-term and long-term dependencies by applying two sliding windows. Next, we propose MultiLAD, a simple and effective generalization of LAD to multi-view graphs. MultiLAD provides the first change point detection method for multi-view dynamic graphs. It aggregates the singular values of the normalized graph Laplacian from different views through the scalar power mean operation. Through extensive synthetic experiments, we show that (i) LAD and MultiLAD are accurate and outperforms state-of-the-art baselines and their multi-view extensions by a large margin, (ii) MultiLAD’s advantage over contenders significantly increases when additional views are available, and (iii) MultiLAD is highly robust to noise from individual views. In five real-world dynamic graphs, we demonstrate that LAD and MultiLAD identify significant events as top anomalies such as the implementation of government COVID-19 interventions which impacted the population mobility in multi-view traffic networks.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4388421529",
    "type": "article"
  },
  {
    "title": "Trajectory-User Linking via Hierarchical Spatio-Temporal Attention Networks",
    "doi": "https://doi.org/10.1145/3635718",
    "publication_date": "2023-12-04",
    "publication_year": 2023,
    "authors": "Wei Chen; Chao Huang; Yanwei Yu; Yongguo Jiang; Junyu Dong",
    "corresponding_authors": "",
    "abstract": "Trajectory-User Linking (TUL) is crucial for human mobility modeling by linking different trajectories to users with the exploration of complex mobility patterns. Existing works mainly rely on the recurrent neural framework to encode the temporal dependencies in trajectories, have fall short in capturing spatial-temporal global context for TUL prediction. To fill this gap, this work presents a new hierarchical spatio-temporal attention neural network, called AttnTUL , to jointly encode the local trajectory transitional patterns and global spatial dependencies for TUL. Specifically, our first model component is built over the graph neural architecture to preserve the local and global context and enhance the representation paradigm of geographical regions and user trajectories. Additionally, a hierarchically structured attention network is designed to simultaneously encode the intra-trajectory and inter-trajectory dependencies, with the integration of the temporal attention mechanism and global elastic attentional encoder. Extensive experiments demonstrate the superiority of our AttnTUL method as compared to state-of-the-art baselines on various trajectory datasets. The source code of our model is available at https://github.com/Onedean/AttnTUL .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4389307115",
    "type": "article"
  },
  {
    "title": "Personalized Federated Learning with Layer-Wise Feature Transformation via Meta-Learning",
    "doi": "https://doi.org/10.1145/3638252",
    "publication_date": "2023-12-28",
    "publication_year": 2023,
    "authors": "Jingke Tu; Jiaming Huang; Lei Yang; Wanyu Lin",
    "corresponding_authors": "",
    "abstract": "Federated learning enables multiple clients to collaboratively learn machine learning models in a privacy-preserving manner. However, in real-world scenarios, a key challenge encountered in federated learning is the statistical heterogeneity among clients. Existing work mainly focused on a single global model shared across the clients, making it hard to generalize well to all clients due to the large discrepancy in the data distributions. To address this challenge, we propose pFedLT , a novel approach that can adapt the single global model to different data distributions. Specifically, we propose to perform a pluggable layer-wise transformation during the local update phase based on scaling and shifting operations. In particular, these operations are learned with a meta-learning strategy. By doing so, pFedLT can capture the diversity of data distribution among clients, therefore, can generalize well even when the data distributions among clients exhibit high statistical heterogeneity. We conduct extensive experiments on synthetic and real-world datasets (MNIST, Fashion_MNIST, CIFAR-10, and Office+Caltech10) under different Non-IID settings. Experimental results demonstrate that pFedLT significantly improves the model accuracy by up to 11.67% and reduces the communication costs compared with state-of-the-art approaches.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4390317556",
    "type": "article"
  },
  {
    "title": "Diverse Structure-Aware Relation Representation in Cross-Lingual Entity Alignment",
    "doi": "https://doi.org/10.1145/3638778",
    "publication_date": "2023-12-29",
    "publication_year": 2023,
    "authors": "Yuhong Zhang; Jianqing Wu; Kui Yu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Cross-lingual entity alignment (CLEA) aims to find equivalent entity pairs between knowledge graphs (KGs) in different languages. It is an important way to connect heterogeneous KGs and facilitate knowledge completion. Existing methods have found that incorporating relations into entities can effectively improve KG representation and benefit entity alignment, and these methods learn relation representation depending on entities, which cannot capture the diverse structures of relations. However, multiple relations in KG form diverse structures, such as adjacency structure and ring structure. This diversity of relation structures makes the relation representation challenging. Therefore, we propose to construct the weighted line graphs to model the diverse structures of relations and learn relation representation independently from entities. Especially, owing to the diversity of adjacency structures and ring structures, we propose to construct adjacency line graph and ring line graph, respectively, to model the structures of relations and to further improve entity representation. In addition, to alleviate the hubness problem in alignment, we introduce the optimal transport into alignment and compute the distance matrix in a different way. From a global perspective, we calculate the optimal 1-to-1 alignment bi-directionally to improve the alignment accuracy. Experimental results on two benchmark datasets show that our proposed method significantly outperforms state-of-the-art CLEA methods in both supervised and unsupervised manners.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4390393145",
    "type": "article"
  },
  {
    "title": "ID-SR: Privacy-Preserving Social Recommendation Based on Infinite Divisibility for Trustworthy AI",
    "doi": "https://doi.org/10.1145/3639412",
    "publication_date": "2024-01-02",
    "publication_year": 2024,
    "authors": "Jingyi Cui; Guangquan Xu; Jian Liu; Shicheng Feng; Jianli Wang; Hao Peng; Shihui Fu; Zhaohua Zheng; Xi Zheng; Shaoying Liu",
    "corresponding_authors": "",
    "abstract": "Recommendation systems powered by artificial intelligence (AI) are widely used to improve user experience. However, AI inevitably raises privacy leakage and other security issues due to the utilization of extensive user data. Addressing these challenges can protect users’ personal information, benefit service providers, and foster service ecosystems. Presently, numerous techniques based on differential privacy have been proposed to solve this problem. However, existing solutions encounter issues such as inadequate data utilization and a tenuous trade-off between privacy protection and recommendation effectiveness. To enhance recommendation accuracy and protect users’ private data, we propose ID-SR, a novel privacy-preserving social recommendation scheme for trustworthy AI based on the infinite divisibility of Laplace distribution. We first introduce a novel recommendation method adopted in ID-SR, which is established based on matrix factorization with a newly designed social regularization term for improving recommendation effectiveness. We then propose a differential privacy-preserving scheme tailored to the above method that leverages the Laplace distribution’s characteristics to safeguard user data. Theoretical analysis and experimentation evaluation on two publicly available datasets demonstrate that our scheme achieves a superior balance between privacy protection and recommendation effectiveness, ultimately delivering an enhanced user experience.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390490868",
    "type": "article"
  },
  {
    "title": "Interpreting Deep Forest through Feature Contribution and MDI Feature Importance",
    "doi": "https://doi.org/10.1145/3641108",
    "publication_date": "2024-01-19",
    "publication_year": 2024,
    "authors": "Yi-Xiao He; Shen-Huan Lyu; Yuan Jiang",
    "corresponding_authors": "",
    "abstract": "Deep forest is a non-differentiable deep model that has achieved impressive empirical success across a wide variety of applications, especially on categorical/symbolic or mixed modeling tasks. Many of the application fields prefer explainable models, such as random forests with feature contributions that can provide a local explanation for each prediction, and Mean Decrease Impurity (MDI) that can provide global feature importance. However, deep forest, as a cascade of random forests, possesses interpretability only at the first layer. From the second layer on, many of the tree splits occur on the new features generated by the previous layer, which makes existing explaining tools for random forests inapplicable. To disclose the impact of the original features in the deep layers, we design a calculation method with an estimation step followed by a calibration step for each layer, and propose our feature contribution and MDI feature importance calculation tools for deep forest. Experimental results on both simulated data and real-world data verify the effectiveness of our methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391026313",
    "type": "article"
  },
  {
    "title": "Package Arrival Time Prediction via Knowledge Distillation Graph Neural Network",
    "doi": "https://doi.org/10.1145/3643033",
    "publication_date": "2024-01-24",
    "publication_year": 2024,
    "authors": "Lei Zhang; Yong Liu; Zhiwei Zeng; Yiming Cao; Xingyu Wu; Yonghui Xu; Zhiqi Shen; Lizhen Cui",
    "corresponding_authors": "",
    "abstract": "Accurately estimating packages’ arrival time in e-commerce can enhance users’ shopping experience and improve the placement rate of products. This problem is often formalized as an Origin-Destination (OD)-based ETA (i.e., estimated time of arrival) prediction task, where the delivery time is estimated mainly based on sender and receiver addresses and other context information. One inherent challenge of the OD-based ETA problem is that the delivery time highly depends on the actual delivery trajectory which is unknown at the time of prediction. In this article, we tackle this challenge by effectively exploiting historical delivery trajectories. We propose a novel Knowledge Distillation Graph neural network-based package ETA prediction (KDG-ETA) model, which uses knowledge distillation in the training phase to distill the knowledge of historical trajectories into OD pair embeddings. In KDG-ETA, a multi-level trajectory graph representation model is proposed to fully exploit trajectory information at the node-level, edge-level, and path-level. Then, the OD representations embedded with trajectory knowledge are combined with context embeddings from feature extraction module for delivery time prediction using an adaptive attention module. KDG-ETA consistently outperforms existing state-of-the-art OD-based ETA prediction methods on three real-world Alibaba datasets, reducing the Mean Absolute Error (MAE) by 3.0%–39.1% as demonstrated in our extensive empirical evaluation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391169496",
    "type": "article"
  },
  {
    "title": "Multi-Scenario and Multi-Task Aware Feature Interaction for Recommendation System",
    "doi": "https://doi.org/10.1145/3651312",
    "publication_date": "2024-03-06",
    "publication_year": 2024,
    "authors": "Derun Song; Enneng Yang; Guibing Guo; Li Shen; Linying Jiang; Xingwei Wang",
    "corresponding_authors": "",
    "abstract": "Multi-scenario and multi-task recommendation can use various feedback behaviors of users in different scenarios to learn users’ preferences and then make recommendations, which has attracted attention. However, the existing work ignores feature interactions and the fact that a pair of feature interactions will have differing levels of importance under different scenario-task pairs, leading to sub-optimal user preference learning. In this article, we propose a M ulti-scenario and M ulti-task aware F eature I nteraction model, dubbed MMFI , to explicitly model feature interactions and learn the importance of feature interaction pairs in different scenarios and tasks. Specifically, MMFI first incorporates a pairwise feature interaction unit and a scenario-task interaction unit to effectively capture the interaction of feature pairs and scenario-task pairs. Then MMFI designs a scenario-task aware attention layer for learning the importance of feature interactions from coarse-grained to fine-grained, improving the model’s performance on various scenario-task pairs. More specifically, this attention layer consists of three modules: a fully shared bottom module, a partially shared middle module, and a specific output module. Finally, MMFI adapts two sparsity-aware functions to remove some useless feature interactions. Extensive experiments on two public datasets demonstrate the superiority of the proposed method over the existing multi-task recommendation, multi-scenario recommendation, and multi-scenario &amp; multi-task recommendation models.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392499463",
    "type": "article"
  },
  {
    "title": "DeepMeshCity: A Deep Learning Model for Urban Grid Prediction",
    "doi": "https://doi.org/10.1145/3652859",
    "publication_date": "2024-03-15",
    "publication_year": 2024,
    "authors": "Chi Zhang; L. Cai; Meng Chen; Xiucheng Li; Gao Cong",
    "corresponding_authors": "",
    "abstract": "Urban grid prediction can be applied to many classic spatial-temporal prediction tasks such as air quality prediction, crowd density prediction, and traffic flow prediction, which is of great importance to smart city building. In light of its practical values, many methods have been developed for it and have achieved promising results. Despite their successes, two main challenges remain open: (a) how to well capture the global dependencies and (b) how to effectively model the multi-scale spatial-temporal correlations? To address these two challenges, we propose a novel method— DeepMeshCity , with a carefully-designed Self-Attention Citywide Grid Learner ( SA-CGL ) block comprising of a self-attention unit and a Citywide Grid Learner ( CGL ) unit. The self-attention block aims to capture the global spatial dependencies, and the CGL unit is responsible for learning the spatial-temporal correlations. In particular, a multi-scale memory unit is proposed to traverse all stacked SA-CGL blocks along a zigzag path to capture the multi-scale spatial-temporal correlations. In addition, we propose to initialize the single-scale memory units and the multi-scale memory units by using the corresponding ones in the previous fragment stack, so as to speed up the model training. We evaluate the performance of our proposed model by comparing with several state-of-the-art methods on four real-world datasets for two urban grid prediction applications. The experimental results verify the superiority of DeepMeshCity over the existing ones. The code is available at https://github.com/ILoveStudying/DeepMeshCity.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392849759",
    "type": "article"
  },
  {
    "title": "Representative and Back-In-Time Sampling from Real-world Hypergraphs",
    "doi": "https://doi.org/10.1145/3653306",
    "publication_date": "2024-03-19",
    "publication_year": 2024,
    "authors": "Minyoung Choe; Jaemin Yoo; Geon Lee; Woonsung Baek; U Kang; Kijung Shin",
    "corresponding_authors": "",
    "abstract": "Graphs are widely used for representing pairwise interactions in complex systems. Since such real-world graphs are large and often evergrowing, sampling subgraphs is useful for various purposes, including simulation, visualization, stream processing, representation learning, and crawling. However, many complex systems consist of group interactions (e.g., collaborations of researchers and discussions on online Q&amp;A platforms) and thus are represented more naturally and accurately by hypergraphs than by ordinary graphs. Motivated by the prevalence of large-scale hypergraphs, we study the problem of sampling from real-world hypergraphs, aiming at answering (Q1) how can we measure the goodness of sub-hypergraphs, and (Q2) how can we efficiently find a “good” sub-hypergraph. Regarding Q1, we distinguish between two goals: (a) representative sampling , which aims at capturing the characteristics of the input hypergraph, and (b) back-in-time sampling , which aims at closely approximating a past snapshot of the input time-evolving hypergraph. To evaluate the similarity of the sampled sub-hypergraph to the target (i.e., the input hypergraph or its past snapshot), we consider 10 graph-level, hyperedge-level, and node-level statistics. Regarding Q2, we first conduct a thorough analysis of various intuitive approaches using 11 real-world hypergraphs. Then, based on this analysis, we propose MiDaS and MiDaS-B , designed for representative sampling and back-in-time sampling, respectively. Regarding representative sampling, we demonstrate through extensive experiments that MiDaS , which employs a sampling bias toward high-degree nodes in hyperedge selection, is (a) Representative : finding overall the most representative samples among 15 considered approaches, (b) Fast : several orders of magnitude faster than the strongest competitors, and (c) Automatic : automatically tuning the degree of sampling bias. Regarding back-in-time sampling, we demonstrate that MiDaS-B inherits the strengths of MiDaS despite an additional challenge—the unavailability of the target (i.e., past snapshot). It effectively handles this challenge by focusing on replicating universal evolutionary patterns, rather than directly replicating the target.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392956330",
    "type": "article"
  },
  {
    "title": "SA2E-AD: A Stacked Attention Autoencoder for Anomaly Detection in Multivariate Time Series",
    "doi": "https://doi.org/10.1145/3653677",
    "publication_date": "2024-03-26",
    "publication_year": 2024,
    "authors": "Mengyao Li; Zhiyong Li; Zhibang Yang; Xu Zhou; Yifan Li; Ziyan Wu; Lingzhao Kong; Ke Nai",
    "corresponding_authors": "",
    "abstract": "Anomaly detection for multivariate time series is an essential task in the modern industrial field. Although several methods have been developed for anomaly detection, they usually fail to effectively exploit the metrical-temporal correlation and the other dependencies among multiple variables. To address this problem, we propose a stacked attention autoencoder for anomaly detection in multivariate time series (SA2E-AD); it focuses on fully utilizing the metrical and temporal relationships among multivariate time series. We design a multiattention block, alternately containing the temporal attention and metrical attention components in a hierarchical structure to better reconstruct normal time series, which is helpful in distinguishing the anomalies from the normal time series. Meanwhile, a two-stage training strategy is designed to further separate the anomalies from the normal data. Experiments on three publicly available datasets show that SA2E-AD outperforms the advanced baseline methods in detection performance and demonstrate the effectiveness of each part of the process in our method.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4393191768",
    "type": "article"
  },
  {
    "title": "Variate Associated Domain Adaptation for Unsupervised Multivariate Time Series Anomaly Detection",
    "doi": "https://doi.org/10.1145/3663573",
    "publication_date": "2024-05-03",
    "publication_year": 2024,
    "authors": "Yifan He; Yatao Bian; Xi Ding; Bingzhe Wu; Jihong Guan; Ji Zhang; Shuigeng Zhou",
    "corresponding_authors": "",
    "abstract": "Multivariate Time Series Anomaly Detection (MTS-AD) is crucial for the effective management and maintenance of devices in complex systems, such as server clusters, spacecrafts, and financial systems, and so on. However, upgrade or cross-platform deployment of these devices will introduce the issue of cross-domain distribution shift, which leads to the prototypical problem of domain adaptation for MTS-AD. Compared with general domain adaptation problems, MTS-AD domain adaptation presents two peculiar challenges: (1) the dimensions of data from the source domain and the target domain are usually different, so alignment without losing any information is necessary; and (2) the association between different variates plays a vital role in the MTS-AD task, which is overlooked by traditional domain adaptation approaches. Aiming at addressing the above issues, we propose a Variate Associated Domain Adaptation Method Combined with a Graph Deviation Network (VANDA) for MTS-AD, which includes two major contributions. First, we characterize the intra-domain variate associations of the source domain by a graph deviation network (GDN), which can share parameters across domains without dimension alignment. Second, we propose a sliding similarity to measure the inter-domain variate associations and perform joint training by minimizing the optimal transport distance between source and target data for transferring variate associations across domains. VANDA achieves domain adaptation by transferring both variate associations and GDN parameters from the source domain to the target domain. We construct two pairs of MTS-AD datasets from existing MTS-AD data and combine three domain adaptation strategies with six MTS-AD backbones as the benchmark methods for experimental evaluation and comparison. Extensive experiments demonstrate the effectiveness of our approach, which outperforms the benchmark methods, and significantly improves the AD performance of the target domain by effectively utilizing the source domain knowledge.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4396609011",
    "type": "article"
  },
  {
    "title": "Data Completion-Guided Unified Graph Learning for Incomplete Multi-View Clustering",
    "doi": "https://doi.org/10.1145/3664290",
    "publication_date": "2024-05-09",
    "publication_year": 2024,
    "authors": "Tianhai Liang; Qiangqiang Shen; Shuqin Wang; Yongyong Chen; Guokai Zhang; Junxin Chen",
    "corresponding_authors": "",
    "abstract": "Due to its heterogeneous property, multi-view data has been widely concerned over single-view data for performance improvement. Unfortunately, some instances may be with partially available information because of some uncontrollable factors, for which the incomplete multi-view clustering (IMVC) problem is raised. IMVC aims to partition unlabeled incomplete multi-view data into their clusters by exploiting the heterogeneity of multi-view data and overcoming the difficulty of data loss. However, most existing IMVC methods like BSV, MIC, OMVC, and IVC tend to conduct basic completion processing on the input data, without taking advantage of the correlation between samples and information redundancy. To overcome the above issue, we propose one novel IMVC method named data completion-guided unified graph learning (DCUGL), which could complete the data of missing views and fuse multiple learned view-specific similarity matrices into one unified graph. Specifically, we first reduce the dimension of the input data to learn multiple view-specific similarity matrices. By stacking all view-specific similarity matrices, DCUGL constructs a third-order tensor with the low-rank constraint, such that sample correlation within and between views can be well explored. Finally, by dividing the original data into observed data and unobserved data, DCUGL can infer and complete the missing data according to the view-specific similarity matrices, and obtain a unified graph, which can be directly used for clustering. To solve the proposed model, we design an iterative algorithm, which is based on the alternating direction method of multipliers framework. The proposed model proves to be superior by benchmarking on six challenging datasets compared with state-of-the-art IMVC methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4396759876",
    "type": "article"
  },
  {
    "title": "Imbalance-Robust Multi-Label Self-Adjusting kNN",
    "doi": "https://doi.org/10.1145/3663575",
    "publication_date": "2024-05-11",
    "publication_year": 2024,
    "authors": "Victor Nicola; Karina Valdivia Delgado; Marcelo de Souza Lauretto",
    "corresponding_authors": "",
    "abstract": "In the task of multi-label classification in data streams, instances arriving in real-time need to be associated with multiple labels simultaneously. Various methods based on the k Nearest Neighbors algorithm have been proposed to address this task. However, these methods face limitations when dealing with imbalanced data streams, a problem that has received limited attention in existing works. To approach this gap, this article introduces the Imbalance-Robust Multi-Label Self-Adjusting kNN (IRMLSAkNN), designed to tackle multi-label imbalanced data streams. IRMLSAkNN’s strength relies on maintaining relevant instances with imbalance labels by using a discarding mechanism that considers the imbalance ratio per label. On the other hand, it evaluates subwindows with an imbalance-aware measure to discard older instances that are lacking performance. We conducted statistical experiments on 32 benchmark data streams, evaluating IRMLSAkNN against eight multi-label classification algorithms using common accuracy-aware and imbalance-aware measures. The obtained results demonstrate that IRMLSAkNN consistently outperforms these algorithms in terms of predictive capacity and time cost across various levels of imbalance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4396826967",
    "type": "article"
  },
  {
    "title": "Structure-Information-Based Reasoning over the Knowledge Graph: A Survey of Methods and Applications",
    "doi": "https://doi.org/10.1145/3671148",
    "publication_date": "2024-06-06",
    "publication_year": 2024,
    "authors": "Siyuan Meng; Jie Zhou; Xuxin Chen; Yufei Liu; Fengyuan Lu; Xinli Huang",
    "corresponding_authors": "",
    "abstract": "The knowledge graph (KG) is an efficient form of knowledge organization and expression, providing prior knowledge support for various downstream tasks, and has received extensive attention in natural language processing. However, existing large-scale KGs have many hidden facts that need to be discovered. How to effectively use the structure information of KG is an important research direction of knowledge reasoning. Structure-Information-based reasoning over the KG is a technique used to find the missing facts by the structure information of KG. This survey summarizes the methods and applications of Structure-Information-based reasoning and hopes to be helpful to the research in this field. First, we introduced the definition of knowledge reasoning and the conceptual description of related tasks. Then, we reviewed the methods of Structure-Information-based reasoning. Specifically, we categorized them into four representative classes: PRA-based reasoning, Path-Embedding-based reasoning, RL-based reasoning, and GNN-based reasoning. We compared the motivations and details between practices in the same category. After that, we described the application of Structure-Information-based knowledge reasoning in the KG Completion, Question Answering System, Recommendation System, and other fields. Finally, we discussed the future research directions of Structure-Information-based reasoning.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4399382092",
    "type": "article"
  },
  {
    "title": "Heterogeneous Meta-Path Graph Learning for Higher-Order Social Recommendation",
    "doi": "https://doi.org/10.1145/3673658",
    "publication_date": "2024-06-15",
    "publication_year": 2024,
    "authors": "Munan Li; Kai Liu; Hongbo Liu; Zheng Zhao; Tomás Ward; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Recommendation systems have become an indispensable part of daily life. Social recommendation systems, which utilize social relationships and past behaviors to infer users’ preferences, have gained popularity in recent years. Exploring the inherent characteristics implied by higher-order relationships offers a new approach to social recommendation. However, it is challenging due to sparse social networks, influence heterogeneity, and noisy feedback. In this article, we propose a Heterogeneous Meta-path Graph Learning model for Higher-order Social Recommendation (HEAL). Within HEAL, we introduce a heterogeneous graph in social recommendation and utilize a meta-path-guided random walk to generate higher-order relationships. By encoding higher-order structures and semantics along different meta-graphs, HEAL can mitigate the limitation of data sparsity. Moreover, HEAL exploits aspect-aware and semantic-aware attentions to adaptively propagate and aggregate useful features from different meta-neighbors and higher-order relations. These attention-based aggregation layers allow HEAL to suppress the heterogeneity of social influences. Furthermore, HEAL adopts contrastive learning as a supplemental task to the recommendation task by maximizing the consistency between the self-discriminating objectives. This auxiliary task enables the model to learn more differentiated representations, further reducing its sensitivity to noisy feedback. We evaluate the performance of HEAL through extensive experiments on public datasets. The results demonstrate that leveraging higher-order relations can enhance the quality of social recommendations by better capturing the complexity and diversity of users’ preferences and interactions.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4399702413",
    "type": "article"
  },
  {
    "title": "Learning Individual Treatment Effects under Heterogeneous Interference in Networks",
    "doi": "https://doi.org/10.1145/3673761",
    "publication_date": "2024-06-19",
    "publication_year": 2024,
    "authors": "Ziyu Zhao; Yuqi Bai; Ruoxuan Xiong; Qingyu Cao; Chao Ma; Ning Jiang; Fei Wu; Kun Kuang",
    "corresponding_authors": "",
    "abstract": "Estimating individual treatment effects in networked observational data is a crucial and increasingly recognized problem. One major challenge of this problem is violating the stable unit treatment value assumption (SUTVA), which posits that a unit’s outcome is independent of others’ treatment assignments. However, in network data, a unit’s outcome is influenced not only by its treatment (i.e., direct effect) but also by the treatments of others (i.e., spillover effect) since the presence of interference. Moreover, the interference from other units is always heterogeneous (e.g., friends with similar interests have a different influence than those with different interests). In this article, we focus on the problem of estimating individual treatment effects (including direct effect and spillover effect) under heterogeneous interference in networks. To address this problem, we propose a novel dual weighting regression (DWR) algorithm by simultaneously learning attention weights to capture the heterogeneous interference from neighbors and sample weights to eliminate the complex confounding bias in networks. We formulate the learning process as a bi-level optimization problem. Theoretically, we give a generalization error bound for the expected estimation error of the individual treatment effects. Extensive experiments on four benchmark datasets demonstrate that the proposed DWR algorithm outperforms the state-of-the-art methods in estimating individual treatment effects under heterogeneous network interference.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4399815117",
    "type": "article"
  },
  {
    "title": "Subspace-Contrastive Multi-View Clustering",
    "doi": "https://doi.org/10.1145/3674839",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Lele Fu; Sheng Huang; Lei Zhang; Jing‐Hua Yang; Zibin Zheng; Chuanfu Zhang; Chuan Chen",
    "corresponding_authors": "",
    "abstract": "Most multi-view clustering methods based on shallow models are limited in sound nonlinear information perception capability, or fail to effectively exploit complementary information hidden in different views. To tackle these issues, we propose a novel Subspace-Contrastive Multi-View Clustering (SCMC) approach. Specifically, SCMC utilizes a set of view-specific auto-encoders to map the original multi-view data into compact features capturing its nonlinear structures. Considering the large semantic gap of data from different modalities, we project multiple heterogeneous features into a joint semantic space, namely the embedded compact features are passed through the self-expression layers to learn the subspace representations, respectively. In order to enhance the discriminability and efficiently excavate the complementarity of various subspace representations, we use the contrastive strategy to maximize the similarity between positive pairs while differentiate negative pairs. Thus, the graph regularization is employed to encode the local geometric structure within varying subspaces for optimizing the consistent affinity matrix. Furthermore, to endow the proposed SCMC with the ability of handling the multi-view out-of-samples, we develop a consistent sparse representation (CSR) learning mechanism over the in-samples. To demonstrate the effectiveness of the proposed model, we conduct a large number of comparative experiments on ten challenging datasets, and the experimental results show that SCMC outperforms existing shallow and deep multi-view clustering methods. In addition, the experimental results on out-of-samples illustrate the effectiveness of the proposed CSR.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400112754",
    "type": "article"
  },
  {
    "title": "Billiards Sports Analytics: Datasets and Tasks",
    "doi": "https://doi.org/10.1145/3686804",
    "publication_date": "2024-08-06",
    "publication_year": 2024,
    "authors": "Qianru Zhang; Zheng Wang; Cheng Long; Siu Ming Yiu",
    "corresponding_authors": "",
    "abstract": "Nowadays, it becomes a common practice to capture some data of sports games with devices such as GPS sensors and cameras and then use the data to perform various analyses on sports games, including tactics discovery, similar game retrieval, performance study, and so forth. While this practice has been conducted to many sports such as basketball and soccer, it remains largely unexplored on the billiards sports, which is mainly due to the lack of publicly available datasets. Motivated by this, we collect a dataset of billiards sports, which includes the layouts (i.e., locations) of billiards balls after performing break shots, called break shot layouts, the traces of the balls as a result of strikes (in the form of trajectories), and detailed statistics and performance indicators. We then study and develop techniques for three tasks on the collected dataset, including (1) prediction and (2) generation on the layouts data, and (3) similar billiards layout retrieval on the layouts data, which can serve different users such as coaches, players and fans. We conduct extensive experiments on the collected dataset and the results show that our methods perform effectively and efficiently.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401379769",
    "type": "article"
  },
  {
    "title": "Feature Selection as Deep Sequential Generative Learning",
    "doi": "https://doi.org/10.1145/3687485",
    "publication_date": "2024-08-09",
    "publication_year": 2024,
    "authors": "Wangyang Ying; Dongjie Wang; Haifeng Chen; Yanjie Fu",
    "corresponding_authors": "",
    "abstract": "Feature selection aims to identify the most pattern-discriminative feature subset. In prior literature, filter (e.g., backward elimination) and embedded (e.g., LASSO) methods have hyperparameters (e.g., top- k , score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly. To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that distills feature knowledge and generates decision sequences. Our method includes three steps: (1) We develop a deep variational transformer model over a joint of sequential reconstruction, variational, and performance evaluator losses. Our model can distill feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences into embedding vectors associated with utility scores. (2) We leverage the trained feature subset utility evaluator as a gradient provider to guide the identification of the optimal feature subset embedding; (3) We decode the optimal feature subset embedding to autoregressively generate the best feature selection decision sequence with autostop. Extensive experimental results show this generative perspective is effective and generic, without large discrete search space and expert-specific hyperparameters. The code is available at http://tinyurl.com/FSDSGL .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401476348",
    "type": "article"
  },
  {
    "title": "Partial Multi-Label Learning via Exploiting Instance and Label Correlations",
    "doi": "https://doi.org/10.1145/3700879",
    "publication_date": "2024-10-25",
    "publication_year": 2024,
    "authors": "Weichao Liang; Guangliang Gao; Lei Chen; Youquan Wang",
    "corresponding_authors": "",
    "abstract": "The goal of partial multi-label learning is to induce a multi-label classifier from partial multi-label data where each instance is annotated with a number of candidate labels but only a subset of them are valid. Many of the existing studies either fail to fully utilize instance and label correlations to eliminate noisy labels or build an oversimplified multi-label classifier, both of which are unfavorable for the improvement of generalization performance. In this article, we put forward a novel model named Pml-ilc to learn a multi-label classifier from partial multi-label data. Specifically, Pml-ilc first encodes instances and labels into a compact semantic space and takes full advantage of instance and label correlations to eliminate noisy labels. Then, it induces a linear mapping from the feature space to the label space while exploiting label-specific features and instance correlations to facilitate the multi-label classifier learning process. Finally, the above two steps are combined into a joint optimization problem and an efficient alternating optimization procedure is developed to find a satisfactory solution. Extensive experiments show that Pml-ilc achieves superior performance on both real-world and synthetic partial multi-label datasets in terms of different evaluation metrics.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4403762106",
    "type": "article"
  },
  {
    "title": "Adaptive Rumor Suppression on Social Networks: A Multi-Round Hybrid Approach",
    "doi": "https://doi.org/10.1145/3701738",
    "publication_date": "2024-10-31",
    "publication_year": 2024,
    "authors": "Qiang He; Z. Y. Zhang; Tingting Bi; Hui Fang; Xiushuang Yi; Keping Yu",
    "corresponding_authors": "",
    "abstract": "Rumor suppression is targeted at diminishing the impact of false and negative information within social networks by decreasing the prevalence of belief in such rumors among individuals, utilizing diverse strategies. Previous studies have broadly delineated rumor suppression strategies into two primary categories: targeting key nodes or edges for obstruction, and enlisting high-influence nodes to disseminate truth-related accurate information. Traditionally, employing a singular strategy involves utilizing a static algorithm throughout the rumor suppression endeavor. This method, however, encounters difficulties in adapting to fluctuating external conditions, rendering it less efficacious in the management of rumor proliferation. In response to these challenges, we introduce the concept of Adaptive Rumor Suppression (ARS), which aims to dynamically counter rumors by taking into account the nuances of propagation dynamics and the surrounding environmental context. We propose a multi-label state transition linear threshold model to more closely mirror the complex process of information diffusion across social networks. Furthermore, we advocate for a multi-round hybrid strategy that amalgamates blocking and clarification tactics to address the ARS problem within the confines of limited resource allocations. To navigate the complexities of adaptive rumor suppression, we introduce the Hybrid Strategy of each Round (HS-R) algorithm, which synergizes multiple strategies to effectively counter the spread of rumors. In extension, we present the Multi-Round Multi-Label (MRML) algorithm, designed to augment the efficiency of the HS-R algorithm. Experimental evaluations conducted on authentic social network datasets illustrate that our methodologies significantly outshine baseline algorithms, offering a more effective and adaptable solution to curb rumor propagation across varied environments.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4403989856",
    "type": "article"
  },
  {
    "title": "A new efficient probabilistic model for mining labeled ordered trees applied to glycobiology",
    "doi": "https://doi.org/10.1145/1342320.1342326",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Kosuke Hashimoto; Kiyoko F. Aoki‐Kinoshita; Nobuhisa Ueda; Minoru Kanehisa; Hiroshi Mamitsuka",
    "corresponding_authors": "",
    "abstract": "Mining frequent patterns from large datasets is an important issue in data mining. Recently, complex and unstructured (or semi-structured) datasets have appeared as targets for major data mining applications, including text mining, web mining and bioinformatics. Our work focuses on labeled ordered trees, which are typically semi-structured datasets. In bioinformatics, carbohydrate sugar chains, or glycans, can be modeled as labeled ordered trees. Glycans are the third major class of biomolecules, having important roles in signaling and recognition. For mining labeled ordered trees, we propose a new probabilistic model and its efficient learning scheme which significantly improves the time and space complexity of an existing probabilistic model for labeled ordered trees. We evaluated the performance of the proposed model, comparing it with those of other probabilistic models, using synthetic as well as real datasets from glycobiology. Experimental results showed that the proposed model drastically reduced the computation time of the competing model, keeping the predictive power and avoiding overfitting to the training data. Finally, we assessed our results on real data from a variety of biological viewpoints, verifying known facts in glycobiology.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1981606701",
    "type": "article"
  },
  {
    "title": "Discovering semantic biomedical relations utilizing the Web",
    "doi": "https://doi.org/10.1145/1342320.1342323",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Saurav Sahay; Sougata Mukherjea; Eugene Agichtein; Ernest Garcia; Shamkant B. Navathe; Ashwin Ram",
    "corresponding_authors": "",
    "abstract": "To realize the vision of a Semantic Web for Life Sciences, discovering relations between resources is essential. It is very difficult to automatically extract relations from Web pages expressed in natural language formats. On the other hand, because of the explosive growth of information, it is difficult to manually extract the relations. In this paper we present techniques to automatically discover relations between biomedical resources from the Web. For this purpose we retrieve relevant information from Web Search engines and Pubmed database using various lexico-syntactic patterns as queries over SOAP web services. The patterns are initially handcrafted but can be progressively learnt. The extracted relations can be used to construct and augment ontologies and knowledge bases. Experiments are presented for general biomedical relation discovery and domain specific search to show the usefulness of our technique.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1981793043",
    "type": "article"
  },
  {
    "title": "Developmental stage annotation of Drosophila gene expression pattern images via an entire solution path for LDA",
    "doi": "https://doi.org/10.1145/1342320.1342324",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Jieping Ye; Jianhui Chen; Ravi Janardan; Sudhir Kumar",
    "corresponding_authors": "",
    "abstract": "Gene expression in a developing embryo occurs in particular cells (spatial patterns) in a time-specific manner (temporal patterns), which leads to the differentiation of cell fates. Images of a Drosophila melanogaster embryo at a given developmental stage, showing a particular gene expression pattern revealed by a gene-specific probe, can be compared for spatial overlaps. The comparison is fundamentally important to formulating and testing gene interaction hypotheses. Expression pattern comparison is most biologically meaningful when images from a similar time point (developmental stage) are compared. In this paper, we present LdaPath, a novel formulation of Linear Discriminant Analysis (LDA) for automatic developmental stage range classification. It employs multivariate linear regression with the L 1 -norm penalty controlled by a regularization parameter for feature extraction and visualization. LdaPath computes an entire solution path for all values of regularization parameter with essentially the same computational cost as fitting one LDA model. Thus, it facilitates efficient model selection. It is based on the equivalence relationship between LDA and the least squares method for multiclass classifications. This equivalence relationship is established under a mild condition, which we show empirically to hold for many high-dimensional datasets, such as expression pattern images. Our experiments on a collection of 2705 expression pattern images show the effectiveness of the proposed algorithm. Results also show that the LDA model resulting from LdaPath is sparse, and irrelevant features may be removed. Thus, LdaPath provides a general framework for simultaneous feature selection and feature extraction.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1987375873",
    "type": "article"
  },
  {
    "title": "Bellwether analysis",
    "doi": "https://doi.org/10.1145/1497577.1497582",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Bee-Chung Chen; Raghu Ramakrishnan; Jude Shavlik; Pradeep Tamma",
    "corresponding_authors": "",
    "abstract": "How to mine massive datasets is a challenging problem with great potential value. Motivated by this challenge, much effort has concentrated on developing scalable versions of machine learning algorithms. However, the cost of mining large datasets is not just computational; preparing the datasets into the “right form” so that learning algorithms can be applied is usually costly, due to the human labor that is typically required and a large number of choices in data preparation, which include selecting different subsets of data and aggregating data at different granularities. We make the key observation that, for a number of practically motivated problems, these choices can be defined using database queries and analyzed in an automatic and systematic manner. Specifically, we propose a new class of data-mining problem, called bellwether analysis , in which the goal is to find a few query-defined predictors (e.g., first week sales of Peoria, IL of an item) that can be used to accurately predict the result of a target query (e.g., first year worldwide sales of the item) from a large number of queries that define candidate predictors. To make a prediction for a new item, the data needed to generate such predictors has to be collected (e.g., selling the new item in Peoria, IL for a week and collecting the sales data). A useful predictor is one that has high prediction accuracy and a low data-collection cost. We call such a cost-effective predictor a bellwether . This article introduces bellwether analysis, which integrates database query processing and predictive modeling into a single framework, and provides scalable algorithms for large datasets that cannot fit in main memory. Through a series of extensive experiments, we show that bellwethers do exist in real-world databases, and that our computation techniques achieve good efficiency on large datasets.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2125409135",
    "type": "article"
  },
  {
    "title": "Generative Models for Evolutionary Clustering",
    "doi": "https://doi.org/10.1145/2297456.2297459",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Tian-Bing Xu; Zhongfei Zhang; Philip S. Yu; Bo Long",
    "corresponding_authors": "",
    "abstract": "This article studies evolutionary clustering, a recently emerged hot topic with many important applications, noticeably in dynamic social network analysis. In this article, based on the recent literature on nonparametric Bayesian models, we have developed two generative models: DPChain and HDP-HTM. DPChain is derived from the Dirichlet process mixture (DPM) model, with an exponential decaying component along with the time. HDP-HTM combines the hierarchical dirichlet process (HDP) with a hierarchical transition matrix (HTM) based on the proposed Infinite hierarchical Markov state model (iHMS). Both models substantially advance the literature on evolutionary clustering, in the sense that not only do they both perform better than those in the existing literature, but more importantly, they are capable of automatically learning the cluster numbers and explicitly addressing the corresponding issues. Extensive evaluations have demonstrated the effectiveness and the promise of these two solutions compared to the state-of-the-art literature.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2006498170",
    "type": "article"
  },
  {
    "title": "Large-Scale Bayesian Probabilistic Matrix Factorization with Memo-Free Distributed Variational Inference",
    "doi": "https://doi.org/10.1145/3161886",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Guangyong Chen; Fengyuan Zhu; Pheng‐Ann Heng",
    "corresponding_authors": "",
    "abstract": "Bayesian Probabilistic Matrix Factorization (BPMF) is a powerful model in many dyadic data prediction problems, especially the applications of Recommender system. However, its poor scalability has limited its wide applications on massive data. Based on the conditional independence property of observed entries in BPMF model, we propose a novel distributed memo-free variational inference method for large-scale matrix factorization problems. Compared with the state-of-the-art methods, the proposed method is favored for several attractive properties. Specifically, it does not require tuning of learning rate carefully, shuffling the training set at each iteration, or storing massive redundant variables, and can introduce new agents into the computations on the fly. We conduct extensive experiments on both synthetic and real-world datasets. The experimental results show that our method can converge significantly faster with better prediction performance than alternative algorithms.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2784713518",
    "type": "article"
  },
  {
    "title": "ClassiNet -- Predicting Missing Features for Short-Text Classification",
    "doi": "https://doi.org/10.1145/3201578",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Danushka Bollegala; Vincent Atanasov; Takanori Maehara; Ken‐ichi Kawarabayashi",
    "corresponding_authors": "",
    "abstract": "Short and sparse texts such as tweets, search engine snippets, product reviews, and chat messages are abundant on the Web. Classifying such short-texts into a pre-defined set of categories is a common problem that arises in various contexts, such as sentiment classification, spam detection, and information recommendation. The fundamental problem in short-text classification is feature sparseness -- the lack of feature overlap between a trained model and a test instance to be classified. We propose ClassiNet -- a network of classifiers trained for predicting missing features in a given instance, to overcome the feature sparseness problem. Using a set of unlabeled training instances, we first learn binary classifiers as feature predictors for predicting whether a particular feature occurs in a given instance. Next, each feature predictor is represented as a vertex v i in the ClassiNet, where a one-to-one correspondence exists between feature predictors and vertices. The weight of the directed edge e ij connecting a vertex v i to a vertex v j represents the conditional probability that given v i exists in an instance, v j also exists in the same instance. We show that ClassiNets generalize word co-occurrence graphs by considering implicit co-occurrences between features. We extract numerous features from the trained ClassiNet to overcome feature sparseness. In particular, for a given instance x , we find similar features from ClassiNet that did not appear in x , and append those features in the representation of x . Moreover, we propose a method based on graph propagation to find features that are indirectly related to a given short-text. We evaluate ClassiNets on several benchmark datasets for short-text classification. Our experimental results show that by using ClassiNet, we can statistically significantly improve the accuracy in short-text classification tasks, without having to use any external resources such as thesauri for finding related features.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2797076554",
    "type": "article"
  },
  {
    "title": "Near-Optimal and Practical Algorithms for Graph Scan Statistics with Connectivity Constraints",
    "doi": "https://doi.org/10.1145/3309712",
    "publication_date": "2019-04-26",
    "publication_year": 2019,
    "authors": "José Cadena; Feng Chen; Anil Vullikanti",
    "corresponding_authors": "",
    "abstract": "One fundamental task in network analysis is detecting “hotspots” or “anomalies” in the network; that is, detecting subgraphs where there is significantly more activity than one would expect given historical data or some baseline process. Scan statistics is one popular approach used for anomalous subgraph detection. This methodology involves maximizing a score function over all connected subgraphs, which is a challenging computational problem. A number of heuristics have been proposed for these problems, but they do not provide any quality guarantees. Here, we propose a framework for designing algorithms for optimizing a large class of scan statistics for networks, subject to connectivity constraints. Our algorithms run in time that scales linearly on the size of the graph and depends on a parameter we call the “effective solution size,” while providing rigorous approximation guarantees. In contrast, most prior methods have super-linear running times in terms of graph size. Extensive empirical evidence demonstrates the effectiveness and efficiency of our proposed algorithms in comparison with state-of-the-art methods. Our approach improves on the performance relative to all prior methods, giving up to over 25% increase in the score. Further, our algorithms scale to networks with up to a million nodes, which is 1--2 orders of magnitude larger than all prior applications.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2942550645",
    "type": "article"
  },
  {
    "title": "Partial Sum Minimization of Singular Values Representation on Grassmann Manifolds",
    "doi": "https://doi.org/10.1145/3092690",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Boyue Wang; Yongli Hu; Junbin Gao; Yanfeng Sun; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Clustering is one of the fundamental topics in data mining and pattern recognition. As a prospective clustering method, the subspace clustering has made considerable progress in recent researches, e.g., sparse subspace clustering (SSC) and low rank representation (LRR). However, most existing subspace clustering algorithms are designed for vectorial data from linear spaces, thus not suitable for high-dimensional data with intrinsic non-linear manifold structure. For high-dimensional or manifold data, few research pays attention to clustering problems. The purpose of clustering on manifolds tends to cluster manifold-valued data into several groups according to the mainfold-based similarity metric. This article proposes an extended LRR model for manifold-valued Grassmann data that incorporates prior knowledge by minimizing partial sum of singular values instead of the nuclear norm, namely Partial Sum minimization of Singular Values Representation (GPSSVR). The new model not only enforces the global structure of data in low rank, but also retains important information by minimizing only smaller singular values. To further maintain the local structures among Grassmann points, we also integrate the Laplacian penalty with GPSSVR. The proposed model and algorithms are assessed on a public human face dataset, some widely used human action video datasets and a real scenery dataset. The experimental results show that the proposed methods obviously outperform other state-of-the-art methods.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2964157783",
    "type": "article"
  },
  {
    "title": "Probabilistic Mixture Model for Mapping the Underground Pipes",
    "doi": "https://doi.org/10.1145/3344721",
    "publication_date": "2019-10-04",
    "publication_year": 2019,
    "authors": "Xiren Zhou; Huanhuan Chen; Jinlong Li",
    "corresponding_authors": "",
    "abstract": "Buried pipes beneath our city are blood vessels that feed human civilization through the supply of water, gas, electricity, and so on, and mapping the buried pipes has long been addressed as an issue. In this article, a suitable coordinate of the detected area is established, the noisy Ground Penetrating Radar (GPR) and Global Positioning System (GPS) data are analyzed and normalized, and the pipeline is described mathematically. Based on these, the Probabilistic Mixture Model is proposed to map the buried pipes, which takes discrete noisy GPR and GPS data as the input and the accurate pipe locations and directions as the output. The proposed model consists of the Preprocessing, the Pipe Fitting algorithm, the Classification Fitting Expectation Maximization (CFEM) algorithm, and the Angle-limited Hough (Al-Hough) transform. The direction information of the detecting point is added into the measuring of the distance from the point to nearby pipelines, to handle some areas where the pipes are intersected or difficult to classify. The Expectation Maximization (EM) algorithm is upgraded to CFEM algorithm that is able to classify detecting points into different classes, and connect and fit multiple points in each class to get accurate pipeline locations and directions, and the Al-Hough transform provides reliable initializations for CFEM, to some extent, ensuring the convergence of the proposed model. The experimental results on the simulated and real-world datasets demonstrate the effectiveness of the proposed model.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2998340687",
    "type": "article"
  },
  {
    "title": "Physics-Based Anomaly Detection Defined on Manifold Space",
    "doi": "https://doi.org/10.1145/2641574",
    "publication_date": "2014-09-23",
    "publication_year": 2014,
    "authors": "Hao Huang; Hong Qin; Shinjae Yoo; Dantong Yu",
    "corresponding_authors": "",
    "abstract": "Current popular anomaly detection algorithms are capable of detecting global anomalies but often fail to distinguish local anomalies from normal instances. Inspired by contemporary physics theory (i.e., heat diffusion and quantum mechanics), we propose two unsupervised anomaly detection algorithms. Building on the embedding manifold derived from heat diffusion, we devise Local Anomaly Descriptor (LAD), which faithfully reveals the intrinsic neighborhood density. It uses a scale-dependent umbrella operator to bridge global and local properties, which makes LAD more informative within an adaptive scope of neighborhood. To offer more stability of local density measurement on scaling parameter tuning, we formulate Fermi Density Descriptor (FDD), which measures the probability of a fermion particle being at a specific location. By choosing the stable energy distribution function, FDD steadily distinguishes anomalies from normal instances with any scaling parameter setting. To further enhance the efficacy of our proposed algorithms, we explore the utility of anisotropic Gaussian kernel (AGK), which offers better manifold-aware affinity information. We also quantify and examine the effect of different Laplacian normalizations for anomaly detection. Comprehensive experiments on both synthetic and benchmark datasets verify that our proposed algorithms outperform the existing anomaly detection algorithms.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1992246785",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on computational aspects of social and information networks",
    "doi": "https://doi.org/10.1145/2556608",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Wei Chen; Jie Tang",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2074516855",
    "type": "article"
  },
  {
    "title": "Sampling for Nyström Extension-Based Spectral Clustering",
    "doi": "https://doi.org/10.1145/2934693",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Xianchao Zhang; Linlin Zong; Quanzeng You; Xing Yong",
    "corresponding_authors": "",
    "abstract": "Sampling is the key aspect for Nyström extension based spectral clustering. Traditional sampling schemes select the set of landmark points on a whole and focus on how to lower the matrix approximation error. However, the matrix approximation error does not have direct impact on the clustering performance. In this article, we propose a sampling framework from an incremental perspective, i.e., the landmark points are selected one by one, and each next point to be sampled is determined by previously selected landmark points. Incremental sampling builds explicit relationships among landmark points; thus, they work together well and provide a theoretical guarantee on the clustering performance. We provide two novel analysis methods and propose two schemes for selecting-the-next-one of the framework. The first scheme is based on clusterability analysis, which provides a better guarantee on clustering performance than schemes based on matrix approximation error analysis. The second scheme is based on loss analysis, which provides maximized predictive ability of the landmark points on the (implicit) labels of the unsampled points. Experimental results on a wide range of benchmark datasets demonstrate the superiorities of our proposed incremental sampling schemes over existing sampling schemes.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2487974819",
    "type": "article"
  },
  {
    "title": "Fine-Grained Air Quality Inference with Remote Sensing Data and Ubiquitous Urban Data",
    "doi": "https://doi.org/10.1145/3340847",
    "publication_date": "2019-09-24",
    "publication_year": 2019,
    "authors": "Yanan Xu; Yanmin Zhu; Yanyan Shen; Jiadi Yu",
    "corresponding_authors": "",
    "abstract": "Air quality has gained much attention in recent years and is of great importance to protecting people’s health. Due to the influence of multiple factors, the limited air quality monitoring stations deployed in cities are unable to provide fine-grained air quality information. One cost-effective way is to infer air quality with records from existing monitoring stations. However, the severe data sparsity problem (e.g., only 0.2% data are known) leads to the failure of most inference methods. We observe that remote sensing data are of high quality and have a strong correlation with the air quality. Therefore, we propose to integrate remote sensing data and ubiquitous urban data for the air quality inference. But there are two main challenges, i.e., data heterogeneity and incompleteness of the remote sensing data. To address the challenges, we propose a two-stage approach. In the first stage, we infer and predict air quality conditions of some places leveraging the remote sensing data and meteorological data with two proposed ANN-based methods, respectively. This stage significantly alleviates the data sparsity problem. In the second stage, the records and estimated air quality data are put in a tensor. A tensor decomposition method is applied to complete the tensor. The features extracted from urban data are classified into the spatial features (i.e., road features and POI features) and the temporal features (i.e., meteorological features) as the constraints to further address the data sparsity problem. In addition, an iterative training framework is proposed to improve the inference performance. Experiments on a real-world dataset show that our approach outperforms state-of-the-art methods, such as U-Air.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2976330027",
    "type": "article"
  },
  {
    "title": "A Unified Multi-view Clustering Algorithm Using Multi-objective Optimization Coupled with Generative Model",
    "doi": "https://doi.org/10.1145/3365673",
    "publication_date": "2020-02-03",
    "publication_year": 2020,
    "authors": "Sayantan Mitra; Mohammed Hasanuzzaman; Sriparna Saha",
    "corresponding_authors": "",
    "abstract": "There is a large body of works on multi-view clustering that exploit multiple representations (or views) of the same input data for better convergence. These multiple views can come from multiple modalities (image, audio, text) or different feature subsets. Obtaining one consensus partitioning after considering different views is usually a non-trivial task. Recently, multi-objective based multi-view clustering methods have suppressed the performance of single objective based multi-view clustering techniques. One key problem is that it is difficult to select a single solution from a set of alternative partitionings generated by multi-objective techniques on the final Pareto optimal front. In this article, we propose a novel multi-objective based multi-view clustering framework that overcomes the problem of selecting a single solution in multi-objective based techniques. In particular, our proposed framework has three major components as follows: (i) multi-view based multi-objective algorithm, Multiview-AMOSA, for initial clustering of data points; (ii) a generative model for generating a combined solution having probabilistic labels; and (iii) K -means algorithm for obtaining the final labels. As the first component, we have adopted a recently developed multi-view based multi-objective clustering algorithm to generate different possible consensus partitionings of a given dataset taking into account different views. A generative model is coupled with the first component to generate a single consensus partitioning after considering multiple solutions. It exploits the latent subsets of the non-dominated solutions obtained from the multi-objective clustering algorithm and combines them to produce a single probabilistic labeled solution. Finally, a simple clustering algorithm, namely K -means, is applied on the generated probabilistic labels to obtain the final cluster labels. Experimental validation of our proposed framework is carried out over several benchmark datasets belonging to three different domains; UCI datasets, multi-view datasets, search result clustering datasets, and patient stratification datasets. Experimental results show that our proposed framework achieves an improvement of around 2%--4% over different evaluation metrics in all the four domains in comparison to state-of-the art methods.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3005001078",
    "type": "article"
  },
  {
    "title": "Discovering Anomalies by Incorporating Feedback from an Expert",
    "doi": "https://doi.org/10.1145/3396608",
    "publication_date": "2020-06-22",
    "publication_year": 2020,
    "authors": "Shubhomoy Das; Weng‐Keen Wong; Thomas G. Dietterich; Alan Fern; Andrew Emmott",
    "corresponding_authors": "",
    "abstract": "Unsupervised anomaly detection algorithms search for outliers and then predict that these outliers are the anomalies. When deployed, however, these algorithms are often criticized for high false-positive and high false-negative rates. One main cause of poor performance is that not all outliers are anomalies and not all anomalies are outliers. In this article, we describe the Active Anomaly Discovery (AAD) algorithm, which incorporates feedback from an expert user that labels a queried data instance as an anomaly or nominal point. This feedback is intended to adjust the anomaly detector so that the outliers it discovers are more in tune with the expert user’s semantic understanding of the anomalies. The AAD algorithm is based on a weighted ensemble of anomaly detectors. When it receives a label from the user, it adjusts the weights on each individual ensemble member such that the anomalies rank higher in terms of their anomaly score than the outliers. The AAD approach is designed to operate in an interactive data exploration loop. In each iteration of this loop, our algorithm first selects a data instance to present to the expert as a potential anomaly and then the expert labels the instance as an anomaly or as a nominal data point. When it receives the instance label, the algorithm updates its internal model and the loop continues until a budget of B queries is spent. The goal of our approach is to maximize the total number of true anomalies in the B instances presented to the expert. We show that the AAD method performs well and in some cases doubles the number of true anomalies found compared to previous methods. In addition we present approximations that make the AAD algorithm much more computationally efficient while maintaining a desirable level of performance.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3044735541",
    "type": "article"
  },
  {
    "title": "Accelerating Large-Scale Heterogeneous Interaction Graph Embedding Learning via Importance Sampling",
    "doi": "https://doi.org/10.1145/3418684",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Yugang Ji; Mingyang Yin; Hongxia Yang; Jingren Zhou; Vincent W. Zheng; Chuan Shi; Yuan Fang",
    "corresponding_authors": "",
    "abstract": "In real-world problems, heterogeneous entities are often related to each other through multiple interactions, forming a Heterogeneous Interaction Graph (HIG). While modeling HIGs to deal with fundamental tasks, graph neural networks present an attractive opportunity that can make full use of the heterogeneity and rich semantic information by aggregating and propagating information from different types of neighborhoods. However, learning on such complex graphs, often with millions or billions of nodes, edges, and various attributes, could suffer from expensive time cost and high memory consumption. In this article, we attempt to accelerate representation learning on large-scale HIGs by adopting the importance sampling of heterogeneous neighborhoods in a batch-wise manner, which naturally fits with most batch-based optimizations. Distinct from traditional homogeneous strategies neglecting semantic types of nodes and edges, to handle the rich heterogeneous semantics within HIGs, we devise both type-dependent and type-fusion samplers where the former respectively samples neighborhoods of each type and the latter jointly samples from candidates of all types. Furthermore, to overcome the imbalance between the down-sampled and the original information, we respectively propose heterogeneous estimators including the self-normalized and the adaptive estimators to improve the robustness of our sampling strategies. Finally, we evaluate the performance of our models for node classification and link prediction on five real-world datasets, respectively. The empirical results demonstrate that our approach performs significantly better than other state-of-the-art alternatives, and is able to reduce the number of edges in computation by up to 93%, the memory cost by up to 92% and the time cost by up to 86%.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3112372431",
    "type": "article"
  },
  {
    "title": "Span-core Decomposition for Temporal Networks",
    "doi": "https://doi.org/10.1145/3418226",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Edoardo Galimberti; Martino Ciaperoni; Alain Barrat; Francesco Bonchi; Ciro Cattuto; Francesco Gullo",
    "corresponding_authors": "",
    "abstract": "When analyzing temporal networks, a fundamental task is the identification of dense structures (i.e., groups of vertices that exhibit a large number of links), together with their temporal span (i.e., the period of time for which the high density holds). In this article, we tackle this task by introducing a notion of temporal core decomposition where each core is associated with two quantities, its coreness, which quantifies how densely it is connected, and its span, which is a temporal interval: we call such cores span-cores . For a temporal network defined on a discrete temporal domain T , the total number of time intervals included in T is quadratic in | T |, so that the total number of span-cores is potentially quadratic in | T | as well. Our first main contribution is an algorithm that, by exploiting containment properties among span-cores, computes all the span-cores efficiently. Then, we focus on the problem of finding only the maximal span-cores , i.e., span-cores that are not dominated by any other span-core by both their coreness property and their span. We devise a very efficient algorithm that exploits theoretical findings on the maximality condition to directly extract the maximal ones without computing all span-cores. Finally, as a third contribution, we introduce the problem of temporal community search , where a set of query vertices is given as input, and the goal is to find a set of densely-connected subgraphs containing the query vertices and covering the whole underlying temporal domain T . We derive a connection between this problem and the problem of finding (maximal) span-cores. Based on this connection, we show how temporal community search can be solved in polynomial-time via dynamic programming, and how the maximal span-cores can be profitably exploited to significantly speed-up the basic algorithm. We provide an extensive experimentation on several real-world temporal networks of widely different origins and characteristics. Our results confirm the efficiency and scalability of the proposed methods. Moreover, we showcase the practical relevance of our techniques in a number of applications on temporal networks, describing face-to-face contacts between individuals in schools. Our experiments highlight the relevance of the notion of (maximal) span-core in analyzing social dynamics, detecting/correcting anomalies in the data, and graph-embedding-based network classification.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4288076427",
    "type": "article"
  },
  {
    "title": "Synchronization-Core-Based Discovery of Processes with Decomposable Cyclic Dependencies",
    "doi": "https://doi.org/10.1145/2845086",
    "publication_date": "2016-02-24",
    "publication_year": 2016,
    "authors": "Faming Lu; Qingtian Zeng; Hua Duan",
    "corresponding_authors": "",
    "abstract": "Traditional process discovery techniques mine process models based upon event traces giving little consideration to workflow relevant data recorded in event logs. The neglect of such information usually leads to incorrect discovered models, especially when activities have decomposable cyclic dependencies. To address this problem, the recorded workflow relevant data and decision tree learning technique are utilized to classify cases into case clusters. Each case cluster contains causality and concurrency activity dependencies only. Then, a set of activity ordering relations are derived based on case clusters. And a synchronization-core-based process model is discovered from the ordering relations and composite cases. Finally, the discovered model is transformed to a BPMN model. The proposed approach is validated with a medical treatment process and an open event log. Meanwhile, a prototype system is presented.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2342802641",
    "type": "article"
  },
  {
    "title": "Jointly Modeling Label and Feature Heterogeneity in Medical Informatics",
    "doi": "https://doi.org/10.1145/2768831",
    "publication_date": "2016-05-11",
    "publication_year": 2016,
    "authors": "Pei Yang; Hongxia Yang; Haoda Fu; Dawei Zhou; Jieping Ye; Theodoros Lappas; Jingrui He",
    "corresponding_authors": "",
    "abstract": "Multiple types of heterogeneity including label heterogeneity and feature heterogeneity often co-exist in many real-world data mining applications, such as diabetes treatment classification, gene functionality prediction, and brain image analysis. To effectively leverage such heterogeneity, in this article, we propose a novel graph-based model for Learning with both Label and Feature heterogeneity, namely L 2 F . It models the label correlation by requiring that any two label-specific classifiers behave similarly on the same views if the associated labels are similar, and imposes the view consistency by requiring that view-based classifiers generate similar predictions on the same examples. The objective function for L 2 F is jointly convex. To solve the optimization problem, we propose an iterative algorithm, which is guaranteed to converge to the global optimum. One appealing feature of L 2 F is that it is capable of handling data with missing views and labels. Furthermore, we analyze its generalization performance based on Rademacher complexity, which sheds light on the benefits of jointly modeling the label and feature heterogeneity. Experimental results on various biomedical datasets show the effectiveness of the proposed approach.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2356678342",
    "type": "article"
  },
  {
    "title": "Heterogeneous Translated Hashing",
    "doi": "https://doi.org/10.1145/2744204",
    "publication_date": "2016-07-27",
    "publication_year": 2016,
    "authors": "Ying Wei; Yangqiu Song; Yi Zhen; Bo Liu; Qiang Yang",
    "corresponding_authors": "",
    "abstract": "Multi-modal similarity search has attracted considerable attention to meet the need of information retrieval across different types of media. To enable efficient multi-modal similarity search in large-scale databases recently, researchers start to study multi-modal hashing. Most of the existing methods are applied to search across multi-views among which explicit correspondence is provided. Given a multi-modal similarity search task, we observe that abundant multi-view data can be found on the Web which can serve as an auxiliary bridge. In this paper, we propose a Heterogeneous Translated Hashing (HTH) method with such auxiliary bridge incorporated not only to improve current multi-view search but also to enable similarity search across heterogeneous media which have no direct correspondence. HTH provides more flexible and discriminative ability by embedding heterogeneous media into different Hamming spaces, compared to almost all existing methods that map heterogeneous data in a common Hamming space. We formulate a joint optimization model to learn hash functions embedding heterogeneous media into different Hamming spaces, and a translator aligning different Hamming spaces. The extensive experiments on two real-world datasets, one publicly available dataset of Flickr, and the other MIRFLICKR-Yahoo Answers dataset, highlight the effectiveness and efficiency of our algorithm.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2482105843",
    "type": "article"
  },
  {
    "title": "World Knowledge as Indirect Supervision for Document Clustering",
    "doi": "https://doi.org/10.1145/2953881",
    "publication_date": "2016-12-26",
    "publication_year": 2016,
    "authors": "Chenguang Wang; Yangqiu Song; Dan Roth; Ming Zhang; Jiawei Han",
    "corresponding_authors": "",
    "abstract": "One of the key obstacles in making learning protocols realistic in applications is the need to supervise them, a costly process that often requires hiring domain experts. We consider the framework to use the world knowledge as indirect supervision. World knowledge is general-purpose knowledge, which is not designed for any specific domain. Then, the key challenges are how to adapt the world knowledge to domains and how to represent it for learning. In this article, we provide an example of using world knowledge for domain-dependent document clustering. We provide three ways to specify the world knowledge to domains by resolving the ambiguity of the entities and their types, and represent the data with world knowledge as a heterogeneous information network. Then, we propose a clustering algorithm that can cluster multiple types and incorporate the sub-type information as constraints. In the experiments, we use two existing knowledge bases as our sources of world knowledge. One is Freebase, which is collaboratively collected knowledge about entities and their organizations. The other is YAGO2, a knowledge base automatically extracted from Wikipedia and maps knowledge to the linguistic knowledge base, WordNet. Experimental results on two text benchmark datasets (20newsgroups and RCV1) show that incorporating world knowledge as indirect supervision can significantly outperform the state-of-the-art clustering algorithms as well as clustering algorithms enhanced with world knowledge features. A preliminary version of this work appeared in the proceedings of KDD 2015 [Wang et al. 2015a]. This journal version has made several major improvements. First, we have proposed a new and general learning framework for machine learning with world knowledge as indirect supervision, where document clustering is a special case in the original paper. Second, in order to make our unsupervised semantic parsing method more understandable, we add several real cases from the original sentences to the resulting logic forms with all the necessary information. Third, we add details of the three semantic filtering methods and conduct deep analysis of the three semantic filters, by using case studies to show why the conceptualization-based semantic filter can produce more accurate indirect supervision. Finally, in addition to the experiment on 20 newsgroup data and Freebase, we have extended the experiments on clustering results by using all the combinations of text (20 newsgroup, MCAT, CCAT, ECAT) and world knowledge sources (Freebase, YAGO2).",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2495832907",
    "type": "article"
  },
  {
    "title": "Comparing Clustering with Pairwise and Relative Constraints",
    "doi": "https://doi.org/10.1145/2996467",
    "publication_date": "2016-12-03",
    "publication_year": 2016,
    "authors": "Yuanli Pei; Xiaoli Z. Fern; Teresa Vania Tjahja; Rómer Rosales",
    "corresponding_authors": "",
    "abstract": "Clustering can be improved with the help of side information about the similarity relationships among instances. Such information has been commonly represented by two types of constraints: pairwise constraints and relative constraints, regarding similarities about instance pairs and triplets, respectively. Prior work has mostly considered these two types of constraints separately and developed individual algorithms to learn from each type. In practice, however, it is critical to understand/compare the usefulness of the two types of constraints as well as the cost of acquiring them, which has not been studied before. This paper provides an extensive comparison of clustering with these two types of constraints. Specifically, we compare their impacts both on human users that provide such constraints and on the learning system that incorporates such constraints into clustering. In addition, to ensure that the comparison of clustering is performed on equal ground (without the potential bias introduced by different learning algorithms), we propose a probabilistic semi-supervised clustering framework that can learn from either type of constraints. Our experiments demonstrate that the proposed semi-supervised clustering framework is highly effective at utilizing both types of constraints to aid clustering. Our user study provides valuable insights regarding the impact of the constraints on human users, and our experiments on clustering with the human-labeled constraints reveal that relative constraint is often more efficient at improving clustering.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2560462742",
    "type": "article"
  },
  {
    "title": "Joint Transferable Dictionary Learning and View Adaptation for Multi-view Human Action Recognition",
    "doi": "https://doi.org/10.1145/3434746",
    "publication_date": "2021-03-05",
    "publication_year": 2021,
    "authors": "Bin Sun; Dehui Kong; Shaofan Wang; Lichun Wang; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Multi-view human action recognition remains a challenging problem due to large view changes. In this article, we propose a transfer learning-based framework called transferable dictionary learning and view adaptation (TDVA) model for multi-view human action recognition. In the transferable dictionary learning phase, TDVA learns a set of view-specific transferable dictionaries enabling the same actions from different views to share the same sparse representations, which can transfer features of actions from different views to an intermediate domain. In the view adaptation phase, TDVA comprehensively analyzes global, local, and individual characteristics of samples, and jointly learns balanced distribution adaptation, locality preservation, and discrimination preservation, aiming at transferring sparse features of actions of different views from the intermediate domain to a common domain. In other words, TDVA progressively bridges the distribution gap among actions from various views by these two phases. Experimental results on IXMAS, ACT4 2 , and NUCLA action datasets demonstrate that TDVA outperforms state-of-the-art methods.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3134602404",
    "type": "article"
  },
  {
    "title": "Clustering Heterogeneous Information Network by Joint Graph Embedding and Nonnegative Matrix Factorization",
    "doi": "https://doi.org/10.1145/3441449",
    "publication_date": "2021-06-10",
    "publication_year": 2021,
    "authors": "Benhui Zhang; Maoguo Gong; Jianbin Huang; Xiaoke Ma",
    "corresponding_authors": "",
    "abstract": "Many complex systems derived from nature and society consist of multiple types of entities and heterogeneous interactions, which can be effectively modeled as heterogeneous information network (HIN). Structural analysis of heterogeneous networks is of great significance by leveraging the rich semantic information of objects and links in the heterogeneous networks. And, clustering heterogeneous networks aims to group vertices into classes, which sheds light on revealing the structure–function relations of the underlying systems. The current algorithms independently perform the feature extraction and clustering, which are criticized for not fully characterizing the structure of clusters. In this study, we propose a learning model by joint &lt;underline&gt;G&lt;/underline&gt;raph &lt;underline&gt;E&lt;/underline&gt;mbedding and &lt;underline&gt;N&lt;/underline&gt;onnegative &lt;underline&gt;M&lt;/underline&gt;atrix &lt;underline&gt;F&lt;/underline&gt;actorization (aka GEjNMF ), where feature extraction and clustering are simultaneously learned by exploiting the graph embedding and latent structure of networks. We formulate the objective function of GEjNMF and transform the heterogeneous network clustering problem into a constrained optimization problem, which is effectively solved by l 0 -norm optimization. The advantage of GEjNMF is that features are selected under the guidance of clustering, which improves the performance and saves the running time of algorithms at the same time. The experimental results on three benchmark heterogeneous networks demonstrate that GEjNMF achieves the best performance with the least running time compared with the best state-of-the-art methods. Furthermore, the proposed algorithm is robust across heterogeneous networks from various fields. The proposed model and method provide an effective alternative for heterogeneous network clustering.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3167130564",
    "type": "article"
  },
  {
    "title": "Jointly Modeling Heterogeneous Student Behaviors and Interactions among Multiple Prediction Tasks",
    "doi": "https://doi.org/10.1145/3458023",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Haobing Liu; Yanmin Zhu; Tianzi Zang; Yanan Xu; Jiadi Yu; Feilong Tang",
    "corresponding_authors": "",
    "abstract": "Prediction tasks about students have practical significance for both student and college. Making multiple predictions about students is an important part of a smart campus. For instance, predicting whether a student will fail to graduate can alert the student affairs office to take predictive measures to help the student improve his/her academic performance. With the development of information technology in colleges, we can collect digital footprints which encode heterogeneous behaviors continuously. In this paper, we focus on modeling heterogeneous behaviors and making multiple predictions together, since some prediction tasks are related and learning the model for a specific task may have the data sparsity problem. To this end, we propose a variant of LSTM and a soft-attention mechanism. The proposed LSTM is able to learn the student profile-aware representation from heterogeneous behavior sequences. The proposed soft-attention mechanism can dynamically learn different importance degrees of different days for every student. In this way, heterogeneous behaviors can be well modeled. In order to model interactions among multiple prediction tasks, we propose a co-attention mechanism based unit. With the help of the stacked units, we can explicitly control the knowledge transfer among multiple tasks. We design three motivating behavior prediction tasks based on a real-world dataset collected from a college. Qualitative and quantitative experiments on the three prediction tasks have demonstrated the effectiveness of our model.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3183480400",
    "type": "article"
  },
  {
    "title": "Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists",
    "doi": "https://doi.org/10.1145/3502736",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Aparna S. Varde",
    "corresponding_authors": "Aparna S. Varde",
    "abstract": "Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4220944709",
    "type": "article"
  },
  {
    "title": "Methods and Applications of Clusterwise Linear Regression: A Survey and Comparison",
    "doi": "https://doi.org/10.1145/3550074",
    "publication_date": "2022-07-26",
    "publication_year": 2022,
    "authors": "Qiang Long; Adil Bagirov; Sona Taheri; Nargiz Sultanova; Xue Wu",
    "corresponding_authors": "",
    "abstract": "Clusterwise linear regression (CLR) is a well-known technique for approximating a data using more than one linear function. It is based on the combination of clustering and multiple linear regression methods. This article provides a comprehensive survey and comparative assessments of CLR including model formulations, description of algorithms, and their performance on small to large-scale synthetic and real-world datasets. Some applications of the CLR algorithms and possible future research directions are also discussed.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4287981147",
    "type": "article"
  },
  {
    "title": "Contact Tracing and Epidemic Intervention via Deep Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3546870",
    "publication_date": "2022-08-22",
    "publication_year": 2022,
    "authors": "Tao Feng; Sirui Song; Xia Tong; Yong Li",
    "corresponding_authors": "",
    "abstract": "The recent outbreak of COVID-19 poses a serious threat to people’s lives. Epidemic control strategies have also caused damage to the economy by cutting off humans’ daily commute. In this article, we develop an Individual-based Reinforcement Learning Epidemic Control Agent (IDRLECA) to search for smart epidemic control strategies that can simultaneously minimize infections and the cost of mobility intervention. IDRLECA first hires an infection probability model to calculate the current infection probability of each individual. Then, the infection probabilities together with individuals’ health status and movement information are fed to a novel GNN to estimate the spread of the virus through human contacts. The estimated risks are used to further support an RL agent to select individual-level epidemic-control actions. The training of IDRLECA is guided by a specially designed reward function considering both the cost of mobility intervention and the effectiveness of epidemic control. Moreover, we design a constraint for control-action selection that eases its difficulty and further improve exploring efficiency. Extensive experimental results demonstrate that IDRLECA can suppress infections at a very low level and retain more than 95% of human mobility.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4292596058",
    "type": "article"
  },
  {
    "title": "Introduction of the Special Issue on Trustworthy Artificial Intelligence",
    "doi": "https://doi.org/10.1145/3712184",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Wenqi Fan; Shu Zhao; Jiliang Tang",
    "corresponding_authors": "",
    "abstract": "The special issue on Explainable Artificial Intelligence (XAI) provides a representative snapshot of the state of the art in the 2020-2021 time-frame and highlights future research directions. The scope of the special issue is ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406423273",
    "type": "article"
  },
  {
    "title": "Efficient Distributed Sparse Relative Similarity Learning",
    "doi": "https://doi.org/10.1145/3712603",
    "publication_date": "2025-01-16",
    "publication_year": 2025,
    "authors": "Dezhong Yao; Sanmu Li; Zhiwei Wang; Peilin Zhao; Gang Wu; Yu Chen; Hai Jin",
    "corresponding_authors": "",
    "abstract": "Learning a good similarity measure for large-scale high-dimensional data is a crucial task in machine learning applications, yet it poses a significant challenge. Distributed minibatch stochastic gradient descent (SGD) serves as an efficient optimization method in large-scale distributed training, allowing linear speedup in proportion to the number of workers. However, communication efficiency in distributed SGD requires a sufficiently large minibatch size, presenting two distinct challenges. Firstly, a large minibatch size leads to high memory usage and computational complexity during parallel training of high-dimensional models. Second, a larger batch size of data reduces the convergence rate. To overcome these challenges, we propose an efficient distributed sparse relative similarity learning framework EDSRSL . This framework integrates two strategies: local minibatch SGD and sparse relative similarity learning. By effectively reducing the number of updates through synchronous delay while maintaining a large batch size, we address the issue of high computational cost. Additionally, we incorporate sparse model learning into the training process, significantly reducing computational cost. This paper also provides theoretical proof that the convergence rate does not decrease significantly with increasing batch size. Various experiments on six high-dimensional real-world datasets demonstrate the efficacy and efficiency of the proposed algorithms, with a communication cost reduction of up to \\(90.89\\%\\) and a maximum wall time speedup of \\(5.66\\times\\) compared to the baseline methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406428816",
    "type": "article"
  },
  {
    "title": "Pattern-oriented Attention Mechanism for Multivariate Time Series Forecasting",
    "doi": "https://doi.org/10.1145/3712606",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Hanwen Hu; Zhengjie Han; Shiyou Qian; Dingyu Yang; Jian Cao; Guangtao Xue",
    "corresponding_authors": "",
    "abstract": "Multivariate time series forecasting is applied in many domains, such as finance, transportation and industry. The main challenge of precise forecasting lies in accurately capturing latent dependencies. Recent studies develop various frameworks to reduce computational complexity or to enhance the learning of intricate relationships, while lacking interpretability and generality. In this paper, we aim to elucidate the capture of dependencies as the recognition of patterns. We believe that patterns can be formally described from two aspects: the shapes of segments that frequently repeat, and the corresponding forms of repetitions. Drawing upon this idea, we design a multivariate time series forecasting model named PRformer 1 , which incorporates a pattern-oriented attention mechanism and a pattern-based projector. The attention mechanism can perceive different forms of repetitions by embedded with various similarity evaluation metrics between segments, and filter out noise from segments to extract potential patterns with a statistical-driven weighting scheme. The pattern-based projector is employed to form the forecasting results by deriving the representative patterns from the set of potential ones. By incorporating explicit definitions of patterns, PRformer is interpretable and general to various time series scenarios. Experimental results on seven datasets demonstrate that PRformer outperforms six state-of-the-art models by about 10.7% in forecasting accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406530572",
    "type": "article"
  },
  {
    "title": "Stacking Factorizing Partitioned Expressions in Hybrid Bayesian Network Models",
    "doi": "https://doi.org/10.1145/3714473",
    "publication_date": "2025-01-23",
    "publication_year": 2025,
    "authors": "Peng Lin; Martin Neil; Norman Fenton",
    "corresponding_authors": "",
    "abstract": "Hybrid Bayesian networks (HBN) contain complex conditional probabilistic distributions (CPD) specified as partitioned expressions over discrete and continuous variables. The size of these CPDs grows exponentially with the number of parent nodes, and when using discrete inference methods, it results in significant execution time and space inefficiency. To reduce the CPD size, a binary factorization (BF) algorithm can be used to decompose the statistical or arithmetic functions in the CPD by factorizing the number of connected parent nodes into sets of size two. However, the BF algorithm was not designed to handle partitioned expressions. Therefore, we propose a new stacking factorization (SF) algorithm to decompose partitioned expressions. The SF algorithm creates intermediate nodes to incrementally reconstruct the conditional densities in the original partitioned expression, ensuring that no more than two continuous parent nodes are connected to each child node in the resulting HBN. It generally applies to both discrete and continuous child nodes with complex partitioned expressions. When we combine SF with a dynamic discretization (DD) inference algorithm, we achieve a significant improvement in inference efficiency. Experimental results demonstrate that the combination of SF and DD can effectively manage HBNs with complex CPDs that may challenge other algorithms, which also outperform competing inference algorithms in accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406761237",
    "type": "article"
  },
  {
    "title": "GFairHint: Improving Individual Fairness for Graph Neural Networks via Fairness Hint",
    "doi": "https://doi.org/10.1145/3714472",
    "publication_date": "2025-01-29",
    "publication_year": 2025,
    "authors": "Paiheng Xu; Yuhang Zhou; Bang An; Wei Ai; Fang Huang",
    "corresponding_authors": "",
    "abstract": "Given the growing concerns about fairness in machine learning and the impressive performance of Graph Neural Networks (GNNs) on graph data learning, algorithmic fairness in GNNs has attracted significant attention. While many existing studies improve fairness at the group level, only a few works promote individual fairness, which renders similar outcomes for similar individuals. A desirable framework that promotes individual fairness should (1) balance fairness and performance, (2) accommodate two commonly-used individual similarity measures (externally annotated and computed from input features), and, (3) generalize across various GNNs. Unfortunately, none of the prior work achieves all the desirables. In this work, we propose a novel method, GFairHint , which promotes individual fairness in GNNs and achieves all aforementioned desirables. GFairHint learns fairness representations through an auxiliary link prediction task, which is inspired by a theoretical analysis of the definition of individual fairness. We then concatenate the representations with the learned node embeddings in original GNNs as a “fairness hint” . Through extensive experimental investigations on five real-world graph datasets under three prevalent GNNs covering both individual similarity measures above, GFairHint achieves the best fairness results in almost all combinations of datasets with various backbone models, while generating comparable utility results, with much less computational cost compared to the previous state-of-the-art (SoTA) method. 1",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406956692",
    "type": "article"
  },
  {
    "title": "Time Series Classification with Elasticity Using Augmented Path Signatures",
    "doi": "https://doi.org/10.1145/3715702",
    "publication_date": "2025-01-29",
    "publication_year": 2025,
    "authors": "Chenyang Wang; Ling Luo; Uwe Aickelin",
    "corresponding_authors": "",
    "abstract": "We often compare time-dependent data elastically such that some compression or dilation along the time dimension can be ignored, for example, spatial trajectories of vehicles moving at different speeds or accelerometer data for exercises completed at variable rhythms. Traditionally this is possible via an alignment-based elastic distance measure, such as Dynamic Time Warping (DTW). We may also control the degree of allowable warping with warping constraints. However, these elastic distance measures are not easy to use in large scale time series classification, as they need to be evaluated pairwise and often cannot be directly converted into feature sets that we may use with arbitrary classifiers or combine with other features. In this research, we focus on the study of path signatures, a transformation with time warping invariance property, and how we may augment a time series to make its signature space representation reflect common warping constraints. We demonstrate that the comparing signatures is analogous to comparing time series with elastic distances, and that augmented signature features can serve as warping invariant or insensitive features in time series classification. Finally, we construct Multiple Path Signatures with Constraining Augmentations Classifier (MultiPSCA), a general-purpose minimal tuning time series classifier using augmented signatures, and show that it is able to beat existing best-performing elastic time series classification algorithms without per-dataset hyperparameter tuning.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406956946",
    "type": "article"
  },
  {
    "title": "Similarity Metrics: Chebyshev Coulomb Force and Resultant Force for High Dimensional Data",
    "doi": "https://doi.org/10.1145/3715963",
    "publication_date": "2025-01-31",
    "publication_year": 2025,
    "authors": "Jianying Liu; Chaowei Zhang; Min Zhang; Xiao Qin; Jifu Zhang",
    "corresponding_authors": "",
    "abstract": "The similarity metric has garnered widespread attention thanks to its potential applications in the fields of data mining, machine learning, etc. Due to the interference of “distance concentration” caused by “Curse of dimensionality”, however, existing similarity metrics are inadequate in high-dimensional data analysis. In this study, we propose two innovative similarity metrics – Chebyshev Coulomb force and Chebyshev Coulomb resultant force – anchored on Chebyshev p-norms. In the initial phase, we eliminate dependency relationships among attributes by applying a metric matrix – and the theoretical analysis reveals that the Chebyshev p-norms is capable of mitigating the effect of “distance concentration” among high-dimensional data objects. Next, we devise two similarity metrics – Chebyshev Coulomb force and Chebyshev Coulomb resultant force – by adopting the metric matrix and Chebyshev p-norms. Chebyshev Coulomb force and Chebyshev Coulomb resultant force, being effective in characterizing the similarity among data objects, quantify the deviation of data objects from their respective dataset centers. Additionally, the two metrics alleviate the interference of “distance concentration”. Importantly, the discrepancy of data objects in attribute dimensions is captured by Chebyshev Coulomb force vector, rendering the similarity metric interpretable. By utilizing the UCI dataset, the experimental validation demonstrates the superiority of our similarity metrics, confirming their efficacy in mitigating the interference of “distance concentration”. Compared with the existing similarity metric approaches, the AUC index of outlier detection shows an average improvement of 8.18% – and the ARI, NMI and F_score indices of clustering are revamped by averages 6.56%, 6.87% and 6.01%, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407028895",
    "type": "article"
  },
  {
    "title": "Subclass-wise Logit Perturbation for Multi-label Learning",
    "doi": "https://doi.org/10.1145/3715919",
    "publication_date": "2025-01-31",
    "publication_year": 2025,
    "authors": "Zhu Yu; Ou Wu; Fengguang Su",
    "corresponding_authors": "",
    "abstract": "Logit perturbation refers to adding perturbation on logit, which has been shown to be capable of enhancing the robustness and generalization capabilities of deep neural networks in machine learning. However, studies on logit perturbation for multi-label learning are limited and they only consider the issue of class imbalance in the training data. Furthermore, the logit perturbation vectors in these methods are identical for negative classes containing different subclasses when multi-label learning is viewed as a multiple binary classification problem. This study investigates logit perturbation by exploring the characteristics of subclass-wise multi-label training data. First, the influence of the characteristics of multi-label training data on classification performance is analyzed in terms of the three data characteristics, namely, proportion, variance, and co-occurrence for each category (or subclass). Quantitative analyses reveal that variance differences among the subclasses in the negative class of a decomposed binary task also negatively impact the training performance, and if multiple characteristics affect simultaneously, the performance deterioration will be more severe. Second, theoretical analysis is performed for subclass-wise logit perturbation and a new subclass-wise logit perturbation method is proposed for multi-label learning. In our method, each class/subclass has a carefully designed perturbation implementation according to its proportion, variance, and co-occurrence. Finally, our proposed method is further explained through a regularization view. Extensive experiments demonstrate that our method consistently enhances the generalization performance of popular depth networks on multi-label benchmark datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407029011",
    "type": "article"
  },
  {
    "title": "Gradual Drift Detection in Process Models Using Conformance Metrics",
    "doi": "https://doi.org/10.1145/3716169",
    "publication_date": "2025-02-04",
    "publication_year": 2025,
    "authors": "Víctor Gallego-Fontenla; Pedro Gamallo-Fernandez; Juan C. Vidal; Manuel Lama",
    "corresponding_authors": "",
    "abstract": "Changes, planned or unexpected, are common during the execution of real-life processes. Detecting these changes is a must for optimizing the performance of organizations running such processes. Most of the algorithms present in the state-of-the-art focus on the detection of sudden changes, leaving aside other types of changes. In this paper, we will focus on the automatic detection of gradual drifts, a special type of change, in which the cases of two models overlap during a period of time. The proposed algorithm relies on conformance checking metrics to carry out the automatic detection of the changes, performing also a fully automatic classification of these changes into sudden or gradual. The approach has been validated with a synthetic dataset consisting of 120 logs with different distributions of changes, getting better results in terms of detection and classification accuracy, delay and change region overlapping than the main state-of-the-art algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407137918",
    "type": "article"
  },
  {
    "title": "Together is better: knowledge-aware model with resume fusion for online job recommendation",
    "doi": "https://doi.org/10.1145/3716503",
    "publication_date": "2025-02-07",
    "publication_year": 2025,
    "authors": "Xiao Gu; Ling Jian; C. R. Rao; Zhaohui Bu; Xianggang Cheng",
    "corresponding_authors": "",
    "abstract": "Widespread adoption of online recruitment platforms has led to explosive growth in employment information, resulting in an ever-increasing demand from job seekers for accurate and effective job recommendations. Existing studies on the Person-Job Fit models focus on the correlation between resumes and job descriptions, with rare consideration given to user historical behavior such as click and application. On the contrary, job recommendation methods always ignore the crucial information lurking in the resume text. In addition, the continuous influx of a vast amount of job data poses challenges to the updating of online recommendation results. To this end, we propose a novel O nline J ob R ecommendation model via R esume F usion (OJRRF) in this paper, aimed at making accurate and efficient online job recommendations with the merits of addressing job cold start and long tail problems. The key contribution lies in two facets: (1) incorporating resume text information into the knowledge graph attention framework to enhance job seekers’ vector representations jointly; (2) designing a hybrid recommender strategy by combining the knowledge-aware offline model with the content-based online model. Finally, we conducted extensive comparison experiments and online A/B test on the recruitment platform of JiuYeJie big data company to validate the effectiveness and real-time capability of OJRRF. The release code can be found in https://github.com/urnotada/OJRRF .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407240234",
    "type": "article"
  },
  {
    "title": "CM-CaFE: A Clustering Method with Causality-based Feature Embedding",
    "doi": "https://doi.org/10.1145/3717068",
    "publication_date": "2025-02-18",
    "publication_year": 2025,
    "authors": "Xuechun Jing; Fuyuan Cao; Kui Yu; Jiye Liang",
    "corresponding_authors": "",
    "abstract": "Clustering is a fundamental technique widely used for exploring the inherent data structure. Many studies indicate that an appropriate feature representation can effectively improve clustering performance. However, the existing feature representation methods are based on correlation to select or extract features, which makes it hard to deal with spurious correlations. The spurious correlations mislead the correlation-based methods to consider features that have no causal relationship as being correlative, which limits the clustering performance and feature interpretability. To tackle this issue, inspired by causal learning, we propose a new joint optimization C lustering M ethod with Ca usal F eature E mbedding (CM-CaFE), which utilizes the causality of features to learn more discriminative representation for clustering. Specifically, to eliminate spurious correlations among features, we first employ any state-of-the-art Markov blanket learning method to learn an undirected causal graph. Next, we extract the maximal fully-connected causal subgraphs from the learned undirected causal graph and propose an approach to merge them to generate the causal matrix. Based on the causal matrix, we present an objective function that consists of a clustering loss term and a causal matrix fitting term to learn a causal transformation matrix. The causal transformation matrix is utilized to map the original data into a new space for clustering. Finally, we comprehensively compare the proposed method with some state-of-the-art clustering approaches on several datasets to demonstrate the effectiveness and interpretability of the proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407698822",
    "type": "article"
  },
  {
    "title": "Principled Mining, Forecasting and Monitoring of Honeybee Time Series with EBV+",
    "doi": "https://doi.org/10.1145/3719014",
    "publication_date": "2025-02-21",
    "publication_year": 2025,
    "authors": "Mst. Shamima Hossain; Christos Faloutsos; Boris Baer; Hyoseung Kim; Vassilis J. Tsotras",
    "corresponding_authors": "",
    "abstract": "Honeybees, as natural crop pollinators, play a significant role in biodiversity and food production for human civilization. Bees actively regulate hive temperature (homeostasis) to maintain a colony’s proper functionality. Deviations from usual thermoregulation behavior due to external stressors (e.g., extreme environmental temperature, parasites, pesticide exposure, etc.) indicate an impending colony collapse. Anticipating such threats by forecasting hive temperature and finding changes in temperature patterns would allow beekeepers to take early preventive measures and avoid critical issues. In that case, how can we model bees’ thermoregulation behavior for an interpretable and effective hive monitoring system? In this paper, we propose the principled EBV + (Electronic Bee-Veterinarian plus) method based on the thermal diffusion equation and a novel ‘sigmoid’ feedback-loop (P) controller for analyzing hive health with the following properties: (i) it is effective on multiple, real-world beehive time sequences (recorded and streaming), (ii) it is explainable with only a few parameters (e.g., hive health factor) that beekeepers can easily quantify and trust, (iii) it issues proactive alerts to beekeepers before any potential issue affecting homeostasis becomes detrimental, and (iv) it is scalable with a time complexity of O(t) for reconstructing and O(t×m) for finding cuts of a sequence with C time-ticks. Experimental results on multiple real-world time sequences showcase the potential and practical feasibility of EBV+. Our method yields accurate forecasting (up to 72% improvement in RMSE) with up to 600 times fewer parameters compared to baselines (ARX, seasonal ARX, Holt-winters, and DeepAR), as well as detects discontinuities and raises alerts that coincide with domain experts’ opinions. Moreover, EBV+ is scalable and fast, taking less than 1 minute on a stock laptop to reconstruct two months of sensor data.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407841718",
    "type": "article"
  },
  {
    "title": "<i>OTM</i> : Efficient K-Order-Based Core Maintenance in Large-Scale Dynamic Hypergraphs",
    "doi": "https://doi.org/10.1145/3719205",
    "publication_date": "2025-02-24",
    "publication_year": 2025,
    "authors": "Xiangfei Fang; Chengying Huan; Heng Zhang; Yongchao Liu; Shaonan Ma; Yanjun Wu; Chen Zhao",
    "corresponding_authors": "",
    "abstract": "The \\(k\\) -core model has garnered widespread adoption for preserving essential cohesive subgraphs owing to its linear-time computability, making it particularly suitable for hypergraph analysis. However, considering the continuously evolving characteristics of real-world hypergraphs, recent research efforts have focused on developing efficient algorithms that can maintain the core value of each vertex amid structural alterations. Despite these efforts, frequent insertions and deletions in dynamic hypergraphs continue to pose significant inefficiencies, primarily due to the increased traversal overhead incurred by hyperedge insertion algorithms. This exacerbates performance disparities between handling hyperedge insertions and deletions, underscoring the persistent challenge of effective \\(k\\) -core analysis in hypergraphs. To effectively address these challenges, we have gained key insights that enable us to define a specific order, termed the hypergraph \\(k\\) -order, which significantly reduces redundant vertex traversal and narrows down the search space during hyperedge insertions. Based on the proposed hypergraph \\(k\\) -order, we define two indices, the order index and the pivotal index, aimed at minimizing traversal costs and expediting the hyperedge insertion algorithm. Moreover, it is essential to recognize that the recomputation of the support degree ( \\(sd\\) ) for all vertices following each hyperedge deletion can significantly diminish the performance efficiency of deletion algorithms. To address this, we introduce an optimized approach that leverages the incremental maintenance of the support degree ( \\(sd\\) ) value to expedite the hyperedge deletion process. By leveraging these optimizations, we introduce a novel Order-based Traversal core Maintenance methodology, designated as OTM , which markedly enhances the efficiency of core maintenance in dynamic hypergraphs. Our comprehensive evaluation, which covers 12 real-world hypergraph datasets and a synthetic dataset, reveals that OTM achieves staggering speedup, outperforming the state-of-the-art approach with a 41,420 \\(\\times\\) speedup in the insertion algorithm and 8,284 \\(\\times\\) speedup in the deletion algorithm, underscoring its remarkable efficiency and effectiveness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407870117",
    "type": "article"
  },
  {
    "title": "Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning",
    "doi": "https://doi.org/10.1145/3719350",
    "publication_date": "2025-02-26",
    "publication_year": 2025,
    "authors": "Xinjian Luo; X. Zhang",
    "corresponding_authors": "",
    "abstract": "Federated learning (FL) is a decentralized model training framework that aims to merge isolated data islands while maintaining data privacy. However, recent studies have revealed that Generative Adversarial Network (GAN) based attacks can be employed in FL to learn the distribution of private datasets and reconstruct recognizable images. In this paper, we exploit defenses against GAN-based attacks in FL and propose a framework, Anti-GAN, to prevent attackers from learning the real distribution of the victim’s data. The core idea of Anti-GAN is to manipulate the visual features of private training images to make them indistinguishable to human eyes even restored by attackers. Specifically, Anti-GAN projects the private dataset onto a GAN’s generator and combines the generated fake images with the actual images to create the training dataset, which is then used for federated model training. The experimental results demonstrate that Anti-GAN is effective in preventing attackers from learning the distribution of private images while causing minimal harm to the accuracy of the federated model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407957581",
    "type": "article"
  },
  {
    "title": "Compressing Deep Neural Networks with Goal-Specific Pruning and Self-Distillation",
    "doi": "https://doi.org/10.1145/3721293",
    "publication_date": "2025-03-04",
    "publication_year": 2025,
    "authors": "Fuchen Chen; Yun-Jui Hsu; Chia-Hsun Lu; Hong-Han Shuai; Lo‐Yao Yeh; Chih-Ya Shen",
    "corresponding_authors": "",
    "abstract": "Neural network (NN) compression aims at reducing the model size and receives much research attention. Nevertheless, we observe that when compressing convolutional neural networks (CNNs), previous approaches may not well measure the impact of filters to loss, resulting in a significant performance degradation after compression. On the other hand, for compressing the fully-connected neural networks (FCNNs), we observe that converting the weight matrix to the block diagonal structure would result in better compression. Therefore, for compressing CNNs, we propose a new pipeline in this paper, named Retraining-Aware Pruning (RAP) , with a new self-distillation approach, named High-level Activation-guided Attention-preserving Self-distillation (HAP) and a novel filter pruning strategy, named Normalized Gradients and Geometric Median (NGGM) to effectively improve the accuracy and reduce the model size. Further, for reducing the model size of FCNNs, we formulate a new research problem, i.e., Compression with Difference-Minimized Block Diagonal Structure (COMIS) , and propose a new algorithm, Memory-Efficient and Structure-Aware Compression (MESA) to effectively prune the weights into a block diagonal structure to significantly boost the compression rate. Extensive experiments on different models show that our approaches significantly outperform the state-of-the-art baselines in terms of compression rate, accuracy, and inference speed-up.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408126114",
    "type": "article"
  },
  {
    "title": "SED2AM: Solving Multi-Trip Time-Dependent Vehicle Routing Problem using Deep Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3721983",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Arash Mozhdehi; Yunli Wang; Sun Sun; Xin Wang",
    "corresponding_authors": "",
    "abstract": "Deep reinforcement learning (DRL)-based frameworks, featuring Transformer-style policy networks, have demonstrated their efficacy across various vehicle routing problem (VRP) variants. However, the application of these methods to the multi-trip time-dependent vehicle routing problem (MTTDVRP) with maximum working hours constraints—a pivotal element of urban logistics—remains largely unexplored. This paper introduces a DRL-based method called the Simultaneous Encoder and Dual Decoder Attention Model (SED2AM), tailored for the MTTDVRP with maximum working hours constraints. The proposed method introduces a temporal locality inductive bias to the encoding module of the policy networks, enabling it to effectively account for the time-dependency in travel distance/time. The decoding module of SED2AM includes a vehicle selection decoder that selects a vehicle from the fleet, effectively associating trips with vehicles for functional multi-trip routing. Additionally, this decoding module is equipped with a trip construction decoder leveraged for constructing trips for the vehicles. This policy model is equipped with two classes of state representations, fleet state and routing state, providing the information needed for effective route construction in the presence of maximum working hours constraints. Experimental results using real-world datasets from two major Canadian cities not only show that SED2AM outperforms the current state-of-the-art DRL-based and metaheuristic-based baselines but also demonstrate its generalizability to solve larger-scale problems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408221131",
    "type": "article"
  },
  {
    "title": "Optimizing Matching for On-Demand Ride-Pooling with Stochastic Day-to-Day Dynamics",
    "doi": "https://doi.org/10.1145/3721434",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Yaling Zhao; Lei Tang; Yunji Liang; Zeyu He; Junchi Ma",
    "corresponding_authors": "",
    "abstract": "Ride-pooling significantly reduces traffic congestion by enhancing fleet utilization through effective ride-matching. Real-world ride-pooling systems are dynamic, with fluctuations in driver availability and demand throughout the day. This necessitates adaptive ride-matching strategies that can quickly adjust to changing proximities and identify new carpooling opportunities by recalculating driver-rider correlations. However, most current methods primarily focus on static demand-supply scenarios and short-term accessibility, falling short in dynamic environment. In this study, we introduce a dynamic heterogeneous network model that captures the evolving nature of ride-pooling systems, where new requests and carpooling arrangements continuously emerge. We propose an embedding model-based matching decision process that operates online, adjusting to changes in the network’s structure. This process involves constructing a dynamic heterogeneous ride-pooling network that encompasses diverse node attributes and driver-rider connections, updating these representations to reflect the network’s evolution, and quickly identifying and ranking candidate riders for efficient online matching. Our approach demonstrates improved performance in offline evaluations using datasets from Austin, TX (RideAustin) and Chengdu, China (DiDi Chuxing). We observe a reduction in the necessary fleet size as new orders are placed, and an improvement in drivers’ matching probability compared to existing methods (e.g., an increase of 5.4–31.1% in the assignment rate on DiDi dataset), showcasing the advantage of employing dynamic network embedding to cut down on matching time (e.g., a decrease of 3.7–228.8 seconds in running time on DiDi dataset). Furthermore, we develop a simulated ride-pooling system (SRPool) that mimics dynamic demand-supply fluctuations and supports vehicle routing, providing a robust platform for evaluating ride-matching strategies. Our strategy not only excels in the SRPool environment but also effectively minimizes the total trip distance and rider waiting times.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408221172",
    "type": "article"
  },
  {
    "title": "Deep Disease Label-guided Graph Convolutional Network for Medical Report Generation",
    "doi": "https://doi.org/10.1145/3722226",
    "publication_date": "2025-03-10",
    "publication_year": 2025,
    "authors": "Liming Xu; Y Wang; Chunlin He; Quan Tang; Xianhua Zeng; Jiancheng Lv",
    "corresponding_authors": "",
    "abstract": "Medical report generation which extracts pathological information within medical images and subsequently produces diagnostic text autonomously aims to alleviate the workload of medical experts and offers auxiliary support in diagnoses. Despite some preliminary progress have been made, several limitations still persist, including lack of specificity in extracted visual features, insufficient consideration of cross-modal alignment and extensive preparatory work required for prior knowledge.To address these issues, we, in this paper, propose a novel deep label-guided graph convolutional network for medical report generation which utilizes disease label to guide to extract pathological information from medical images. To be specific, we firstly construct graph convolutional network to guide the model to extract the specific visual features based on disease labels, which allowing us to selectively extract disease specificity information resided in medical images. Then, we develop cross-modal alignment module to guide the alignment across medical image, diagnose report and disease label, which enables more accurate generation with more precise description. Besides, we build pre-constructed relational matrix to guide report generation model to learn the relationship between visual features and disease types with minimal additional workload to further reduce intensive workload. Extensive experiments on three benchmark datasets, i.e., IU X-ray, MIMIC-CXR, and COV-CTR, demonstrate that the proposed method outperforms the recent state-of-the-art medical report generation methods. Ours shows a 9.2% improvement in BLEU-4 score on the IU X-ray dataset, and both BLEU-4 and CIDEr scores improve by 6.31% on the MIMIC-CXR dataset. Additionally, the results show that it can be easily to applied and extended to medical image report generation with different modalities.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408287808",
    "type": "article"
  },
  {
    "title": "MA-GCL4SR: Improving Graph Contrastive Learning-Based Sequential Recommendation with Model Augmentation",
    "doi": "https://doi.org/10.1145/3722561",
    "publication_date": "2025-03-11",
    "publication_year": 2025,
    "authors": "Chunyan Sang; Ming Gong; Shi-Gen Liao; Wei Zhou",
    "corresponding_authors": "",
    "abstract": "Sequential recommendation (SR) has leveraged the advantages of graph contrastive learning (GCL) to enhance the representation of SR, which mitigates to some extent the constraint of scarce labeled data for supervision in SR. Existing work applies general graph data augmentation strategies to generate positive sample pairs, then further representation learning is conducted through a shared graph neural network. In this study, we identify limitations in applying traditional GCL to sequential recommendation: after the data augmentation, the shared graph neural network architecture used for feature learning fails to supply sufficiently diverse contrastive views, which are necessary to effectively identify and focus on the key information that is truly relevant for sequential recommendation. To ease this limitation, we propose a novel framework named Model Augmented Graph Contrastive Learning for Sequential Recommendation (MA-GCL4SR), which emphasizes modifying the internal architectures of the graph neural network through the use of model augmentation strategies, rather than focusing on making improvements during the data augmentation phase before encoding. Thereby, we construct a non-shared view encoder for SR, enriching the samples of user’s interaction sequences and strengthen the stability of the augmented sequence. Extensive experiments on four real-world datasets confirm the effectiveness of the proposed MA-GCL4SR paradigm, showcasing its consistent ability to elevate model performance across various real-world scenarios.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408302059",
    "type": "article"
  },
  {
    "title": "Representation Learning Based on Ordinary Differential Equations for Dynamic Networks",
    "doi": "https://doi.org/10.1145/3723359",
    "publication_date": "2025-03-14",
    "publication_year": 2025,
    "authors": "Dongjie Li; Dong Li; Guang Lian",
    "corresponding_authors": "",
    "abstract": "Representation learning on networks, mapping the network into a low-dimensional vector space, has received signification attention recently due to its widespread application in graph data mining tasks. With the success of representation learning in static networks, we push further for practical scenarios of dynamic networks. Existing methods model the dynamic network by dividing the dynamic network into sequences of network snapshots, see each network snapshot as a static network, and utilize the dynamic evolution between snapshots, they capture the discrete dynamic evolution of dynamic networks. However, a dynamic network continuously evolves over time. Capturing the continuously dynamic evolution of dynamic networks is important for dynamic network representation. In this paper, we regard a dynamic network as a dynamic system, use the Ordinary Differential Equation (ODE) to model the dynamic evolution of dynamic networks, and integrate the ODE over continuous-time to capture continuously dynamic evolution of dynamic networks; and design a new encoder-decoder model for dynamic networks representation. We improve the GRU module (only capturing the discrete dynamic evolution of the dynamic network and structure information) by combining an ODE and a Gated Recurrent Unit (GRU). The improved GRU as the encoder can learn the continuously dynamic evolution, and structure information of the dynamic network, where the ODE parameterized by a graph neural network models the continuously dynamic evolution of each network snapshot. Use the ODE and Inner-Productor as the decoder, where the ODE is integrated over continuous-time to learn the continuous dynamic evolution of the latent representation of the whole dynamic network, and the Inner-Productor reconstructs the topological structure of each snapshot by doing the inner-product between nodes representation, the reconstructing errors as the objective function of our method. To assess our model, we expand the experiment on several real-world dynamic networks, results show that our method consistently outperforms existing baselines in three dynamic link prediction tasks, the best is up to 6.54 \\(\\%\\) improvement. To our knowledge, our method is the first work using the ODE to capture the continuously dynamic evolution of dynamic networks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408449394",
    "type": "article"
  },
  {
    "title": "Non-Parallel Story Author-Style Transfer with Disentangled Representation Learning",
    "doi": "https://doi.org/10.1145/3726870",
    "publication_date": "2025-03-28",
    "publication_year": 2025,
    "authors": "Hongbin Xia; Xiangzhong Meng; Yuan Liu",
    "corresponding_authors": "",
    "abstract": "Non-parallel story author-style transfer is an important but challenging task in natural language process, which requires transferring an input story into another author style while maintaining source semantics. Despite recent progress, current text style transfer systems still face the challenges of low robustness of the model and low quality of the generated stories. To address these challenges, we propose an end-to-end framework incorporating dual encoder components and a fusion mechanism, which can achieve explicit style-content disentanglement and effectively fusing source-domain content with target-domain stylistic features. First, we extract text from source stories containing content information using empirical extraction rules and prompt engineering. And then, we propose a novel generation model which achieves story style transfer through capturing source content features and target style features and then fusing them. We use two additional training objectives to learn high-level discourse representations. Moreover, we have constructed a new data set for this task. Extensive experiments based on automatic and human evaluation show that our model significantly outperforms state-of-the-art baselines, achieving approximately 8.5% average improvement in comprehensive performance metrics, demonstrating the effectiveness of our model in story-style transfer.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408958597",
    "type": "article"
  },
  {
    "title": "Beyond Static Boundaries: Unraveling Temporal Overlapping Communities with Information Bottleneck Guidance",
    "doi": "https://doi.org/10.1145/3716391",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Moli Lu; Linhao Luo; Xiaofeng Zhang",
    "corresponding_authors": "",
    "abstract": "Community detection has gained significant research interest within the data mining field. It involves identifying subsets of nodes with dense internal connections and sparse external connections. Most studies on community detection focus solely on identifying non-overlapping communities in a static graph. However, in practice, communities often overlap, and the structure of the graphs is dynamically evolving. This dynamic nature leads to community changes and poses a significant challenge in detecting overlapping communities on temporal graphs (T-OCD). While graph neural networks have shown great performance in generating node representations for community detection, learning representations that capture temporal graph structures and support overlapping community detection remain an open question. To address these challenges, we present T-OCDIB , a novel approach for T emporal O verlapping C ommunity D etection guided by I nformation B ottleneck. Specifically, we first propose an overlapping community detection approach for static graphs, under the guidance of a community-oriented information bottleneck. This approach allows us to learn discriminative node representations specific to each community, facilitating the detection of overlapping communities. Following this, we extend this method to temporal graphs by presenting a temporal convolution module. This module uses adaptive weight matrices based on evolving graph structures to capture temporal dependencies for community detection. Additionally, to promote smooth transitions between consecutive communities, we introduce a temporal smoothing module to further constrain changes in community structure. We evaluate the proposed approach on both real-world and synthetic temporal networks. Experimental results illustrate the superiority of T-OCDIB over other community detection methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409078484",
    "type": "article"
  },
  {
    "title": "Fully dynamic clustering and diversity maximization in doubling metrics",
    "doi": "https://doi.org/10.1145/3727881",
    "publication_date": "2025-04-03",
    "publication_year": 2025,
    "authors": "Paolo Pellizzoni; Andrea Pietracaprina; Geppino Pucci",
    "corresponding_authors": "",
    "abstract": "We present approximation algorithms for some variants of \\(k\\) -center clustering and diversity maximization in a fully dynamic setting, where the active pointset evolves through arbitrary insertions and deletions. All algorithms employ a coreset-based strategy and rely on the use of the cover tree data structure, which we crucially augment to maintain, at any time, some additional information enabling the efficient extraction of the solution for the specific problem. For all the problems under consideration, our algorithms compute \\((\\alpha+\\varepsilon)\\) -approximate solutions, where \\(\\alpha\\) is the best known approximation attainable in polynomial time in the standard static setting, and \\(\\varepsilon&gt;0\\) is a user-provided accuracy parameter. Remarkably, and unlike previous works, the (cover tree) data structure used by our algorithms and the running times of the update procedures are both independent of the accuracy parameter \\(\\varepsilon\\) and, for the \\(k\\) -center variants, also of parameter \\(k\\) . The analysis is performed in terms of the doubling dimension of the metric space which the points belong to, and it shows that, for spaces of bounded doubling dimension, the times required to extract solutions to the above problems are dramatically smaller than those that would be required to recompute solutions on the entire active pointset from scratch. To the best of our knowledge, ours are the first solutions for the matroid-center and diversity maximization problems in the fully dynamic setting. The theoretical results are complemented by an extensive set of experiments, which demonstrate the efficiency and effectiveness of our algorithms for \\(k\\) -center without and with outliers against previously known ones.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409125412",
    "type": "article"
  },
  {
    "title": "MultiGraphMatch: a subgraph matching algorithm for multigraphs",
    "doi": "https://doi.org/10.1145/3728361",
    "publication_date": "2025-04-08",
    "publication_year": 2025,
    "authors": "Giovanni Micale; Antonio Di Maria; Roberto Grasso; Vincenzo Bonnici; Alfredo Ferro; Dennis Shasha; Rosalba Giugno; Alfredo Pulvirenti",
    "corresponding_authors": "",
    "abstract": "Subgraph matching is the problem of finding all the occurrences of a small graph, called the query, in a larger graph, called the target. Although the problem has been widely studied in simple graphs, few solutions have been proposed for multigraphs, in which two nodes can be connected by multiple edges, each denoting a possibly different type of relationship. In our new algorithm MultiGraphMatch, nodes and edges can be associated with labels and multiple properties. MultiGraphMatch introduces a novel data structure called bit matrix to efficiently index both the query and the target and filter the set of target edges that are matchable with each query edge. In addition, the algorithm proposes a new technique for ordering the processing of query edges based on the cardinalities of the sets of matchable edges. Using the CYPHER query definition language, MultiGraphMatch can perform queries with logical conditions on node and edge labels. We compare MultiGraphMatch with SuMGra and graph database systems Memgraph and Neo4J, showing comparable or better performance in all queries on a wide variety of synthetic and real-world graphs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409257295",
    "type": "article"
  },
  {
    "title": "Privacy-Preserving Machine Learning Based on Cryptography: A Survey",
    "doi": "https://doi.org/10.1145/3729234",
    "publication_date": "2025-04-15",
    "publication_year": 2025,
    "authors": "Congcong Chen; Lifei Wei; Jintao Xie; Yang Shi",
    "corresponding_authors": "",
    "abstract": "Machine learning has profoundly influenced various aspects of our lives. However, privacy breaches have caused significant unease and concern among the general public. Preserving the privacy of sensitive data during the training and inference phases of machine learning is a key challenge. Cryptography-based privacy-preserving machine learning (crypto-based PPML) offers a viable solution to this challenge. In this paper, we studied over 100 publications on crypto-based PPML frameworks published between 2016 and 2024, including 55 client-server architecture frameworks and 64 multi-party architecture frameworks. We provide a comprehensive overview of these frameworks, highlighting their features across various dimensions. Furthermore, we conduct an in-depth analysis, delving into scenarios, privacy goals, threat models, and optimization techniques that underpin these innovative solutions. We also discuss the challenges in the field of crypto-based PPML, including aspects of security &amp; privacy , efficiency , and availability &amp; usability . Finally, we offer an outlook on future research directions, aiming to provide valuable insights for both scholars and practitioners.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409475825",
    "type": "article"
  },
  {
    "title": "Rethinking Natural Language Generation with Layer-Wise Multi-View Decoding",
    "doi": "https://doi.org/10.1145/3729536",
    "publication_date": "2025-04-15",
    "publication_year": 2025,
    "authors": "Fenglin Liu; Xuancheng Ren; Guangxiang Zhao; Chenyu You; Shenghu Ma; Xian Wu; Wei Fan; Xu Sun",
    "corresponding_authors": "",
    "abstract": "In natural language generation, language models, particularly those based on decoder-only architectures as in popular large language models (LLMs), have demonstrated impressive performance across a wide range of tasks. However, encoder-decoder architectures remain highly effective for tasks involving non-text data, such as images and time-series data. The decoder relies on the attention mechanism to efficiently extract information from the encoder. While it is common practice to draw information from only the last encoder layer, this might lead to insufficient training of the encoder layer stack due to the hierarchy bypassing problem. In this work, we propose layer-wise multi-view decoding for improved encoder-decoder language models, where for each decoder layer, together with the representations from the last encoder layer, which serve as a global view, those from other encoder layers are supplemented for a stereoscopic view of the source inputs. Systematic experiments and analyses show that we successfully address the hierarchy bypassing problem, require almost negligible parameter increase, and substantially improve the performance of sequence learning with deep representations on six diverse tasks, i.e., machine translation, abstractive summarization, image captioning, video captioning, medical report generation, and paraphrase generation. In particular, our approach achieves new state-of-the-art results on ten benchmark datasets, including a low-resource machine translation dataset and two low-resource medical report generation datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409476112",
    "type": "article"
  },
  {
    "title": "Learning to Reduce the Scale of Large Graphs: A Comprehensive Survey",
    "doi": "https://doi.org/10.1145/3729427",
    "publication_date": "2025-04-15",
    "publication_year": 2025,
    "authors": "Hongjia Xu; Liangliang Zhang; Yao Ma; Sheng Zhou; Zhuonan Zheng; Jiajun Bu",
    "corresponding_authors": "",
    "abstract": "Graph data, prevalent across domains like social networks, biological systems, and recommendation systems, presents significant challenges due to its large scale and complex structure. The advent of Graph Neural Networks (GNNs) has revolutionized graph data mining by effectively capturing node dependencies and neighborhood information. However, the computational complexity of processing large-scale graphs remains a major hurdle, as real-world graphs often consist of millions or even billions of nodes and edges. Efficient techniques like message passing and sampling have helped mitigate this issue, but memory and processing constraints persist. A promising approach to addressing these challenges is learning to reduce the size of large-scale graphs while retaining essential information, thus facilitating faster and more efficient graph data mining tasks, such as graph condensation, reduction, coarsening, and summarization, etc. Despite the differences in terminology, approaches under these topics share the same motivation: to generate smaller yet informative graphs that can replace the original large-scale datasets. In this paper, we unify these approaches under the concept of Graph Scaling (GS) , highlighting the shared motivation across multiple topics. Alongside this definition, to clarify the question of what principles should be followed when scaling a graph and how a scaled graph was formulated, we propose a taxonomy to methodically categorize and understand existing methods. Moreover, by organizing the dataset and evaluation metrics, we aim to provide a more comprehensive understanding of the GS methods from a practical perspective. Moving forward, We delve into the limitations and challenges of GS methods, identifying the shortcomings and potential in the literature. Finally, we conclude this paper by outlining future directions and offering concise guidelines to inspire future research in this field. A full paper list and online resources about GS are available at https://github.com/Frostland12138/Awesome-Graph-Scaling .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409476199",
    "type": "article"
  },
  {
    "title": "FDphormer: Beyond Homophily with Feature-Difference Position Encoding",
    "doi": "https://doi.org/10.1145/3727882",
    "publication_date": "2025-04-15",
    "publication_year": 2025,
    "authors": "Dong Li; A. Hongyu Zhang; Huan Xiong; Junqi Gao; Biqing Qi",
    "corresponding_authors": "",
    "abstract": "Graph Transformers have garnered significant attention due to their ability to address the challenges of long-distance interactions in previous GNNs. However, most current graph Transformers face difficulties when dealing with heterophilic graphs. To investigate this issue, we first analyzed the distribution of attention weights for homophilic and heterophilic graphs. We discovered that heterophily interferes with the allocation of attention weights, leading to errors in node classification. Further investigation revealed that the root cause may be the difficulty of current graph Transformers in capturing the difference between the features of each node and its neighbors. To alleviate this issue, we propose a position encoding strategy called DiSP to better capture the feature difference, and introduce FDphormer, a new efficient and simple graph Transformer model based on DiSP. Additionally, we analyze the generalization error of existing graph Transformer models and provide an upper bound on the generalization error of current graph Transformers with the introduction of DiSP. Extensive experiments demonstrate that FDphormer not only outperforms state-of-the-art methods on diverse heterogeneous datasets but also exhibits competitive performance under homophily.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409478333",
    "type": "article"
  },
  {
    "title": "COLANet: Cross-Domain Recommender Systems with Latent Overlapping Items on Graph Neural Networks",
    "doi": "https://doi.org/10.1145/3730404",
    "publication_date": "2025-04-16",
    "publication_year": 2025,
    "authors": "Pongsakorn Jirachanchaisiri; Saranya Maneeroj; Atsuhiro Takasu",
    "corresponding_authors": "",
    "abstract": "Cross-domain recommender systems (CDRSs) enhance recommendations by transferring knowledge of overlapping users across two domains. Deep canonical correlation analysis (DCCA) shows promising results in CDRSs by maximizing correlations between representations of overlapping users, enabling cross-domain knowledge transfer that depends on the degree of relationship between domains. As a result, DCCA selectively shares only relevant knowledge, alleviating the problem of noisy representation found in traditional CDRSs, where they transfer knowledge regardless of the correlation strength between domains. Although DCCA is used for user transfer, item transfer, referring to the transfer of explicit knowledge of the same items between domains, is impossible due to the absence of overlapping items to facilitate direct knowledge transfer. Meanwhile, graph neural networks (GNNs) embed users and items from separate user and item graphs in each domain. Therefore, better representations are obtained from captured complex relationships and collaborative signals. To construct graphs of overlapping items, latent linkages among items between domains could be discovered by the neural topic model (NTM), forming new graphs representing the latent relationships. Therefore, COLANet, a GNN-based CDRS, is proposed to solve the DCCA limitation on item transfer by proposing the extraction of item representations that do not exist in another domain using latent characteristics. First, user-user graphs are constructed using user similarity, and the item-topic graph is constructed using latent topics learned from item descriptions with NTM. Hence, user and item graphs of each domain are constructed separately, preventing domain relationship misalignment. Second, these graphs are fed to GNN to obtain user and item representations. Third, these representations are fed to DCCA to transfer knowledge between user-user and item-item. Finally, correlated user and item representations of each domain are used to predict ratings. The experiments demonstrate that COLANet outperforms the baselines across four pairs of domains, including both similar and different domains.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409498913",
    "type": "article"
  },
  {
    "title": "An Systematic Study and Analysis of Graph Neural Networks Under Noise",
    "doi": "https://doi.org/10.1145/3733605",
    "publication_date": "2025-05-01",
    "publication_year": 2025,
    "authors": "Yufei Jin; Xingquan Zhu",
    "corresponding_authors": "",
    "abstract": "Graph neural networks (GNN) have shown superb performance in handling networked data, mainly attributed to their message passing and convolution process across neighbors. For most literature, the performance of GNNs is mainly reported based on noise-free data environments. No study has systematically evaluated GNNs’ performance under noise. In this paper, we carry out an empirical study and theoretical analysis of four types of GNNs, including graph convolutional networks (GCN), graph attention networks (GAT), graph contrastive networks (GCL), and graph Unifilter under three types of noise, including attribute noise, structure noise, and label noise. Our study shows that GNNs behave tremendously differently in response to different types of noise. Overall, GAT is the most noise vulnerable and sensitive, whereas GCL is the most noise resilient. We further carry out theoretical analysis to explain the reason causing GAT to be sensitive to noise, and propose a solution to enhance its noise resilience. Our study brings in-depth firsthand knowledge of GNNs under noise for researchers and practitioners to better utilize GNNs in real-world applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410008090",
    "type": "article"
  },
  {
    "title": "Online Learning for Noisy Labeled Streams",
    "doi": "https://doi.org/10.1145/3734875",
    "publication_date": "2025-05-08",
    "publication_year": 2025,
    "authors": "Jinjie Qiu; Shengda Zhuo; Philip S. Yu; Chang‐Dong Wang; Shuqiang Huang",
    "corresponding_authors": "",
    "abstract": "Online learning, characterized by its feature space’s adaptability over time, has emerged as a flexible learning paradigm that has attracted widespread attention. However, existing online learning methods often overlook the distributional differences between instances and the presence of label noise in streaming data, thus significantly hindering the effectiveness and robustness of these algorithms. To overcome these challenges, we propose an online confidence learning algorithm for noisy labeled features, which aims to achieve robustness against arbitrary data streams and noisy labels. It employs two new strategies: online confidence inference, which applies the principle of empirical risk minimization to identify inconsistencies in spatial distributions, and geometric structure learning, which utilizes dynamic instance confidence to compute disparities between instances and their labels. Empirical findings demonstrate that our label correction mechanism enhances classification accuracy more effectively across various types of noisy labels ( i.e. , symmetric, asymmetric, and flipped). Additionally, a case study on image datasets was conducted to illustrate in detail the effectiveness of our OLNLS algorithm. Code is released in https://github.com/Zhuosd/OLNLS",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410192481",
    "type": "article"
  },
  {
    "title": "Communities in Streaming Graphs: Small Space Data Structure, Benchmark Data Generation, and Linear Algorithm",
    "doi": "https://doi.org/10.1145/3735976",
    "publication_date": "2025-05-15",
    "publication_year": 2025,
    "authors": "Shubham Gupta; Suman Kundu",
    "corresponding_authors": "",
    "abstract": "Identifying and preserving community structures in a streaming graph is a very challenging task. However, many applications require the identification of these communities in very limited space and time. In this paper, we design Community Sketch, a small space data structure that efficiently preserves communities. On query, it provides communities in constant time. With the use of community sketch data structure, a linear streaming community detection algorithm is proposed. Experimental results on the large real-world networks show that our algorithm outperforms other state-of-the-art algorithms in terms of quality metrics (NMI, F1-score, and WCC). Further, we propose an algorithm to produce benchmark network, namely, Temporal Community Benchmark Dataset (TCBD) which contains both true community labels and temporal information of edges. These synthetic networks are used to validate the proposed algorithm. 1",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410397901",
    "type": "article"
  },
  {
    "title": "Mining Linguistic Styles in Bilateral Matching: A Contrastive Learning Approach to Reciprocal Recommendation",
    "doi": "https://doi.org/10.1145/3736418",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Yue Guan; Yumei He; Ni Huang; Xunhua Guo; Guoqing Chen",
    "corresponding_authors": "",
    "abstract": "Reciprocal recommendation systems are crucial for online dating platforms to provide quality matches and reduce choice overload. However, the design of reciprocal recommendation systems grapples with the challenges of estimating interpersonal compatibility and predicting the likelihood that two prospective partners will accept each other. Furthermore, despite the crucial role of users’ linguistic styles in determining user match decision-making, the contemporary design of such recommendation systems has not yet effectively incorporated this information. To bridge these gaps, we develop an end-to-end personalized Linguistic Style Matching-based Reciprocal Recommendation System (LS-RRS). We propose cross-user and within-user contrastive learning strategies combined with random masking to extract users’ linguistic styles, and further integrate visual and textual information using an efficient convolution block. LS-RRS further models the matching probability using a conditional probability function and introduces a preference inflation factor on the receiver side to account for the asymmetric roles of the bilateral sides. The proposed model addresses the challenge of incorporating users’ linguistic styles into reciprocal recommendation and details the modeling of the two-stage matching process. Extensive experiments show that LS-RRS outperforms state-of-the-art models in recommendation performance, with a 29.35% increase in NDCG@10 when incorporating linguistic styles. Our follow-up analyses further validate the importance and effectiveness of the linguistic style extraction design through word level and sentence level visualizations, as well as qualitative case studies. This research contributes to the literature on reciprocal recommendation and offers a viable solution for alleviating user choice overload on online dating platforms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410512967",
    "type": "article"
  },
  {
    "title": "Exploring and Exploiting Data Heterogeneity in Recommendation",
    "doi": "https://doi.org/10.1145/3737290",
    "publication_date": "2025-05-27",
    "publication_year": 2025,
    "authors": "Zimu Wang; Jiashuo Liu; Hao Zou; Xingxuan Zhang; Yue He; Dongxu Liang; Peng Cui",
    "corresponding_authors": "",
    "abstract": "Massive amounts of data are the foundation of data-driven recommendation models. As an inherent nature of big data, data heterogeneity widely exists in real-world recommendation systems. It reflects the differences in the properties among sub-populations. Ignoring the heterogeneity in recommendation data could mislead the models, hurt the sub-populational robustness and finally limit the performance of recommendation models. However, data heterogeneity has not received substantial attention within the recommendation community, prompting us to adequately explore and exploit data heterogeneity to solve these challenges and enhance data analysis. In this study, we specifically focus on two representative categories of heterogeneity in recommendation data: heterogeneity of prediction mechanism and covariate distribution. To explore the data heterogeneity, we propose an algorithm based on bilevel clustering. Additionally, we demonstrate how the explored data heterogeneity can be exploited for prediction and debias in recommendation scenarios, specifically by building models using multiple sub-models and augmenting the propensity score estimation. Extensive experiments conducted on real-world data substantiate the existence of heterogeneity in recommendation data and validate the effectiveness of exploring and exploiting data heterogeneity in improving recommendation performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410772121",
    "type": "article"
  },
  {
    "title": "What are anomalies in a network?",
    "doi": "https://doi.org/10.1145/3723007",
    "publication_date": "2025-05-27",
    "publication_year": 2025,
    "authors": "Kai Ming Ting; Zhong Zhuang; Guansong Pang; Zongyou Liu; Tang Liang; Qiujun Zhao",
    "corresponding_authors": "",
    "abstract": "This paper examines a collection of assumptions used in the current literature on node anomaly detection in a network. The examination raises the question: What are anomalies in a network? Our attempt to answer this question has provided some interesting findings and led to some open questions. This is the first paper which formally defines anomalies in a network, and introduces the concept of self-verifiability of a detector without ground-truths in a network. They enable existing detectors to be categorized into two types along the line whether they are self-verifiable or not. We suggest a method to evaluate self-verifiable detectors without ground-truths, as an alternative to the existing evaluation method that relies on ground-truths.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410772233",
    "type": "article"
  },
  {
    "title": "Medical Federated Learning with Improved Representation and Personalized Aggregation",
    "doi": "https://doi.org/10.1145/3737649",
    "publication_date": "2025-05-28",
    "publication_year": 2025,
    "authors": "Qinghe Liu; Mengya Jiang; Pin Wang; Hongli Xu; Rilige Wu; Zhenfeng Zhu; Xinwang Liu; Yawei Zhao; Kunlun He",
    "corresponding_authors": "",
    "abstract": "Federated learning is a promising bridge that connects machine learning methods and multi-central medical data. It trains models using the local data, and protects the privacy of data. There are many methods for federated learning to aggregate models, especially personalized methods, which show relatively excellent performance. However, most of them excessively pay attention to global and local information while ignoring the random components during aggregating. That limits their performance in metrics like accuracy, specificity and sensitivity. We propose a method (denoted by FedDiv) to make a balance between these metrics. The basic idea is to extract centralized features meanwhile filtering random components, and conduct personalized aggregation. These centralized features draw encoders’ attention, which enhances the performance of personalized models in specificity and sensitivity. Besides, they contain more global and local information, which is advantageous for personalized aggregation. Meanwhile, our personalized method preserves the local information as far as possible during aggregating models. These local information are the critical factor for the personalized models to perform better in accuracy. Finally, we validate this method in 3 public and 1 private medical datasets. Comparing with 14 federated methods, our method achieves the best performance in metrics including accuracy, specificity, sensitivity and F1 score.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410818785",
    "type": "article"
  },
  {
    "title": "Subgraph Federated Learning with Information Bottleneck Constrained Generative Learning",
    "doi": "https://doi.org/10.1145/3737879",
    "publication_date": "2025-05-30",
    "publication_year": 2025,
    "authors": "Shangyang Li; Jiayan Guo",
    "corresponding_authors": "",
    "abstract": "Federated Learning (FL) is a groundbreaking approach that enables multiple clients to jointly train deep learning models by pooling their data, while addressing privacy and bandwidth issues that prevent direct data sharing. This approach is particularly suitable for building strong and widely applicable graph models, given the increasing amounts of graph data stored across different locations. However, FL for subgraph models faces significant challenges, such as the diversity of data and the risk of attacks, which can affect the strength and reliability of these models. In response to these challenges, our research delves into the complexities of FL for subgraphs from an information theory perspective. We identify a major issue that affects the performance of graph models: the bias in the optimization goal of the commonly used FedAVG training method. To address this, we propose InfoFedGNN, an innovative FL framework for subgraphs that is based on the Information Bottleneck principle. InfoFedGNN is designed to overcome the problem of non-identically distributed (non-i.i.d.) data in FL and to significantly improve its defense against security threats. Our thorough evaluation of InfoFedGNN on five public datasets, with both uniform and diverse data distributions, highlights its improved defense capabilities and better training outcomes. These results confirm the effectiveness of InfoFedGNN in enhancing the security and efficiency of FL, demonstrating its potential to push forward the development of federated graph models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410890183",
    "type": "article"
  },
  {
    "title": "Deep Code Search with Naming-Agnostic Contrastive Multi-view Learning",
    "doi": "https://doi.org/10.1145/3737878",
    "publication_date": "2025-05-30",
    "publication_year": 2025,
    "authors": "Jiadong Feng; Wei Li; Shing Trong Wu; Zhao Wei; Yong Xu; Juhong Wang; Hui Li",
    "corresponding_authors": "",
    "abstract": "Software development is a repetitive task, as developers usually reuse or get inspiration from existing implementations. Code search, which refers to the retrieval of relevant code snippets from a codebase according to the developer’s intent that has been expressed as a query, has become increasingly important in the software development process. Due to the success of deep learning in various applications, a great number of deep learning based code search approaches have sprung up and achieved promising results. However, developers may not follow the same naming conventions and the same variable may have different variable names in different implementations, bringing a challenge to deep learning based code search methods that rely on explicit variable correspondences to understand source code. To overcome this challenge, we propose a naming-agnostic code search method (NACS) based on contrastive multi-view code representation learning. NACS strips information bound to variable names from Abstract Syntax Tree (AST), the representation of the abstract syntactic structure of source code, and focuses on capturing intrinsic properties solely from AST structures. We use semantic-level and syntax-level augmentation techniques to prepare realistically rational data and adopt contrastive learning to design a graph-view modeling component in NACS to enhance the understanding of code snippets. We further model ASTs in a path view to strengthen the graph-view modeling component through multi-view learning. Extensive experiments show that NACS provides superior code search performance compared to baselines and NACS can be adapted to help existing code search methods overcome the impact of different naming conventions. Our implementation is available at https://github.com/KDEGroup/NACS .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410890665",
    "type": "article"
  },
  {
    "title": "DisambiguART: A Neural-based Inference Model for Knowledge Graph Disambiguation",
    "doi": "https://doi.org/10.1145/3737880",
    "publication_date": "2025-05-30",
    "publication_year": 2025,
    "authors": "Budhitama Subagdja; D Shanthoshigaa; Ah‐Hwee Tan",
    "corresponding_authors": "",
    "abstract": "One main challenge in constructing a knowledge graph (KG) is to deal with ambiguity. Specifically, an entity in the graph can be assigned with multiple meanings while two or more entities considered to have different meanings may actually be the same. Assigning an entity with the correct meaning may involve re-evaluation of its relevant contexts. This costly operation typically involves searching for other similar entities within the KG such that the context can be determined. In this paper, a new model called DisambiguART is proposed leveraging multi-channel matching and inference in a self-organizing neural network for sense disambiguation in knowledge graphs. Unlike other disambiguation methods that rely on representation learning to identify the relevant contexts whereby similarities among entities are learned, DisambiguART extends the working principle of multi-channel Adaptive Resonance Theory (ART) to conduct inferences directly over the graph representation through bi-directional interactions of bottom-up activations and top-down matching to find similar entities and select the correct meaning according to the right context. The proposed method is evaluated on the tasks of entity sense disambiguation in three domain KGs (jet-engine, biomedical, and kinship) and author-name disambiguation in bibliographic KGs, demonstrating the effectiveness and efficiency of DisambiguART against the state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410891683",
    "type": "article"
  },
  {
    "title": "CAT-LLM: Style-enhanced Large Language Models with Text Style Definition for Chinese Article-style Transfer",
    "doi": "https://doi.org/10.1145/3744250",
    "publication_date": "2025-06-10",
    "publication_year": 2025,
    "authors": "Zhen Tao; Dinghao Xi; Zhiyu Li; Liumin Tang; Wei Xu",
    "corresponding_authors": "",
    "abstract": "Text style transfer plays a vital role in online entertainment and social media. However, existing models struggle to handle the complexity of Chinese long texts, such as rhetoric, structure, and culture, which restricts their broader application. To bridge this gap, we propose a C hinese A rticle-style T ransfer ( CAT-LLM ) framework, which addresses the challenges of style transfer in complex Chinese long texts. At its core, CAT-LLM features a bespoke pluggable T ext S tyle D efinition ( TSD ) module that integrates machine learning algorithms to analyze and model article styles at both word and sentence levels. This module acts as a bridge, enabling large language models (LLMs) to better understand and adapt to the complexities of Chinese article styles. Furthermore, it supports the dynamic expansion of internal style trees, enabling the framework to seamlessly incorporate new and diverse style definitions, enhancing adaptability and scalability for future research and applications. Additionally, to facilitate robust evaluation, we created ten parallel datasets using a combination of ChatGPT and various Chinese texts, each corresponding to distinct writing styles, significantly improving the accuracy of the model evaluation and establishing a novel paradigm for text style transfer research. Extensive experimental results demonstrate that CAT-LLM, combined with GPT-3.5-Turbo, achieves state-of-the-art performance, with a transfer accuracy F1 score of 79.36% and a content preservation F1 score of 96.47% on the “Fortress Besieged” dataset. These results highlight CAT-LLM's innovative contributions to style transfer research, including its ability to preserve content integrity while achieving precise and flexible style transfer across diverse Chinese text domains. Building on these contributions, CAT-LLM presents significant potential for advancing Chinese digital media and facilitating automated content creation. Source code is available at GitHub 1 .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411176024",
    "type": "article"
  },
  {
    "title": "Understanding the Robustness of Deep Recommendation under Adversarial Attacks",
    "doi": "https://doi.org/10.1145/3744570",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Wenjie Zheng; Wenbin Chen; Hai Chen; Yan Cui; Shu Zhao; Yanping Zhang",
    "corresponding_authors": "",
    "abstract": "It has been shown that deep recommendation models are susceptible to adversarial attacks, with this vulnerability potentially leading to significant economic losses in the e-commerce field. However, the robustness of deep recommendation models in response to adversarial attacks has not been systematically investigated. In this paper, therefore, we comprehensively evaluate the adversarial robustness of various representative deep models in different settings, aiming to analyze their performance impact under adversarial attacks and compare it with traditional collaborative filtering models. Notably, we examine poisoning attacks under different proportions of fake users and various popularity conditions to understand why certain deep recommendation models perform exceptionally or sub-optimally. On this basis, we further proposed practical robustness improvement strategy for the problems found in the evaluation and fully verified it through rigorous experiments. Key findings include: 1) the sparser the training dataset, the weaker the robustness of a recommendation model's performance under adversarial attacks; 2) deep recommendation models exhibit greater robustness in recommending popular items under adversarial attacks, while they are more vulnerable when attacked with non-popular items; 3) the robustness of deep recommendation models is not consistently weaker than that of traditional collaborative filtering models across all attack settings. These findings highlight the security concerns in deep recommendation systems and contribute to developing more reliable models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411371226",
    "type": "article"
  },
  {
    "title": "Causal Meta-learning with Multi-view Graphs for Cold-start Recommendation",
    "doi": "https://doi.org/10.1145/3732943",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Huiting Liu; Wei Zhang; Peipei Li; Peng Zhao; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Cold-start recommendation is a well-known problem in practical application scenarios. Generating reliable recommendations can be challenging when interactions are typically sparse. To mitigate the cold-start problem, some methods incorporate auxiliary information about users and items, and others adopt meta-learning to improve recommendation accuracy. However, these approaches overlook the fact that items are interdependent and likely to be related or similar. Moreover, user preference distributions in the meta-training and meta-testing phases are different in the cold-start scenario. To address these problems, we present a novel strategy called Causal Meta-learning with Multi-View Graphs (CausalMMG). Specifically, we first construct multi-view item-item graphs to explore the correlations and similarities between items from multiple perspectives. A multi-view item representer is then used to learn item representations, exploiting graph convolution neural networks to capture the structure of these different item-item graphs. We then resort to the structural causal models of causal inference and further develop a causality-enhanced bi-level adaptive meta-learner to eliminate bias caused by the different distributions of user preferences. Moreover, the meta-learner learns the user preferences for items in different orders through hierarchical and task-level adaptations. Finally, we evaluate CausalMMG on several real-world datasets, demonstrating its effectiveness in various scenarios. The results show that the proposed CausalMMG is significantly superior to competitive baseline methods for cold-start recommendation on all datasets, highlighting the importance of incorporating the multiple relationships between items and modeling different user preference distributions in recommender systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411371305",
    "type": "article"
  },
  {
    "title": "Raker: A Relation-aware Knowledge Reasoning Model for Inductive Relation Prediction",
    "doi": "https://doi.org/10.1145/3745029",
    "publication_date": "2025-06-20",
    "publication_year": 2025,
    "authors": "Jiaqi Wang; Wengen Li; Yulou Shu; Jihong Guan; Yichao Zhang; Shuigeng Zhou",
    "corresponding_authors": "",
    "abstract": "Inductive relation prediction, an important task for knowledge graph completion, is to predict the relations between entities that are unseen at the training stage. The latest methods use pre-trained language models (PLMs) to encode the paths between the head entity and tail entity, and achieve state-of-the-art prediction performance. However, these methods cannot handle no-path scenarios well and lack the capability to learn comprehensive relation representations for distinguishing different relations. To tackle this issue, we propose a novel R elation- a ware k nowledg e r easoning model entitled Raker which introduces an adaptive reasoning information extraction method to identify relation-aware reasoning neighbors of entities in the target triple to handle no-path scenarios, and enables the PLM to better distinguish different relations via the relation-specific soft prompting. Raker is evaluated on three public datasets and achieves SOTA performance in inductive relation prediction when compared with the baseline methods. Notably, the absolute improvement of Raker is even more than 5% on the FB15k-237 dataset in the inductive setting. Moreover, Raker also demonstrates the superiority in transductive, few-shot, and unseen relations settings. The code of Raker is available at https://github.com/ADMIS-TONGJI/Raker .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411495249",
    "type": "article"
  },
  {
    "title": "Towards Recommendation on Good Quality Data Science Solutions",
    "doi": "https://doi.org/10.1145/3746235",
    "publication_date": "2025-06-26",
    "publication_year": 2025,
    "authors": "Jian Chen; Yile Chen; Zeyi Wen; Y. Chen; Jin Huang",
    "corresponding_authors": "",
    "abstract": "Data science aims to solve real-world problems with the knowledge derived from data. Successfully tackling a data science problem requires practitioners to choose an appropriate solution, which potentially comprises various components such as pre-processing techniques, learning algorithms, hyper-parameters, and so on. Therefore, a problem-driven recommendation for the promising solution is invaluable, as it facilitates efficient and convenient problem-solving. However, existing solution recommendation approaches confront notable challenges when dealing with limited and sparse prior experience in practical applications. Learning from such prior easily leads to overfitting and poor generalization in solution recommendations. To address this issue, we propose a novel solution recommendation method that can predict a good-quality data science solution, including the pre-processing, the learning algorithm, and hyper-parameters, for a given problem. The foundation of our method is a carefully designed ranking model that exploits a weight-sharing structure and a newly proposed loss. The ranking model focuses on incorporating relative ranking information into the predicted performance score of each solution. With these techniques, our method can recommend the solution with the highest score and effectively mitigate the limitations of using sparse prior experience. Our experiments demonstrate the superiority of our method in predicting solutions with higher accuracy and rank, even trained on highly sparse historical performance records. It also reduces recommendation time significantly compared to the baselines, offering remarkable efficiency and convenience for practitioners.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411673865",
    "type": "article"
  },
  {
    "title": "A Propagation Model of Derived Topic Based on Cognitive Accumulation and Transfer Learning",
    "doi": "https://doi.org/10.1145/3747187",
    "publication_date": "2025-07-03",
    "publication_year": 2025,
    "authors": "Qian Li; Wen-Long Zhang; Bojian Hu; Tun Li; Rong Wang; Shihong Wei; Yunpeng Xiao",
    "corresponding_authors": "",
    "abstract": "The propagation of hot topics often gives rise to a series of derivative topics. In view of the sparsity of user behavior data and the cognitive accumulation of the original topic, a prediction model of derived topic propagation based on cognitive accumulation and transfer learning is proposed. First, for the complexity of the derived topic feature space. Considering the relation and difference between derivative topics and original topics, this study designs I(Iterative)T(Topic)2vec, a topic iterative representation method based on original topics to get the low - dimensional representation of the derived topic feature space more richly from the perspectives of both original topics and derivative topics. Secondly, it aims at the problem of users’ cognitive accumulation of the original topic before the outbreak of derivative topic. The subjective game theory is introduced to construct the cognitive influence of users. At the same time, considering the timeliness of the propagation cycle of derivative topics, we discretized the derivative topic data, and further proposed a derivative topic propagation model based on SA-CNN (Subjective Adapt-CNN). Finally, the sparsity of effective behavior data of users at the beginning of the outbreak of derivative topics is discussed. Considering the rich user behavior data in the communication history of the original topic, data migration is carried out by using the original topic. At the same time, the domain adaptive method based on TCA (Transfer Component Analysis) is introduced to achieve feature adaptation from the original topic data to the derived topic data, further improving the accuracy of the derived topic propagation model. Experiments show that this model can not only effectively alleviate the problem of data sparsity, but also perceive the propagation situation of derived topics well.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412013258",
    "type": "article"
  },
  {
    "title": "Graph Self-attention Mechanism for Interpretable Multi-hop Knowledge Graph Link Prediction",
    "doi": "https://doi.org/10.1145/3737702",
    "publication_date": "2025-07-07",
    "publication_year": 2025,
    "authors": "Hao Liu; Dong Li; Bing Zeng; Wei Liang; Dongjie Li",
    "corresponding_authors": "",
    "abstract": "Knowledge graphs (KGs) are extensively used in recommendation systems and information retrieval but often suffer from incompleteness. A popular solution to this problem is multi-hop inference through a reinforcement learning framework, which provides an interpretable path for predicting missing links in KGs. Most previous work focuses on improving the performance of multi-hop link prediction. However, it has been observed that many multi-hop paths generated by these methods are irrational; they often fail to reasonably explain the predicted answer entities. To address this challenge, we introduce the J oint M ulti-hop L ink P rediction framework (JMLP). The framework consists of a relation attention network and an entity attention network, which collaboratively generate the reasoning paths. The relation attention module utilizes an induction network to encode historical paths and employs the graph self-attention mechanism to refine the interaction of relation contextual information. The entity attention module uses the graph attention mechanism to obtain the aggregated contextual features and leverages self-attention to strengthen the correlation between local and global contextual entity features. Extensive experiments on five datasets validate the effectiveness of our approach, demonstrating significant improvements both in predictive performance and interpretability compared to state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412070790",
    "type": "article"
  },
  {
    "title": "Weakly Supervised Open-Domain Aspect-Based Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3747849",
    "publication_date": "2025-07-09",
    "publication_year": 2025,
    "authors": "Mohna Chakraborty; Adithya Kulkarni; Qi Li",
    "corresponding_authors": "",
    "abstract": "Aspect-Based Sentiment Analysis (ABSA) comprises several subtasks: aspect term extraction (ATE), opinion term extraction (OTE), aspect term sentiment extraction (ATSE), aspect-opinion pair extraction (AOPE), and aspect sentiment triplet extraction (ASTE). Existing unified frameworks for ABSA rely heavily on large-scale annotated data, limiting scalability across domains. We propose UAOS, a double-layer unified span extraction framework that performs all five ABSA subtasks under weak supervision. Our approach first extracts aspect-opinion pairs using universal dependency-based rules from unannotated corpora. Sentiment labels for these pairs are generated via a novel zero-shot, domain-agnostic prompt-based method. The resulting weak labels train a unified span extraction architecture equipped with canonical correlation analysis for early stopping and a self-training mechanism to mitigate noise and bias in supervision. Extensive experiments on four ABSA benchmarks demonstrate that UAOS achieves competitive or superior performance compared to fully supervised baselines. It improves upon the state-of-the-art ODAO by +1.54 F1 for ATE, +0.56 for OTE, and +0.82 for AOPE. In ATSE and ASTE, where no weakly supervised baselines exist, UAOS outperforms several supervised models, setting new benchmarks. To assess domain generalizability, we evaluate UAOS on a psychology/education-domain dataset of student reflections spanning four instructional conditions. Without in-domain fine-tuning, it achieves macro F1 scores of 71.05 (ATE), 74.39 (OTE), 68.24 (AOPE), and 60.56 (ASTE). These results highlight the model's ability to generalize to out-of-distribution, non-commercial text, underscoring its scalability for low-resource ABSA applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412119872",
    "type": "article"
  },
  {
    "title": "Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies and Opportunities",
    "doi": "https://doi.org/10.1145/3748259",
    "publication_date": "2025-07-11",
    "publication_year": 2025,
    "authors": "Hanchen Yang; Jiannong Cao; Wengen Li; Shuyu Wang; Hui Li; Jihong Guan; Shuigeng Zhou",
    "corresponding_authors": "",
    "abstract": "With the rapid amassing of spatial-temporal (ST) ocean data, many spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, including climate forecasting and disaster warning. Compared with typical ST data (e.g., traffic data), ST ocean data presents some unique characteristics, e.g., diverse regionality and high sparsity. These characteristics make it difficult to design and train STDM models on ST ocean data. To the best of our knowledge, a comprehensive survey of existing studies remains missing in the literature, which hinders not only computer scientists from identifying the research issues in ocean data mining but also ocean scientists to apply advanced STDM techniques. In this paper, we provide a comprehensive survey of existing STDM studies for ocean science. Concretely, we first review the widely-used ST ocean datasets and highlight their unique characteristics. Then, typical ST ocean data quality enhancement techniques are discussed. Next, we classify existing STDM studies for ocean science into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate the techniques for these tasks. Finally, promising research opportunities are discussed. This survey can help scientists from both computer science and ocean science better understand the fundamental concepts, key techniques, and open challenges of STDM for ocean science.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412198931",
    "type": "article"
  },
  {
    "title": "New Automated Approach to Selection of Mapper Clustering Parameters",
    "doi": "https://doi.org/10.1145/3746065",
    "publication_date": "2025-07-11",
    "publication_year": 2025,
    "authors": "Padraig Fitzpatrick; Anna Jurek-Loughrey; Paweł Dłotko",
    "corresponding_authors": "",
    "abstract": "Topological methods have recently gained traction as powerful tools for extracting insights from high-dimensional data, forming the foundation of an approach known as Topological Data Analysis (TDA). Among the key developments in TDA is the Mapper algorithm, which constructs graph-based representations of complex datasets, capturing their topological structure at a user-defined resolution. The Mapper algorithm has shown promise across various applications, particularly in biomedical data analysis. However, its application requires careful selection of several parameters, especially the clustering algorithm and its settings. Without prior knowledge and a deep understanding of the data, these choices are non-trivial and can be a major barrier for researchers aiming to leverage Mapper effectively. In this work, we introduce enhancements to the Mapper algorithm to address this challenge. Specifically, we investigate the integration of ensemble learning (EL) techniques into Mapper's graph construction to eliminate the need for arbitrary parameter selection. Additionally, we propose a data-driven criterion for selecting the clustering method best suited to the Mapper algorithm. Our experimental results demonstrate that the proposed approach enables the construction of Mapper graphs that accurately capture the underlying structure of the input data, all without manual parameter tuning.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412199137",
    "type": "article"
  },
  {
    "title": "Growth Scale-Free Networks by Various Generative Ways",
    "doi": "https://doi.org/10.1145/3748512",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Fei Ma; Ping Wang",
    "corresponding_authors": "",
    "abstract": "In this paper, the popularly discussed topic, i.e., how to construct available theoretical networked models that certainly capture some structural features popularly observed on realistic networks, is still our focus. Specifically, we first propose an evolving deterministic network \\(N(t)\\) using three types of growth ways. Then, we study some topological structural parameters including degree distribution, diameter and clustering coefficient on network \\(N(t)\\) . The results demonstrate that the proposed network has scale-free feature and small-world property. In the meantime, we obtain an interesting finding, i.e., the first handshake between Fibonacci series and the “pure” preferential attachment mechanism. Next, we enumerate spanning trees on network \\(N(t)\\) , and derive the closed-form solution of spanning trees number. Secondly, we introduce randomness into the growth process of network \\(N(t)\\) to further establish evolving stochastic networks \\(\\mathfrak{N}(t)\\) that follow the same degree distribution as network \\(N(t)\\) , and also determine some topological structural parameters so as to investigate effect of randomness on structural properties. We show analytically that such a randomization approach makes the resulting stochastic networks not only to greatly inherit some fundamental structural properties from deterministic network \\(N(t)\\) , but also to considerably improve the robustness of network when encountering deliberate removal of edge. Lastly, we list out some open problems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412400227",
    "type": "article"
  },
  {
    "title": "Topology Augmented Multi-Band and Multi-Scale Filtering for Graph Anomaly Detection",
    "doi": "https://doi.org/10.1145/3748727",
    "publication_date": "2025-07-16",
    "publication_year": 2025,
    "authors": "Jingyuan Zhang; Lei Yu; Zhirong Huang; Li Yang; Fengjun Zhang",
    "corresponding_authors": "",
    "abstract": "Graph Anomaly Detection (GAD) has gained significant attention in areas such as financial risk control and social network security, becoming a critical research problem. Vanilla graph neural networks (GNNs), as a popular method for graph modeling, are known to perform poorly in GAD due to the assumption of homophily preferences. This paper argues that the issue lies in the insufficient feature extraction ability caused by their single filtering property (low-pass filtering), and revealing the effectiveness of multi-band filtering to deal with GAD. From this, we note two other overlooked issues: 1) How can multi-band band-pass filtering further fuse multi-scale neighborhood information? 2) Adaptation between raw attributes of nodes and graph filters (graph topology). The former bridges the respective advantages of spectral domain and spatial domain, and the latter is an important bottleneck for the encoding capacity of the filters. To address these, we propose a new graph anomaly detection method, Graph Perturbed Networks (GraphPN). Each hidden layer of GraphPN is a band-pass filter, enabling multi-band and multi-scale filtering through simple stacking and skip connections. We analyze its spectral locality and spatial locality to provide theoretical support. Additionally, GraphPN is supplemented with a tailored feature activation module to complete the adaptation of the above two. This module readjusts node indices and decouples graph convolution, introducing rich topological information to node attributes. In addition to further enhancing detection performance, another possibly counter-intuitive effect is that the distinguishability of the two classes of nodes is improved even before filtering. The proposed method performs well in real-world datasets compared with the current state-of-the-art baselines, which fully demonstrates its superiority. Codes are available at https://github.com/Thankstaro/GraphPN .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412476108",
    "type": "article"
  },
  {
    "title": "Integrating Heterogeneous Graph Attention Network With Label Propagation for Detecting Spammer Groups on E-commerce Platforms",
    "doi": "https://doi.org/10.1145/3749846",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "X Li; Peng Zhang; Ru Ma; Chenghang Huo; Fuzhi Zhang",
    "corresponding_authors": "",
    "abstract": "The collusive fraudulent behaviors on e-commerce platforms lead to proliferation of fraudulent reviews, which disrupt fair competition among merchants and mislead consumers’ shopping decisions. Detection of spammer groups helps purify the e-commerce environment and enhances consumers’ shopping experience. However, existing graph-based methods for detecting spammer groups first learn user node vector representations from the graph, and then use clustering methods to obtain candidate groups. Such separate two-stage detection methods are difficult to obtain high-quality candidate groups, resulting in suboptimal detection performance. Additionally, current graph construction methods used in spammer group detection do not fully consider the characteristics of spammer groups, which limits the detection performance. Aiming these concerns, we integrate heterogeneous graph attention network (HGAN) with label propagation (LP) for detecting spammer groups. First, we build a heterogeneous weighted directed (HWD) graph by analyzing the dataset and assign an initial label to each node. Then, we integrate a HGAN-module with a LP-module to obtain the HWD graph's node embeddings and simultaneously generate candidate groups. We enhance the quality of embeddings and groups through the collaborative optimization between the predicted labels obtained from the HGAN-module and the pseudo-labels obtained from the LP-module. Finally, we calculate the suspiciousness values of groups using the reconstruction loss of the autoencoder for spammer group identification. Experiments conducted on real-world review datasets, including Amazon, Yelp, and YelpChi, demonstrate that our method achieves significant improvements in average Precision@k and Recall@k metrics compared with state-of-the-art baseline approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412564177",
    "type": "article"
  },
  {
    "title": "Understanding User Perspectives for MOOC Quality Evaluation with Hypergraph Learning",
    "doi": "https://doi.org/10.1145/3749845",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Lu Jiang; Ruisu Zhang; Yanan Xiao; Kunpeng Liu; Kaidi Wang; Minghao Yin",
    "corresponding_authors": "",
    "abstract": "Evaluation of Massive open online course (MOOC) quality is crucial to enhance the educational resources, benefiting user services, and enhancing students’ learning efficiency. Despite achieving encouraging results, current efforts are hindered by complex relationships between entities and individual varies. To address the above problem, in this paper, we frame the issue as a task of learning course representations and proceed to develop an U ser-centric H ypergraph R epresentation L earning (called UHRL ) for online course quality evaluation. In particular, we initially construct a MOOC hypergraph to depict the interactions and connections between the entities and use cross hyperedge alignment to reveal the semantics of courses. And then we incorporates an attention mechanism in the information transmission process to ensure semantic integrity. Furthermore, to tackle the bias of users’ preference, our framework exploit mutual information for preserving the fairness of representation learning. Finally, our comprehensive experiments on three real-world datasets confirm the effectiveness of our approach compared to cutting-edge methods in evaluating online course quality across various performance metrics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412581624",
    "type": "article"
  },
  {
    "title": "GFformer: A Graph Transformer for Extracting All Frequency Information from Large-scale Graphs",
    "doi": "https://doi.org/10.1145/3750051",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Qi Zhang; Meng-Meng Si; Yanfeng Sun; Shaofan Wang; Junbin Gao; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Graph Transformers have demonstrated outstanding performance across various graph-based applications. Despite their success, applying them to large-scale graphs presents significant scalability challenges, limiting their practical use in industrial environments. Recent studies have attempted to overcome this challenge by focusing on the spatial domain of graphs, leading to the development of various scalable models. However, these approaches neglect the spectral characteristics of graphs, which are crucial for adaptively extracting information from full frequency bands based on the graph's inherent properties. As a result, existing scalable Graph Transformers tend to rely heavily on low-frequency features, overlooking valuable mid- and high-frequency information. This paper proposes the Graph Filter Transformer (GFformer), a framework designed to effectively extract full frequency information from large-scale graphs. Unlike existing Graph Transformers, GFformer integrates graph filters into the Transformer architecture, thereby enhancing its ability to model both structural and frequency-related properties. Utilizing the proposed Spectral Token Converter (ST-Converter), GFformer generates a unique spectral token sequence for each node by incorporating features from diverse frequencies that act as tokens. This design enables the independent learning of node representations in parallel and supports mini-batch training with flexible batch sizes, making GFformer highly scalable. ST-Converter employs spectral graph filters, including low-, mid-, and high-pass filters, to extract features serving as tokens. Consequently, each sequence encompasses features from various frequencies, enabling GFformer to capture comprehensive frequency information effectively. Extensive experiments on datasets of varying scales, including both homophilic and heterophilic graphs, consistently demonstrate that GFformer outperforms existing representative methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412603550",
    "type": "article"
  },
  {
    "title": "SE-GCL: A Semantic-Enhanced Graph Contrastive Learning Framework for Road Network Embedding",
    "doi": "https://doi.org/10.1145/3757921",
    "publication_date": "2025-08-04",
    "publication_year": 2025,
    "authors": "Jie Zhao; Chao Chen; Wanyi Zhang; Mingyu Deng; Huayan Pu; Jun Luo",
    "corresponding_authors": "",
    "abstract": "Representation learning of road networks is essential for various downstream traffic-related tasks, as road network contain multi-modal data with rich information, and the learned embeddings can be directly used in machine learning models. However, due to the dynamic changes in road networks with respect to topology and associated data, as well as the local and long-range dependency caused by complex mobility semantics, learning robust and effective representations remains challenging. To this end, we exploit the properties of the road network and the mobility semantics embedded in trajectories, and propose a novel S emantic- E nhanced G raph C ontrastive L earning (SE-GCL) framework, for learning general-purpose embeddings of road networks. Specifically, in this framework, we propose (1) a multi-modal feature embedding module to capture both the attribute and visual information of road segments, (2) a semantic-enhanced graph augmentation strategy to simulate topological changes and data missing in the road network, and (3) a semantic-enhanced contrastive optimization module that leverages geo-locality and mobility semantics to guide representation learning. Extensive experiments are conducted on two real-world road networks with three representative downstream tasks. The result demonstrate that SE-GCL yields more robust and effective representations, outperforming the state-of-the-art baselines. The source code is available at https://github.com/csjiezhao/SE-GCL .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412928621",
    "type": "article"
  },
  {
    "title": "Multi-Grained Spatial-Temporal Feature Complementarity for Accurate Online Cellular Traffic Prediction",
    "doi": "https://doi.org/10.1145/3758099",
    "publication_date": "2025-08-05",
    "publication_year": 2025,
    "authors": "Ningning Fu; Shengheng Liu; Weiliang Xie; Yongming Huang",
    "corresponding_authors": "",
    "abstract": "Knowledge discovered from telecom data can facilitate proactive understanding of network dynamics and user behaviors, which in turn empowers service providers to optimize cellular traffic scheduling and resource allocation. Nevertheless, the telecom industry still heavily relies on manual expert intervention. Existing studies have been focused on exhaustively exploring the spatial-temporal correlations. However, they often overlook the underlying characteristics of cellular traffic, which are shaped by the sporadic and bursty nature of telecom services. Additionally, concept drift creates substantial obstacles to maintaining satisfactory accuracy in continuous cellular forecasting tasks. To resolve these problems, we put forward an online cellular traffic prediction method grounded in Multi-Grained Spatial-Temporal feature Complementarity (MGSTC). The proposed method is devised to achieve high-precision predictions in practical continuous forecasting scenarios. Concretely, MGSTC segments historical data into chunks and employs the coarse-grained temporal attention to offer a trend reference for the prediction horizon. Subsequently, fine-grained spatial attention is utilized to capture detailed correlations among network elements, which enables localized refinement of the established trend. The complementarity of these multi-grained spatial-temporal features facilitates the efficient transmission of valuable information. To accommodate continuous forecasting needs, we implement an online learning strategy that can detect concept drift in real-time and promptly switch to the appropriate parameter update stage. Experiments carried out on four real-world datasets demonstrate that MGSTC outperforms eleven state-of-the-art baselines consistently.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412991735",
    "type": "article"
  },
  {
    "title": "Balancing Fairness and Accuracy in Data-Restricted Binary Classification",
    "doi": "https://doi.org/10.1145/3747850",
    "publication_date": "2025-07-18",
    "publication_year": 2025,
    "authors": "Zachary McBride Lazri; Danial Dervovic; Antigoni Polychroniadou; Ivan Brugere; Dana Dachman-Soled; Fang Huang; Min Wu",
    "corresponding_authors": "",
    "abstract": "Fair decision-making in machine learning (ML) remains a critical challenge, particularly when access to sensitive information is restricted due to legal, ethical, or organizational constraints. These limitations affect both accuracy and fairness, creating trade-offs central to the deployment of ML systems in the real world. While prior work has studied fairness-accuracy trade-offs, most approaches focus on model outputs rather than directly examining how restricted data access impacts fairness. This leaves an important gap: understanding how fairness constraints affect model performance under real-world data restrictions . To address this gap, we propose a framework that explicitly models fairness-accuracy trade-offs in data-restricted environments. Unlike prior work, our approach analyzes the behavior of the optimal Bayesian classifier using a discrete approximation of the data distribution, allowing us to systematically isolate the effects of fairness constraints. We evaluate our framework on three benchmark datasets—Adult, Law, and Dutch Census—revealing key insights: (1) enforcing equal accuracy on imbalanced datasets can substantially degrade performance under additional fairness constraints, (2) individual and group fairness often impose conflicting constraints, and (3) decorrelating sensitive attributes from features does not usually reduce accuracy. These findings demonstrate that our framework provides an effective, structured approach for practitioners to assess fairness constraints in decision-making pipelines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413056492",
    "type": "article"
  },
  {
    "title": "Multi-level Contrastive Learning for Knowledge Tracing",
    "doi": "https://doi.org/10.1145/3759920",
    "publication_date": "2025-08-12",
    "publication_year": 2025,
    "authors": "Xiaoxuan Shen; Fenghua Yu; Qian Wan; Ruxia Liang; Jianwen Sun",
    "corresponding_authors": "",
    "abstract": "Knowledge Tracing (KT) is the task of predicting students’ future performance based on their past interactions with educational resources. A key aspect of KT is representation learning, which aims to capture meaningful features from students’ learning behaviors to improve prediction performance. Recently, contrastive learning methods have shown great promise in representation learning. As a result, KT models based on contrastive learning have been introduced to enhance representation learning for knowledge tracing. However, these models have posed several challenges. Firstly, most of these models adopt the contrastive learning approach used in other fields, which involves data augmentation followed by contrastive learning, yet effectively applying data augmentation in KT remains an open challenge. Secondly, these models typically apply contrastive learning to only one of the fundamental components of KT: questions, interactions, or knowledge states, thereby limiting their overall performance. To address these issues, this paper proposes a Multi-level Contrastive learning model for Knowledge Tracing (MCKT). MCKT 1 does not rely on data augmentation strategies; instead, it deeply integrates domain knowledge and performs contrastive learning at three levels: questions, interactions, and knowledge states. Experimental results on four publicly available datasets, compared against a total of 20 state-of-the-art KT models, demonstrate that MCKT consistently outperforms other models. Subsequent experiments further validate the effectiveness of the multi-level contrastive learning approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413105632",
    "type": "article"
  },
  {
    "title": "Jointly Optimizing Deployment and Antenna of Base Stations using Hierarchical Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3763795",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Weikang Su; Haoqiang Liu; Tong Li; Xingzai Lv; Rui Hua; Wenzhen Huang; Zhaocheng Wang; Yong Li",
    "corresponding_authors": "",
    "abstract": "The coordinated deployment of multiple base stations (BS) and tuning of antenna configuration plays a crucial role in ensuring high-quality communication services, especially in the context of dense 5G BS deployment in megacities. However, traditional optimization methods, such as heuristics and reinforcement learning (RL), face challenges in addressing such problems involving the coordination of hundreds of BSs due to their limitations in handling the complexity and scale of large-scale scenarios. To address these challenges, this paper proposes the Hierarchical Multi-Agent Proximal Policy Optimization with Representation Learning (HMAPPO-RL). By employing a hierarchical structure, we effectively decouple the optimization problem into two sub-problems: BS deployment and antenna parameter tuning. Different from the step-by-step method of optimizing the BS location and antenna, HMAPPO-RL achieves joint optimization of the two problems through an ingenious interactive mechanism, fully considering the mutual influence of the BS location and antenna. To address the large-scale challenge posed by hundreds of BSs, we utilize the upsampling and downsampling mechanisms of the UNet network to integrate global and local information from large-scale state information for performance enhancement. Since complex environmental information will cause great difficulties for the agent to evaluate the state value in large-scale scenarios, we add a representation learning module to enhance the accuracy of the agent's state value estimation. The experiments using a precise mobile network simulator demonstrate the superiority of the proposed HMAPPO-RL, offering a comparative analysis with existing state-of-the-art methods. HMAPPO-RL achieves a coverage rate of 91.66% and an average throughput of 4,983,537 bit/s. These results represent improvements of 3.62% and 6.75% in coverage rate and throughput respectively when compared with the MAPPO algorithm.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413591389",
    "type": "article"
  },
  {
    "title": "ORIC V2: Improved Feature Interaction Detection Model through Online Random Interaction Chains for Click-Through Rate Prediction",
    "doi": "https://doi.org/10.1145/3762667",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Yannian Kou; Qiuqiang Lin; Y. Wen; Di Fan; Chuanhou Gao",
    "corresponding_authors": "",
    "abstract": "Predicting the probability that a user clicks a specific item is fundamental in online advertising and recommendation. Further, it is crucial to use the latest and historical data appropriately in online scenarios to train CTR models. Online Random Interaction Chains (ORIC) was proposed to detect informative and interpretable feature interactions without retraining on historical data in online scenario, and the Streaming Integrated Model (SIM) framework was designed to integrate these time-varying feature interactions into CTR prediction models. Unfortunately, ORIC exhibits latency when provides the feature interactions used to evaluate SIM, and ORIC is not applicable for numerical features. For these reasons, we propose ORIC-V2 that uses time series models to predict the confidence of candidate evaluating feature interactions and selects reasonable feature interactions, and combines numerical features with ORIC-V2 through a discretization model to obtain DORIC-V2. Feeding the feature interactions found by ORIC-V2 and DORIC-V2 into SIM obtains significant experimental results on three datasets, demonstrating the effectiveness and interpretability of ORIC-V2 and DORIC-V2.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413592351",
    "type": "article"
  },
  {
    "title": "Relative Entropy-based Regularized Non-negative Matrix Factorization for Attributed Graph Clustering",
    "doi": "https://doi.org/10.1145/3765742",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Kamal Berahmand; Mehrnoush Mohammadi; Razieh Sheikhpour; Mahdi Jalili; Richi Nayak; Hassan Khosravi",
    "corresponding_authors": "",
    "abstract": "Attributed graph clustering is a fundamental task in network mining, essential for uncovering valuable insights in various applications. However, the heterogeneity of information from structural and attribute spaces poses significant challenges in achieving consistent and meaningful clustering. To address this, we propose Relative Entropy-based Regularized Non-negative Matrix Factorization (RENMF), a novel approach that integrates structural and attribute information through advanced matrix factorization techniques. RENMF employs Symmetric NMF and Projective NMF to extract community membership distributions from the structural and attribute spaces, respectively. By treating these distributions as homogeneous, RENMF preserves distinct, denoised information from both spaces while considering their heterogeneous complementary information. We introduce Relative Entropy (RE) as a novel regularization term to facilitate interaction between these spaces, aiming to maximize consistency between the discovered latent distributions. In this interaction, we leverage the asymmetric property of RE to emphasize attributes as essential complementary information for structural clustering. The RENMF model is solved using a new iterative multiplicative update rule, with convergence theoretically proven. We evaluate RENMF’s effectiveness through extensive experiments on ten real-world networks, comparing it to eleven state-of-the-art clustering methods. The results demonstrate RENMF’s superiority in ground truth matching and key quality metrics, outperforming existing methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413993551",
    "type": "article"
  },
  {
    "title": "ProtoNAM: Prototypical Neural Additive Models for Interpretable Deep Tabular Learning",
    "doi": "https://doi.org/10.1145/3766072",
    "publication_date": "2025-09-05",
    "publication_year": 2025,
    "authors": "Guangzhi Xiong; Sanchit Sinha; Aidong Zhang",
    "corresponding_authors": "",
    "abstract": "Generalized additive models (GAMs) have long been a powerful white-box tool for the intelligible analysis of tabular data, revealing the influence of each feature on the model predictions. Despite the success of neural networks (NNs) in various domains, their application as NN-based GAMs in tabular data analysis remains suboptimal compared to tree-based ones, and the opacity of encoders in NN-GAMs also prevents users from understanding how networks learn the functions. In this work, we propose a new deep tabular learning method, termed Prototypical Neural Additive Model (ProtoNAM), which introduces prototypes into neural networks in the framework of GAMs. With the introduced prototype-based feature activation, ProtoNAM can flexibly model the irregular mapping from tabular features to the outputs while maintaining the explainability of the final prediction. We also propose a gradient-boosting inspired hierarchical shape function modeling method, facilitating the discovery of complex feature patterns and bringing transparency into the learning process of each network layer. Our empirical evaluations demonstrate that ProtoNAM outperforms all existing NN-based GAMs, while providing additional insights into the shape function learned for each feature. The source code of ProtoNAM is available at https://github.com/Teddy-XiongGZ/ProtoNAM",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414014360",
    "type": "article"
  },
  {
    "title": "A Rumor Propagation Model Based on User Cognition and Evolutionary Game",
    "doi": "https://doi.org/10.1145/3767161",
    "publication_date": "2025-09-11",
    "publication_year": 2025,
    "authors": "Rong Wang; Z. Wu; Liangyu Wang; Chaolong Jia; Yunpeng Xiao",
    "corresponding_authors": "",
    "abstract": "In social networks, studying rumor propagation patterns is essential for curbing the spread of rumors. Given the coexistence and conflict of multiple-type rumor information, as well as users’ cognitive differences, this paper presents a rumor propagation model grounded in user cognition and evolutionary game theory. Firstly, considering the potential impact of social relationships between users on rumor propagation, the KD-Tree algorithm is employed to uncover hidden connections between users, thereby enriching the topology of the user's social network. Secondly, a user behavior driving mechanism for rumor, anti-rumor, and motivation-rumor types is constructed based on evolutionary games to reflect the interactive and strategic nature of users’ responses. Moreover, the Lotka-Volterra equation is utilized to explore the dynamic game of multi-type rumor information and the cognitive process of users. Finally, to address differences in users’ cognition, this paper introduces the anti-rumor trust state A and the motivation-rumor trust state M , which arise from users’ exposure to multiple types of rumor information. Based on these trust states, a rumor propagation model, SIAMR, is constructed using user cognition and evolutionary game theory. Experiments demonstrate that the model accurately captures the dynamic interactions between multi-type rumor information and the transmission process of rumor topics in social networks. The proposed model integrates cognitive psychology with a strategic interaction framework, offering a more realistic representation of rumor propagation behavior in the real world. Experimental results reveal that SIAMR improves prediction accuracy by 14.23 % over baseline models in simulating the dynamics of multiple types of rumors, effectively capturing users’ cognitive influences and the mechanisms of information competition.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414153597",
    "type": "article"
  },
  {
    "title": "Dual-View Anomaly Detection in Heterogeneous Information Networks with Hierarchical Neighborhood Fusion",
    "doi": "https://doi.org/10.1145/3767156",
    "publication_date": "2025-09-15",
    "publication_year": 2025,
    "authors": "Xiangjie Kong; Siyue Shuai; Hui Wang; Guojiang Shen; Feng Xia",
    "corresponding_authors": "",
    "abstract": "The primary objective of graph node anomaly detection is to pinpoint rare patterns that display marked deviations from the typical one. Existing methods utilize Graph Convolutional Networks (GCNs) to model complex interactions in Heterogeneous Information Networks (HINs), typically homogenizing HINs using meta-paths to effectively focus on particular semantic scenarios. However, meta-paths excessively emphasize specific nodes and their connections on predefined paths, leading to the neglect of one-hop context-rich neighbors. Furthermore, the conversion from heterogeneous to homogeneous structures disrupts inherent relationships, resulting in an irreversible loss of direct links. Thus, we propose a dual-view based H eterogeneous I nformation N etworks Node Ano maly Detection framework, HINAno, to mitigate structural loss. HINAno adopts a synergetic approach that balances local structural information with semantic richness, drawing from both the one-hop neighbor view and the meta-path view. Specifically, this dual-view utilizes hierarchical fusion mechanisms at node, type and semantic levels to capture one-hop and multi-hop neighborhoods in a level-wise manner. In addition, HINAno adopts self-supervised contrastive learning and GCNs to amplify the gap between normal and abnormal nodes, thereby reducing the reliance on anomalous labels and enhancing the capability of anomaly detection. Finally, we successfully verify that the HINAno framework is effective and superior on four real-world datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414203916",
    "type": "article"
  },
  {
    "title": "“A 6 or a 9?”: Ensemble Learning Through the Multiplicity of Performant Models and Explanations",
    "doi": "https://doi.org/10.1145/3767735",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Gianlucca Zuin; Adriano Veloso",
    "corresponding_authors": "",
    "abstract": "Creating models from past observations and ensuring their effectiveness on new data is the essence of machine learning. However, selecting models that generalize well remains a challenging task. Related to this topic, the Rashomon Effect refers to cases where multiple models perform similarly well for a given learning problem. This often occurs in real-world scenarios, like the manufacturing process or medical diagnosis, where diverse patterns in data lead to multiple high-performing solutions. We propose the Rashomon Ensemble, a method that strategically selects models from these diverse high-performing solutions to improve generalization. By grouping models based on both their performance and explanations, we construct ensembles that maximize diversity while maintaining predictive accuracy. This selection ensures that each model covers a distinct region of the solution space, making the ensemble more robust to distribution shifts and variations in unseen data. We validate our approach on both open and proprietary collaborative real-world datasets, demonstrating up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large. Additionally, we demonstrate tangible benefits for businesses in various real-world applications, highlighting the robustness, practicality, and effectiveness of our approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414241766",
    "type": "article"
  },
  {
    "title": "Interest-Disentangled Contrastive Sample Generation for Recommendation",
    "doi": "https://doi.org/10.1145/3768160",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Meng Jian; Ruoxi Li; Meishan Liu; Meijuan Yang; Shaona Wang; Lifang Wu",
    "corresponding_authors": "",
    "abstract": "In the domain of recommendations, previous works often retrieve items through sampling strategies from the database to gather negative signals for exploring implicit feedback. However, because of extremely sparse records, the existing items used as negative samples may not sufficiently support the interacted items in depicting the diverse interests of users. Consequently, the generation of negative samples needs to be explored in recommendation systems. In this study, we propose an interest-disentangled contrastive sample generation (IDCG) model to enhance interest modeling by contrasting interacted items with the generated samples for recommendation. Specifically, we decouple the interacted items of users into positively relevant and irrelevant factors of interest, providing a valuable clue to learn negatively relevant factors in personalized interests. Then, negative samples are generated by merging the learned negatively relevant factors and irrelevant factors. At this point, a two-level contrast is constructed between positive and negative samples and between the relevant factors of positives and negatives, providing auxiliary collaborative signals to debiase and alleviate the interaction sparsity issue. Extensive experiments on three real datasets demonstrate the effectiveness of IDCG in generating targeted and meaningful negative samples from the perspective of disentangling relevant factors to promote interest modeling for recommendation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414243568",
    "type": "article"
  },
  {
    "title": "Accuracy, Fairness, Diversity All at Once: An Influence Function Guided Data Enhancement Approach for Recommender System",
    "doi": "https://doi.org/10.1145/3768316",
    "publication_date": "2025-09-17",
    "publication_year": 2025,
    "authors": "Tianyang Xie; Yong Ge; Shuojia Guo",
    "corresponding_authors": "",
    "abstract": "Recommender systems play a pivotal role in curating high-quality content for users, predominantly leveraging data-driven algorithms and machine learning methodologies. However, the intrinsic data-centric nature of these systems raises critical concerns; biased datasets and algorithms can inadvertently propagate biases to end-users. Furthermore, machine learning techniques, while powerful, can overfit a user’s preference, leading to a monotonous stream of content suggestions. Both the CS and IS community have well-recognized the need of fairness and diversity in recommender systems and many studies are proposed to mitigate these challenges. Yet, a tangible solution that holistically addressed all three components—accuracy, fairness, and diversity—in unison remains elusive. This paper aims to bridge this gap, introducing a novel Influence-function-guided, Fair, and Diverse Data Enhancement (InFoDance) approach that enhances all three perspectives simultaneously. It consists of four interconnected modules: model training, candidate data generation, influence function-based candidate evaluation, and virtual data selection. It iteratively generates virtual data to update the trained recommender system. The empirical evaluation has shown that our approach can improve accuracy, fairness, and diversity by up to 24.27%, 55.29%, and 1.85% simultaneously and significantly outperform the state-of-the-art baselines on multiple evaluation metrics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414276975",
    "type": "article"
  },
  {
    "title": "Integrating Group Consensus for Competitive Influence Maximization in OSNs",
    "doi": "https://doi.org/10.1145/3768583",
    "publication_date": "2025-09-18",
    "publication_year": 2025,
    "authors": "Guo-Bang Chen; Wenjun Jiang; Kenli Li; Jingjing Wang; Jie Wu; Kian‐Lee Tan",
    "corresponding_authors": "",
    "abstract": "In online social networks (OSNs), people usually join groups for communication. Information diffusion often occurs with some cost, either between individuals or within/among groups; and different opinions may compete with each other. The groups can make decisions based on the majority of the group members. This type of group consensus is common in group activities. However, existing research on maximization of competitive influence often neglects the effects of group consensus. To this end, we introduce the process of group consensus reaching in influence maximization and propose a novel Group consensus-based Competitive Linear Threshold (GCLT) propagation model; then we study the Budgeted Competitive Influence Maximization (BCIM) problem under the GCLT model. We reveal that the problem is NP-hard, and the objective function is proven to be neither submodular nor supermodular. To this end, we construct an equivalent Group consensus-based Competitive Live Edge (GCLE) model of GCLT by sampling method. Based on GCLE, develop two submodular functions of the upper and lower bounds. Then, we propose the SBG algorithm by applying the S andwich Approximation framework for the B CIM problem under the G CLT model. In SBG , we provide an approximate solution to the lower bound and the upper bound by the proposed OPIM-B algorithm. Then, we select the seed set of solutions that achieves the best influence spread in Monte Carlo simulations. We also propose two strategies to optimize SBG . The experiments on six real social network datasets verify the effectiveness and scalability of our method, and validate the impact of group consensus on the competitive influence dissemination process, as well as the importance of considering the process of reaching group consensus.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414321036",
    "type": "article"
  },
  {
    "title": "Stable Subsampling under Model Misspecification and Covariate Shift",
    "doi": "https://doi.org/10.1145/3769077",
    "publication_date": "2025-09-23",
    "publication_year": 2025,
    "authors": "Jinjing Yang; Shaohua Xu; Zebin Yang; Aijun Zhang; Yongdao Zhou",
    "corresponding_authors": "",
    "abstract": "The presence of covariate shift between training and test datasets, coupled with model misspecification, can lead to instability in regression predictions across diverse datasets. Meanwhile, training complex models with massive data imposes significant computational burden. In this paper, we present a novel model-free subsampling algorithm for stable prediction, which employs uniform design and confounder balancing methods. Our subsampling algorithm aims to find the nearest neighbor subsampling points of uniform design with the goal of minimizing global stability loss, thereby reducing the data volume while achieving stable predictions. Theoretic analyses show that the uniform measure minimizes the maximum integral mean square error (MIMSE) and the global stability loss evaluates the independence among variables in each candidate MIMSE-optimal subsampled sets. Simulation studies conducted on synthetic datasets, as well as applications on real datasets, demonstrate the superiority of our proposed method under model misspecification and covariate shift.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414425407",
    "type": "article"
  },
  {
    "title": "Towards Robust and Interpretable Spatial-Temporal Graph Modeling for Traffic Prediction",
    "doi": "https://doi.org/10.1145/3769297",
    "publication_date": "2025-09-25",
    "publication_year": 2025,
    "authors": "Hanchen Yang; Jiannong Cao; Wengen Li; Yu Yang; Xiaoyi Li; Lingbai Kong; Yichao Zhang; Jihong Guan; Shuigeng Zhou",
    "corresponding_authors": "",
    "abstract": "Accurate spatial-temporal (ST) traffic prediction plays an essential role in intelligent transportation systems. Existing advanced traffic prediction methods typically utilize spatial-temporal graph neural networks (STGNNs) to capture the ST correlations and achieve excellent prediction performance. However, our experimental investigation reveals that existing static and dynamic graph-based STGNNs still incur excessive noise and redundancy, and fail to discover robust and reliable ST correlations in traffic networks. Moreover, most methods cannot explain the underlying reasons behind the ST correlations. To solve these problems, we propose a novel S patial- T emporal G raph M odeling framework via A daptive contrastive learning (ST-GMA). Firstly, we design a robust augmentation learning module to generate high-level and robust data augmentations via a self-supervised task for modeling reliable correlations. Then, we develop an adaptive contrastive learning module to update correlation graphs by effectively selecting positive and negative augmentations, reducing redundant calculations, and providing insights into the correlation changes. Finally, ST-GMA integrates the generated correlation graphs with ST convolution blocks to conduct traffic prediction tasks. Experimental results on five real-world datasets demonstrate that ST-GMA not only achieves significant prediction performance compared with state-of-the-art methods but also exhibits a new perspective on the interpretability of correlation changes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414512648",
    "type": "article"
  },
  {
    "title": "Hidden inverted specific-class distance measure for nominal attributes",
    "doi": "https://doi.org/10.1145/3769293",
    "publication_date": "2025-09-25",
    "publication_year": 2025,
    "authors": "Fang Gong; Tao Lü; Kuayue Liu",
    "corresponding_authors": "",
    "abstract": "The inverted specific-class distance measure (ISCDM) is a popular distance metric that uses conditional probability term to calculate the distance between two nominal attribute values, but the reliability of the conditional probability term is limited by the attribute independence assumption, which leads to the suboptimal performance in applications involving sophisticated attribute dependencies. To obtain more accurate conditional probability estimation, in this study, we derive an enhanced ISCDM by leveraging structure extension to alleviate the unrealistic attribute assumption. We denominate the resulting model as the hidden inverted specific-class distance measure (HISCDM). In HISCDM, the structure extension scheme of hidden naive bayes is adopted to find the weighted dependence relationships between attributes, and then is incorporated into the conditional probability estimation. The comprehensive experimental results demonstrate that our proposed HISCDM significantly outperforms all other methods used for comparison in terms of classification accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414530218",
    "type": "article"
  },
  {
    "title": "Graph Out-of-Distribution Generalization based on Structural Entropy guided Information Bottleneck",
    "doi": "https://doi.org/10.1145/3767162",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Zijun Di; Zheng Peng; Bin Lü; Kaige Guan; Luoyi Fu; Ningdi Jin; Ye Chen; Xiaoying Gan; Lei Zhou; Xinbing Wang; Chenghu Zhou",
    "corresponding_authors": "",
    "abstract": "Out-of-Distribution (OOD) generalization is a promising yet challenging goal that guarantees the test performance of GNNs in open-world settings. However, due to the intricate internal topology of graph-structured data, redundant information from the spurious topologies severely confuses GNNs to deviate from the labels. Extracting concise and label-relevant subgraphs from the original graphs can alleviate this problem. Unfortunately, existing methods either overlook the global structural distribution or rely heavily on manually predefined assumptions. As a result, they fall short of well capturing the structural distribution changes between input graph and extracted subgraph, thus compromising adaptability of extracted invariant subgraphs to diverse OOD scenarios. This motivates us to propose a framework called S tructural E ntropy guided I nformation B ottleneck (OOD-SEIB) that aims to more traceably measure the inherent information changes for better and more flexible OOD generalization. The core of OOD-SEIB lies in concise topology extraction module, where we measure the mutual information flow between input graph and extracted subgraph based on structural entropy, termed Compression Index (CI). Specifically, the CI is a quantifiable metric that calculates the codeword length required to describe entire graph structure via a biased random walk. Under this guidance, OOD-SEIB then launches a structural information bottleneck compression module that jointly optimizes both CI and label-relavance of the subgraph topology by iteratively balancing between informativeness and compression. To further improve GNN's invariant subgraph identification capability, OOD-SEIB generates multiple augmented environments and distill the invariant subgraphs into GNN as knowledge in an inside-out manner. When iteratively optimizing in above prescribed way, OOD-SEIB progressively reinforce the invariant subgraph extraction, thereby enhancing its generalization capability. Extensive experiments on synthetic and three real-world graph-level OOD benchmarks demonstrate that our proposed OOD-SEIB improves classification accuracy by \\(4.85\\%\\) - \\(38.03\\%\\) on average compared to state-of-the-art baselines. Additionally, we extend OOD-SEIB to two node-level benchmarks, achieving average classification accuracy improvements of \\(14.52\\%\\) and \\(13.15\\%\\) .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414799616",
    "type": "article"
  },
  {
    "title": "MCSS: Discovering Consistently Determined Relation in Multi-view Clustering Based on Sample’s Stability",
    "doi": "https://doi.org/10.1145/3771274",
    "publication_date": "2025-10-10",
    "publication_year": 2025,
    "authors": "Feijiang Li; Xin Liu; Jieting Wang; Yuhua Qian",
    "corresponding_authors": "",
    "abstract": "Multi-view clustering aims to discover the group knowledge in the widely existing multi-view data. Consistency is one of the fundamental factors for effectively handling the multi-view data clustering problem. It has been observed that there are two types of consistent relations, consistently ambiguous relations and consistently determined relations, which have negative and positive impacts on clustering, respectively. However, most of the existing multi-view clustering methods treat the consistency relation without distinction. In this paper, the sample’s stability in the sense of multi-view clustering is defined to recognize the consistently determined relations. Theoretically, it is revealed that the samples with higher stability have consistently determined relations with more other samples in all views, indicating a clear cluster structure. The rationality of the sample’s stability in multi-view is illustrated by experimental analysis. Further, a multi-view clustering method based on the sample’s stability (MCSS) is proposed. This method first calculates the sample’s stability and divides the samples into the stable region and unstable region. Then, the cluster structure in the stable region is discovered. Finally, the samples in the unstable region are assigned based on the pre-discovered cluster structure. The effectiveness of the proposed method based on sample’s stability is illustrated on nine benchmark multi-view data sets compared with ten multi-view clustering methods. The demo code is available at https://github.com/FeijiangLi/MCSS.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415047178",
    "type": "article"
  },
  {
    "title": "Discovering Knowledge-Sharing Communities in Question-Answering Forums",
    "doi": "https://doi.org/10.1145/1870096.1870099",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Mohamed Bouguessa; Shengrui Wang; Benoît Dumoulin",
    "corresponding_authors": "",
    "abstract": "In this article, we define a knowledge-sharing community in a question-answering forum as a set of askers and authoritative users such that, within each community, askers exhibit more homogeneous behavior in terms of their interactions with authoritative users than elsewhere. A procedure for discovering members of such a community is devised. As a case study, we focus on Yahoo! Answers, a large and diverse online question-answering service. Our contribution is twofold. First, we propose a method for automatic identification of authoritative actors in Yahoo! Answers. To this end, we estimate and then model the authority scores of participants as a mixture of gamma distributions. The number of components in the mixture is determined using the Bayesian Information Criterion (BIC), while the parameters of each component are estimated using the Expectation-Maximization (EM) algorithm. This method allows us to automatically discriminate between authoritative and nonauthoritative users. Second, we represent the forum environment as a type of transactional data such that each transaction summarizes the interaction of an asker with a specific set of authoritative users. Then, to group askers on the basis of their interactions with authoritative users, we propose a parameter-free transaction data clustering algorithm which is based on a novel criterion function. The identified clusters correspond to the communities that we aim to discover. To evaluate the suitability of our clustering algorithm, we conduct a series of experiments on both synthetic data and public real-life data. Finally, we put our approach to work using data from Yahoo! Answers which represent users’ activities over one full year.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1993150471",
    "type": "article"
  },
  {
    "title": "Modeling Social Annotation",
    "doi": "https://doi.org/10.1145/1870096.1870100",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Anon Plangprasopchok; Kristina Lerman",
    "corresponding_authors": "",
    "abstract": "Collaborative tagging systems, such as Delicious, CiteULike , and others, allow users to annotate resources, for example, Web pages or scientific papers, with descriptive labels called tags . The social annotations contributed by thousands of users can potentially be used to infer categorical knowledge, classify documents, or recommend new relevant information. Traditional text inference methods do not make the best use of social annotation, since they do not take into account variations in individual users’ perspectives and vocabulary. In a previous work, we introduced a simple probabilistic model that takes the interests of individual annotators into account in order to find hidden topics of annotated resources. Unfortunately, that approach had one major shortcoming: the number of topics and interests must be specified a priori. To address this drawback, we extend the model to a fully Bayesian framework, which offers a way to automatically estimate these numbers. In particular, the model allows the number of interests and topics to change as suggested by the structure of the data. We evaluate the proposed model in detail on the synthetic and real-world data by comparing its performance to Latent Dirichlet Allocation on the topic extraction task. For the latter evaluation, we apply the model to infer topics of Web resources from social annotations obtained from Delicious in order to discover new resources similar to a specified one. Our empirical results demonstrate that the proposed model is a promising method for exploiting social knowledge contained in user-generated annotations.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2077018916",
    "type": "article"
  },
  {
    "title": "<i>FrauDetector</i> <sup>+</sup>",
    "doi": "https://doi.org/10.1145/3234943",
    "publication_date": "2018-08-28",
    "publication_year": 2018,
    "authors": "Josh Jia-Ching Ying; Ji Zhang; Che-Wei Huang; Kuan-Ta Chen; Vincent S. Tseng",
    "corresponding_authors": "",
    "abstract": "In recent years, telecommunication fraud has become more rampant internationally with the development of modern technology and global communication. Because of rapid growth in the volume of call logs, the task of fraudulent phone call detection is confronted with big data issues in real-world implementations. Although our previous work, FrauDetector , addressed this problem and achieved some promising results, it can be further enhanced because it focuses only on fraud detection accuracy, whereas the efficiency and scalability are not top priorities. Other known approaches for fraudulent call number detection suffer from long training times or cannot accurately detect fraudulent phone calls in real time. However, the learning process of FrauDetector is too time-consuming to support real-world application. Although we have attempted to accelerate the the learning process of FrauDetector by parallelization, the parallelized learning process, namely PFrauDetector , still cannot afford the computing cost. In this article, we propose a highly efficient incremental graph-mining-based fraudulent phone call detection approach, namely FrauDetector + , which can automatically label fraudulent phone numbers with a “fraud” tag a crucial prerequisite for distinguishing fraudulent phone call numbers from nonfraudulent ones. FrauDetector + initially generates smaller, more manageable subnetworks from original graph and performs a parallelized weighted HITS algorithm for a significant speed increase in the graph learning module. It adopts a novel aggregation approach to generate a trust (or experience) value for each phone number (or user) based on their respective local values. After the initial procedure, we can incrementally update the trust (or experience) value for each phone number (or user) while a new fraud phone number is identified. An efficient fraud-centric hash structure is constructed to support fast real-time detection of fraudulent phone numbers in the detection module. We conduct a comprehensive experimental study based on real datasets collected through an antifraud mobile application called Whoscall . The results demonstrate a significantly improved efficiency of our approach compared with FrauDetector as well as superior performance against other major classifier-based methods.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2889403065",
    "type": "article"
  },
  {
    "title": "Coupled Clustering Ensemble by Exploring Data Interdependence",
    "doi": "https://doi.org/10.1145/3230967",
    "publication_date": "2018-08-28",
    "publication_year": 2018,
    "authors": "Can Wang; Chi‐Hung Chi; Zhong She; Longbing Cao; Bela Stantić",
    "corresponding_authors": "",
    "abstract": "Clustering ensembles combine multiple partitions of data into a single clustering solution. It is an effective technique for improving the quality of clustering results. Current clustering ensemble algorithms are usually built on the pairwise agreements between clusterings that focus on the similarity via consensus functions, between data objects that induce similarity measures from partitions and re-cluster objects, and between clusters that collapse groups of clusters into meta-clusters. In most of those models, there is a strong assumption on IIDness (i.e., independent and identical distribution), which states that base clusterings perform independently of one another and all objects are also independent. In the real world, however, objects are generally likely related to each other through features that are either explicit or even implicit. There is also latent but definite relationship among intermediate base clusterings because they are derived from the same set of data. All these demand a further investigation of clustering ensembles that explores the interdependence characteristics of data. To solve this problem, a new coupled clustering ensemble ( CCE ) framework that works on the interdependence nature of objects and intermediate base clusterings is proposed in this article. The main idea is to model the coupling relationship between objects by aggregating the similarity of base clusterings, and the interactive relationship among objects by addressing their neighborhood domains. Once these interdependence relationships are discovered, they will act as critical supplements to clustering ensembles. We verified our proposed framework by using three types of consensus function: clustering-based, object-based, and cluster-based. Substantial experiments on multiple synthetic and real-life benchmark datasets indicate that CCE can effectively capture the implicit interdependence relationships among base clusterings and among objects with higher clustering accuracy, stability, and robustness compared to 14 state-of-the-art techniques, supported by statistical analysis. In addition, we show that the final clustering quality is dependent on the data characteristics (e.g., quality and consistency) of base clusterings in terms of sensitivity analysis. Finally, the applications in document clustering, as well as on the datasets with much larger size and dimensionality, further demonstrate the effectiveness, efficiency, and scalability of our proposed models.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2888926673",
    "type": "article"
  },
  {
    "title": "Semi-supervised Learning Meets Factorization",
    "doi": "https://doi.org/10.1145/3264745",
    "publication_date": "2018-10-01",
    "publication_year": 2018,
    "authors": "Chaochao Chen; Kevin Chen–Chuan Chang; Qibing Li; Xiaolin Zheng",
    "corresponding_authors": "",
    "abstract": "Recently, latent factor model (LFM) has been drawing much attention in recommender systems due to its good performance and scalability. However, existing LFMs predict missing values in a user-item rating matrix only based on the known ones, and thus the sparsity of the rating matrix always limits their performance. Meanwhile, semi-supervised learning (SSL) provides an effective way to alleviate the label (i.e., rating) sparsity problem by performing label propagation, which is mainly based on the smoothness insight on affinity graphs. However, graph-based SSL suffers serious scalability and graph unreliable problems when directly being applied to do recommendation. In this article, we propose a novel probabilistic chain graph model (CGM) to marry SSL with LFM. The proposed CGM is a combination of Bayesian network and Markov random field . The Bayesian network is used to model the rating generation and regression procedures, and the Markov random field is used to model the confidence-aware smoothness constraint between the generated ratings. Experimental results show that our proposed CGM significantly outperforms the state-of-the-art approaches in terms of four evaluation metrics, and with a larger performance margin when data sparsity increases.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2895429976",
    "type": "article"
  },
  {
    "title": "Multi-task Crowdsourcing via an Optimization Framework",
    "doi": "https://doi.org/10.1145/3310227",
    "publication_date": "2019-05-29",
    "publication_year": 2019,
    "authors": "Yao Zhou; Lei Ying; Jingrui He",
    "corresponding_authors": "",
    "abstract": "The unprecedented amounts of data have catalyzed the trend of combining human insights with machine learning techniques, which facilitate the use of crowdsourcing to enlist label information both effectively and efficiently. One crucial challenge in crowdsourcing is the diverse worker quality, which determines the accuracy of the label information provided by such workers. Motivated by the observations that same set of tasks are typically labeled by the same set of workers, we studied their behaviors across multiple related tasks and proposed an optimization framework for learning from task and worker dual heterogeneity. The proposed method uses a weight tensor to represent the workers’ behaviors across multiple tasks, and seeks to find the optimal solution of the tensor by exploiting its structured information. Then, we propose an iterative algorithm to solve the optimization problem and analyze its computational complexity. To infer the true label of an example, we construct a worker ensemble based on the estimated tensor, whose decisions will be weighted using a set of entropy weight. We also prove that the gradient of the most time-consuming updating block is separable with respect to the workers, which leads to a randomized algorithm with faster speed. Moreover, we extend the learning framework to accommodate to the multi-class setting. Finally, we test the performance of our framework on several datasets, and demonstrate its superiority over state-of-the-art techniques.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2946965516",
    "type": "article"
  },
  {
    "title": "Incomplete Network Alignment",
    "doi": "https://doi.org/10.1145/3384203",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Si Zhang; Hanghang Tong; Jie Tang; Jiejun Xu; Wei Fan",
    "corresponding_authors": "",
    "abstract": "Networks are prevalent in many areas and are often collected from multiple sources. However, due to the veracity characteristics, more often than not, networks are incomplete. Network alignment and network completion have become two fundamental cornerstones behind a wealth of high-impact graph mining applications. The state-of-the-art have been addressing these two tasks in parallel . That is, most of the existing network alignment methods have implicitly assumed that the topology of the input networks for alignment are perfectly known a priori, whereas the existing network completion methods admit either a single network (i.e., matrix completion) or multiple aligned networks (e.g., tensor completion). In this article, we argue that network alignment and completion are inherently complementary with each other, and hence propose to jointly address them so that the two tasks can mutually benefit from each other. We formulate the problem from the optimization perspective, and propose an effective algorithm ( iNeAt ) to solve it. The proposed method offers two distinctive advantages. First ( Alignment accuracy ), our method benefits from the higher-quality input networks while mitigates the effect of the incorrectly inferred links introduced by the completion task itself. Second ( Alignment efficiency ), thanks to the low-rank structure of the complete networks and the alignment matrix, the alignment process can be significantly accelerated. We perform extensive experiments which show that (1) the network completion can significantly improve the alignment accuracy, i.e., up to 30% over the baseline methods; (2) the network alignment can in turn help recover more missing edges than the baseline methods; and (3) our method achieves a good balance between the running time and the accuracy, and scales with a provable linear complexity in both time and space.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3040911262",
    "type": "article"
  },
  {
    "title": "REMIAN",
    "doi": "https://doi.org/10.1145/3412364",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Qian Ma; Yu Gu; Wang-Chien Lee; Ge Yu; Hongbo Liu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Missing value (MV) imputation is a critical preprocessing means for data mining. Nevertheless, existing MV imputation methods are mostly designed for batch processing, and thus are not applicable to streaming data, especially those with poor quality. In this article, we propose a framework, called Real-time and Error-tolerant Missing vAlue ImputatioN (REMAIN), to impute MVs in poor-quality streaming data. Instead of imputing MVs based on all the observed data, REMAIN first initializes the MV imputation model based on a-RANSAC which is capable of detecting and rejecting anomalies in an efficient manner, and then incrementally updates the model parameters upon the arrival of new data to support real-time MV imputation. As the correlations among attributes of the data may change over time in unforseenable ways, we devise a deterioration detection mechanism to capture the deterioration of the imputation model to further improve the imputation accuracy. Finally, we conduct an extensive evaluation on the proposed algorithms using real-world and synthetic datasets. Experimental results demonstrate that REMAIN achieves significantly higher imputation accuracy over existing solutions. Meanwhile, REMAIN improves up to one order of magnitude in time cost compared with existing approaches.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3090342467",
    "type": "article"
  },
  {
    "title": "Better Classifier Calibration for Small Datasets",
    "doi": "https://doi.org/10.1145/3385656",
    "publication_date": "2020-05-13",
    "publication_year": 2020,
    "authors": "Tuomo Alasalmi; Jaakko Suutala; Juha Röning; Heli Koskimäki",
    "corresponding_authors": "",
    "abstract": "Classifier calibration does not always go hand in hand with the classifier’s ability to separate the classes. There are applications where good classifier calibration, i.e., the ability to produce accurate probability estimates, is more important than class separation. When the amount of data for training is limited, the traditional approach to improve calibration starts to crumble. In this article, we show how generating more data for calibration is able to improve calibration algorithm performance in many cases where a classifier is not naturally producing well-calibrated outputs and the traditional approach fails. The proposed approach adds computational cost but considering that the main use case is with small datasets this extra computational cost stays insignificant and is comparable to other methods in prediction time. From the tested classifiers, the largest improvement was detected with the random forest and naive Bayes classifiers. Therefore, the proposed approach can be recommended at least for those classifiers when the amount of data available for training is limited and good calibration is essential.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3125457653",
    "type": "article"
  },
  {
    "title": "A Framework of Mining Trajectories from Untrustworthy Data in Cyber-Physical System",
    "doi": "https://doi.org/10.1145/2700394",
    "publication_date": "2015-02-17",
    "publication_year": 2015,
    "authors": "Lu‐An Tang; Xiao Yu; Quanquan Gu; Jiawei Han; Guofei Jiang; Alice Leung; Thomas La Porta",
    "corresponding_authors": "",
    "abstract": "A cyber-physical system (CPS) integrates physical (i.e., sensor) devices with cyber (i.e., informational) components to form a context-sensitive system that responds intelligently to dynamic changes in real-world situations. The CPS has wide applications in scenarios such as environment monitoring, battlefield surveillance, and traffic control. One key research problem of CPS is called mining lines in the sand . With a large number of sensors (sand) deployed in a designated area, the CPS is required to discover all trajectories (lines) of passing intruders in real time. There are two crucial challenges that need to be addressed: (1) the collected sensor data are not trustworthy, and (2) the intruders do not send out any identification information. The system needs to distinguish multiple intruders and track their movements. This study proposes a method called LiSM (Line-in-the-Sand Miner) to discover trajectories from untrustworthy sensor data. LiSM constructs a watching network from sensor data and computes the locations of intruder appearances based on the link information of the network. The system retrieves a cone model from the historical trajectories to track multiple intruders. Finally, the system validates the mining results and updates sensors’ reliability scores in a feedback process. In addition, LoRM (Line-on-the-Road Miner) is proposed for trajectory discovery on road networks— mining lines on the roads . LoRM employs a filtering-and-refinement framework to reduce the distance computational overhead on road networks and uses a shortest-path-measure to track intruders. The proposed methods are evaluated with extensive experiments on big datasets. The experimental results show that the proposed methods achieve higher accuracy and efficiency in trajectory mining tasks.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2134867519",
    "type": "article"
  },
  {
    "title": "Collective Graph Identification",
    "doi": "https://doi.org/10.1145/2818378",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "Galileo Namata; Ben London; Lise Getoor",
    "corresponding_authors": "",
    "abstract": "Data describing networks—such as communication networks, transaction networks, disease transmission networks, collaboration networks, etc.—are becoming increasingly available. While observational data can be useful, it often only hints at the actual underlying process that governs interactions and attributes. For example, an email communication network provides insight into its users and their relationships, but is not the same as the “real” underlying social network. In this article, we introduce the problem of graph identification , i.e., discovering the latent graph structure underlying an observed network. We cast the problem as a probabilistic inference task, in which we must infer the nodes, edges, and node labels of a hidden graph, based on evidence. This entails solving several canonical problems in network analysis: entity resolution (determining when two observations correspond to the same entity), link prediction (inferring the existence of links), and node labeling (inferring hidden attributes). While each of these subproblems has been well studied in isolation, here we consider them as a single, collective task. We present a simple, yet novel, approach to address all three subproblems simultaneously. Our approach, which we refer to as C 3 , consists of a collection of Coupled Collective Classifiers that are applied iteratively to propagate inferred information among the subproblems. We consider variants of C 3 using different learning and inference techniques and empirically demonstrate that C 3 is superior, both in terms of predictive accuracy and running time, to state-of-the-art probabilistic approaches on four real problems.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2254919919",
    "type": "article"
  },
  {
    "title": "Dynamic Graph Mining for Multi-weight Multi-destination Route Planning with Deadlines Constraints",
    "doi": "https://doi.org/10.1145/3412363",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Yu Huang; Josh Jia-Ching Ying; Philip S. Yu; Vincent S. Tseng",
    "corresponding_authors": "",
    "abstract": "Route planning satisfied multiple requests is an emerging branch in the route planning field and has attracted significant attention from the research community in recent years. The prevailing studies focus only on seeking a route by minimizing a single kind of Travel Cost, such as trip time or distance, among others. In reality, most users would like to choose an appropriate route, neither fastest nor shortest route. Usually, a user may have multiple requirements, and an appropriate route would satisfy all requirements requested by the user. In fact, planning an appropriate route could be formulated as a problem of Multi-weight Multi-destination Route Planning with Deadlines Constraints (MWMDRP-DC). In this article, we propose a framework, namely, MWMD-Router, which addresses the MWMDRP-DC problem comprehensively. To consider the travel costs with time-variation, we propose not only four novel dynamic graph miner to extract travel costs that reveal users’ requirements but also two new algorithms, namely, Basic MWMD Route Planning and Advanced MWMD Route Planning , to plan a route that satisfies deadline requirements and optimizes another criterion like travel cost with time-variation efficiently. To the best of our knowledge, this is the first work on route planning that considers handling multiple deadlines for multi-destination planning as well as optimizing multiple travel costs with time-variation simultaneously. Experimental results demonstrate that our proposed algorithms deliver excellent performance with respect to efficiency and effectiveness.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3111151078",
    "type": "article"
  },
  {
    "title": "Automatic Recommendation of a Distance Measure for Clustering Algorithms",
    "doi": "https://doi.org/10.1145/3418228",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Xiaoyan Zhu; Yingbin Li; Jiayin Wang; Tian Zheng; Jingwen Fu",
    "corresponding_authors": "",
    "abstract": "With a large number of distance measures, the appropriate choice for clustering a given data set with a specified clustering algorithm becomes an important problem. In this article, an automatic distance measure recommendation method for clustering algorithms is proposed. The recommendation method consists of the following steps: (1) metadata extraction, including meta-feature collection and meta-target identification; (2) recommendation model construction using metadata; and (3) distance measure recommendation for a new data set by the recommendation model. Two different types of meta-targets and meta-learning techniques are utilized considering the possible different requirements of users. To validate the necessity and effectiveness of the distance measure recommendation method, an empirical study is conducted with 199 publicly available data sets, 9 distance measures, and 2 widely used clustering algorithms. The experimental results indicate that distance measure significantly influences the performance of the clustering algorithm for a given data set. Furthermore, performance analysis of the proposed recommendation method proves its effectiveness.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3112291449",
    "type": "article"
  },
  {
    "title": "Noise Corrected Sampling of Online Social Networks",
    "doi": "https://doi.org/10.1145/3434749",
    "publication_date": "2021-03-05",
    "publication_year": 2021,
    "authors": "Michele Coscia",
    "corresponding_authors": "Michele Coscia",
    "abstract": "In this article, we propose a new method to perform topological network sampling. Topological network sampling is a process for extracting a subset of nodes and edges from a network, such that analyses on the sample provide results and conclusions comparable to the ones they would return if run on whole structure. We need network sampling because the largest online network datasets are accessed through low-throughput application programming interface (API) systems, rendering the collection of the whole network infeasible. Our method is inspired by the literature on network backboning, specifically the noise-corrected backbone. We select the next node to explore by following the edge we identify as the one providing the largest information gain, given the topology of the sample explored so far. We evaluate our method against the most commonly used sampling methods. We do so in a realistic framework, considering a wide array of network topologies, network analysis, and features of API systems. There is no method that can provide the best sample in all possible scenarios, thus in our results section, we show the cases in which our method performs best and the cases in which it performs worst. Overall, the noise-corrected network sampling performs well: it has the best rank average among the tested methods across a wide range of applications.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3134531423",
    "type": "article"
  },
  {
    "title": "TipTap: Approximate Mining of Frequent <i>k</i> -Subgraph Patterns in Evolving Graphs",
    "doi": "https://doi.org/10.1145/3442590",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Muhammad Anis Uddin Nasir; Çiğdem Aslay; Gianmarco De Francisci Morales; Matteo Riondato",
    "corresponding_authors": "",
    "abstract": "\"Perhaps he could dance first and think afterwards, if it isn't too much to ask him.\"S. Beckett, Waiting for GodotGiven a labeled graph, the collection of -vertex induced connected subgraph patterns that appear in the graph more frequently than a user-specified minimum threshold provides a compact summary of the characteristics of the graph, and finds applications ranging from biology to network science. However, finding these patterns is challenging, even more so for dynamic graphs that evolve over time, due to the streaming nature of the input and the exponential time complexity of the problem. We study this task in both incremental and fully-dynamic streaming settings, where arbitrary edges can be added or removed from the graph. We present TipTap, a suite of algorithms to compute high-quality approximations of the frequent -vertex subgraphs w.r.t. a given threshold, at any time (i.e., point of the stream), with high probability. In contrast to existing state-of-the-art solutions that require iterating over the entire set of subgraphs in the vicinity of the updated edge, TipTap operates by efficiently maintaining a uniform sample of connected -vertex subgraphs, thanks to an optimized neighborhood-exploration procedure. We provide a theoretical analysis of the proposed algorithms in terms of their unbiasedness and of the sample size needed to obtain a desired approximation quality. Our analysis relies on sample-complexity bounds that use Vapnik-Chervonenkis dimension, a key concept from statistical learning theory, which allows us to derive a sufficient sample size that is independent from the size of the graph. The results of our empirical evaluation demonstrates that TipTap returns high-quality results more efficiently and accurately than existing baselines.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3156588038",
    "type": "article"
  },
  {
    "title": "Link Recommendation for Social Influence Maximization",
    "doi": "https://doi.org/10.1145/3449023",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Federico Corò; Gianlorenzo D’Angelo; Yllka Velaj",
    "corresponding_authors": "",
    "abstract": "Social link recommendation systems, like “People-you-may-know” on Facebook, “Who-to-follow” on Twitter, and “Suggested-Accounts” on Instagram assist the users of a social network in establishing new connections with other users. While these systems are becoming more and more important in the growth of social media, they tend to increase the popularity of users that are already popular. Indeed, since link recommenders aim to predict user behavior, they accelerate the creation of links that are likely to be created in the future and, consequently, reinforce social bias by suggesting few (popular) users, giving few chances to most users to create new connections and increase their popularity. In this article, we measure the popularity of a user by means of her social influence, which is her capability to influence other users’ opinions, and we propose a link recommendation algorithm that evaluates the links to suggest according to their increment in social influence instead of their likelihood of being created. In detail, we give a factor approximation algorithm for the problem of maximizing the social influence of a given set of target users by suggesting a fixed number of new connections considering the Linear Threshold model as model for diffusion. We experimentally show that, with few new links and small computational time, our algorithm is able to increase by far the social influence of the target users. We compare our algorithm with several baselines and show that it is the most effective one in terms of increased influence.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3160533917",
    "type": "article"
  },
  {
    "title": "Fast and Robust Dictionary-based Classification for Image Data",
    "doi": "https://doi.org/10.1145/3449360",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Shaoning Zeng; Bob Zhang; Jianping Gou; Yong Xu; Wei Huang",
    "corresponding_authors": "",
    "abstract": "Dictionary-based classification has been promising in knowledge discovery from image data, due to its good performance and interpretable theoretical system. Dictionary learning effectively supports both small- and large-scale datasets, while its robustness and performance depends on the atoms of the dictionary most of the time. Empirically, using a large number of atoms is helpful to obtain a robust classification, while robustness cannot be ensured when setting a small number of atoms. However, learning a huge dictionary dramatically slows down the speed of classification, which is especially worse on the large-scale datasets. To address the problem, we propose a Fast and Robust Dictionary-based Classification (FRDC) framework, which fully utilizes the learned dictionary for classification by staging - and -norms to obtain a robust sparse representation. The new objective function, on the one hand, introduces an additional -norm term upon the conventional -norm optimization, which generates a more robust classification. On the other hand, the optimization based on both - and -norms is solved in two stages, which is much easier and faster than current solutions. In this way, even when using a limited size of dictionary, which makes sure the classification runs very fast, it still can gain higher robustness for multiple types of image data. The optimization is then theoretically analyzed in a new formulation, close but distinct to elastic-net, to prove it is crucial to improve the performance under the premise of robustness. According to our extensive experiments conducted on four image datasets for face and object classification, FRDC keeps generating a robust classification no matter whether using a small or large number of atoms. This guarantees a fast and robust dictionary-based image classification. Furthermore, when simply using deep features extracted via some popular pre-trained neural networks, it outperforms many state-of-the-art methods on the specific datasets.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3161044356",
    "type": "article"
  },
  {
    "title": "Robust Image Representation via Low Rank Locality Preserving Projection",
    "doi": "https://doi.org/10.1145/3434768",
    "publication_date": "2021-06-18",
    "publication_year": 2021,
    "authors": "Shuai Yin; Yanfeng Sun; Junbin Gao; Yongli Hu; Boyue Wang; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Locality preserving projection (LPP) is a dimensionality reduction algorithm preserving the neighhorhood graph structure of data. However, the conventional LPP is sensitive to outliers existing in data. This article proposes a novel low-rank LPP model called LR-LPP. In this new model, original data are decomposed into the clean intrinsic component and noise component. Then the projective matrix is learned based on the clean intrinsic component which is encoded in low-rank features. The noise component is constrained by the ℓ 1 -norm which is more robust to outliers. Finally, LR-LPP model is extended to LR-FLPP in which low-dimensional feature is measured by F-norm. LR-FLPP will reduce aggregated error and weaken the effect of outliers, which will make the proposed LR-FLPP even more robust for outliers. The experimental results on public image databases demonstrate the effectiveness of the proposed LR-LPP and LR-FLPP.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3174874173",
    "type": "article"
  },
  {
    "title": "Opinion Dynamics Optimization by Varying Susceptibility to Persuasion via Non-Convex Local Search",
    "doi": "https://doi.org/10.1145/3466617",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Rediet Abebe; T-H. Hubert Chan; Jon Kleinberg; Zhibin Liang; David C. Parkes; Mauro Sozio; Charalampos E. Tsourakakis",
    "corresponding_authors": "",
    "abstract": "A long line of work in social psychology has studied variations in people’s susceptibility to persuasion—the extent to which they are willing to modify their opinions on a topic. This body of literature suggests an interesting perspective on theoretical models of opinion formation by interacting parties in a network: in addition to considering interventions that directly modify people’s intrinsic opinions, it is also natural to consider interventions that modify people’s susceptibility to persuasion. In this work, motivated by this fact, we propose an influence optimization problem. Specifically, we adopt a popular model for social opinion dynamics, where each agent has some fixed innate opinion, and a resistance that measures the importance it places on its innate opinion; agents influence one another’s opinions through an iterative process. Under certain conditions, this iterative process converges to some equilibrium opinion vector. For the unbudgeted variant of the problem, the goal is to modify the resistance of any number of agents (within some given range) such that the sum of the equilibrium opinions is minimized; for the budgeted variant, in addition the algorithm is given upfront a restriction on the number of agents whose resistance may be modified. We prove that the objective function is in general non-convex. Hence, formulating the problem as a convex program as in an early version of this work (Abebe et al., KDD’18) might have potential correctness issues. We instead analyze the structure of the objective function, and show that any local optimum is also a global optimum, which is somehow surprising as the objective function might not be convex. Furthermore, we combine the iterative process and the local search paradigm to design very efficient algorithms that can solve the unbudgeted variant of the problem optimally on large-scale graphs containing millions of nodes. Finally, we propose and evaluate experimentally a family of heuristics for the budgeted variant of the problem.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3186154190",
    "type": "article"
  },
  {
    "title": "Network Public Opinion Detection During the Coronavirus Pandemic: A Short-Text Relational Topic Model",
    "doi": "https://doi.org/10.1145/3480246",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Yuanchun Jiang; Ruicheng Liang; Ji Zhang; Jianshan Sun; Yezheng Liu; Yang Qian",
    "corresponding_authors": "",
    "abstract": "Online social media provides rich and varied information reflecting the significant concerns of the public during the coronavirus pandemic. Analyzing what the public is concerned with from social media information can support policy-makers to maintain the stability of the social economy and life of the society. In this article, we focus on the detection of the network public opinions during the coronavirus pandemic. We propose a novel Relational Topic Model for Short texts (RTMS) to draw opinion topics from social media data. RTMS exploits the feature of texts in online social media and the opinion propagation patterns among individuals. Moreover, a dynamic version of RTMS (DRTMS) is proposed to capture the evolution of public opinions. Our experiment is conducted on a real-world dataset which includes 67,592 comments from 14,992 users. The results demonstrate that, compared with the benchmark methods, the proposed RTMS and DRTMS models can detect meaningful public opinions by leveraging the feature of social media data. It can also effectively capture the evolution of public concerns during different phases of the coronavirus pandemic.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3210412721",
    "type": "article"
  },
  {
    "title": "Mining for Topics to Suggest Knowledge Model Extensions",
    "doi": "https://doi.org/10.1145/2997657",
    "publication_date": "2016-12-03",
    "publication_year": 2016,
    "authors": "Carlos M. Lorenzetti; Ana Gabriela Maguitman; David Leake; Filippo Menczer; Thomas Reichherzer",
    "corresponding_authors": "",
    "abstract": "Electronic concept maps, interlinked with other concept maps and multimedia resources, can provide rich knowledge models to capture and share human knowledge. This article presents and evaluates methods to support experts as they extend existing knowledge models, by suggesting new context-relevant topics mined from Web search engines. The task of generating topics to support knowledge model extension raises two research questions: first, how to extract topic descriptors and discriminators from concept maps; and second, how to use these topic descriptors and discriminators to identify candidate topics on the Web with the right balance of novelty and relevance. To address these questions, this article first develops the theoretical framework required for a “topic suggester” to aid information search in the context of a knowledge model under construction. It then presents and evaluates algorithms based on this framework and applied in E xtender , an implemented tool for topic suggestion. E xtender has been developed and tested within CmapTools, a widely used system for supporting knowledge modeling using concept maps. However, the generality of the algorithms makes them applicable to a broad class of knowledge modeling systems, and to Web search in general.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2560163623",
    "type": "article"
  },
  {
    "title": "Lifecycle Modeling for Buzz Temporal Pattern Discovery",
    "doi": "https://doi.org/10.1145/2994605",
    "publication_date": "2016-12-09",
    "publication_year": 2016,
    "authors": "Yi Chang; Makoto Yamada; Antonio Ortega; Yan Liu",
    "corresponding_authors": "",
    "abstract": "In social media analysis, one critical task is detecting a burst of topics or buzz , which is reflected by extremely frequent mentions of certain keywords in a short-time interval. Detecting buzz not only provides useful insights into the information propagation mechanism, but also plays an essential role in preventing malicious rumors. However, buzz modeling is a challenging task because a buzz time-series often exhibits sudden spikes and heavy tails, wherein most existing time-series models fail. In this article, we propose novel buzz modeling approaches that capture the rise and fade temporal patterns via Product Lifecycle (PLC) model, a classical concept in economics. More specifically, we propose to model multiple peaks in buzz time-series with PLC mixture or PLC group mixture and develop a probabilistic graphical model (K-Mixture of Product Lifecycle ( K-MPLC ) to automatically discover inherent lifecycle patterns within a collection of buzzes. Furthermore, we effectively utilize the model parameters of PLC mixture or PLC group mixture for burst prediction. Our experimental results show that our proposed methods significantly outperform existing leading approaches on buzz clustering and buzz-type prediction.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2560628693",
    "type": "article"
  },
  {
    "title": "Streaming Data Preprocessing via Online Tensor Recovery for Large Environmental Sensor Networks",
    "doi": "https://doi.org/10.1145/3532189",
    "publication_date": "2022-05-04",
    "publication_year": 2022,
    "authors": "Yue Hu; Ao Qu; Yanbing Wang; Daniel B. Work",
    "corresponding_authors": "",
    "abstract": "Measuring the built and natural environment at a fine-grained scale is now possible with low-cost urban environmental sensor networks. However, fine-grained city-scale data analysis is complicated by tedious data cleaning including removing outliers and imputing missing data. While many methods exist to automatically correct anomalies and impute missing entries, challenges still exist on data with large spatial-temporal scales and shifting patterns. To address these challenges, we propose an online robust tensor recovery (OLRTR) method to preprocess streaming high-dimensional urban environmental datasets. A small-sized dictionary that captures the underlying patterns of the data is computed and constantly updated with new data. OLRTR enables online recovery for large-scale sensor networks that provide continuous data streams, with a lower computational memory usage compared to offline batch counterparts. In addition, we formulate the objective function so that OLRTR can detect structured outliers, such as faulty readings over a long period of time. We validate OLRTR on a synthetically degraded National Oceanic and Atmospheric Administration temperature dataset, and apply it to the Array of Things city-scale sensor network in Chicago, IL, showing superior results compared with several established online and batch-based low-rank decomposition methods.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3198381955",
    "type": "article"
  },
  {
    "title": "Privacy-Preserving Mechanisms for Multi-Label Image Recognition",
    "doi": "https://doi.org/10.1145/3491231",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Honghui Xu; Zhipeng Cai; Wei Li",
    "corresponding_authors": "",
    "abstract": "Multi-label image recognition has been an indispensable fundamental component for many real computer vision applications. However, a severe threat of privacy leakage in multi-label image recognition has been overlooked by existing studies. To fill this gap, two privacy-preserving models, Privacy-Preserving Multi-label Graph Convolutional Networks (P2-ML-GCN) and Robust P2-ML-GCN (RP2-ML-GCN), are developed in this article, where differential privacy mechanism is implemented on the model’s outputs so as to defend black-box attack and avoid large aggregated noise simultaneously. In particular, a regularization term is exploited in the loss function of RP2-ML-GCN to increase the model prediction accuracy and robustness. After that, a proper differential privacy mechanism is designed with the intention of decreasing the bias of loss function in P2-ML-GCN and increasing prediction accuracy. Besides, we analyze that a bounded global sensitivity can mitigate excessive noise’s side effect and obtain a performance improvement for multi-label image recognition in our models. Theoretical proof shows that our two models can guarantee differential privacy for model’s outputs, weights and input features while preserving model robustness. Finally, comprehensive experiments are conducted to validate the advantages of our proposed models, including the implementation of differential privacy on model’s outputs, the incorporation of regularization term into loss function, and the adoption of bounded global sensitivity for multi-label image recognition.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4205711500",
    "type": "article"
  },
  {
    "title": "Computer Science Diagram Understanding with Topology Parsing",
    "doi": "https://doi.org/10.1145/3522689",
    "publication_date": "2022-03-18",
    "publication_year": 2022,
    "authors": "Shaowei Wang; Lingling Zhang; Xuan Luo; Yi Yang; Xin Hu; Tao Qin; Jun Liu",
    "corresponding_authors": "",
    "abstract": "Diagram is a special form of visual expression for representing complex concepts, logic, and knowledge, which widely appears in educational scenes such as textbooks, blogs, and encyclopedias. Current research on diagrams preliminarily focuses on natural disciplines such as Biology and Geography, whose expressions are still similar to natural images. In this article, we construct the first novel geometric type of diagrams dataset in Computer Science field, which has more abstract expressions and complex logical relations. The dataset has exhaustive annotations of objects and relations for about 1,300 diagrams and 3,500 question-answer pairs. We introduce the tasks of diagram classification (DC) and diagram question answering (DQA) based on the new dataset, and propose the Diagram Paring Net (DPN) that focuses on analyzing the topological structure and text information of diagrams. We use DPN-based models to solve DC and DQA tasks, and compare the performances to well-known natural images classification models and visual question answering models. Our experiments show the effectiveness of the proposed DPN-based models on diagram understanding tasks, also indicate that our dataset is more complex compared to previous natural image understanding datasets. The presented dataset opens new challenges for research in diagram understanding, and the DPN method provides a novel perspective for studying such data. Our dataset can be available from https://github.com/WayneWong97/CSDia.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4220983034",
    "type": "article"
  },
  {
    "title": "Semi-Supervised Graph Pattern Matching and Rematching for Expert Community Location",
    "doi": "https://doi.org/10.1145/3532623",
    "publication_date": "2022-05-02",
    "publication_year": 2022,
    "authors": "Lei Li; Mengjiao Yan; Zhenchao Tao; Huanhuan Chen; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Graph pattern matching (GPM) is widely used in social network analysis, such as expert finding, social group query, and social position detection. Technically, GPM is to find matched subgraphs that meet the requirements of pattern graphs in big social networks. In the application of expert community location, the nodes in the pattern graph and data graph represent expert entities, and the edges represent previous cooperations between them. However, the existing GPM methods focus on shortening the matching time and without considering the preference of the decision maker (DM), which makes it difficult for the DM to find ideal teams from numerous matches to complete the assigned task. In this article, as for the process of graph pattern matching and rematching, with a preferred expert set, i.e., the DM hopes that one or more experts in this set will appear in matched subgraphs, we propose a Dual Simulation-based Edge Sequencing-oriented Semi-Supervised GPM method (DsEs-ssGPM). In addition, considering a preferred expert set and a dispreferred expert set together, the DM hopes that experts in the dispreferred expert set will not appear in final matches, so we have the DsEs-ssGPM+ method. Technically, these DsEs-ssGPM methods conduct the matching process from the preferred expert set during dual simulation-based edge sequencing, and based on the edge sequence, these edges are searched recursively. Especially, as for the rematching process, when the preferred and/or the dispreferred expert sets change continuously, to process the GPM again is unnecessary and it is possible to revise the previous matched results partially with DsEs-ssGPM methods. Experiments on four large datasets demonstrate the effectiveness, efficiency and stability of our proposed DsEs-ssGPM methods, and the necessity of introducing an edge sequencing mechanism.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4225262111",
    "type": "article"
  },
  {
    "title": "Meta-Information Fusion of Hierarchical Semantics Dependency and Graph Structure for Structured Text Classification",
    "doi": "https://doi.org/10.1145/3537971",
    "publication_date": "2022-05-17",
    "publication_year": 2022,
    "authors": "Shaokang Wang; Li Pan; Yu Wu",
    "corresponding_authors": "",
    "abstract": "Structured text with plentiful hierarchical structure information is an important part in real-world complex texts. Structured text classification is attracting more attention in natural language processing due to the increasing complexity of application scenarios. Most existing methods treat structured text from a local hierarchy perspective, focusing on the semantics dependency and the graph structure of the structured text independently. However, structured text has global hierarchical structures with sophisticated dependency when compared to unstructured text. According to the variety of structured texts, it is not appropriate to use the existing methods directly. The function of distinction information within semantics dependency and graph structure for structured text, referred to as meta-information, should be stated more precisely. In this article, we propose HGMETA, a novel meta-information embedding frame network for structured text classification, to obtain the fusion embedding of hierarchical semantics dependency and graph structure in a structured text, and to distill the meta-information from fusion characteristics. To integrate the global hierarchical features with fused structured text information, we design a hierarchical LDA module and a structured text embedding module. Specially, we employ a multi-hop message passing mechanism to explicitly incorporate complex dependency into a meta-graph. The meta-information is constructed from meta-graph via neighborhood-based propagation to distill redundant information. Furthermore, using an attention-based network, we investigate the complementarity of semantics dependency and graph structure based on global hierarchical characteristics and meta-information. Finally, the fusion embedding and the meta-information can be straightforwardly incorporated for structured text classification. Experiments conducted on three real-world datasets show the effectiveness of meta-information and demonstrate the superiority of our method.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4280641716",
    "type": "article"
  },
  {
    "title": "On Dynamically Pricing Crowdsourcing Tasks",
    "doi": "https://doi.org/10.1145/3544018",
    "publication_date": "2022-06-14",
    "publication_year": 2022,
    "authors": "Xiaoye Miao; Huanhuan Peng; Yunjun Gao; Zongfu Zhang; Jianwei Yin",
    "corresponding_authors": "",
    "abstract": "Crowdsourcing techniques have been extensively explored in the past decade, including task allocation, quality assessment, and so on. Most of professional crowdsourcing platforms adopt the fixed pricing scheme to offer a fixed price for crowd tasks. It is neither incentive for crowd workers to produce good performance, nor profitable for the requester to gain high utility with low budget. In this article, we study the problem of pricing crowdsourcing tasks with optional bonuses. We propose a dynamic pricing mechanism, named CrowdPricer for incentively delivering bonuses to the crowd workers of completing tasks, in addition to offering a base payment for completing a task. We leverage a deep time sequence model to learn the effect of bonuses on workers’ quality for crowd tasks. CrowdPricer makes decisions on whether to provide bonuses on workers, so as to maximize the requester’s utility in expectation. We present an efficient bonus delivery algorithm under the help of beam search technique, in order to efficiently solve the decision making problem. Extensive experiments using both a real crowdsourcing platform and simulations demonstrate that CrowdPricer yields the higher utility for the requester. It also obtains more correct crowd answers than the state-of-the-art pricing methods.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4282840193",
    "type": "article"
  },
  {
    "title": "A Weighted Ensemble Classification Algorithm Based on Nearest Neighbors for Multi-Label Data Stream",
    "doi": "https://doi.org/10.1145/3570960",
    "publication_date": "2022-11-30",
    "publication_year": 2022,
    "authors": "Hongxin Wu; Meng Han; zhiqiang Chen; Muhang Li; Xilong Zhang",
    "corresponding_authors": "",
    "abstract": "With the rapid development of data stream, multi-label algorithms for mining dynamic data become more and more important. At the same time, when data distribution changes, concept drift will occur, which will make the existing classification models lose effectiveness. Ensemble methods have been used for multi-label classification, but few methods consider both the accuracy and diversity of base classifiers. To address the above-mentioned problem, a Weighted Ensemble classification algorithm based on Nearest Neighbors for Multi-Label data stream (WENNML) is proposed. WENNML uses data blocks to train Active candidate Ensemble Classifiers (AEC) and Passive candidate Ensemble Classifiers (PEC). The base classifiers of AEC and PEC are dynamically updated using geometric and diversity weighting methods. When the difference value between the number of current instances and the number of warning instances reaches the passive warning value, the algorithm selects the optimal base classifiers from AEC and PEC according to the subset accuracy and hamming score and puts them into the predictive ensemble classifiers. Experiments are carried out on 12 kinds of datasets with 9 comparison algorithms. The results show that WENNML achieves the best average rankings among the four evaluation metrics.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4311011941",
    "type": "article"
  },
  {
    "title": "<scp>SILVAN</scp> : Estimating Betweenness Centralities with Progressive Sampling and Non-uniform Rademacher Bounds",
    "doi": "https://doi.org/10.1145/3628601",
    "publication_date": "2023-10-20",
    "publication_year": 2023,
    "authors": "Leonardo Pellegrina; Fabio Vandin",
    "corresponding_authors": "",
    "abstract": "“Sim Sala Bim!” —Silvan, https://en.wikipedia.org/wiki/Silvan_(illusionist) Betweenness centrality is a popular centrality measure with applications in several domains and whose exact computation is impractical for modern-sized networks. We present SILVAN , a novel, efficient algorithm to compute, with high probability, accurate estimates of the betweenness centrality of all nodes of a graph and a high-quality approximation of the top- k betweenness centralities. SILVAN follows a progressive sampling approach and builds on novel bounds based on Monte Carlo Empirical Rademacher Averages, a powerful and flexible tool from statistical learning theory. SILVAN relies on a novel estimation scheme providing non-uniform bounds on the deviation of the estimates of the betweenness centrality of all the nodes from their true values and a refined characterisation of the number of samples required to obtain a high-quality approximation. Our extensive experimental evaluation shows that SILVAN extracts high-quality approximations while outperforming, in terms of number of samples and accuracy, the state-of-the-art approximation algorithm with comparable quality guarantees.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3171018669",
    "type": "article"
  },
  {
    "title": "Misinformation Blocking Problem in Virtual and Real Interconversion Social Networks",
    "doi": "https://doi.org/10.1145/3578936",
    "publication_date": "2023-01-03",
    "publication_year": 2023,
    "authors": "Peikun Ni; Jianming Zhu; Guoqing Wang",
    "corresponding_authors": "",
    "abstract": "With the in-depth development of intelligent media technology, online and offline fusion, reality and virtual entanglement, information content generalization, the boundary between positive and negative information is blurred, all kinds of misinformation in the social network fission spread, and cyberspace governance has become a global consensus. In this article, we comprehensively consider the spread of misinformation in location-based interpersonal social network and online social network, and systematically tackle the novel problem of minimizing the influence of misinformation under individual protection strategies. We first analyze the complexity and modularity of the problem. Then, we leverage the Lovász extension to devise a nonsubmodular set function continuity approximate convex relaxation method, and develop an approximate projected subgradient procedure to obtain a solution with a factor approximate guarantee. Finally, experiments on three assembled real-world datasets demonstrate the effectiveness and feasibility of our designed method and developed the algorithm.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4313442089",
    "type": "article"
  },
  {
    "title": "Optimal Scale-Free Small-World Graphs with Minimum Scaling of Cover Time",
    "doi": "https://doi.org/10.1145/3583691",
    "publication_date": "2023-02-10",
    "publication_year": 2023,
    "authors": "Wanyue Xu; Zhongzhi Zhang",
    "corresponding_authors": "",
    "abstract": "The cover time of random walks on a graph has found wide practical applications in different fields of computer science, such as crawling and searching on the World Wide Web and query processing in sensor networks, with the application effects dependent on the behavior of cover time: the smaller the cover time, the better the application performance. It was proved that over all graphs with $N$ nodes, complete graphs have the minimum cover time $N\\log N$. However, complete graphs cannot mimic real-world networks with small average degree and scale-free small-world properties, for which the cover time has not been examined carefully, and its behavior is still not well understood. In this paper, we first experimentally evaluate the cover time for various real-world networks with scale-free small-world properties, which scales as $N\\log N$. To better understand the behavior of the cover time for real-world networks, we then study the cover time of three scale-free small-world model networks by using the connection between cover time and resistance diameter. For all the three networks, their cover time also behaves as $N\\log N$. This work indicates that sparse networks with scale-free and small-world topology are favorable architectures with optimal scaling of cover time. Our results deepen understanding the behavior of cover time in real-world networks with scale-free small-world structure, and have potential implications in the design of efficient algorithms related to cover time.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4319966434",
    "type": "article"
  },
  {
    "title": "DuCape: Dual Quaternion and Capsule Network–Based Temporal Knowledge Graph Embedding",
    "doi": "https://doi.org/10.1145/3589644",
    "publication_date": "2023-03-28",
    "publication_year": 2023,
    "authors": "Sensen Zhang; Xun Liang; Hui Tang; Xiangping Zheng; Alex X. Zhang; Yuefeng Ma",
    "corresponding_authors": "",
    "abstract": "Recently, with the development of temporal knowledge graph technology, more and more Temporal Knowledge Graph Embedded (TKGE) models have been developed. The effectiveness of TKGE largely depends on the ability to model intrinsic relation patterns and capture specific information about entities and relations. However, existing approaches can capture only some of them with insufficient modeling capacity, and none has a “deep” architecture for modeling the entries in a quadruple at the same dimension. In this article, we propose a more powerful KGE framework named DuCape , which combines a dual quaternion and capsule network in modeling for the first time to make up for the defects of existing TKGE models. In dual quaternion vector space, the head entity learns a k -dimensional rigid transformation parametrized by relation and time, falling near its corresponding tail entity. Further, we employ the embeddings of entities, relations, and time trained from dual quaternion vector space as the input to capsule networks. Experimental results on several basic datasets show that the DuCape model constructed in this article is superior to existing state-of-the-art models.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4361217552",
    "type": "article"
  },
  {
    "title": "Low-rank Representation with Adaptive Dimensionality Reduction via Manifold Optimization for Clustering",
    "doi": "https://doi.org/10.1145/3589767",
    "publication_date": "2023-04-17",
    "publication_year": 2023,
    "authors": "Haoran Chen; Chen Xu; Hongwei Tao; Zuhe Li; Xiao Wang",
    "corresponding_authors": "",
    "abstract": "The dimensionality reduction techniques are often used to reduce data dimensionality for computational efficiency or other purposes in existing low-rank representation (LRR)-based methods. However, the two steps of dimensionality reduction and learning low-rank representation coefficients are implemented in an independent way; thus, the adaptability of representation coefficients to the original data space may not be guaranteed. This article proposes a novel model, i.e., low-rank representation with adaptive dimensionality reduction (LRRARD) via manifold optimization for clustering, where dimensionality reduction and learning low-rank representation coefficients are integrated into a unified framework. This model introduces a low-dimensional projection matrix to find the projection that best fits the original data space. And the low-dimensional projection matrix and the low-rank representation coefficients interact with each other to simultaneously obtain the best projection matrix and representation coefficients. In addition, a manifold optimization method is employed to obtain the optimal projection matrix, which is an unconstrained optimization method in a constrained search space. The experimental results on several real datasets demonstrate the superiority of our proposed method.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4366087567",
    "type": "article"
  },
  {
    "title": "CoupledGT: Coupled Geospatial-temporal Data Modeling for Air Quality Prediction",
    "doi": "https://doi.org/10.1145/3604616",
    "publication_date": "2023-06-19",
    "publication_year": 2023,
    "authors": "Siyuan Ren; Bin Guo; Ke Li; Qianru Wang; Qinfen Wang; Zhiwen Yu",
    "corresponding_authors": "",
    "abstract": "Air pollution seriously affects public health, while effective air quality prediction remains a challenging problem since the complex spatial-temporal couplings exist in multi-area monitoring data of the city. Current approaches rarely consider relative geographical locations when capturing spatial-temporal relations, instead the latent inter-dependencies (i.e., implicit spatial relations) of data as a replacement. However, such relations cannot necessarily reflect the diffusion of air pollutants in the real world, and genuine location-related information could be lost during the implicit relation learning process. In this article, we introduce a new concept, geospatial-temporal data, and propose a novel deep neural network architecture, CoupledGT, to learn the geospatial-temporal couplings within data for air quality prediction. Specifically, the asymmetric diffusion relation of air quality data between two areas is first explicitly represented by the newly developed planar Gaussian diffusion (PGD) equation. And then, a geospatial couplings diffuser (GCD) is designed to parameterize the PGD equation and learn multi-areas diffusion mutually affected geospatial couplings. Besides, the RNN is employed to capture temporal couplings of each area, and incorporated with GCD to learn both shared and unique characteristics of the geospatial-temporal data simultaneously, which empowers the generalization and efficiency of the model. Extensive experiments on two real-world datasets demonstrate our method is robust and outperforms existing baseline methods in air quality prediction tasks.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4381185750",
    "type": "article"
  },
  {
    "title": "Resisting the Edge-Type Disturbance for Link Prediction in Heterogeneous Networks",
    "doi": "https://doi.org/10.1145/3614099",
    "publication_date": "2023-08-07",
    "publication_year": 2023,
    "authors": "Huan Wang; Ruigang Liu; Chuanqi Shi; Junyang Chen; Lei Fang; Shun Liu; Zhiguo Gong",
    "corresponding_authors": "",
    "abstract": "The rapid development of heterogeneous networks has proposed new challenges to the long-standing link prediction problem. Existing models trained on the verified edge samples from different types usually learn type-specific knowledge, and their type-specific predictions may be contradictory for unverified edge samples with uncertain types. This challenge is termed edge-type disturbance in link prediction in heterogeneous networks. To address this challenge, we develop a disturbance-resilient prediction method ( DRPM ) comprising a structural characterizer, a type differentiator, and a resilient predictor. The structural characterizer is responsible for learning edge representations for link prediction. Concurrently, the type differentiator distinguishes type-specific edge representations to generate diverse type experts while maximizing their link prediction performances on specific types. Furthermore, the resilient predictor evaluates the reliability weights of different type experts to develop a resilient prediction mechanism to aggregate discriminable predictions. Extensive experiments conducted on various real-world datasets demonstrate the importance of the explainable introduction of the edge-type disturbance and the superiority of DRPM over state-of-the-art methods.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4385636941",
    "type": "article"
  },
  {
    "title": "A Clique-Querying Mining Framework for Discovering High Utility Co-Location Patterns without Generating Candidates",
    "doi": "https://doi.org/10.1145/3617378",
    "publication_date": "2023-08-23",
    "publication_year": 2023,
    "authors": "Lizhen Wang; Vanha Tran; Thanh-Cong Do",
    "corresponding_authors": "",
    "abstract": "Groups of spatial features whose instances frequently appear together in nearby areas are regarded as prevalent co-location patterns (PCPs). Traditional PCP mining ignores the significance of instances and features. However, in reality, these instances and features have different significance, the traditional PCPs may not sufficiently expose knowledge from spatial data. This study focuses on discovering high utility co-location patterns (HUCPs) in which each instance is assigned a utility to reflect its significance. To filter HUCPs, an adaptive utility participation index (UPI) is designed. Unfortunately, the UPI does not hold the downward closure property. The performance of mining HUCPs is very inefficient since unnecessary candidates cannot be early pruned. Thus, an efficient clique-querying mining framework is devised without generating candidates. This framework first divides neighboring instances into cliques, then compacts these cliques into a hash table structure. Next, the adaptive UPI of any patterns can be quickly calculated based on their participating instances that are obtained by executing a querying scheme on the hash table. Finally, HUCPs are filtered efficiently. The effectiveness and efficiency of the proposed method are proved in both theory and experiments to make a promise that the patterns mined are more meaningful and the mining performance is significantly improved compared to the previous methods.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386096278",
    "type": "article"
  },
  {
    "title": "MEGA: Meta-Graph Augmented Pre-Training Model for Knowledge Graph Completion",
    "doi": "https://doi.org/10.1145/3617379",
    "publication_date": "2023-08-25",
    "publication_year": 2023,
    "authors": "Yashen Wang; Xiaoye Ouyang; Dayu Guo; Xiaoling Zhu",
    "corresponding_authors": "",
    "abstract": "Nowadays, a large number of Knowledge Graph Completion (KGC) methods have been proposed by using embedding based manners, to overcome the incompleteness problem faced with knowledge graph (KG). One important recent innovation in Natural Language Processing (NLP) domain is the employ of deep neural models that make the most of pre-training, culminating in BERT, the most popular example of this line of approaches today. Recently, a series of new KGC methods introducing a pre-trained language model, such as KG-BERT, have been developed and released compelling performance. However, previous pre-training based KGC methods usually train the model by using simple training task and only utilize one-hop relational signals in KG, which leads that they cannot model high-order semantic contexts and multi-hop complex relatedness. To overcome this problem, this article presents a novel pre-training framework for KGC task, which especially consists of both one-hop relation level task (low-order) and multi-hop meta-graph level task (high-order). Hence, the proposed method can capture not only the elaborate sub-graph structure but also the subtle semantic information on the given KG. The empirical results show the efficiency of the proposed method on the widely used real-world datasets.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386167896",
    "type": "article"
  },
  {
    "title": "REST: Debiased Social Recommendation via Reconstructing Exposure Strategies",
    "doi": "https://doi.org/10.1145/3624986",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Ruichu Cai; Fengzhu Wu; Zijian Li; Jie Qiao; Wei Chen; Yuexing Hao; Hao Gu",
    "corresponding_authors": "",
    "abstract": "The recommendation system, relying on historical observational data to model the complex relationships among users and items, has achieved great success in real-world applications. Selection bias is one of the most important issues of the existing observational data-based approaches, which is actually caused by multiple types of unobserved exposure strategies (e.g., promotions and holiday effects). Though various methods have been proposed to address this problem, they are mainly relying on the implicit debiasing techniques but not explicitly modeling the unobserved exposure strategies. By explicitly Reconstructing Exposure STrategies (REST), we formalize the recommendation problem as the counterfactual reasoning and propose the debiased social recommendation method. In REST, we assume that the exposure of an item is controlled by the latent exposure strategies, the user, and the item. Based on the above generation process, we first provide the theoretical guarantee of our method via identification analysis. Second, we employ a variational auto-encoder to reconstruct the latent exposure strategies, with the help of the social networks and the items. Third, we devise a counterfactual reasoning based recommendation algorithm by leveraging the recovered exposure strategies. Experiments on four real-world datasets, including three published datasets and one private WeChat Official Account dataset, demonstrate significant improvements over several state-of-the-art methods.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386890420",
    "type": "article"
  },
  {
    "title": "Rationalizing Graph Neural Networks with Data Augmentation",
    "doi": "https://doi.org/10.1145/3638781",
    "publication_date": "2023-12-28",
    "publication_year": 2023,
    "authors": "Gang Liu; Eric Inae; Tengfei Luo; Meng Jiang",
    "corresponding_authors": "",
    "abstract": "Graph rationales are representative subgraph structures that best explain and support the graph neural network (GNN) predictions. Graph rationalization involves the joint identification of these subgraphs during GNN training, resulting in improved interpretability and generalization. GNN is widely used for node-level tasks such as paper classification and graph-level tasks such as molecular property prediction. However, on both levels, little attention has been given to GNN rationalization and the lack of training examples makes it difficult to identify the optimal graph rationales. In this work, we address the problem by proposing a unified data augmentation framework with two novel operations on environment subgraphs to rationalize GNN prediction. We define the environment subgraph as the remaining subgraph after rationale identification and separation. The framework efficiently performs rationale–environment separation in the representation space for a node’s neighborhood graph or a graph’s complete structure to avoid the high complexity of explicit graph decoding and encoding. We conduct experiments on 17 datasets spanning node classification, graph classification, and graph regression. Results demonstrate that our framework is effective and efficient in rationalizing and enhancing GNNs for different levels of tasks on graphs.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4390317538",
    "type": "article"
  },
  {
    "title": "Nonlinear Correct and Smooth for Graph-Based Semi-Supervised Learning",
    "doi": "https://doi.org/10.1145/3712604",
    "publication_date": "2025-01-21",
    "publication_year": 2025,
    "authors": "Yuanhang Shao; Xiuwen Liu",
    "corresponding_authors": "",
    "abstract": "Graph-based semi-supervised learning (GSSL) has achieved significant success across various applications by leveraging the graph structure and labeled samples for classification tasks. In the field of GSSL, Label Propagation (LP) and Graph Neural Networks (GNNs) are two complementary methods, in which LP iteratively propagates and updates node labels through connected nodes, whereas GNNs aggregate node features by incorporating information from their neighbors. Recently, the complementary nature of LP and GNNs has been utilized to improve performance through the combination of two approaches. However, the utilization of higher-order graph structures within these combined approaches, such as triangles, is still underexplored. Therefore, to advance understanding in this ongoing research, we first model GSSL as a two-step Feature-Label process. Then, we introduce Nonlinear Correct and Smooth (NLCS) in the post-processing step, a combined method that incorporates nonlinearity and higher-order structures into the residual propagation to handle intricate node relationships effectively. We propose a new synthetic graph generator to deepen the analysis and broaden the experimentation, providing insights into the mechanisms that enable NLCS to handle intricate node relationships effectively. Our systematic evaluations across six synthetic graphs show that NLCS outperforms base predictions by an average of 12.44% and the existing state-of-the-art post-processing method by 8.04%. Furthermore, on six commonly used real-world datasets, NLCS demonstrates a 10.9% improvement over six base prediction models and a 1.6% over the state-of-the-art post-processing method. Our comparisons and analyses reveal that NLCS substantially enhances the prediction accuracy of nodes within complex graph structures by effectively utilizing higher-order structures of graphs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406686634",
    "type": "article"
  },
  {
    "title": "Detecting Both Seen and Unseen Anomalies in Time Series",
    "doi": "https://doi.org/10.1145/3717071",
    "publication_date": "2025-02-20",
    "publication_year": 2025,
    "authors": "Chen Liu; Shibo He; Shizhong Li; Zhenyu Shi; Wenchao Meng",
    "corresponding_authors": "",
    "abstract": "A plethora of methods for time series anomaly detection has surfaced recently, encompassing both supervised and unsupervised settings. However, few approaches are designed to accommodate both settings simultaneously. Moreover, methods tailored for supervised scenarios often struggle to identify out-of-distribution (OOD) anomalies, while those for unsupervised scenarios may easily overlook in-distribution (ID) anomalies. To bridge this gap, we propose InvAD, a unified framework capable of detecting both ID and OOD anomalies, which seamlessly scales between unsupervised and supervised settings. To address the potential information loss in existing feature extraction techniques, our framework incorporates an information-preserving invertible neural network (INN) with a mathematical guarantee. Specifically, InvAD adopts a dual-branch structure with a shared feature encoder stacked by INN blocks. This encoder decomposes original intricate signals into ID features and OOD features without discarding any information. For ID anomalies, we treat all training data as ID samples and push their OOD features close to a predefined constant value. This step aims to encode all valuable information of ID samples within the ID features, which are then employed to recognize seen anomalies through a classifier. For OOD anomalies, during the training stage, we endeavor to reconstruct original signals from ID features and the constant value through the backward process of the feature encoder. OOD anomalies are detected when a significant discrepancy exists between the reconstructed and original signals during the testing stage. Additionally, while contrastive learning has shown remarkable success in time series analysis, the construction of effective sample pairs remains underexplored. To address this, we introduce a label-guided contrastive learning module. This module leverages pseudo labels provided by the classifier to correct false positive pairs generated by conventional data augmentation methods. Extensive experiments on 12 real-world datasets validate the superiority of InvAD under unsupervised, supervised, and weakly supervised settings. Furthermore, ablation studies demonstrate that the pseudo labels can effectively enhance the performance of contrastive learning in time series anomaly detection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407766101",
    "type": "article"
  },
  {
    "title": "Hypergraph-Enhanced Multi-Granularity Stochastic Weight Completion in Sparse Road Networks",
    "doi": "https://doi.org/10.1145/3719013",
    "publication_date": "2025-02-21",
    "publication_year": 2025,
    "authors": "Xiaolin Han; Yikun Zhang; Chenhao Ma; Xuequn Shang; Reynold Cheng; Tobias Grubenmann; Xiaodong Li",
    "corresponding_authors": "",
    "abstract": "Road network applications, such as navigation, incident detection, and Point-of-Interest (POI) recommendation, make extensive use of network edge weights (e.g., traveling times). Some of these weights can be missing, especially in a road network where traffic data may not be available for every road. In this paper, we study the stochastic weight completion (SWC) problem, which computes the weight distributions of missing road edges. This is difficult, due to the intricate temporal and spatial correlations among neighboring edges. Besides, the road network can be sparse , i.e., there is a lack of traveling information in a large portion of the network. To tackle these challenges, we propose a multi-granularity framework for Reg ion-wise G raph C ompletion (RegGC). To learn coarse spatial correlations among distantly located roads, we construct a region-wise hypergraph neural architecture based on semantic region dependencies. For finer spatial correlations, we incorporate contextual road network properties (e.g., speed limits, lane counts, and road types). Moreover, it incorporates recent and periodic dimensions of road traffic. We evaluate RegGC against ten existing methods on three real road network datasets. They show that RegGC is more effective and efficient than state-of-the-art solutions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407841690",
    "type": "article"
  },
  {
    "title": "Forecasting Graph-Based Time-Dependent Data with Graph Sequence Attention",
    "doi": "https://doi.org/10.1145/3721435",
    "publication_date": "2025-03-04",
    "publication_year": 2025,
    "authors": "Yang Li; Di Wang; José M. F. Moura",
    "corresponding_authors": "",
    "abstract": "Forecasting graph-based, time-dependent data has broad practical applications but presents challenges. Effective models must capture both spatial and temporal dependencies in the data, while also incorporating auxiliary information to enhance prediction accuracy. In this paper, we identify limitations in current state-of-the-art models regarding temporal dependency handling. To overcome this, we introduce GSA-Forecaster, a new deep learning model designed for forecasting in graph-based, time-dependent contexts. GSA-Forecaster utilizes graph sequence attention, a new attention mechanism proposed in this paper, to effectively manage temporal dependencies. GSA-Forecaster integrates the data’s graph structure directly into its architecture, addressing spatial dependencies. Additionally, it incorporates auxiliary information to refine its predictions further. We validate its performance using real-world graph-based, time-dependent datasets, where it demonstrates superior effectiveness compared to existing state-of-the-art models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408126100",
    "type": "article"
  },
  {
    "title": "A Relation-Constraint Link Prediction Model for Dynamic Knowledge Graphs with Entity Drift",
    "doi": "https://doi.org/10.1145/3725815",
    "publication_date": "2025-03-25",
    "publication_year": 2025,
    "authors": "Xiulin Zheng; Peipei Li; Zan Zhang; Jia Wu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Knowledge Graphs (KGs) often suffer from incompleteness and this issue motivates the task of Knowledge Graph Completion (KGC). Traditional KGC models mainly concentrate on static KGs with a fixed set of entities and relations, or dynamic KGs with temporal characteristics, faltering in their generalization to constantly evolving KGs with possible irregular entity drift. Thus, in this paper, we propose a novel link prediction model based on the embedding representation to handle the incompleteness of KGs with entity drift, termed as DCEL. Unlike traditional link prediction, DCEL could generate precise embeddings for drifted entity without imposing any regular temporal characteristic. The drifted entity is added into the KG with its links to the existing entity predicted in an incremental fashion with no requirement to retrain the whole KG for computational efficiency. In terms of DCEL model, it fully takes advantages of unstructured textual description, and is composed of four modules, namely MRC (Machine Reading Comprehension), RCAA (Relation Constraint Attentive Aggregator), RSA (Relation Specific Alignment) and RCEO (Relation Constraint Embedding Optimization). Specifically, the MRC module is first employed to extract short texts from long and redundant descriptions. Then, RCAA is used to aggregate the embeddings of textual description of drifted entity and the pre-trained word embeddings learned from corpus to a single text-based entity embedding while shielding the impact of noise and irrelevant information. After that, RSA is applied to align the text-based entity embedding to graph-based space to obtain the corresponding graph-based entity embedding, and then the learned embeddings are fed into the gate structure to be optimized based on the RCEO to improve the accuracy of representation learning. Finally, the graph-based model TransE is used to perform link prediction for drifted entity. Extensive experiments conducted on benchmark datasets in terms of evaluation protocols of MRR and Hits@ \\(k\\) reveal the superiority of DCEL model compared to its SOTAs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408811909",
    "type": "article"
  },
  {
    "title": "Intent-aware Recommendation Based on Principal Component Analysis",
    "doi": "https://doi.org/10.1145/3731761",
    "publication_date": "2025-04-24",
    "publication_year": 2025,
    "authors": "Ai Yan; Chaoqun Li; Liangxiao Jiang",
    "corresponding_authors": "",
    "abstract": "In recommender systems, exploring user intents allows for a better understanding and exploration of user preferences, thereby improving recommendation performance. However, existing methods for modeling user intents often do so by statically setting the intent count, which can result in redundancy or insufficiency in capturing the full range of user intents. In order to solve this problem, this paper proposes a model named I ntent-aware Recommendation Based on P rincipal C omponent A nalysis (Intent PCA). Intent PCA is a novel application of PCA in the field of recommender systems. This model defines intents as users’ preferences for some specific relations shown on a knowledge graph, and constructs a user-relation matrix to calculate users’ preferences for relations. Then PCA is applied on the user-relation matrix to extract user intents. Benefit from good characteristics of PCA, our PCA-based user intent extraction model can comprehensively model user intents while simultaneously avoid intent redundancy. Moreover, by combining the user intents, this paper designs an intent-based information propagation method to differentially aggregate information from surrounding neighbor nodes. Experiments conducted on three datasets validate the effectiveness of the proposed Intent PCA model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409735516",
    "type": "article"
  },
  {
    "title": "Distributed Keyword-guided Topic Model with Lexical Knowledge Supervision",
    "doi": "https://doi.org/10.1145/3737881",
    "publication_date": "2025-05-30",
    "publication_year": 2025,
    "authors": "Rui Wang; Yanan Wang; Ziang Li; Haitao Cheng; Guozi Sun",
    "corresponding_authors": "",
    "abstract": "Topic models are often used to discover latent semantic patterns from document collections. However, existing unsupervised approaches have the following drawbacks: 1). The mined topics may not match user interests; 2). They are prone to extract semantically similar topics and sacrifice diversity; 3). The mined topics often have low interpretability, which does not meet common sense knowledge. To address these limitations, we propose the Distributed Keyword-guided Topic Model (DiskTM) that incorporates Gaussian-distributed keyword prior knowledge into the modeling process to mine user-interested topics. Furthermore, to inject common-sense knowledge and improve the topic’s interpretability, we extend DiskTM and propose the Distributed Keyword-guided Topic Model with Lexical Knowledge (DiskTM-LK). Experimental results on three publicly available text corpora show that our proposed approaches could extract topics that match user interests (keywords). Moreover, DiskTM and DiskTM-LK could also obtain more coherent and diverse topics, outperforming the state-of-the-art baseline approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410891545",
    "type": "article"
  },
  {
    "title": "Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation",
    "doi": "https://doi.org/10.1145/3742436",
    "publication_date": "2025-06-02",
    "publication_year": 2025,
    "authors": "Haifeng Sun; Yuanyi Wang; Han Li; Wei Tang; Zirui Zhuang; Qi Qi; Jingyu Wang",
    "corresponding_authors": "",
    "abstract": "Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments. Despite substantial advances in aggregation-based weakly supervised EA, the underlying mechanisms in this setting remain unexplored. In this paper, we present a propagation perspective to analyze weakly supervised EA and explain the existing aggregation-based EA models. Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities. We further prove that, despite the structural heterogeneity across different KGs, the potentially aligned entities within aggregation-based EA models exhibit isomorphic subgraphs, a fundamental yet underexplored premise of EA. Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs. We develop a general EA framework, PipEA, incorporating this operator to improve the accuracy of every type of aggregation-based model without altering the learning process. Extensive experiments substantiate our theoretical findings and demonstrate PipEA’s significant performance gains over state-of-the-art weakly supervised EA methods. Our work advances the field and enhances our comprehension of aggregation-based weakly supervised EA.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410953459",
    "type": "article"
  },
  {
    "title": "Robust distributed estimation for modal regression under least squares approximation",
    "doi": "https://doi.org/10.1145/3742477",
    "publication_date": "2025-06-03",
    "publication_year": 2025,
    "authors": "Zhaodu Zhang; Yue Chao; Xuejun Ma",
    "corresponding_authors": "",
    "abstract": "Massive datasets pose a serious challenge to traditional statistical methods. Modal regression has greater robustness and high inference efficiency compared to mean regression and likelihood-based methods. In this paper, we present a robust distributed least squares approximation (RDLSA) method for heterogeneously distributed massive datasets under the modal regression framework. Specifically, we first approximate the local objective function for each worker/server/machine by using the Taylor expansion, where it is necessary to remain the main quadratic term. Then, each of local machines calculate their corresponding estimates and uploads them to a master machine for obtaining the approximated aggregated estimator. This process yields a one-step global estimator such that one round of communication is required. Correspondingly, the consistency and asymptotic normality of the estimator are rigorously established under some mild conditions. To further reduce the estimation bias, we perform one Newton-Raphson iteration to obtain a two-step global aggregated estimator. In addition, we develop a variable selection procedure for the distributed modal regression under the robust least squares approximation procedure. Finally, we provide simulation experiments and a real data application to verify the superiority of our method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410995641",
    "type": "article"
  },
  {
    "title": "FeadSeq: A Personalized Federated Anomaly Detection Framework for Discrete Event Sequences",
    "doi": "https://doi.org/10.1145/3742896",
    "publication_date": "2025-06-05",
    "publication_year": 2025,
    "authors": "Wei Guan; Jian Cao; Haiyan Zhao; Yang Gu; Shiyou Qian",
    "corresponding_authors": "",
    "abstract": "Event sequence anomaly detection has garnered considerable attention in research, encompassing applications such as identifying anomalies in system logs, anomalous transaction users, etc. Yet, prevailing anomaly detection methods often rely solely on local data for training, potentially leading to imperfect detection performance. In this article, we introduce a personalized Fe derated a nomaly d etection framework for discrete event Seq uences, named FeadSeq. Specifically, we propose a separate architecture for sequence reconstruction networks (SEPRE) which partitions the network into two parts: a shared part and a standalone part, better suited for federated learning schemes. In tandem, we propose a novel partial shared federated learning scheme that employs a mask strategy to alleviate communication overhead and produce personalized local models to address the statistical heterogeneity of data among clients. This scheme dictates that a subset of weights is communicated between clients and servers for collaborative training, while the remaining weights are trained exclusively locally. To evaluate the effectiveness of FeadSeq, we conduct extensive experiments on both system logs and business process event logs. The results affirm the superiority of FeadSeq over existing personalized federated learning algorithms, showcasing not only improved performance but also reduced communication overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411059854",
    "type": "article"
  },
  {
    "title": "Convergence Guaranteed Federated Learning through Gradient Trajectory Smoothing with Triple-Objective Decomposition",
    "doi": "https://doi.org/10.1145/3743142",
    "publication_date": "2025-06-06",
    "publication_year": 2025,
    "authors": "Haoran Zong; Xiao Zhang; R Q Li; Jian-Hui Duan; Derun Zou; Wenzhong Li",
    "corresponding_authors": "",
    "abstract": "Federated Learning (FL) has been widely adopted as a distributed machine learning paradigm aiming to derive a global model without transferring local data to the server. In the context of heterogeneous environments typical of many FL deployments, our research has identified the performance oscillation problem in existing FL methods, resulting in slow convergence and severe performance drop. In this paper, we first investigate the global optimizing objective in FL and demonstrate that, due to data heterogeneity and partial client participation, the global updates in a single training epoch may diverge from the intended objectives of conventional FL methods. To address this problem, we introduce a triple-objective decomposition mechanism to decompose the overarching global objective into three distinct local objectives aimed at aligning client gradients. Subsequently, we propose a gradient trajectory smoothing technique known as FedGTS, which refines local updates by estimating a pseudo-gradient leveraging historical global update trajectories. This approach is designed to mitigate performance oscillations and enhance the stability of the learning process. We theoretically demonstrate that our approach reduces variance of local updates and achieves a guaranteed convergence rate. We experimentally show that the proposed method outperforms the baselines with faster convergence and higher accuracy. Extensive experiments validate the effectiveness of the proposed approach across various heterogeneity settings. Our codes are publicly available on GitHub 1 .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411089358",
    "type": "article"
  },
  {
    "title": "PRIME: Pretraining for Patient Condition Representation with Irregular Multimodal Electronic Health Records",
    "doi": "https://doi.org/10.1145/3744251",
    "publication_date": "2025-06-10",
    "publication_year": 2025,
    "authors": "Bohao Li; Bowen Du; Junchen Ye",
    "corresponding_authors": "",
    "abstract": "With the increasing collection of electronic health records (EHRs), deep learning has become a crucial tool for real-time treatment analysis. However, due to patient privacy concerns, the scarcity of labeled data limits the end-to-end models that rely on large training data. Self-supervised pretraining offers a promising solution. Nevertheless, applying pretraining to EHRs faces two key issues: 1) EHRs exhibit multimodality, including monitoring data and recorded clinical note. For multimodal pretraining, designing a self-supervised task that can establish cross-modal associations while preserving all modal-unique information remains challenging. 2) Both modalities are sequential and irregular, with varying intervals between monitoring or records. Aligning monitoring times with recorded times poses a significant issue for fine-grained cross-modal pretraining. Existing pretraining models either focus on a single modality or only models regular data, failing to address them together. To fill this gap and fully utilize unlabel EHRs data, we propose a p retraining model to learn patient r epresentation using unlabel i rregular m ultimodal E HRs, named PRIME. We first utilize a multi-element encoding module to extract patient condition snapshots from both modalities. Then, to construct multiple aligned cross-modal positive sample pairs that span the entire treatment process from irregular data, we employ patient condition alignment modules that integrate time-aware and feature-aware components to transfer snapshots to the aligned timestamps. Next, to preserve both shared and unique information of each modality, our decoupled representation learning strategy first uses a constraint matrix to separate shared information. We then employ contrastive-based cross-modal learning and reconstruction-based intra-modal learning to model shared and complete information respectively. Extensive experiments on two real-world tasks demonstrate the superiority of PRIME over the state-of-the-art models, especially with limited labels.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411176027",
    "type": "article"
  },
  {
    "title": "Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing",
    "doi": "https://doi.org/10.1145/3743143",
    "publication_date": "2025-06-11",
    "publication_year": 2025,
    "authors": "Lucas Potin; Rosa Figueiredo; Vincent Labatut; Christine Largeron",
    "corresponding_authors": "",
    "abstract": "Graph classification aims to categorize graphs based on their structural and attribute features, with applications in diverse fields such as social network analysis and bioinformatics. Among the methods proposed to solve this task, those relying on patterns (i.e. subgraphs) provide good explainability, as the patterns used for classification can be directly interpreted. To identify meaningful patterns, a standard approach is to use a quality measure, i.e. a function that evaluates the discriminative power of each pattern. However, the literature provides tens of such measures, making it difficult to select the most appropriate for a given application. Only a handful of surveys try to provide some insight by comparing these measures, and none of them specifically focuses on graphs. This typically results in the systematic use of the most widespread measures, without thorough evaluation. To address this issue, we present a comparative analysis of 38 quality measures from the literature. We characterize them theoretically, based on four mathematical properties. We leverage publicly available datasets to constitute a benchmark, and propose a method to elaborate a gold standard ranking of the patterns. We exploit these resources to perform an empirical comparison of the measures, both in terms of pattern ranking and classification performance. Moreover, we propose a clustering-based preprocessing step, which groups patterns appearing in the same graphs to enhance classification performance. Our experimental results demonstrate the effectiveness of this step, reducing the number of patterns to be processed while achieving comparable performance. Additionally, we show that some popular measures widely used in the literature are not associated with the best results.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411210291",
    "type": "article"
  },
  {
    "title": "Interactive Fusion Label Enhancement for Multi-Label Learning",
    "doi": "https://doi.org/10.1145/3744571",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Xingyu Zhao; Yuexuan An; Ning Xu; Lei Qi; Xin Geng",
    "corresponding_authors": "",
    "abstract": "Multi-label learning (MLL) involves the task of assigning a set of relevant labels to a given instance. Recently, Label Enhancement (LE) has gained significant attention in various MLL tasks, as it allows for effective mining the implicit relative importance information of different labels. However, in existing LE-based MLL methods, the LE process is decoupled from the MLL process. Consequently, the label distribution recovered by the LE process may not be suitable for training the predictive model, thus affecting the overall learning system. In this study, we propose a novel approach named interactive Fusion Label Enhancement for Multi-label learning ( Flem ) that seamlessly integrates the LE process with the MLL process. Specifically, we introduce a matching and interaction mechanism comprising a novel interaction label enhancement loss and a contrastive alignment approach to prevent object mismatch. Furthermore, we present a unified label distribution loss that establishes the relationship between the recovered label distribution and the training of the predictive model. By leveraging these losses, the label distributions obtained from the LE process can be efficiently utilized for training the predictive model. Experimental results on multiple benchmark datasets demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411238361",
    "type": "article"
  },
  {
    "title": "Hierarchical Spatial Decompositions Under Local Differential Privacy",
    "doi": "https://doi.org/10.1145/3744569",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Ece Alptekin; Berkay Kemal Balioglu; Mehmet Emre Gürsoy",
    "corresponding_authors": "",
    "abstract": "The popularity of smartphones, GPS-enabled devices, social networks, and connected vehicles all contribute to the increasing volume of spatial data. Spatial decompositions assist in handling big spatial data, and they have been commonly used in the differential privacy (DP) literature for range query answering, spatial indexing, count-of-counts histograms, data summarization, and visualization. However, their applications under the emerging local differential privacy (LDP) notion are relatively scarce. In this paper, we study the problem of building hierarchical spatial decompositions under LDP, focusing on two methods: quadtrees and kd-trees. We develop two solutions for quadtrees: a baseline solution that is inspired by the centralized DP literature, and a proposed solution that utilizes a single data collection step from users, propagates density estimates to remaining nodes, and performs structural corrections to the quadtree. Since kd-trees rely on node medians which are data-dependent, we observe that it is not feasible to build kd-trees using a single data collection step. We therefore propose an iterative solution that constructs kd-trees in top-down fashion by utilizing a novel algorithm for estimating node medians at each tree depth. We experimentally evaluate our quadtree and kd-tree algorithms using four real-world spatial datasets, multiple utility metrics, varying privacy budgets, and tree parameters. Results demonstrate that our algorithms enable the building of accurate spatial decompositions that provide high utility in practice. Notably, our quadtrees and kd-trees achieve substantially lower errors in answering spatial density queries (up to 10-fold improvement) when compared with a state-of-the-art method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411238461",
    "type": "article"
  },
  {
    "title": "Towards Sequence Utility Maximization under Utility Occupancy Measure",
    "doi": "https://doi.org/10.1145/3744344",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Gengsen Huang; Wensheng Gan; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "The discovery of utility-driven patterns is a valuable and difficult research topic. It can extract significant and interesting information from specific and varied databases, increasing the value of the services provided. In practice, the utility measure is often used to reflect the importance, profit, or risk of an object or pattern. In the database, while utility is a flexible criterion for patterns, it is also a somewhat limited criterion due to the overlook of utility sharing. This leads to the derived patterns only exploring partial and local knowledge in the database. Utility occupancy considers the problem of mining with high utility but low occupancy. However, existing studies are focused on itemsets that cannot reveal the temporal relationship of object occurrences. Therefore, this paper first defines the concept of utility occupancy of sequence data and raises the problem of High Utility Occupancy Sequential Pattern Mining (HUOSPM). Three dimensions, including frequency, utility, and occupancy, are comprehensively evaluated in HUOSPM. An algorithm called Sequence Utility Maximization with Utility occupancy measure (SUMU) is proposed. Furthermore, two data structures for storing pattern-related information, including Utility-Occupancy-List-Chain (UOL-Chain) and Utility-Occupancy-Table (UO-Table), are designed, and six upper bounds are proposed to improve efficiency. Extensive experiments are conducted to evaluate the efficiency and effectiveness of the novel algorithm. A specific case study is provided, and the effects of different upper bounds and pruning strategies are analyzed. The comprehensive results suggest that the HUOSPM task is useful and efficient.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411238518",
    "type": "article"
  },
  {
    "title": "Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis",
    "doi": "https://doi.org/10.1145/3744652",
    "publication_date": "2025-06-13",
    "publication_year": 2025,
    "authors": "Naaek Chinpattanakarn; Chainarong Amornbunchornvej",
    "corresponding_authors": "",
    "abstract": "Knowing who follows whom and what patterns they are following are crucial steps to understand collective behaviors (e.g. a group of human, a school of fish, or a stock market). Time series is one of resources that can be used to get insight regarding following relations. However, the concept of following patterns or motifs and the solution to find them in time series are not obvious. In this work, we formalize a concept of following motifs between two time series and present a framework to infer following patterns between two time series. The framework utilizes one of efficient and scalable methods to retrieve motifs from time series called the Matrix Profile Inference Method. We compare our proposed framework with several baselines. The framework performs better than baselines in the simulation datasets. In the dataset of sound recording, the framework is able to retrieve the following motifs within a pair of time series that two singers sing following each other. In the cryptocurrency dataset, the framework is capable of capturing the following motifs within a pair of time series from two digital currencies, which implies that the values of one currency follow the values of another currency patterns. Our framework can be utilized in any field of time series to get insight regarding following patterns between time series.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411275637",
    "type": "article"
  },
  {
    "title": "Latent Representation Learning for attributed graph Anomaly Detection",
    "doi": "https://doi.org/10.1145/3733604",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Shichao Zhang; Penghui Xi; Jiang Mengqi; Guixian Zhang; Debo Cheng",
    "corresponding_authors": "",
    "abstract": "Anomaly detection in attributed graph data has been widely applied in real applications. However, the intricate topology of graph data, high-dimensional attributes, and class imbalance inherent in anomaly detection tasks render attributed graph anomaly detection a challenging task. To detect anomalies using the intricate topology information of graph data, a dual Masked Autoencoders is proposed for attribute Graph Anomaly Detection, denoted as MAGAD. Specifically, in the MAGAD, the class imbalance in attributed graph data is dealt with by randomly masking the original graph data to obtain masked graph data for the anomaly detection task. And then, a latent representation of the graph data is obtained by training dual autoencoders, where one autoencoder is developed for reconstructing the original graph data, and another for reconstructing randomly masked graph data. This assists in identifying abnormal nodes in the attributed graph data. Subsequently, to capture anomalous information from relevant features, MAGAD uses a random re-masking strategy for latent representations learned from the masked graph. Finally, the anomaly scores of the nodes are calculated using the learned latent representations from the decoders of the dual auto-encoders. Experimental results on five real-world datasets demonstrate that the MAGAD algorithm outperforms state-of-the-art anomaly detection algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411371322",
    "type": "article"
  },
  {
    "title": "Online Learning from Mix-typed, Drifted, and Incomplete Streaming Features",
    "doi": "https://doi.org/10.1145/3744712",
    "publication_date": "2025-06-19",
    "publication_year": 2025,
    "authors": "Shengda Zhuo; Di Wu; Yi He; Shuqiang Huang; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Online learning, where feature spaces can change over time, offers a flexible learning paradigm that has attracted considerable attention. However, it still faces three significant challenges. First, the heterogeneity of real-world data streams with mixed feature types presents challenges for traditional parametric modeling. Second, data stream distributions can shift over time, causing an abrupt and substantial decline in model performance. Additionally, the time and cost constraints make it infeasible to label every data instance in a supervised setting. To overcome these challenges, we propose a new algorithm Online Learning from Mix-typed, Drifted, and Incomplete Streaming Features (OL-MDISF), which aims to relax restrictions on both feature types, data distribution, and supervision information. Our approach involves utilizing copula models to create a comprehensive latent space, employing an adaptive sliding window for detecting drift points to ensure model stability, and establishing label proximity information based on geometric structural relationships. To demonstrate the model’s efficiency and effectiveness, we provide theoretical analysis and comprehensive experimental results.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411448906",
    "type": "article"
  },
  {
    "title": "Knowledge Graph Fine-grained Modeling Network with Contrastive Learning for Recommendation",
    "doi": "https://doi.org/10.1145/3744926",
    "publication_date": "2025-06-20",
    "publication_year": 2025,
    "authors": "Xiya Bu; Yu Liu",
    "corresponding_authors": "",
    "abstract": "Knowledge graph (KG) is often introduced into recommendation systems because of its large amount of edge information. The method based on graph neural networks (GNNs) has gradually become the mainstream of KG-aware recommendation. However, traditional KG-aware recommendation models based on GNNs fail to utilize the dependencies of items and item attributes to model user preferences at a fine-grained level, which will result in a lack of interpretability in the model’s recommendations to users. In addition, traditional KG-aware recommendation models based on GNNs fail to mine supervision signals from the perspective of user preferences and item attributes, which will result in a lack of effective supervision signals in the model. In this study, we utilize a combination of items and attributes behind the items to model user preferences at a fine-grained level, so as to achieve independence between different user preferences. Furthermore, we utilize the KG and the user-item interaction graph (UIIG) to construct the user-specific preference similarity view and the item-specific attribute correlation views, respectively, and then apply the contrastive learning framework to effectively mine the association signals between users and between items. Based on this, we propose a novel model named Knowledge Graph Fine-grained Modeling Network with Contrastive Learning (KGFM-CL). Extensive experiments conducted on two real-world datasets demonstrate that KGFM-CL significantly outperforms state-of-the-art baseline models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411495248",
    "type": "article"
  },
  {
    "title": "On the Use of Relative Validity Indices for Comparing Clustering Approaches",
    "doi": "https://doi.org/10.1145/3748726",
    "publication_date": "2025-07-16",
    "publication_year": 2025,
    "authors": "Luke Yerbury; Ricardo J. G. B. Campello; G. C. Livingston; Mark Goldsworthy; Lachlan O’Neil",
    "corresponding_authors": "",
    "abstract": "Relative Validity Indices (RVIs) such as the Silhouette Width Criterion, Calinski-Harabasz and Davies-Bouldin indices are the most widely used tools for evaluating and optimising clustering outcomes. Traditionally, their ability to rank collections of candidate dataset partitions has been used to guide the selection of the number of clusters, and to compare partitions from different clustering algorithms. However, there is a growing trend in the literature to use RVIs when selecting a Similarity Paradigm (SP) for clustering — the combination of normalisation procedure, representation method, and distance measure which affects the computation of object dissimilarities used in clustering. Despite the growing prevalence of this practice, there has been no empirical or theoretical investigation into the suitability of RVIs for this purpose. Moreover, since RVIs are computed using object dissimilarities, it remains unclear how they would need to be implemented for fair comparisons of different SPs. This study presents the first comprehensive investigation into the reliability of RVIs for SP selection. We conducted extensive experiments with seven popular RVIs on over 2.7 million clustering partitions of synthetic and real-world datasets, encompassing feature-vector and time-series data. We identified fundamental conceptual limitations undermining the use of RVIs for SP selection, and our empirical findings confirmed this predicted unsuitability. Among our recommendations, we suggest instead that practitioners select SPs by using external validation on high quality labelled datasets or carefully designed outcome-oriented objective criteria, both of which should be informed by careful consideration of dataset characteristics, and domain requirements. Our findings have important implications for clustering methodology and evaluation, suggesting the need for more rigorous approaches to SP selection in clustering applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412476219",
    "type": "article"
  },
  {
    "title": "Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout",
    "doi": "https://doi.org/10.1145/3749376",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Ji Liu; Beichen Ma; Qiaolin Yu; Ruoming Jin; Jingbo Zhou; Yang Zhou; Huaiyu Dai; Haixun Wang; Dejing Dou; Patrick Valduriez",
    "corresponding_authors": "",
    "abstract": "Federated Learning (FL) is a promising distributed machine learning approach that enables collaborative training of a global model using multiple edge devices. The data distributed among the edge devices is highly heterogeneous. Thus, FL faces the challenge of data distribution and heterogeneity, where non-Independent and Identically Distributed (non-IID) data across edge devices may yield in significant accuracy drop. Furthermore, the limited computation and communication capabilities of edge devices increase the likelihood of stragglers, thus leading to slow model convergence. In this paper, we propose the FedDHAD FL framework, which comes with two novel methods: Dynamic Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH dynamically adjusts the weights of each local model within the model aggregation process based on the non-IID degree of heterogeneous data to deal with the statistical data heterogeneity. FedAD performs neuron-adaptive operations in response to heterogeneous devices to improve accuracy while achieving superb efficiency. The combination of these two methods makes FedDHAD significantly outperform state-of-the-art solutions in terms of accuracy (up to 6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to 15.0% smaller).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412581601",
    "type": "article"
  },
  {
    "title": "MCAKE: Memory Augmented Autoencoder with Contrastive Learning for Unsupervised Anomaly Detection",
    "doi": "https://doi.org/10.1145/3759460",
    "publication_date": "2025-08-12",
    "publication_year": 2025,
    "authors": "Chengsen Wang; Qi Qi; Jinming Wu; Haifeng Sun; Zirui Zhuang; Yuhan Jing; Lianyuan Li; Jingyu Wang",
    "corresponding_authors": "",
    "abstract": "Recently, reconstruction-based deep models have gained widespread usage in unsupervised anomaly detection. However, they may overlook some anomalies owing to the over-generalization of neural networks. Several studies have incorporated memory networks to mitigate this problem. Nonetheless, some of them lack an explicit memory updating process, while others rely on data-driven updating methods that are sensitive to initial values and unsuitable for end-to-end training. Additionally, the traditional criterion for detection computed in the high-dimensional input space may collapse as the spike in the deviation score is averaged across numerous dimensions. To address these challenges, we propose MCAKE, a M emory-augmented C ontrastive A utoencoder with K NN-based E xtraction. It is designed to highlight the deviation score for anomalies by reconstructing input using fixed normal prototypes recorded in the memory. We explicitly encourage the memory to be autonomously learned and effectively allocated through contrastive learning with multiple positive and multiple negative samples. Furthermore, we introduce a bivariate detection criterion that calculates anomaly scores considering both input and latent space to tackle the collapse. Extensive experiments on 50 datasets across various categories demonstrate the superiority of our approach, with a 2% relative improvement over the previous state-of-the-art models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413105678",
    "type": "article"
  },
  {
    "title": "Convolutional Network Integrated with Frequency Adaptive Learning for Multivariate Time Series Classification",
    "doi": "https://doi.org/10.1145/3761818",
    "publication_date": "2025-08-18",
    "publication_year": 2025,
    "authors": "Yingxia Tang; Yanxuan Wei; Yupeng Hu; Xiangwei Zheng; Cun Ji",
    "corresponding_authors": "",
    "abstract": "Multivariate time series classification (MTSC) is a significant research topic in the realm of data mining, with broad applications in different industries, including healthcare, finance, meteorology, and traffic. While existing studies have designed many classifiers based on LSTMs, CNNs, and Transformer, the sophisticated architectures raise concerns regarding efficiency in computation. Additionally, most methods concentrate on a single dimension, typically temporal patterns, without fully considering multi-dimensional information such as the independence and interactions across variables that are essential in multivariate settings. To address these challenges, this article introduces FreConvNet, a lightweight convolutional network integrated with frequency adaptive learning. Inheriting the modular design paradigm of Transformer to achieve multi-view modeling of multivariate time series. FreConvNet consists of two key components: the frequency adaptive block (FAB) and the convolutional feed-forward network (ConvFFN). The FAB leverages the Fourier Transform in conjunction with adaptive filters to capture both long-term and short-term dependencies in the temporal dimension. Following that, ConvFFN captures cross-variable and cross-feature interactions by controlling inter-channel information flow through grouped pointwise convolutions, while introducing non-linearity to enhance representational capacity. Extensive experiments conducted on the well-known UEA archive validate that FreConvNet outperforms existing convolution-based, Transformer-based, and hybrid methods in classification performance and offers a computationally efficient solution.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413279295",
    "type": "article"
  },
  {
    "title": "Adaptive Modality Interaction Transformer for Multimodal Knowledge Graph Completion",
    "doi": "https://doi.org/10.1145/3760786",
    "publication_date": "2025-08-18",
    "publication_year": 2025,
    "authors": "Yue Jian; Miao Zhang; Ziyue Qin; Chuan Xie; Kui Xiao; Yan Zhang; Zhifei Li",
    "corresponding_authors": "",
    "abstract": "Knowledge graphs (KGs) are frequently confronted with the challenge of incompleteness, a problem that extends to multimodal knowledge graphs (MKGs). The primary goal of multimodal knowledge graph completion (MKGC) is to predict missing entities within MKGs. However, current MKGC methods face difficulties in adequately addressing modal preferences and imbalances in modal information. To overcome these issues, we introduce AdaMKGC, an innovative hybrid model incorporating an adaptive modality interaction transformer. This model employs a dynamic attention interaction strategy and a self-enhancing sampling approach. AdaMKGC achieves a more precise utilization of multimodal information by integrating modal preference information into modal interactions. Additionally, it effectively mitigates the issue of modal imbalance through targeted sampling and adjustment for entities with deficient information. Experimental evaluations demonstrate AdaMKGC’s superior performance in overcoming these prevalent challenges. Compared to existing state-of-the-art MKGC models, AdaMKGC shows a notable enhancement of 28% in MR on the WN18-IMG dataset and an improvement of 2.7% in Hits@1 on the FB15k-237-IMG dataset. Our code is available at https://github.com/HubuKG/AdaMKGC .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413279402",
    "type": "article"
  },
  {
    "title": "Website Owner Identification through Multi-level Contrastive Representation Learning",
    "doi": "https://doi.org/10.1145/3767155",
    "publication_date": "2025-09-11",
    "publication_year": 2025,
    "authors": "Cheng Tu; Yunshan Ma; Yang Li; Min Zhang; Miao Hu; Fan Shi; Xiang Wang",
    "corresponding_authors": "",
    "abstract": "Website owner identification aims to recognize the organization or individual who owns a given website that is served on the web. It is a crucial step for cyberspace surveying and mapping, playing a significant role in cyberspace administration and governance. Existing widely-employed solutions for website owner identification mainly fall into two paradigms: 1) querying the public information databases such as WHOIS, which store the Internet resource’s registered users or assignees; and 2) directly extracting the organization or individual name of the website owner from the webpage using the technique of named entity recognition. However, the former is less reliable due to the incomplete, encrypted, and outdated records in the public information databases. Meanwhile, the latter requires that the webpages explicitly and precisely present their owner names without ambiguity, which is often hard to guarantee in practice. To address these limitations, we propose to formulate website owner identification as a problem of webpage representation learning, thereby introducing a novel representation learning framework empowered by large language model-based text Re writing and M ulti-level c on trastive learning, named ReMon. First, we devise a prompt to rewrite the webpages using large language models, which effectively filters out noise from the original webpages. Second, we model website-website, website-owner, and owner-owner interactions through multi-level contrastive learning, fully utilizing the self-supervision signals on long-tail items to learn the multi-level constraints. Third, we design a retrieval-based prediction framework and a clustering-based framework to apply websites’ and owners’ representations for different scenarios of the website owner identification task. To evaluate ReMon under our formulation, we construct two datasets based on real-world data. Compared to existing approaches, our ReMon can address the challenging scenarios when valid information cannot be found in public information databases and the owner’s name does not appear on the webpage. Meanwhile, the experimental results show that ReMon outperforms all representation learning-based baselines and significantly enhances training efficiency. The code is available at https://github.com/tuchen9/ReMon .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414115177",
    "type": "article"
  },
  {
    "title": "Towards Fair Decision Boundaries in Clustering: Integrating Disparate Impact Criteria into Maximum Margin Clustering",
    "doi": "https://doi.org/10.1145/3770078",
    "publication_date": "2025-10-01",
    "publication_year": 2025,
    "authors": "Adithya K Moorthy; Jaya Teja Reddy Pochimireddy; Vijaya Saradhi Vedula; Bhanu Prasad",
    "corresponding_authors": "",
    "abstract": "Extensive application of machine learning in the areas that impact human lives have significantly spurred considerable interest in developing algorithms that are demonstrably fair. Recent efforts in this field have led to the creation of numerous algorithms addressing the paradigm of clustering with fairness constraints. In this research, we adopt disparate impact criteria from supervised learning scenarios and incorporate it into clustering by specifically focusing on decision boundary fairness. The existing fairness definitions in clustering scenarios mostly deal with the Balance of the clusters or the representation of the sensitive groups in the clusters. We developed a new algorithm called Fair Maximum Margin Clustering (FMMC), by incorporating the disparate impact criteria into the Maximum Margin Clustering (MMC) algorithm. The FMMC algorithm ensures that the distance of each data point from the hyperplane is uncorrelated with that data point’s sensitive attribute value. This constraint is designed to prevent any sensitive group from being negatively impacted by the decision boundary. We show that the performance of the FMMC algorithm is better than that of MMC algorithm in terms of traditional fairness measures such as Balance. We also demonstrate that the FMMC algorithm achieves fair clustering while maintaining the clustering performance of the original MMC algorithm. We validate the effectiveness of our approach through experiments on synthetic and real-world datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414688402",
    "type": "article"
  },
  {
    "title": "Fast Assessment of Eulerian Trails in Graphs with Applications",
    "doi": "https://doi.org/10.1145/3771997",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Alessio Conte; Roberto Grossi; Grigorios Loukides; Nadia Pisanti; Solon P. Pissis; Giulia Punzi",
    "corresponding_authors": "",
    "abstract": "Enumerating or counting combinatorial objects in graphs is a fundamental data mining task. We consider the problem of assessing the number of Eulerian trails in directed graphs, which is formalized as follows: Given a directed graph \\(G=(V,E)\\) , with \\(|V|=n\\) nodes and \\(|E|=m\\) edges, and an integer \\(z\\) , assess whether the number \\(\\#ET(G)\\) of Eulerian trails of \\(G\\) is at least \\(z\\) . This problem underlies many applications in domains ranging from data privacy to computational biology, data compression, and transportation networks. Practitioners currently address this problem by applying the famous BEST theorem, which, in fact, counts \\(\\#ET(G)\\) instead of just assessing whether \\(\\#ET(G)\\geq z\\) . Unfortunately, this solution takes \\(\\mathcal{O}(n^{\\omega})\\) arithmetic operations, where \\(\\omega&lt;2.373\\) denotes the matrix multiplication exponent . Since in most real-world graphs, the number \\(m\\) of edges is comparable to the number \\(n\\) of nodes, and \\(z\\) is moderate in practice, the algorithmic challenge is: Can we solve the problem faster for certain values of \\(m\\) and \\(z\\) ? We want to design a combinatorial algorithm for assessing whether \\(\\#ET(G)\\geq z\\) , which does not resort to the BEST theorem and has a predictably bounded cost as a function of \\(m\\) and \\(z\\) . We address this challenge as follows. We first introduce a general algorithmic scheme for assessing (and enumerating) Eulerian trails. We then introduce a novel tree data structure to reduce the number of iterations in this general scheme. Finally, we complement the above with further combinatorial insight leading to an algorithm with a worst-case bound of \\(\\mathcal{O}(m\\cdot\\min\\{z,\\#ET(G)\\})\\) time. Our experiments using six benchmark datasets with multi-million edges from different domains show that our implementations are up to two orders of magnitude faster than the BEST theorem, perform much fewer than \\(mz\\) iterations and scale near-linearly with \\(m\\) in most cases. Our experiments further show that our implementations bring substantial efficiency benefits in a data privacy application which employs the BEST theorem for the assessment.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415435759",
    "type": "article"
  },
  {
    "title": "PHE: Structure and Semantic Enhanced Pre-training of Graph Neural Networks for Large-Scale Heterogeneous Graphs",
    "doi": "https://doi.org/10.1145/3772278",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Shengyin Sun; Chen Ma; Jiehao Chen",
    "corresponding_authors": "",
    "abstract": "In recent years, graph neural networks (GNNs) have facilitated the development of graph data mining. However, training GNNs requires sufficient labeled task-specific data, which is expensive and sometimes unavailable. To be less dependent on labeled data, recent studies propose to pre-train GNNs in a self-supervised manner and then apply the pre-trained GNNs to downstream tasks with limited labeled data. However, most existing methods are designed solely for homogeneous graphs (real-world graphs are mostly heterogeneous) and do not consider semantic mismatch (the semantic difference between the original data and the ideal data containing more transferable semantic information). In this paper, we propose an effective framework to pre-train GNNs on the large-scale heterogeneous graph. We first design a structure-aware pre-training task, which aims to capture structural properties in heterogeneous graphs. Then, we design a semantic-aware pre-training task to tackle the mismatch. Specifically, we construct a perturbation subspace composed of semantic neighbors to help deal with the semantic mismatch. Semantic neighbors make the model focus more on the general knowledge in the semantic space, which in turn assists the model in learning knowledge with better transferability. Finally, extensive experiments are conducted on real-world large-scale heterogeneous graphs to demonstrate the superiority of the proposed method over state-of-the-art baselines. Code available at https://github.com/sunshy-1/PHE .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415435769",
    "type": "article"
  },
  {
    "title": "On disclosure risk analysis of anonymized itemsets in the presence of prior knowledge",
    "doi": "https://doi.org/10.1145/1409620.1409623",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Laks V. S. Lakshmanan; Raymond T. Ng; Ganesh Ramesh",
    "corresponding_authors": "",
    "abstract": "Decision makers of companies often face the dilemma of whether to release data for knowledge discovery, vis-a-vis the risk of disclosing proprietary or sensitive information. Among the various methods employed for “sanitizing” the data prior to disclosure, we focus in this article on anonymization, given its widespread use in practice. We do due diligence to the question “just how safe is the anonymized data?” We consider both those scenarios when the hacker has no information and, more realistically, when the hacker may have partial information about items in the domain. We conduct our analyses in the context of frequent set mining and address the safety question at two different levels: (i) how likely of being cracked (i.e., re-identified by a hacker), are the identities of individual items and (ii) how likely are sets of items cracked? For capturing the prior knowledge of the hacker, we propose a belief function , which amounts to an educated guess of the frequency of each item. For various classes of belief functions which correspond to different degrees of prior knowledge, we derive formulas for computing the expected number of cracks of single items and for itemsets, the probability of cracking the itemsets. While obtaining, exact values for more general situations is computationally hard, we propose a series of heuristics called the O-estimates . They are easy to compute and are shown fairly accurate, justified by empirical results on real benchmark datasets. Based on the O-estimates, we propose a recipe for the decision makers to resolve their dilemma. Our recipe operates at two different levels, depending on whether the data owner wants to reason in terms of single items or sets of items (or both). Finally, we present techniques for ascertaining a hacker's knowledge of correlation in terms of co-occurrence of items likely. This information regarding the hacker's knowledge can be incorporated into our framework of disclosure risk analysis and we present experimental results demonstrating how this knowledge affects the heuristic estimates we have developed.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1993625478",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on bioinformatics",
    "doi": "https://doi.org/10.1145/1342320.1342321",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Mohammed J. Zaki; George Karypis; Jiong Yang; Wei Wang",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2012266507",
    "type": "article"
  },
  {
    "title": "Feature-preserved sampling over streaming data",
    "doi": "https://doi.org/10.1145/1460797.1460798",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Kun-Ta Chuang; Hung-Leng Chen; Ming-Syan Chen⋆",
    "corresponding_authors": "",
    "abstract": "In this article, we explore a novel sampling model, called feature preserved sampling ( FPS ) that sequentially generates a high-quality sample over sliding windows. The sampling quality we consider refers to the degree of consistency between the sample proportion and the population proportion of each attribute value in a window. Due to the time-variant nature of real-world datasets, users are more likely to be interested in the most recent data. However, previous works have not been able to generate a high-quality sample over sliding windows that precisely preserves up-to-date population characteristics. Motivated by this shortcoming, we have developed the FPS algorithm, which has several advantages: (1) it sequentially generates a sample from a time-variant data source over sliding windows; (2) the execution time of FPS is linear with respect to the database size; (3) the relative proportional differences between the sample proportions and population proportions of most distinct attribute values are guaranteed to be below a specified error threshold, ε, while the relative proportion differences of the remaining attribute values are as close to ε as possible, which ensures that the generated sample is of high quality; (4) the sample rate is close to the user specified rate so that a high quality sampling result can be obtained without increasing the sample size; (5) by a thorough analytical and empirical study, we prove that FPS has acceptable space overheads, especially when the attribute values have Zipfian distributions, and FPS can also excellently preserve the population proportion of multivariate features in the sample; and (6) FPS can be applied to infinite streams and finite datasets equally, and the generated samples can be used for various applications. Our experiments on both real and synthetic data validate that FPS can effectively obtain a high quality sample of the desired size. In addition, while using the sample generated by FPS in various mining applications, a significant improvement in efficiency can be achieved without compromising the model's precision.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2020208858",
    "type": "article"
  },
  {
    "title": "Continuous-Time User Modeling in Presence of Badges",
    "doi": "https://doi.org/10.1145/3162050",
    "publication_date": "2018-03-23",
    "publication_year": 2018,
    "authors": "Ali Khodadadi; Seyed Abbas Hosseini; Erfan Tavakoli; Hamid R. Rabiee",
    "corresponding_authors": "",
    "abstract": "User modeling plays an important role in delivering customized web services to the users and improving their engagement. However, most user models in the literature do not explicitly consider the temporal behavior of users. More recently, continuous-time user modeling has gained considerable attention and many user behavior models have been proposed based on temporal point processes. However, typical point process-based models often considered the impact of peer influence and content on the user participation and neglected other factors. Gamification elements are among those factors that are neglected, while they have a strong impact on user participation in online services. In this article, we propose interdependent multi-dimensional temporal point processes that capture the impact of badges on user participation besides the peer influence and content factors. We extend the proposed processes to model user actions over the community-based question and answering websites, and propose an inference algorithm based on Variational-Expectation Maximization that can efficiently learn the model parameters. Extensive experiments on both synthetic and real data gathered from Stack Overflow show that our inference algorithm learns the parameters efficiently and the proposed method can better predict the user behavior compared to the alternatives.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2587446185",
    "type": "article"
  },
  {
    "title": "Combining Structured Node Content and Topology Information for Networked Graph Clustering",
    "doi": "https://doi.org/10.1145/2996197",
    "publication_date": "2017-03-21",
    "publication_year": 2017,
    "authors": "Ting Guo; Jia Wu; Xingquan Zhu; Chengqi Zhang",
    "corresponding_authors": "",
    "abstract": "Graphs are popularly used to represent objects with shared dependency relationships. To date, all existing graph clustering algorithms consider each node as a single attribute or a set of independent attributes, without realizing that content inside each node may also have complex structures. In this article, we formulate a new networked graph clustering task where a network contains a set of inter-connected (or networked) super-nodes, each of which is a single-attribute graph. The new super-node representation is applicable to many real-world applications, such as a citation network where each node denotes a paper whose content can be described as a graph, and citation relationships between papers form a networked graph (i.e., a super-graph). Networked graph clustering aims to find similar node groups, each of which contains nodes with similar content and structure information. The main challenge is to properly calculate the similarity between super-nodes for clustering. To solve the problem, we propose to characterize node similarity by integrating structure and content information of each super-node. To measure node content similarity, we use cosine distance by considering overlapped attributes between two super-nodes. To measure structure similarity, we propose an Attributed Random Walk Kernel (ARWK) to calculate the similarity between super-nodes. Detailed node content analysis is also included to build relationships between super-nodes with shared internal structure information, so the structure similarity can be calculated in a precise way. By integrating the structure similarity and content similarity as one matrix, the spectral clustering is used to achieve networked graph clustering. Our method enjoys sound theoretical properties, including bounded similarities and better structure similarity assessment than traditional graph clustering methods. Experiments on real-world applications demonstrate that our method significantly outperforms baseline approaches.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2601530027",
    "type": "article"
  },
  {
    "title": "A Viewable Indexing Structure for the Interactive Exploration of Dynamic and Large Image Collections",
    "doi": "https://doi.org/10.1145/3047011",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Frédéric Rayar; Sabine Barrat; Fatma Bouali; Gilles Venturini",
    "corresponding_authors": "",
    "abstract": "Thanks to the capturing devices cost reduction and the advent of social networks, the size of image collections is becoming extremely huge. Many works in the literature have addressed the indexing of large image collections for search purposes. However, there is a lack of support for exploratory data mining. One may want to wander around the images and experience serendipity in the exploration process. Thus, effective paradigms not only for organising, but also visualising these image collections become necessary. In this article, we present a study to jointly index and visualise large image collections. The work focuses on satisfying three constraints. First, large image collections, up to million of images, shall be handled. Second, dynamic collections, such as ever-growing collections, shall be processed in an incremental way, without reprocessing the whole collection at each modification. Finally, an intuitive and interactive exploration system shall be provided to the user to allow him to easily mine image collections. To this end, a data partitioning algorithm has been modified and proximity graphs have been used to fit the visualisation purpose. A custom web platform has been implemented to visualise the hierarchical and graph-based hybrid structure. The results of a user evaluation we have conducted show that the exploration of the collections is intuitive and smooth thanks to the proposed structure. Furthermore, the scalability of the proposed indexing method is proved using large public image collections.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2785836887",
    "type": "article"
  },
  {
    "title": "Learning to Infer Competitive Relationships in Heterogeneous Networks",
    "doi": "https://doi.org/10.1145/3051127",
    "publication_date": "2018-02-13",
    "publication_year": 2018,
    "authors": "Yang Yang; Jie Tang; Juanzi Li",
    "corresponding_authors": "",
    "abstract": "Detecting and monitoring competitors is fundamental to a company to stay ahead in the global market. Existing studies mainly focus on mining competitive relationships within a single data source, while competing information is usually distributed in multiple networks. How to discover the underlying patterns and utilize the heterogeneous knowledge to avoid biased aspects in this issue is a challenging problem. In this article, we study the problem of mining competitive relationships by learning across heterogeneous networks. We use Twitter and patent records as our data sources and statistically study the patterns behind the competitive relationships. We find that the two networks exhibit different but complementary patterns of competitions. Overall, we find that similar entities tend to be competitors, with a probability of 4 times higher than chance. On the other hand, in social network, we also find a 10 minutes phenomenon: when two entities are mentioned by the same user within 10 minutes, the likelihood of them being competitors is 25 times higher than chance. Based on the discovered patterns, we propose a novel Topical Factor Graph Model. Generally, our model defines a latent topic layer to bridge the Twitter network and patent network. It then employs a semi-supervised learning algorithm to classify the relationships between entities (e.g., companies or products). We test the proposed model on two real data sets and the experimental results validate the effectiveness of our model, with an average of +46% improvement over alternative methods. Besides, we further demonstrate the competitive relationships inferred by our proposed model can be applied in the job-hopping prediction problem by achieving an average of +10.7% improvement.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2786111244",
    "type": "article"
  },
  {
    "title": "ProgressER",
    "doi": "https://doi.org/10.1145/3154410",
    "publication_date": "2018-03-23",
    "publication_year": 2018,
    "authors": "Yasser Altowim; Dmitri V. Kalashnikov; Sharad Mehrotra",
    "corresponding_authors": "",
    "abstract": "Entity resolution (ER) is the process of identifying which entities in a dataset refer to the same real-world object. In relational ER, the dataset consists of multiple entity-sets and relationships among them. Such relationships cause the resolution of some entities to influence the resolution of other entities. For instance, consider a relational dataset that consists of a set of research paper entities and a set of venue entities. In such a dataset, deciding that two research papers are the same may trigger the fact that their venues are also the same. This article proposes a progressive approach to relational ER, named ProgressER, that aims to produce the highest quality result given a constraint on the resolution budget, specified by the user. Such a progressive approach is useful for many emerging analytical applications that require low latency response (and thus cannot tolerate delays caused by cleaning the entire dataset) and/or in situations where the underlying resources are constrained or costly to use. To maximize the quality of the result, ProgressER follows an adaptive strategy that periodically monitors and reassesses the resolution progress to determine which parts of the dataset should be resolved next and how they should be resolved. More specifically, ProgressER divides the input budget into several resolution windows and analyzes the resolution progress at the beginning of each window to generate a resolution plan for the current window. A resolution plan specifies which blocks of entities and which entity pairs within blocks need to be resolved during the plan execution phase of that window. In addition, ProgressER specifies, for each identified pair of entities, the order in which the similarity functions should be applied on the pair. Such an order plays a significant role in reducing the overall cost because applying the first few functions in this order might be sufficient to resolve the pair. The empirical evaluation of ProgressER demonstrates its significant advantage in terms of progressiveness over the traditional ER techniques for the given problem settings.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2790120012",
    "type": "article"
  },
  {
    "title": "Tied Kronecker Product Graph Models to Capture Variance in Network Populations",
    "doi": "https://doi.org/10.1145/3161885",
    "publication_date": "2018-03-23",
    "publication_year": 2018,
    "authors": "Sebastián Moreno; Jennifer Neville; Sergey Kirshner",
    "corresponding_authors": "",
    "abstract": "Much of the past work on mining and modeling networks has focused on understanding the observed properties of single example graphs. However, in many real-life applications it is important to characterize the structure of populations of graphs. In this work, we analyze the distributional properties of probabilistic generative graph models (PGGMs) for network populations. PGGMs are statistical methods that model the network distribution and match common characteristics of real-world networks. Specifically, we show that most PGGMs cannot reflect the natural variability in graph properties observed across multiple networks because their edge generation process assumes independence among edges. Then, we propose the mixed Kronecker Product Graph Model (mKPGM), a scalable generalization of KPGMs that uses tied parameters to increase the variability of the sampled networks, while preserving the edge probabilities in expectation. We compare mKPGM to several other graph models. The results show that learned mKPGMs accurately represent the characteristics of real-world networks, while also effectively capturing the natural variability in network structure.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2792749098",
    "type": "article"
  },
  {
    "title": "Maximally Correlated Principal Component Analysis Based on Deep Parameterization Learning",
    "doi": "https://doi.org/10.1145/3332183",
    "publication_date": "2019-07-29",
    "publication_year": 2019,
    "authors": "Hao Chen; Jinghua Li; Junbin Gao; Yanfeng Sun; Yongli Hu; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Dimensionality reduction is widely used to deal with high-dimensional data. As a famous dimensionality reduction method, principal component analysis (PCA) aiming at finding the low dimension feature of original data has made great successes, and many improved PCA algorithms have been proposed. However, most algorithms based on PCA only consider the linear correlation of data features. In this article, we propose a novel dimensionality reduction model called maximally correlated PCA based on deep parameterization learning (MCPCADP), which takes nonlinear correlation into account in the deep parameterization framework for the purpose of dimensionality reduction. The new model explores nonlinear correlation by maximizing Ky-Fan norm of the covariance matrix of nonlinearly mapped data features. A new BP algorithm for model optimization is derived. In order to assess the proposed method, we conduct experiments on both a synthetic database and several real-world databases. The experimental results demonstrate that the proposed algorithm is comparable to several widely used algorithms.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2964931725",
    "type": "article"
  },
  {
    "title": "Time-Sync Video Tag Extraction Using Semantic Association Graph",
    "doi": "https://doi.org/10.1145/3332932",
    "publication_date": "2019-07-02",
    "publication_year": 2019,
    "authors": "Wenmian Yang; Kun Wang; Na Ruan; Wenyuan Gao; Weijia Jia; Wei Zhao; Nan Liu; Yunyong Zhang",
    "corresponding_authors": "",
    "abstract": "Time-sync comments (TSCs) reveal a new way of extracting the online video tags. However, such TSCs have lots of noises due to users’ diverse comments, introducing great challenges for accurate and fast video tag extractions. In this article, we propose an unsupervised video tag extraction algorithm named Semantic Weight-Inverse Document Frequency (SW-IDF). Specifically, we first generate corresponding semantic association graph (SAG) using semantic similarities and timestamps of the TSCs. Second, we propose two graph cluster algorithms, i.e., dialogue-based algorithm and topic center-based algorithm, to deal with the videos with different density of comments. Third, we design a graph iteration algorithm to assign the weight to each comment based on the degrees of the clustered subgraphs, which can differentiate the meaningful comments from the noises. Finally, we gain the weight of each word by combining Semantic Weight (SW) and Inverse Document Frequency (IDF). In this way, the video tags are extracted automatically in an unsupervised way. Extensive experiments have shown that SW-IDF (dialogue-based algorithm) achieves 0.4210 F1-score and 0.4932 MAP (Mean Average Precision) in high-density comments, 0.4267 F1-score and 0.3623 MAP in low-density comments; while SW-IDF (topic center-based algorithm) achieves 0.4444 F1-score and 0.5122 MAP in high-density comments, 0.4207 F1-score and 0.3522 MAP in low-density comments. It has a better performance than the state-of-the-art unsupervised algorithms in both F1-score and MAP.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3122161324",
    "type": "article"
  },
  {
    "title": "Formal and computational properties of the confidence boost of association rules",
    "doi": "https://doi.org/10.1145/2541268.2541272",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "José L. Balcázar",
    "corresponding_authors": "José L. Balcázar",
    "abstract": "Some existing notions of redundancy among association rules allow for a logical-style characterization and lead to irredundant bases of absolutely minimum size. We push the intuition of redundancy further to find an intuitive notion of novelty of an association rule, with respect to other rules. Namely, an irredundant rule is so because its confidence is higher than what the rest of the rules would suggest; then, one can ask: how much higher? We propose to measure such a sort of novelty through the confidence boost of a rule. Acting as a complement to confidence and support, the confidence boost helps to obtain small and crisp sets of mined association rules and solves the well-known problem that, in certain cases, rules of negative correlation may pass the confidence bound. We analyze the properties of two versions of the notion of confidence boost, one of them a natural generalization of the other. We develop algorithms to filter rules according to their confidence boost, compare the concept to some similar notions in the literature, and describe the results of some experimentation employing the new notions on standard benchmark datasets. We describe an open source association mining tool that embodies one of our variants of confidence boost in such a way that the data mining process does not require the user to select any value for any parameter.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2067930953",
    "type": "article"
  },
  {
    "title": "Structured Sparse Boosting for Graph Classification",
    "doi": "https://doi.org/10.1145/2629328",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "Hongliang Fei; Jun Huan",
    "corresponding_authors": "",
    "abstract": "Boosting is a highly effective algorithm that produces a linear combination of weak classifiers (a.k.a. base learners) to obtain high-quality classification models. In this article, we propose a generalized logit boost algorithm in which base learners have structural relationships in the functional space. Although such relationships are generic, our work is particularly motivated by the emerging topic of pattern-based classification for semistructured data including graphs. Toward an efficient incorporation of the structure information, we have designed a general model in which we use an undirected graph to capture the relationship of subgraph-based base learners. In our method, we employ both L 1 and Laplacian-based L 2 regularization to logit boosting to achieve model sparsity and smoothness in the functional space spanned by the base learners. We have derived efficient optimization algorithms based on coordinate descent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping base learners and that the resulting estimator is consistent. Additionally, motivated by the connection between logit boosting and logistic regression, we extend our structured sparse regularization framework to logistic regression for vectorial data in which features are structured. Using comprehensive experimental study and comparing our work with the state-of-the-art, we have demonstrated the effectiveness of the proposed learning method.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2079495826",
    "type": "article"
  },
  {
    "title": "Inferring Lifetime Status of Point-of-Interest",
    "doi": "https://doi.org/10.1145/3369799",
    "publication_date": "2020-02-03",
    "publication_year": 2020,
    "authors": "Xinjiang Lu; Zhiwen Yu; Chuanren Liu; Yanchi Liu; Hui Xiong; Bin Guo",
    "corresponding_authors": "",
    "abstract": "A Point-of-Interest (POI) refers to a specific location that people may find useful or interesting. In modern cities, a large number of POIs emerge, grow, stabilize for a period, then finally disappear. The stages (e.g., emerge and grow) in this process are called lifetime statuses of a POI. While a large body of research has been devoted to identifying and recommending POIs, there are few studies on inferring the lifetime status of POIs. Indeed, the predictive analytics of POI lifetime status can be valuable for various tasks, such as urban planning, business site selection, and real estate appraisal. In this article, we propose a multitask learning approach, named inferring POI lifetime status, to inferring the POI lifetime status with multifaceted data sources. Specifically, we first define three types of POI lifetime status, i.e., booming, decaying, and stable. Then, we formulate a serial classification problem to predict the sequential/successive lifetime statuses of POIs over time. Leveraging geographical data and human mobility data, we examine and integrate three aspects of features related to the prosperity of POIs, i.e., region popularity, region demands, and peer competitiveness. Next, as the booming/decaying POIs are relatively rare in our data, we perform stable class decomposition to alleviate the imbalance between stable POIs and booming/decaying POIs. Finally, we develop a POI lifetime status classifier by exploiting the multitask learning framework as well as the multiclass kernel-based vector machines. We perform extensive experiments using large-scale and real-world datasets of New York City. The experimental results validate the effectiveness of our approach to automatically inferring POI lifetime status.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3004827789",
    "type": "article"
  },
  {
    "title": "Data Sharing via Differentially Private Coupled Matrix Factorization",
    "doi": "https://doi.org/10.1145/3372408",
    "publication_date": "2020-05-13",
    "publication_year": 2020,
    "authors": "Beyza Ermiş; Ali Taylan Cemgil",
    "corresponding_authors": "",
    "abstract": "We address the privacy-preserving data-sharing problem in a distributed multiparty setting. In this setting, each data site owns a distinct part of a dataset and the aim is to estimate the parameters of a statistical model conditioned on the complete data without any site revealing any information about the individuals in their own parts. The sites want to maximize the utility of the collective data analysis while providing privacy guarantees for their own portion of the data as well as for each participating individual . Our first contribution is to classify these different privacy requirements as (i) site-level and (ii) user-level differential privacy and present formal privacy guarantees for these two cases under the model of differential privacy. To satisfy a stronger form of differential privacy, we use a variant of differential privacy which is local differential privacy where the sensitive data is perturbed with a randomized response mechanism prior to the estimation. In this study, we assume that the data instances that are partitioned between several parties are arranged as matrices. A natural statistical model for this distributed scenario is coupled matrix factorization. We present two generic frameworks for privatizing Bayesian inference for coupled matrix factorization models that are able to guarantee proposed differential privacy notions based on the privacy requirements of the model. To privatize Bayesian inference, we first exploit the connection between differential privacy and sampling from a Bayesian posterior via stochastic gradient Langevin dynamics and then derive an efficient coupled matrix factorization method. In the local privacy context, we propose two models that have an additional privatization mechanism to achieve a stronger measure of privacy and introduce a Gibbs sampling based algorithm. We demonstrate that the proposed methods are able to provide good prediction accuracy on synthetic and real datasets while adhering to the introduced privacy constraints.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3028131348",
    "type": "article"
  },
  {
    "title": "Hierarchical Physician Recommendation via Diversity-enhanced Matrix Factorization",
    "doi": "https://doi.org/10.1145/3418227",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Hao Wang; Shuai Ding; Yeqing Li; Xiaojian Li; Youtao Zhang",
    "corresponding_authors": "",
    "abstract": "Recent studies have shown that there exhibits significantly imbalanced medical resource allocation across public hospitals. Patients, regardless of their diseases, tend to choose hospitals and physicians with a better reputation, which often overloads major hospitals while leaving others underutilized. Guiding patients to hospitals that can serve their treatment needs both timely and with good quality can make the best use of precious medical resources. Unfortunately, it remains one of the major challenges both for research and in practice. In this article, we propose a novel diversity-enhanced hierarchical physician recommendation approach to address this issue. We adopt matrix factorization to estimate physician competency and exploit implicit similarity relationships to improve the competency estimation of physicians that we are of little information of. We then balance the patient preference and physician diversity using two novel heuristic algorithms. We evaluate our proposed approach and compare it with the state of the art. Experiments show that our approach significantly improves both accuracy and recommendation diversity over existing approaches.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3110955246",
    "type": "article"
  },
  {
    "title": "Recommending Statutes",
    "doi": "https://doi.org/10.1145/3424671",
    "publication_date": "2021-01-04",
    "publication_year": 2021,
    "authors": "Yi Feng; Chuanyi Li; Jidong Ge; Bin Luo; Vincent Ng",
    "corresponding_authors": "",
    "abstract": "Legal judgment prediction, which aims at predicting judgment results such as penalty, charges, and statutes for cases, has attracted much attention recently. In this article, we focus on building a recommender system to predict the associated statutes for a case given the facts of the case as input. For this purpose, we propose a two-step neural network-based machine learning framework to assist judges as well as ordinary people to reduce their effort in finding applicable statutes. The proposed model takes advantage of recurrent neural networks with a max-pooling layer to obtain contextual representations of documents, i.e., the facts associated with the cases. Moreover, an attention mechanism is used to automatically focus on the important words contributing to the prediction of statutes. In addition, we apply an encoder--decoder ranking approach to extract correlations between statutes to achieve more accurate recommendation results. We evaluate our model on a real-world dataset. Experimental results show that, compared with existing baseline methods, our method can predict statutes that are more likely to appear in real judgments.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3121061107",
    "type": "article"
  },
  {
    "title": "Mitigating Class-Boundary Label Uncertainty to Reduce Both Model Bias and Variance",
    "doi": "https://doi.org/10.1145/3429447",
    "publication_date": "2021-03-05",
    "publication_year": 2021,
    "authors": "Matthew Almeida; Yong Zhuang; Wei Ding; Scott E. Crouter; Ping Chen",
    "corresponding_authors": "",
    "abstract": "The study of model bias and variance with respect to decision boundaries is critically important in supervised learning and artificial intelligence. There is generally a tradeoff between the two, as fine-tuning of the decision boundary of a classification model to accommodate more boundary training samples (i.e., higher model complexity) may improve training accuracy (i.e., lower bias) but hurt generalization against unseen data (i.e., higher variance). By focusing on just classification boundary fine-tuning and model complexity, it is difficult to reduce both bias and variance. To overcome this dilemma, we take a different perspective and investigate a new approach to handle inaccuracy and uncertainty in the training data labels, which are inevitable in many applications where labels are conceptual entities and labeling is performed by human annotators. The process of classification can be undermined by uncertainty in the labels of the training data; extending a boundary to accommodate an inaccurately labeled point will increase both bias and variance. Our novel method can reduce both bias and variance by estimating the pointwise label uncertainty of the training set and accordingly adjusting the training sample weights such that those samples with high uncertainty are weighted down and those with low uncertainty are weighted up. In this way, uncertain samples have a smaller contribution to the objective function of the model’s learning algorithm and exert less pull on the decision boundary. In a real-world physical activity recognition case study, the data present many labeling challenges, and we show that this new approach improves model performance and reduces model variance.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3134390489",
    "type": "article"
  },
  {
    "title": "Preserve Integrity in Realtime Event Summarization",
    "doi": "https://doi.org/10.1145/3442344",
    "publication_date": "2021-05-03",
    "publication_year": 2021,
    "authors": "Chen Lin; Zhichao Ouyang; Xiaoli Wang; Hui Li; Zhenhua Huang",
    "corresponding_authors": "",
    "abstract": "Online text streams such as Twitter are the major information source for users when they are looking for ongoing events. Realtime event summarization aims to generate and update coherent and concise summaries to describe the state of a given event. Due to the enormous volume of continuously coming texts, realtime event summarization has become the de facto tool to facilitate information acquisition. However, there exists a challenging yet unexplored issue in current text summarization techniques: how to preserve the integrity, i.e., the accuracy and consistency of summaries during the update process. The issue is critical since online text stream is dynamic and conflicting information could spread during the event period. For example, conflicting numbers of death and injuries might be reported after an earthquake. Such misleading information should not appear in the earthquake summary at any timestamp. In this article, we present a novel realtime event summarization framework called IAEA (i.e., Integrity-Aware Extractive-Abstractive realtime event summarization). Our key idea is to integrate an inconsistency detection module into a unified extractive–abstractive framework. In each update, important new tweets are first extracted in an extractive module, and the extraction is refined by explicitly detecting inconsistency between new tweets and previous summaries. The extractive module is able to capture the sentence-level attention which is later used by an abstractive module to obtain the word-level attention. Finally, the word-level attention is leveraged to rephrase words. We conduct comprehensive experiments on real-world datasets. To reduce efforts required for building sufficient training data, we also provide automatic labeling steps of which the effectiveness has been empirically verified. Through experiments, we demonstrate that IAEA can generate better summaries with consistent information than state-of-the-art approaches.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3159156191",
    "type": "article"
  },
  {
    "title": "App2Vec: Context-Aware Application Usage Prediction",
    "doi": "https://doi.org/10.1145/3451396",
    "publication_date": "2021-06-28",
    "publication_year": 2021,
    "authors": "Huandong Wang; Yong Li; Mu Du; Zhenhui Li; Depeng Jin",
    "corresponding_authors": "",
    "abstract": "Both app developers and service providers have strong motivations to understand when and where certain apps are used by users. However, it has been a challenging problem due to the highly skewed and noisy app usage data. Moreover, apps are regarded as independent items in existing studies, which fail to capture the hidden semantics in app usage traces. In this article, we propose App2Vec, a powerful representation learning model to learn the semantic embedding of apps with the consideration of spatio-temporal context. Based on the obtained semantic embeddings, we develop a probabilistic model based on the Bayesian mixture model and Dirichlet process to capture when , where , and what semantics of apps are used to predict the future usage. We evaluate our model using two different app usage datasets, which involve over 1.7 million users and 2,000+ apps. Evaluation results show that our proposed App2Vec algorithm outperforms the state-of-the-art algorithms in app usage prediction with a performance gap of over 17.0%.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3174793231",
    "type": "article"
  },
  {
    "title": "Dynamically Adjusting Diversity in Ensembles for the Classification of Data Streams with Concept Drift",
    "doi": "https://doi.org/10.1145/3466616",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Juan Isidro González Hidalgo; Silas Garrido Teixeira de Carvalho Santos; Roberto Souto Maior de Barros",
    "corresponding_authors": "",
    "abstract": "A data stream can be defined as a system that continually generates a lot of data over time. Today, processing data streams requires new demands and challenging tasks in the data mining and machine learning areas. Concept Drift is a problem commonly characterized as changes in the distribution of the data within a data stream. The implementation of new methods for dealing with data streams where concept drifts occur requires algorithms that can adapt to several scenarios to improve its performance in the different experimental situations where they are tested. This research proposes a strategy for dynamic parameter adjustment in the presence of concept drifts. Parameter Estimation Procedure (PEP) is a general method proposed for dynamically adjusting parameters which is applied to the diversity parameter (λ) of several classification ensembles commonly used in the area. To this end, the proposed estimation method (PEP) was used to create Boosting-like Online Learning Ensemble with Parameter Estimation (BOLE-PE), Online AdaBoost-based M1 with Parameter Estimation (OABM1-PE), and Oza and Russell’s Online Bagging with Parameter Estimation (OzaBag-PE), based on the existing ensembles BOLE, OABM1, and OzaBag, respectively. To validate them, experiments were performed with artificial and real-world datasets using Hoeffding Tree (HT) as base classifier. The accuracy results were statistically evaluated using a variation of the Friedman test and the Nemenyi post-hoc test. The experimental results showed that the application of the dynamic estimation in the diversity parameter (λ) produced good results in most scenarios, i.e., the modified methods have improved accuracy in the experiments with both artificial and real-world datasets.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3185363055",
    "type": "article"
  },
  {
    "title": "Embedding Heterogeneous Information Network in Hyperbolic Spaces",
    "doi": "https://doi.org/10.1145/3468674",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Yiding Zhang; Xiao Wang; Nian Liu; Chuan Shi",
    "corresponding_authors": "",
    "abstract": "Heterogeneous information network (HIN) embedding, aiming to project HIN into a low-dimensional space, has attracted considerable research attention. Most of the existing HIN embedding methods focus on preserving the inherent network structure and semantic correlations in Euclidean spaces. However, one fundamental problem is whether the Euclidean spaces are the intrinsic spaces of HIN? Recent researches find the complex network with hyperbolic geometry can naturally reflect some properties, e.g., hierarchical and power-law structure. In this article, we make an effort toward embedding HIN in hyperbolic spaces. We analyze the structures of three HINs and discover some properties, e.g., the power-law distribution, also exist in HINs. Therefore, we propose a novel HIN embedding model HHNE. Specifically, to capture the structure and semantic relations between nodes, HHNE employs the meta-path guided random walk to sample the sequences for each node. Then HHNE exploits the hyperbolic distance as the proximity measurement. We also derive an effective optimization strategy to update the hyperbolic embeddings iteratively. Since HHNE optimizes different relations in a single space, we further propose the extended model HHNE++. HHNE++ models different relations in different spaces, which enables it to learn complex interactions in HINs. The optimization strategy of HHNE++ is also derived to update the parameters of HHNE++ in a principle manner. The experimental results demonstrate the effectiveness of our proposed models.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3200551854",
    "type": "article"
  },
  {
    "title": "Instance Annotation for Multi-Instance Multi-Label Learning",
    "doi": "https://doi.org/10.1145/2513092.2500491",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Forrest Briggs; Xiaoli Z. Fern; Raviv Raich; Qi Lou",
    "corresponding_authors": "",
    "abstract": "Multi-instance multi-label learning (MIML) is a framework for supervised classification where the objects to be classified are bags of instances associated with multiple labels. For example, an image can be represented as a bag of segments and associated with a list of objects it contains. Prior work on MIML has focused on predicting label sets for previously unseen bags. We instead consider the problem of predicting instance labels while learning from data labeled only at the bag level. We propose a regularized rank-loss objective designed for instance annotation, which can be instantiated with different aggregation models connecting instance-level labels with bag-level label sets. The aggregation models that we consider can be factored as a linear function of a \"support instance\" for each class, which is a single feature vector representing a whole bag. Hence we name our proposed methods rank-loss Support Instance Machines (SIM). We propose two optimization methods for the rank-loss objective, which is nonconvex. One is a heuristic method that alternates between updating support instances, and solving a convex problem in which the support instances are treated as constant. The other is to apply the constrained concave-convex procedure (CCCP), which can also be interpreted as iteratively updating support instances and solving a convex problem. To solve the convex problem, we employ the Pegasos framework of primal subgradient descent, and prove that it finds an ϵ-suboptimal solution in runtime that is linear in the number of bags, instances, and 1/ϵ. Additionally, we suggest a method of extending the linear learning algorithm to nonlinear classification, without increasing the runtime asymptotically. Experiments on artificial and real-world datasets including images and audio show that the proposed methods achieve higher accuracy than other loss functions used in prior work, e.g., Hamming loss, and recent work in ambiguous label classification.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4232995536",
    "type": "article"
  },
  {
    "title": "Bayesian Variable Selection in Linear Regression in One Pass for Large Datasets",
    "doi": "https://doi.org/10.1145/2629617",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "Carlos Ordońẽz; Carlos Garcia-Alvarado; Veerabhadaran Baladandayuthapani",
    "corresponding_authors": "",
    "abstract": "Bayesian models are generally computed with Markov Chain Monte Carlo (MCMC) methods. The main disadvantage of MCMC methods is the large number of iterations they need to sample the posterior distributions of model parameters, especially for large datasets. On the other hand, variable selection remains a challenging problem due to its combinatorial search space, where Bayesian models are a promising solution. In this work, we study how to accelerate Bayesian model computation for variable selection in linear regression. We propose a fast Gibbs sampler algorithm, a widely used MCMC method that incorporates several optimizations. We use a Zellner prior for the regression coefficients, an improper prior on variance, and a conjugate prior Gaussian distribution, which enable dataset summarization in one pass, thus exploiting an augmented set of sufficient statistics. Thereafter, the algorithm iterates in main memory. Sufficient statistics are indexed with a sparse binary vector to efficiently compute matrix projections based on selected variables. Discovered variable subsets probabilities, selecting and discarding each variable, are stored on a hash table for fast retrieval in future iterations. We study how to integrate our algorithm into a Database Management System (DBMS), exploiting aggregate User-Defined Functions for parallel data summarization and stored procedures to manipulate matrices with arrays. An experimental evaluation with real datasets evaluates accuracy and time performance, comparing our DBMS-based algorithm with the R package. Our algorithm is shown to produce accurate results, scale linearly on dataset size, and run orders of magnitude faster than the R package.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1999398874",
    "type": "article"
  },
  {
    "title": "Utility-aware Privacy Perturbation for Training Data",
    "doi": "https://doi.org/10.1145/3639411",
    "publication_date": "2024-01-02",
    "publication_year": 2024,
    "authors": "Xinjiao Li; Guowei Wu; Lin Yao; Zhaolong Zheng; Shisong Geng",
    "corresponding_authors": "",
    "abstract": "Data perturbation under differential privacy constraint is an important approach of protecting data privacy. However, as the data dimensions increase, the privacy budget allocated to each dimension decreases and thus the amount of noise added increases, which eventually leads to lower data utility in training tasks. To protect the privacy of training data while enhancing data utility, we propose a Utility-aware training data Privacy Perturbation scheme based on attribute Partition and budget Allocation (UPPPA). UPPPA includes three procedures: the quantification of attribute privacy and attribute importance, attribute partition, and budget allocation. The quantification of attribute privacy and attribute importance based on information entropy and attribute correlation provide an arithmetic basis for attribute partition and budget allocation. During the attribute partition, all attributes of training data are classified into high and low classes to achieve privacy amplification and utility enhancement. During the budget allocation, a γ-privacy model is proposed to balance data privacy and data utility so as to provide privacy constraint and guide budget allocation. Three comprehensive sets of real-world data are applied to evaluate the performance of UPPPA. Experiments and privacy analysis show that our scheme can achieve the tradeoff between privacy and utility.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390490813",
    "type": "article"
  },
  {
    "title": "Asymmetric Learning for Graph Neural Network based Link Prediction",
    "doi": "https://doi.org/10.1145/3640347",
    "publication_date": "2024-01-10",
    "publication_year": 2024,
    "authors": "Kai-Lang Yao; Wu-Jun Li",
    "corresponding_authors": "",
    "abstract": "Link prediction is a fundamental problem in many graph-based applications, such as protein-protein interaction prediction. Recently, graph neural network (GNN) has been widely used for link prediction. However, existing GNN-based link prediction (GNN-LP) methods suffer from scalability problem during training for large-scale graphs, which has received little attention from researchers. In this paper, we first analyze the computation complexity of existing GNN-LP methods, revealing that one reason for the scalability problem stems from their symmetric learning strategy in applying the same class of GNN models to learn representation for both head nodes and tail nodes. We then propose a novel method, called a sym m etric l earning (AML), for GNN-LP. More specifically, AML applies a GNN model to learn head node representation while applying a multi-layer perceptron (MLP) model to learn tail node representation. To the best of our knowledge, AML is the first GNN-LP method to adopt an asymmetric learning strategy for node representation learning. Furthermore, we design a novel model architecture and apply a row-wise mini-batch sampling strategy to ensure promising model accuracy and training efficiency for AML. Experiments on three real large-scale datasets show that AML is 1.7×∼7.3× faster in training than baselines with a symmetric learning strategy while having almost no accuracy loss.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390722132",
    "type": "article"
  },
  {
    "title": "Voxel-Wise Medical Image Generalization for Eliminating Distribution Shift",
    "doi": "https://doi.org/10.1145/3643034",
    "publication_date": "2024-01-25",
    "publication_year": 2024,
    "authors": "Feifei Li; Yuanbin Wang; Oya Beyan; Mirjam Schöneck; Liliana Caldeira",
    "corresponding_authors": "",
    "abstract": "Currently, the medical field is witnessing an increase in the use of machine learning techniques. Supervised learning methods adopted in classification, prediction, and segmentation tasks for medical images always experience decreased performance when the training and testing datasets do not follow the independent and identically distributed assumption. These distribution shift situations seriously influence machine learning applications’ robustness, fairness, and trustworthiness in the medical domain. Hence, in this article, we adopt the CycleGAN (generative adversarial network) method to cycle train the computed tomography data from different scanners/manufacturers. It aims to eliminate the distribution shift from diverse data terminals based on our previous work [ 14 ]. However, due to the model collapse problem and generative mechanisms of the GAN-based model, the images we generated contained serious artifacts. To remove the boundary marks and artifacts, we adopt score-based diffusion generative models to refine the images voxel-wisely. This innovative combination of two generative models enhances the quality of data providers while maintaining significant features. Meanwhile, we use five paired patients’ medical images to deal with the evaluation experiments with structural similarity index measure metrics and the segmentation model’s performance comparison. We conclude that CycleGAN can be utilized as an efficient data augmentation technique rather than a distribution-shift-eliminating method. In contrast, the denoising diffusion the denoising diffusion model is more suitable for dealing with the distribution shift problem aroused by the different terminal modules. The limitation of generative methods applied in medical images is the difficulty in obtaining large and diverse datasets that accurately capture the complexity of biological structure and variability. In our following research, we plan to assess the initial and generated datasets to explore more possibilities to overcome the above limitation. We will also incorporate the generative methods into the federated learning architecture, which can maintain their advantages and resolve the distribution shift issue on a larger scale.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391221822",
    "type": "article"
  },
  {
    "title": "Attacking Click-through Rate Predictors via Generating Realistic Fake Samples",
    "doi": "https://doi.org/10.1145/3643685",
    "publication_date": "2024-01-27",
    "publication_year": 2024,
    "authors": "Mingxing Duan; Kenli Li; Weinan Zhang; Jiarui Qin; Bin Xiao",
    "corresponding_authors": "",
    "abstract": "How to construct imperceptible (realistic) fake samples is critical in adversarial attacks. Due to the sample feature diversity of a recommender system (containing both discrete and continuous features), traditional gradient-based adversarial attack methods may fail to construct realistic fake samples. Meanwhile, most recommendation models adopt click-through rate (CTR) predictors, which usually utilize black-box deep models with discrete features as input. Thus, how to efficiently construct realistic fake samples for black-box recommender systems is still full of challenges. In this article, we propose a hierarchical adversarial attack method against black-box CTR models via generating realistic fake samples, named CTRAttack. To better train the generation network, the weights of its embedding layer are shared with those of the substitute model, with both the similarity loss and classification loss used to update the generation network. To ensure that the discrete features of the generated fake samples are all real, we first adopt the similarity loss to ensure that the distribution of the generated perturbed samples is sufficiently close to the distribution of the real features, and then the nearest neighbor algorithm is used to retrieve the most appropriate features for non-existent discrete features from the candidate instance set. Extensive experiments demonstrate that CTRAttack can not only effectively attack the black-box recommender systems but also improve the robustness of these models while maintaining prediction accuracy.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391278016",
    "type": "article"
  },
  {
    "title": "Domain Generalization in Time Series Forecasting",
    "doi": "https://doi.org/10.1145/3643035",
    "publication_date": "2024-01-31",
    "publication_year": 2024,
    "authors": "Songgaojun Deng; Olivier Sprangers; Ming Li; Sebastian Schelter; Maarten de Rijke",
    "corresponding_authors": "",
    "abstract": "Domain generalization aims to design models that can effectively generalize to unseen target domains by learning from observed source domains. Domain generalization poses a significant challenge for time series data, due to varying data distributions and temporal dependencies. Existing approaches to domain generalization are not designed for time series data, which often results in suboptimal or unstable performance when confronted with diverse temporal patterns and complex data characteristics. We propose a novel approach to tackle the problem of domain generalization in time series forecasting. We focus on a scenario where time series domains share certain common attributes and exhibit no abrupt distribution shifts. Our method revolves around the incorporation of a key regularization term into an existing time series forecasting model: domain discrepancy regularization . In this way, we aim to enforce consistent performance across different domains that exhibit distinct patterns. We calibrate the regularization term by investigating the performance within individual domains and propose the domain discrepancy regularization with domain difficulty awareness . We demonstrate the effectiveness of our method on multiple datasets, including synthetic and real-world time series datasets from diverse domains such as retail, transportation, and finance. Our method is compared against traditional methods, deep learning models, and domain generalization approaches to provide comprehensive insights into its performance. In these experiments, our method showcases superior performance, surpassing both the base model and competing domain generalization models across all datasets. Furthermore, our method is highly general and can be applied to various time series models.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391388021",
    "type": "article"
  },
  {
    "title": "On the Value of Head Labels in Multi-Label Text Classification",
    "doi": "https://doi.org/10.1145/3643853",
    "publication_date": "2024-02-05",
    "publication_year": 2024,
    "authors": "Haobo Wang; Peng Cheng; Hede Dong; Lei Feng; Weiwei Liu; Tianlei Hu; Ke Chen; Gang Chen",
    "corresponding_authors": "",
    "abstract": "A formidable challenge in the multi-label text classification (MLTC) context is that the labels often exhibit a long-tailed distribution, which typically prevents deep MLTC models from obtaining satisfactory performance. To alleviate this problem, most existing solutions attempt to improve tail performance by means of sampling or introducing extra knowledge. Data-rich labels, though more trustworthy, have not received the attention they deserve. In this work, we propose a multiple-stage training framework to exploit both model- and feature-level knowledge from the head labels, to improve both the representation and generalization ability of MLTC models. Moreover, we theoretically prove the superiority of our framework design over other alternatives. Comprehensive experiments on widely used MLTC datasets clearly demonstrate that the proposed framework achieves highly superior results to state-of-the-art methods, highlighting the value of head labels in MLTC.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391540437",
    "type": "article"
  },
  {
    "title": "Generation-based Multi-view Contrast for Self-supervised Graph Representation Learning",
    "doi": "https://doi.org/10.1145/3645095",
    "publication_date": "2024-02-09",
    "publication_year": 2024,
    "authors": "Yuehui Han",
    "corresponding_authors": "Yuehui Han",
    "abstract": "Graph contrastive learning has made remarkable achievements in the self-supervised representation learning of graph-structured data. By employing perturbation function (i.e., perturbation on the nodes or edges of graph), most graph contrastive learning methods construct contrastive samples on the original graph. However, the perturbation-based data augmentation methods randomly change the inherent information (e.g., attributes or structures) of the graph. Therefore, after nodes embedding on the perturbed graph, we cannot guarantee the validity of the contrastive samples as well as the learned performance of graph contrastive learning. To this end, in this article, we propose a novel generation-based multi-view contrastive learning framework (GMVC) for self-supervised graph representation learning, which generates the contrastive samples based on our generator rather than perturbation function. Specifically, after nodes embedding on the original graph we first employ random walk in the neighborhood to develop multiple relevant node sequences for each anchor node. We then utilize the transformer to generate the representations of relevant contrastive samples of anchor node based on the features and structures of the sampled node sequences. Finally, by maximizing the consistency between the anchor view and the generated views, we force the model to effectively encode graph information into nodes embeddings. We perform extensive experiments of node classification and link prediction tasks on eight benchmark datasets, which verify the effectiveness of our generation-based multi-view graph contrastive learning method.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391682422",
    "type": "article"
  },
  {
    "title": "Dual-Side Adversarial Learning Based Fair Recommendation for Sensitive Attribute Filtering",
    "doi": "https://doi.org/10.1145/3648683",
    "publication_date": "2024-02-19",
    "publication_year": 2024,
    "authors": "Shenghao Liu; Y Zhang; Lingzhi Yi; Xianjun Deng; Laurence T. Yang; Bang Wang",
    "corresponding_authors": "",
    "abstract": "With the development of recommendation algorithms, researchers are paying increasing attention to fairness issues such as user discrimination in recommendations. To address these issues, existing works often filter users’ sensitive information that may cause discrimination during the process of learning user representations. However, these approaches overlook the latent relationship between items’ content attributes and users’ sensitive information. In this article, we propose DALFRec, a fairness-aware recommendation algorithm based on user-side and item-side adversarial learning to mitigate the effects of sensitive information on both sides of the recommendation process. First, we conduct a statistical analysis to demonstrate the latent relationship between items’ information and users’ sensitive attributes. Then, we design a dual-side adversarial learning network that simultaneously filters out users’ sensitive information on the user and item side. Additionally, we propose a new evaluation strategy that leverages the latent relationship between items’ content attributes and users’ sensitive attributes to better assess the algorithm’s ability to reduce discrimination. Our experiments on three real datasets demonstrate the superiority of our proposed algorithm over state-of-the-art methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391947651",
    "type": "article"
  },
  {
    "title": "BapFL: You can Backdoor Personalized Federated Learning",
    "doi": "https://doi.org/10.1145/3649316",
    "publication_date": "2024-02-23",
    "publication_year": 2024,
    "authors": "Tiandi Ye; Cen Chen; Yinggui Wang; Xiang Li; Ming Gao",
    "corresponding_authors": "",
    "abstract": "In federated learning (FL), malicious clients could manipulate the predictions of the trained model through backdoor attacks, posing a significant threat to the security of FL systems. Existing research primarily focuses on backdoor attacks and defenses within the generic federated learning scenario, where all clients collaborate to train a single global model. A recent study conducted by Qin et al. [ 24 ] marks the initial exploration of backdoor attacks within the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with parameter decoupling can significantly enhance robustness against backdoor attacks. However, in this article, we whistleblow that pFL methods with parameter decoupling are still vulnerable to backdoor attacks. The resistance of pFL methods with parameter decoupling is attributed to the heterogeneous classifiers between malicious clients and benign counterparts. We analyze two direct causes of the heterogeneous classifiers: (1) data heterogeneity inherently exists among clients and (2) poisoning by malicious clients further exacerbates the data heterogeneity. To address these issues, we propose a two-pronged attack method, BapFL, which comprises two simple yet effective strategies: (1) poisoning only the feature encoder while keeping the classifier fixed and (2) diversifying the classifier through noise introduction to simulate that of the benign clients. Extensive experiments on three benchmark datasets under varying conditions demonstrate the effectiveness of our proposed attack. Additionally, we evaluate the effectiveness of six widely used defense methods and find that BapFL still poses a significant threat even in the presence of the best defense, Multi-Krum. We hope to inspire further research on attack and defense strategies in pFL scenarios. The code is available at: https://github.com/BapFL/code",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392121135",
    "type": "article"
  },
  {
    "title": "Citation Forecasting with Multi-Context Attention-Aided Dependency Modeling",
    "doi": "https://doi.org/10.1145/3649140",
    "publication_date": "2024-02-23",
    "publication_year": 2024,
    "authors": "Taoran Ji; Nathan Self; Kaiqun Fu; Zhiqian Chen; Naren Ramakrishnan; Chang‐Tien Lu",
    "corresponding_authors": "",
    "abstract": "Forecasting citations of scientific patents and publications is a crucial task for understanding the evolution and development of technological domains and for foresight into emerging technologies. By construing citations as a time series, the task can be cast into the domain of temporal point processes. Most existing work on forecasting with temporal point processes, both conventional and neural network-based, only performs single-step forecasting. In citation forecasting, however, the more salient goal is n -step forecasting: predicting the arrival of the next n citations. In this article, we propose Dynamic Multi-Context Attention Networks (DMA-Nets), a novel deep learning sequence-to-sequence (Seq2Seq) model with a novel hierarchical dynamic attention mechanism for long-term citation forecasting. Extensive experiments on two real-world datasets demonstrate that the proposed model learns better representations of conditional dependencies over historical sequences compared to state-of-the-art counterparts and thus achieves significant performance for citation predictions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392121233",
    "type": "article"
  },
  {
    "title": "A Fully Test-time Training Framework for Semi-supervised Node Classification on Out-of-Distribution Graphs",
    "doi": "https://doi.org/10.1145/3649507",
    "publication_date": "2024-02-26",
    "publication_year": 2024,
    "authors": "Jiaxin Zhang; Yiqi Wang; Xihong Yang; En Zhu",
    "corresponding_authors": "",
    "abstract": "Graph neural networks (GNNs) have shown great potential in representation learning for various graph tasks. However, the distribution shift between the training and test sets poses a challenge to the efficiency of GNNs. To address this challenge, HomoTTT proposes a fully test-time training framework for GNNs to enhance the model’s generalization capabilities for node classification tasks. Specifically, our proposed HomoTTT designs a homophily-based and parameter-free graph contrastive learning task with adaptive augmentation to guide the model’s adaptation during the test-time training, allowing the model to adapt for specific target data. In the inference stage, HomoTTT proposes to integrate the original GNN model and the adapted model after TTT using a homophily-based model selection method, which prevents potential performance degradation caused by unconstrained model adaptation. Extensive experimental results on six benchmark datasets demonstrate the effectiveness of our proposed framework. Additionally, the exploratory study further validates the rationality of the homophily-based graph contrastive learning task with adaptive augmentation and the homophily-based model selection designed in HomoTTT .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392163152",
    "type": "article"
  },
  {
    "title": "Building Shortcuts between Distant Nodes with Biaffine Mapping for Graph Convolutional Networks",
    "doi": "https://doi.org/10.1145/3650113",
    "publication_date": "2024-03-01",
    "publication_year": 2024,
    "authors": "Acong Zhang; Jincheng Huang; Ping Li; Kai Zhang",
    "corresponding_authors": "",
    "abstract": "Multiple recent studies show a paradox in graph convolutional networks (GCNs)—that is, shallow architectures limit the capability of learning information from high-order neighbors, whereas deep architectures suffer from over-smoothing or over-squashing. To enjoy the simplicity of shallow architectures and overcome their limits of neighborhood extension, in this work we introduce a biaffine technique to improve the expressiveness of GCNs with a shallow architecture. The core design of our method is to learn direct dependency on long-distance neighbors for nodes, with which only 1-hop message passing is capable of capturing rich information for node representation. Besides, we propose a multi-view contrastive learning method to exploit the representations learned from long-distance dependencies. Extensive experiments on nine graph benchmark datasets suggest that the shallow biaffine graph convolutional networks (BAGCN) significantly outperform state-of-the-art GCNs (with deep or shallow architectures) on semi-supervised node classification. We further verify the effectiveness of biaffine design in node representation learning and the performance consistency on different sizes of training data.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392386405",
    "type": "article"
  },
  {
    "title": "Attacking Social Media via Behavior Poisoning",
    "doi": "https://doi.org/10.1145/3654673",
    "publication_date": "2024-03-27",
    "publication_year": 2024,
    "authors": "Chenwang Wu; Defu Lian; Yong Ge; Min Zhou; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "Since social media such as Facebook and X (formerly known as Twitter) have permeated various aspects of daily life, people have strong incentives to influence information dissemination on these platforms and differentiate their content from the fierce competition. Existing dissemination strategies typically employ marketing techniques, such as seeking publicity through renowned actors or targeted advertising placements. Despite their various forms, most simply spread information to strengthen user impressions without conducting formal analyses of specific influence enhancement. And coupled with high costs, most fall short of expectations. To this end, we ingeniously formulate the task of social media dissemination as poisoning attacks, which influence specified content’s dissemination among target users by intervening in some users’ social media behaviors (including retweeting, following, and profile modifying). Correspondingly, we propose a novel poisoning attack, Influence-based Social Media Attack (ISMA) to generate discrete poisoning behaviors, which is difficult to achieve with existing attacks. In ISMA, we first contribute an efficient influence evaluator to quantify the spread influence of poisoning behaviors. Based on the estimated influence, we then present an imperceptible hierarchical selector and a profile modification method ProMix to select influential behaviors to poison. Notably, our attack is driven by custom attack objectives, which allows one to flexibly design different optimization goals to change the information flow, which could solve the blindness of existing influence maximization methods. Besides, behaviors such as retweeting are gentle and simple to implement. These properties make our attack more cost-effective and practical. Extensive experiments on two large-scale real-world datasets demonstrate the superiority of our method as it significantly outperforms baselines, and additionally, the proposed evaluator’s analysis of user influence provides new insights for influence maximization on social media.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393224513",
    "type": "article"
  },
  {
    "title": "Properties of Fairness Measures in the Context of Varying Class Imbalance and Protected Group Ratios",
    "doi": "https://doi.org/10.1145/3654659",
    "publication_date": "2024-03-28",
    "publication_year": 2024,
    "authors": "Dariusz Brzeziński; Julia Stachowiak; Jerzy Stefanowski; Izabela Szczęch; Robert Susmaga; Sofya Aksenyuk; Uladzimir Ivashka; Oleksandr Yasinskyi",
    "corresponding_authors": "",
    "abstract": "Society is increasingly relying on predictive models in fields like criminal justice, credit risk management, or hiring. To prevent such automated systems from discriminating against people belonging to certain groups, fairness measures have become a crucial component in socially relevant applications of machine learning. However, existing fairness measures have been designed to assess the bias between predictions for protected groups without considering the imbalance in the classes of the target variable. Current research on the potential effect of class imbalance on fairness focuses on practical applications rather than dataset-independent measure properties. In this paper, we study the general properties of fairness measures for changing class and protected group proportions. For this purpose, we analyze the probability mass functions of six of the most popular group fairness measures. We also measure how the probability of achieving perfect fairness changes for varying class imbalance ratios. Moreover, we relate the dataset-independent properties of fairness measures described in this paper to classifier fairness in real-life tasks. Our results show that measures such as Equal Opportunity and Positive Predictive Parity are more sensitive to changes in class imbalance than Accuracy Equality. These findings can help guide researchers and practitioners in choosing the most appropriate fairness measures for their classification problems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393260124",
    "type": "article"
  },
  {
    "title": "Towards Robust Rumor Detection with Graph Contrastive and Curriculum Learning",
    "doi": "https://doi.org/10.1145/3653023",
    "publication_date": "2024-03-30",
    "publication_year": 2024,
    "authors": "Wen-Ming Zhuang; Chih-Yao Chen; Cheng–Te Li",
    "corresponding_authors": "",
    "abstract": "Establishing a robust rumor detection model is vital in safeguarding the veracity of information on social media platforms. However, existing approaches to stopping rumor from spreading rely on abundant and clean training data, which is rarely available in real-world scenarios. In this work, we aim to develop a trustworthy rumor detection model that can handle inadequate and noisy labeled data. Our work addresses robust rumor detection, including classic and early detection, as well as five types of robustness issues: noisy and incomplete propagation, label scarcity and noise, and user disappearance. We propose a novel method, Robustness-Enhanced Rumor Detection (RERD), which mainly leverages the information propagation graphs of source tweets, along with user profiles and retweeting knowledge, for model learning. The novelty of RERD is four-fold. First, we jointly exploit the propagation structures of non-text and text retweets to learn the representation of a source tweet. Second, we simultaneously utilize the top-down and bottom-up information flows with relational propagations for graph representation learning. Third, to have effective early and robust detection, we implement contrastive learning on graphs with early and complete views of information propagation so that small snapshots can foresee their future shapes. Last, we use curriculum pseudo-labeling to mitigate the impact of label scarcity and noisy labels, and to correct representations learned from corrupted data. Experimental results on three benchmark datasets demonstrate that RERD consistently outperforms competitors in classic, early, and robust rumor detection scenarios. To the best of our knowledge, we are the first to simultaneously cope with early and five robust detections of rumors.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393342233",
    "type": "article"
  },
  {
    "title": "FETILDA: Evaluation Framework for Effective Representations of Long Financial Documents",
    "doi": "https://doi.org/10.1145/3657299",
    "publication_date": "2024-04-10",
    "publication_year": 2024,
    "authors": "Bolun Xia; Vipula Rawte; Aparna Gupta; Mohammed J. Zaki",
    "corresponding_authors": "",
    "abstract": "In the financial sphere, there is a wealth of accumulated unstructured financial data, such as the textual disclosure documents that companies submit on a regular basis to regulatory agencies, such as the Securities and Exchange Commission. These documents are typically very long and tend to contain valuable soft information about a company’s performance that is not present in quantitative predictors. It is therefore of great interest to learn predictive models from these long textual documents, especially for forecasting numerical key performance indicators. In recent years, there has been great progress in natural language processing via pre-trained language models (LMs) learned from large corpora of textual data. This prompts the important question of whether they can be used effectively to produce representations for long documents, as well as how we can evaluate the effectiveness of representations produced by various LMs. Our work focuses on answering this critical question, namely, the evaluation of the efficacy of various LMs in extracting useful soft information from long textual documents for prediction tasks. In this article, we propose and implement a deep learning evaluation framework that utilizes a sequential chunking approach combined with an attention mechanism. We perform an extensive set of experiments on a collection of 10-K reports submitted annually by U.S. banks, and another dataset of reports submitted by U.S. companies, to investigate thoroughly the performance of different types of language models. Overall, our framework using LMs outperforms strong baseline methods for textual modeling as well as for numerical regression. Our work provides better insights into how utilizing pre-trained domain-specific and fine-tuned long-input LMs for representing long documents can improve the quality of representation of textual data and, therefore, help in improving predictive analyses.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4394683350",
    "type": "article"
  },
  {
    "title": "Congestion-aware Spatio-Temporal Graph Convolutional Network-based A* Search Algorithm for Fastest Route Search",
    "doi": "https://doi.org/10.1145/3657640",
    "publication_date": "2024-04-11",
    "publication_year": 2024,
    "authors": "Hongjie Sui; Huan Yan; Tianyi Zheng; Wenzhen Huang; Yunlin Zhuang; Yong Li",
    "corresponding_authors": "",
    "abstract": "The fastest route search, which is to find a path with the shortest travel time when the user initiates a query, has become one of the most important services in many map applications. To enhance the user experience of travel, it is necessary to achieve accurate and real-time route search. However, traffic conditions are changing dynamically, and the frequent occurrence of traffic congestion may greatly increase travel time. Thus, it is challenging to achieve the above goal. To deal with it, we present a congestion-aware spatio-temporal graph convolutional network-based A* search algorithm for the task of fastest route search. We first identify a sequence of consecutive congested traffic conditions as a traffic congestion event. Then, we propose a spatio-temporal graph convolutional network that jointly models the congestion events and changing travel time to capture their complex spatio-temporal correlations, which can predict the future travel-time information of each road segment as the basis of route planning. Further, we design a path-aided neural network to achieve effective origin-destination (OD) shortest travel-time estimation by encoding the complex relationships between OD pairs and their corresponding fastest paths. Finally, the cost function in the A* algorithm is set by fusing the output results of the two components, which is used to guide the route search. Our experimental results on the two real-world datasets show the superior performance of the proposed method.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4394717766",
    "type": "article"
  },
  {
    "title": "Quantum Nearest Neighbor Collaborative Filtering Algorithm for Recommendation System",
    "doi": "https://doi.org/10.1145/3674982",
    "publication_date": "2024-06-29",
    "publication_year": 2024,
    "authors": "Jiaye Li; Jinjing Shi; Jian Zhang; Yuhu Lu; Qin Li; Chunlin Yu; Shichao Zhang",
    "corresponding_authors": "",
    "abstract": "Recommendation has become especially crucial during the COVID-19 pandemic as a significant number of people rely on online shopping from home. Existing recommendation algorithms, designed to address issues like cold start and data sparsity, often overlook the time constraints of users. Specifically, users expect to receive recommendations for products of interest in the shortest possible time. To address this challenge, we propose a novel collaborative filtering recommendation algorithm that leverages the advantages of quantum computing circuits based on data reconstruction. This approach allows for the rapid identification of users similar to the target user, thereby improving recommendation speed. In our method, we utilize the information of known users to linearly reconstruct that of the target users, forming a relational matrix. Subsequently, we employ \\(l_{2,1}-\\) norm and \\(l_{1}-\\) norm to sparsely constrain the relationship matrix, deducing the weight of each known user. The final step involves providing similar recommendations to target users based on these weights. Furthermore, we implement the proposed algorithm using a quantum circuit, enabling exponential acceleration. The final weight matrix is derived from the quantum state outputted by the circuit. The speed of this process is theoretically demonstrated in detail. Experimental results indicate that our algorithm outperforms state-of-the-art methods in terms of root mean squared error (RMSE), mean absolute error (MAE) and normalized discounted cumulative gain (NDCG). Compared to state-of-the-art comparison algorithms, the proposed algorithm achieves the fastest recommendation speed across eight public datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400151247",
    "type": "article"
  },
  {
    "title": "Neighbor-Enhanced Representation Learning for Link Prediction in Dynamic Heterogeneous Attributed Networks",
    "doi": "https://doi.org/10.1145/3676559",
    "publication_date": "2024-07-04",
    "publication_year": 2024,
    "authors": "Xiangyu Wei; Wei Wang; Chongsheng Zhang; Weiping Ding; Bin Wang; Yaguan Qian; Zhen Han; Chunhua Su",
    "corresponding_authors": "",
    "abstract": "Dynamic link prediction aims to predict future connections among unconnected nodes in a network. It can be applied for friend recommendations, link completion, and other tasks. Network representation learning algorithms have demonstrated considerable effectiveness in various prediction tasks. However, most network representation learning algorithms are based on homogeneous networks and static networks for link prediction that do not consider rich semantic and dynamic information. Additionally, existing dynamic network representation learning methods neglect the neighborhood interaction structure of the node. In this work, we design a neighbor-enhanced dynamic heterogeneous attributed network embedding method (NeiDyHNE) for link prediction. In light of the impressive achievements of the heuristic methods, we learn the information of common neighbors and neighbors’ interaction in heterogeneous networks to preserve the neighbors proximity and common neighbors proximity. NeiDyHNE encodes the attributes and neighborhood structure of nodes as well as the evolutionary features of the dynamic network. More specifically, NeiDyHNE consists of the hierarchical structure attention module and the convolutional temporal attention module. The hierarchical structure attention module captures the rich features and semantic structure of nodes. The convolutional temporal attention module captures the evolutionary features of the network over time in dynamic heterogeneous networks. We evaluate our method and various baseline methods on the dynamic link prediction task. Experimental results demonstrate that our method is superior to baseline methods in terms of accuracy.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400317755",
    "type": "article"
  },
  {
    "title": "Modeling Sequences as Star Graphs to Address Over-Smoothing in Self-Attentive Sequential Recommendation",
    "doi": "https://doi.org/10.1145/3676560",
    "publication_date": "2024-07-09",
    "publication_year": 2024,
    "authors": "Bo Peng; Ziqi Chen; Srinivasan Parthasarathy; Xia Ning",
    "corresponding_authors": "",
    "abstract": "Self-attention (SA) mechanisms have been widely used in developing sequential recommendation (SR) methods, and demonstrated state-of-the-art performance. However, in this article, we show that self-attentive SR methods substantially suffer from the over-smoothing issue that item embeddings within a sequence become increasingly similar across attention blocks. As widely demonstrated in the literature, this issue could lead to a loss of information in individual items, and significantly degrade models’ scalability and performance. To address the over-smoothing issue, in this article, we view items within a sequence constituting a star graph and develop a method, denoted as \\(\\mathop{\\mathtt{MSSG}}\\limits\\) , for SR. Different from existing self-attentive methods, \\(\\mathop{\\mathtt{MSSG}}\\limits\\) introduces an additional internal node to specifically capture the global information within the sequence, and does not require information propagation among items. This design fundamentally addresses the over-smoothing issue and enables \\(\\mathop{\\mathtt{MSSG}}\\limits\\) a linear time complexity with respect to the sequence length. We compare \\(\\mathop{\\mathtt{MSSG}}\\limits\\) with eleven state-of-the-art baseline methods on six public benchmark datasets. Our experimental results demonstrate that \\(\\mathop{\\mathtt{MSSG}}\\limits\\) significantly outperforms the baseline methods, with an improvement of as much as 10.10%. Our analysis shows the superior scalability of \\(\\mathop{\\mathtt{MSSG}}\\limits\\) over the state-of-the-art self-attentive methods. Our complexity analysis and runtime performance comparison together show that \\(\\mathop{\\mathtt{MSSG}}\\limits\\) is both theoretically and practically more efficient than self-attentive methods. Our analysis of the attention weights learned in SA-based methods indicates that on sparse recommendation data, modeling dependencies in all item pairs using the SA mechanism yields limited information gain, and thus, might not benefit the recommendation performance. Our source code and data are publicly accessible through GitHub .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400457919",
    "type": "article"
  },
  {
    "title": "Meta-GPS \\(++\\) : Enhancing Graph Meta-Learning with Contrastive Learning and Self-Training",
    "doi": "https://doi.org/10.1145/3679018",
    "publication_date": "2024-07-19",
    "publication_year": 2024,
    "authors": "Yonghao Liu; Mengyu Li; Ximing Li; Lan Huang; Fausto Giunchiglia; Yanchun Liang; Xiaoyue Feng; Renchu Guan",
    "corresponding_authors": "",
    "abstract": "Node classification is an essential problem in graph learning. However, many models typically obtain unsatisfactory performance when applied to few-shot scenarios. Some studies have attempted to combine meta-learning with graph neural networks to solve few-shot node classification on graphs. Despite their promising performance, some limitations remain. First, they employ the node encoding mechanism of homophilic graphs to learn node embeddings, even in heterophilic graphs. Second, existing models based on meta-learning ignore the interference of randomness in the learning process. Third, they are trained using only limited labeled nodes within the specific task, without explicitly utilizing numerous unlabeled nodes. Finally, they treat almost all sampled tasks equally without customizing them for their uniqueness. To address these issues, we propose a novel framework for few-shot node classification called Meta-GPS++. Specifically, we first adopt an efficient method to learn discriminative node representations on homophilic and heterophilic graphs. Then, we leverage a prototype-based approach to initialize parameters and contrastive learning for regularizing the distribution of node embeddings. Moreover, we apply self-training to extract valuable information from unlabeled nodes. Additionally, we adopt S$^2$ (scaling & shifting) transformation to learn transferable knowledge from diverse tasks. The results on real-world datasets show the superiority of Meta-GPS++. Our code is available here.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400814416",
    "type": "article"
  },
  {
    "title": "A Survey of Co-Clustering",
    "doi": "https://doi.org/10.1145/3681793",
    "publication_date": "2024-07-25",
    "publication_year": 2024,
    "authors": "Hongjun Wang; Yi Song; Wei Chen; Zhipeng Luo; Chongshou Li; Tianrui Li",
    "corresponding_authors": "",
    "abstract": "Co-clustering is to cluster samples and features simultaneously, which can also reveal the relationship between row clusters and column clusters. Therefore, lots of scientists have drawn much attention to conduct extensive research on it, and co-clustering is widely used in recommendation systems, gene analysis, medical data analysis, natural language processing, image analysis, and social network analysis. In this article, we survey the entire research aspect of co-clustering, especially the latest advances in co-clustering, and discover the current research challenges and future directions. First, due to different views from researchers on the definition of co-clustering, this article summarizes the definition of co-clustering and its extended definitions, as well as related issues, based on the perspectives of various scientists. Second, existing co-clustering techniques are approximately categorized into four classes: information-theory-based, graph-theory-based, matrix-factorization-based, and other theories-based. Third, co-clustering is applied in various aspects such as recommendation systems, medical data analysis, natural language processing, image analysis, and social network analysis. Furthermore, 10 popular co-clustering algorithms are empirically studied on 10 benchmark datasets with 4 metrics—accuracy, purity, block discriminant index, and running time, and their results are objectively reported. Finally, future work is provided to get insights into the research challenges of co-clustering.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400982249",
    "type": "article"
  },
  {
    "title": "Triangle Centrality",
    "doi": "https://doi.org/10.1145/3685677",
    "publication_date": "2024-07-31",
    "publication_year": 2024,
    "authors": "Paul Burkhardt",
    "corresponding_authors": "Paul Burkhardt",
    "abstract": "Triangle centrality is introduced for finding important vertices in a graph based on the concentration of triangles surrounding each vertex. It has the distinct feature of allowing a vertex to be central if it is in many triangles or none at all. Given a simple, undirected graph \\(G=(V,E)\\) with \\(n=|V|\\) vertices and \\(m=|E|\\) edges, let \\(\\triangle(v)\\) and \\(\\triangle(G)\\) denote the respective triangle counts of \\(v\\) and \\(G\\) . Let \\(N(v)\\) be the neighborhood set of \\(v\\) . Respectively, \\(N_{\\triangle}(v)\\) and \\(N_{\\triangle}[v]=\\{v\\}\\cup N_{\\triangle}(v)\\) denote the set of neighbors that are in triangles with \\(v\\) and the closed set including \\(v\\) . Then the triangle centrality for a vertex \\(v\\) is \\(\\begin{align*}TC(v)=\\frac{\\frac{1}{3}\\sum_{u\\in N_{\\triangle}[v]}\\triangle(u)+\\sum_{w\\in\\{N(v)\\setminus N_{\\triangle}(v)\\}}\\triangle(w)}{\\triangle(G)}.\\end{align*}\\) We show experimentally that triangle centrality is broadly applicable to many different types of networks. Our empirical results demonstrate that 30% of the time triangle centrality identified central vertices that differed with those found by five well-known centrality measures, which suggests novelty without being overly specialized. It is also asymptotically faster to compute on sparse graphs than all but the most trivial of these other measures. We introduce optimal algorithms that compute triangle centrality in \\(O(m\\overline{\\delta})\\) time and \\(O(m+n)\\) space, where \\(\\overline{\\delta}\\leq O(\\sqrt{m})\\) is the average degeneracy introduced by Burkhardt, Faber, and Harris (2020). In practical applications, \\(\\overline{\\delta}\\) is much smaller than \\(\\sqrt{m}\\) so triangle centrality can be computed in nearly linear time. On a Concurrent Read Exclusive Write (CREW) Parallel Random Access Machine (PRAM), we give a near work-optimal parallel algorithm that takes \\(O(\\log n)\\) time using \\(O(m\\sqrt{m})\\) CREW PRAM processors. In MapReduce, we show it takes four rounds using \\(O(m\\sqrt{m})\\) communication bits and is therefore optimal. We also derive a linear algebraic formulation of triangle centrality which can be computed in \\(O(m\\overline{\\delta})\\) time on sparse graphs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401170210",
    "type": "article"
  },
  {
    "title": "Efficient GNN Explanation via Learning Removal-based Attribution",
    "doi": "https://doi.org/10.1145/3685678",
    "publication_date": "2024-08-07",
    "publication_year": 2024,
    "authors": "Yao Rong; Guanchu Wang; Qizhang Feng; Ninghao Liu; Zirui Liu; Enkelejda Kasneci; Xia Hu",
    "corresponding_authors": "",
    "abstract": "As Graph Neural Networks (GNNs) have been widely used in real-world applications, model explanations are required not only by users but also by legal regulations. However, simultaneously achieving high fidelity and low computational costs in generating explanations has been a challenge for current methods. In this work, we propose a framework of GNN explanation named L e A rn R emoval-based A ttribution (LARA) to address this problem. Specifically, we introduce removal-based attribution and demonstrate its substantiated link to interpretability fidelity theoretically and experimentally. The explainer in LARA learns to generate removal-based attribution which enables providing explanations with high fidelity. A strategy of subgraph sampling is designed in LARA to improve the scalability of the training process. In the deployment, LARA can efficiently generate the explanation through a feed-forward pass. We benchmark our approach with other state-of-the-art GNN explanation methods on six datasets. Results highlight the effectiveness of our framework regarding both efficiency and fidelity. In particular, LARA is 3.1 \\(\\times\\) faster and achieves higher fidelity than the state-of-the-art method on the large dataset ogbn-arxiv (more than 160K nodes and 1M edges), showing its great potential in real-world applications. Our source code is available at https://github.com/yaorong0921/LARA .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401390801",
    "type": "article"
  },
  {
    "title": "VITR: Augmenting Vision Transformers with Relation-Focused Learning for Cross-modal Information Retrieval",
    "doi": "https://doi.org/10.1145/3686805",
    "publication_date": "2024-08-09",
    "publication_year": 2024,
    "authors": "Yan Gong; Georgina Cosma; Axel Finke",
    "corresponding_authors": "",
    "abstract": "The relations expressed in user queries are vital for cross-modal information retrieval. Relation-focused cross-modal retrieval aims to retrieve information that corresponds to these relations, enabling effective retrieval across different modalities. Pre-trained networks, such as Contrastive Language-Image Pre-training networks, have gained significant attention and acclaim for their exceptional performance in various cross-modal learning tasks. However, the Vision Transformer (ViT) used in these networks is limited in its ability to focus on image region relations. Specifically, ViT is trained to match images with relevant descriptions at the global level, without considering the alignment between image regions and descriptions. This article introduces VITR, a novel network that enhances ViT by extracting and reasoning about image region relations based on a local encoder. VITR is comprised of two key components. Firstly, it extends the capabilities of ViT-based cross-modal networks by enabling them to extract and reason with region relations present in images. Secondly, VITR incorporates a fusion module that combines the reasoned results with global knowledge to predict similarity scores between images and descriptions. The proposed VITR network was evaluated through experiments on the tasks of relation-focused cross-modal information retrieval. The results derived from the analysis of the Flickr30K, MS-COCO, RefCOCOg, and CLEVR datasets demonstrated that the proposed VITR network consistently outperforms state-of-the-art networks in image-to-text and text-to-image retrieval.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401455192",
    "type": "article"
  },
  {
    "title": "ProcessGAN: Generating Privacy-Preserving Time-Aware Process Data with Conditional Generative Adversarial Nets",
    "doi": "https://doi.org/10.1145/3687464",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Keyi Li; Sen Yang; Travis M. Sullivan; Randall S. Burd; Ivan Marsic",
    "corresponding_authors": "",
    "abstract": "Process data constructed from event logs provides valuable insights into procedural dynamics over time. The confidential information in process data, together with the data's intricate nature, makes the datasets not sharable and challenging to collect. Consequently, research is limited using process data and analytics in the process mining domain. In this study, we introduced a synthetic process data generation task to address the limitation of sharable process data. We introduced a generative adversarial network, called ProcessGAN, to generate process data with activity sequences and corresponding timestamps. ProcessGAN consists of a transformer-based network as the generator, and a time-aware self-attention network as the discriminator. It can generate privacy-preserving process data from random noise. ProcessGAN considers the duration of the process and time intervals between activities to generate realistic activity sequences with timestamps. We evaluated ProcessGAN on five real-world datasets, two that are public and three collected in medical domains that are private. To evaluate the synthetic data, in addition to statistical metrics, we trained a supervised model to score the synthetic processes. We also used process mining to discover workflows for synthetic medical processes and had domain experts evaluate the clinical applicability of the synthetic workflows. ProcessGAN outperformed the existing generative models in generating complex processes with valid parallel pathways. The synthetic process data generated by ProcessGAN better represented the long-range dependencies between activities, a feature relevant to complicated medical and other processes. The timestamps generated by the ProcessGAN model showed similar distributions with the authentic timestamps. In addition, we trained a transformer-based network to generate synthetic contexts (e.g., patient demographics) that were associated with the synthetic processes. The synthetic contexts generated by our model outperformed the baseline models, with the distributions similar to the authentic contexts. We conclude that ProcessGAN can generate sharable synthetic process data indistinguishable from authentic data. Our source code is available in https://github.com/raaachli/ProcessGAN.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401944163",
    "type": "article"
  },
  {
    "title": "LSTGCN: Inductive Spatial Temporal Imputation Using Long Short-Term Dependencies",
    "doi": "https://doi.org/10.1145/3690645",
    "publication_date": "2024-09-02",
    "publication_year": 2024,
    "authors": "Longji Huang; Jianbin Huang; He Li; Jiangtao Cui",
    "corresponding_authors": "",
    "abstract": "Spatial temporal forecasting of urban sensors is essentially important for many urban systems, such as intelligent transportation and smart cities. However, due to the problem of hardware failure or network failure, there are some missing values or missing monitoring sensors that need to be interpolated. Recent research on deep learning has made substantial progress on imputation problem, especially temporal aspect (i.e., time series imputation), while little attention has been paid to spatial aspect (both dynamic and static) and long-term temporal dependencies. In this article, we proposed a spatial temporal imputation model, named Long Short-Term Graph Convolution Networks (LSTGCN), which includes gated temporal extraction (GTE) module, multi-head attention-based temporal capture (MHAT) module, long-term periodic temporal encoding (LPTE) module, and bidirectional spatial graph convolution (BSGC) module. The GTE adopts a gated mechanism to filter short-term temporal information, while the MHAT utilizes position encoding to enhance the difference of each timestamps, then use multi-head attention to capture short-term temporal dependency. The BSGC is adopted to handle with spatial relationships between sensor nodes. And we design a periodic encoding technique to process long-term temporal dependencies. The BSGC handles spatial relationships between sensor nodes, and a periodic encoding technique is used to process long-term temporal dependencies. Our experimental analysis includes completion and forecasting tasks, as well as transfer and ablation analyses. The results show that our proposed model outperforms state-of-the-art baselines on real-world datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402131318",
    "type": "article"
  },
  {
    "title": "SDE-HNN: Accurate and Well-calibrated Forecasting using Stochastic Differential Equations",
    "doi": "https://doi.org/10.1145/3691346",
    "publication_date": "2024-09-10",
    "publication_year": 2024,
    "authors": "Peng Cui; Zhijie Deng; Wenbo Hu; Jun Zhu",
    "corresponding_authors": "",
    "abstract": "It is crucial yet challenging for deep learning models to properly characterize uncertainty that is pervasive in real-world environments. Heteroscedastic neural networks (HNNs) are promising methods that capture data uncertainty for forecasting problems while existing HNNs have difficulties in conjoining calibrated uncertainty estimation and satisfactory predictive performance due to the failure to construct an explicit interaction between the prediction and its associated uncertainty. This article develops SDE-HNN, an improved HNN equipped with stochastic differential equations (SDE), to characterize the interaction between the predictive mean and variance inside HNNs for accurate and reliable forecasting. The existence and uniqueness of the solution to the devised neural SDE are guaranteed. Moreover, based on the bias-variance tradeoff for the optimization in SDE-HNN, we design an enhanced numerical SDE solver to improve learning stability. Finally, we present two new diagnostic uncertainty metrics to systematically evaluate the predictive uncertainty. Experiments on various challenging datasets show that our method significantly outperforms state-of-the-art baselines on both predictive performance and uncertainty quantification, delivering well-calibrated and sharp prediction intervals in time-series forecasting.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402413094",
    "type": "article"
  },
  {
    "title": "Label Distribution Guided Hashing for Cross-Modal Retrieval",
    "doi": "https://doi.org/10.1145/3697353",
    "publication_date": "2024-09-26",
    "publication_year": 2024,
    "authors": "Fatang Lei; Chao Zhang; Huaxiong Li; Gao Yang; Chunlin Chen",
    "corresponding_authors": "",
    "abstract": "Hashing methods have recently attracted extensive attention in cross-modal retrieval. Most supervised hashing methods attempt to preserve the semantic information into hash codes by leveraging the original logical label matrix. However, they generally treat all labels equally, and ignore the relative significance of different labels due to the variety of data features. In this paper, we argue that exploring the relative importance of labels benefits the enhancement of semantic information, and we propose a novel LAbel Distribution guided Hashing (LADH) method for cross-modal retrieval. In particular, LADH first learns a feature-induced label distribution for each sample to weigh different labels, which leverages the multi-modal feature information to enrich the semantic label information. By jointly using the learned label distributions and multi-modal features, the latent representation and hash codes are obtained with multi-modal feature selection and enhanced semantic similarities embedded. An efficient algorithm is designed to solve the proposed method whose time complexity is linear to the number of the training instances. Experimental results on several public benchmark datasets verify the effectiveness and efficiency of our method compared with the state-of-the-art methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402860305",
    "type": "article"
  },
  {
    "title": "FGTL: Federated Graph Transfer Learning for Node Classification",
    "doi": "https://doi.org/10.1145/3699962",
    "publication_date": "2024-10-14",
    "publication_year": 2024,
    "authors": "Chengyuan Mai; Tianchi Liao; Chuan Chen; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "Unsupervised multi-source domain transfer in federated scenario has become an emerging research direction, which can help unlabeled target domain to obtain the adapted model through source domains under privacy-preserving. However, when local data is graph, the difference of domains (or data heterogeneity) mainly originate from the difference in node attributes and subgraph structures, leading to serious model drift, which is not considered by the existing related algorithms. Currently, there are two challenges in this scenario: (1) The node representations extracted directly through conventional GNNs lack inter-domain generalized and consistent information, making it difficult to apply existing federated learning algorithms. (2) The knowledge of source domains has quality differences, which may lead to negative transfer. To address these issues, we propose a novel two-phase Federated Graph Transfer Learning (FGTL) framework. In the generalization phase, FGTL utilizes local contrastive learning and global context embedding to force node representations to capture the inter-domain generalized and consistent information, lightly alleviating model drift. In the transfer phase, FGTL utilizes consensus knowledge to force the decision bound of classifier to adapt to the target client. In addition, FGTL+ exploits model grouping to make consensus knowledge generation more efficient, further enhancing the scalability of FGTL. Extensive experiments show that FGTL significantly outperforms state-of-the-art related methods, while FGTL+ further enhances privacy protection and reduces both communication and computation overhead.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403393729",
    "type": "article"
  },
  {
    "title": "Controllable Human Trajectory Generation Using Profile-Guided Latent Diffusion",
    "doi": "https://doi.org/10.1145/3701736",
    "publication_date": "2024-10-25",
    "publication_year": 2024,
    "authors": "Yiwen Song; Jingtao Ding; Jian Yuan; Qingmin Liao; Yong Li",
    "corresponding_authors": "",
    "abstract": "Trajectory generation is a vital element in AI applications. Firstly, it enables simulation such as traffic simulation and epidemic spreading modeling. Secondly, it can provide synthetic privacy-preserving data for training AI models. Notably, trajectory generation featuring controllable user profiles holds substantial value in generating customized mobility trajectories tailored to diverse requirements. However, relevant work is still lacking. On the one hand, traditional deep generative models fall short in guiding controllable trajectory generation due to the statistical nature of human mobility patterns and the corresponding insufficient control mechanisms. On the other hand, though the diffusion model has demonstrated strong generative capabilities in many fields, to achieve controllable generation on discrete trajectory data, we still need to redesign the structure of the continuous diffusion model. In this paper, we introduce a controllable trajectory generation framework that leverages a continuous diffusion model and classifier guidance for more robust condition control. Our proposed framework comprises two modules: a latent trajectory diffusion model and a trajectory classifier for profile guidance. Experiments on two real-world mobility datasets consistently demonstrate its capability of generating trajectories matching given user profiles and conforming to human mobility patterns. Our source code and trained models are released at https://github.com/tsinghua-fib-lab/User-Profile-Guided-Latent-Diffusion .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403762769",
    "type": "article"
  },
  {
    "title": "JobFormer: Skill-Aware Job Recommendation with Semantic-Enhanced Transformer",
    "doi": "https://doi.org/10.1145/3701735",
    "publication_date": "2024-10-26",
    "publication_year": 2024,
    "authors": "Zhihao Guan; Jia‐Qi Yang; Yang Yang; Hengshu Zhu; Wenjie Li; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "Job recommendation aims to provide potential talents with suitable job descriptions (JDs) consistent with their career trajectory, which plays an essential role in proactive talent recruitment. In real-world management scenarios, the available JD-user records always consist of JDs, user profiles, and click data, in which the user profiles are typically summarized as the user's skill distribution for privacy reasons. Although existing sophisticated recommendation methods can be directly employed, effective recommendation still has challenges considering the information deficit of JD itself and the natural heterogeneous gap between JD and user profile. To address these challenges, we proposed a novel skill-aware recommendation model based on the designed semantic-enhanced Transformer to parse JDs and complete personalized job recommendation. Specifically, we first model the relative items of each JD and then adopt an encoder with the local-global attention mechanism to better mine the intra-job and inter-job dependencies from JD tuples. Moreover, we adopt a two-stage learning strategy for skill-aware recommendation, in which we utilize the skill distribution to guide JD representation learning in the recall stage, and then combine the user profiles for final prediction in the ranking stage. Consequently, we can embed rich contextual semantic representations for learning JDs, while skill-aware recommendation provides effective JD-user joint representation for click-through rate (CTR) prediction. To validate the superior performance of our method for job recommendation, we present a thorough empirical analysis of large-scale real-world and public datasets to demonstrate its effectiveness and interpretability.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403789845",
    "type": "article"
  },
  {
    "title": "Digital Twin Enhanced Multi-Agent Reinforcement Learning for Large-Scale Mobile Network Coverage Optimization",
    "doi": "https://doi.org/10.1145/3702644",
    "publication_date": "2024-10-31",
    "publication_year": 2024,
    "authors": "Haoqiang Liu; Weikang Su; Tong Li; Wenzhen Huang; Yong Li",
    "corresponding_authors": "",
    "abstract": "With the rapid advancement of communication technology and the exponential growth of mobile users, improving network coverage quality and throughput has become increasingly important. In particular, large-scale base station (BS) cooperative optimization has become a highly significant topic. BSs can adjust various parameters for high-quality communication, but automating this optimization remains challenging due to environmental sensitivity and interdependencies. Traditional methods for network optimization are constrained by the intricate nature of real-world environments. Further, reinforcement learning (RL) techniques, which are effective for configuration policies, encounter difficulties in intricate, high-dimensional wireless communication networks, especially in multi-agent cooperative optimization. To overcome these challenges, this paper proposes the Enhanced Multi-Agent Proximal Policy Optimization (EMAPPO), which utilizes the capabilities of the UNet network to extract multi-spatial relationships among a massive number of network elements and employs the DiffPool network to efficiently depict the impact of large-scale action coordination among massive agents on coverage performance. To facilitate evaluation in communication optimization, we further introduce a high-fidelity digital twin-driven mobile network. Extensive experiments validate the effectiveness and superior performance of EMAPPO by utilizing the network digital twin. The results demonstrate significant improvements in signal coverage rate and network throughput compared to the competing methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403989853",
    "type": "article"
  },
  {
    "title": "Fuzzy Neural Logic Reasoning for Robust Classification",
    "doi": "https://doi.org/10.1145/3704728",
    "publication_date": "2024-11-18",
    "publication_year": 2024,
    "authors": "Guo Lin; Yongfeng Zhang",
    "corresponding_authors": "",
    "abstract": "The efficacy of neural networks is widely recognized across a multitude of machine learning tasks, yet their black-box nature impedes the understanding of their decision-making processes. Such lack of explainability limits their use in high-stake fields such as medicine and finance, where transparent decision-making is essential. In contrast, traditional rule-based models offer clear input-output mappings, but often lag in performance when compared to their neural network counterparts. To address this challenge, this study introduces Fuzzy Neural Logic Reasoning (FNLR), a novel architecture that combines the best of both rule-based and deep learning models to achieve performance, interpretability, and noise robustness simultaneously. At its core, FNLR employs a “Symbolic Pre-training + Neural Fine-tuning” paradigm. Initially, the model adapts a pre-fitted binary decision tree. It then performs a “neuralization” process, replacing each node of the tree with a corresponding neural network equivalent. This transformation is facilitated through three shallow MLP modules, which are trained to emulate the relational operators intrinsic to decision trees. The model architecture is also extensible, allowing it to further boost expressiveness. Furthermore, FNLR incorporates fuzzy logic by proposing novel fuzzy relational operators, accounting for satisfaction degrees of propositions and thus eliminating rigid decision boundaries. This approach enhances model flexibility, enabling all paths of the decision tree to contribute to the target prediction in a weighted manner. Empirical evaluations on tabular datasets from various domains demonstrate that FNLR performs comparably to, or better than, state-of-the-art deep learning models designed for tabular data, while also exhibiting strong robustness to noise.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404462694",
    "type": "article"
  },
  {
    "title": "Learning Knowledge-diverse Experts for Long-tailed Graph Classification",
    "doi": "https://doi.org/10.1145/3705323",
    "publication_date": "2024-11-22",
    "publication_year": 2024,
    "authors": "Zhengyang Mao; Wei Ju; Siyu Yi; Yifan Wang; Zhiping Xiao; Qingqing Long; Nan Yin; Xinwang Liu; Ming Zhang",
    "corresponding_authors": "",
    "abstract": "Graph neural networks (GNNs) have shown remarkable success in graph-level classification tasks. However, most of the existing GNN-based studies are based on balanced datasets, while many real-world datasets exhibit long-tailed distributions. In such datasets, the tail classes receive limited attention during training, leading to prediction bias and degraded performance. To address this issue, a range of long-tailed learning strategies have been proposed, such as data re-balancing, and transfer learning. However, these approaches encounter several challenges, including insufficient representation capacity for tail classes and their evaluation solely on uniform test data, limiting their capacity to handle unknown class distributions. To tackle these challenges, we introduce a novel framework, namely K nowledge- D iverse EX perts (KDEX) for long-tailed graph classification. Our KDEX leverages a dynamic memory module to enable the transfer of knowledge from head to tail, which improves the representation ability of the tail. To deal with unknown test distributions, KDEX introduces a knowledge-diverse expert training approach to train experts with different capacities in managing various test distributions. Moreover, we train the hierarchical router in a self-supervised manner to dynamically aggregate each knowledge-diverse expert during testing. Experimental results on multiple benchmarks reveal that our KDEX outperforms current baselines in both standard and test-agnostic long-tailed graph classification.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404637705",
    "type": "article"
  },
  {
    "title": "Heterogeneous Hyperbolic Hypergraph Neural Network for Friend Recommendation in Location-based Social Networks",
    "doi": "https://doi.org/10.1145/3708999",
    "publication_date": "2024-12-19",
    "publication_year": 2024,
    "authors": "Yongkang Li; Zipei Fan; Xuan Song",
    "corresponding_authors": "",
    "abstract": "Friend recommendation is an important real-world application in location-based social networks (LBSN), helping users discover potential friends and enhance their overall happiness. LBSN mainly comprises two distinct data structures: spatio-temporal data for human mobility and graph data for social networks. These two data structures make it challenging to model the complex relationships between them, which are essential for comprehensively understanding users’ lives. Previous studies have either modeled user trajectories and social networks separately or used classical simple graph-based methods, where a simple edge links only two nodes, failing to capture the multiple relationships inherent in LBSN. Furthermore, most studies have relied on Euclidean space to train their graph models, which could result in significant distortion because of tree-like social network data structure. To address these limitations, we propose a novel heterogeneous LBSN hypergraph that represents user check-in records and continuous trajectories—comprising multiple Points of Interest (POI)—as hyperedges, enabling the representation of complex spatio-temporal relationships. This approach enables us to link multiple nodes of different types by hyperedges and use hyperbolic spaces to create more efficient graph representations. Additionally, we devise a new type-specific attention mechanism for our Heterogeneous Hyperbolic Hypergraph Neural Network (H 3 GNN), which is end-to-end trainable and employs supervised contrastive learning to learn hypergraph node embeddings for the subsequent friend recommendation task with the help of hyperbolic space. Finally, our model H 3 GNN achieves better results than existing methods on six real-world city datasets, and our ablation studies demonstrate the effectiveness of each component. Additionally, our experiments indicate that H 3 GNN requires less data storage and training time compared to previous methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405576465",
    "type": "article"
  },
  {
    "title": "Bregman bubble clustering",
    "doi": "https://doi.org/10.1145/1376815.1376817",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Gunjan Gupta; Joydeep Ghosh",
    "corresponding_authors": "",
    "abstract": "In classical clustering, each data point is assigned to at least one cluster. However, in many applications only a small subset of the available data is relevant for the problem and the rest needs to be ignored in order to obtain good clusters. Certain nonparametric density-based clustering methods find the most relevant data as multiple dense regions, but such methods are generally limited to low-dimensional data and do not scale well to large, high-dimensional datasets. Also, they use a specific notion of “distance”, typically Euclidean or Mahalanobis distance, which further limits their applicability. On the other hand, the recent One Class Information Bottleneck (OC-IB) method is fast and works on a large class of distortion measures known as Bregman Divergences, but can only find a single dense region. This article presents a broad framework for finding k dense clusters while ignoring the rest of the data. It includes a seeding algorithm that can automatically determine a suitable value for k . When k is forced to 1, our method gives rise to an improved version of OC-IB with optimality guarantees. We provide a generative model that yields the proposed iterative algorithm for finding k dense regions as a special case. Our analysis reveals an interesting and novel connection between the problem of finding dense regions and exponential mixture models; a hard model corresponding to k exponential mixtures with a uniform background results in a set of k dense clusters. The proposed method describes a highly scalable algorithm for finding multiple dense regions that works with any Bregman Divergence, thus extending density based clustering to a variety of non-Euclidean problems not addressable by earlier methods. We present empirical results on three artificial, two microarray and one text dataset to show the relevance and effectiveness of our methods.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2167839901",
    "type": "article"
  },
  {
    "title": "Semi-analytical method for analyzing models and model selection measures based on moment analysis",
    "doi": "https://doi.org/10.1145/1497577.1497579",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Amit Dhurandhar; Alin Dobra",
    "corresponding_authors": "",
    "abstract": "In this article we propose a moment-based method for studying models and model selection measures. By focusing on the probabilistic space of classifiers induced by the classification algorithm rather than on that of datasets, we obtain efficient characterizations for computing the moments, which is followed by visualization of the resulting formulae that are too complicated for direct interpretation. By assuming the data to be drawn independently and identically distributed from the underlying probability distribution, and by going over the space of all possible datasets, we establish general relationships between the generalization error, hold-out-set error, cross-validation error, and leave-one-out error. We later exemplify the method and the results by studying the behavior of the errors for the naive Bayes classifier.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2144488378",
    "type": "article"
  },
  {
    "title": "VisIRR",
    "doi": "https://doi.org/10.1145/3070616",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Jaegul Choo; Hannah Kim; Edward Clarkson; Zhicheng Liu; Changhyun Lee; Fuxin Li; Han‐Seung Lee; Ramakrishnan Kannan; Charles D. Stolper; John Stasko; Haesun Park",
    "corresponding_authors": "",
    "abstract": "In this article, we present an interactive visual information retrieval and recommendation system, called VisIRR, for large-scale document discovery. VisIRR effectively combines the paradigms of (1) a passive pull through query processes for retrieval and (2) an active push that recommends items of potential interest to users based on their preferences. Equipped with an efficient dynamic query interface against a large-scale corpus, VisIRR organizes the retrieved documents into high-level topics and visualizes them in a 2D space, representing the relationships among the topics along with their keyword summary. In addition, based on interactive personalized preference feedback with regard to documents, VisIRR provides document recommendations from the entire corpus, which are beyond the retrieved sets. Such recommended documents are visualized in the same space as the retrieved documents, so that users can seamlessly analyze both existing and newly recommended ones. This article presents novel computational methods, which make these integrated representations and fast interactions possible for a large-scale document corpus. We illustrate how the system works by providing detailed usage scenarios. Additionally, we present preliminary user study results for evaluating the effectiveness of the system.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2785921989",
    "type": "article"
  },
  {
    "title": "PSP-AMS",
    "doi": "https://doi.org/10.1145/3281632",
    "publication_date": "2018-12-19",
    "publication_year": 2018,
    "authors": "Bijay Prasad Jaysawal; Jen-Wei Huang",
    "corresponding_authors": "",
    "abstract": "Sequential pattern mining is used to find frequent data sequences over time. When sequential patterns are generated, the newly arriving patterns may not be identified as frequent sequential patterns due to the existence of old data and sequences. Progressive sequential pattern mining aims to find the most up-to-date sequential patterns given that obsolete items will be deleted from the sequences. When sequences come with multiple data streams, it is difficult to maintain and update the current sequential patterns. Even worse, when we consider the sequences across multiple streams, previous methods cannot efficiently compute the frequent sequential patterns. In this work, we propose an efficient algorithm PSP-AMS to address this problem. PSP-AMS uses a novel data structure PSP-MS-tree to insert new items, update current items, and delete obsolete items. By maintaining a PSP-MS-tree, PSP-AMS efficiently finds the frequent sequential patterns across multiple streams. The experimental results show that PSP-AMS significantly outperforms previous algorithms for mining of progressive sequential patterns across multiple streams on synthetic data as well as real data.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2906474394",
    "type": "article"
  },
  {
    "title": "Interactive Discovery of Coordinated Relationship Chains with Maximum Entropy Models",
    "doi": "https://doi.org/10.1145/3047017",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Hao Wu; Maoyuan Sun; Peng Mi; Nikolaj Tatti; Chris North; Naren Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "Modern visual analytic tools promote human-in-the-loop analysis but are limited in their ability to direct the user toward interesting and promising directions of study. This problem is especially acute when the analysis task is exploratory in nature, e.g., the discovery of potentially coordinated relationships in massive text datasets. Such tasks are very common in domains like intelligence analysis and security forensics where the goal is to uncover surprising coalitions bridging multiple types of relations. We introduce new maximum entropy models to discover surprising chains of relationships leveraging count data about entity occurrences in documents. These models are embedded in a visual analytic system called MERCER (Maximum Entropy Relational Chain ExploRer) that treats relationship bundles as first class objects and directs the user toward promising lines of inquiry. We demonstrate how user input can judiciously direct analysis toward valid conclusions, whereas a purely algorithmic approach could be led astray. Experimental results on both synthetic and real datasets from the intelligence community are presented.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2964170309",
    "type": "article"
  },
  {
    "title": "Learning bayesian networks from Markov random fields",
    "doi": "https://doi.org/10.1145/2362383.2362384",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Zhenxing Wang; Laiwan Chan",
    "corresponding_authors": "",
    "abstract": "Dependency analysis is a typical approach for Bayesian network learning, which infers the structures of Bayesian networks by the results of a series of conditional independence (CI) tests. In practice, testing independence conditioning on large sets hampers the performance of dependency analysis algorithms in terms of accuracy and running time for the following reasons. First, testing independence on large sets of variables with limited samples is not stable. Second, for most dependency analysis algorithms, the number of CI tests grows at an exponential rate with the sizes of conditioning sets, and the running time grows of the same rate. Therefore, determining how to reduce the number of CI tests and the sizes of conditioning sets becomes a critical step in dependency analysis algorithms. In this article, we address a two-phase algorithm based on the observation that the structures of Markov random fields are similar to those of Bayesian networks. The first phase of the algorithm constructs a Markov random field from data, which provides a close approximation to the structure of the true Bayesian network; the second phase of the algorithm removes redundant edges according to CI tests to get the true Bayesian network. Both phases use Markov blanket information to reduce the sizes of conditioning sets and the number of CI tests without sacrificing accuracy. An empirical study shows that the two-phase algorithm performs well in terms of accuracy and efficiency.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1981029964",
    "type": "article"
  },
  {
    "title": "Classification in P2P networks with cascade support vector machines",
    "doi": "https://doi.org/10.1145/2541268.2541273",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Hock Hee Ang; Vivekanand Gopalkrishnan; Steven C. H. Hoi; Wee Keong Ng",
    "corresponding_authors": "",
    "abstract": "Classification in Peer-to-Peer (P2P) networks is important to many real applications, such as distributed intrusion detection, distributed recommendation systems, and distributed antispam detection. However, it is very challenging to perform classification in P2P networks due to many practical issues, such as scalability, peer dynamism, and asynchronism. This article investigates the practical techniques of constructing Support Vector Machine (SVM) classifiers in the P2P networks. In particular, we demonstrate how to efficiently cascade SVM in a P2P network with the use of reduced SVM. In addition, we propose to fuse the concept of cascade SVM with bootstrap aggregation to effectively balance the trade-off between classification accuracy, model construction, and prediction cost. We provide theoretical insights for the proposed solutions and conduct an extensive set of empirical studies on a number of large-scale datasets. Encouraging results validate the efficacy of the proposed approach.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2010211461",
    "type": "article"
  },
  {
    "title": "Designing Size Consistent Statistics for Accurate Anomaly Detection in Dynamic Networks",
    "doi": "https://doi.org/10.1145/3185059",
    "publication_date": "2018-04-16",
    "publication_year": 2018,
    "authors": "Timothy La Fond; Jennifer Neville; Brian Gallagher",
    "corresponding_authors": "",
    "abstract": "An important task in network analysis is the detection of anomalous events in a network time series. These events could merely be times of interest in the network timeline or they could be examples of malicious activity or network malfunction. Hypothesis testing using network statistics to summarize the behavior of the network provides a robust framework for the anomaly detection decision process. Unfortunately, choosing network statistics that are dependent on confounding factors like the total number of nodes or edges can lead to incorrect conclusions (e.g., false positives and false negatives). In this article, we describe the challenges that face anomaly detection in dynamic network streams regarding confounding factors. We also provide two solutions to avoiding error due to confounding factors: the first is a randomization testing method that controls for confounding factors, and the second is a set of size-consistent network statistics that avoid confounding due to the most common factors, edge count and node count.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2802022101",
    "type": "article"
  },
  {
    "title": "Large-Scale Adversarial Sports Play Retrieval with Learning to Rank",
    "doi": "https://doi.org/10.1145/3230667",
    "publication_date": "2018-08-22",
    "publication_year": 2018,
    "authors": "Mingyang Di; Diego Klabjan; Long Sha; Patrick Lucey",
    "corresponding_authors": "",
    "abstract": "As teams of professional leagues are becoming more and more analytically driven, the interest in effective data management and access of sports plays has dramatically increased. In this article, we present a retrieval system that can quickly find the most relevant plays from historical games given an input query. To search through a large number of games at an interactive speed, our system is built upon a distributed framework so that each query-result pair is evaluated in parallel. We also propose a pairwise learning to rank approach to improve search ranking based on users’ clickthrough behavior. The similarity metric in training the rank function is based on automatically learnt features from a convolutional autoencoder. Finally, we showcase the efficacy of our learning to rank approach by demonstrating rank quality in a user study.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2888134386",
    "type": "article"
  },
  {
    "title": "Efficient Nonnegative Tensor Factorization via Saturating Coordinate Descent",
    "doi": "https://doi.org/10.1145/3385654",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Thirunavukarasu Balasubramaniam; Richi Nayak; Chau Yuen",
    "corresponding_authors": "",
    "abstract": "With the advancements in computing technology and web-based applications, data are increasingly generated in multi-dimensional form. These data are usually sparse due to the presence of a large number of users and fewer user interactions. To deal with this, the Nonnegative Tensor Factorization (NTF) based methods have been widely used. However existing factorization algorithms are not suitable to process in all three conditions of size, density, and rank of the tensor. Consequently, their applicability becomes limited. In this article, we propose a novel fast and efficient NTF algorithm using the element selection approach. We calculate the element importance using Lipschitz continuity and propose a saturation point-based element selection method that chooses a set of elements column-wise for updating to solve the optimization problem. Empirical analysis reveals that the proposed algorithm is scalable in terms of tensor size, density, and rank in comparison to the relevant state-of-the-art algorithms.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3009375285",
    "type": "article"
  },
  {
    "title": "Continuous Influence Maximization",
    "doi": "https://doi.org/10.1145/3380928",
    "publication_date": "2020-03-13",
    "publication_year": 2020,
    "authors": "Yang Yu; Xiangbo Mao; Jian Pei; Xiaofei He",
    "corresponding_authors": "",
    "abstract": "Imagine we are introducing a new product through a social network, where we know for each user in the network the function of purchase probability with respect to discount. Then, what discounts should we offer to those social network users so that, under a predefined budget, the adoption of the product is maximized in expectation? Although influence maximization has been extensively explored, this appealing practical problem still cannot be answered by the existing influence maximization methods. In this article, we tackle the problem systematically. We formulate the general continuous influence maximization problem, investigate the essential properties, and develop a general coordinate descent algorithmic framework as well as the engineering techniques for practical implementation. Our investigation does not assume any specific influence model and thus is general and principled. At the same time, using the most popularly adopted triggering model as a concrete example, we demonstrate that more efficient methods are feasible under specific influence models. Our extensive empirical study on four benchmark real-world networks with synthesized purchase probability curves clearly illustrates that continuous influence maximization can improve influence spread significantly with very moderate extra running time comparing to the classical influence maximization methods.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3011191765",
    "type": "article"
  },
  {
    "title": "Mining Career Paths from Large Resume Databases",
    "doi": "https://doi.org/10.1145/3379984",
    "publication_date": "2020-05-13",
    "publication_year": 2020,
    "authors": "Theodoros Lappas",
    "corresponding_authors": "Theodoros Lappas",
    "abstract": "The emergence of online professional platforms, such as LinkedIn and Indeed, has led to unprecedented volumes of rich resume data that have revolutionized the study of careers. One of the most prevalent problems in this space is the extraction of prototype career paths from a workforce. Previous research has consistently relied on a two-step approach to tackle this problem. The first step computes the pairwise distances between all the career sequences in the database. The second step uses the distance matrix to create clusters, with each cluster representing a different prototype path. As we demonstrate in this work, this approach faces two significant challenges when applied on large resume databases. First, the overwhelming diversity of job titles in the modern workforce prevents the accurate evaluation of distance between career sequences. Second, the clustering step of the standard approach leads to highly heterogeneous clusters, due to its inability to handle categorical sequences and sensitivity to outliers. This leads to non-representative centroids and spurious prototype paths that do not accurately represent the actual groups in the workforce. Our work addresses these two challenges and has practical implications for the numerous researchers and practitioners working on the analysis of career data across domains.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3026770102",
    "type": "article"
  },
  {
    "title": "CrowdWT",
    "doi": "https://doi.org/10.1145/3421712",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Jinzheng Tu; Guoxian Yu; Jun Wang; Carlotta Domeniconi; Maozu Guo; Xiangliang Zhang",
    "corresponding_authors": "",
    "abstract": "Crowdsourcing is a relatively inexpensive and efficient mechanism to collect annotations of data from the open Internet. Crowdsourcing workers are paid for the provided annotations, but the task requester usually has a limited budget. It is desirable to wisely assign the appropriate task to the right workers, so the overall annotation quality is maximized while the cost is reduced. In this article, we propose a novel task assignment strategy (CrowdWT) to capture the complex interactions between tasks and workers, and properly assign tasks to workers. CrowdWT first develops a Worker Bias Model (WBM) to jointly model the worker’s bias, the ground truths of tasks, and the task features. WBM constructs a mapping between task features and worker annotations to dynamically assign the task to a group of workers, who are more likely to give correct annotations for the task. CrowdWT further introduces a Task Difficulty Model (TDM), which builds a Kernel ridge regressor based on task features to quantify the intrinsic difficulty of tasks and thus to assign the difficult tasks to more reliable workers. Finally, CrowdWT combines WBM and TDM into a unified model to dynamically assign tasks to a group of workers and recall more reliable and even expert workers to annotate the difficult tasks. Our experimental results on two real-world datasets and two semi-synthetic datasets show that CrowdWT achieves high-quality answers within a limited budget, and has the best performance against competitive methods.&lt;?vsp -1.5pt?&gt;",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3111504093",
    "type": "article"
  },
  {
    "title": "Maximum Likelihood Estimation of Power-law Degree Distributions via Friendship Paradox-based Sampling",
    "doi": "https://doi.org/10.1145/3451166",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Buddhika Nettasinghe; Vikram Krishnamurthy",
    "corresponding_authors": "",
    "abstract": "This article considers the problem of estimating a power-law degree distribution of an undirected network using sampled data. Although power-law degree distributions are ubiquitous in nature, the widely used parametric methods for estimating them (e.g., linear regression on double-logarithmic axes and maximum likelihood estimation with uniformly sampled nodes) suffer from the large variance introduced by the lack of data-points from the tail portion of the power-law degree distribution. As a solution, we present a novel maximum likelihood estimation approach that exploits the friendship paradox to sample more efficiently from the tail of the degree distribution. We analytically show that the proposed method results in a smaller bias, variance and a Cramèr–Rao lower bound compared to the vanilla maximum likelihood estimate obtained with uniformly sampled nodes (which is the most commonly used method in literature). Detailed numerical and empirical results are presented to illustrate the performance of the proposed method under different conditions and how it compares with alternative methods. We also show that the proposed method and its desirable properties (i.e., smaller bias, variance, and Cramèr–Rao lower bound compared to vanilla method based on uniform samples) extend to parametric degree distributions other than the power-law such as exponential degree distributions as well. All the numerical and empirical results are reproducible and the code is publicly available on Github.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3160332889",
    "type": "article"
  },
  {
    "title": "Semi-Supervised Ensemble Learning for Dealing with Inaccurate and Incomplete Supervision",
    "doi": "https://doi.org/10.1145/3473910",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Mona Nashaat; Aindrila Ghosh; James A. Miller; Shaikh Quader",
    "corresponding_authors": "",
    "abstract": "In real-world tasks, obtaining a large set of noise-free data can be prohibitively expensive. Therefore, recent research tries to enable machine learning to work with weakly supervised datasets, such as inaccurate or incomplete data. However, the previous literature treats each type of weak supervision individually, although, in most cases, different types of weak supervision tend to occur simultaneously. Therefore, in this article, we present Smart MEnDR, a Classification Model that applies Ensemble Learning and Data-driven Rectification to deal with inaccurate and incomplete supervised datasets. The model first applies a preliminary phase of ensemble learning in which the noisy data points are detected while exploiting the unlabelled data. The phase employs a semi-supervised technique with maximum likelihood estimation to decide on the disagreement rate. Second, the proposed approach applies an iterative meta-learning step to tackle the problem of knowing which points should be made correct to improve the performance of the final classifier. To evaluate the proposed framework, we report the classification performance, noise detection, and the labelling accuracy of the proposed method against state-of-the-art techniques. The experimental results demonstrate the effectiveness of the proposed framework in detecting noise, providing correct labels, and attaining high classification performance.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3209720970",
    "type": "article"
  },
  {
    "title": "Pairwised Specific Distance Learning from Physical Linkages",
    "doi": "https://doi.org/10.1145/2700405",
    "publication_date": "2015-04-01",
    "publication_year": 2015,
    "authors": "Juhua Hu; De‐Chuan Zhan; Xintao Wu; Yuan Jiang; Zhi‐Hua Zhou",
    "corresponding_authors": "",
    "abstract": "In real tasks, usually a good classification performance can only be obtained when a good distance metric is obtained; therefore, distance metric learning has attracted significant attention in the past few years. Typical studies of distance metric learning evaluate how to construct an appropriate distance metric that is able to separate training data points from different classes or satisfy a set of constraints (e.g., must-links and/or cannot-links). It is noteworthy that this task becomes challenging when there are only limited labeled training data points and no constraints are given explicitly. Moreover, most existing approaches aim to construct a global distance metric that is applicable to all data points. However, different data points may have different properties and may require different distance metrics. We notice that data points in real tasks are often connected by physical links (e.g., people are linked with each other in social networks; personal webpages are often connected to other webpages, including nonpersonal webpages), but the linkage information has not been exploited in distance metric learning. In this article, we develop a pairwised specific distance (PSD) approach that exploits the structures of physical linkages and in particular captures the key observations that nonmetric and clique linkages imply the appearance of different or unique semantics, respectively. It is noteworthy that, rather than generating a global distance, PSD generates different distances for different pairs of data points; this property is desired in applications involving complicated data semantics. We mainly present PSD for multi-class learning and further extend it to multi-label learning. Experimental results validate the effectiveness of PSD, especially in the scenarios in which there are very limited labeled training data points and no explicit constraints are given.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2104739568",
    "type": "article"
  },
  {
    "title": "Toward Generalizing the Unification with Statistical Outliers",
    "doi": "https://doi.org/10.1145/2829956",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "Fabrizio Angiulli; Fabio Fassetti",
    "corresponding_authors": "",
    "abstract": "In this work, we introduce a novel definition of outlier, namely the Gradient Outlier Factor (or GOF), with the aim to provide a definition that unifies with the statistical one on some standard distributions but has a different behavior in the presence of mixture distributions. Intuitively, the GOF score measures the probability to stay in the neighborhood of a certain object. It is directly proportional to the density and inversely proportional to the variation of the density. We derive formal properties under which the GOF definition unifies the statistical outlier definition and show that the unification holds for some standard distributions, while the GOF is able to capture tails in the presence of different distributions even if their densities sensibly differ. Moreover, we provide a probabilistic interpretation of the GOF score, by means of the notion of density of the data density. Experimental results confirm that there are scenarios in which the novel definition can be profitably employed. To the best of our knowledge, except for distance-based outlier, no other data mining outlier definition has a so clearly established relationship with statistical outliers.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2289647598",
    "type": "article"
  },
  {
    "title": "Mining User Development Signals for Online Community Churner Detection",
    "doi": "https://doi.org/10.1145/2798730",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "Matthew Rowe",
    "corresponding_authors": "Matthew Rowe",
    "abstract": "Churners are users who stop using a given service after previously signing up. In the domain of telecommunications and video games, churners represent a loss of revenue as a user leaving indicates that they will no longer pay for the service. In the context of online community platforms (e.g., community message boards, social networking sites, question--answering systems, etc.), the churning of a user can represent different kinds of loss: of social capital, of expertise, or of a vibrant individual who is a mediator for interaction and communication. Detecting which users are likely to churn from online communities, therefore, enables community managers to offer incentives to entice those users back; as retention is less expensive than re-signing users up. In this article, we tackle the task of detecting churners on four online community platforms by mining user development signals. These signals explain how users have evolved along different dimensions (i.e., social and lexical) relative to their prior behaviour and the community in which they have interacted. We present a linear model, based upon elastic-net regularisation, that uses extracted features from the signals to detect churners. Our evaluation of this model against several state of the art baselines, including our own prior work, empirically demonstrates the superior performance that this approach achieves for several experimental settings. This article presents a novel approach to churn prediction that takes a different route from existing approaches that are based on measuring static social network properties of users (e.g., centrality, in-degree, etc.).",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2290071532",
    "type": "article"
  },
  {
    "title": "Co-Clustering Structural Temporal Data with Applications to Semiconductor Manufacturing",
    "doi": "https://doi.org/10.1145/2875427",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Yada Zhu; Jingrui He",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed data explosion in semiconductor manufacturing due to advances in instrumentation and storage techniques. The large amount of data associated with process variables monitored over time form a rich reservoir of information, which can be used for a variety of purposes, such as anomaly detection, quality control, and fault diagnostics. In particular, following the same recipe for a certain Integrated Circuit device, multiple tools and chambers can be deployed for the production of this device, during which multiple time series can be collected, such as temperature, impedance, gas flow, electric bias, etc. These time series naturally fit into a two-dimensional array (matrix), i.e., each element in this array corresponds to a time series for one process variable from one chamber. To leverage the rich structural information in such temporal data, in this article, we propose a novel framework named C-Struts to simultaneously cluster on the two dimensions of this array. In this framework, we interpret the structural information as a set of constraints on the cluster membership, introduce an auxiliary probability distribution accordingly, and design an iterative algorithm to assign each time series to a certain cluster on each dimension. Furthermore, we establish the equivalence between C-Struts and a generic optimization problem, which is able to accommodate various distance functions. Extensive experiments on synthetic, benchmark, as well as manufacturing datasets demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2399090549",
    "type": "article"
  },
  {
    "title": "Dual-MGAN: An Efficient Approach for Semi-supervised Outlier Detection with Few Identified Anomalies",
    "doi": "https://doi.org/10.1145/3522690",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Zhe Li; Chunhua Sun; Chunli Liu; Xiayu Chen; Meng Wang; Yezheng Liu",
    "corresponding_authors": "",
    "abstract": "Outlier detection is an important task in data mining, and many technologies for it have been explored in various applications. However, owing to the default assumption that outliers are not concentrated, unsupervised outlier detection may not correctly identify group anomalies with higher levels of density. Although high detection rates and optimal parameters can usually be achieved by using supervised outlier detection, obtaining a sufficient number of correct labels is a time-consuming task. To solve these problems, we focus on semi-supervised outlier detection with few identified anomalies and a large amount of unlabeled data. The task of semi-supervised outlier detection is first decomposed into the detection of discrete anomalies and that of partially identified group anomalies, and a distribution construction sub-module and a data augmentation sub-module are then proposed to identify them, respectively. In this way, the dual multiple generative adversarial networks (Dual-MGAN) that combine the two sub-modules can identify discrete as well as partially identified group anomalies. In addition, in view of the difficulty of determining the stop node of training, two evaluation indicators are introduced to evaluate the training status of the sub-GANs. Extensive experiments on synthetic and real-world data show that the proposed Dual-MGAN can significantly improve the accuracy of outlier detection, and the proposed evaluation indicators can reflect the training status of the sub-GANs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4220943531",
    "type": "article"
  },
  {
    "title": "PSL: An Algorithm for Partial Bayesian Network Structure Learning",
    "doi": "https://doi.org/10.1145/3508071",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Zhaolong Ling; Kui Yu; Lin Liu; Jiuyong Li; Yiwen Zhang; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Learning partial Bayesian network (BN) structure is an interesting and challenging problem. In this challenge, it is computationally expensive to use global BN structure learning algorithms, while only one part of a BN structure is interesting, local BN structure learning algorithms are not a favourable solution either due to the issue of false edge orientation. To address the problem, this article first presents a detailed analysis of the false edge orientation issue with local BN structure learning algorithms and then proposes PSL, an efficient and accurate P artial BN S tructure L earning (PSL) algorithm. Specifically, PSL divides V-structures in a Markov blanket (MB) into two types: Type-C V-structures and Type-NC V-structures, then it starts from the given node of interest and recursively finds both types of V-structures in the MB of the current node until all edges in the partial BN structure are oriented. To further improve the efficiency of PSL, the PSL-FS algorithm is designed by incorporating F eature S election (FS) into PSL. Extensive experiments with six benchmark BNs validate the efficiency and accuracy of the proposed algorithms.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4221116764",
    "type": "article"
  },
  {
    "title": "Segment-Wise Time-Varying Dynamic Bayesian Network with Graph Regularization",
    "doi": "https://doi.org/10.1145/3522589",
    "publication_date": "2022-05-04",
    "publication_year": 2022,
    "authors": "Xing Yang; Chen Zhang; Baihua Zheng",
    "corresponding_authors": "",
    "abstract": "Time-varying dynamic Bayesian network (TVDBN) is essential for describing time-evolving directed conditional dependence structures in complex multivariate systems. In this article, we construct a TVDBN model, together with a score-based method for its structure learning. The model adopts a vector autoregressive (VAR) model to describe inter-slice and intra-slice relations between variables. By allowing VAR parameters to change segment-wisely over time, the time-varying dynamics of the network structure can be described. Furthermore, considering some external information can provide additional similarity information of variables. Graph Laplacian is further imposed to regularize similar nodes to have similar network structures. The regularized maximum a posterior estimation in the Bayesian inference framework is used as a score function for TVDBN structure evaluation, and the alternating direction method of multipliers (ADMM) with L-BFGS-B algorithm is used for optimal structure learning. Thorough simulation studies and a real case study are carried out to verify our proposed method’s efficacy and efficiency.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4229028016",
    "type": "article"
  },
  {
    "title": "Unsupervised Graph-Based Entity Resolution for Complex Entities",
    "doi": "https://doi.org/10.1145/3533016",
    "publication_date": "2022-05-10",
    "publication_year": 2022,
    "authors": "Nishadi Kirielle; Peter Christen; Thilina Ranbaduge",
    "corresponding_authors": "",
    "abstract": "Entity resolution (ER) is the process of linking records that refer to the same entity. Traditionally, this process compares attribute values of records to calculate similarities and then classifies pairs of records as referring to the same entity or not based on these similarities. Recently developed graph-based ER approaches combine relationships between records with attribute similarities to improve linkage quality. Most of these approaches only consider databases containing basic entities that have static attribute values and static relationships, such as publications in bibliographic databases. In contrast, temporal record linkage addresses the problem where attribute values of entities can change over time. However, neither existing graph-based ER nor temporal record linkage can achieve high linkage quality on databases with complex entities , where an entity (such as a person) can change its attribute values over time while having different relationships with other entities at different points in time. In this article, we propose an unsupervised graph-based ER framework that is aimed at linking records of complex entities. Our framework provides five key contributions. First, we propagate positive evidence encountered when linking records to use in subsequent links by propagating attribute values that have changed. Second, we employ negative evidence by applying temporal and link constraints to restrict which candidate record pairs to consider for linking. Third, we leverage the ambiguity of attribute values to disambiguate similar records that, however, belong to different entities. Fourth, we adaptively exploit the structure of relationships to link records that have different relationships. Fifth, using graph measures, we refine matched clusters of records by removing likely wrong links between records. We conduct extensive experiments on seven real-world datasets from different domains showing that on average our unsupervised graph-based ER framework can improve precision by up to 25% and recall by up to 29% compared to several state-of-the-art ER techniques.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4280528717",
    "type": "article"
  },
  {
    "title": "Multi-objective Learning to Overcome Catastrophic Forgetting in Time-series Applications",
    "doi": "https://doi.org/10.1145/3502728",
    "publication_date": "2022-06-17",
    "publication_year": 2022,
    "authors": "Reem Mahmoud; Hazem Hajj",
    "corresponding_authors": "",
    "abstract": "One key objective of artificial intelligence involves the continuous adaptation of machine learning models to new tasks. This branch of continual learning is also referred to as lifelong learning (LL), where a major challenge is to minimize catastrophic forgetting, or forgetting previously learned tasks. While previous work on catastrophic forgetting has been focused on vision problems; this work targets time-series data. In addition to choosing an architecture appropriate for time-series sequences, our work addresses limitations in previous work, including the handling of distribution shifts in class labels. We present multi-objective learning with three loss functions to minimize catastrophic forgetting, prediction error, and errors in generalizing across label shifts, simultaneously. We build a multi-task autoencoder network with a hierarchical convolutional recurrent architecture. The proposed method is capable of learning multiple time-series tasks simultaneously. For cases where the model needs to learn multiple new tasks, we propose sequential learning, starting with tasks that have the best individual performances. This solution was evaluated on four benchmark human activity recognition datasets collected from mobile sensing devices. A wide set of baseline comparisons is performed, and an ablation analysis is run to evaluate the impact of the different losses in the proposed multi-objective method. The results demonstrate an up to 4% performance improvement in catastrophic forgetting compared to the use of loss functions in state-of-the-art solutions while demonstrating minimal losses compared to upper bound methods of traditional fine-tuning (FT) and multi-task learning (MTL).",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4283013819",
    "type": "article"
  },
  {
    "title": "Microblog Retrieval Based on Concept-Enhanced Pre-Training Model",
    "doi": "https://doi.org/10.1145/3552311",
    "publication_date": "2022-08-02",
    "publication_year": 2022,
    "authors": "Yashen Wang; Zhaoyu Wang; Huanhuan Zhang; Zhirun Liu",
    "corresponding_authors": "",
    "abstract": "Despite substantial interest in applications of neural networks to information retrieval, neural ranking models have mostly been applied to conventional ad-hoc retrieval tasks over web pages and newswire articles. This article proposes a concept-enhanced pre-training model for microblog retrieval task, leveraging Semantic Matching Model (SMM) objective and Concept Correlation Model (CCM) objective. The proposed model is a novel neural ranking model specifically designed for ranking short-text microblog, which could merge the advantage of pre-training methodology for generating valid contextualized embedding with the superiority of the prior lexical knowledge (e.g., concept knowledge) for understanding short-text language semantic. We conduct experiments on widely used real-world datasets, and the experimental results demonstrate the efficiency of the proposed model, even compared with latest state-of-the-art neural-based models and pre-training based models.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4289397461",
    "type": "article"
  },
  {
    "title": "Computing Graph Descriptors on Edge Streams",
    "doi": "https://doi.org/10.1145/3591468",
    "publication_date": "2023-04-08",
    "publication_year": 2023,
    "authors": "Zohair Raza Hassan; Sarwan Ali; Imdadullah Khan; Mudassir Shabbir; Waseem Abbas",
    "corresponding_authors": "",
    "abstract": "Feature extraction is an essential task in graph analytics. These feature vectors, called graph descriptors, are used in downstream vector-space-based graph analysis models. This idea has proved fruitful in the past, with spectral-based graph descriptors providing state-of-the-art classification accuracy. However, known algorithms to compute meaningful descriptors do not scale to large graphs since: (1) they require storing the entire graph in memory, and (2) the end-user has no control over the algorithm’s runtime. In this article, we present streaming algorithms to approximately compute three different graph descriptors capturing the essential structure of graphs. Operating on edge streams allows us to avoid storing the entire graph in memory, and controlling the sample size enables us to keep the runtime of our algorithms within desired bounds. We demonstrate the efficacy of the proposed descriptors by analyzing the approximation error and classification accuracy. Our scalable algorithms compute descriptors of graphs with millions of edges within minutes. Moreover, these descriptors yield predictive accuracy comparable to the state-of-the-art methods but can be computed using only 25% as much memory.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3197540743",
    "type": "article"
  },
  {
    "title": "Slack-Factor-Based Fuzzy Support Vector Machine for Class Imbalance Problems",
    "doi": "https://doi.org/10.1145/3579050",
    "publication_date": "2023-01-03",
    "publication_year": 2023,
    "authors": "Jinjun Ren; Yuping Wang; Xiyan Deng",
    "corresponding_authors": "",
    "abstract": "Class imbalance and noisy data widely exist in real-world problems, and the support vector machine (SVM) is hard to construct good classifiers on these data. Fuzzy SVMs (FSVMs), as variants of SVM, use a fuzzy membership function both to reflect the samples’ importance and to remove the impact of noises, and employ cost-sensitive technology to address the class imbalance. They can handle the noise and class imbalance problems in many cases; however, the fuzzy membership functions are often affected by the class imbalance data, leading to inaccurate measures for samples’ performance and affecting the performance of FSVMs. To solve this problem, we design a new fuzzy membership function and combine it with cost-sensitive learning to deal with the class imbalance problem with noisy data, named Slack-Factor-based FSVM (SFFSVM). In SFFSVM, the relative distances between samples and an estimated hyperplane, called slack factors, are used to define the fuzzy membership function. To eliminate the impact of class imbalance on the function and gain more accurate samples’ importance, we rectify the importance according to the positional relationship between the estimated hyperplane and the optimal hyperplane of the problem, and the slack factors of samples. Comprehensive experiments on artificial and real-world datasets demonstrate that SFFSVM outperforms other comparative methods on F1, MCC, and AUC-PR metrics.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4313437247",
    "type": "article"
  },
  {
    "title": "Multi-Stage Machine Learning Model for Hierarchical Tie Valence Prediction",
    "doi": "https://doi.org/10.1145/3579096",
    "publication_date": "2023-01-10",
    "publication_year": 2023,
    "authors": "Karandeep Singh; Seungeon Lee; Giuseppe Labianca; Jesse Fagan; Meeyoung Cha",
    "corresponding_authors": "",
    "abstract": "Individuals interacting in organizational settings involving varying levels of formal hierarchy naturally form a complex network of social ties having different tie valences (e.g., positive and negative connections). Social ties critically affect employees’ satisfaction, behaviors, cognition, and outcomes—yet identifying them solely through survey data is challenging because of the large size of some organizations or the often hidden nature of these ties and their valences. We present a novel deep learning model encompassing NLP and graph neural network techniques that identifies positive and negative ties in a hierarchical network. The proposed model uses human resource attributes as node information and web-logged work conversation data as link information. Our findings suggest that the presence of conversation data improves the tie valence classification by 8.91% compared to employing user attributes alone. This gain came from accurately distinguishing positive ties, particularly for male, non-minority, and older employee groups. We also show a substantial difference in conversation patterns for positive and negative ties with positive ties being associated with more messages exchanged on weekends, and lower use of words related to anger and sadness. These findings have broad implications for facilitating collaboration and managing conflict within organizational and other social networks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4315491122",
    "type": "article"
  },
  {
    "title": "LoSAC: An Efficient Local Stochastic Average Control Method for Federated Optimization",
    "doi": "https://doi.org/10.1145/3566128",
    "publication_date": "2023-01-17",
    "publication_year": 2023,
    "authors": "Huiming Chen; Huandong Wang; Quanming Yao; Yong Li; Depeng Jin; Qiang Yang",
    "corresponding_authors": "",
    "abstract": "Federated optimization (FedOpt), which targets at collaboratively training a learning model across a large number of distributed clients, is vital for federated learning. The primary concerns in FedOpt can be attributed to the model divergence and communication efficiency, which significantly affect the performance. In this article, we propose a new method, i.e., LoSAC, to learn from heterogeneous distributed data more efficiently. Its key algorithmic insight is to locally update the estimate for the global full gradient after each regular local model update. Thus, LoSAC can keep clients’ information refreshed in a more compact way. In particular, we have studied the convergence result for LoSAC. Besides, the bonus of LoSAC is the ability to defend the information leakage from the recent technique Deep Leakage Gradients (DLG). Finally, experiments have verified the superiority of LoSAC comparing with state-of-the-art FedOpt algorithms. Specifically, LoSAC significantly improves communication efficiency by more than 100% on average, mitigates the model divergence problem, and equips with the defense ability against DLG.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4316813716",
    "type": "article"
  },
  {
    "title": "Differentially Private Release of Heterogeneous Network for Managing Healthcare Data",
    "doi": "https://doi.org/10.1145/3580367",
    "publication_date": "2023-01-18",
    "publication_year": 2023,
    "authors": "Rashid Hussain Khokhar; Benjamin C. M. Fung; Farkhund Iqbal; Khalil Al-Hussaeni; Mohammed Hussain",
    "corresponding_authors": "",
    "abstract": "With the increasing adoption of digital health platforms through mobile apps and online services, people have greater flexibility connecting with medical practitioners, pharmacists, and laboratories and accessing resources to manage their own health-related concerns. Many healthcare institutions are connecting with each other to facilitate the exchange of healthcare data, with the goal of effective healthcare data management. The contents generated over these platforms are often shared with third parties for a variety of purposes. However, sharing healthcare data comes with the potential risk of exposing patients’ sensitive information to privacy threats. In this article, we address the challenge of sharing healthcare data while protecting patients’ privacy. We first model a complex healthcare dataset using a heterogeneous information network that consists of multi-type entities and their relationships. We then propose DiffHetNet , an edge-based differentially private algorithm, to protect the sensitive links of patients from inbound and outbound attacks in the heterogeneous health network. We evaluate the performance of our proposed method in terms of information utility and efficiency on different types of real-life datasets that can be modeled as networks. Experimental results suggest that DiffHetNet generally yields less information loss and is significantly more efficient in terms of runtime in comparison with existing network anonymization methods. Furthermore, DiffHetNet is scalable to large network datasets.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4317209824",
    "type": "article"
  },
  {
    "title": "Towards Informative and Diverse Dialogue Systems Over Hierarchical Crowd Intelligence Knowledge Graph",
    "doi": "https://doi.org/10.1145/3583758",
    "publication_date": "2023-02-13",
    "publication_year": 2023,
    "authors": "Hao Wang; Bin Guo; Jiaqi Liu; Yasan Ding; Zhiwen Yu",
    "corresponding_authors": "",
    "abstract": "Knowledge-enhanced dialogue systems aim at generating factually correct and coherent responses by reasoning over knowledge sources, which is a promising research trend. The truly harmonious human-agent dialogue systems need to conduct engaging conversations from three aspects as humans, namely (1) stating factual contents (e.g., records in Wikipedia), (2) conveying subjective and informative opinions about objects (e.g., user discussions on Twitter), and (3) impressing interlocutors with diverse expression styles (e.g., personalized expression habits). The existing knowledge base is a standardized and unified coding for factual knowledge, which could not portray the other two kinds of knowledge to make responses more informative and expressive diverse. To address this, we present CrowdDialog , a crowd intelligence knowledge-enhanced dialogue system, which takes advantage of “crowd intelligence knowledge” extracted from social media (with rich subjective descriptions and diversified expression styles) to promote the performance of dialogue systems. Firstly, to thoroughly mine and organize the crowd intelligence knowledge underlying large-scale and unstructured online contents, we elaborately design the C rowd I ntelligence K nowledge G raph ( CIKG ) structure, including the domain commonsense subgraph, descriptive subgraph, and expressive subgraph. Secondly, to reasonably integrate heterogeneous crowd intelligence knowledge into responses while ensuring logicality and fluency, we propose the G ated F usion with D ynamic Knowledge- D ependent ( GFDD ) model, which generates responses from the semantic and syntactic perspective with the context-aware knowledge gate and dynamic knowledge decoding. Finally, extensive experiments over both Chinese and English dialogue datasets demonstrate that our approach GFDD outperforms competitive baselines in terms of both automatic evaluation and human judgments. Besides, ablation studies indicate that the proposed CIKG has the potential to promote dialogue systems to generate fluent, informative, and diverse dialogue responses.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4320497802",
    "type": "article"
  },
  {
    "title": "SMONE: A Session-based Recommendation Model Based on Neighbor Sessions with Similar Probabilistic Intentions",
    "doi": "https://doi.org/10.1145/3587099",
    "publication_date": "2023-03-09",
    "publication_year": 2023,
    "authors": "Bohan Jia; Jian Cao; Shiyou Qian; Nengjun Zhu; Xin Dong; Liang Zhang; Lei Cheng; Linjian Mo",
    "corresponding_authors": "",
    "abstract": "A session-based recommendation system (SRS) tries to predict the next possible choice of anonymous users. In recent years, graph neural network (GNN) models have been successfully applied to SRSs and have achieved great success. Using GNN models in SRSs, each session graph is processed successively to obtain the embedding of the node (i.e, each action on an item), which is then imported into the prediction module to generate recommendation results. However, solely depending on the session graph to obtain the node embeddings is not sufficient because each session only involves a few items. Therefore, neighbor sessions have been used to extend the session graph to learn more informative node representations. In this paper, we introduce a S ession-based recommendation MO del based on N eighbor sessions with similar probabilistic int E ntions(SMONE). SMONE models the intentions behind sessions in a probabilistic way and retrieves the neighbor sessions with similar intentions. After the neighbor sessions are found, the target session and its neighbor sessions are modeled as a hypyergraph to learn the contextualized embeddings, which are combined with item embeddings through GNN to produce the final item recommendations. Experiments on real-world datasets prove the effectiveness and superiority of SMONE.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4323662805",
    "type": "article"
  },
  {
    "title": "A Novel Classification Technique based on Formal Methods",
    "doi": "https://doi.org/10.1145/3592796",
    "publication_date": "2023-04-14",
    "publication_year": 2023,
    "authors": "Gerardo Canfora; Francesco Mercaldo; Antonella Santone",
    "corresponding_authors": "",
    "abstract": "In last years, we are witnessing a growing interest in the application of supervised machine learning techniques in the most disparate fields. One winning factor of machine learning is represented by its ability to easily create models, as it does not require prior knowledge about the application domain. Complementary to machine learning are formal methods, that intrinsically offer safeness check and mechanism for reasoning on failures. Considering the weaknesses of machine learning, a new challenge could be represented by the use of formal methods. However, formal methods require the expertise of the domain, knowledge about modeling language with its semantic and mathematical rigour to specify properties. In this article, we propose a novel learning technique based on the adoption of formal methods for classification thanks to the automatic generation both of the formula and of the model. In this way the proposed method does not require any human intervention and thus it can be applied also to complex/large datasets. This leads to less effort both in using formal methods and in a better explainability and reasoning about the obtained results. Through a set of case studies from different real-world domains (i.e., driver detection, scada attack identification, arrhythmia characterization, mobile malware detection, and radiomics for lung cancer analysis), we demonstrate the usefulness of the proposed method, by showing that we are able to overcome the performances obtained from widespread classification algorithms.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4365509942",
    "type": "article"
  },
  {
    "title": "Multi-view Graph Representation Learning Beyond Homophily",
    "doi": "https://doi.org/10.1145/3592858",
    "publication_date": "2023-04-17",
    "publication_year": 2023,
    "authors": "Bei Lin; You Li; Ning Gui; Zhuopeng Xu; Zhiwu Yu",
    "corresponding_authors": "",
    "abstract": "Unsupervised graph representation learning (GRL) aims at distilling diverse graph information into task-agnostic embeddings without label supervision. Due to a lack of support from labels, recent representation learning methods usually adopt self-supervised learning, and embeddings are learned by solving a handcrafted auxiliary task (so-called pretext task). However, partially due to the irregular non-Euclidean data in graphs, the pretext tasks are generally designed under homophily assumptions and cornered in the low-frequency signals, which results in significant loss of other signals, especially high-frequency signals widespread in graphs with heterophily. Motivated by this limitation, we propose a multi-view perspective and the usage of diverse pretext tasks to capture different signals in graphs into embeddings. A novel framework, denoted as Multi-view Graph Encoder (MVGE), is proposed, and a set of key designs are identified. More specifically, a set of new pretext tasks are designed to encode different types of signals, and a straightforward operation is proposed to maintain both the commodity and personalization in both the attribute and the structural levels. Extensive experiments on synthetic and real-world network datasets show that the node representations learned with MVGE achieve significant performance improvements in three different downstream tasks, especially on graphs with heterophily.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4366087554",
    "type": "article"
  },
  {
    "title": "Accurate Open-Set Recognition for Memory Workload",
    "doi": "https://doi.org/10.1145/3597027",
    "publication_date": "2023-05-15",
    "publication_year": 2023,
    "authors": "Jun-Gi Jang; Sooyeon Shim; Vladimir Egay; Jeeyong Lee; Jong-Min Park; Suhyun Chae; U Kang",
    "corresponding_authors": "",
    "abstract": "How can we accurately identify new memory workloads while classifying known memory workloads? Verifying DRAM (Dynamic Random Access Memory) using various workloads is an important task to guarantee the quality of DRAM. A crucial component in the process is open-set recognition which aims to detect new workloads not seen in the training phase. Despite its importance, however, existing open-set recognition methods are unsatisfactory in terms of accuracy since they fail to exploit the characteristics of workload sequences. In this article, we propose Acorn , an accurate open-set recognition method capturing the characteristics of workload sequences. Acorn extracts two types of feature vectors to capture sequential patterns and spatial locality patterns in memory access. Acorn then uses the feature vectors to accurately classify a subsequence into one of the known classes or identify it as the unknown class. Experiments show that Acorn achieves state-of-the-art accuracy, giving up to 37% points higher unknown class detection accuracy while achieving comparable known class classification accuracy than existing methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4376616704",
    "type": "article"
  },
  {
    "title": "Same or Different? Diff-Vectors for Authorship Analysis",
    "doi": "https://doi.org/10.1145/3609226",
    "publication_date": "2023-07-15",
    "publication_year": 2023,
    "authors": "Silvia Corbara; Alejandro Moreo; Fabrizio Sebastiani",
    "corresponding_authors": "",
    "abstract": "In this article, we investigate the effects on authorship identification tasks (including authorship verification, closed-set authorship attribution, and closed-set and open-set same-author verification) of a fundamental shift in how to conceive the vectorial representations of documents that are given as input to a supervised learner. In “classic” authorship analysis, a feature vector represents a document, the value of a feature represents (an increasing function of) the relative frequency of the feature in the document, and the class label represents the author of the document. We instead investigate the situation in which a feature vector represents an unordered pair of documents, the value of a feature represents the absolute difference in the relative frequencies (or increasing functions thereof) of the feature in the two documents, and the class label indicates whether the two documents are from the same author or not. This latter (learner-independent) type of representation has been occasionally used before, but has never been studied systematically. We argue that it is advantageous, and that, in some cases (e.g., authorship verification), it provides a much larger quantity of information to the training process than the standard representation. The experiments that we carry out on several publicly available datasets (among which one that we here make available for the first time) show that feature vectors representing pairs of documents (that we here call Diff-Vectors ) bring about systematic improvements in the effectiveness of authorship identification tasks, and especially so when training data are scarce (as it is often the case in real-life authorship identification scenarios). Our experiments tackle same-author verification, authorship verification, and closed-set authorship attribution; while DVs are naturally geared for solving the 1st, we also provide two novel methods for solving the 2nd and 3rd that use a solver for the 1st as a building block. The code to reproduce our experiments is open-source and available online. 1",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4384407086",
    "type": "article"
  },
  {
    "title": "Adversary for Social Good: Leveraging Adversarial Attacks to Protect Personal Attribute Privacy",
    "doi": "https://doi.org/10.1145/3614098",
    "publication_date": "2023-08-07",
    "publication_year": 2023,
    "authors": "Xiaoting Li; Lingwei Chen; Dinghao Wu",
    "corresponding_authors": "",
    "abstract": "Social media has drastically reshaped the world that allows billions of people to engage in such interactive environments to conveniently create and share content with the public. Among them, text data (e.g., tweets, blogs) maintains the basic yet important social activities and generates a rich source of user-oriented information. While those explicit sensitive user data like credentials have been significantly protected by all means, personal private attribute (e.g., age, gender, location) disclosure due to inference attacks is somehow challenging to avoid, especially when powerful natural language processing (NLP) techniques have been effectively deployed to automate attribute inferences from implicit text data. This puts users’ attribute privacy at risk. To address this challenge, in this article, we leverage the inherent vulnerability of machine learning to adversarial attacks, and design a novel text-space Adv ersarial attack for S ocial G ood, called Adv4SG . In other words, we cast the problem of protecting personal attribute privacy as an adversarial attack formulation problem over the social media text data to defend against NLP-based attribute inference attacks. More specifically, Adv4SG proceeds with a sequence of word perturbations under given constraints such that the probed attribute cannot be identified correctly. Different from the prior works, we advance Adv4SG by considering social media property, and introducing cost-effective mechanisms to expedite attribute obfuscation over text data under the black-box setting. Extensive experiments on real-world social media datasets have demonstrated that our method can effectively degrade the inference accuracy with less computational cost over different attribute settings, which substantially helps mitigate the impacts of inference attacks and thus achieve high performance in user attribute privacy protection.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4385637196",
    "type": "article"
  },
  {
    "title": "Adapting Knowledge Inference Algorithms to Measure Geometry Competencies through a Puzzle Game",
    "doi": "https://doi.org/10.1145/3614436",
    "publication_date": "2023-08-11",
    "publication_year": 2023,
    "authors": "Sofia Strukova; José A. Ruipérez‐Valiente; Félix Gómez Mármol",
    "corresponding_authors": "",
    "abstract": "The rapid technological evolution of the last years has motivated students to develop capabilities that will prepare them for an unknown future in the 21st century. In this context, many teachers intend to optimise the learning process, making it more dynamic and exciting through the introduction of gamification. Thus, this article focuses on a data-driven assessment of geometry competencies, which are essential for developing problem-solving and higher-order thinking skills. Our main goal is to adapt, evaluate and compare Bayesian Knowledge Tracing (BKT), Performance Factor Analysis (PFA), Elo, and Deep Knowledge Tracing (DKT) algorithms applied to the data of a geometry game named Shadowspect, in order to predict students’ performance by means of several classifier metrics. We analysed two algorithmic configurations, with and without prioritisation of Knowledge Components (KCs) – the skills needed to complete a puzzle successfully, and we found Elo to be the algorithm with the best prediction power with the ability to model the real knowledge of students. However, the best results are achieved without KCs because it is a challenging task to differentiate between KCs effectively in game environments. Our results prove that the above-mentioned algorithms can be applied in formal education to improve teaching, learning, and organisational efficiency.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4385768469",
    "type": "article"
  },
  {
    "title": "Transfer Learning across Graph Convolutional Networks: Methods, Theory, and Applications",
    "doi": "https://doi.org/10.1145/3617376",
    "publication_date": "2023-08-23",
    "publication_year": 2023,
    "authors": "Meng Jiang",
    "corresponding_authors": "Meng Jiang",
    "abstract": "Graph neural networks have been widely used for learning representations of nodes for many downstream tasks on graph data. Existing models were designed for the nodes on a single graph, which would not be able to utilize information across multiple graphs. The real world does have multiple graphs where the nodes are often partially aligned . For examples, knowledge graphs share a number of named entities though they may have different relation schema; collaboration networks on publications and awarded projects share some researcher nodes who are authors and investigators, respectively; people use multiple web services, shopping, tweeting, rating movies, and some may register the same e-mail account across the platforms. In this article, we propose partially aligned graph convolutional networks to learn node representations across the models. We provide multiple methods such as model sharing, regularization, and alignment reconstruction, as well as theoretical analysis to positively transfer knowledge across the set of partially aligned nodes. Extensive experiments on real-world knowledge graphs, collaboration networks, and bipartite rating graphs show the superior performance of our proposed methods on relation classification, link prediction, and item recommendation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386091468",
    "type": "article"
  },
  {
    "title": "Structure-Driven Representation Learning for Deep Clustering",
    "doi": "https://doi.org/10.1145/3623400",
    "publication_date": "2023-09-08",
    "publication_year": 2023,
    "authors": "Xiang Wang; Liping Jing; Huafeng Liu; Jian Yu",
    "corresponding_authors": "",
    "abstract": "As an important branch of unsupervised learning methods, clustering makes a wide contribution in the area of data mining. It is well known that capturing the group-discriminative properties of each sample for clustering is crucial. Among them, deep clustering delivers promising results due to the strong representational power of neural networks. However, most of them adopt sample-level learning strategies, and the standalone data point barely captures its holistic cluster’s context and may undergo sub-optimal cluster assignment. To tackle this issue, we propose a Structure-driven Representation Learning (SRL) method by introducing latent structure information into the representation learning process at both the local and global levels. Specifically, a local-structure-driven sample representation strategy is proposed to approximate the estimation of data distribution, which models the neighborhood distribution of samples with potential structure information and exploits statistical dependencies between them to improve cluster consistency. A global-structure-driven cluster representation strategy is designed, where the context of each cluster is sufficiently encoded according to its samples (exemplar-theory) and corresponding prototype (prototype-theory). In this case, each cluster can only be related to its most similar samples, and different clusters are separated as much as possible. These two models are seamlessly combined into a joint optimization problem, which can be efficiently solved. Experiments on six widely-used datasets demonstrate the superiority of SRL over state-of-the-art clustering methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386546915",
    "type": "article"
  },
  {
    "title": "Modeling Interference for Individual Treatment Effect Estimation from Networked Observational Data",
    "doi": "https://doi.org/10.1145/3628449",
    "publication_date": "2023-10-18",
    "publication_year": 2023,
    "authors": "Qiang Huang; Jing Ma; Jundong Li; Ruocheng Guo; Huiyan Sun; Yi Chang",
    "corresponding_authors": "",
    "abstract": "Estimating individual treatment effect (ITE) from observational data has attracted great interest in recent years, which plays a crucial role in decision-making across many high-impact domains such as economics, medicine, and e-commerce. Most existing studies of ITE estimation assume that different units at play are independent and do not influence each other. However, many social science experiments have shown that there often exist different levels of interactions between units in observational data, especially in a networked environment. As a result, the treatment assignment of one unit can affect the outcome of other units connected to it in the network, which is referred to as the interference or spillover effect . In this article, we study an important problem of ITE estimation from networked observational data by modeling the interference between different units and provide a principled framework to support such study. Methodologically, we propose a novel framework, SPNet , that first captures the influence of hidden confounders with the aid of graph convolutional network and then models the interference by introducing an environment summary variable and developing a masked attention mechanism. Experimental evaluations on several semi-synthetic datasets based on real-world networks corroborate the superiority of our proposed framework over state-of-the-art individual treatment effect estimation methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4387735123",
    "type": "article"
  },
  {
    "title": "Totally-ordered Sequential Rules for Utility Maximization",
    "doi": "https://doi.org/10.1145/3628450",
    "publication_date": "2023-10-23",
    "publication_year": 2023,
    "authors": "Chunkai Zhang; Maohua Lyu; Wensheng Gan; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "High-utility sequential pattern mining (HUSPM) is a significant and valuable activity in knowledge discovery and data analytics with many real-world applications. In some cases, HUSPM can not provide an excellent measure to predict what will happen. High-utility sequential rule mining (HUSRM) discovers high utility and high confidence sequential rules, so it can solve the issue in HUSPM. However, all existing HUSRM algorithms aim to find high-utility partially-ordered sequential rules (HUSRs), which are not consistent with reality and may generate fake HUSRs. Therefore, in this article, we formulate the problem of high-utility totally-ordered sequential rule mining and propose a novel algorithm, called TotalSR, which aims to identify all high-utility totally-ordered sequential rules (HTSRs). TotalSR introduces a left-first expansion strategy that can utilize the anti-monotonic property to use a confidence pruning strategy. TotalSR also designs a new utility upper bound: RSPEU , which is tighter than the existing upper bounds. TotalSR can drastically reduce the search space with the help of utility upper bounds pruning strategies, avoiding much more meaningless computation. To effectively compute the information, TotalSR proposes an auxiliary antecedent record table that can efficiently calculate the antecedent’s support and a utility prefix sum list that can compute the upper bound in O (1) time for a sequence. Finally, there are numerous experimental results on both real and synthetic datasets demonstrating that TotalSR is more efficient than the existing algorithms.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4387868952",
    "type": "article"
  },
  {
    "title": "Robust Graph Meta-Learning for Weakly Supervised Few-Shot Node Classification",
    "doi": "https://doi.org/10.1145/3630260",
    "publication_date": "2023-11-10",
    "publication_year": 2023,
    "authors": "Kaize Ding; Jianling Wang; Jundong Li; James Caverlee; Huan Liu",
    "corresponding_authors": "",
    "abstract": "Graph machine learning (Graph ML) models typically require abundant labeled instances to provide sufficient supervision signals, which is commonly infeasible in real-world scenarios since labeled data for newly emerged concepts (e.g., new categorizations of nodes) on graphs is rather limited. To efficiently learn with a small amount of data on graphs, meta-learning has been investigated in Graph ML. By transferring the knowledge learned from previous experiences to new tasks, graph meta-learning approaches have demonstrated promising performance on few-shot graph learning problems. However, most existing efforts predominately assume that all the data from the seen classes is gold labeled, yet those methods may lose their efficacy when the seen data is weakly labeled with severe label noise. As such, we aim to investigate a novel problem of weakly supervised graph meta-learning for improving the model robustness in terms of knowledge transfer. To achieve this goal, we propose Meta-GIN (Meta Graph Interpolation Network), a new graph meta-learning framework. Based on a new robustness-enhanced episodic training paradigm, Meta-GIN is meta-learned to interpolate node representations from weakly labeled data and extracts highly transferable meta-knowledge, which enables the model to quickly adapt to unseen tasks with few labeled instances. Extensive experiments demonstrate the superiority of Meta-GIN over existing graph meta-learning studies on the task of weakly supervised few-shot node classification.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4388574703",
    "type": "article"
  },
  {
    "title": "Privacy-Preserving Non-Negative Matrix Factorization with Outliers",
    "doi": "https://doi.org/10.1145/3632961",
    "publication_date": "2023-11-16",
    "publication_year": 2023,
    "authors": "Swapnil Saha; Hafiz Imtiaz",
    "corresponding_authors": "",
    "abstract": "Non-negative matrix factorization is a popular unsupervised machine learning algorithm for extracting meaningful features from inherently non-negative data. Such data often contain privacy-sensitive user information. Additionally, the dataset can contain outliers, which may lead to extracting sub-optimal features from the data. It is, therefore, necessary to address these two issues while analyzing privacy-sensitive data that may contain outliers. In this work, we develop a non-negative matrix factorization algorithm in the privacy-preserving framework that (i) considers the presence of outliers in the data, and (ii) can achieve results comparable to those of the non-private algorithm. We design our method in such a way that one has the control to select the degree of privacy grantee based on the required utility gap. We show the effectiveness of our proposed algorithm’s performance on six real and diverse datasets. The experimental results show that our proposed method can achieve a performance that closely approximates the performance of the non-private algorithm under some parameter choices, while ensuring strict privacy guarantees.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4388731371",
    "type": "article"
  },
  {
    "title": "Swarm Self-supervised Hypergraph Embedding for Recommendation",
    "doi": "https://doi.org/10.1145/3638058",
    "publication_date": "2023-12-20",
    "publication_year": 2023,
    "authors": "Meng Jian; Yulong Bai; Jingjing Guo; Lifang Wu",
    "corresponding_authors": "",
    "abstract": "The information era brings both opportunities and challenges to information services. Confronting information overload, recommendation technology is dedicated to filtering personalized content to meet users’ requirements. The extremely sparse interaction records and their imbalanced distribution become a big obstacle to building a high-quality recommendation model. In this article, we propose a swarm self-supervised hypergraph embedding (SHE) model to predict users’ interests by hypergraph convolution and self-supervised discrimination. SHE builds a hypergraph with multiple interest clues to alleviate the interaction sparsity issue and performs interest propagation to embed CF signals in hybrid learning on the hypergraph. It follows an auxiliary local view by similar hypergraph construction and interest propagation to restrain unnecessary propagation between user swarms. Besides, interest contrast further inserts self-discrimination to deal with long-tail bias issue and enhance interest modeling, which aid recommendation by a multi-task learning optimization. Experiments on public datasets show that the proposed SHE outperforms the state-of-the-art models demonstrating the effectiveness of hypergraph-based interest propagation and swarm-aware interest contrast to enhance embedding for recommendation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390003009",
    "type": "article"
  },
  {
    "title": "PU-Detector: A PU Learning-based Framework for Real Money Trading Detection in MMORPG",
    "doi": "https://doi.org/10.1145/3638561",
    "publication_date": "2023-12-29",
    "publication_year": 2023,
    "authors": "Yilin Wang; Sha Zhao; Shiwei Zhao; Runze Wu; Yuhong Xu; Jianrong Tao; Tangjie Lv; Shijian Li; Zhipeng Hu; Gang Pan",
    "corresponding_authors": "",
    "abstract": "Massive multiplayer online role-playing games (MMORPG) have been becoming one of the most popular and exciting online games. In recent years, a cheating phenomenon called real money trading (RMT) has arisen and damaged the fantasy world in many ways. RMT is the sale of in-game items, currency, or even characters to earn real money, breaking the balance of the game economy ecosystem and damaging the game experience. Therefore, some studies have emerged to address the problem of RMT detection. However, they cannot well handle the label uncertainty problem in practice, where there are only labeled RMT samples (positive samples) and unlabeled samples, which could either be RMT samples or normal transactions (negative samples). Meanwhile, the trading relationship between RMTers is modeled in a simple way, leading to some normal transactions being falsely classified as RMT. In this article, we propose PU-Detector, a novel framework based on PU learning (learning from positive and unlabeled data) for RMT detection, considering the fact that there are only labeled RMT samples and other unlabeled transactions. We first automatically estimate the likelihood of one transaction being RMT by developing an improved PU learning method and proposing an assessment rule. Sequentially, we use the estimated likelihood as edge weight to construct a trading graph to learn trader representation. Then, with the trader representations and basic trading features, we detect RMT samples by the improved PU learning method. PU-Detector is evaluated on a large-scale real world dataset consisting of 33,809,956 transaction logs generated by 43,217 unique players. Compared with other approaches, it achieves the state-of-the-art performance and demonstrates its advantages in detecting underlying RMT samples.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390393161",
    "type": "article"
  },
  {
    "title": "A Model-Agnostic Framework for Fast Spatial Anomaly Detection",
    "doi": "https://doi.org/10.1145/1857947.1857952",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Mingxi Wu; Chris Jermaine; Sanjay Ranka; Xiuyao Song; John G. Gums",
    "corresponding_authors": "",
    "abstract": "Given a spatial dataset placed on an n × n grid, our goal is to find the rectangular regions within which subsets of the dataset exhibit anomalous behavior. We develop algorithms that, given any user-supplied arbitrary likelihood function, conduct a likelihood ratio hypothesis test (LRT) over each rectangular region in the grid, rank all of the rectangles based on the computed LRT statistics, and return the top few most interesting rectangles. To speed this process, we develop methods to prune rectangles without computing their associated LRT statistics.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2012890979",
    "type": "article"
  },
  {
    "title": "Enhancing Clustering Quality through Landmark-Based Dimensionality Reduction",
    "doi": "https://doi.org/10.1145/1921632.1921637",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Panagis Magdalinos; Christos Doulkeridis; Michalis Vazirgiannis",
    "corresponding_authors": "",
    "abstract": "Scaling up data mining algorithms for data of both high dimensionality and cardinality has been lately recognized as one of the most challenging problems in data mining research. The reason is that typical data mining tasks, such as clustering, cannot produce high quality results when applied on high-dimensional and/or large (in terms of cardinality) datasets. Data preprocessing and in particular dimensionality reduction constitute promising tools to deal with this problem. However, most of the existing dimensionality reduction algorithms share also the same disadvantages with data mining algorithms, when applied on large datasets of high dimensionality. In this article, we propose a fast and efficient dimensionality reduction algorithm (FEDRA), which is particularly scalable and therefore suitable for challenging datasets. FEDRA follows the landmark-based paradigm for embedding data objects in a low-dimensional projection space. By means of a theoretical analysis, we prove that FEDRA is efficient, while we demonstrate the achieved quality of results through experiments on datasets of higher cardinality and dimensionality than those employed in the evaluation of competitive algorithms. The obtained results prove that FEDRA manages to retain or ameliorate clustering quality while projecting in less than 10% of the initial dimensionality. Moreover, our algorithm produces embeddings that enable the faster convergence of clustering algorithms. Therefore, FEDRA emerges as a powerful and generic tool for data pre-processing, which can be integrated in other data mining algorithms, thus enhancing their performance.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2044728660",
    "type": "article"
  },
  {
    "title": "ciForager",
    "doi": "https://doi.org/10.1145/2362383.2362385",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Jeffrey Chan; James Bailey; Christopher Leckie; Michael E. Houle",
    "corresponding_authors": "",
    "abstract": "Data mining techniques for understanding how graphs evolve over time have become increasingly important. Evolving graphs arise naturally in diverse applications such as computer network topologies, multiplayer games and medical imaging. A natural and interesting problem in evolving graph analysis is the discovery of compact subgraphs that change in a similar manner. Such subgraphs are known as regions of correlated change and they can both summarise change patterns in graphs and help identify the underlying events causing these changes. However, previous techniques for discovering regions of correlated change suffer from limited scalability, making them unsuitable for analysing the evolution of very large graphs. In this paper, we introduce a new algorithm called ciForager, that addresses this scalability challenge and offers considerable improvements. The efficiency of ciForager is based on the use of new incremental techniques for detecting change, as well as the use of Voronoi representations for efficiently determining distance. We experimentally show that ciForager can achieve speedups of up to 1000 times over previous approaches. As a result, it becomes feasible for the first time to discover regions of correlated change in extremely large graphs, such as the entire BGP routing topology of the Internet.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2065289816",
    "type": "article"
  },
  {
    "title": "Profit Maximization with Sufficient Customer Satisfactions",
    "doi": "https://doi.org/10.1145/3110216",
    "publication_date": "2018-01-10",
    "publication_year": 2018,
    "authors": "Cheng Long; Raymond Chi-Wing Wong; Victor Junqiu Wei",
    "corresponding_authors": "",
    "abstract": "In many commercial campaigns, we observe that there exists a tradeoff between the number of customers satisfied by the company and the profit gained. Merely satisfying as many customers as possible or maximizing the profit is not desirable. To this end, in this article, we propose a new problem called k - &lt;underline&gt;S&lt;/underline&gt;atisfiability &lt;underline&gt;A&lt;/underline&gt;ssignment for &lt;underline&gt;M&lt;/underline&gt;aximizing the &lt;underline&gt;P&lt;/underline&gt;rofit ( k -SAMP), where k is a user parameter and a non-negative integer. Given a set P of products and a set O of customers, k -SAMP is to find an assignment between P and O such that at least k customers are satisfied in the assignment and the profit incurred by this assignment is maximized. Although we find that this problem is closely related to two classic computer science problems, namely maximum weight matching and maximum matching, the techniques developed for these classic problems cannot be adapted to our k -SAMP problem. In this work, we design a novel algorithm called Adjust for the k -SAMP problem. Given an assignment A , Adjust iteratively increases the profit of A by adjusting some appropriate matches in A while keeping at least k customers satisfied in A . We prove that Adjust returns a global optimum. Extensive experiments were conducted that verified the efficiency of Adjust .",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2627064876",
    "type": "article"
  },
  {
    "title": "Cross-Dependency Inference in Multi-Layered Networks",
    "doi": "https://doi.org/10.1145/3056562",
    "publication_date": "2017-06-29",
    "publication_year": 2017,
    "authors": "Chen Chen; Hanghang Tong; Lei Xie; Lei Ying; Qing He",
    "corresponding_authors": "",
    "abstract": "The increasingly connected world has catalyzed the fusion of networks from different domains, which facilitates the emergence of a new network model-multi-layered networks. Examples of such kind of network systems include critical infrastructure networks, biological systems, organization-level collaborations, cross-platform e-commerce, and so forth. One crucial structure that distances multi-layered network from other network models is its cross-layer dependency, which describes the associations between the nodes from different layers. Needless to say, the cross-layer dependency in the network plays an essential role in many data mining applications like system robustness analysis and complex network control. However, it remains a daunting task to know the exact dependency relationships due to noise, limited accessibility, and so forth. In this article, we tackle the cross-layer dependency inference problem by modeling it as a collective collaborative filtering problem. Based on this idea, we propose an effective algorithm Fascinate that can reveal unobserved dependencies with linear complexity. Moreover, we derive Fascinate-ZERO, an online variant of Fascinate that can respond to a newly added node timely by checking its neighborhood dependencies. We perform extensive evaluations on real datasets to substantiate the superiority of our proposed approaches.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2733883865",
    "type": "article"
  },
  {
    "title": "C <scp>ommunity</scp> D <scp>iff</scp>",
    "doi": "https://doi.org/10.1145/3047009",
    "publication_date": "2018-01-10",
    "publication_year": 2018,
    "authors": "Srayan Datta; Eytan Adar",
    "corresponding_authors": "",
    "abstract": "Community detection is an oft-used analytical function of network analysis but can be a black art to apply in practice. Grouping of related nodes is important for identifying patterns in network datasets but also notoriously sensitive to input data and algorithm selection. This is further complicated by the fact that, depending on domain and use case, the ground truth knowledge of the end-user can vary from none to complete. In this work, we present C ommunity D iff , an interactive visualization system that combines visualization and active learning (AL) to support the end-user’s analytical process. As the end-user interacts with the system, a continuous refinement process updates both the community labels and visualizations. C ommunity D iff features a mechanism for visualizing ensemble spaces , weighted combinations of algorithm output, that can identify patterns, commonalities, and differences among multiple community detection algorithms. Among other features, C ommunity D iff introduces an AL mechanism that visually indicates uncertainty about community labels to focus end-user attention and supporting end-user control that ranges from explicitly indicating the number of expected communities to merging and splitting communities. Based on this end-user input, C ommunity D iff dynamically recalculates communities. We demonstrate the viability of our through a study of speed of end-user convergence on satisfactory community labels. As part of building C ommunity D iff , we describe a design process that can be adapted to other Interactive Machine Learning applications.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2782827812",
    "type": "article"
  },
  {
    "title": "D <scp>igger</scp>",
    "doi": "https://doi.org/10.1145/3267106",
    "publication_date": "2018-12-19",
    "publication_year": 2018,
    "authors": "Xiaoming Liu; Chao Shen; Xiaohong Guan; Yadong Zhou",
    "corresponding_authors": "",
    "abstract": "People participate in multiple online social networks, e.g., Facebook, Twitter, and Linkedin, and these social networks with heterogeneous social content and user relationship are named as heterogeneous social networks. Group structure widely exists in heterogeneous social networks, which reveals the evolution of human cooperation. Detecting similar groups in heterogeneous networks has a great significance for many applications, such as recommendation system and spammer detection, using the wealth of group information. Although promising, this novel problem encounters a variety of technical challenges, including incomplete data, high time complexity, and ground truth. To address the research gap and technical challenges, we take advantage of a ratio-cut optimization function to model this novel problem by the linear mixed-effects method and graph spectral theory. Based on this model, we propose an efficient algorithm called D igger to detect the similar groups in the large graphs. D igger consists of three steps, including measuring user similarity, construct a matching graph, and detecting similar groups. We adopt several strategies to lower the computational cost and detail the basis of labeling the ground truth. We evaluate the effectiveness and efficiency of our algorithm on five different types of online social networks. The extensive experiments show that our method achieves 0.693, 0.783, and 0.735 in precision, recall, and F1-measure, which significantly surpass the state-of-arts by 24.4%, 15.3%, and 20.7%, respectively. The results demonstrate that our proposal can detect similar groups in heterogeneous networks effectively.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2906458786",
    "type": "article"
  },
  {
    "title": "A Distance Measure for the Analysis of Polar Opinion Dynamics in Social Networks",
    "doi": "https://doi.org/10.1145/3332168",
    "publication_date": "2019-08-08",
    "publication_year": 2019,
    "authors": "Victor Amelkin; Petko Bogdanov; Ambuj K. Singh",
    "corresponding_authors": "",
    "abstract": "Analysis of opinion dynamics in social networks plays an important role in today’s life. For predicting users’ political preference, it is particularly important to be able to analyze the dynamics of competing polar opinions, such as pro-Democrat vs. pro-Republican. While observing the evolution of polar opinions in a social network over time, can we tell when the network evolved abnormally? Furthermore, can we predict how the opinions of the users will change in the future? To answer such questions, it is insufficient to study individual user behavior, since opinions can spread beyond users’ ego-networks. Instead, we need to consider the opinion dynamics of all users simultaneously and capture the connection between the individuals’ behavior and the global evolution pattern of the social network. In this work, we introduce the Social Network Distance (SND)—a distance measure that quantifies the likelihood of evolution of one snapshot of a social network into another snapshot under a chosen model of polar opinion dynamics. SND has a rich semantics of a transportation problem, yet, is computable in time linear in the number of users and, as such, is applicable to large-scale online social networks. In our experiments with synthetic and Twitter data, we demonstrate the utility of our distance measure for anomalous event detection. It achieves a true positive rate of 0.83, twice as high as that of alternatives. The same predictions presented in precision-recall space show that SND retains perfect precision for recall up to 0.2. Its precision then decreases while maintaining more than 2-fold improvement over alternatives for recall up to 0.95. When used for opinion prediction in Twitter data, SND’s accuracy is 75.6%, which is 7.5% higher than that of the next best method.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2968871805",
    "type": "article"
  },
  {
    "title": "CT LIS",
    "doi": "https://doi.org/10.1145/3363570",
    "publication_date": "2019-11-11",
    "publication_year": 2019,
    "authors": "Shenghua Liu; Huawei Shen; Houdong Zheng; Xueqi Cheng; Xiangwen Liao",
    "corresponding_authors": "",
    "abstract": "How to quantify influences between users, seeing that social network users influence each other in their temporal behaviors? Previous work has directly defined an independent model parameter to capture the interpersonal influence between each pair of users. To do so, these models need a parameter for each pair of users, which results in high-dimensional models becoming easily trapped into the overfitting problem. However, such models do not consider how influences depend on each other if influences are sent from the same user or if influences are received by the same user. Therefore, we propose a model that defines parameters for every user with a latent influence vector and a susceptibility vector, opposite to define influences on user pairs. Such low-dimensional representations naturally cause the interpersonal influences involving the same user to be coupled with each other, thus reducing the model’s complexity. Additionally, the model can easily consider the temporal information and sentimental polarities of users’ messages. Finally, we conduct extensive experiments on two real-world Microblog datasets, showing that our model with such representations achieves best performance on three prediction tasks, compared to the state-of-the-art and pair-wise baselines.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2986897364",
    "type": "article"
  },
  {
    "title": "AR <sup>2</sup> Net",
    "doi": "https://doi.org/10.1145/3372406",
    "publication_date": "2020-02-09",
    "publication_year": 2020,
    "authors": "Yanan Xu; Yanyan Shen; Yanmin Zhu; Jiadi Yu",
    "corresponding_authors": "",
    "abstract": "Business location selection is crucial to the success of businesses. Traditional approaches like manual survey investigate multiple factors, such as foot traffic, neighborhood structure, and available workforce, which are typically hard to measure. In this article, we propose to explore both satellite data (e.g., satellite images and nighttime light data) and urban data for business location selection tasks of various businesses. We extract discriminative features from the two kinds of data and perform empirical analysis to evaluate the correlation between extracted features and the business popularity of locations. A novel neural network approach named R 2 Net is proposed to learn deep interactions among features and predict the business popularity of locations. The proposed approach is trained with a regression-and-ranking combined loss function to preserve accurate popularity estimation and the ranking order of locations simultaneously. To support the location selection for multiple businesses, we propose an approach named AR 2 Net with three attention modules, which enable the approach to focus on different latent features according to business types. Comprehensive experiments on a real-world dataset demonstrate that the satellite features are effective and our models outperform the state-of-the-art methods in terms of four metrics.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3005835085",
    "type": "article"
  },
  {
    "title": "Generalizing Long Short-Term Memory Network for Deep Learning from Generic Data",
    "doi": "https://doi.org/10.1145/3366022",
    "publication_date": "2020-02-09",
    "publication_year": 2020,
    "authors": "Huimei Han; Xingquan Zhu; Ying Li",
    "corresponding_authors": "",
    "abstract": "Long Short-Term Memory (LSTM) network, a popular deep-learning model, is particularly useful for data with temporal correlation, such as texts, sequences, or time series data, thanks to its well-sought after recurrent network structures designed to capture temporal correlation. In this article, we propose to generalize LSTM to generic machine-learning tasks where data used for training do not have explicit temporal or sequential correlation. Our theme is to explore feature correlation in the original data and convert each instance into a synthetic sentence format by using a two-gram probabilistic language model. More specifically, for each instance represented in the original feature space, our conversion first seeks to horizontally align original features into a sequentially correlated feature vector, resembling to the letter coherence within a word. In addition, a vertical alignment is also carried out to create multiple time points and simulate word sequential order in a sentence ( i.e., word correlation). The two dimensional horizontal-and-vertical alignments not only ensure feature correlations are maximally utilized, but also preserve the original feature values in the new representation. As a result, LSTM model can be utilized to achieve good classification accuracy, even if the underlying data do not have temporal or sequential dependency. Experiments on 20 generic datasets show that applying LSTM to generic data can improve the classification accuracy, compared to conventional machine-learning methods. This research opens a new opportunity for LSTM deep learning to be broadly applied to generic machine-learning tasks.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3006225386",
    "type": "article"
  },
  {
    "title": "A General Coreset-Based Approach to Diversity Maximization under Matroid Constraints",
    "doi": "https://doi.org/10.1145/3402448",
    "publication_date": "2020-08-05",
    "publication_year": 2020,
    "authors": "Matteo Ceccarello; Andrea Pietracaprina; Geppino Pucci",
    "corresponding_authors": "",
    "abstract": "Diversity maximization is a fundamental problem in web search and data mining. For a given dataset S of n elements, the problem requires to determine a subset of S containing k ≪ n “representatives” which maximize some diversity function expressed in terms of pairwise distances, where distance models dissimilarity. An important variant of the problem prescribes that the solution satisfy an additional orthogonal requirement, which can be specified as a matroid constraint (i.e., a feasible solution must be an independent set of size k of a given matroid). While unconstrained diversity maximization admits efficient coreset-based strategies for several diversity functions, known approaches dealing with the additional matroid constraint apply only to one diversity function (sum of distances), and are based on an expensive, inherently sequential, local search over the entire input dataset. We devise the first coreset-based algorithms for diversity maximization under matroid constraints for various diversity functions, together with efficient sequential, MapReduce, and Streaming implementations. Technically, our algorithms rely on the construction of a small coreset, that is, a subset of S containing a feasible solution which is no more than a factor 1−ɛ away from the optimal solution for S . While our algorithms are fully general, for the partition and transversal matroids, if ɛ is a constant in (0,1) and S has bounded doubling dimension, the coreset size is independent of n and it is small enough to afford the execution of a slow sequential algorithm to extract a final, accurate, solution in reasonable time. Extensive experiments show that our algorithms are accurate, fast, and scalable, and therefore they are capable of dealing with the large input instances typical of the big data scenario.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3047043265",
    "type": "article"
  },
  {
    "title": "A Unified Framework for Sparse Online Learning",
    "doi": "https://doi.org/10.1145/3361559",
    "publication_date": "2020-08-17",
    "publication_year": 2020,
    "authors": "Peilin Zhao; D. Wang; Pengcheng Wu; Steven C. H. Hoi",
    "corresponding_authors": "",
    "abstract": "The amount of data in our society has been exploding in the era of big data. This article aims to address several open challenges in big data stream classification. Many existing studies in data mining literature follow the batch learning setting, which suffers from low efficiency and poor scalability. To tackle these challenges, we investigate a unified online learning framework for the big data stream classification task. Different from the existing online data stream classification techniques, we propose a unified Sparse Online Classification (SOC) framework. Based on SOC, we derive a second-order online learning algorithm and a cost-sensitive sparse online learning algorithm, which could successfully handle online anomaly detection tasks with the extremely unbalanced class distribution. As the performance evaluation, we analyze the theoretical bounds of the proposed algorithms and conduct an extensive set of experiments. The encouraging experimental results demonstrate the efficacy of the proposed algorithms over the state-of-the-art techniques on multiple data stream classification tasks.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3093978682",
    "type": "article"
  },
  {
    "title": "Combinatorial Algorithms for String Sanitization",
    "doi": "https://doi.org/10.1145/3418683",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Giulia Bernardini; Huiping Chen; Alessio Conte; Roberto Grossi; Grigorios Loukides; Nadia Pisanti; Solon P. Pissis; Giovanna Rosone; Michelle Sweering",
    "corresponding_authors": "",
    "abstract": "String data are often disseminated to support applications such as location-based service provision or DNA sequence analysis. This dissemination, however, may expose sensitive patterns that model confidential knowledge (e.g., trips to mental health clinics from a string representing a user’s location history). In this article, we consider the problem of sanitizing a string by concealing the occurrences of sensitive patterns, while maintaining data utility, in two settings that are relevant to many common string processing tasks. In the first setting, we aim to generate the minimal-length string that preserves the order of appearance and frequency of all non-sensitive patterns. Such a string allows accurately performing tasks based on the sequential nature and pattern frequencies of the string. To construct such a string, we propose a time-optimal algorithm, TFS-ALGO. We also propose another time-optimal algorithm, PFS-ALGO, which preserves a partial order of appearance of non-sensitive patterns but produces a much shorter string that can be analyzed more efficiently. The strings produced by either of these algorithms are constructed by concatenating non-sensitive parts of the input string. However, it is possible to detect the sensitive patterns by “reversing” the concatenation operations. In response, we propose a heuristic, MCSR-ALGO, which replaces letters in the strings output by the algorithms with carefully selected letters, so that sensitive patterns are not reinstated, implausible patterns are not introduced, and occurrences of spurious patterns are prevented. In the second setting, we aim to generate a string that is at minimal edit distance from the original string, in addition to preserving the order of appearance and frequency of all non-sensitive patterns. To construct such a string, we propose an algorithm, ETFS-ALGO, based on solving specific instances of approximate regular expression matching. We implemented our sanitization approach that applies TFS-ALGO, PFS-ALGO, and then MCSR-ALGO, and experimentally show that it is effective and efficient. We also show that TFS-ALGO is nearly as effective at minimizing the edit distance as ETFS-ALGO, while being substantially more efficient than ETFS-ALGO.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3110665143",
    "type": "article"
  },
  {
    "title": "An Exponential Factorization Machine with Percentage Error Minimization to Retail Sales Forecasting",
    "doi": "https://doi.org/10.1145/3426238",
    "publication_date": "2021-01-04",
    "publication_year": 2021,
    "authors": "Chongshou Li; Brenda Cheang; Zhixing Luo; Andrew Lim",
    "corresponding_authors": "",
    "abstract": "This article proposes a new approach to sales forecasting for new products (stock-keeping units [SKUs]) with long lead time but short product life cycle. These SKUs are usually sold for one season only, without any replenishments. An exponential factorization machine (EFM) sales forecast model is developed to solve this problem which not only takes into account SKU attributes, but also pairwise interactions. The EFM model is significantly different from the original Factorization Machines (FM) from two fold: (1) the attribute-level formulation for explanatory/input variables; and (2) exponential formulation for the positive response/output/target variable. The attribute-level formation excludes infeasible intra-attribute interactions and results in more efficient feature engineering comparing with the conventional one-hot encoding, while the exponential formulation is demonstrated more effective than the log-transformation for the positive but not skewed distributed responses. In order to estimate the parameters, percentage error squares (PES) and error squares (ES) are minimized by a proposed adaptive batch gradient descent method over the training set. To overcome the over-fitting problem, a greedy forward stepwise feature selection method is proposed to select the most useful attributes and interactions. Real-world data provided by a footwear retailer in Singapore are used for testing the proposed approach. The forecasting performance in terms of both mean absolute percentage error (MAPE) and mean absolute error (MAE) compares favorably with not only off-the-shelf models but also results reported by extant sales and demand forecasting studies. The effectiveness of the proposed approach is also demonstrated by two external public datasets. Moreover, we prove the theoretical relationships between PES and ES minimization, and present an important property of the PES minimization for regression models; that it trains models to underestimate data. This property fits the situation of sales forecasting where unit-holding cost is much greater than the unit-shortage cost (e.g., perishable products).",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3119932600",
    "type": "article"
  },
  {
    "title": "Exploring Deep Reinforcement Learning for Task Dispatching in Autonomous On-Demand Services",
    "doi": "https://doi.org/10.1145/3442343",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Lei Yang; Xi Yu; Jiannong Cao; Xuxun Liu; Pan Zhou",
    "corresponding_authors": "",
    "abstract": "Autonomous on-demand services, such as GOGOX (formerly GoGoVan) in Hong Kong, provide a platform for users to request services and for suppliers to meet such demands. In such a platform, the suppliers have autonomy to accept or reject the demands to be dispatched to him/her, so it is challenging to make an online matching between demands and suppliers. Existing methods use round-based approaches to dispatch demands. In these works, the dispatching decision is based on the predicted response patterns of suppliers to demands in the current round, but they all fail to consider the impact of future demands and suppliers on the current dispatching decision. This could lead to taking a suboptimal dispatching decision from the future perspective. To solve this problem, we propose a novel demand dispatching model using deep reinforcement learning. In this model, we make each demand as an agent. The action of each agent, i.e., the dispatching decision of each demand, is determined by a centralized algorithm in a coordinated way. The model works in the following two steps. (1) It learns the demand’s expected value in each spatiotemporal state using historical transition data. (2) Based on the learned values, it conducts a Many-To-Many dispatching using a combinatorial optimization algorithm by considering both immediate rewards and expected values of demands in the next round. In order to get a higher total reward, the demands with a high expected value (short response time) in the future may be delayed to the next round. On the contrary, the demands with a low expected value (long response time) in the future would be dispatched immediately. Through extensive experiments using real-world datasets, we show that the proposed model outperforms the existing models in terms of Cancellation Rate and Average Response Time.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3152848684",
    "type": "article"
  },
  {
    "title": "Knowledge Transfer with Weighted Adversarial Network for Cold-Start Store Site Recommendation",
    "doi": "https://doi.org/10.1145/3442203",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Yan Liu; Bin Guo; Daqing Zhang; Djamal Zeghlache; Jingmin Chen; Ke Hu; Sizhe Zhang; Dan Zhou; Zhiwen Yu",
    "corresponding_authors": "",
    "abstract": "Store site recommendation aims to predict the value of the store at candidate locations and then recommend the optimal location to the company for placing a new brick-and-mortar store. Most existing studies focus on learning machine learning or deep learning models based on large-scale training data of existing chain stores in the same city. However, the expansion of chain enterprises in new cities suffers from data scarcity issues, and these models do not work in the new city where no chain store has been placed (i.e., cold-start problem). In this article, we propose a unified approach for cold-start store site recommendation, Weighted Adversarial Network with Transferability weighting scheme (WANT), to transfer knowledge learned from a data-rich source city to a target city with no labeled data. In particular, to promote positive transfer, we develop a discriminator to diminish distribution discrepancy between source city and target city with different data distributions, which plays the minimax game with the feature extractor to learn transferable representations across cities by adversarial learning. In addition, to further reduce the risk of negative transfer, we design a transferability weighting scheme to quantify the transferability of examples in source city and reweight the contribution of relevant source examples to transfer useful knowledge. We validate WANT using a real-world dataset, and experimental results demonstrate the effectiveness of our proposed model over several state-of-the-art baseline models.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3154524254",
    "type": "article"
  },
  {
    "title": "Incremental Community Detection on Large Complex Attributed Network",
    "doi": "https://doi.org/10.1145/3451216",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Zhe Chen; Aixin Sun; Xiaokui Xiao",
    "corresponding_authors": "",
    "abstract": "Community detection on network data is a fundamental task, and has many applications in industry. Network data in industry can be very large, with incomplete and complex attributes, and more importantly, growing. This calls for a community detection technique that is able to handle both attribute and topological information on large scale networks, and also is incremental. In this article, we propose inc-AGGMMR, an incremental community detection framework that is able to effectively address the challenges that come from scalability, mixed attributes, incomplete values, and evolving of the network. Through construction of augmented graph, we map attributes into the network by introducing attribute centers and belongingness edges. The communities are then detected by modularity maximization. During this process, we adjust the weights of belongingness edges to balance the contribution between attribute and topological information to the detection of communities. The weight adjustment mechanism enables incremental updates of community membership of all vertices. We evaluate inc-AGGMMR on five benchmark datasets against eight strong baselines. We also provide a case study to incrementally detect communities on a PayPal payment network which contains users with transactions. The results demonstrate inc-AGGMMR’s effectiveness and practicability.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3160097043",
    "type": "article"
  },
  {
    "title": "Faster Motif Counting via Succinct Color Coding and Adaptive Sampling",
    "doi": "https://doi.org/10.1145/3447397",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Marco Bressan; Stefano Leucci; Alessandro Panconesi",
    "corresponding_authors": "",
    "abstract": "We address the problem of computing the distribution of induced connected subgraphs, aka \\emph{graphlets} or \\emph{motifs}, in large graphs. The current state-of-the-art algorithms estimate the motif counts via uniform sampling, by leveraging the color coding technique by Alon, Yuster and Zwick. In this work we extend the applicability of this approach, by introducing a set of algorithmic optimizations and techniques that reduce the running time and space usage of color coding and improve the accuracy of the counts. To this end, we first show how to optimize color coding to efficiently build a compact table of a representative subsample of all graphlets in the input graph. For $8$-node motifs, we can build such a table in one hour for a graph with $65$M nodes and $1.8$B edges, which is $2000$ times larger than the state of the art. We then introduce a novel adaptive sampling scheme that breaks the \"additive error barrier\" of uniform sampling, guaranteeing multiplicative approximations instead of just additive ones. This allows us to count not only the most frequent motifs, but also extremely rare ones. For instance, on one graph we accurately count nearly $10.000$ distinct $8$-node motifs whose relative frequency is so small that uniform sampling would literally take centuries to find them. Our results show that color coding is still the most promising approach to scalable motif counting.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3163895487",
    "type": "article"
  },
  {
    "title": "Similarity Embedding Networks for Robust Human Activity Recognition",
    "doi": "https://doi.org/10.1145/3448021",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Chenglin Li; Carrie Lu Tong; Di Niu; Bei Jiang; Xiao Ming Zuo; Lei Cheng; Jian Xiong; Jianming Yang",
    "corresponding_authors": "",
    "abstract": "Deep learning models for human activity recognition (HAR) based on sensor data have been heavily studied recently. However, the generalization ability of deep models on complex real-world HAR data is limited by the availability of high-quality labeled activity data, which are hard to obtain. In this paper, we design a similarity embedding neural network that maps input sensor signals onto real vectors through carefully designed convolutional and LSTM layers. The embedding network is trained with a pairwise similarity loss, encouraging the clustering of samples from the same class in the embedded real space, and can be effectively trained on a small dataset and even on a noisy dataset with mislabeled samples. Based on the learned embeddings, we further propose both nonparametric and parametric approaches for activity recognition. Extensive evaluation based on two public datasets has shown that the proposed similarity embedding network significantly outperforms state-of-the-art deep models on HAR classification tasks, is robust to mislabeled samples in the training set, and can also be used to effectively denoise a noisy dataset.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3163912335",
    "type": "article"
  },
  {
    "title": "Critique on Natural Noise in Recommender Systems",
    "doi": "https://doi.org/10.1145/3447780",
    "publication_date": "2021-05-29",
    "publication_year": 2021,
    "authors": "Wissam Al Jurdi; Jacques Bou Abdo; Jacques Demerjian; Abdallah Makhoul",
    "corresponding_authors": "",
    "abstract": "Recommender systems have been upgraded, tested, and applied in many, often incomparable ways. In attempts to diligently understand user behavior in certain environments, those systems have been frequently utilized in domains like e-commerce, e-learning, and tourism. Their increasing need and popularity have allowed the existence of numerous research paths on major issues like data sparsity, cold start, malicious noise, and natural noise, which immensely limit their performance. It is typical that the quality of the data that fuel those systems should be extremely reliable. Inconsistent user information in datasets can alter the performance of recommenders, albeit running advanced personalizing algorithms. The consequences of this can be costly as such systems are employed in abundant online businesses. Successfully managing these inconsistencies results in more personalized user experiences. In this article, the previous works conducted on natural noise management in recommender datasets are thoroughly analyzed. We adequately explore the ways in which the proposed methods measure improved performances and touch on the different natural noise management techniques and the attributes of the solutions. Additionally, we test the evaluation methods employed to assess the approaches and discuss several key gaps and other improvements the field should realize in the future. Our work considers the likelihood of a modern research branch on natural noise management and recommender assessment.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3171860698",
    "type": "article"
  },
  {
    "title": "Page-Level Main Content Extraction From Heterogeneous Webpages",
    "doi": "https://doi.org/10.1145/3451168",
    "publication_date": "2021-06-28",
    "publication_year": 2021,
    "authors": "Julián Alarte; Josep Silva",
    "corresponding_authors": "",
    "abstract": "The main content of a webpage is often surrounded by other boilerplate elements related to the template, such as menus, advertisements, copyright notices, and comments. For crawlers and indexers, isolating the main content from the template and other noisy information is an essential task, because processing and storing noisy information produce a waste of resources such as bandwidth, storage space, and computing time. Besides, the detection and extraction of the main content is useful in different areas, such as data mining, web summarization, and content adaptation to low resolutions. This work introduces a new technique for main content extraction. In contrast to most techniques, this technique not only extracts text, but also other types of content, such as images, and animations. It is a Document Object Model-based page-level technique, thus it only needs to load one single webpage to extract the main content. As a consequence, it is efficient enough as to be used online (in real-time). We have empirically evaluated the technique using a suite of real heterogeneous benchmarks producing very good results compared with other well-known content extraction techniques.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3174829772",
    "type": "article"
  },
  {
    "title": "A Joint Passenger Flow Inference and Path Recommender System for Deploying New Routes and Stations of Mass Transit Transportation",
    "doi": "https://doi.org/10.1145/3451393",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Fandel Lin; Hsun-Ping Hsieh",
    "corresponding_authors": "",
    "abstract": "In this work, a novel decision assistant system for urban transportation, called Route Scheme Assistant (RSA), is proposed to address two crucial issues that few former researches have focused on: route-based passenger flow (PF) inference and multivariant high-PF route recommendation. First, RSA can estimate the PF of arbitrary user-designated routes effectively by utilizing Deep Neural Network (DNN) for regression based on geographical information and spatial-temporal urban informatics. Second, our proposed Bidirectional Prioritized Spanning Tree (BDPST) intelligently combines the parallel computing concept and Gaussian mixture model (GMM) for route recommendation under users’ constraints running in a timely manner. We did experiments on bus-ticket data of Tainan and Chicago and the experimental results show that the PF inference model outperforms baseline and comparative methods from 41% to 57%. Moreover, the proposed BDPST algorithm's performance is not far away from the optimal PF and outperforms other comparative methods from 39% to 71% in large-scale route recommendations.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3184411799",
    "type": "article"
  },
  {
    "title": "S2OSC: A Holistic Semi-Supervised Approach for Open Set Classification",
    "doi": "https://doi.org/10.1145/3468675",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Yang Yang; Hongchen Wei; Zhenqiang Sun; Guangyu Li; Yuanchun Zhou; Hui Xiong; Jian Yang",
    "corresponding_authors": "",
    "abstract": "Open set classification (OSC) tackles the problem of determining whether the data are in-class or out-of-class during inference, when only provided with a set of in-class examples at training time. Traditional OSC methods usually train discriminative or generative models with the owned in-class data, and then utilize the pre-trained models to classify test data directly. However, these methods always suffer from the embedding confusion problem, i.e., partial out-of-class instances are mixed with in-class ones of similar semantics, making it difficult to classify. To solve this problem, we unify semi-supervised learning to develop a novel OSC algorithm, S2OSC, which incorporates out-of-class instances filtering and model re-training in a transductive manner. In detail, given a pool of newly coming test data, S2OSC firstly filters the mostly distinct out-of-class instances using the pre-trained model, and annotates super-class for them. Then, S2OSC trains a holistic classification model by combing in-class and out-of-class labeled data with the remaining unlabeled test data in a semi-supervised paradigm. Furthermore, considering that data are usually in the streaming form in real applications, we extend S2OSC into an incremental update framework (I-S2OSC), and adopt a knowledge memory regularization to mitigate the catastrophic forgetting problem in incremental update. Despite the simplicity of proposed models, the experimental results show that S2OSC achieves state-of-the-art performance across a variety of OSC tasks, including 85.4% of F1 on CIFAR-10 with only 300 pseudo-labels. We also demonstrate how S2OSC can be expanded to incremental OSC setting effectively with streaming data.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3199180109",
    "type": "article"
  },
  {
    "title": "Utility-Theoretic Ranking for Semiautomated Text Classification",
    "doi": "https://doi.org/10.1145/2742548",
    "publication_date": "2015-07-22",
    "publication_year": 2015,
    "authors": "Giacomo Berardi; Andrea Esuli; Fabrizio Sebastiani",
    "corresponding_authors": "",
    "abstract": "Semiautomated Text Classification (SATC) may be defined as the task of ranking a set D of automatically labelled textual documents in such a way that, if a human annotator validates (i.e., inspects and corrects where appropriate) the documents in a top-ranked portion of D with the goal of increasing the overall labelling accuracy of D , the expected increase is maximized. An obvious SATC strategy is to rank D so that the documents that the classifier has labelled with the lowest confidence are top ranked. In this work, we show that this strategy is suboptimal. We develop new utility-theoretic ranking methods based on the notion of validation gain , defined as the improvement in classification effectiveness that would derive by validating a given automatically labelled document. We also propose a new effectiveness measure for SATC-oriented ranking methods, based on the expected reduction in classification error brought about by partially validating a list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method mentioned earlier, and according to the proposed measure, our utility-theoretic ranking methods can achieve substantially higher expected reductions in classification error.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1529173354",
    "type": "article"
  },
  {
    "title": "Unbiased Characterization of Node Pairs over Large Graphs",
    "doi": "https://doi.org/10.1145/2700393",
    "publication_date": "2015-04-01",
    "publication_year": 2015,
    "authors": "Pinghui Wang; Junzhou Zhao; John C. S. Lui; Don Towsley; Xiaohong Guan",
    "corresponding_authors": "",
    "abstract": "Characterizing user pair relationships is important for applications such as friend recommendation and interest targeting in online social networks (OSNs). Due to the large-scale nature of such networks, it is infeasible to enumerate all user pairs and thus sampling is used. In this article, we show that it is a great challenge for OSN service providers to characterize user pair relationships, even when they possess the complete graph topology. The reason is that when sampling techniques (i.e., uniform vertex sampling (UVS) and random walk (RW)) are naively applied, they can introduce large biases, particularly for estimating similarity distribution of user pairs with constraints like existence of mutual neighbors, which is important for applications such as identifying network homophily. Estimating statistics of user pairs is more challenging in the absence of the complete topology information, as an unbiased sampling technique like UVS is usually not allowed and exploring the OSN graph topology is expensive. To address these challenges, we present unbiased sampling methods to characterize user pair properties based on UVS and RW techniques. We carry out an evaluation of our methods to show their accuracy and efficiency. Finally, we apply our methods to three OSNs—Foursquare, Douban, and Xiami—and discover that significant homophily is present in these networks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1990790439",
    "type": "article"
  },
  {
    "title": "On Data Publishing with Clustering Preservation",
    "doi": "https://doi.org/10.1145/2700403",
    "publication_date": "2015-04-01",
    "publication_year": 2015,
    "authors": "Michail Vlachos; Johannes Schneider; Vassilios G. Vassiliadis",
    "corresponding_authors": "",
    "abstract": "The emergence of cloud-based storage services is opening up new avenues in data exchange and data dissemination. This has amplified the interest in right-protection mechanisms to establish ownership in the event of data leakage. Current right-protection technologies, however, rarely provide strong guarantees on dataset utility after the protection process. This work presents techniques that explicitly address this topic and provably preserve the outcome of certain mining operations. In particular, we take special care to guarantee that the outcome of hierarchical clustering operations remains the same before and after right protection. Our approach considers all prevalent hierarchical clustering variants: single-, complete-, and average-linkage. We imprint the ownership in a dataset using watermarking principles, and we derive tight bounds on the expansion/contraction of distances incurred by the process. We leverage our analysis to design fast algorithms for right protection without exhaustively searching the vast design space. Finally, because the right-protection process introduces a user-tunable distortion on the dataset, we explore the possibility of using this mechanism for data obfuscation. We quantify the tradeoff between obfuscation and utility for spatiotemporal datasets and discover very favorable characteristics of the process. An additional advantage is that when one is interested in both right-protecting and obfuscating the original data values, the proposed mechanism can accomplish both tasks simultaneously.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2045019660",
    "type": "article"
  },
  {
    "title": "Multimodal Data Mining in a Multimedia Database Based on Structured Max Margin Learning",
    "doi": "https://doi.org/10.1145/2742549",
    "publication_date": "2016-02-24",
    "publication_year": 2016,
    "authors": "Zhen Guo; Zhongfei Zhang; Eric P. Xing; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "Mining knowledge from a multimedia database has received increasing attentions recently since huge repositories are made available by the development of the Internet. In this article, we exploit the relations among different modalities in a multimedia database and present a framework for general multimodal data mining problem where image annotation and image retrieval are considered as the special cases. Specifically, the multimodal data mining problem can be formulated as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables. In addition, in order to reduce the demanding computation, we propose a new max margin structure learning approach called Enhanced Max Margin Learning (EMML) framework, which is much more efficient with a much faster convergence rate than the existing max margin learning methods, as verified through empirical evaluations. Furthermore, we apply EMML framework to develop an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale. The EMML framework allows an efficient multimodal data mining query in a very large scale multimedia database, and excels many existing multimodal data mining methods in the literature that do not scale up at all. The performance comparison with a state-of-the-art multimodal data mining method is reported for the real-world image databases.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2288265387",
    "type": "article"
  },
  {
    "title": "Kernelized Information-Theoretic Metric Learning for Cancer Diagnosis Using High-Dimensional Molecular Profiling Data",
    "doi": "https://doi.org/10.1145/2789212",
    "publication_date": "2016-05-11",
    "publication_year": 2016,
    "authors": "Feiyu Xiong; Moshe Kam; Leonid Hrebien; Beilun Wang; Yanjun Qi",
    "corresponding_authors": "",
    "abstract": "With the advancement of genome-wide monitoring technologies, molecular expression data have become widely used for diagnosing cancer through tumor or blood samples. When mining molecular signature data, the process of comparing samples through an adaptive distance function is fundamental but difficult, as such datasets are normally heterogeneous and high dimensional. In this article, we present kernelized information-theoretic metric learning (KITML) algorithms that optimize a distance function to tackle the cancer diagnosis problem and scale to high dimensionality. By learning a nonlinear transformation in the input space implicitly through kernelization, KITML permits efficient optimization, low storage, and improved learning of distance metric. We propose two novel applications of KITML for diagnosing cancer using high-dimensional molecular profiling data: (1) for sample-level cancer diagnosis, the learned metric is used to improve the performance of k -nearest neighbor classification; and (2) for estimating the severity level or stage of a group of samples, we propose a novel set-based ranking approach to extend KITML. For the sample-level cancer classification task, we have evaluated on 14 cancer gene microarray datasets and compared with eight other state-of-the-art approaches. The results show that our approach achieves the best overall performance for the task of molecular-expression-driven cancer sample diagnosis. For the group-level cancer stage estimation, we test the proposed set-KITML approach using three multi-stage cancer microarray datasets, and correctly estimated the stages of sample groups for all three studies.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2358862191",
    "type": "article"
  },
  {
    "title": "Product Selection Problem",
    "doi": "https://doi.org/10.1145/2753764",
    "publication_date": "2016-06-29",
    "publication_year": 2016,
    "authors": "Silei Xu; John C. S. Lui",
    "corresponding_authors": "",
    "abstract": "It is often crucial for manufacturers to decide what products to produce so that they can increase their market share in an increasingly fierce market. To decide which products to produce, manufacturers need to analyze the consumers’ requirements and how consumers make their purchase decisions so that the new products will be competitive in the market. In this paper, we first present a general distance-based product adoption model to capture consumers’ purchase behavior. Using this model, various distance metrics can be used to describe different real life purchase behavior. We then provide a learning algorithm to decide which set of distance metrics one should use when we are given some accessible historical purchase data. Based on the product adoption model, we formalize the k most marketable products (or k- MMP ) selection problem and formally prove that the problem is NP-hard . To tackle this problem, we propose an efficient greedy-based approximation algorithm with a provable solution guarantee. Using submodularity analysis, we prove that our approximation algorithm can achieve at least 63% of the optimal solution. We apply our algorithm on both synthetic datasets and real-world datasets (TripAdvisor.com), and show that our algorithm can easily achieve five or more orders of speedup over the exhaustive search and achieve about 96% of the optimal solution on average. Our experiments also demonstrate the robustness of our distance metric learning method, and illustrate how one can adopt it to improve the accuracy of product selection.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2734845629",
    "type": "article"
  },
  {
    "title": "Assignment Problems of Different-Sized Inputs in MapReduce",
    "doi": "https://doi.org/10.1145/2987376",
    "publication_date": "2016-12-03",
    "publication_year": 2016,
    "authors": "Foto Afrati; Shlomi Dolev; Ephraim Korach; Shantanu Sharma; Jeffrey D. Ullman",
    "corresponding_authors": "",
    "abstract": "A MapReduce algorithm can be described by a mapping schema , which assigns inputs to a set of reducers, such that for each required output there exists a reducer that receives all the inputs participating in the computation of this output. Reducers have a capacity that limits the sets of inputs they can be assigned. However, individual inputs may vary in terms of size. We consider, for the first time, mapping schemas where input sizes are part of the considerations and restrictions. One of the significant parameters to optimize in any MapReduce job is communication cost between the map and reduce phases. The communication cost can be optimized by minimizing the number of copies of inputs sent to the reducers. The communication cost is closely related to the number of reducers of constrained capacity that are used to accommodate appropriately the inputs, so that the requirement of how the inputs must meet in a reducer is satisfied. In this work, we consider a family of problems where it is required that each input meets with each other input in at least one reducer. We also consider a slightly different family of problems in which each input of a list, X , is required to meet each input of another list, Y , in at least one reducer. We prove that finding an optimal mapping schema for these families of problems is NP-hard, and present a bin-packing-based approximation algorithm for finding a near optimal mapping schema.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W872672612",
    "type": "article"
  },
  {
    "title": "Constant Time Graph Neural Networks",
    "doi": "https://doi.org/10.1145/3502733",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Ryoma Sato; Makoto Yamada; Hisashi Kashima",
    "corresponding_authors": "",
    "abstract": "The recent advancements in graph neural networks (GNNs) have led to state-of-the-art performances in various applications, including chemo-informatics, question-answering systems, and recommender systems. However, scaling up these methods to huge graphs, such as social networks and Web graphs, remains a challenge. In particular, the existing methods for accelerating GNNs either are not theoretically guaranteed in terms of the approximation error or incurred at least a linear time computation cost. In this study, we reveal the query complexity of the uniform node sampling scheme for Message Passing Neural Networks, including GraphSAGE, graph attention networks (GATs), and graph convolutional networks (GCNs). Surprisingly, our analysis reveals that the complexity of the node sampling method is completely independent of the number of the nodes, edges, and neighbors of the input and depends only on the error tolerance and confidence probability while providing a theoretical guarantee for the approximation error. To the best of our knowledge, this is the first article to provide a theoretical guarantee of approximation for GNNs within constant time. Through experiments with synthetic and real-world datasets, we investigated the speed and precision of the node sampling scheme and validated our theoretical results.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2911856984",
    "type": "article"
  },
  {
    "title": "US-Rule: Discovering Utility-driven Sequential Rules",
    "doi": "https://doi.org/10.1145/3532613",
    "publication_date": "2022-08-09",
    "publication_year": 2022,
    "authors": "Gengsen Huang; Wensheng Gan; Jian Weng; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Utility-driven mining is an important task in data science and has many applications in real life. High-utility sequential pattern mining (HUSPM) is one kind of utility-driven mining. It aims at discovering all sequential patterns with high utility. However, the existing algorithms of HUSPM can not provide a relatively accurate probability to deal with some scenarios for prediction or recommendation. High-utility sequential rule mining (HUSRM) is proposed to discover all sequential rules with high utility and high confidence. There is only one algorithm proposed for HUSRM, which is not efficient enough. In this article, we propose a faster algorithm called US-Rule, to efficiently mine high-utility sequential rules. It utilizes the rule estimated utility co-occurrence pruning strategy (REUCP) to avoid meaningless computations. Moreover, to improve its efficiency on dense and long sequence datasets, four tighter upper bounds (LEEU, REEU, LERSU, and RERSU) and corresponding pruning strategies (LEEUP, REEUP, LERSUP, and RERSUP) are designed. US-Rule also proposes the rule estimated utility recomputing pruning strategy (REURP) to deal with sparse datasets. Finally, a large number of experiments on different datasets compared to the state-of-the-art algorithm demonstrate that US-Rule can achieve better performance in terms of execution time, memory consumption, and scalability.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3215428643",
    "type": "article"
  },
  {
    "title": "Adaptive Model Scheduling for Resource-efficient Data Labeling",
    "doi": "https://doi.org/10.1145/3494559",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Mu Yuan; Lan Zhang; Xiang‐Yang Li; Linzhuo Yang; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "Labeling data (e.g., labeling the people, objects, actions, and scene in images) comprehensively and efficiently is a widely needed but challenging task. Numerous models were proposed to label various data and many approaches were designed to enhance the ability of deep learning models or accelerate them. Unfortunately, a single machine-learning model is not powerful enough to extract various semantic information from data. Given certain applications, such as image retrieval platforms and photo album management apps, it is often required to execute a collection of models to obtain sufficient labels. With limited computing resources and stringent delay, given a data stream and a collection of applicable resource-hungry deep-learning models, we design a novel approach to adaptively schedule a subset of these models to execute on each data item, aiming to maximize the value of the model output (e.g., the number of high-confidence labels). Achieving this lofty goal is nontrivial since a model’s output on any data item is content-dependent and unknown until we execute it. To tackle this, we propose an Adaptive Model Scheduling framework, consisting of (1) a deep reinforcement learning-based approach to predict the value of unexecuted models by mining semantic relationship among diverse models, and (2) two heuristic algorithms to adaptively schedule the model execution order under a deadline or deadline-memory constraints, respectively. The proposed framework does not require any prior knowledge of the data, which works as a powerful complement to existing model optimization technologies. We conduct extensive evaluations on five diverse image datasets and 30 popular image labeling models to demonstrate the effectiveness of our design: our design could save around 53% execution time without loss of any valuable labels.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4205720575",
    "type": "article"
  },
  {
    "title": "Synthesis of Longitudinal Human Location Sequences: Balancing Utility and Privacy",
    "doi": "https://doi.org/10.1145/3529260",
    "publication_date": "2022-04-25",
    "publication_year": 2022,
    "authors": "Maya Benarous; Eran Toch; Irad Ben‐Gal",
    "corresponding_authors": "",
    "abstract": "People’s location data are continuously tracked from various devices and sensors, enabling an ongoing analysis of sensitive information that can violate people’s privacy and reveal confidential information. Synthetic data have been used to generate representative location sequences yet to maintain the users’ privacy. Nonetheless, the privacy-accuracy tradeoff between these two measures has not been addressed systematically. In this article, we analyze the use of different synthetic data generation models for long location sequences, including extended short-term memory networks (LSTMs), Markov Chains (MC), and variable-order Markov models (VMMs). We employ different performance measures, such as data similarity and privacy, and discuss the inherent tradeoff. Furthermore, we introduce other measurements to quantify each of these measures. Based on the anonymous data of 300 thousand cellular-phone users, our work offers a road map for developing policies for synthetic data generation processes. We propose a framework for building data generation models and evaluating their effectiveness regarding those accuracy and privacy measures.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4224309813",
    "type": "article"
  },
  {
    "title": "On the Robustness of Metric Learning: An Adversarial Perspective",
    "doi": "https://doi.org/10.1145/3502726",
    "publication_date": "2022-04-05",
    "publication_year": 2022,
    "authors": "Mengdi Huai; Tianhang Zheng; Chenglin Miao; Liuyi Yao; Aidong Zhang",
    "corresponding_authors": "",
    "abstract": "Metric learning aims at automatically learning a distance metric from data so that the precise similarity between data instances can be faithfully reflected, and its importance has long been recognized in many fields. An implicit assumption in existing metric learning works is that the learned models are performed in a reliable and secure environment. However, the increasingly critical role of metric learning makes it susceptible to a risk of being malicious attacked. To well understand the performance of metric learning models in adversarial environments, in this article, we study the robustness of metric learning to adversarial perturbations, which are also known as the imperceptible changes to the input data that are crafted by an attacker to fool a well-learned model. However, different from traditional classification models, metric learning models take instance pairs rather than individual instances as input, and the perturbation on one instance may not necessarily affect the prediction result for an instance pair, which makes it more difficult to study the robustness of metric learning. To address this challenge, in this article, we first provide a definition of pairwise robustness for metric learning, and then propose a novel projected gradient descent-based attack method (called AckMetric) to evaluate the robustness of metric learning models. To further explore the capability of the attacker to change the prediction results, we also propose a theoretical framework to derive the upper bound of the pairwise adversarial loss. Finally, we incorporate the derived bound into the training process of metric learning and design a novel defense method to make the learned models more robust. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed methods.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4225986943",
    "type": "article"
  },
  {
    "title": "Incremental Feature Spaces Learning with Label Scarcity",
    "doi": "https://doi.org/10.1145/3516368",
    "publication_date": "2022-06-27",
    "publication_year": 2022,
    "authors": "Shilin Gu; Yuhua Qian; Chenping Hou",
    "corresponding_authors": "",
    "abstract": "Recently, learning and mining from data streams with incremental feature spaces have attracted extensive attention, where data may dynamically expand over time in both volume and feature dimensions. Existing approaches usually assume that the incoming instances can always receive true labels. However, in many real-world applications, e.g., environment monitoring, acquiring the true labels is costly due to the need of human effort in annotating the data. To tackle this problem, we propose a novel incremental Feature spaces Learning with Label Scarcity (FLLS) algorithm, together with its two variants. When data streams arrive with augmented features, we first leverage the margin-based online active learning to select valuable instances to be labeled and thus build superior predictive models with minimal supervision. After receiving the labels, we combine the online passive-aggressive update rule and margin-maximum principle to jointly update the dynamic classifier in the shared and augmented feature space. Finally, we use the projected truncation technique to build a sparse but efficient model. We theoretically analyze the error bounds of FLLS and its two variants. Also, we conduct experiments on synthetic data and real-world applications to further validate the effectiveness of our proposed algorithms.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4283589974",
    "type": "article"
  },
  {
    "title": "Multiple Imputation Ensembles for Time Series (MIE-TS)",
    "doi": "https://doi.org/10.1145/3551643",
    "publication_date": "2022-07-26",
    "publication_year": 2022,
    "authors": "Aliya Aleryani; Aaron Bostrom; Wenjia Wang; Beatriz de la Iglesia",
    "corresponding_authors": "",
    "abstract": "Time series classification has become an interesting field of research, thanks to the extensive studies conducted in the past two decades. Time series may have missing data, which may affect both the representation and also modeling of time series. Thus, recovering missing data using appropriate time series-based imputation methods is an essential step. Multiple imputation is a data recovery method where it produced multiple imputed data. The method proves its usefulness in terms of reflecting the uncertainty inherit in missing data; however, it is under-researched in time series problems. In this article, we propose two multiple imputation approaches for time series. The first is a multiple imputation method based on interpolation. The second is a multiple imputation and ensemble method. First, we simulate missing consecutive sub-sequences under a Missing Completely at Random mechanism; then, we use single/multiple imputation methods. The imputed data are used to build bagging and stacking ensembles. We build ensembles using standard classification algorithms as well as time series classifiers. The standard classifiers involve Random Forest, Support Vector Machines, K-Nearest Neighbour, C4.5, and PART while TSCHIEF, Proximity Forest, Time Series Forest, RISE, and BOSS are chosen as time series classifiers. Our findings show that the combination of multiple imputation and ensemble improves the performance of the majority of classifiers tested in this study, often above the performance obtained from the complete data, even under increasing missing data scenarios. This may be because the diversity injected by multiple imputation has a very favourable and stabilising effect on the classifier performance, which is a very important finding.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4288039427",
    "type": "article"
  },
  {
    "title": "MIRROR: Mining Implicit Relationships via Structure-Enhanced Graph Convolutional Networks",
    "doi": "https://doi.org/10.1145/3564531",
    "publication_date": "2022-09-22",
    "publication_year": 2022,
    "authors": "Jiaying Liu; Feng Xia; Jing Ren; Bo Xu; Guansong Pang; Lianhua Chi",
    "corresponding_authors": "",
    "abstract": "Data explosion in the information society drives people to develop more effective ways to extract meaningful information. Extracting semantic information and relational information has emerged as a key mining primitive in a wide variety of practical applications. Existing research on relation mining has primarily focused on explicit connections and ignored underlying information, e.g., the latent entity relations. Exploring such information (defined as implicit relationships in this article) provides an opportunity to reveal connotative knowledge and potential rules. In this article, we propose a novel research topic, i.e., how to identify implicit relationships across heterogeneous networks. Specially, we first give a clear and generic definition of implicit relationships. Then, we formalize the problem and propose an efficient solution, namely MIRROR, a graph convolutional network (GCN) model to infer implicit ties under explicit connections. MIRROR captures rich information in learning node-level representations by incorporating attributes from heterogeneous neighbors. Furthermore, MIRROR is tolerant of missing node attribute information because it is able to utilize network structure. We empirically evaluate MIRROR on four different genres of networks, achieving state-of-the-art performance for target relations mining. The underlying information revealed by MIRROR contributes to enriching existing knowledge and leading to novel domain insights.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4296641464",
    "type": "article"
  },
  {
    "title": "An Efficient Aggregation Method for the Symbolic Representation of Temporal Data",
    "doi": "https://doi.org/10.1145/3532622",
    "publication_date": "2022-04-27",
    "publication_year": 2022,
    "authors": "Xinye Chen; Stefan Güttel",
    "corresponding_authors": "",
    "abstract": "Symbolic representations are a useful tool for the dimension reduction of temporal data, allowing for the efficient storage of and information retrieval from time series. They can also enhance the training of machine learning algorithms on time series data through noise reduction and reduced sensitivity to hyperparameters. The adaptive Brownian bridge-based aggregation (ABBA) method is one such effective and robust symbolic representation, demonstrated to accurately capture important trends and shapes in time series. However, in its current form, the method struggles to process very large time series. Here, we present a new variant of the ABBA method, called fABBA. This variant utilizes a new aggregation approach tailored to the piecewise representation of time series. By replacing the k-means clustering used in ABBA with a sorting-based aggregation technique, and thereby avoiding repeated sum-of-squares error computations, the computational complexity is significantly reduced. In contrast to the original method, the new approach does not require the number of time series symbols to be specified in advance. Through extensive tests, we demonstrate that the new method significantly outperforms ABBA with a considerable reduction in runtime while also outperforming the popular SAX and 1d-SAX representations in terms of reconstruction accuracy. We further demonstrate that fABBA can compress other data types such as images.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4299556157",
    "type": "article"
  },
  {
    "title": "Protecting Privacy in Trajectories with a User-Centric Approach",
    "doi": "https://doi.org/10.1145/3233185",
    "publication_date": "2018-08-28",
    "publication_year": 2018,
    "authors": "Cristina Romero-Tris; David Megías",
    "corresponding_authors": "",
    "abstract": "The increased use of location-aware devices, such as smartphones, generates a large amount of trajectory data. These data can be useful in several domains, like marketing, path modeling, localization of an epidemic focus, and so on. Nevertheless, since trajectory information contains personal mobility data, improper use or publication of trajectory data can threaten users’ privacy. It may reveal sensitive details like habits of behavior, religious beliefs, and sexual preferences. Therefore, many users might be unwilling to share their trajectory data without a previous anonymization process. Currently, several proposals to address this problem can be found in the literature. These solutions focus on anonymizing data before its publication, i.e., when they are already stored in the server database. Nevertheless, we argue that this approach gives the user no control about the information she shares. For this reason, we propose anonymizing data in the users’ mobile devices, before they are sent to a third party. This article extends our previous work which was, to the best of our knowledge, the first one to anonymize data at the client side, allowing users to select the amount and accuracy of shared data. In this article, we describe an improved version of the protocol, and we include the implementation together with an analysis of the results obtained after the simulation with real trajectory data.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2889180143",
    "type": "article"
  },
  {
    "title": "Text Mining for Evaluating Authors' Birth and Death Years",
    "doi": "https://doi.org/10.1145/3281631",
    "publication_date": "2019-01-09",
    "publication_year": 2019,
    "authors": "Dror Moghaz; Yaakov HaCohen‐Kerner; Dov M. Gabbay",
    "corresponding_authors": "",
    "abstract": "This article presents a unique method in text and data mining for finding the era, i.e., mining temporal data, in which an anonymous author was living. Finding this era can assist in the examination of a fake document or extracting the time period in which a writer lived. The study and the experiments concern Hebrew, and in some parts, Aramaic and Yiddish rabbinic texts. The rabbinic texts are undated and contain no bibliographic sections, posing an interesting challenge. This work proposes algorithms using key phrases and key words that allow the temporal organization of citations together with linguistic patterns. Based on these key phrases, key words, and the references, we established several types of “Iron-clad,” Heuristic and Greedy rules for estimating the years of birth and death of a writer in an interesting classification task. Experiments were conducted on corpora, including documents authored by 12, 24, and 36 rabbinic writers and demonstrated promising results.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2911213156",
    "type": "article"
  },
  {
    "title": "Real-Time Estimation of the Urban Air Quality with Mobile Sensor System",
    "doi": "https://doi.org/10.1145/3356584",
    "publication_date": "2019-09-24",
    "publication_year": 2019,
    "authors": "Yun Wang; Guojie Song; Lun Du; Zhicong Lu",
    "corresponding_authors": "",
    "abstract": "Recently, real-time air quality estimation has attracted more and more attention from all over the world, which is close to our daily life. With the prevalence of mobile sensors, there is an emerging way to monitor the air quality with mobile sensors on vehicles. Compared with traditional expensive monitor stations, mobile sensors are cheaper and more abundant, but observations from these sensors have unstable spatial and temporal distributions, which results in the existing model could not work very well on this type of data. In this article, taking advantage of air quality data from mobile sensors, we propose an real-time urban air quality estimation method based on the Gaussian Process Regression for air pollution of the unmonitored areas, pivoting on the diffusion effect and the accumulation effect of air pollution. In order to meet the real-time demands, we propose a two-layer ensemble learning framework and a self-adaptivity mechanism to improve computational efficiency and adaptivity. We evaluate our model with real data from mobile sensor system located in Beijing, China. And the experiments show that our proposed model is superior to the state-of-the-art spatial regression methods in both precision and time performances.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2977143152",
    "type": "article"
  },
  {
    "title": "Large-scale Data Exploration Using Explanatory Regression Functions",
    "doi": "https://doi.org/10.1145/3410448",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Fotis Savva; Christos Anagnostopoulos; Peter Triantafillou; Kostas Kolomvatsos",
    "corresponding_authors": "",
    "abstract": "Analysts wishing to explore multivariate data spaces, typically issue queries involving selection operators, i.e., range or equality predicates, which define data subspaces of potential interest. Then, they use aggregation functions, the results of which determine a subspace’s interestingness for further exploration and deeper analysis. However, Aggregate Query (AQ) results are scalars and convey limited information and explainability about the queried subspaces for enhanced exploratory analysis. Analysts have no way of identifying how these results are derived or how they change w.r.t query (input) parameter values. We address this shortcoming by aiding analysts to explore and understand data subspaces by contributing a novel explanation mechanism based on machine learning. We explain AQ results using functions obtained by a three-fold joint optimization problem which assume the form of explainable piecewise-linear regression functions. A key feature of the proposed solution is that the explanation functions are estimated using past executed queries. These queries provide a coarse grained overview of the underlying aggregate function (generating the AQ results) to be learned. Explanations for future, previously unseen AQs can be computed without accessing the underlying data and can be used to further explore the queried data subspaces, without issuing more queries to the backend analytics engine. We evaluate the explanation accuracy and efficiency through theoretically grounded metrics over real-world and synthetic datasets and query workloads.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3089817556",
    "type": "article"
  },
  {
    "title": "Heterogeneous Univariate Outlier Ensembles in Multidimensional Data",
    "doi": "https://doi.org/10.1145/3403934",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Guansong Pang; Longbing Cao",
    "corresponding_authors": "",
    "abstract": "In outlier detection, recent major research has shifted from developing univariate methods to multivariate methods due to the rapid growth of multidimensional data. However, one typical issue of this paradigm shift is that many multidimensional data often mainly contains univariate outliers , in which many features are actually irrelevant. In such cases, multivariate methods are ineffective in identifying such outliers due to the potential biases and the curse of dimensionality brought by irrelevant features. Those univariate outliers might be well detected by applying univariate outlier detectors in individually relevant features. However, it is very challenging to choose a right univariate detector for each individual feature since different features may take very different probability distributions. To address this challenge, we introduce a novel Heterogeneous Univariate Outlier Ensembles (HUOE) framework and its instance ZDD to synthesize a set of heterogeneous univariate outlier detectors as base learners to build heterogeneous ensembles that are optimized for each individual feature. Extensive results on 19 real-world datasets and a collection of synthetic datasets show that ZDD obtains 5%–14% average AUC improvement over four state-of-the-art multivariate ensembles and performs substantially more robustly w.r.t. irrelevant features.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3091079630",
    "type": "article"
  },
  {
    "title": "Probabilistic Modeling for Frequency Vectors Using a Flexible Shifted-Scaled Dirichlet Distribution Prior",
    "doi": "https://doi.org/10.1145/3406242",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Nuha Zamzami; Nizar Bouguila",
    "corresponding_authors": "",
    "abstract": "Burstiness and overdispersion phenomena of count vectors pose significant challenges in modeling such data accurately. While the dependency assumption of the multinomial distribution causes its failure to model frequency vectors in several machine learning and data mining applications, researchers found that by extending the multinomial distribution to the Dirichlet Compound multinomial (DCM), both phenomena modeling can be addressed. However, Dirichlet distribution is not the best choice, as a prior, given its negative-correlation and equal-confidence requirements. Thus, we propose to use a flexible generalization of the Dirichlet distribution, namely, the shifted-scaled Dirichlet, as a prior to the multinomial, which grants the model a capability to better fit real data, and we call the new model the Multinomial Shifted-Scaled Dirichlet (MSSD). Given that the likelihood function plays a key role in statistical inference, e.g., in maximum likelihood estimation and Fisher information matrix investigation, we propose to improve the efficiency of computing the MSSD log-likelihood by approximating its function based on Bernoulli polynomials where the log-likelihood function is computed using the proposed mesh algorithm. Moreover, given the sparsity and high-dimensionality nature of count vectors, we propose to improve its computation efficiency by approximating the novel MSSD as a member of the exponential family of distribution, which we call EMSSD. The clustering is based on mixture models, and for learning a model, selection approach is seamlessly integrated with the estimation of the parameters. The merits of the proposed approach are validated via challenging real-world applications such as hate speech detection in Twitter, real-time recognition of criminal action, and anomaly detection in crowded scenes. Results reveal that the proposed clustering frameworks offer a good compromise between other state-of-the-art techniques and outperform other approaches previously used for frequency vectors modeling. Besides, comparing to the MSSD, the approximation EMSSD has reduced the computational complexity in high-dimensional feature spaces.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3091085709",
    "type": "article"
  },
  {
    "title": "A Reduced Network Traffic Method for IoT Data Clustering",
    "doi": "https://doi.org/10.1145/3423139",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Ricardo De Azevedo; Gabriel Resende Machado; Ronaldo Ribeiro Goldschmidt; Ricardo Choren",
    "corresponding_authors": "",
    "abstract": "Internet of Things (IoT) systems usually involve interconnected, low processing capacity, and low memory sensor nodes (devices) that collect data in several sorts of applications that interconnect people and things. In this scenario, mining tasks, such as clustering, have been commonly deployed to detect behavioral patterns from the collected data. The centralized clustering of IoT data demands high network traffic to transmit the data from the devices to a central node, where a clustering algorithm must be applied. This approach does not scale as the number of devices increases, and the amount of data grows. However, distributing the clustering process through the devices may not be a feasible approach as well, since the devices are usually simple and may not have the ability to execute complex procedures. This work proposes a centralized IoT data clustering method that demands reduced network traffic and low processing power in the devices. The proposed method uses a data grid to summarize the information at the devices before transmitting it to the central node, reducing network traffic. After the data transfer, the proposed method applies a clustering algorithm that was developed to process data in the summarized representation. Tests with seven datasets provided experimental evidence that the proposed method reduces network traffic and produces results comparable to the ones generated by DBSCAN and HDBSCAN, two robust centralized clustering algorithms.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3111420302",
    "type": "article"
  },
  {
    "title": "Optimizing Data Misuse Detection",
    "doi": "https://doi.org/10.1145/2611520",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Asaf Shabtai; Maya Bercovitch; Lior Rokach; Yuval Elovici",
    "corresponding_authors": "",
    "abstract": "Data misuse may be performed by entities such as an organization's employees and business partners who are granted access to sensitive information and misuse their privileges. We assume that users can be either trusted or untrusted. The access of untrusted parties to data objects (e.g., client and patient records) should be monitored in an attempt to detect misuse. However, monitoring data objects is resource intensive and time-consuming and may also cause disturbance or inconvenience to the involved employees. Therefore, the monitored data objects should be carefully selected. In this article, we present two optimization problems carefully designed for selecting specific data objects for monitoring, such that the detection rate is maximized and the monitoring effort is minimized. In the first optimization problem, the goal is to select data objects for monitoring that are accessed by at most c trusted agents while ensuring access to at least k monitored objects by each untrusted agent (both c and k are integer variable). As opposed to the first optimization problem, the goal of the second optimization problem is to select monitored data objects that maximize the number of monitored data objects accessed by untrusted agents while ensuring that each trusted agent does not access more than d monitored data objects ( d is an integer variable as well). Two efficient heuristic algorithms for solving these optimization problems are proposed, and experiments were conducted simulating different scenarios to evaluate the algorithms’ performance. Moreover, we compared the heuristic algorithms’ performance to the optimal solution and conducted sensitivity analysis on the three parameters ( c , k , and d ) and on the ratio between the trusted and untrusted agents.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2057686088",
    "type": "article"
  },
  {
    "title": "Joint Inference of Diffusion and Structure in Partially Observed Social Networks Using Coupled Matrix Factorization",
    "doi": "https://doi.org/10.1145/3599237",
    "publication_date": "2023-05-24",
    "publication_year": 2023,
    "authors": "Maryam Ramezani; Aryan Ahadinia; Amirmohammad Ziaei Bideh; Hamid R. Rabiee",
    "corresponding_authors": "",
    "abstract": "Access to complete data in large-scale networks is often infeasible. Therefore, the problem of missing data is a crucial and unavoidable issue in the analysis and modeling of real-world social networks. However, most of the research on different aspects of social networks does not consider this limitation. One effective way to solve this problem is to recover the missing data as a pre-processing step. In this paper, a model is learned from partially observed data to infer unobserved diffusion and structure networks. To jointly discover omitted diffusion activities and hidden network structures, we develop a probabilistic generative model called \"DiffStru.\" The interrelations among links of nodes and cascade processes are utilized in the proposed method via learning coupled with low-dimensional latent factors. Besides inferring unseen data, latent factors such as community detection may also aid in network classification problems. We tested different missing data scenarios on simulated independent cascades over LFR networks and real datasets, including Twitter and Memtracker. Experiments on these synthetic and real-world datasets show that the proposed method successfully detects invisible social behaviors, predicts links, and identifies latent features.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4377989179",
    "type": "article"
  },
  {
    "title": "Efficient algorithms for genome-wide association study",
    "doi": "https://doi.org/10.1145/1631162.1631167",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Xiang Zhang; Fei Zou; Wei Wang",
    "corresponding_authors": "",
    "abstract": "Studying the association between quantitative phenotype (such as height or weight) and single nucleotide polymorphisms (SNPs) is an important problem in biology. To understand underlying mechanisms of complex phenotypes, it is often necessary to consider joint genetic effects across multiple SNPs. ANOVA (analysis of variance) test is routinely used in association study. Important findings from studying gene-gene (SNP-pair) interactions are appearing in the literature. However, the number of SNPs can be up to millions. Evaluating joint effects of SNPs is a challenging task even for SNP-pairs. Moreover, with large number of SNPs correlated, permutation procedure is preferred over simple Bonferroni correction for properly controlling family-wise error rate and retaining mapping power, which dramatically increases the computational cost of association study. In this article, we study the problem of finding SNP-pairs that have significant associations with a given quantitative phenotype. We propose an efficient algorithm, FastANOVA, for performing ANOVA tests on SNP-pairs in a batch mode, which also supports large permutation test. We derive an upper bound of SNP-pair ANOVA test, which can be expressed as the sum of two terms. The first term is based on single-SNP ANOVA test. The second term is based on the SNPs and independent of any phenotype permutation. Furthermore, SNP-pairs can be organized into groups, each of which shares a common upper bound. This allows for maximum reuse of intermediate computation, efficient upper bound estimation, and effective SNP-pair pruning. Consequently, FastANOVA only needs to perform the ANOVA test on a small number of candidate SNP-pairs without the risk of missing any significant ones. Extensive experiments demonstrate that FastANOVA is orders of magnitude faster than the brute-force implementation of ANOVA tests on all SNP pairs. The principles used in FastANOVA can be applied to categorical phenotypes and other statistics such as Chi-square test.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1976079860",
    "type": "article"
  },
  {
    "title": "Learning correlations using the mixture-of-subsets model",
    "doi": "https://doi.org/10.1145/1324172.1324175",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Manas Somaiya; Christopher Jermaine; Sanjay Ranka",
    "corresponding_authors": "",
    "abstract": "Using a mixture of random variables to model data is a tried-and-tested method common in data mining, machine learning, and statistics. By using mixture modeling it is often possible to accurately model even complex, multimodal data via very simple components. However, the classical mixture model assumes that a data point is generated by a single component in the model. A lot of datasets can be modeled closer to the underlying reality if we drop this restriction. We propose a probabilistic framework, the mixture-of-subsets (MOS) model , by making two fundamental changes to the classical mixture model. First, we allow a data point to be generated by a set of components, rather than just a single component. Next, we limit the number of data attributes that each component can influence. We also propose an EM framework to learn the MOS model from a dataset, and experimentally evaluate it on real, high-dimensional datasets. Our results show that the MOS model learned from the data represents the underlying nature of the data accurately.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2061292150",
    "type": "article"
  },
  {
    "title": "Identifying Linear Models in Multi-Resolution Population Data Using Minimum Description Length Principle to Predict Household Income",
    "doi": "https://doi.org/10.1145/3424670",
    "publication_date": "2021-01-04",
    "publication_year": 2021,
    "authors": "Chainarong Amornbunchornvej; Navaporn Surasvadi; Anon Plangprasopchok; Suttipong Thajchayapong",
    "corresponding_authors": "",
    "abstract": "One shirt size cannot fit everybody, while we cannot make a unique shirt that fits perfectly for everyone because of resource limitations. This analogy is true for policy making as well. Policy makers cannot make a single policy to solve all problems for all regions because each region has its own unique issue. At the other extreme, policy makers also cannot make a policy for each small village due to resource limitations. Would it be better if we can find a set of largest regions such that the population of each region within this set has common issues and we can make a single policy for them? In this work, we propose a framework using regression analysis and Minimum Description Length (MDL) to find a set of largest areas that have common indicators, which can be used to predict household incomes efficiently. Given a set of household features, and a multi-resolution partition that represents administrative divisions, our framework reports a set C * of largest subdivisions that have a common predictive model for population-income prediction. We formalize the problem of finding C * and propose an algorithm that can find C * correctly. We use both simulation datasets as well as a real-world dataset of Thailand’s population household information to demonstrate our framework performance and application. The results show that our framework performance is better than the baseline methods. Moreover, we demonstrate that the results of our method can be used to find indicators of income prediction for many areas in Thailand. By adjusting these indicator values via policies, we expect people in these areas to gain more incomes. Hence, the policy makers will be able to make policies by using these indicators in our results as a guideline to solve low-income issues. Our framework can be used to support policy makers in making policies regarding any other dependent variable beyond income in order to combat poverty and other issues. We provide the R package, MRReg, which is the implementation of our framework in the R language. The MRReg package comes with a documentation for anyone who is interested in analyzing linear regression on multi-resolution population data.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2957828164",
    "type": "article"
  },
  {
    "title": "SAKE",
    "doi": "https://doi.org/10.1145/3441646",
    "publication_date": "2021-04-18",
    "publication_year": 2021,
    "authors": "Mingkai Lin; Wenzhong Li; Lynda Jiwen Song; Cam-Tu Nguyen; Xiaoliang Wang; Sanglu Lu",
    "corresponding_authors": "",
    "abstract": "Katz centrality is a fundamental concept to measure the influence of a vertex in a social network. However, existing approaches to calculating Katz centrality in a large-scale network are unpractical and computationally expensive. In this article, we propose a novel method to estimate Katz centrality based on graph sampling techniques, which object to achieve comparable estimation accuracy of the state-of-the-arts with much lower computational complexity. Specifically, we develop a Horvitz–Thompson estimate for Katz centrality by using a multi-round sampling approach and deriving an unbiased mean value estimator. We further propose SAKE , a S ampling-based A lgorithm for fast K atz centrality E stimation. We prove that the estimator calculated by SAKE is probabilistically guaranteed to be within an additive error from the exact value. Extensive evaluation experiments based on four real-world networks show that the proposed algorithm can estimate Katz centralities for partial vertices with low sampling rate, low computation time, and it works well in identifying high influence vertices in social networks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3134931523",
    "type": "article"
  },
  {
    "title": "Reducing Cumulative Errors of Incremental CP Decomposition in Dynamic Online Social Networks",
    "doi": "https://doi.org/10.1145/3441645",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Jingjing Wang; Wenjun Jiang; Kenli Li; Keqin Li",
    "corresponding_authors": "",
    "abstract": "CANDECOMP/PARAFAC (CP) decomposition is widely used in various online social network (OSN) applications. However, it is inefficient when dealing with massive and incremental data. Some incremental CP decomposition (ICP) methods have been proposed to improve the efficiency and process evolving data, by updating decomposition results according to the newly added data. The ICP methods are efficient, but inaccurate because of serious error accumulation caused by approximation in the incremental updating. To promote the wide use of ICP, we strive to reduce its cumulative errors while keeping high efficiency. We first differentiate all possible errors in ICP into two types: the cumulative reconstruction error and the prediction error. Next, we formulate two optimization problems for reducing the two errors. Then, we propose several restarting strategies to address the two problems. Finally, we test the effectiveness in three typical dynamic OSN applications. To the best of our knowledge, this is the first work on reducing the cumulative errors of the ICP methods in dynamic OSNs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3152937310",
    "type": "article"
  },
  {
    "title": "Heterogeneous Network Approach to Predict Individuals’ Mental Health",
    "doi": "https://doi.org/10.1145/3429446",
    "publication_date": "2021-04-09",
    "publication_year": 2021,
    "authors": "Shikang Liu; Fatemeh Vahedian; David Hachen; Omar Ližardo; Christian Poellabauer; Aaron Striegel; Tijana Milenković",
    "corresponding_authors": "",
    "abstract": "Depression and anxiety are critical public health issues affecting millions of people around the world. To identify individuals who are vulnerable to depression and anxiety, predictive models have been built that typically utilize data from one source. Unlike these traditional models, in this study, we leverage a rich heterogeneous dataset from the University of Notre Dame’s NetHealth study that collected individuals’ (student participants’) social interaction data via smartphones, health-related behavioral data via wearables (Fitbit), and trait data from surveys. To integrate the different types of information, we model the NetHealth data as a heterogeneous information network (HIN). Then, we redefine the problem of predicting individuals’ mental health conditions (depression or anxiety) in a novel manner, as applying to our HIN a popular paradigm of a recommender system (RS), which is typically used to predict the preference that a person would give to an item (e.g., a movie or book). In our case, the items are the individuals’ different mental health states. We evaluate four state-of-the-art RS approaches. Also, we model the prediction of individuals’ mental health as another problem type—that of node classification (NC) in our HIN, evaluating in the process four node features under logistic regression as a proof-of-concept classifier. We find that our RS and NC network methods produce more accurate predictions than a logistic regression model using the same NetHealth data in the traditional non-network fashion as well as a random-approach. Also, we find that the best of the considered RS approaches outperforms all considered NC approaches. This is the first study to integrate smartphone, wearable sensor, and survey data in a HIN manner and use RS or NC on the HIN to predict individuals’ mental health conditions.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3156487566",
    "type": "article"
  },
  {
    "title": "GGATB-LSTM: Grouping and Global Attention-based Time-aware Bidirectional LSTM Medical Treatment Behavior Prediction",
    "doi": "https://doi.org/10.1145/3441454",
    "publication_date": "2021-05-03",
    "publication_year": 2021,
    "authors": "Lin Cheng; Yuliang Shi; Kun Zhang; Xinjun Wang; Zhiyong Chen",
    "corresponding_authors": "",
    "abstract": "In China, with the continuous development of national health insurance policies, more and more people have joined the health insurance. How to accurately predict patients future medical treatment behavior becomes a hotspot issue. The biggest challenge in this issue is how to improve the prediction performance by modeling health insurance data with high-dimensional time characteristics. At present, most of the research is to solve this issue by using Recurrent Neural Networks (RNNs) to construct an overall prediction model for the medical visit sequences. However, RNNs can not effectively solve the long-term dependence, and RNNs ignores the importance of time interval of the medical visit sequence. Additionally, the global model may lose some important content to different groups. In order to solve these problems, we propose a Grouping and Global Attention based Time-aware Bidirectional Long Short-Term Memory (GGATB-LSTM) model to achieve medical treatment behavior prediction. The model first constructs a heterogeneous information network based on health insurance data, and uses a tensor CANDECOMP/PARAFAC decomposition method to achieve similarity grouping. In terms of group prediction, a global attention and time factor are introduced to extend the bidirectional LSTM. Finally, the proposed model is evaluated by using real dataset, and conclude that GGATB-LSTM is better than other methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3157997512",
    "type": "article"
  },
  {
    "title": "On Modeling Influence Maximization in Social Activity Networks under General Settings",
    "doi": "https://doi.org/10.1145/3451218",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Rui Wang; Yongkun Li; Shuai Lin; Hong Xie; Yinlong Xu; John C. S. Lui",
    "corresponding_authors": "",
    "abstract": "Finding the set of most influential users in online social networks (OSNs) to trigger the largest influence cascade is meaningful, e.g., companies may leverage the “word-of-mouth” effect to trigger a large cascade of purchases by offering free samples/discounts to those most influential users. This task is usually modeled as an influence maximization problem, and it has been widely studied in the past decade. However, considering that users in OSNs may participate in various online activities, e.g., joining discussion groups and commenting on same pages or products, influence diffusion through online activities becomes even more significant. In this article, we study the impact of online activities by formulating social-activity networks which contain both users and online activities, and thus induce two types of weighted edges, i.e., edges between users and edges between users and activities. To address the computation challenge, we define an influence centrality via random walks, and use the Monte Carlo framework to efficiently estimate the centrality. Furthermore, we develop a greedy-based algorithm with novel optimizations to find the most influential users for node recommendation. Experiments on real-world datasets show that our approach is very computationally efficient under different influence models, and also achieves larger influence spread by considering online activities.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3162213606",
    "type": "article"
  },
  {
    "title": "A Robust Reputation-Based Group Ranking System and Its Resistance to Bribery",
    "doi": "https://doi.org/10.1145/3462210",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "João Saúde; Guilherme Ramos; Ludovico Boratto; Carlos Caleiro",
    "corresponding_authors": "",
    "abstract": "The spread of online reviews and opinions and its growing influence on people’s behavior and decisions boosted the interest to extract meaningful information from this data deluge. Hence, crowdsourced ratings of products and services gained a critical role in business and governments. Current state-of-the-art solutions rank the items with an average of the ratings expressed for an item, with a consequent lack of personalization for the users, and the exposure to attacks and spamming/spurious users. Using these ratings to group users with similar preferences might be useful to present users with items that reflect their preferences and overcome those vulnerabilities. In this article, we propose a new reputation-based ranking system, utilizing multipartite rating subnetworks, which clusters users by their similarities using three measures, two of them based on Kolmogorov complexity. We also study its resistance to bribery and how to design optimal bribing strategies. Our system is novel in that it reflects the diversity of preferences by (possibly) assigning distinct rankings to the same item, for different groups of users. We prove the convergence and efficiency of the system. By testing it on synthetic and real data, we see that it copes better with spamming/spurious users, being more robust to attacks than state-of-the-art approaches. Also, by clustering users, the effect of bribery in the proposed multipartite ranking system is dimmed, comparing to the bipartite case.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3183971362",
    "type": "article"
  },
  {
    "title": "Establishing Smartphone User Behavior Model Based on Energy Consumption Data",
    "doi": "https://doi.org/10.1145/3461459",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Ming Ding; Tianyu Wang; Xudong Wang",
    "corresponding_authors": "",
    "abstract": "In smartphone data analysis, both energy consumption modeling and user behavior mining have been explored extensively, but the relationship between energy consumption and user behavior has been rarely studied. Such a relationship is explored over large-scale users in this article. Based on energy consumption data, where each users’ feature vector is represented by energy breakdown on hardware components of different apps, User Behavior Models (UBM) are established to capture user behavior patterns (i.e., app preference, usage time). The challenge lies in the high diversity of user behaviors (i.e., massive apps and usage ways), which leads to high dimension and dispersion of data. To overcome the challenge, three mechanisms are designed. First, to reduce the dimension, apps are ranked with the top ones identified as typical apps to represent all. Second, the dispersion is reduced by scaling each users’ feature vector with typical apps to unit ℓ 1 norm. The scaled vector becomes Usage Pattern, while the ℓ 1 norm of vector before scaling is treated as Usage Intensity. Third, the usage pattern is analyzed with a two-layer clustering approach to further reduce data dispersion. In the upper layer, each typical app is studied across its users with respect to hardware components to identify Typical Hardware Usage Patterns (THUP). In the lower layer, users are studied with respect to these THUPs to identify Typical App Usage Patterns (TAUP). The analytical results of these two layers are consolidated into Usage Pattern Models (UPM), and UBMs are finally established by a union of UPMs and Usage Intensity Distributions (UID). By carrying out experiments on energy consumption data from 18,308 distinct users over 10 days, 33 UBMs are extracted from training data. With the test data, it is proven that these UBMs cover 94% user behaviors and achieve up to 20% improvement in accuracy of energy representation, as compared with the baseline method, PCA. Besides, potential applications and implications of these UBMs are illustrated for smartphone manufacturers, app developers, network providers, and so on.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3185047352",
    "type": "article"
  },
  {
    "title": "Deciphering Feature Effects on Decision-Making in Ordinal Regression Problems: An Explainable Ordinal Factorization Model",
    "doi": "https://doi.org/10.1145/3487048",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Mengzhuo Guo; Zhongzhi Xu; Qingpeng Zhang; Xiuwu Liao; Jiapeng Liu",
    "corresponding_authors": "",
    "abstract": "Ordinal regression predicts the objects’ labels that exhibit a natural ordering, which is vital to decision-making problems such as credit scoring and clinical diagnosis. In these problems, the ability to explain how the individual features and their interactions affect the decisions is as critical as model performance. Unfortunately, the existing ordinal regression models in the machine learning community aim at improving prediction accuracy rather than explore explainability. To achieve high accuracy while explaining the relationships between the features and the predictions, we propose a new method for ordinal regression problems, namely the Explainable Ordinal Factorization Model (XOFM). XOFM uses piecewise linear functions to approximate the shape functions of individual features, and renders the pairwise features interaction effects as heat-maps. The proposed XOFM captures the nonlinearity in the main effects and ensures the interaction effects’ same flexibility. Therefore, the underlying model yields comparable performance while remaining explainable by explicitly describing the main and interaction effects. To address the potential sparsity problem caused by discretizing the whole feature scale into several sub-intervals, XOFM integrates the Factorization Machines (FMs) to factorize the model parameters. Comprehensive experiments with benchmark real-world and synthetic datasets demonstrate that the proposed XOFM leads to state-of-the-art prediction performance while preserving an easy-to-understand explainability.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3209063370",
    "type": "article"
  },
  {
    "title": "Behavioral Targeting",
    "doi": "https://doi.org/10.1145/1857947.1857949",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Ye Chen; Dmitry Pavlov; John Canny",
    "corresponding_authors": "",
    "abstract": "Behavioral targeting (BT) leverages historical user behavior to select the ads most relevant to users to display. The state-of-the-art of BT derives a linear Poisson regression model from fine-grained user behavioral data and predicts click-through rate (CTR) from user history. We designed and implemented a highly scalable and efficient solution to BT using Hadoop MapReduce framework. With our parallel algorithm and the resulting system, we can build above 450 BT-category models from the entire Yahoo’s user base within one day, the scale that one can not even imagine with prior systems. Moreover, our approach has yielded 20% CTR lift over the existing production system by leveraging the well-grounded probabilistic model fitted from a much larger training dataset. Specifically, our major contributions include: (1) A MapReduce statistical learning algorithm and implementation that achieve optimal data parallelism, task parallelism, and load balance in spite of the typically skewed distribution of domain data. (2) An in-place feature vector generation algorithm with strict linear-time complexity O ( n ) regardless of the granularity of sliding target window. (3) An in-memory caching scheme that significantly reduces the number of disk IOs to make large-scale learning practical. (4) Highly efficient data structures and sparse representations of models and data to enable fast model updates. We believe that our work makes significant contributions to solving large-scale machine learning problems of industrial relevance in general. Finally, we report comprehensive experimental results, using industrial proprietary codebase and datasets.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1964983104",
    "type": "article"
  },
  {
    "title": "Indexing Network Structure with Shortest-Path Trees",
    "doi": "https://doi.org/10.1145/1993077.1993079",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Marc Maier; Matthew J. Rattigan; David Jensen",
    "corresponding_authors": "",
    "abstract": "The ability to discover low-cost paths in networks has practical consequences for knowledge discovery and social network analysis tasks. Many analytic techniques for networks require finding low-cost paths, but exact methods for search become prohibitive for large networks, and data sets are steadily increasing in size. Short paths can be found efficiently by utilizing an index of network structure, which estimates network distances and enables rapid discovery of short paths. Through experiments on synthetic networks, we demonstrate that one such novel network structure index based on the shortest-path tree outperforms other previously proposed indices. We also show that it generalizes across arbitrarily weighted networks of various structures and densities, provides accurate estimates of distance, and has efficient time and space complexity. We present results on real data sets for several applications, including navigation, diameter estimation, centrality computation, and clustering---all made efficient by virtue of the network structure index.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2126595578",
    "type": "article"
  },
  {
    "title": "Ranking Causal Anomalies for System Fault Diagnosis via Temporal and Dynamical Analysis on Vanishing Correlations",
    "doi": "https://doi.org/10.1145/3046946",
    "publication_date": "2017-06-29",
    "publication_year": 2017,
    "authors": "Wei Cheng; Jingchao Ni; Kai Zhang; Haifeng Chen; Guofei Jiang; Yu Shi; Xiang Zhang; Wei Wang",
    "corresponding_authors": "",
    "abstract": "Detecting system anomalies is an important problem in many fields such as security, fault management, and industrial optimization. Recently, invariant network has shown to be powerful in characterizing complex system behaviours. In the invariant network, a node represents a system component and an edge indicates a stable, significant interaction between two components. Structures and evolutions of the invariance network, in particular the vanishing correlations, can shed important light on locating causal anomalies and performing diagnosis. However, existing approaches to detect causal anomalies with the invariant network often use the percentage of vanishing correlations to rank possible casual components, which have several limitations: (1) fault propagation in the network is ignored, (2) the root casual anomalies may not always be the nodes with a high percentage of vanishing correlations, (3) temporal patterns of vanishing correlations are not exploited for robust detection, and (4) prior knowledge on anomalous nodes are not exploited for (semi-)supervised detection. To address these limitations, in this article we propose a network diffusion based framework to identify significant causal anomalies and rank them. Our approach can effectively model fault propagation over the entire invariant network and can perform joint inference on both the structural and the time-evolving broken invariance patterns. As a result, it can locate high-confidence anomalies that are truly responsible for the vanishing correlations and can compensate for unstructured measurement noise in the system. Moreover, when the prior knowledge on the anomalous status of some nodes are available at certain time points, our approach is able to leverage them to further enhance the anomaly inference accuracy. When the prior knowledge is noisy, our approach also automatically learns reliable information and reduces impacts from noises. By performing extensive experiments on synthetic datasets, bank information system datasets, and coal plant cyber-physical system datasets, we demonstrate the effectiveness of our approach.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2731886076",
    "type": "article"
  },
  {
    "title": "A Randomized Rounding Algorithm for Sparse PCA",
    "doi": "https://doi.org/10.1145/3046948",
    "publication_date": "2017-04-14",
    "publication_year": 2017,
    "authors": "Kimon Fountoulakis; Abhisek Kundu; Eugenia-Maria Kontopoulou; Petros Drineas",
    "corresponding_authors": "",
    "abstract": "We present and analyze a simple, two-step algorithm to approximate the optimal solution of the sparse PCA problem. In the proposed approach, we first solve an ℓ 1 -penalized version of the NP-hard sparse PCA optimization problem and then we use a randomized rounding strategy to sparsify the resulting dense solution. Our main theoretical result guarantees an additive error approximation and provides a tradeoff between sparsity and accuracy. Extensive experimental evaluation indicates that the proposed approach is competitive in practice, even compared to state-of-the-art toolboxes such as Spasm.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2963917064",
    "type": "article"
  },
  {
    "title": "Supporting Exploratory Hypothesis Testing and Analysis",
    "doi": "https://doi.org/10.1145/2701430",
    "publication_date": "2015-06-01",
    "publication_year": 2015,
    "authors": "Guimei Liu; Haojun Zhang; Mengling Feng; Limsoon Wong; See-Kiong Ng",
    "corresponding_authors": "",
    "abstract": "Conventional hypothesis testing is carried out in a hypothesis-driven manner. A scientist must first formulate a hypothesis based on what he or she sees and then devise a variety of experiments to test it. Given the rapid growth of data, it has become virtually impossible for a person to manually inspect all data to find all of the interesting hypotheses for testing. In this article, we propose and develop a data-driven framework for automatic hypothesis testing and analysis. We define a hypothesis as a comparison between two or more subpopulations. We find subpopulations for comparison using frequent pattern mining techniques and then pair them up for statistical hypothesis testing. We also generate additional information for further analysis of the hypotheses that are deemed significant. The number of hypotheses generated can be very large, and many of them are very similar. We develop algorithms to remove redundant hypotheses and present a succinct set of significant hypotheses to users. We conducted a set of experiments to show the efficiency and effectiveness of the proposed algorithms. The results show that our system can help users (1) identify significant hypotheses efficiently, (2) isolate the reasons behind significant hypotheses efficiently, and (3) find confounding factors that form Simpson’s paradoxes with discovered significant hypotheses.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2206815832",
    "type": "article"
  },
  {
    "title": "Learnable Graph-Regularization for Matrix Decomposition",
    "doi": "https://doi.org/10.1145/3544781",
    "publication_date": "2022-06-24",
    "publication_year": 2022,
    "authors": "Penglong Zhai; Shihua Zhang",
    "corresponding_authors": "",
    "abstract": "Low-rank approximation models of data matrices have become important machine learning and data mining tools in many fields, including computer vision, text mining, bioinformatics, and many others. They allow for embedding high-dimensional data into low-dimensional spaces, which mitigates the effects of noise and uncovers latent relations. In order to make the learned representations inherit the structures in the original data, graph-regularization terms are often added to the loss function. However, the prior graph construction often fails to reflect the true network connectivity and the intrinsic relationships. In addition, many graph-regularized methods fail to take the dual spaces into account. Probabilistic models are often used to model the distribution of the representations, but most of previous methods often assume that the hidden variables are independent and identically distributed for simplicity. To this end, we propose a learnable graph-regularization model for matrix decomposition (LGMD), which builds a bridge between graph-regularized methods and probabilistic matrix decomposition models for the first time. LGMD incorporates two graphical structures (i.e., two precision matrices) learned in an iterative manner via sparse precision matrix estimation and is more robust to noise and missing entries. Extensive numerical results and comparison with competing methods demonstrate its effectiveness.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3093231382",
    "type": "article"
  },
  {
    "title": "DexDeepFM: Ensemble Diversity Enhanced Extreme Deep Factorization Machine Model",
    "doi": "https://doi.org/10.1145/3505272",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Ling Chen; Hongyu Shi",
    "corresponding_authors": "",
    "abstract": "Predicting user positive response (e.g., purchases and clicks) probability is a critical task in Web applications. To identify predictive features from raw data, the state-of-the-art extreme deep factorization machine model (xDeepFM) introduces a new interaction network to leverage feature interactions at the vector-wise level explicitly. However, since each hidden layer in the interaction network is a collection of feature maps, it can be viewed essentially as an ensemble of different feature maps. In this case, only using a single objective to minimize the prediction loss may lead to overfitting and generate correlated errors. In this article, an ensemble diversity enhanced extreme deep factorization machine model (DexDeepFM) is proposed, which designs the ensemble diversity measure in each hidden layer and considers both ensemble diversity and prediction accuracy in the objective function. In addition, the attention mechanism is introduced to discriminate the importance of ensemble diversity measures with different feature interaction orders. Extensive experiments on three public real-world datasets are conducted to show the effectiveness of the proposed model.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3150492821",
    "type": "article"
  },
  {
    "title": "Factorization of Binary Matrices: Rank Relations, Uniqueness and Model Selection of Boolean Decomposition",
    "doi": "https://doi.org/10.1145/3522594",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Derek DeSantis; Erik Skau; Duc P. Truong; Boian S. Alexandrov",
    "corresponding_authors": "",
    "abstract": "The application of binary matrices are numerous. Representing a matrix as a mixture of a small collection of latent vectors via low-rank decomposition is often seen as an advantageous method to interpret and analyze data. In this work, we examine the factorizations of binary matrices using standard arithmetic (real and nonnegative) and logical operations (Boolean and ℤ 2 ). We examine the relationships between the different ranks, and discuss when factorization is unique. In particular, we characterize when a Boolean factorization X = W ∧ H has a unique W , a unique H (for a fixed W ), and when both W and H are unique, given a rank constraint. We introduce a method for robust Boolean model selection, called BMF k , and show on numerical examples that BMF k not only accurately determines the correct number of Boolean latent features but reconstruct the pre-determined factors accurately.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3208070302",
    "type": "article"
  },
  {
    "title": "Distributed Triangle Approximately Counting Algorithms in Simple Graph Stream",
    "doi": "https://doi.org/10.1145/3494562",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Xu Yang; Chao Song; Mengdi Yu; Jiqing Gu; Ming Liu",
    "corresponding_authors": "",
    "abstract": "Recently, the counting algorithm of local topology structures, such as triangles, has been widely used in social network analysis, recommendation systems, user portraits and other fields. At present, the problem of counting global and local triangles in a graph stream has been widely studied, and numerous triangle counting steaming algorithms have emerged. To improve the throughput and scalability of streaming algorithms, many researches of distributed streaming algorithms on multiple machines are studied. In this article, we first propose a framework of distributed streaming algorithm based on the Master-Worker-Aggregator architecture. The two core parts of this framework are an edge distribution strategy, which plays a key role to affect the performance, including the communication overhead and workload balance, and aggregation method, which is critical to obtain the unbiased estimations of the global and local triangle counts in a graph stream. Then, we extend the state-of-the-art centralized algorithm TRIÈST into four distributed algorithms under our framework. Compared to their competitors, experimental results show that DVHT-i is excellent in accuracy and speed, performing better than the best existing distributed streaming algorithm. DEHT-b is the fastest algorithm and has the least communication overhead. What’s more, it almost achieves absolute workload balance.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4206034310",
    "type": "article"
  },
  {
    "title": "Distance-Preserving Embedding Adaptive Bipartite Graph Multi-View Learning with Application to Multi-Label Classification",
    "doi": "https://doi.org/10.1145/3537900",
    "publication_date": "2022-05-18",
    "publication_year": 2022,
    "authors": "Xun Lu; Songhe Feng; Gengyu Lyu; Yi Jin; Congyan Lang",
    "corresponding_authors": "",
    "abstract": "Graph-based multi-view learning has attracted much attention due to the efficacy of fusing the information from different views. However, most of them exhibit high computational complexity. We propose an anchor-based bipartite graph embedding approach to accelerate the learning process. Specifically, different from existing anchor-based methods where anchors are obtained from key samples by clustering or weighted averaging strategies, in this article, the anchors are learned in a principled fashion which aims at constructing a distance-preserving embedding for each view from samples to their representations, whose elements are the weights of the edges linking corresponding samples and anchors. In addition, the consistency among different views can be explored by imposing a low-rank constraint on the concatenated embedding representations. We further design a concise yet effective feature collinearity guided feature selection scheme to learn tight multi-label classifiers. The objective function is optimized in an alternating optimization fashion. Both theoretical analysis and experimental results on different multi-label image datasets verify the effectiveness and efficiency of the proposed method.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4280510245",
    "type": "article"
  },
  {
    "title": "Scheduling Hyperparameters to Improve Generalization: From Centralized SGD to Asynchronous SGD",
    "doi": "https://doi.org/10.1145/3544782",
    "publication_date": "2022-06-22",
    "publication_year": 2022,
    "authors": "Jianhui Sun; Ying Yang; Guangxu Xun; Aidong Zhang",
    "corresponding_authors": "",
    "abstract": "This paper 1 studies how to schedule hyperparameters to improve generalization of both centralized single-machine stochastic gradient descent (SGD) and distributed asynchronous SGD (ASGD). SGD augmented with momentum variants (e.g., heavy ball momentum (SHB) and Nesterov’s accelerated gradient (NAG)) has been the default optimizer for many tasks, in both centralized and distributed environments. However, many advanced momentum variants, despite empirical advantage over classical SHB/NAG, introduce extra hyperparameters to tune. The error-prone tuning is the main barrier for AutoML. Centralized SGD : We first focus on centralized single-machine SGD and show how to efficiently schedule the hyperparameters of a large class of momentum variants to improve generalization. We propose a unified framework called multistage quasi-hyperbolic momentum (Multistage QHM), which covers a large family of momentum variants as its special cases (e.g. vanilla SGD/SHB/NAG). Existing works mainly focus on only scheduling learning rate α ’s decay, while multistage QHM allows additional varying hyperparameters (e.g., momentum factor), and demonstrates better generalization than only tuning α . We show the convergence of multistage QHM for general nonconvex objectives. Distributed SGD : We then extend our theory to distributed asynchronous SGD (ASGD), in which a parameter server distributes data batches to several worker machines and updates parameters via aggregating batch gradients from workers. We quantify the asynchrony between different workers (i.e., gradient staleness), model the dynamics of asynchronous iterations via a stochastic differential equation (SDE), and then derive a PAC-Bayesian generalization bound for ASGD. As a byproduct, we show how a moderately large learning rate helps ASGD to generalize better. Our tuning strategies have rigorous justifications rather than a blind trial-and-error as we theoretically prove why our tuning strategies could decrease our derived generalization errors in both cases. Our strategies simplify the tuning process and beat competitive optimizers in test accuracy empirically. Our codes are publicly available https://github.com/jsycsjh/centralized-asynchronous-tuning.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4283315879",
    "type": "article"
  },
  {
    "title": "Random Walk Sampling in Social Networks Involving Private Nodes",
    "doi": "https://doi.org/10.1145/3561388",
    "publication_date": "2022-09-06",
    "publication_year": 2022,
    "authors": "Kazuki Nakajima; Kazuyuki Shudo",
    "corresponding_authors": "",
    "abstract": "Analysis of social networks with limited data access is challenging for third parties. To address this challenge, a number of studies have developed algorithms that estimate properties of social networks via a simple random walk. However, most existing algorithms do not assume private nodes that do not publish their neighbors' data when they are queried in empirical social networks. Here we propose a practical framework for estimating properties via random walk-based sampling in social networks involving private nodes. First, we develop a sampling algorithm by extending a simple random walk to the case of social networks involving private nodes. Then, we propose estimators with reduced biases induced by private nodes for the network size, average degree, and density of the node label. Our results show that the proposed estimators reduce biases induced by private nodes in the existing estimators by up to 92.6% on social network datasets involving private nodes.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4294732833",
    "type": "article"
  },
  {
    "title": "Efficient Node PageRank Improvement via Link-building using Geometric Deep Learning",
    "doi": "https://doi.org/10.1145/3551642",
    "publication_date": "2022-09-19",
    "publication_year": 2022,
    "authors": "Vincenza Carchiolo; Marco Grassia; Alessandro Longheu; Michele Malgeri; Giuseppe Mangioni",
    "corresponding_authors": "",
    "abstract": "Centrality is a relevant topic in the field of network research, due to its various theoretical and practical implications. In general, all centrality metrics aim at measuring the importance of nodes (according to some definition of importance), and such importance scores are used to rank the nodes in the network, therefore the rank improvement is a strictly related topic. In a given network, the rank improvement is achieved by establishing new links, therefore the question shifts to which and how many links should be collected to get a desired rank. This problem, also known as link-building has been shown to be NP-hard, and most heuristics developed failed in obtaining good performance with acceptable computational complexity. In this article, we present LB–GDM , a novel approach that leverages Geometric Deep Learning to tackle the link-building problem. To validate our proposal, 31 real-world networks were considered; tests show that LB–GDM performs significantly better than the state-of-the-art heuristics, while having a comparable or even lower computational complexity, which allows it to scale well even to large networks.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4296347353",
    "type": "article"
  },
  {
    "title": "Crowdsourcing Truth Inference via Reliability-Driven Multi-View Graph Embedding",
    "doi": "https://doi.org/10.1145/3565576",
    "publication_date": "2022-10-04",
    "publication_year": 2022,
    "authors": "Gongqing Wu; Xingrui Zhuo; Xianyu Bao; Xuegang Hu; Richang Hong; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Crowdsourcing truth inference aims to assign a correct answer to each task from candidate answers that are provided by crowdsourced workers. A common approach is to generate workers’ reliabilities to represent the quality of answers. Although crowdsourced triples can be converted into various crowdsourced relationships, the available related methods are not effective in capturing these relationships to alleviate the harm to inference that is caused by conflicting answers. In this research, we propose a Re liability-driven M ulti-view G raph E mbedding framework for T ruth i nference (TiReMGE), which explores multiple crowdsourced relationships by organically integrating worker reliabilities into a graph space that is constructed from crowdsourced triples. Specifically, to create an interactive environment, we propose a reliability-driven initialization criterion for initializing vectors of tasks and workers as interactive carriers of reliabilities. From the perspective of multiple crowdsourced relationships, a multi-view graph embedding framework is proposed for reliability information interaction on a task-worker graph, which encodes latent crowdsourced relationships into vectors of workers and tasks for reliability update and truth inference. A heritable reliability updating method based on the Lagrange multiplier method is proposed to obtain reliabilities that match the quality of workers for interaction by a novel constraint law. Our ultimate goal is to minimize the Euclidean distance between the encoded task vector and the answer that is provided by a worker with high reliability. Extensive experimental results on nine real-world datasets demonstrate that TiReMGE significantly outperforms the nine state-of-the-art baselines.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4301395530",
    "type": "article"
  },
  {
    "title": "Triadic Closure Sensitive Influence Maximization",
    "doi": "https://doi.org/10.1145/3573011",
    "publication_date": "2022-11-30",
    "publication_year": 2022,
    "authors": "Jie Yang; Zhixiao Wang; Xiaobin Rui; Yahui Chai; Philip S. Yu; Lichao Sun",
    "corresponding_authors": "",
    "abstract": "The influence are not linked to any footnote in the text. Please check and suggest. maximization problem aims at selecting the k most influential nodes (i.e., seed nodes) from a social network, where the nodes can maximize the number of influenced nodes activated by a certain propagation model. However, the widely used Independent Cascade model shares the same propagation probability among substantial adjacent node pairs, which is too idealistic and unreasonable in practice. In addition, most heuristic algorithms for influence maximization need to update the expected influence of the remaining nodes in the seed selection process, resulting in high computation cost. To address these non-trivial problems, we propose a novel edge propagation probability calculation method. The method first utilizes the triadic closure structure of social networks to precisely measure the closeness between nodes and assigns different propagation probabilities to each edge, deriving a Triadic Closure-based Independent Cascade (TC-IC) model. Then, we further propose a heuristic influence maximization algorithm named Triadic Closure-based Influence Maximization (TC-IM). The algorithm evaluates the expected influence of a node by integrating the triadic closure weighted propagation probability and the triadic closure weighted degree. Especially, in the seed selection process, only the most influential node that has not been updated in the current round needs to be updated, which significantly improves the efficiency. Besides, we further provide theoretical proofs to guarantee the correctness of this updating strategy. Experimental results on nine real datasets and three propagation models demonstrate that: (1) The TC-IC model can set a proper propagation probability for each node pair, where the IM algorithms could easily identify influential nodes; (2) The TC-IM algorithm can significantly reduce the complexity through an efficient updating strategy with a comparable influence spread to the approximation IM algorithms; (3) Besides, the TC-IM algorithm also exhibits stable performance under other IC models including UIC and WIC, exhibiting good stability and generality.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4311012105",
    "type": "article"
  },
  {
    "title": "<scp>Bavarian</scp>: Betweenness Centrality Approximation with Variance-aware Rademacher Averages",
    "doi": "https://doi.org/10.1145/3577021",
    "publication_date": "2022-12-20",
    "publication_year": 2022,
    "authors": "Cyrus Cousins; Chloe Wohlgemuth; Matteo Riondato",
    "corresponding_authors": "",
    "abstract": "“[A]llain Gersten, Hopfen, und Wasser” — 1516 Reinheitsgebot We present Bavarian , a collection of sampling-based algorithms for approximating the Betweenness Centrality (BC) of all vertices in a graph. Our algorithms use Monte-Carlo Empirical Rademacher Averages (MCERAs), a concept from statistical learning theory, to efficiently compute tight bounds on the maximum deviation of the estimates from the exact values. The MCERAs provide a sample-dependent approximation guarantee much stronger than the state-of-the-art, thanks to its use of variance-aware probabilistic tail bounds. The flexibility of the MCERAs allows us to introduce a unifying framework that can be instantiated with existing sampling-based estimators of BC, thus allowing a fair comparison between them, decoupled from the sample-complexity results with which they were originally introduced. Additionally, we prove novel sample-complexity results showing that, for all estimators, the sample size sufficient to achieve a desired approximation guarantee depends on the vertex-diameter of the graph, an easy-to-bound characteristic quantity. We also show progressive-sampling algorithms and extensions to other centrality measures, such as percolation centrality. Our extensive experimental evaluation of Bavarian shows the improvement over the state-of-the-art made possible by the MCERAs (2–4× reduction in the error bound), and it allows us to assess the different trade-offs between sample size and accuracy guarantees offered by the different estimators.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4312037431",
    "type": "article"
  },
  {
    "title": "Fairness of Information Flow in Social Networks",
    "doi": "https://doi.org/10.1145/3578268",
    "publication_date": "2022-12-23",
    "publication_year": 2022,
    "authors": "Zeinab S. Jalali; Qilan Chen; Shwetha Koushik Manchinahalli Srikanta; Weixiang Wang; Myunghwan Kim; Hema Raghavan; Sucheta Soundarajan",
    "corresponding_authors": "",
    "abstract": "Social networks form a major parts of people’s lives, and individuals often make important life decisions based on information that spreads through these networks. For this reason, it is important to know whether individuals from different protected groups have equal access to information flowing through a network. In this article, we define the Information Unfairness (IUF) metric, which quantifies inequality in access to information across protected groups. We then introduce MinIUF , an algorithm for reducing inequalities in information flow by adding edges to the network. Finally, we provide an in-depth analysis of information flow with respect to an attribute of interest, such as gender, across different types of networks to evaluate whether the structure of these networks allows groups to equally access information flowing in the network. Moreover, we investigate the causes of unfairness in such networks and how it can be improved.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4312179331",
    "type": "article"
  },
  {
    "title": "GEO: A Computational Design Framework for Automotive Exterior Facelift",
    "doi": "https://doi.org/10.1145/3578521",
    "publication_date": "2022-12-27",
    "publication_year": 2022,
    "authors": "Jingmin Huang; Bowei Chen; Zhi Yan; Iadh Ounis; Jun Wang",
    "corresponding_authors": "",
    "abstract": "Exterior facelift has become an effective method for automakers to boost the consumers’ interest in an existing car model before it is redesigned. To support the automotive facelift design process, this study develops a novel computational framework – Generator, Evaluator, Optimiser (GEO) , which comprises three components: a StyleGAN2-based design generator that creates different facelift designs; a convolutional neural network (CNN) -based evaluator that assesses designs from the aesthetics perspective; and a recurrent neural network (RNN) -based decision optimiser that selects designs to maximise the predicted profit of the targeted car model over time. We validate the GEO framework in experiments with real-world datasets and describe some resulting managerial implications for automotive facelift. Our study makes both methodological and application contributions. First, the generator’s mapping network and projection methods are carefully tailored to facelift where only minor changes are performed without affecting the family signature of the automobile brands. Second, two evaluation metrics are proposed to assess the generated designs. Third, profit maximisation is taken into account in the design selection. From a high-level perspective, our study contributes to the recent use of machine learning and data mining in marketing and design studies. To the best of our knowledge, this is the first study that uses deep generative models for automotive regional design upgrading and that provides an end-to-end decision-support solution for automakers and designers.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4313250809",
    "type": "article"
  },
  {
    "title": "Criterion-based Heterogeneous Collaborative Filtering for Multi-behavior Implicit Recommendation",
    "doi": "https://doi.org/10.1145/3611310",
    "publication_date": "2023-07-27",
    "publication_year": 2023,
    "authors": "Xiao Luo; Daqing Wu; Yiyang Gu; Chong Chen; Luchen Liu; Jinwen Ma; Ming Zhang; Minghua Deng; Jianqiang Huang; Xian‐Sheng Hua",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed the explosive growth of interaction behaviors in multimedia information systems, where multi-behavior recommender systems have received increasing attention by leveraging data from various auxiliary behaviors such as tip and collect. Among various multi-behavior recommendation methods, non-sampling methods have shown superiority over negative sampling methods. However, two observations are usually ignored in existing state-of-the-art non-sampling methods based on binary regression: (1) users have different preference strengths for different items, so they cannot be measured simply by binary implicit data; (2) the dependency across multiple behaviors varies for different users and items. To tackle the above issue, we propose a novel non-sampling learning framework named C riterion-guided H eterogeneous C ollaborative F iltering (CHCF). CHCF introduces both upper and lower thresholds to indicate selection criteria, which will guide user preference learning. Besides, CHCF integrates criterion learning and user preference learning into a unified framework, which can be trained jointly for the interaction prediction of the target behavior. We further theoretically demonstrate that the optimization of Collaborative Metric Learning can be approximately achieved by the CHCF learning framework in a non-sampling form effectively. Extensive experiments on three real-world datasets show the effectiveness of CHCF in heterogeneous scenarios.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4287169361",
    "type": "article"
  },
  {
    "title": "Evolving Social Media Background Representation with Frequency Weights and Co-Occurrence Graphs",
    "doi": "https://doi.org/10.1145/3585389",
    "publication_date": "2023-02-24",
    "publication_year": 2023,
    "authors": "Yihong Zhang; Xiu Susie Fang; Takahiro Hara",
    "corresponding_authors": "",
    "abstract": "Social media as a background information source has been utilized in many practical computational tasks, such as stock price prediction, epidemic tracking, and product recommendation. However, proper representation of an evolving social media background is still in an early research stage. In this article, we propose a representation method that considers temporal novelties as well as the fine details of word inter-dependencies. Our method is based on the tf-idf and graph embedding techniques. The proposed method has superiority over other representation methods because it takes the advantage of both the temporal aspect of tf-idf and the semantic aspect of graph embeddings. We compare our method with a variety of baselines in two practical application scenarios using real-world data. In tweet popularity prediction, our representation achieves 5.7% less error and 12.8% higher correlation compared to the best baseline. In e-commerce product recommendation, our representation achieves 17% higher hit-rate and 20% higher NDCG compared to the best baseline.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4321786095",
    "type": "article"
  },
  {
    "title": "Causal Discovery via Causal Star Graphs",
    "doi": "https://doi.org/10.1145/3586997",
    "publication_date": "2023-03-06",
    "publication_year": 2023,
    "authors": "Boxiang Zhao; Shuliang Wang; Lianhua Chi; Qi Li; Xiaojia Liu; Jing Geng",
    "corresponding_authors": "",
    "abstract": "Discovering causal relationships among observed variables is an important research focus in data mining. Existing causal discovery approaches are mainly based on constraint-based methods and functional causal models (FCMs). However, the constraint-based method cannot identify the Markov equivalence class and the functional causal models cannot identify the complex interrelationships when multiple variables affect one variable. To address the two aforementioned problems, we propose a new graph structure Causal Star Graph (CSG) and a corresponding framework Causal Discovery via Causal Star Graphs (CD-CSG) to divide a causal directed acyclic graph into multiple CSGs for causal discovery. In this framework, we also propose a generalized learning in CSGs based on a variational approach to learn the representative intermediate variable of CSG’s non-central variables. Through the generalized learning in CSGs, the asymmetry in the forward and backward model of CD-CSG can be found to identify the causal directions in the directed acyclic graphs. We further divide the CSGs into three categories and provide the causal identification principle under each category in our proposed framework. Experiments using synthetic data show that the causal relationships between variables can be effectively identified with CD-CSG and the accuracy of CD-CSG is higher than the best existing model. By applying CD-CSG to real-world data, our proposed method can greatly augment the applicability and effectiveness of causal discovery.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4323312523",
    "type": "article"
  },
  {
    "title": "Robust Influence Maximization Under Both Aleatory and Epistemic Uncertainty",
    "doi": "https://doi.org/10.1145/3587100",
    "publication_date": "2023-03-07",
    "publication_year": 2023,
    "authors": "Tarun Kumer Biswas; Alireza Abbasi; Ripon K. Chakrabortty",
    "corresponding_authors": "",
    "abstract": "Uncertainty is ubiquitous in almost every real-life optimization problem, which must be effectively managed to get a robust outcome. This is also true for the Influence Maximization (IM) problem, which entails locating a set of influential users within a social network. However, most of the existing IM approaches have overlooked the uncertain factors in finding the optimal solution, which often leads to subpar performance in reality. A few recent studies have considered only the epistemic uncertainty (i.e., arises from the imprecise data), while ignoring completely the aleatory uncertainty (i.e., arises from natural or physical variability). In this article, we propose a formulation and a novel algorithm for the Robust Influence Maximization (RIM) problem under both types of uncertainties. First, we develop a robust influence spread function under aleatory uncertainty that, in contrast to the existing IM theory, is no longer monotone and submodular. Thereafter, we expand our RIM formulation to incorporate epistemic uncertainty aiming to maximize the robust ratio between the selected worst-case solution and the best-case optimal solution, adopting a conservative approach. Furthermore, using a chance-constraint-based method, we investigated feasibility robustness by accounting for the uncertainties related to constraint functions. Finally, an Evolutionary Algorithm (named EA-RIM) is designed to solve the proposed formulation of the RIM problem. Experimental evaluation results on four empirical datasets show that our proposed formulation and algorithm are more effective in dealing with uncertainties and finding an optimal solution for the RIM problem.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4323361312",
    "type": "article"
  },
  {
    "title": "Dual-aware Domain Mining and Cross-aware Supervision for Weakly-supervised Semantic Segmentation",
    "doi": "https://doi.org/10.1145/3589343",
    "publication_date": "2023-03-25",
    "publication_year": 2023,
    "authors": "Yuhui Guo; Xun Liang; Bo Wu; Xiangping Zheng; Xuan Zhang",
    "corresponding_authors": "",
    "abstract": "Weakly Supervised Semantic Segmentation with image-level annotation uses localization maps from the classifier to generate pseudo labels. However, such localization maps focus only on sparse salient object regions, it is difficult to generate high-quality segmentation labels, which deviates from the requirement of semantic segmentation. To address this issue, we propose a dual-aware domain mining and cross-aware supervision (DDMCAS) method for weakly-supervised semantic segmentation. Specifically, we propose a dual-aware domain mining (DDM) module consisting of graph-based global reasoning unit and salient-region extension controller, which produces dense localization maps by exploring object features in salient regions and adjacent non-salient regions simultaneously. In order to further bridge the gap between salient regions and adjacent non-salient regions to generate more refined localization maps, we propose a cross-aware supervision (CAS) strategy to recover missing parts of the target objects and enhance weak attention in adjacent non-salient regions, leading to pseudo labels of higher quality for training the segmentation network. Based on the generated pseudo-labels, extensive experiments on PASCAL VOC 2012 dataset demonstrate that our method outperforms state-of-the-art methods using image-level labels for weakly supervised semantic segmentation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4360946190",
    "type": "article"
  },
  {
    "title": "Urban Knowledge Graph Aided Mobile User Profiling",
    "doi": "https://doi.org/10.1145/3596604",
    "publication_date": "2023-07-03",
    "publication_year": 2023,
    "authors": "Yu Liu; Zhilun Zhou; Yong Li; Depeng Jin",
    "corresponding_authors": "",
    "abstract": "Nowadays, the explosive growth of personalized web applications and the rapid development of artificial intelligence technology have flourished the recent research on mobile user profiling, i.e., inferring the user profile from mobile behavioral data. Particularly, existing studies mainly follow the data-driven paradigm to develop feature engineering and representation learning on such data, which however suffer from the robustness issue, i.e., generalizing poorly across datasets and profiles without considering semantic knowledge therein. In comparison, the rising knowledge-driven paradigm built upon the knowledge graph (KG) offers a potential solution to mitigate such weakness. Therefore, in this article, we propose a Knowledge Graph aided framework for Mobile User Profiling (KG-MUP). Specifically, to distil semantic knowledge among data, we firstly construct an urban knowledge graph (UrbanKG) with domain entities like users, regions, point of interests (POIs), and so on. identified, as well as semantic relations for home, workplace, spatiality, and so on. extracted. Moreover, we leverage tensor decomposition and graph neural network to obtain knowledgeable user representations from UrbanKG. In addition, we introduce several customized features to quantify individual mobility characteristics for mobile user profiling. Extensive experiments on three real-world mobility datasets demonstrate that KG-MUP achieves state-of-the-art performance on user profile inference tasks. Moreover, further results also reveal the importance of various semantic knowledge to user profile inference, which provides meaningful insights on user modeling with mobile behavioral data.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4382938621",
    "type": "article"
  },
  {
    "title": "Multi-Label Quantification",
    "doi": "https://doi.org/10.1145/3606264",
    "publication_date": "2023-07-04",
    "publication_year": 2023,
    "authors": "Alejandro Moreo; Francisco Manuel; Fabrizio Sebastiani",
    "corresponding_authors": "",
    "abstract": "Quantification, variously called supervised prevalence estimation or learning to quantify , is the supervised learning task of generating predictors of the relative frequencies (a.k.a. prevalence values ) of the classes of interest in unlabelled data samples. While many quantification methods have been proposed in the past for binary problems and, to a lesser extent, single-label multiclass problems, the multi-label setting (i.e., the scenario in which the classes of interest are not mutually exclusive) remains by and large unexplored. A straightforward solution to the multi-label quantification problem could simply consist of recasting the problem as a set of independent binary quantification problems. Such a solution is simple but naïve, since the independence assumption upon which it rests is, in most cases, not satisfied. In these cases, knowing the relative frequency of one class could be of help in determining the prevalence of other related classes. We propose the first truly multi-label quantification methods, i.e., methods for inferring estimators of class prevalence values that strive to leverage the stochastic dependencies among the classes of interest in order to predict their relative frequencies more accurately. We show empirical evidence that natively multi-label solutions outperform the naïve approaches by a large margin. The code to reproduce all our experiments is available online.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4383102863",
    "type": "article"
  },
  {
    "title": "Efficient Density-peaks Clustering Algorithms on Static and Dynamic Data in Euclidean Space",
    "doi": "https://doi.org/10.1145/3607873",
    "publication_date": "2023-07-07",
    "publication_year": 2023,
    "authors": "Daichi Amagata; Takahiro Hara",
    "corresponding_authors": "",
    "abstract": "Clustering multi-dimensional points is a fundamental task in many fields, and density-based clustering supports many applications because it can discover clusters of arbitrary shapes. This article addresses the problem of Density-Peaks Clustering (DPC) in Euclidean space. DPC already has many applications, but its straightforward implementation incurs O ( n 2 ) time, where n is the number of points, thereby does not scale to large datasets. To enable DPC on large datasets, we first propose empirically efficient exact DPC algorithm, Ex-DPC. Although this algorithm is much faster than the straightforward implementation, it still suffers from O ( n 2 ) time theoretically. We hence propose a new exact algorithm, Ex-DPC++, that runs in o ( n 2 ) time. We accelerate their efficiencies by leveraging multi-threading. Moreover, real-world datasets may have arbitrary updates (point insertions and deletions). It is hence important to support efficient cluster updates. To this end, we propose D-DPC for fully dynamic DPC. We conduct extensive experiments using real datasets, and our experimental results demonstrate that our algorithms are efficient and scalable.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4383534265",
    "type": "article"
  },
  {
    "title": "Self-Supervised Dynamic Graph Representation Learning via Temporal Subgraph Contrast",
    "doi": "https://doi.org/10.1145/3612931",
    "publication_date": "2023-08-07",
    "publication_year": 2023,
    "authors": "Kejia Chen; Linsong Liu; Linpu Jiang; Jingqiang Chen",
    "corresponding_authors": "",
    "abstract": "Self-supervised learning on graphs has recently drawn a lot of attention due to its independence from labels and its robustness in representation. Current studies on this topic mainly use static information such as graph structures but cannot well capture dynamic information such as timestamps of edges. Realistic graphs are often dynamic, which means the interaction between nodes occurs at a specific time. This article proposes a self-supervised dynamic graph representation learning framework DySubC, which defines a temporal subgraph contrastive learning task to simultaneously learn the structural and evolutional features of a dynamic graph. Specifically, a novel temporal subgraph sampling strategy is firstly proposed, which takes each node of the dynamic graph as the central node and uses both neighborhood structures and edge timestamps to sample the corresponding temporal subgraph. The subgraph representation function is then designed according to the influence of neighborhood nodes on the central node after encoding the nodes in each subgraph. Finally, the structural and temporal contrastive loss are defined to maximize the mutual information between node representation and temporal subgraph representation. Experiments on five real-world datasets demonstrate that (1) DySubC performs better than the related baselines including two graph contrastive learning models and five dynamic graph representation learning models, especially in the link prediction task, and (2) the use of temporal information cannot only sample more effective subgraphs, but also learn better representation by temporal contrastive loss.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385636677",
    "type": "article"
  },
  {
    "title": "Feature Selection for Efficient Local-to-global Bayesian Network Structure Learning",
    "doi": "https://doi.org/10.1145/3624479",
    "publication_date": "2023-09-19",
    "publication_year": 2023,
    "authors": "Kui Yu; Zhaolong Ling; Lin Liu; Peipei Li; Hao Wang; Jiuyong Li",
    "corresponding_authors": "",
    "abstract": "Local-to-global learning approach plays an essential role in Bayesian network (BN) structure learning. Existing local-to-global learning algorithms first construct the skeleton of a DAG (directed acyclic graph) by learning the MB (Markov blanket) or PC (parents and children) of each variable in a dataset, then orient edges in the skeleton. However, existing MB or PC learning methods are often computationally expensive especially with a large-sized BN, resulting in inefficient local-to-global learning algorithms. To tackle the problem, in this article, we link feature selection with local BN structure learning and develop an efficient local-to-global learning approach using filtering feature selection. Specifically, we first analyze the rationale of the well-known Minimum-Redundancy and Maximum-Relevance (MRMR) feature selection approach for learning a PC set of a variable. Based on the analysis, we propose an efficient F2SL (feature selection-based structure learning) approach to local-to-global BN structure learning. The F2SL approach first employs the MRMR approach to learn the skeleton of a DAG, then orients edges in the skeleton. Employing independence tests or score functions for orienting edges, we instantiate the F2SL approach into two new algorithms, F2SL-c (using independence tests) and F2SL-s (using score functions). Compared to the state-of-the-art local-to-global BN learning algorithms, the experiments validated that the proposed algorithms in this article are more efficient and provide competitive structure learning quality than the compared algorithms.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386845186",
    "type": "article"
  },
  {
    "title": "Co-Training-Teaching: A Robust Semi-Supervised Framework for Review-Aware Rating Regression",
    "doi": "https://doi.org/10.1145/3625391",
    "publication_date": "2023-09-26",
    "publication_year": 2023,
    "authors": "Xiangkui Lu; Jun Wu; Junheng Huang; Fangyuan Luo; Jianbo Yuan",
    "corresponding_authors": "",
    "abstract": "Review-aware Rating Regression (RaRR) suffers the severe challenge of extreme data sparsity as the multi-modality interactions of ratings accompanied by reviews are costly to obtain. Although some studies of semi-supervised rating regression are proposed to mitigate the impact of sparse data, they bear the risk of learning from noisy pseudo-labeled data. In this article, we propose a simple yet effective paradigm, called co-training-teaching ( CoT 2 ), for integrating the merits of both co-training and co-teaching toward robust semi-supervised RaRR. CoT 2 employs two predictors trained with different feature sets of textual reviews, each of which functions as both “labeler” and “validator.” Specifically, one predictor (labeler) first labels unlabeled data for its peer predictor (validator); after that, the validator samples reliable instances from the noisy pseudo-labeled data it received and sends them back to the labeler for updating. By exchanging and validating pseudo-labeled instances, the two predictors are reinforced by each other in an iterative learning process. The final prediction is made by averaging the outputs of both the refined predictors. Extensive experiments show that our CoT 2 considerably outperforms the state-of-the-art recommendation techniques in the RaRR task, especially when the training data is severely insufficient.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387056195",
    "type": "article"
  },
  {
    "title": "Put Your Voice on Stage: Personalized Headline Generation for News Articles",
    "doi": "https://doi.org/10.1145/3629168",
    "publication_date": "2023-11-03",
    "publication_year": 2023,
    "authors": "Xiang Ao; Ling Luo; Xiting Wang; Zhao Yang; Jiun-Hung Chen; Ying Qiao; Qing He; Xing Xie",
    "corresponding_authors": "",
    "abstract": "In this article, we study the problem of personalized news headline generation, which aims to produce not only concise and fact-consistent titles for news articles but also decorate these titles as personalized irresistible reading invitations by incorporating readers’ preferences. We propose an approach named PNG ( P ersonalized N ews headline G enerator) by utilizing distant supervision in readers’ past click behaviors to resolve. First, user preference representations are learned through a knowledge-aware user encoder that comprehensively captures the genuine, sequential, and flash interests of users reflected in their historical clicked news. Then, a user-perturbed pointer-generator network is devised to accomplish the headline generation in which the learned user representations implicitly affect the word prediction. The proposed model is optimized by reinforcement learning solvers where indicators on factual, personalized, and linguistic aspects of the generated headline are regarded as rewards. Extensive experiments are conducted on the real-world dataset PENS, 1 which is a large-scale benchmark collected from Microsoft News. Both the quantitative and qualitative results validate the effectiveness of our approach.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4388282450",
    "type": "article"
  },
  {
    "title": "NNC-GCN: Neighbours-to-Neighbours Contrastive Graph Convolutional Network for Semi-Supervised Classification",
    "doi": "https://doi.org/10.1145/3638780",
    "publication_date": "2023-12-28",
    "publication_year": 2023,
    "authors": "Xiao Feng; Youfa Liu; Jia Shao",
    "corresponding_authors": "",
    "abstract": "Contrastive learning (CL) is a popular learning paradigm in deep learning, which uses contrastive principle to learn low-dimensional embeddings, and has been applied in Graph Neural Networks (GNNs) successfully. Existing works of contrastive multi-view GNNs usually focus on point-to-point contrastive learning strategies. However, they neglect the local information in neighbors, which brings isolated positive samples. The quality of selected positive samples is hard to evaluate, and these samples may lead to invalid contrastiveness. Therefore, we propose a simple and efficient neighbors-to-neighbors contrastive graph neural network (NNC-GCN), which constructs a consistent multi-view by using the topologies of original input graphs. Moreover, we raise a new learning problem of unlabeled data base on these constructed multi-view topologies and propose a loss function NNC-InfoNCE to guide its learning process. The NNC-InfoNCE is an improved version of InfoNCE, which can be adapted to neighborhood-level contrast learning. Specifically, the neighborhoods and the remaining nodes of the selected anchor are weighted and treated as positive and negative sample sets. The experimental results show that our method is effective on public benchmark datasets.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390317709",
    "type": "article"
  },
  {
    "title": "Fast likelihood search for hidden Markov models",
    "doi": "https://doi.org/10.1145/1631162.1631166",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Yasuhiro Fujiwara; Yasushi Sakurai; Masaru Kitsuregawa",
    "corresponding_authors": "",
    "abstract": "Hidden Markov models (HMMs) are receiving considerable attention in various communities and many applications that use HMMs have emerged such as mental task classification, biological analysis, traffic monitoring, and anomaly detection. This article has two goals; The first goal is exact and efficient identification of the model whose state sequence has the highest likelihood for the given query sequence (more precisely, no HMM that actually has a high-probability path for the given sequence is missed by the algorithm), and the second goal is exact and efficient monitoring of streaming data sequences to find the best model. We propose SPIRAL, a fast search method for HMM datasets. SPIRAL is based on three ideas; (1) it clusters states of models to compute approximate likelihood, (2) it uses several granularities and approximates likelihood values in search processing, and (3) it focuses on just the promising likelihood computations by pruning out low-likelihood state sequences. Experiments verify the effectiveness of SPIRAL and show that it is more than 490 times faster than the naive method.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2122451052",
    "type": "article"
  },
  {
    "title": "Mining Redescriptions with Siren",
    "doi": "https://doi.org/10.1145/3007212",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Esther Galbrun; Pauli Miettinen",
    "corresponding_authors": "",
    "abstract": "In many areas of science, scientists need to find distinct common characterizations of the same objects and, vice versa, to identify sets of objects that admit multiple shared descriptions. For example, in biology, an important task is to identify the bioclimatic constraints that allow some species to survive, that is, to describe geographical regions both in terms of the fauna that inhabits them and of their bioclimatic conditions. In data analysis, the task of automatically generating such alternative characterizations is called redescription mining. If a domain expert wants to use redescription mining in his research, merely being able to find redescriptions is not enough. He must also be able to understand the redescriptions found, adjust them to better match his domain knowledge, test alternative hypotheses with them, and guide the mining process toward results he considers interesting. To facilitate these goals, we introduce Siren, an interactive tool for mining and visualizing redescriptions. Siren allows to obtain redescriptions in an anytime fashion through efficient, distributed mining, to examine the results in various linked visualizations, to interact with the results either directly or via the visualizations, and to guide the mining algorithm toward specific redescriptions. In this article, we explain the features of Siren and why they are useful for redescription mining. We also propose two novel redescription mining algorithms that improve the generalizability of the results compared to the existing ones.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2586143265",
    "type": "article"
  },
  {
    "title": "DWE-Med",
    "doi": "https://doi.org/10.1145/3310254",
    "publication_date": "2019-03-13",
    "publication_year": 2019,
    "authors": "Kishlay Jha; Guangxu Xun; Vishrawas Gopalakrishnan; Aidong Zhang",
    "corresponding_authors": "",
    "abstract": "Recent advances in unsupervised language processing methods have created an opportunity to exploit massive text corpora for developing high-quality vector space representation (also known as word embeddings) of words. Towards this direction, practitioners have developed and applied several data driven embedding models with quite good rate of success. However, a drawback of these models lies in their premise of static context; wherein, the meaning of a word is assumed to remain the same over the period of time. This is limiting because it is known that the semantic meaning of a concept evolves over time. While such semantic drifts are routinely observed in almost all the domains; their effect is acute in domain such as biomedicine, where the semantic meaning of a concept changes relatively fast. To address this, in this study, we aim to learn temporally aware vector representation of medical concepts from the timestamped text data, and in doing so provide a systematic approach to formalize the problem. More specifically, a dynamic word embedding based model that jointly learns the temporal characteristics of medical concepts and performs across time-alignment is proposed. Apart from capturing the evolutionary characteristics in an optimal manner, the model also factors in the implicit medical properties useful for a variety of bio-medical applications. Empirical studies conducted on two important bio-medical use cases validates the effectiveness of the proposed approach and suggests that the model not only learns quality embeddings but also facilitates intuitive trajectory visualizations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2921634120",
    "type": "article"
  },
  {
    "title": "Feature Selection via Transferring Knowledge Across Different Classes",
    "doi": "https://doi.org/10.1145/3314202",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Zheng Wang; Xiaojun Ye; Chaokun Wang; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "The problem of feature selection has attracted considerable research interest in recent years. Supervised information is capable of significantly improving the quality of selected features. However, existing supervised feature selection methods all require that classes in the labeled data (source domain) and unlabeled data (target domain) to be identical, which may be too restrictive in many cases. In this article, we consider a more challenging cross-class setting where the classes in these two domains are related but different, which has rarely been studied before. We propose a cross-class knowledge transfer feature selection framework which transfers the cross-class knowledge from the source domain to guide target domain feature selection. Specifically, high-level descriptions, i.e., attributes, are used as the bridge for knowledge transfer. To further improve the quality of the selected features, our framework jointly considers the tasks of cross-class knowledge transfer and feature selection. Experimental results on four benchmark datasets demonstrate the superiority of the proposed method.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2948585608",
    "type": "article"
  },
  {
    "title": "Tensorizing Restricted Boltzmann Machine",
    "doi": "https://doi.org/10.1145/3321517",
    "publication_date": "2019-06-07",
    "publication_year": 2019,
    "authors": "Fujiao Ju; Yanfeng Sun; Junbin Gao; Michael Antolovich; Junliang Dong; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Restricted Boltzmann machine (RBM) is a famous model for feature extraction and can be used as an initializer for neural networks. When applying the classic RBM to multidimensional data such as 2D/3D tensors, one needs to vectorize such as high-order data. Vectorizing will result in dimensional disaster and valuable spatial information loss. As RBM is a model with fully connected layers, it requires a large amount of memory. Therefore, it is difficult to use RBM with high-order data on low-end devices. In this article, to utilize classic RBM on tensorial data directly, we propose a new tensorial RBM model parameterized by the tensor train format (TTRBM). In this model, both visible and hidden variables are in tensorial form, which are connected by a parameter matrix in tensor train format. The biggest advantage of the proposed model is that TTRBM can obtain comparable performance compared with the classic RBM with much fewer model parameters and faster training process. To demonstrate the advantages of TTRBM, we conduct three real-world applications, face reconstruction, handwritten digit recognition, and image super-resolution in the experiments.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2952985875",
    "type": "article"
  },
  {
    "title": "Boosting Item-based Collaborative Filtering via Nearly Uncoupled Random Walks",
    "doi": "https://doi.org/10.1145/3406241",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Αθανάσιος Ν. Νικολακόπουλος; George Karypis",
    "corresponding_authors": "",
    "abstract": "Item-based models are among the most popular collaborative filtering approaches for building recommender systems. Random walks can provide a powerful tool for harvesting the rich network of interactions captured within these models. They can exploit indirect relations between the items, mitigate the effects of sparsity, ensure wider itemspace coverage, as well as increase the diversity of recommendation lists. Their potential however, can be hindered by the tendency of the walks to rapidly concentrate towards the central nodes of the graph, thereby significantly restricting the range of K -step distributions that can be exploited for personalized recommendations. In this work, we introduce RecWalk ; a novel random walk-based method that leverages the spectral properties of nearly uncoupled Markov chains to provably lift this limitation and prolong the influence of users’ past preferences on the successive steps of the walk—thereby allowing the walker to explore the underlying network more fruitfully. A comprehensive set of experiments on real-world datasets verify the theoretically predicted properties of the proposed approach and indicate that they are directly linked to significant improvements in top- n recommendation accuracy. They also highlight RecWalk’s potential in providing a framework for boosting the performance of item-based models. RecWalk achieves state-of-the-art top- n recommendation quality outperforming several competing approaches, including recently proposed methods that rely on deep neural networks.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2971860711",
    "type": "article"
  },
  {
    "title": "Recurrent Meta-Structure for Robust Similarity Measure in Heterogeneous Information Networks",
    "doi": "https://doi.org/10.1145/3364226",
    "publication_date": "2019-12-06",
    "publication_year": 2019,
    "authors": "Yu Zhou; Jianbin Huang; Heli Sun; Yizhou Sun; Shaojie Qiao; Stephen Wambura",
    "corresponding_authors": "",
    "abstract": "Similarity measure is one of the fundamental task in heterogeneous information network (HIN) analysis. It has been applied to many areas, such as product recommendation, clustering, and Web search. Most of the existing metrics can provide personalized services for users by taking a meta-path or meta-structure as input. However, these metrics may highly depend on the user-specified meta-path or meta-structure. In addition, users must know how to select an appropriate meta-path or meta-structure. In this article, we propose a novel similarity measure in HINs, called Recurrent Meta-Structure (RecurMS)-based Similarity (RMSS). The RecurMS as a schematic structure in HINs provides a unified framework for integrating all of the meta-paths and meta-structures, and can be constructed automatically by means of repetitively traversing the network schema. In order to formalize the semantics, the RecurMS is decomposed into several recurrent meta-paths and recurrent meta-trees, and we then define the commuting matrices of the recurrent meta-paths and meta-trees. All of these commuting matrices are combined together according to different weights. We propose two kinds of weighting strategies to determine the weights. The first is called the local weighting strategy that depends on the sparsity of the commuting matrices, and the second is called the global weighting strategy that depends on the strength of the commuting matrices. As a result, RMSS is defined by means of the weighted summation of the commuting matrices. Note that RMSS can also provide personalized services for users by means of the weights of the recurrent meta-paths and meta-trees. Experimental evaluations show that the proposed RMSS is robust and outperforms the existing metrics in terms of ranking and clustering task.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3002932982",
    "type": "article"
  },
  {
    "title": "New Algorithms of Feature Selection and Big Data Assignment for CBR System Integrated by Bayesian Network",
    "doi": "https://doi.org/10.1145/3373086",
    "publication_date": "2020-02-18",
    "publication_year": 2020,
    "authors": "Yuan Guo; Yu Sun; Kai Wu; Kerong Jiang",
    "corresponding_authors": "",
    "abstract": "Under big data, the integrated system of case-based reasoning and Bayesian network has exhibited great advantage in implementing the intelligence of engineering application in many domains. To further improve the performance of the hybrid system, this article proposes Probability Change Measurement of Solution Parameters (PCMSP)–Half-Division-Cross (HDC) method, which includes two algorithms, namely PCMSP and HDC algorithm. PCMSP algorithm can select principal problem features according to their effects upon all solution features measured by calculating the weighted relative probability (RP) change of all solution features caused by each problem feature. PCMSP algorithm can perfectly work under big data no matter how complex the data types are and how huge the data size is. HDC algorithm is used to assign the computation task of big data to enhance the efficiency of the integrated system. HDC algorithm assigns big data by grouping all the problem parameters into many small sub-groups and then distributing the data which covers the same sub-group of problem parameters to a slave node. HDC algorithm can guarantee enough efficiency of the integrated system under big data no matter how large the number of problem parameters is. Finally, lots of experiments are executed to validate the proposed method.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3007226346",
    "type": "article"
  },
  {
    "title": "MP <sup>2</sup> SDA",
    "doi": "https://doi.org/10.1145/3374919",
    "publication_date": "2020-03-13",
    "publication_year": 2020,
    "authors": "Jiang Bian; Haoyi Xiong; Yanjie Fu; Jun Huan; Zhishan Guo",
    "corresponding_authors": "",
    "abstract": "Sparse Discriminant Analysis (SDA) has been widely used to improve the performance of classical Fisher’s Linear Discriminant Analysis in supervised metric learning, feature selection, and classification. With the increasing needs of distributed data collection, storage, and processing, enabling the Sparse Discriminant Learning to embrace the multi-party distributed computing environments becomes an emerging research topic. This article proposes a novel multi-party SDA algorithm, which can learn SDA models effectively without sharing any raw data and basic statistics among machines. The proposed algorithm (1) leverages the direct estimation of SDA to derive a distributed loss function for the discriminant learning, (2) parameterizes the distributed loss function with local/global estimates through bootstrapping, and (3) approximates a global estimation of linear discriminant projection vector by optimizing the “distributed bootstrapping loss function” with gossip-based stochastic gradient descent. Experimental results on both synthetic and real-world benchmark datasets show that our algorithm can compete with the aggregated SDA with similar performance, and significantly outperforms the most recent distributed SDA in terms of accuracy and F1-score.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3012286120",
    "type": "article"
  },
  {
    "title": "Robust Adaptive Linear Discriminant Analysis with Bidirectional Reconstruction Constraint",
    "doi": "https://doi.org/10.1145/3409478",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Jipeng Guo; Yanfeng Sun; Junbin Gao; Yongli Hu; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Linear discriminant analysis (LDA) is a well-known supervised method for dimensionality reduction in which the global structure of data can be preserved. The classical LDA is sensitive to the noises, and the projection direction of LDA cannot preserve the main energy. This article proposes a novel feature extraction model with l 2,1 norm constraint based on LDA, termed as RALDA. This model preserves within-class local structure in the latent subspace according to the label information. To reduce information loss, it learns a projection matrix and an inverse projection matrix simultaneously. By introducing an implicit variable and matrix norm transformation, the alternating direction multiple method with updating variables is designed to solve the RALDA model. Moreover, both computational complexity and weak convergence property of the proposed algorithm are investigated. The experimental results on several public databases have demonstrated the effectiveness of our proposed method.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3089552784",
    "type": "article"
  },
  {
    "title": "Explainable Artificial Intelligence-Based Competitive Factor Identification",
    "doi": "https://doi.org/10.1145/3451529",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Juhee Han; Younghoon Lee",
    "corresponding_authors": "",
    "abstract": "Competitor analysis is an essential component of corporate strategy, providing both offensive and defensive strategic contexts to identify opportunities and threats. The rapid development of social media has recently led to several methodologies and frameworks facilitating competitor analysis through online reviews. Existing studies only focused on detecting comparative sentences in review comments or utilized low-performance models. However, this study proposes a novel approach to identifying the competitive factors using a recent explainable artificial intelligence approach at the comprehensive product feature level. We establish a model to classify the review comments for each corresponding product and evaluate the relevance of each keyword in such comments during the classification process. We then extract and prioritize the keywords and determine their competitiveness based on relevance. Our experiment results show that the proposed method can effectively extract the competitive factors both qualitatively and quantitatively.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3184255946",
    "type": "article"
  },
  {
    "title": "DiMBERT: Learning Vision-Language Grounded Representations with Disentangled Multimodal-Attention",
    "doi": "https://doi.org/10.1145/3447685",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Fenglin Liu; Xian Wu; Shen Ge; Xuancheng Ren; Wei Fan; Xu Sun; Yuexian Zou",
    "corresponding_authors": "",
    "abstract": "Vision-and-language (V-L) tasks require the system to understand both vision content and natural language, thus learning fine-grained joint representations of vision and language (a.k.a. V-L representations) is of paramount importance. Recently, various pre-trained V-L models are proposed to learn V-L representations and achieve improved results in many tasks. However, the mainstream models process both vision and language inputs with the same set of attention matrices. As a result, the generated V-L representations are entangled in one common latent space . To tackle this problem, we propose DiMBERT (short for Di sentangled M ultimodal-Attention BERT ), which is a novel framework that applies separated attention spaces for vision and language, and the representations of multi-modalities can thus be disentangled explicitly. To enhance the correlation between vision and language in disentangled spaces, we introduce the visual concepts to DiMBERT which represent visual information in textual format. In this manner, visual concepts help to bridge the gap between the two modalities. We pre-train DiMBERT on a large amount of image–sentence pairs on two tasks: bidirectional language modeling and sequence-to-sequence language modeling. After pre-train, DiMBERT is further fine-tuned for the downstream tasks. Experiments show that DiMBERT sets new state-of-the-art performance on three tasks (over four datasets), including both generation tasks (image captioning and visual storytelling) and classification tasks (referring expressions). The proposed DiM (short for Di sentangled M ultimodal-Attention) module can be easily incorporated into existing pre-trained V-L models to boost their performance, up to a 5% increase on the representative task. Finally, we conduct a systematic analysis and demonstrate the effectiveness of our DiM and the introduced visual concepts.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3186513731",
    "type": "article"
  },
  {
    "title": "Bayesian Additive Matrix Approximation for Social Recommendation",
    "doi": "https://doi.org/10.1145/3451391",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Huafeng Liu; Liping Jing; Jingxuan Wen; Pengyu Xu; Jian Yu; Michael K. Ng",
    "corresponding_authors": "",
    "abstract": "Social relations between users have been proven to be a good type of auxiliary information to improve the recommendation performance. However, it is a challenging issue to sufficiently exploit the social relations and correctly determine the user preference from both social and rating information. In this article, we propose a unified Bayesian Additive Matrix Approximation model (BAMA), which takes advantage of rating preference and social network to provide high-quality recommendation. The basic idea of BAMA is to extract social influence from social networks, integrate them to Bayesian additive co-clustering for effectively determining the user clusters and item clusters, and provide an accurate rating prediction. In addition, an efficient algorithm with collapsed Gibbs Sampling is designed to inference the proposed model. A series of experiments were conducted on six real-world social datasets. The results demonstrate the superiority of the proposed BAMA by comparing with the state-of-the-art methods from three views, all users, cold-start users, and users with few social relations. With the aid of social information, furthermore, BAMA has ability to provide the explainable recommendation.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3186710985",
    "type": "article"
  },
  {
    "title": "CrowdTC: Crowd-powered Learning for Text Classification",
    "doi": "https://doi.org/10.1145/3457216",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Keyu Yang; Yunjun Gao; Lei Liang; Song Bian; Lu Chen; Baihua Zheng",
    "corresponding_authors": "",
    "abstract": "Text classification is a fundamental task in content analysis. Nowadays, deep learning has demonstrated promising performance in text classification compared with shallow models. However, almost all the existing models do not take advantage of the wisdom of human beings to help text classification. Human beings are more intelligent and capable than machine learning models in terms of understanding and capturing the implicit semantic information from text. In this article, we try to take guidance from human beings to classify text. We propose Crowd-powered learning for Text Classification (CrowdTC for short). We design and post the questions on a crowdsourcing platform to extract keywords in text. Sampling and clustering techniques are utilized to reduce the cost of crowdsourcing. Also, we present an attention-based neural network and a hybrid neural network to incorporate the extracted keywords as human guidance into deep neural networks. Extensive experiments on public datasets confirm that CrowdTC improves the text classification accuracy of neural networks by using the crowd-powered keyword guidance.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3186746296",
    "type": "article"
  },
  {
    "title": "Corpus-level and Concept-based Explanations for Interpretable Document Classification",
    "doi": "https://doi.org/10.1145/3477539",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Tian Shi; Xuchao Zhang; Ping Wang; Chandan K. Reddy",
    "corresponding_authors": "",
    "abstract": "Using attention weights to identify information that is important for models’ decision making is a popular approach to interpret attention-based neural networks. This is commonly realized in practice through the generation of a heat-map for every single document based on attention weights. However, this interpretation method is fragile and it is easy to find contradictory examples. In this article, we propose a corpus-level explanation approach, which aims at capturing causal relationships between keywords and model predictions via learning the importance of keywords for predicted labels across a training corpus based on attention weights. Based on this idea, we further propose a concept-based explanation method that can automatically learn higher level concepts and their importance to model prediction tasks. Our concept-based explanation method is built upon a novel Abstraction-Aggregation Network (AAN), which can automatically cluster important keywords during an end-to-end training process. We apply these methods to the document classification task and show that they are powerful in extracting semantically meaningful keywords and concepts. Our consistency analysis results based on an attention-based Naïve Bayes classifier (NBC) also demonstrate that these keywords and concepts are important for model predictions.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3208613448",
    "type": "article"
  },
  {
    "title": "A Sequential Sampling Framework for Spectral k-Means Based on Efficient Bootstrap Accuracy Estimations",
    "doi": "https://doi.org/10.1145/2297456.2297457",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Dimitrios Mavroeidis; Panagis Magdalinos",
    "corresponding_authors": "",
    "abstract": "The scalability of learning algorithms has always been a central concern for data mining researchers, and nowadays, with the rapid increase in data storage capacities and availability, its importance has increased. To this end, sampling has been studied by several researchers in an effort to derive sufficiently accurate models using only small data fractions. In this article we focus on spectral k -means, that is, the k -means approximation as derived by the spectral relaxation, and propose a sequential sampling framework that iteratively enlarges the sample size until the k -means results (objective function and cluster structure) become indistinguishable from the asymptotic (infinite-data) output. In the proposed framework we adopt a commonly applied principle in data mining research that considers the use of minimal assumptions concerning the data generating distribution. This restriction imposes several challenges, mainly related to the efficiency of the sequential sampling procedure. These challenges are addressed using elements of matrix perturbation theory and statistics. Moreover, although the main focus is on spectral k -means, we also demonstrate that the proposed framework can be generalized to handle spectral clustering. The proposed sequential sampling framework is consecutively employed for addressing the distributed clustering problem, where the task is to construct a global model for data that resides in distributed network nodes. The main challenge in this context is related to the bandwidth constraints that are commonly imposed, thus requiring that the distributed clustering algorithm consumes a minimal amount of network load. This illustrates the applicability of the proposed approach, as it enables the determination of a minimal sample size that can be used for constructing an accurate clustering model that entails the distributional characteristics of the data. As opposed to the relevant distributed k -means approaches, our framework takes into account the fact that the choice of the number of clusters has a crucial effect on the required amount of communication. More precisely, the proposed algorithm is able to derive a statistical estimation of the required relative sizes for all possible values of k . This unique feature of our distributed clustering framework enables a network administrator to choose an economic solution that identifies the crude cluster structure of a dataset and not devote excessive network resources for identifying all the “correct” detailed clusters.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2134587493",
    "type": "article"
  },
  {
    "title": "Partitioned Similarity Search with Cache-Conscious Data Traversal",
    "doi": "https://doi.org/10.1145/3014060",
    "publication_date": "2017-04-14",
    "publication_year": 2017,
    "authors": "Xun Tang; Maha Alabduljalil; Xin Jin; Tao Yang",
    "corresponding_authors": "",
    "abstract": "All pairs similarity search (APSS) is used in many web search and data mining applications. Previous work has used techniques such as comparison filtering, inverted indexing, and parallel accumulation of partial results. However, shuffling intermediate results can incur significant communication overhead as data scales up. This paper studies a scalable two-phase approach called Partition-based Similarity Search (PSS). The first phase is to partition the data and group vectors that are potentially similar. The second phase is to run a set of tasks where each task compares a partition of vectors with other candidate partitions. Due to data sparsity and the presence of memory hierarchy, accessing feature vectors during the partition comparison phase incurs significant overhead. This paper introduces a cache-conscious design for data layout and traversal to reduce access time through size-controlled data splitting and vector coalescing, and it provides an analysis to guide the choice of optimization parameters. The evaluation results show that for the tested datasets, the proposed approach can lead to an early elimination of unnecessary I/O and data communication while sustaining parallel efficiency with one order of magnitude of performance improvement and it can also be integrated with LSH for approximated APSS.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2606885797",
    "type": "article"
  },
  {
    "title": "<scp>come</scp>N<scp>go</scp>",
    "doi": "https://doi.org/10.1145/3059214",
    "publication_date": "2017-07-27",
    "publication_year": 2017,
    "authors": "Tianyang Zhang; Peng Cui; Christos Faloutsos; Yunfei Lu; Hao Ye; Wenwu Zhu; Shiqiang Yang",
    "corresponding_authors": "",
    "abstract": "How do social groups, such as Facebook groups and Wechat groups, dynamically evolve over time? How do people join the social groups, uniformly or with burst? What is the pattern of people quitting from groups? Is there a simple universal model to depict the come-and-go patterns of various groups? In this article, we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users. We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR. For example, we observe both diffusion and non-diffusion mechanism in the group joining process, and power-law decay in group quitting process, rather than exponential decay as expected in SIR model. Therefore, we propose a new model come N go , a concise yet flexible dynamic model for group evolution. Our model has the following advantages: (a) Unification power: it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation. (b) Succinctness and interpretability: it contains only six parameters with clear physical meanings. (c) Accuracy: it can capture various kinds of group evolution patterns preciously, and the goodness of fit increases by 58% over baseline. (d) Usefulness: it can be used in multiple application scenarios, such as forecasting and pattern discovery. Furthermore, our model can provide insights about different evolution patterns of social groups, and we also find that group structure and its evolution has notable relations with temporal patterns of group evolution.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2740505012",
    "type": "article"
  },
  {
    "title": "An Efficient Algorithm For Weak Hierarchical Lasso",
    "doi": "https://doi.org/10.1145/2791295",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "Yashu Liu; Jie Wang; Jieping Ye",
    "corresponding_authors": "",
    "abstract": "Linear regression is a widely used tool in data mining and machine learning. In many applications, fitting a regression model with only linear effects may not be sufficient for predictive or explanatory purposes. One strategy that has recently received increasing attention in statistics is to include feature interactions to capture the nonlinearity in the regression model. Such model has been applied successfully in many biomedical applications. One major challenge in the use of such model is that the data dimensionality is significantly higher than the original data, resulting in the small sample size large dimension problem. Recently, weak hierarchical Lasso, a sparse interaction regression model, is proposed that produces a sparse and hierarchical structured estimator by exploiting the Lasso penalty and a set of hierarchical constraints. However, the hierarchical constraints make it a non-convex problem and the existing method finds the solution to its convex relaxation, which needs additional conditions to guarantee the hierarchical structure. In this article, we propose to directly solve the non-convex weak hierarchical Lasso by making use of the General Iterative Shrinkage and Thresholding (GIST) optimization framework, which has been shown to be efficient for solving non-convex sparse formulations. The key step in GIST is to compute a sequence of proximal operators. One of our key technical contributions is to show that the proximal operator associated with the non-convex weak hierarchical Lasso admits a closed-form solution. However, a naive approach for solving each subproblem of the proximal operator leads to a quadratic time complexity, which is not desirable for large-size problems. We have conducted extensive experiments on both synthetic and real datasets. Results show that our proposed algorithm is much more efficient and effective than its convex relaxation. To this end, we further develop an efficient algorithm for computing the subproblems with a linearithmic time complexity. In addition, we extend the technique to perform the optimization-based hierarchical testing of pairwise interactions for binary classification problems, which is essentially the proximal operator associated with weak hierarchical Lasso. Simulation studies show that the non-convex hierarchical testing framework outperforms the convex relaxation when a hierarchical structure exists between main effects and interactions.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2296456607",
    "type": "article"
  },
  {
    "title": "Social Behavior Analysis in Exclusive Enterprise Social Networks by FastHAND",
    "doi": "https://doi.org/10.1145/3646552",
    "publication_date": "2024-02-12",
    "publication_year": 2024,
    "authors": "Yang Yang; Feifei Wang; Enqiang Zhu; Fei Jiang; Wen Yao",
    "corresponding_authors": "",
    "abstract": "There is an emerging trend in the Chinese automobile industries that automakers are introducing exclusive enterprise social networks (EESNs) to expand sales and provide after-sale services. The traditional online social networks (OSNs) and enterprise social networks (ESNs), such as X (formerly known as Twitter) and Yammer, are ingeniously designed to facilitate unregulated communications among equal individuals. However, users in EESNs are naturally social stratified, consisting of both enterprise staffs and customers. In addition, the motivation to operate EESNs can be quite complicated, including providing customer services and facilitating communication among enterprise staffs. As a result, the social behaviors in EESNs can be quite different from those in OSNs and ESNs. In this work, we aim to analyze the social behaviors in EESNs. We consider the Chinese car manufacturer NIO as a typical example of EESNs and provide the following contributions. First, we formulate the social behavior analysis in EESNs as a link prediction problem in heterogeneous social networks. Second, to analyze this link prediction problem, we derive plentiful user features and build multiple meta-path graphs for EESNs. Third, we develop a novel Fast (H)eterogeneous graph (A)ttention (N)etwork algorithm for (D)irected graphs (FastHAND) to predict directed social links among users in EESNs. This algorithm introduces feature group attention at the node-level and uses an edge sampling algorithm over directed meta-path graphs to reduce the computation cost. By conducting various experiments on the NIO community data, we demonstrate the predictive power of our proposed FastHAND method. The experimental results also verify our intuitions about social affinity propagation in EESNs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391753470",
    "type": "article"
  },
  {
    "title": "Do We Really Need Imputation in AutoML Predictive Modeling?",
    "doi": "https://doi.org/10.1145/3643643",
    "publication_date": "2024-02-16",
    "publication_year": 2024,
    "authors": "George Paterakis; Stefanos Fafalios; Paulos Charonyktakis; Vassilis Christophides; Ioannis Tsamardinos",
    "corresponding_authors": "",
    "abstract": "Numerous real-world data contain missing values, while in contrast, most Machine Learning (ML) algorithms assume complete datasets. For this reason, several imputation algorithms have been proposed to predict and fill in the missing values. Given the advances in predictive modeling algorithms tuned in an Automated Machine Learning context (AutoML) setting, a question that naturally arises is to what extent sophisticated imputation algorithms (e.g., Neural Network based) are really needed, or we can obtain a descent performance using simple methods like Mean/Mode (MM). In this article, we experimentally compare six state-of-the-art representatives of different imputation algorithmic families from an AutoML predictive modeling perspective, including a feature selection step and combined algorithm and hyper-parameter selection. We used a commercial AutoML tool for our experiments, in which we included the selected imputation methods. Experiments ran on 25 binary classification real-world incomplete datasets with missing values and 10 binary classification complete datasets in which synthetic missing values are introduced according to different missingness mechanisms, at varying missing frequencies. The main conclusion drawn from our experiments is that the best method on average is the Denoise AutoEncoder on real-world datasets and the MissForest in simulated datasets, followed closely by MM. In addition, binary indicator variables encoding missingness patterns actually improve predictive performance, on average. Last, although there are cases where Neural-Network-based imputation significantly improves predictive performance, this comes at a great computational cost and requires measuring all feature values to impute new samples.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391761597",
    "type": "article"
  },
  {
    "title": "iGRM",
    "doi": "https://doi.org/10.1145/3186268",
    "publication_date": "2018-04-16",
    "publication_year": 2018,
    "authors": "Nashreen Nesa; Tania Ghosh; Indrajit Banerjee",
    "corresponding_authors": "",
    "abstract": "Occupancy detection is one of the many applications of Building Automation Systems (BAS) or Heating, Ventilation, and Air Conditioning (HVAC) control systems, especially, with the rising demand of Internet of Things (IoT) services. This article describes the fusion of data collected from sensors by exploiting their potential to sense occupancy in a room. For this purpose, a sensor test bed is deployed that includes four sensors measuring temperature, relative humidity, distance from the first obstacle, and light along with a Arduino micro-controller to validate our model. In addition, this article proposes three algorithms for efficient fusion of the sensor data that is inspired by the Grey theory. An improved Grey Relational Model (iGRM) is proposed, which acts as the base classifier for the other two algorithms, namely, Grey Relational Model with Bagging (iGRM-BG) and Grey Relational Model with Boosting (iGRM-BT). Furthermore, all three algorithms use a sliding window concept, where only the samples inside the window participate in model training. Also, we have considered varying number of window size for optimal comparison. The algorithms were tested against the experimental data collected through a test bed as well as on a publicly available large dataset, where both the ensemble models, iGRM-BG and iGRM-BT, are seen to enhance the performance of iGRM. The results reveal exceptionally high performances with accuracies above 95% (iGRM) and up to 100% (iGRM-BT) for the experimental dataset and above 98.24% (iGRM) and up to 99.49% (iGRM-BG) using the publicly available dataset. Among the three proposed models, iGRM-BG was observed to outperform both iGRM and iGRM-BT owing to its advantage of being an ensemble model and its robustness against over-fitting.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2802124319",
    "type": "article"
  },
  {
    "title": "Social Network Monitoring for Bursty Cascade Detection",
    "doi": "https://doi.org/10.1145/3178048",
    "publication_date": "2018-04-16",
    "publication_year": 2018,
    "authors": "Wei Xie; Feida Zhu; Jing Xiao; Jianzong Wang",
    "corresponding_authors": "",
    "abstract": "Social network services have become important and efficient platforms for users to share all kinds of information. The capability to monitor user-generated information and detect bursts from information diffusions in these social networks brings value to a wide range of real-life applications, such as viral marketing. However, in reality, as a third party, there is always a cost for gathering information from each user or so-called social network sensor. The question then arises how to select a budgeted set of social network sensors to form the data stream for burst detection without compromising the detection performance. In this article, we present a general sensor selection solution for different burst detection approaches. We formulate this problem as a constraint satisfaction problem that has high computational complexity. To reduce the computational cost, we first reduce most of the constraints by making use of the fact that bursty cascades are rare among the whole population. We then transform the problem into an Linear Programming (LP) problem. Furthermore, we use the sub-gradient method instead of the standard simplex method or interior-point method to solve the LP problem, which makes it possible for our solution to scale up to large social networks. Evaluating our solution on millions of real information cascades, we demonstrate both the effectiveness and efficiency of our approach.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2802399614",
    "type": "article"
  },
  {
    "title": "Exploiting User Posts for Web Document Summarization",
    "doi": "https://doi.org/10.1145/3186566",
    "publication_date": "2018-06-08",
    "publication_year": 2018,
    "authors": "Minh-Tien Nguyen; Vu Tran; Le-Minh Nguyen; Xuan-Hieu Phan",
    "corresponding_authors": "",
    "abstract": "Relevant user posts such as comments or tweets of a Web document provide additional valuable information to enrich the content of this document. When creating user posts, readers tend to borrow salient words or phrases in sentences. This can be considered as word variation. This article proposes a framework that models the word variation aspect to enhance the quality of Web document summarization. Technically, the framework consists of two steps: scoring and selection. In the first step, the social information of a Web document such as user posts is exploited to model intra-relations and inter-relations in lexical and semantic levels. These relations are denoted by a mutual reinforcement similarity graph used to score each sentence and user post. After scoring, summaries are extracted by using a ranking approach or concept-based method formulated in the form of Integer Linear Programming. To confirm the efficiency of our framework, sentence and story highlight extraction tasks were taken as a case study on three datasets in two languages, English and Vietnamese. Experimental results show that: (i) the framework can improve ROUGE-scores compared to state-of-the-art baselines of social context summarization and (ii) the combination of the two relations benefits the sentence extraction of single Web documents.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2808450271",
    "type": "article"
  },
  {
    "title": "Active Two Phase Collaborative Representation Classifier",
    "doi": "https://doi.org/10.1145/3326919",
    "publication_date": "2019-07-02",
    "publication_year": 2019,
    "authors": "Fadi Dornaika",
    "corresponding_authors": "Fadi Dornaika",
    "abstract": "The Sparse Representation Classifier, the Collaborative Representation Classifier (CRC), and the Two Phase Test Sample Sparse Representation (TPTSSR) classifier were introduced in recent times. All these frameworks are supervised and passive in the sense that they cannot benefit from unlabeled data samples. In this paper, inspired by active learning paradigms, we introduce an active CRC that can be used by these frameworks. More precisely, we are interested in the TPTSSR framework due to its good performance and its reasonable computational cost. Our proposed Active Two Phase Collaborative Representation Classifier (ATPCRC) starts by predicting the label of the available unlabeled samples. At testing stage, two coding processes are carried out separately on the set of originally labeled samples and the whole set (original and predicted label). The two types of class-wise reconstruction errors are blended in order to decide the class of any test image. Experiments conducted on four public image datasets show that the proposed ATPCRC can outperform the classic TPTSSR as well as many state-of-the-art methods that exploit label and unlabeled data samples.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2955349463",
    "type": "article"
  },
  {
    "title": "On the Impact of Voice Encoding and Transmission on the Predictions of Speaker Warmth and Attractiveness",
    "doi": "https://doi.org/10.1145/3332146",
    "publication_date": "2019-07-26",
    "publication_year": 2019,
    "authors": "Laura Fernández Gallardo; Ramón Sanchez‐Iborra",
    "corresponding_authors": "",
    "abstract": "Modern human-computer interaction systems may not only be based on interpreting natural language but also on detecting speaker interpersonal characteristics in order to determine dialog strategies. This may be of high interest in different fields such as telephone marketing or automatic voice-based interactive services. However, when such systems encounter signals transmitted over a communication network instead of clean speech, e.g., in call centers, the speaker characterization accuracy might be impaired by the degradations caused in the speech signal by the encoding and communication processes. This article addresses a binary classification of high versus low warm--attractive speakers over different channel and encoding conditions. The ground truth is derived from ratings given to clean speech extracted from an extensive subjective test. Our results show that, under the considered conditions, the AMR-WB+ codec permits good levels of classification accuracy, comparable to the classification with clean, non-degraded speech. This is especially notable for the case of a Random Forest-based classifier, which presents the best performance among the set of evaluated algorithms. The impact of different packet loss rates has been examined, whereas jitter effects have been found to be negligible.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2965782104",
    "type": "article"
  },
  {
    "title": "Real-time Transportation Prediction Correction using Reconstruction Error in Deep Learning",
    "doi": "https://doi.org/10.1145/3369871",
    "publication_date": "2020-02-09",
    "publication_year": 2020,
    "authors": "Shuai Liu; Guojie Song; Wenhao Huang",
    "corresponding_authors": "",
    "abstract": "In online complex systems such as transportation system, an important work is real-time traffic prediction. Due to the data shift, data model inconsistency, and sudden change of traffic patterns (like transportation accident), the prediction result derived from an offline-built model would be unreliable. Retraining the model is usually not time affordable for online prediction, especially when the prediction model is very complex and costs a lot of training time (for example, deep neural networks). A real-time prediction correction strategy would be of great value under this situation. Traditionally, the prediction correction usually relies on the prediction error in several previous time intervals. They assume that the error pattern is similar in the current time interval, so that it is time-delayed to some extent. In this article, we propose the prediction correction strategy using the reconstruction error in the deep neural network. The reconstruction error can reflect the model’s ability on feature representation and then determine the fitness of an input data to the model. We first build the relationship between reconstruction error and prediction error. From the perspective of the prediction interval, we demonstrate that the reconstruction error is in positive relation with the prediction interval. Thus the prediction result is more reliable when the reconstruction error is smaller. Then we propose two mechanisms of real-time prediction correction using the reconstruction error. The data driven prediction correction approach selects several training instances with similar reconstruction errors to the current instance and using their average prediction error in correcting the prediction result. The model-driven approach builds several component deep neural networks in training. The component training set for each network is selected according to the reconstruction error of training instances. For a predicting instance, it first computes the reconstruction error of the sample in each component network and then averages the results by the reconstruction error and prediction interval. The model-driven approach is actually a reconstruction error-based deep neural network ensemble approach. Finally, a series of experiments demonstrated that reconstruction error based prediction correction approaches are effective in several prediction problems in transportation including traffic flow prediction on road, traffic flow prediction in entrance and exit station and travel time prediction. Besides the high overall accuracy, our approach can also provide many observations of using the reconstruction error in transportation prediction.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3008689442",
    "type": "article"
  },
  {
    "title": "Framework for Inferring Following Strategies from Time Series of Movement Data",
    "doi": "https://doi.org/10.1145/3385730",
    "publication_date": "2020-05-13",
    "publication_year": 2020,
    "authors": "Chainarong Amornbunchornvej; Tanya Berger‐Wolf",
    "corresponding_authors": "",
    "abstract": "How do groups of individuals achieve consensus in movement decisions? Do individuals follow their friends, the one predetermined leader, or whomever just happens to be nearby? To address these questions computationally, we formalize \"Coordination Strategy Inference Problem\". In this setting, a group of multiple individuals moves in a coordinated manner towards a target path. Each individual uses a specific strategy to follow others (e.g. nearest neighbors, pre-defined leaders, preferred friends). Given a set of time series that includes coordinated movement and a set of candidate strategies as inputs, we provide the first methodology (to the best of our knowledge) to infer whether each individual uses local-agreement-system or dictatorship-like strategy to achieve movement coordination at the group level. We evaluate and demonstrate the performance of the proposed framework by predicting the direction of movement of an individual in a group in both simulated datasets as well as two real-world datasets: a school of fish and a troop of baboons. Moreover, since there is no prior methodology for inferring individual-level strategies, we compare our framework with the state-of-the-art approach for the task of classification of group-level-coordination models. The results show that our approach is highly accurate in inferring the correct strategy in simulated datasets even in complicated mixed strategy settings, which no existing method can infer. In the task of classification of group-level-coordination models, our framework performs better than the state-of-the-art approach in all datasets. Animal data experiments show that fish, as expected, follow their neighbors, while baboons have a preference to follow specific individuals. Our methodology generalizes to arbitrary time series data of real numbers, beyond movement data.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3027660952",
    "type": "article"
  },
  {
    "title": "MiSoSouP",
    "doi": "https://doi.org/10.1145/3385653",
    "publication_date": "2020-06-21",
    "publication_year": 2020,
    "authors": "Matteo Riondato; Fabio Vandin",
    "corresponding_authors": "",
    "abstract": "We present MiSoSouP, a suite of algorithms for extracting high-quality approximations of the most interesting subgroups, according to different popular interestingness measures, from a random sample of a transactional dataset. We describe a new formulation of these measures as functions of averages, that makes it possible to approximate them using sampling. We then discuss how pseudodimension, a key concept from statistical learning theory, relates to the sample size needed to obtain an high-quality approximation of the most interesting subgroups. We prove an upper bound on the pseudodimension of the problem at hand, which depends on characteristic quantities of the dataset and of the language of patterns of interest. This upper bound then leads to small sample sizes. Our evaluation on real datasets shows that MiSoSouP outperforms state-of-the-art algorithms offering the same guarantees, and it vastly speeds up the discovery of subgroups w.r.t. analyzing the whole dataset.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3036474431",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1497577",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "As a prolific research area in data mining, subspace clustering and related problems induced a vast quantity of proposed solutions. However, many publications compare a new proposition—if at all—with one or two competitors, or even with a so-called “...",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4237041271",
    "type": "paratext"
  },
  {
    "title": "Cross-Guided Clustering",
    "doi": "https://doi.org/10.1145/2297456.2297461",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Indrajit Bhattacharya; Shantanu Godbole; Sachindra Joshi; Ashish Verma",
    "corresponding_authors": "",
    "abstract": "Lack of supervision in clustering algorithms often leads to clusters that are not useful or interesting to human reviewers. We investigate if supervision can be automatically transferred for clustering a target task, by providing a relevant supervised partitioning of a dataset from a different source task. The target clustering is made more meaningful for the human user by trading-off intrinsic clustering goodness on the target task for alignment with relevant supervised partitions in the source task, wherever possible. We propose a cross-guided clustering algorithm that builds on traditional k-means by aligning the target clusters with source partitions. The alignment process makes use of a cross-task similarity measure that discovers hidden relationships across tasks. When the source and target tasks correspond to different domains with potentially different vocabularies, we propose a projection approach using pivot vocabularies for the cross-domain similarity measure. Using multiple real-world and synthetic datasets, we show that our approach improves clustering accuracy significantly over traditional k-means and state-of-the-art semi-supervised clustering baselines, over a wide range of data characteristics and parameter settings.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1964877627",
    "type": "article"
  },
  {
    "title": "Asymmetric Multi-Task Learning with Local Transference",
    "doi": "https://doi.org/10.1145/3514252",
    "publication_date": "2022-02-04",
    "publication_year": 2022,
    "authors": "Saullo Haniell Galvão de Oliveira; André Gonçalves; Fernando J. Von Zuben",
    "corresponding_authors": "",
    "abstract": "In this article, we present the Group Asymmetric Multi-Task Learning (GAMTL) algorithm that automatically learns from data how tasks transfer information among themselves at the level of a subset of features. In practice, for each group of features GAMTL extracts an asymmetric relationship supported by the tasks, instead of assuming a single structure for all features. The additional flexibility promoted by local transference in GAMTL allows any two tasks to have multiple asymmetric relationships. The proposed method leverages the information present in these multiple structures to bias the training of individual tasks towards more generalizable models. The solution to the GAMTL’s associated optimization problem is an alternating minimization procedure involving tasks parameters and multiple asymmetric relationships, thus guiding to convex smaller sub-problems. GAMTL was evaluated on both synthetic and real datasets. To evidence GAMTL versatility, we generated a synthetic scenario characterized by diverse profiles of structural relationships among tasks. GAMTL was also applied to the problem of Alzheimer’s Disease (AD) progression prediction. Our experiments indicated that the proposed approach not only increased prediction performance, but also estimated scientifically grounded relationships among multiple cognitive scores, taken here as multiple regression tasks, and regions of interest in the brain, directly associated here with groups of features. We also employed stability selection analysis to investigate GAMTL’s robustness to data sampling rate and hyper-parameter configuration. GAMTL source code is available on GitHub: https://github.com/shgo/gamtl .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4210528603",
    "type": "article"
  },
  {
    "title": "Generalized Euclidean Measure to Estimate Distances on Multilayer Networks",
    "doi": "https://doi.org/10.1145/3529396",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Michele Coscia",
    "corresponding_authors": "Michele Coscia",
    "abstract": "Estimating the distance covered by a spreading event on a network can lead to a better understanding of epidemics, economic growth, and human behavior. There are many methods solving this problem – which has been called Node Vector Distance (NVD) – for single layer networks. However, many phenomena are better represented by multilayer networks: networks in which nodes can connect in qualitatively different ways. In this paper, we extend the literature by proposing an algorithm solving NVD for multilayer networks. We do so by adapting the Mahalanobis distance, incorporating the graph’s topology via the pseudoinverse of its Laplacian. Since this is a proper generalization of the Euclidean distance in a complex space defined by the topology of the graph, and that it works on multilayer networks, we call our measure the Multi Layer Generalized Euclidean (MLGE). In our experiments, we show that MLGE is intuitive, theoretically simpler than the alternatives, performs well in recovering infection parameters, and it is useful in specific case studies. MLGE requires solving a special case of the effective resistance on the graph, which has a high time complexity. However, this needs to be done only once per network. In the experiments, we show that MLGE can cache its most computationally-heavy parts, allowing it to solve hundreds of NVD problems on the same network with little to no additional runtime. MLGE is provided as a free open source tool, along with the data and the code necessary to replicate our results.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4220751136",
    "type": "article"
  },
  {
    "title": "A News-Based Framework for Uncovering and Tracking City Area Profiles: Assessment in Covid-19 Setting",
    "doi": "https://doi.org/10.1145/3532186",
    "publication_date": "2022-04-25",
    "publication_year": 2022,
    "authors": "Alessio Bechini; Alessandro Bondielli; José Luis Corcuera Bárcena; Pietro Ducange; Francesco Marcelloni; Alessandro Renda",
    "corresponding_authors": "",
    "abstract": "In the last years, there has been an ever-increasing interest in profiling various aspects of city life, especially in the context of smart cities. This interest has become even more relevant recently when we have realized how dramatic events, such as the Covid-19 pandemic, can deeply affect the city life, producing drastic changes. Identifying and analyzing such changes, both at the city level and within single neighborhoods, may be a fundamental tool to better manage the current situation and provide sound strategies for future planning. Furthermore, such fine-grained and up-to-date characterization can represent a valuable asset for other tools and services, e.g., web mapping applications or real estate agency platforms. In this article, we propose a framework featuring a novel methodology to model and track changes in areas of the city by extracting information from online newspaper articles. The problem of uncovering clusters of news at specific times is tackled by means of the joint use of state-of-the-art language models to represent the articles, and of a density-based streaming clustering algorithm, properly shaped to deal with high-dimensional text embeddings. Furthermore, we propose a method to automatically label the obtained clusters in a semantically meaningful way, and we introduce a set of metrics aimed at tracking the temporal evolution of clusters. A case study focusing on the city of Rome during the Covid-19 pandemic is illustrated and discussed to evaluate the effectiveness of the proposed approach.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4224315168",
    "type": "article"
  },
  {
    "title": "When Less Is More: Systematic Analysis of Cascade-Based Community Detection",
    "doi": "https://doi.org/10.1145/3494563",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Liudmila Prokhorenkova; Alexey Tikhonov; Nelly Litvak",
    "corresponding_authors": "",
    "abstract": "Information diffusion, spreading of infectious diseases, and spreading of rumors are fundamental processes occurring in real-life networks. In many practical cases, one can observe when nodes become infected, but the underlying network, over which a contagion or information propagates, is hidden. Inferring properties of the underlying network is important since these properties can be used for constraining infections, forecasting, viral marketing, and so on. Moreover, for many applications, it is sufficient to recover only coarse high-level properties of this network rather than all its edges. This article conducts a systematic and extensive analysis of the following problem: Given only the infection times, find communities of highly interconnected nodes. This task significantly differs from the well-studied community detection problem since we do not observe a graph to be clustered. We carry out a thorough comparison between existing and new approaches on several large datasets and cover methodological challenges specific to this problem. One of the main conclusions is that the most stable performance and the most significant improvement on the current state-of-the-art are achieved by our proposed simple heuristic approaches agnostic to a particular graph structure and epidemic model. We also show that some well-known community detection algorithms can be enhanced by including edge weights based on the cascade data.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4226253343",
    "type": "article"
  },
  {
    "title": "Graph Deep Factors for Probabilistic Time-series Forecasting",
    "doi": "https://doi.org/10.1145/3543511",
    "publication_date": "2022-06-21",
    "publication_year": 2022,
    "authors": "Hongjie Chen; Ryan A. Rossi; Kanak Mahadik; Sungchul Kim; Hoda Eldardiry",
    "corresponding_authors": "",
    "abstract": "Effective time-series forecasting methods are of significant importance to solve a broad spectrum of research problems. Deep probabilistic forecasting techniques have recently been proposed for modeling large collections of time-series. However, these techniques explicitly assume either complete independence (local model) or complete dependence (global model) between time-series in the collection. This corresponds to the two extreme cases where every time-series is disconnected from every other time-series in the collection or likewise, that every time-series is related to every other time-series resulting in a completely connected graph. In this work, we propose a deep hybrid probabilistic graph-based forecasting framework called Graph Deep Factors (GraphDF) that goes beyond these two extremes by allowing nodes and their time-series to be connected to others in an arbitrary fashion. GraphDF is a hybrid forecasting framework that consists of a relational global and relational local model. In particular, a relational global model learns complex non-linear time-series patterns globally using the structure of the graph to improve both forecasting accuracy and computational efficiency. Similarly, instead of modeling every time-series independently, a relational local model not only considers its individual time-series but also the time-series of nodes that are connected in the graph. The experiments demonstrate the effectiveness of the proposed deep hybrid graph-based forecasting model compared to the state-of-the-art methods in terms of its forecasting accuracy, runtime, and scalability. Our case study reveals that GraphDF can successfully generate cloud usage forecasts and opportunistically schedule workloads to increase cloud cluster utilization by 47.5% on average. Furthermore, we target addressing the common nature of many time-series forecasting applications where time-series are provided in a streaming version; however, most methods fail to leverage the newly incoming time-series values and result in worse performance over time. In this article, we propose an online incremental learning framework for probabilistic forecasting. The framework is theoretically proven to have lower time and space complexity. The framework can be universally applied to many other machine learning-based methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4283217920",
    "type": "article"
  },
  {
    "title": "Combining Filtering and Cross-Correlation Efficiently for Streaming Time Series",
    "doi": "https://doi.org/10.1145/3502738",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Sheng Zhong; Vinícius M. A. Souza; Abdullah Mueen",
    "corresponding_authors": "",
    "abstract": "Monitoring systems have hundreds or thousands of distributed sensors gathering and transmitting real-time streaming data. The early detection of events in these systems, such as an earthquake in a seismic monitoring system, is the base for essential tasks as warning generations. To detect such events is usual to compute pairwise correlation across the disparate signals generated by the sensors. Since the data sources (e.g., sensors) are spatially separated, it is essential to consider the lagged correlation between the signals. Besides, many applications require to process a specific band of frequencies depending on the event’s type, demanding a pre-processing step of filtering before computing correlations. Due to the high speed of data generation and a large number of sensors in these systems, the operations of filtering and lagged cross-correlation need to be efficient to provide real-time responses without data losses. This article proposes a technique named FilCorr that efficiently computes both operations in one single step. We achieve an order of magnitude speedup by maintaining frequency transforms over sliding windows. Our method is exact, devoid of sensitive parameters, and easily parallelizable. Besides our algorithm, we also provide a publicly available real-time system named Seisviz that employs FilCorr in its core mechanism for monitoring a seismometer network. We demonstrate that our technique is suitable for several monitoring applications as seismic signal monitoring, motion monitoring, and neural activity monitoring.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4300662192",
    "type": "article"
  },
  {
    "title": "Universal and Distinct Properties of Communication Dynamics",
    "doi": "https://doi.org/10.1145/2700399",
    "publication_date": "2015-04-01",
    "publication_year": 2015,
    "authors": "Pedro O. S. Vaz de Melo; Christos Faloutsos; Renato Assunção; Rodrigo Alves; Antonio A. F. Loureiro",
    "corresponding_authors": "",
    "abstract": "With the advancement of information systems, means of communications are becoming cheaper, faster, and more available. Today, millions of people carrying smartphones or tablets are able to communicate practically any time and anywhere they want. They can access their e-mails, comment on weblogs, watch and post videos and photos (as well as comment on them), and make phone calls or text messages almost ubiquitously. Given this scenario, in this article, we tackle a fundamental aspect of this new era of communication: How the time intervals between communication events behave for different technologies and means of communications. Are there universal patterns for the Inter-Event Time Distribution (IED)? How do inter-event times behave differently among particular technologies? To answer these questions, we analyzed eight different datasets from real and modern communication data and found four well-defined patterns seen in all the eight datasets. Moreover, we propose the use of the Self-Feeding Process (SFP) to generate inter-event times between communications. The SFP is an extremely parsimonious point process that requires at most two parameters and is able to generate inter-event times with all the universal properties we observed in the data. We also show three potential applications of the SFP: as a framework to generate a synthetic dataset containing realistic communication events of any one of the analyzed means of communications, as a technique to detect anomalies, and as a building block for more specific models that aim to encompass the particularities seen in each of the analyzed systems.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4300878592",
    "type": "article"
  },
  {
    "title": "Learning Shared Representations for Recommendation with Dynamic Heterogeneous Graph Convolutional Networks",
    "doi": "https://doi.org/10.1145/3565575",
    "publication_date": "2022-10-10",
    "publication_year": 2022,
    "authors": "Mengyuan Jing; Yanmin Zhu; Yanan Xu; Haobing Liu; Tianzi Zang; Chunyang Wang; Jiadi Yu",
    "corresponding_authors": "",
    "abstract": "Graph Convolutional Networks (GCNs) have been widely used for collaborative filtering, due to their effectiveness in exploiting high-order collaborative signals. However, two issues have not been well addressed by existing studies. First, usually only one kind of information is utilized, i.e., user preference in user-item graphs or item dependency in item-item graphs. Second, they usually adopt static graphs, which cannot retain the temporal evolution of the information. These can limit the recommendation quality. To address these limitations, we propose to mine three kinds of information (user preference, item dependency, and user behavior similarity) and their temporal evolution by constructing multiple discrete dynamic heterogeneous graphs (i.e., a user-item dynamic graph, an item-item dynamic graph, and a user-subseq dynamic graph) from interaction data. A novel network (PDGCN) is proposed to learn the representations of users and items in these dynamic graphs. Moreover, we designed a structural neighbor aggregation module with novel pooling and convolution operations to aggregate the features of structural neighbors. We also design a temporal neighbor aggregation module based on self-attention mechanism to aggregate the features of temporal neighbors. We conduct extensive experiments on four real-world datasets. The results indicate that our approach outperforms several competing methods in terms of Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). Dynamic graphs are also shown to be effective in improving recommendation performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4304146415",
    "type": "article"
  },
  {
    "title": "Semi-Supervised Sentiment Classification and Emotion Distribution Learning Across Domains",
    "doi": "https://doi.org/10.1145/3571736",
    "publication_date": "2022-11-18",
    "publication_year": 2022,
    "authors": "Yufu Chen; Yanghui Rao; Shurui Chen; Zhiqi Lei; Haoran Xie; Raymond Y.K. Lau; Jian Yin",
    "corresponding_authors": "",
    "abstract": "In this study, sentiment classification and emotion distribution learning across domains are both formulated as a semi-supervised domain adaptation problem, which utilizes a small amount of labeled documents in the target domain for model training. By introducing a shared matrix that captures the stable association between document clusters and word clusters, non-negative matrix tri-factorization (NMTF) is robust to the labeled target domain data and has shown remarkable performance in cross-domain text classification. However, the existing NMTF-based models ignore the incompatible relationship of sentiment polarities and the relatedness among emotions. Besides, their applications on large-scale datasets are limited by the high computation complexity. To address these issues, we propose a semi-supervised NMTF framework for sentiment classification and emotion distribution learning across domains. Based on a many-to-many mapping between document clusters and sentiment polarities (or emotions), we first incorporate the prior information of label dependency to improve the model performance. Then, we develop a parallel algorithm based on message passing interface (MPI) to further enhance the model scalability. Extensive experiments on real-world datasets validate the effectiveness of our method.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4309287897",
    "type": "article"
  },
  {
    "title": "Characterizing and Forecasting Urban Vibrancy Evolution: A Multi-View Graph Mining Perspective",
    "doi": "https://doi.org/10.1145/3568683",
    "publication_date": "2022-11-30",
    "publication_year": 2022,
    "authors": "Hao Liu; Qingyu Guo; Hengshu Zhu; Yanjie Fu; Fuzhen Zhuang; Xiaojuan Ma; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "Urban vibrancy describes the prosperity, diversity, and accessibility of urban areas, which is vital to a city’s socio-economic development and sustainability. While many efforts have been made for statically measuring and evaluating urban vibrancy, there are few studies on the evolutionary process of urban vibrancy, yet we know little about the relationship between urban vibrancy evolution and sophisticated spatiotemporal dynamics. In this article, we make use of multi-sourced urban data to develop a data-driven framework, U-Evolve , to investigate urban vibrancy evolution. Specifically, we first exploit the spatiotemporal characteristics of urban areas to create multi-view time-dependent graphs. Then, we analyze the contextual features and graph patterns of multi-view time-dependent graphs in terms of informing future urban vibrancy variations. Our analysis validates the informativeness of multi-view time-dependent graphs for characterizing and informing future urban vibrancy evolution. After that, we construct a feature based model to forecast future urban vibrancy evolution and quantify each feature’s importance. Moreover, to further enhance the forecasting effectiveness, we propose a graph learning based model to capture spatiotemporal autocorrelation of urban areas based on multi-view time-dependent graphs in an end-to-end manner. Finally, extensive experiments on two metropolises, Beijing and Shanghai, demonstrate the effectiveness of our forecasting models. The U-Evolve framework has also been deployed in the production environment to deliver real-world urban development and planning insights for various cities in China.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4310388583",
    "type": "article"
  },
  {
    "title": "Finding Subgraphs with Maximum Total Density and Limited Overlap in Weighted Hypergraphs",
    "doi": "https://doi.org/10.1145/3639410",
    "publication_date": "2024-01-02",
    "publication_year": 2024,
    "authors": "Oana Balalau; Francesco Bonchi; T-H. Hubert Chan; Francesco Gullo; Mauro Sozio; Hao Xie",
    "corresponding_authors": "",
    "abstract": "Finding dense subgraphs in large (hyper)graphs is a key primitive in a variety of real-world application domains, encompassing social network analytics, event detection, biology, and finance. In most such applications, one typically aims at finding several (possibly overlapping) dense subgraphs, which might correspond to communities in social networks or interesting events. While a large amount of work is devoted to finding a single densest subgraph, perhaps surprisingly, the problem of finding several dense subgraphs in weighted hypergraphs with limited overlap has not been studied in a principled way, to the best of our knowledge. In this work, we define and study a natural generalization of the densest subgraph problem in weighted hypergraphs, where the main goal is to find at most k subgraphs with maximum total aggregate density, while satisfying an upper bound on the pairwise weighted Jaccard coefficient, i.e., the ratio of weights of intersection divided by weights of union on two nodes sets of the subgraphs. After showing that such a problem is NP-Hard, we devise an efficient algorithm that comes with provable guarantees in some cases of interest, as well as, an efficient practical heuristic. Our extensive evaluation on large real-world hypergraphs confirms the efficiency and effectiveness of our algorithms.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390490762",
    "type": "article"
  },
  {
    "title": "Multiple-instance Learning from Triplet Comparison Bags",
    "doi": "https://doi.org/10.1145/3638776",
    "publication_date": "2024-01-02",
    "publication_year": 2024,
    "authors": "Senlin Shu; Deng-Bao Wang; Suqin Yuan; Hongxin Wei; Jiuchuan Jiang; Lei Feng; Min-Ling Zhang",
    "corresponding_authors": "",
    "abstract": "Multiple-instance learning (MIL) solves the problem where training instances are grouped in bags, and a binary (positive or negative) label is provided for each bag. Most of the existing MIL studies need fully labeled bags for training an effective classifier, while it could be quite hard to collect such data in many real-world scenarios, due to the high cost of data labeling process. Fortunately, unlike fully labeled data, triplet comparison data can be collected in a more accurate and human-friendly way. Therefore, in this article, we for the first time investigate MIL from only triplet comparison bags , where a triplet (X a , X b , X c ) contains the weak supervision information that bag X a is more similar to X b than to X c . To solve this problem, we propose to train a bag-level classifier by the empirical risk minimization framework and theoretically provide a generalization error bound. We also show that a convex formulation can be obtained only when specific convex binary losses such as the square loss and the double hinge loss are used. Extensive experiments validate that our proposed method significantly outperforms other baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390490892",
    "type": "article"
  },
  {
    "title": "EffCause: Discover Dynamic Causal Relationships Efficiently from Time-Series",
    "doi": "https://doi.org/10.1145/3640818",
    "publication_date": "2024-01-16",
    "publication_year": 2024,
    "authors": "Yicheng Pan; Yifan Zhang; Xinrui Jiang; Meng Ma; Ping Wang",
    "corresponding_authors": "",
    "abstract": "Since the proposal of Granger causality, many researchers have followed the idea and developed extensions to the original algorithm. The classic Granger causality test aims to detect the existence of the static causal relationship. Notably, a fundamental assumption underlying most previous studies is the stationarity of causality, which requires the causality between variables to keep stable. However, this study argues that it is easy to break in real-world scenarios. Fortunately, our paper presents an essential observation: if we consider a sufficiently short window when discovering the rapidly changing causalities, they will keep approximately static and thus can be detected using the static way correctly. In light of this, we develop EffCause, bringing dynamics into classic Granger causality. Specifically, to efficiently examine the causalities on different sliding window lengths, we design two optimization schemes in EffCause and demonstrate the advantage of EffCause through extensive experiments on both simulated and real-world datasets. The results validate that EffCause achieves state-of-the-art accuracy in continuous causal discovery tasks while achieving faster computation. Case studies from cloud system failure analysis and traffic flow monitoring show that EffCause effectively helps us understand real-world time-series data and solve practical problems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390912481",
    "type": "article"
  },
  {
    "title": "Math Word Problem Generation via Disentangled Memory Retrieval",
    "doi": "https://doi.org/10.1145/3639569",
    "publication_date": "2024-01-26",
    "publication_year": 2024,
    "authors": "Wei Qin; Xiaowei Wang; Zhenzhen Hu; Lei Wang; Yunshi Lan; Richang Hong",
    "corresponding_authors": "",
    "abstract": "The task of math word problem (MWP) generation, which generates an MWP given an equation and relevant topic words, has increasingly attracted researchers’ attention. In this work, we introduce a simple memory retrieval module to search related training MWPs, which are used to augment the generation. To retrieve more relevant training data, we also propose a disentangled memory retrieval module based on the simple memory retrieval module. To this end, we first disentangle the training MWPs into logical description and scenario description and then record them in respective memory modules. Later, we use the given equation and topic words as queries to retrieve relevant logical descriptions and scenario descriptions from the corresponding memory modules, respectively. The retrieved results are then used to complement the process of the MWP generation. Extensive experiments and ablation studies verify the superior performance of our method and the effectiveness of each proposed module. The code is available at https://github.com/mwp-g/MWPG-DMR .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391262616",
    "type": "article"
  },
  {
    "title": "Incorporating Multi-Level Sampling with Adaptive Aggregation for Inductive Knowledge Graph Completion",
    "doi": "https://doi.org/10.1145/3644822",
    "publication_date": "2024-02-07",
    "publication_year": 2024,
    "authors": "Kai Sun; Huajie Jiang; Yongli Hu; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "In recent years, Graph Neural Networks (GNNs) have achieved unprecedented success in handling graph-structured data, thereby driving the development of numerous GNN-oriented techniques for inductive knowledge graph completion (KGC). A key limitation of existing methods, however, is their dependence on pre-defined aggregation functions, which lack the adaptability to diverse data, resulting in suboptimal performance on established benchmarks. Another challenge arises from the exponential increase in irrelated entities as the reasoning path lengthens, introducing unwarranted noise and consequently diminishing the model’s generalization capabilities. To surmount these obstacles, we design an innovative framework that synergizes M ulti- L evel S ampling with an A daptive A ggregation mechanism (MLSAA). Distinctively, our model couples GNNs with enhanced set transformers, enabling dynamic selection of the most appropriate aggregation function tailored to specific datasets and tasks. This adaptability significantly boosts both the model’s flexibility and its expressive capacity. Additionally, we unveil a unique sampling strategy designed to selectively filter irrelevant entities, while retaining potentially beneficial targets throughout the reasoning process. We undertake an exhaustive evaluation of our novel inductive KGC method across three pivotal benchmark datasets and the experimental results corroborate the efficacy of MLSAA.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391614674",
    "type": "article"
  },
  {
    "title": "Multi-Instance Learning with One Side Label Noise",
    "doi": "https://doi.org/10.1145/3644076",
    "publication_date": "2024-02-07",
    "publication_year": 2024,
    "authors": "Tianxiang Luan; Shilin Gu; Xijia Tang; Wenzhang Zhuge; Chenping Hou",
    "corresponding_authors": "",
    "abstract": "Multi-instance Learning (MIL) is a popular learning paradigm arising from many real applications. It assigns a label to a set of instances, which is called a bag, and the bag’s label is determined by the instances within it. A bag is positive if and only if it has at least one positive instance. Since labeling bags is more complicated than labeling each instance, we will often face the mislabeling problem in MIL. Furthermore, it is more common that a negative bag has been mislabeled to a positive one, since one mislabeled instance will lead to the change of the whole bag label. This is an important problem that originated from real applications, e.g., web mining and image classification, but little research has concentrated on it as far as we know. In this article, we focus on this MIL problem with one side label noise that the negative bags are mislabeled as positive ones. To address this challenging problem, we propose, to the best our our knowledge, a novel multi-instance learning method with one side label noise. We design a new double weighting approach under traditional framework to characterize the “faithfulness” of each instance and each bag in learning the classifier. Briefly, on the instance level, we employ a sparse weighting method to select the key instances, and the MIL problem with one size label noise is converted to a mislabeled supervised learning scenario. On the bag level, the weights of bags, together with the selected key instances, will be utilized to identify the real positive bags. In addition, we have solved our proposed model by an alternative iteration method with proved convergence behavior. Empirical studies on various datasets have validated the effectiveness of our method.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391614728",
    "type": "article"
  },
  {
    "title": "Mining Top-k High On-shelf Utility Itemsets Using Novel Threshold Raising Strategies",
    "doi": "https://doi.org/10.1145/3645115",
    "publication_date": "2024-02-08",
    "publication_year": 2024,
    "authors": "Kuldeep Singh; Bhaskar Biswas",
    "corresponding_authors": "",
    "abstract": "High utility itemsets (HUIs) mining is an emerging area of data mining which discovers sets of items generating a high profit from transactional datasets. In recent years, several algorithms have been proposed for this task. However, most of them do not consider the on-shelf time period of items and negative utility of items. High on-shelf utility itemset (HOUIs) mining is more difficult than traditional HUIs mining because it deals with on-shelf-based time period and negative utility of items. Moreover, most algorithms need minimum utility threshold ( min_util ) to find rules. However, specifying the appropriate min_util threshold is a difficult problem for users. A smaller min_util threshold may generate too many rules and a higher one may generate a few rules, which can degrade performance. To address these issues, a novel top-k HOUIs mining algorithm named TKOS ( T op- K high O n- S helf utility itemsets miner) is proposed which considers on-shelf time period and negative utility. TKOS presents a novel branch and bound-based strategy to raise the internal min_util threshold efficiently. It also presents two pruning strategies to speed up the mining process. In order to reduce the dataset scanning cost, we utilize transaction merging and dataset projection techniques. Extensive experiments have been conducted on real and synthetic datasets having various characteristics. Experimental results show that the proposed algorithm outperforms the state-of-the-art algorithms. The proposed algorithm is up to 42 times faster and uses up-to 19 times less memory compared to the state-of-the-art KOSHU. Moreover, the proposed algorithm has excellent scalability in terms of time periods and the number of transactions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391646016",
    "type": "article"
  },
  {
    "title": "Local Community Detection in Multiple Private Networks",
    "doi": "https://doi.org/10.1145/3644078",
    "publication_date": "2024-02-10",
    "publication_year": 2024,
    "authors": "Ni Li; Rui Ye; Wenjian Luo; Yiwen Zhang",
    "corresponding_authors": "",
    "abstract": "Individuals are often involved in multiple online social networks. Considering that owners of these networks are unwilling to share their networks, some global algorithms combine information from multiple networks to detect all communities in multiple networks without sharing their edges. When data owners are only interested in the community containing a given node, it is unnecessary and computationally expensive for multiple networks to interact with each other to mine all communities. Moreover, data owners who are specifically looking for a community typically prefer to provide less data than the global algorithms require. Therefore, we propose the Local Collaborative Community Detection problem (LCCD). It exploits information from multiple networks to jointly detect the local community containing a given node without directly sharing edges between networks. To address the LCCD problem, we present a method developed from M method, called colM, to detect the local community in multiple networks. This method adopts secure multiparty computation protocols to protect each network’s private information. Our experiments were conducted on real-world and synthetic datasets. Experimental results show that colM method could effectively identify community structures and outperform comparison algorithms.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391717788",
    "type": "article"
  },
  {
    "title": "<i>nSimplex</i> <i>Zen</i> : A Novel Dimensionality Reduction for Euclidean and Hilbert Spaces",
    "doi": "https://doi.org/10.1145/3647642",
    "publication_date": "2024-02-10",
    "publication_year": 2024,
    "authors": "Richard Connor; Lucia Vadicamo",
    "corresponding_authors": "",
    "abstract": "Dimensionality reduction techniques map values from a high dimensional space to one with a lower dimension. The result is a space which requires less physical memory and has a faster distance calculation. These techniques are widely used where required properties of the reduced-dimension space give an acceptable accuracy with respect to the original space. Many such transforms have been described. They have been classified in two main groups: linear and topological . Linear methods such as Principal Component Analysis (PCA) and Random Projection (RP) define matrix-based transforms into a lower dimension of Euclidean space. Topological methods such as Multidimensional Scaling (MDS) attempt to preserve higher-level aspects such as the nearest-neighbour relation, and some may be applied to non-Euclidean spaces. Here, we introduce nSimplex Zen , a novel topological method of reducing dimensionality. Like MDS, it relies only upon pairwise distances measured in the original space. The use of distances, rather than coordinates, allows the technique to be applied to both Euclidean and other Hilbert spaces, including those governed by Cosine, Jensen–Shannon and Quadratic Form distances. We show that in almost all cases, due to geometric properties of high-dimensional spaces, our new technique gives better properties than others, especially with reduction to very low dimensions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391732389",
    "type": "article"
  },
  {
    "title": "Learning to Generate Temporal Origin-destination Flow Based-on Urban Regional Features and Traffic Information",
    "doi": "https://doi.org/10.1145/3649141",
    "publication_date": "2024-02-20",
    "publication_year": 2024,
    "authors": "Can Rong; Zhicheng Liu; Jingtao Ding; Yong Li",
    "corresponding_authors": "",
    "abstract": "Origin-destination (OD) flow contains population mobility information between every two regions in the city, which is of great value in urban planning and transportation management. Nevertheless, the collection of OD flow data is extremely difficult due to the hindrance of privacy issues and collection costs. Significant efforts have been made to generate OD flow based on urban regional features, e.g., demographics, land use, and so on, since spatial heterogeneity of urban function is the primary cause that drives people to move from one place to another. On the other hand, people travel through various routes between OD, which will have effects on urban traffic, e.g., road travel speed and time. These effects of OD flows reveal the fine-grained spatiotemporal patterns of population mobility. Few works have explored the effectiveness of incorporating urban traffic information into OD generation. To bridge this gap, we propose to generate real-world daily temporal OD flows enhanced by urban traffic information in this paper. Our model consists of two modules: Urban2OD and OD2Traffic . In the Urban2OD module, we devise a spatiotemporal graph neural network to model the complex dependencies between daily temporal OD flows and regional features. In the OD2Traffic module, we introduce an attention-based neural network to predict urban traffic based on OD flow from the Urban2OD module. Then, by utilizing gradient backpropagation, these two modules are able to enhance each other to generate high-quality OD flow data. Extensive experiments conducted on real-world datasets demonstrate the superiority of our proposed model over the state of the art.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391971843",
    "type": "article"
  },
  {
    "title": "DP-GCN: Node Classification by Connectivity and Local Topology Structure on Real-World Network",
    "doi": "https://doi.org/10.1145/3649460",
    "publication_date": "2024-02-28",
    "publication_year": 2024,
    "authors": "Zhe Chen; Aixin Sun",
    "corresponding_authors": "",
    "abstract": "Node classification is to predict the class label of a node by analyzing its properties and interactions in a network. We note that many existing solutions for graph-based node classification only consider node connectivity but not the node’s local topology structure. However, nodes residing in different parts of a real-world network may share similar local topology structures. For example, local topology structures in a payment network may reveal sellers’ business roles (e.g., supplier or retailer). To model both connectivity and local topology structure for better node classification performance, we present DP-GCN, a dual-path graph convolution network. DP-GCN consists of three main modules: (i) a C-GCN module to capture the connectivity relationships between nodes, (ii) a T-GCN module to capture the topology structure similarity among nodes, and (iii) a multi-head self-attention module to align both properties. We evaluate DP-GCN on seven benchmark datasets against diverse baselines to demonstrate its effectiveness. We also provide a case study of running DP-GCN on three large-scale payment networks from PayPal, a leading payment service provider, for risky seller detection. Experimental results show DP-GCN’s effectiveness and practicability in large-scale settings. PayPal’s internal testing also shows DP-GCN’s effectiveness in defending against real risks from transaction networks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392240564",
    "type": "article"
  },
  {
    "title": "Node Embedding Preserving Graph Summarization",
    "doi": "https://doi.org/10.1145/3649505",
    "publication_date": "2024-03-08",
    "publication_year": 2024,
    "authors": "Houquan Zhou; Shenghua Liu; Huawei Shen; Xueqi Cheng",
    "corresponding_authors": "",
    "abstract": "Graph summarization is a useful tool for analyzing large-scale graphs. Some works tried to preserve original node embeddings encoding rich structural information of nodes on the summary graph. However, their algorithms are designed heuristically and not theoretically guaranteed. In this article, we theoretically study the problem of preserving node embeddings on summary graph. We prove that three matrix-factorization-based node embedding methods of the original graph can be approximated by that of the summary graph, and we propose a novel graph summarization method, named HCSumm , based on this analysis. Extensive experiments are performed on real-world datasets to evaluate the effectiveness of our proposed method. The experimental results show that our method outperforms the state-of-the-art methods in preserving node embeddings.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392594821",
    "type": "article"
  },
  {
    "title": "Computing Random Forest-distances in the presence of missing data",
    "doi": "https://doi.org/10.1145/3656345",
    "publication_date": "2024-04-08",
    "publication_year": 2024,
    "authors": "Manuele Bicego; Ferdinando Cicalese",
    "corresponding_authors": "",
    "abstract": "In this article, we study the problem of computing Random Forest-distances in the presence of missing data. We present a general framework which avoids pre-imputation and uses in an agnostic way the information contained in the input points. We centre our investigation on RatioRF, an RF-based distance recently introduced in the context of clustering and shown to outperform most known RF-based distance measures. We also show that the same framework can be applied to several other state-of-the-art RF-based measures and provide their extensions to the missing data case. We provide significant empirical evidence of the effectiveness of the proposed framework, showing extensive experiments with RatioRF on 15 datasets. Finally, we also positively compare our method with many alternative literature distances, which can be computed with missing values.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4394569855",
    "type": "article"
  },
  {
    "title": "Distributed Pseudo-Likelihood Method for Community Detection in Large-Scale Networks",
    "doi": "https://doi.org/10.1145/3657300",
    "publication_date": "2024-04-16",
    "publication_year": 2024,
    "authors": "Jiayi Deng; Danyang Huang; Bo Zhang",
    "corresponding_authors": "",
    "abstract": "This paper proposes a distributed pseudo-likelihood method (DPL) to conveniently identify the community structure of large-scale networks. Specifically, we first propose a block-wise splitting method to divide large-scale network data into several subnetworks and distribute them among multiple workers. For simplicity, we assume the classical stochastic block model. Then, the DPL algorithm is iteratively implemented for the distributed optimization of the sum of the local pseudo-likelihood functions. At each iteration, the worker updates its local community labels and communicates with the master. The master then broadcasts the combined estimator to each worker for the new iterative steps. Based on the distributed system, DPL significantly reduces the computational complexity of the traditional pseudo-likelihood method using a single machine. Furthermore, to ensure statistical accuracy, we theoretically discuss the requirements of the worker sample size. Moreover, we extend the DPL method to estimate degree-corrected stochastic block models. The superior performance of the proposed distributed algorithm is demonstrated through extensive numerical studies and real data analysis.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4394844177",
    "type": "article"
  },
  {
    "title": "Learning with Asynchronous Labels",
    "doi": "https://doi.org/10.1145/3662186",
    "publication_date": "2024-05-03",
    "publication_year": 2024,
    "authors": "Yu-Yang Qian; Zhenyu Zhang; Peng Zhao; Zhi‐Hua Zhou",
    "corresponding_authors": "",
    "abstract": "Learning with data streams has attracted much attention in recent decades. Conventional approaches typically assume that the feature and label of a data item can be timely observed at each round. In many real-world tasks, however, it often occurs that either the feature or the label is observed firstly while the other arrives with delay. For instance, in distributed learning systems, a central processor collects training data from different sub-processors to train a learning model, whereas the feature and label of certain data items can arrive asynchronously due to network latency. The problem of learning with asynchronous feature or label in streams encompasses many applications but still lacks sound solutions. In this article, we formulate the problem and propose a new approach to alleviate the negative effect of asynchronicity and mining asynchronous data streams. Our approach carefully exploits the timely arrived information and builds an online ensemble structure to adaptively reuse historical models and instances. We provide the theoretical guarantees of our approach and conduct extensive experiments to validate its effectiveness.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396608890",
    "type": "article"
  },
  {
    "title": "Boosting Fair Classifier Generalization through Adaptive Priority Reweighing",
    "doi": "https://doi.org/10.1145/3665895",
    "publication_date": "2024-05-23",
    "publication_year": 2024,
    "authors": "Zhihao Hu; Yiran Xu; Mengnan Du; Jindong Gu; Xinmei Tian; Fengxiang He",
    "corresponding_authors": "",
    "abstract": "With the increasing penetration of machine learning applications in critical decision-making areas, calls for algorithmic fairness are more prominent. Although there have been various modalities to improve algorithmic fairness through learning with fairness constraints, their performance does not generalize well in the test set. A performance-promising fair algorithm with better generalizability is needed. This paper proposes a novel adaptive reweighing method to eliminate the impact of the distribution shifts between training and test data on model generalizability. Most previous reweighing methods propose to assign a unified weight for each (sub)group. Rather, our method granularly models the distance from the sample predictions to the decision boundary. Our adaptive reweighing method prioritizes samples closer to the decision boundary and assigns a higher weight to improve the generalizability of fair classifiers. Extensive experiments are performed to validate the generalizability of our adaptive priority reweighing method for accuracy and fairness measures (i.e., equal opportunity, equalized odds, and demographic parity) in tabular benchmarks. We also highlight the performance of our method in improving the fairness of language and vision models. The code is available at https://github.com/che2198/APW .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4398232352",
    "type": "article"
  },
  {
    "title": "Self-attentive Rationalization for Interpretable Graph Contrastive Learning",
    "doi": "https://doi.org/10.1145/3665894",
    "publication_date": "2024-05-23",
    "publication_year": 2024,
    "authors": "Sihang Li; Yanchen Luo; An Zhang; Xiang Wang; Longfei Li; Jun Zhou; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "Graph augmentation is the key component to reveal instance-discriminative features of a graph as its rationale – an interpretation for it – in graph contrastive learning (GCL). And existing rationale-aware augmentation mechanisms in GCL frameworks roughly fall into two categories and suffer from inherent limitations: (1) non-heuristic methods with the guidance of domain knowledge to preserve salient features, which require expensive expertise and lack generality, or (2) heuristic augmentations with a co-trained auxiliary model to identify crucial substructures, which face not only the dilemma between system complexity and transformation diversity, but also the instability stemming from the co-training of two separated submodels. Inspired by recent studies on transformers, we propose S elf-attentive R ationale guided G raph C ontrastive L earning (SR-GCL), which integrates rationale generator and encoder together, leverages the self-attention values in transformer module as a natural guidance to delineate semantically informative substructures from both node- and edge-wise perspectives, and contrasts on rationale-aware augmented pairs. On real-world biochemistry datasets, visualization results verify the effectivenes and interpretability of self-attentive rationalization, and the performance on downstream tasks demonstrates the state-of-theart performance of SR-GCL for graph model pre-training. Codes are available at https://github.com/lsh0520/SR-GCL .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4398251448",
    "type": "article"
  },
  {
    "title": "Utility-Oriented Reranking with Counterfactual Context",
    "doi": "https://doi.org/10.1145/3671004",
    "publication_date": "2024-06-04",
    "publication_year": 2024,
    "authors": "Yunjia Xi; Weiwen Liu; Xinyi Dai; Ruiming Tang; Qing Liu; Weinan Zhang; Yong Yu",
    "corresponding_authors": "",
    "abstract": "As a critical task for large-scale commercial recommender systems, reranking rearranges items in the initial ranking lists from the previous ranking stage to better meet users’ demands. Foundational work in reranking has shown the potential of improving recommendation results by uncovering mutual influence among items. However, rather than considering the context of initial lists as most existing methods do, an ideal reranking algorithm should consider the counterfactual context— the position and the alignment of the items in the reranked lists . In this work, we propose a novel pairwise reranking framework, Utility-oriented Reranking with Counterfactual Context (URCC), which maximizes the overall utility after reranking efficiently. Specifically, we first design a utility-oriented evaluator, which applies Bi-LSTM and graph attention mechanism to estimate the listwise utility via the counterfactual context modeling. Then, under the guidance of the evaluator, we propose a pairwise reranker model to find the most suitable position for each item by swapping misplaced item pairs. Extensive experiments on two benchmark datasets and a proprietary real-world dataset demonstrate that URCC significantly outperforms the state-of-the-art models in terms of both relevance-based metrics and utility-based metrics.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399318827",
    "type": "article"
  },
  {
    "title": "A Comprehensive Understanding of the Impact of Data Augmentation on the Transferability of 3D Adversarial Examples",
    "doi": "https://doi.org/10.1145/3673232",
    "publication_date": "2024-06-15",
    "publication_year": 2024,
    "authors": "Fulan Qian; Yuanjun Zou; Mengyao Xu; X. Zhang; Chonghao Zhang; Chenchu Xu; Hai Chen",
    "corresponding_authors": "",
    "abstract": "3D point cloud classifiers exhibit vulnerability to imperceptible perturbations, which poses a serious threat to the security and reliability of deep learning models in practical applications, making the robustness evaluation of deep 3D point cloud models increasingly important. Due to the difficulty in obtaining model parameters, black-box attacks have become a mainstream means of assessing the adversarial robustness of 3D classification models. The core of improving the transferability of adversarial examples generated by black-box attacks is to generate better generalized adversarial examples, where data augmentation has become one of the popular approaches. In this paper, we employ five mainstream attack methods and combine six data augmentation strategies, namely point dropping, flipping, rotating, scaling, shearing, and translating, in order to comprehensively explore the impact of these strategies on the transferability of adversarial examples. Our research reveals that data augmentation methods generally improve the transferability of the adversarial examples, and the effect is better when the methods are stacked. The interaction between data augmentation methods, model characteristics, attack and defense strategies collectively determines the transferability of adversarial examples. In order to comprehensively understand and improve the effectiveness of adversarial examples, it is necessary to comprehensively consider these complex interrelationships.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399702383",
    "type": "article"
  },
  {
    "title": "Towards Domain-Aware Stable Meta Learning for Out-of-Distribution Generalization",
    "doi": "https://doi.org/10.1145/3676558",
    "publication_date": "2024-07-03",
    "publication_year": 2024,
    "authors": "Mingchen Sun; Yingji Li; Ying Wang; Xin Wang",
    "corresponding_authors": "",
    "abstract": "Deep learning models are often trained on datasets that are limited in size and distribution, which may not fully represent the entire range of data encountered in practice. Thus, making deep learning models generalize to out-of-distribution data has received a significant amount of attention in recent studies due to the critical importance of this ability in real-world applications. Meta learning as an effective knowledge transfer paradigm, which learns a base model with high generalization ability to adapt to new data distributions by minimizing domain shifts across tasks during meta-training. However, most existing meta learning methods assume that the base model can access the labels of different domains, and this assumption is demanding in many real application scenarios. In addition, these methods focus on narrowing data-level domain shifts, while ignoring task-level domain shifts, which may lead to inadequate or even negative transfer. Inspired by human learners who use induction to learn and master new tasks, we propose a novel domain-aware meta learning framework for out-of-distribution generalization, termed SMLG. This framework enables the base model to generalize effectively to unseen domains without relying on domain-specific labels. Specifically, we develop a domain-aware transformation module to obtain meta representation and pseudo domain labels. As a result, the base model can be trained robustly without the need for direct domain label input. Furthermore, to investigate the impact of domain shifts at different levels, we introduce a joint loss function that combines cross-entropy with a domain alignment constraint. Extensive experiments on benchmark datasets demonstrate the efficacy of our framework.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400290505",
    "type": "article"
  },
  {
    "title": "Fair Federated Learning with Multi-Objective Hyperparameter Optimization",
    "doi": "https://doi.org/10.1145/3676968",
    "publication_date": "2024-07-12",
    "publication_year": 2024,
    "authors": "Chunnan Wang; Xiangyu Shi; Hongzhi Wang",
    "corresponding_authors": "",
    "abstract": "Federated learning (FL) is an attractive paradigm for privacy-aware distributed machine learning, which enables clients to collaboratively learn a global model without sharing clients’ data. Recently, many strategies have been proposed to improve the generality of the global model and thus improve FL effect. However, existing strategies either ignore the fairness among clients or sacrifice performance for fairness. They cannot ensure that the gap among clients is as small as possible without sacrificing federated performance. To address this issue, we propose ParetoFed , a new local information aggregation method dedicated to obtaining better federated performance with smaller gap among clients. Specifically, we propose to use multi-objective hyperparameter optimization (HPO) algorithm to gain global models that are both fair and effective. Then, we send Pareto Optimal global models to each client, allowing them to choose the most suitable one as the base to optimize their local model. ParetoFed not only make the global models more fair but also make the selection of local models more personalized, which can further improve the federated performance. Extensive experiments show that ParetoFed outperforms existing FL methods in terms of fairness, and even achieves better federated performance, which demonstrates the significance of our method.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400583849",
    "type": "article"
  },
  {
    "title": "A Survey on Knowledge Graph Related Research in Smart City Domain",
    "doi": "https://doi.org/10.1145/3672615",
    "publication_date": "2024-07-19",
    "publication_year": 2024,
    "authors": "Zhu Wang; Fengxia Han; Shengjie Zhao",
    "corresponding_authors": "",
    "abstract": "Knowledge graph employs the specific graph structure to store knowledge in the form of entities, relations, attributes, and so forth, which can effectively represent correlations among data and has been applied in many fields, including search engine optimization, intelligent question answering, and recommendation systems. In this article, we mainly focus on the research and application of the domain-specific knowledge graph in the field of the smart city, which has not been fully paid attention to. Currently, the major problem faced by the smart city lies in data mining and proper application. On the one hand, data are usually stored by government management departments, which creates challenges such as high data storing overhead and inefficient data usage. On the other hand, data cannot be coordinated and collaborated between different city management systems, because data silos exist. By constructing the corresponding knowledge graph, the data of urban traffic, services, and public resources are integrated to provide help for city builders and managers to make important decisions. Therefore, we will review the related literature on the knowledge graph existing in the smart city domain to expore reasearch scopes. Specifically, we will analyze and summarize knowledge graph construction research in the field of smart cities from four perspectives, i.e., smart city ontology, urban data processing, urban knowledge graph construction, and their application. Finally, the research limitations and prospects of the urban knowledge graph are provided.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400814507",
    "type": "article"
  },
  {
    "title": "Exploiting Pre-Trained Models and Low-Frequency Preference for Cost-Effective Transfer-based Attack",
    "doi": "https://doi.org/10.1145/3680553",
    "publication_date": "2024-07-25",
    "publication_year": 2024,
    "authors": "Mingyuan Fan; Cen Chen; Chengyu Wang; Jun Huang",
    "corresponding_authors": "",
    "abstract": "The transferability of adversarial examples enables practical transfer-based attacks. However, existing theoretical analysis cannot effectively reveal what factors contribute to cross-model transferability. Furthermore, the assumption that the target model dataset is available together with expensive prices of training proxy models also leads to insufficient practicality. We first propose a novel frequency perspective to study the transferability and then identify two factors that impair the transferability: an unchangeable intrinsic difference term along with a controllable perturbation-related term. To enhance the transferability, an optimization task with the constraint that decreases the impact of the perturbation-related term is formulated and an approximate solution for the task is designed to address the intractability of Fourier expansion. To address the second issue, we suggest employing pre-trained models as proxy models, which are freely available. Leveraging these advancements, we introduce cost-effective transfer-based attack ( CTA ), which addresses the optimization task in pre-trained models. CTA can be unleashed against broad applications, at any time, with minimal effort and nearly zero cost to attackers . This remarkable feature indeed makes CTA an effective, versatile, and fundamental tool for attacking and understanding a wide range of target models, regardless of their architecture or training dataset used. Extensive experiments show impressive attack performance of CTA across various models trained in seven black-box domains, highlighting the broad applicability and effectiveness of CTA .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400982275",
    "type": "article"
  },
  {
    "title": "A Segment Augmentation and Prediction Consistency Framework for Multi-Label Unknown Intent Detection",
    "doi": "https://doi.org/10.1145/3680286",
    "publication_date": "2024-07-26",
    "publication_year": 2024,
    "authors": "Jiacheng Yang; Miaoxin Chen; Cao Liu; Boqi Dai; Hai-Tao Zheng; Hui Wang; Rui Xie; Hong‐Gee Kim",
    "corresponding_authors": "",
    "abstract": "Multi-label unknown intent detection is a challenging task where each utterance may contain not only multiple known but also unknown intents. To tackle this challenge, pioneers proposed to predict the intent number of the utterance first, then compare it with the results of known intent matching to decide whether the utterence contains unknown intent(s). Though they have made remarkable progress on this task, their methods still suffer from two important issues: (1) It is inadequate to extract multiple intents using only utterance encoding; (2) Optimizing two sub-tasks (intent number prediction and known intent matching) independently leads to inconsistent predictions. In this article, we propose to incorporate segment augmentation rather than only use utterance encoding to better detect multiple intents. We also design a prediction consistency module to bridge the gap between the two sub-tasks. Empirical results on MultiWOZ2.3 and MixSNIPS datasets show that our method achieves state-of-the-art performance and significantly improves the best baseline.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401025309",
    "type": "article"
  },
  {
    "title": "Fair-RGNN: Mitigating Relational Bias on Knowledge Graphs",
    "doi": "https://doi.org/10.1145/3681792",
    "publication_date": "2024-07-31",
    "publication_year": 2024,
    "authors": "Yu-Neng Chuang; Kwei-Herng Lai; Ruixiang Tang; Mengnan Du; Chia-Yuan Chang; Na Zou; Xia Hu",
    "corresponding_authors": "",
    "abstract": "Knowledge graph data are prevalent in real-world applications, and knowledge graph neural networks (KGNNs) are essential techniques for knowledge graph representation learning. Although KGNN effectively models the structural information from knowledge graphs, these frameworks amplify the underlying data bias that leads to discrimination towards certain groups or individuals in resulting applications. Additionally, as existing debiasing approaches mainly focus on entity-wise bias, eliminating the multi-hop relational bias that pervasively exists in knowledge graphs remains an open question. However, it is very challenging to eliminate relational bias due to the sparsity of the paths that generate the bias and the non-linear proximity structure of knowledge graphs. To tackle the challenges, we propose Fair-KGNN, a KGNN framework that simultaneously alleviates multi-hop bias and preserves the proximity information of entity-to-relation in knowledge graphs. The proposed framework is generalizable to mitigate relational bias for all types of KGNN. Fair-KGNN is applicable to incorporate two stateof- the-art KGNN models, RGCN and CompGCN, to mitigate gender-occupation and nationality-salary bias. The experiments carried out on three benchmark knowledge graph datasets demonstrate that Fair-KGNN can effectively mitigate unfair situations during representation learning while preserving the predictive performance of KGNN models. The source code of the proposed method is available at: https://github.com/ynchuang/Mitigating-Relational-Bias-on-Knowledge-Graphs .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401170163",
    "type": "article"
  },
  {
    "title": "Efficient Generation of Hidden Outliers for Improved Outlier Detection",
    "doi": "https://doi.org/10.1145/3690827",
    "publication_date": "2024-08-31",
    "publication_year": 2024,
    "authors": "Jose Cribeiro-Ramallo; Vadim Arzamasov; Klemens Böhm",
    "corresponding_authors": "",
    "abstract": "Outlier generation is a popular technique used to solve important outlier detection tasks. Generating outliers with realistic behavior is challenging. Popular existing methods tend to disregard the “multiple views” property of outliers in high-dimensional spaces.The only existing method accounting for this property falls short in efficiency and effectiveness. We propose Bisect , a new outlier generation method that creates realistic outliers mimicking said property. To do so, Bisect employs a novel proposition introduced in this article stating how to efficiently generate said realistic outliers. Our method has better guarantees and complexity than the current method for recreating “multiple views”. We use the synthetic outliers generated by Bisect to effectively enhance outlier detection in diverse datasets, for multiple use cases. For instance, oversampling with Bisect reduced the error by up to 3 times when compared with the baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402089158",
    "type": "article"
  },
  {
    "title": "NeuralCODE: Neural Compartmental Ordinary Differential Equations Model with AutoML for Interpretable Epidemic Forecasting",
    "doi": "https://doi.org/10.1145/3694688",
    "publication_date": "2024-09-10",
    "publication_year": 2024,
    "authors": "Yuxi Huang; Huandong Wang; Guanghua Liu; Yong Li; Tao Jiang",
    "corresponding_authors": "",
    "abstract": "In order to prevent the re-emergence of an epidemic, predicting its trend while gaining insight into the intrinsic factors affecting it is a key issue in urban governance. Traditional SIR-like compartment models provide insight into the explanatory parameters of an outbreak, and the vast majority of existing deep learning models can predict the course of an outbreak well, but neither performs well in the other’s domain. Simultaneously, studying the commonalities and diversities in the causes of outbreaks among different countrywide regions is also a way to interrupt outbreaks. To address the issues of outbreak intrinsic relationships and prediction, we propose the Neural Compartmental Ordinary Differential Equations (NeuralCODE) model to study the relationship between population movements and outbreak development in different regions. Furthermore, to incorporate the commonalities and diversities in causes among different regions into the prediction and intrinsic inquiry problem, we propose an AutoML framework. Our results found that simply using the NeuralCODE algorithm could obtain better prediction and insight capabilities within different regions. With the introduction of AutoML, it became possible to explore the factors inherent in the epidemic’s development across regions and further improve the original algorithm’s predictive performance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402412191",
    "type": "article"
  },
  {
    "title": "STORM: A MapReduce framework for Symbolic Time Intervals series classification",
    "doi": "https://doi.org/10.1145/3694788",
    "publication_date": "2024-09-11",
    "publication_year": 2024,
    "authors": "Omer David Harel; Robert Moskovitch",
    "corresponding_authors": "",
    "abstract": "Symbolic Time Intervals (STIs) represent events having a non-zero time duration, which are common in various application domains. In this paper, we focus on the challenge of STIs series classification (STIC). While in the related problem of time series classification (TSC) Rocket is well-known for its exceptionally fast runtime while achieving accuracy comparable to state-of-the-art, it has only recently been studied in the field of STIC. However, since Rocket as well as its enhanced variants for TSC (e.g., MiniRocket and MultiRocket) solely rely on global features; they might not always fit best for the classification of thousands of time-units long STI series out-of-the-box, which are rather common in STIC. We introduce STROM – a novel, generic MapReduce framework for STIC, which 1) converts raw input STIs series into multivariate time series (MTS) representation; 2) partitions the converted MTS into fixed-sized blocks, each transformed independently into a uniform latent space via a common, desired Rocket variant used as a base transformation in STORM; and 3) performs sequence classification of the blocks’ transformed feature vectors via a deep, lightweight, bidirectional LSTM network. The evaluation demonstrates that STORM significantly improves accuracy over eight state-of-the-art methods for STIC either when applied with MiniRocket and MultiRocket as base transformations, as well as over the baselines of applying the respective Rocket variants directly to the converted MTS representation. That is, while also reporting overall comparable training times, on a benchmark of eight real-world STIC datasets including both extremely long and short STIs series.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402456872",
    "type": "article"
  },
  {
    "title": "Scale-Aware Neural Architecture Search for Multivariate Time Series Forecasting",
    "doi": "https://doi.org/10.1145/3701038",
    "publication_date": "2024-10-18",
    "publication_year": 2024,
    "authors": "Donghui Chen; Ling Chen; Zongjiang Shang; Youdong Zhang; Bo Wen; Chenghu Yang",
    "corresponding_authors": "",
    "abstract": "Multivariate time series (MTS) forecasting has attracted much attention in many intelligent applications. It is not a trivial task, as we need to consider both intra-variable dependencies and inter-variable dependencies. However, existing works are designed for specific scenarios, and require much domain knowledge and expert efforts, which is difficult to transfer between different scenarios. In this paper, we propose a scale-aware neural architecture search framework for MTS forecasting (SNAS4MTF). A multi-scale decomposition module transforms raw time series into multi-scale sub-series, which can preserve multi-scale temporal patterns. An adaptive graph learning module infers the different inter-variable dependencies under different time scales without any prior knowledge. For MTS forecasting, a search space is designed to capture both intra-variable dependencies and inter-variable dependencies at each time scale. The multi-scale decomposition, adaptive graph learning, and neural architecture search modules are jointly learned in an end-to-end framework. Extensive experiments on two real-world datasets demonstrate that SNAS4MTF achieves a promising performance compared with the state-of-the-art methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403539871",
    "type": "article"
  },
  {
    "title": "SOHUPDS+: An Efficient One-phase Algorithm for Mining High Utility Patterns over a Data Stream",
    "doi": "https://doi.org/10.1145/3702645",
    "publication_date": "2024-11-04",
    "publication_year": 2024,
    "authors": "Bijay Prasad Jaysawal; Jen-Wei Huang",
    "corresponding_authors": "",
    "abstract": "Existing algorithms for mining high utility patterns over a data stream are two-phase algorithms that are not scalable due to the large number of candidates generation in the first phase, particularly when the minimum utility threshold is low. Moreover, in the second phase, the algorithm needs to scan the database again to find out actual utility for candidates. In this paper, we propose one-phase algorithm SOHUPDS+ to mine high utility itemsets in the current sliding window of the data stream with respect to absolute or relative minimum utility threshold. To facilitate SOHUPDS+, we propose a data structure IUDataListSW+ , which stores and maintains utility and upper-bound values of the items in the current sliding window when sliding window advances. In addition, we propose a transaction merging strategy, called BitmapTransactionMerging , which saves execution time for utility and upper-bound values computations in denser datasets. Moreover, we propose update strategies to utilize mined high utility patterns from the previous sliding window to update high utility patterns in the current sliding window. The results of experiments illustrate that SOHUPDS+ is more efficient than the state-of-the-art algorithms in terms of execution time as well as memory usage in most of the experiments on various datasets.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404022116",
    "type": "article"
  },
  {
    "title": "Question Embedding on Weighted Heterogeneous Information Network for Knowledge Tracing",
    "doi": "https://doi.org/10.1145/3703158",
    "publication_date": "2024-11-04",
    "publication_year": 2024,
    "authors": "Jianwen Sun; Shangheng Du; Jianpeng Zhou; X. Yuan; Xiaoxuan Shen; Ruxia Liang",
    "corresponding_authors": "",
    "abstract": "Knowledge Tracing (KT) aims to predict students’ future performance on answering questions based on their historical exercise sequences. To alleviate the problem of data sparsity in KT, recent works have introduced auxiliary information to mine question similarity, resulting in the enhancement of question embeddings. Nonetheless, there remains a gap in developing an approach that effectively incorporates various forms of auxiliary information, including relational information (e.g., question-student , question-skill relation), relationship attributes (e.g., correctness indicating a student's performance on a question), and node attributes (e.g., student ability ). To tackle this challenge, the Similarity-enhanced Question Embedding (SimQE) method for KT is proposed, with its central feature being the utilization of weighted and attributed meta-paths for extracting question similarity. To capture multi-dimensional question similarity semantics by integrating multiple relations, various meta-paths are constructed for learning question embeddings separately. These embeddings, each encoding different similarity semantics, are then fused to serve the task of KT. To capture finer-grained similarity by leveraging the relationship attributes and node attributes on the meta-paths, the biased random walk algorithm is designed. In addition, the auxiliary node generation method is proposed to capture high-order question similarity. Finally, extensive experiments conducted on 6 datasets demonstrate that SimQE performs the best among 10 representative question embedding methods. Furthermore, SimQE proves to be more effective in alleviating the problem of data sparsity.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404022126",
    "type": "article"
  },
  {
    "title": "MagNet: Multilevel Dynamic Wavelet Graph Neural Network for Multivariate Time Series Classification",
    "doi": "https://doi.org/10.1145/3703915",
    "publication_date": "2024-11-12",
    "publication_year": 2024,
    "authors": "Xiaobin Hong; Jiangyi Hu; Taishan Xu; Xiancheng Ren; Feng Wu; Xiangkai Ma; Wenzhong Li",
    "corresponding_authors": "",
    "abstract": "Multivariate time series classification (MTSC) is a fundamental data mining task, which is widely applied in the fields like health care and energy management. However, the existing MTSC methods are mostly adapted from univariate versions and model the static patterns among series in the time domain. We argue they fail to capture the inter-dependencies across variables and rarely consider the unique dynamic features in multilevel frequencies, which are susceptible to signal noise and lack sufficient feature extraction capability to achieve satisfactory classification accuracy. To address these issues, we propose a novel M ultilevel dyn a mic wavelet g raph neural Net work called MagNet, which effectively captures inherent temporal-frequency dependencies in multivariate time series data in a global view, facilitating the information flow among interrelated variables and leveraging learnable graph neural networks (GNNs) to uncover dynamic frequency dependencies. We propose an orthogonal temporal convolution layer that utilizes soft orthogonal losses to constrain features learned at different frequency components to reduce feature redundancy. Additionally, we introduce a hierarchical graph coarsening operator to address the flat learning challenges in traditional GNNs. Our dynamic wavelet graph neural network and hierarchical coarsening enable deep model stacking and end-to-end learning. Extensive experiments on 30 UEA benchmarks demonstrate that our method outperforms the state-of-the-art baselines in the MTSC tasks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404271517",
    "type": "article"
  },
  {
    "title": "Bayesian Frequency Estimation Under Local Differential Privacy With an Adaptive Randomized Response Mechanism",
    "doi": "https://doi.org/10.1145/3706584",
    "publication_date": "2024-12-03",
    "publication_year": 2024,
    "authors": "Soner Aydin; Sinan Yıldırım",
    "corresponding_authors": "",
    "abstract": "Frequency estimation plays a critical role in many applications involving personal and private categorical data. Such data are often collected sequentially over time, making it valuable to estimate their distribution online while preserving privacy. We propose AdOBEst-LDP, a new algorithm for adaptive, online Bayesian estimation of categorical distributions under local differential privacy (LDP). The key idea behind AdOBEst-LDP is to enhance the utility of future privatized categorical data by leveraging inference from previously collected privatized data. To achieve this, AdOBEst-LDP uses a new adaptive LDP mechanism to collect privatized data. This LDP mechanism constrains its output to a subset of categories that ‘predicts’ the next user's data. By adapting the subset selection process to the past privatized data via Bayesian estimation, the algorithm improves the utility of future privatized data. To quantify utility, we explore various well-known information metrics, including (but not limited to) the Fisher information matrix, total variation distance, and information entropy. For Bayesian estimation, we utilize posterior sampling through stochastic gradient Langevin dynamics, a computationally efficient approximate Markov chain Monte Carlo (MCMC) method. We provide a theoretical analysis showing that (i) the posterior distribution of the category probabilities targeted with Bayesian estimation converges to the true probabilities even for approximate posterior sampling, and (ii) AdOBEst-LDP eventually selects the optimal subset for its LDP mechanism with high probability if posterior sampling is performed exactly. We also present numerical results to validate the estimation accuracy of AdOBEst-LDP. Our comparisons show its superior performance against non-adaptive and semi-adaptive competitors across different privacy levels and distributional parameters.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404962768",
    "type": "article"
  },
  {
    "title": "A Multiple Attention Layer-shareable Method for Link Prediction in Multilayer Networks",
    "doi": "https://doi.org/10.1145/3709142",
    "publication_date": "2024-12-24",
    "publication_year": 2024,
    "authors": "Huan Wang; Teng Yu; Lingsong Qin; Xuan Guo; Po Hu",
    "corresponding_authors": "",
    "abstract": "Link prediction in multilayer networks aims to predict missing links at the target layer by incorporating structural information from both auxiliary layers and the target layer. Existing methods tend to learn layer-specific knowledge to maximize the link prediction performance on a specific network layer. However, they have difficulty incorporating multilayer structural information to improve the link prediction performance. Therefore, we propose a Multiple Attention Layer-shareable Method (MALM) for link prediction in multilayer networks, which consists of a feature encoder, a knowledge learner, and a fusion predictor. The feature encoder introduces multiple attention mechanisms to encode the feature representations of links by differentiating the importance of structural information for each link. In cooperation with the feature encoder, the knowledge learner splits the link prediction tasks into different layers and employs meta-learning to learn layer-shareable knowledge from these link prediction tasks. Finally, the fusion predictor combines the learned layer-shareable knowledge with the layer-specific knowledge at the target layer for link prediction. Experiments on real-world datasets demonstrate that the proposed MALM outperforms existing state-of-the-art baselines in link prediction in multilayer networks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405737191",
    "type": "article"
  },
  {
    "title": "Sampling Sparse Representations with Randomized Measurement Langevin Dynamics",
    "doi": "https://doi.org/10.1145/3427585",
    "publication_date": "2021-02-10",
    "publication_year": 2021,
    "authors": "Kafeng Wang; Haoyi Xiong; Jiang Bian; Zhanxing Zhu; Qian Gao; Zhishan Guo; Chengzhong Xu; Jun Huan; Dejing Dou",
    "corresponding_authors": "",
    "abstract": "Stochastic Gradient Langevin Dynamics (SGLD) have been widely used for Bayesian sampling from certain probability distributions, incorporating derivatives of the log-posterior. With the derivative evaluation of the log-posterior distribution, SGLD methods generate samples from the distribution through performing as a thermostats dynamics that traverses over gradient flows of the log-posterior with certainly controllable perturbation. Even when the density is not known, existing solutions still can first learn the kernel density models from the given datasets, then produce new samples using the SGLD over the kernel density derivatives. In this work, instead of exploring new samples from kernel spaces, a novel SGLD sampler, namely, Randomized Measurement Langevin Dynamics (RMLD) is proposed to sample the high-dimensional sparse representations from the spectral domain of a given dataset. Specifically, given a random measurement matrix for sparse coding, RMLD first derives a novel likelihood evaluator of the probability distribution from the loss function of LASSO, then samples from the high-dimensional distribution using stochastic Langevin dynamics with derivatives of the logarithm likelihood and Metropolis–Hastings sampling. In addition, new samples in low-dimensional measuring spaces can be regenerated using the sampled high-dimensional vectors and the measurement matrix. The algorithm analysis shows that RMLD indeed projects a given dataset into a high-dimensional Gaussian distribution with Laplacian prior, then draw new sparse representation from the dataset through performing SGLD over the distribution. Extensive experiments have been conducted to evaluate the proposed algorithm using real-world datasets. The performance comparisons on three real-world applications demonstrate the superior performance of RMLD beyond baseline methods.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3131934271",
    "type": "article"
  },
  {
    "title": "CoCoS: Fast and Accurate Distributed Triangle Counting in Graph Streams",
    "doi": "https://doi.org/10.1145/3441487",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Kijung Shin; Euiwoong Lee; Jinoh Oh; Mohammad Hammoud; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "Given a graph stream, how can we estimate the number of triangles in it using multiple machines with limited storage? Specifically, how should edges be processed and sampled across the machines for rapid and accurate estimation? The count of triangles (i.e., cliques of size three) has proven useful in numerous applications, including anomaly detection, community detection, and link recommendation. For triangle counting in large and dynamic graphs, recent work has focused largely on streaming algorithms and distributed algorithms but little on their combinations for “the best of both worlds.” In this work, we propose CoCoS , a fast and accurate distributed streaming algorithm for estimating the counts of global triangles (i.e., all triangles) and local triangles incident to each node. Making one pass over the input stream, CoCoS carefully processes and stores the edges across multiple machines so that the redundant use of computational and storage resources is minimized. Compared to baselines, CoCoS is: (a) accurate: giving up to smaller estimation error; (b) fast : up to faster, scaling linearly with the size of the input stream; and (c) theoretically sound : yielding unbiased estimates.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3152595148",
    "type": "article"
  },
  {
    "title": "Attribute-Guided Network Sampling Mechanisms",
    "doi": "https://doi.org/10.1145/3441445",
    "publication_date": "2021-04-18",
    "publication_year": 2021,
    "authors": "Suhansanu Kumar; Hari Sundaram",
    "corresponding_authors": "",
    "abstract": "This article introduces a novel task-independent sampler for attributed networks. The problem is important because while data mining tasks on network content are common, sampling on internet-scale networks is costly. Link-trace samplers such as Snowball sampling, Forest Fire, Random Walk, and Metropolis–Hastings Random Walk are widely used for sampling from networks. The design of these attribute-agnostic samplers focuses on preserving salient properties of network structure, and are not optimized for tasks on node content. This article has three contributions. First, we propose a task-independent, attribute aware link-trace sampler grounded in Information Theory. Our sampler greedily adds to the sample the node with the most informative (i.e., surprising) neighborhood. The sampler tends to rapidly explore the attribute space, maximally reducing the surprise of unseen nodes. Second, we prove that content sampling is an NP-hard problem. A well-known algorithm best approximates the optimization solution within 1 − 1/ e , but requires full access to the entire graph. Third, we show through empirical counterfactual analysis that in many real-world datasets, network structure does not hinder the performance of surprise based link-trace samplers. Experimental results over 18 real-world datasets reveal: surprise-based samplers are sample efficient and outperform the state-of-the-art attribute-agnostic samplers by a wide margin (e.g., 45% performance improvement in clustering tasks).",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3152612488",
    "type": "article"
  },
  {
    "title": "Online Tensor-Based Learning Model for Structural Damage Detection",
    "doi": "https://doi.org/10.1145/3451217",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Ali Anaissi; Basem Suleiman; Seid Miad Zandavi",
    "corresponding_authors": "",
    "abstract": "The online analysis of multi-way data stored in a tensor <?TeX $\\mathcal {X} \\in \\mathbb {R} ^{I_1 \\times \\dots \\times I_N}$?> has become an essential tool for capturing the underlying structures and extracting the sensitive features that can be used to learn a predictive model. However, data distributions often evolve with time and a current predictive model may not be sufficiently representative in the future. Therefore, incrementally updating the tensor-based features and model coefficients are required in such situations. A new efficient tensor-based feature extraction, named Nesterov Stochastic Gradient Descent (NeSGD), is proposed for online <?TeX $CANDECOMP/PARAFAC$?> (CP) decomposition. According to the new features obtained from the resultant matrices of NeSGD, a new criterion is triggered for the updated process of the online predictive model. Experimental evaluation in the field of structural health monitoring using laboratory-based and real-life structural datasets shows that our methods provide more accurate results compared with existing online tensor analysis and model learning. The results showed that the proposed methods significantly improved the classification error rates, were able to assimilate the changes in the positive data distribution over time, and maintained a high predictive accuracy in all case studies.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3160385264",
    "type": "article"
  },
  {
    "title": "Sequential Transform Learning",
    "doi": "https://doi.org/10.1145/3447394",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Shalini Sharma; Angshul Majumdar",
    "corresponding_authors": "",
    "abstract": "This work proposes a new approach for dynamical modeling; we call it sequential transform learning. This is loosely based on the transform (analysis dictionary) learning formulation. This is the first work on this topic. Transform learning, was originally developed for static problems; we modify it to model dynamical systems by introducing a feedback loop. The learnt transform coefficients for the t th instant are fed back along with the t + 1st sample, thereby establishing a Markovian relationship. Furthermore, the formulation is made supervised by the label consistency cost. Our approach keeps the best of two worlds, marrying the interpretability and uncertainty measure of signal processing with the function approximation ability of neural networks. We have carried out experiments on one of the most challenging problems in dynamical modeling - stock forecasting. Benchmarking with the state-of-the-art has shown that our method excels over the rest.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3161120128",
    "type": "article"
  },
  {
    "title": "Constrained Dual-Level Bandit for Personalized Impression Regulation in Online Ranking Systems",
    "doi": "https://doi.org/10.1145/3461340",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Zhao Li; Junshuai Song; Zehong Hu; Zhen Wang; Jun Gao",
    "corresponding_authors": "",
    "abstract": "Impression regulation plays an important role in various online ranking systems, e.g. , e-commerce ranking systems always need to achieve local commercial demands on some pre-labeled target items like fresh item cultivation and fraudulent item counteracting while maximizing its global revenue. However, local impression regulation may cause “butterfly effects” on the global scale, e.g. , in e-commerce, the price preference fluctuation in initial conditions (overpriced or underpriced items) may create a significantly different outcome, thus affecting shopping experience and bringing economic losses to platforms. To prevent “butterfly effects”, some researchers define their regulation objectives with global constraints, by using contextual bandit at the page-level that requires all items on one page sharing the same regulation action, which fails to conduct impression regulation on individual items. To address this problem, in this article, we propose a personalized impression regulation method that can directly makes regulation decisions for each user-item pair. Specifically, we model the regulation problem as a C onstrained D ual-level B andit (CDB) problem, where the local regulation action and reward signals are at the item-level while the global effect constraint on the platform impression can be calculated at the page-level only. To handle the asynchronous signals, we first expand the page-level constraint to the item-level and then derive the policy updating as a second-order cone optimization problem. Our CDB approaches the optimal policy by iteratively solving the optimization problem. Experiments are performed on both offline and online datasets, and the results, theoretically and empirically, demonstrate CDB outperforms state-of-the-art algorithms.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3183239424",
    "type": "article"
  },
  {
    "title": "Generic Multi-label Annotation via Adaptive Graph and Marginalized Augmentation",
    "doi": "https://doi.org/10.1145/3451884",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Lichen Wang; Zhengming Ding; Yun Fu",
    "corresponding_authors": "",
    "abstract": "Multi-label learning recovers multiple labels from a single instance. It is a more challenging task compared with single-label manner. Most multi-label learning approaches need large-scale well-labeled samples to achieve high accurate performance. However, it is expensive to build such a dataset. In this work, we propose a generic multi-label learning framework based on Adaptive Graph and Marginalized Augmentation (AGMA) in a semi-supervised scenario. Generally speaking, AGMA makes use of a small amount of labeled data associated with a lot of unlabeled data to boost the learning performance. First, an adaptive similarity graph is learned to effectively capture the intrinsic structure within the data. Second, marginalized augmentation strategy is explored to enhance the model generalization and robustness. Third, a feature-label autoencoder is further deployed to improve inferring efficiency. All the modules are jointly trained to benefit each other. State-of-the-art benchmarks in both traditional and zero-shot multi-label learning scenarios are evaluated. Experiments and ablation studies illustrate the accuracy and efficiency of our AGMA method.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3185442830",
    "type": "article"
  },
  {
    "title": "Wealth Flow Model: Online Portfolio Selection Based on Learning Wealth Flow Matrices",
    "doi": "https://doi.org/10.1145/3464308",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Jianfei Yin; Ruili Wang; Yeqing Guo; Yizhe Bai; Shunda Ju; Liu Weili; Joshua Zhexue Huang",
    "corresponding_authors": "",
    "abstract": "This article proposes a deep learning solution to the online portfolio selection problem based on learning a latent structure directly from a price time series. It introduces a novel wealth flow matrix for representing a latent structure that has special regular conditions to encode the knowledge about the relative strengths of assets in portfolios. Therefore, a wealth flow model (WFM) is proposed to learn wealth flow matrices and maximize portfolio wealth simultaneously. Compared with existing approaches, our work has several distinctive benefits: (1) the learning of wealth flow matrices makes our model more generalizable than models that only predict wealth proportion vectors, and (2) the exploitation of wealth flow matrices and the exploration of wealth growth are integrated into our deep reinforcement algorithm for the WFM. These benefits, in combination, lead to a highly-effective approach for generating reasonable investment behavior, including short-term trend following, the following of a few losers, no self-investment, and sparse portfolios. Extensive experiments on five benchmark datasets from real-world stock markets confirm the theoretical advantage of the WFM, which achieves the Pareto improvements in terms of multiple performance indicators and the steady growth of wealth over the state-of-the-art algorithms.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3201637219",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on social computing, behavioral modeling, and prediction",
    "doi": "https://doi.org/10.1145/1514888.1514889",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Huan Liu; John Salerno; Michael Young; Rakesh Agrawal; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to special issue on social computing, behavioral modeling, and prediction Editors: Huan Liu View Profile , John Salerno View Profile , Michael Young View Profile , Rakesh Agrawal View Profile , Philip S. Yu View Profile Authors Info & Claims ACM Transactions on Knowledge Discovery from DataVolume 3Issue 2April 2009 Article No.: 6pp 1–3https://doi.org/10.1145/1514888.1514889Published:21 April 2009Publication History 6citation751DownloadsMetricsTotal Citations6Total Downloads751Last 12 Months2Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2088192114",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1644873",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Recommender systems provide users with personalized suggestions for products or services. These systems often rely on collaborating filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The ...",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4250065571",
    "type": "paratext"
  },
  {
    "title": "Listwise Learning to Rank from Crowds",
    "doi": "https://doi.org/10.1145/2910586",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Ou Wu; You Qiang; Fen Xia; Lei Ma; Weiming Hu",
    "corresponding_authors": "",
    "abstract": "Learning to rank has received great attention in recent years as it plays a crucial role in many applications such as information retrieval and data mining. The existing concept of learning to rank assumes that each training instance is associated with a reliable label. However, in practice, this assumption does not necessarily hold true as it may be infeasible or remarkably expensive to obtain reliable labels for many learning to rank applications. Therefore, a feasible approach is to collect labels from crowds and then learn a ranking function from crowdsourcing labels. This study explores the listwise learning to rank with crowdsourcing labels obtained from multiple annotators, who may be unreliable. A new probabilistic ranking model is first proposed by combining two existing models. Subsequently, a ranking function is trained by proposing a maximum likelihood learning approach, which estimates ground-truth labels and annotator expertise, and trains the ranking function iteratively. In practical crowdsourcing machine learning, valuable side information (e.g., professional grades) about involved annotators is normally attainable. Therefore, this study also investigates learning to rank from crowd labels when side information on the expertise of involved annotators is available. In particular, three basic types of side information are investigated, and corresponding learning algorithms are consequently introduced. Further, the top-k learning to rank from crowdsourcing labels are explored to deal with long training ranking lists. The proposed algorithms are tested on both synthetic and real-world data. Results reveal that the maximum likelihood estimation approach significantly outperforms the average approach and existing crowdsourcing regression methods. The performances of the proposed algorithms are comparable to those of the learning model in consideration reliable labels. The results of the investigation further indicate that side information is helpful in inferring both ranking functions and expertise degrees of annotators.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2499321295",
    "type": "article"
  },
  {
    "title": "Summarizing User-item Matrix By Group Utility Maximization",
    "doi": "https://doi.org/10.1145/3578586",
    "publication_date": "2023-01-03",
    "publication_year": 2023,
    "authors": "Yongjie Wang; Ke Wang; Cheng Long; Chunyan Miao",
    "corresponding_authors": "",
    "abstract": "A user-item utility matrix represents the utility (or preference) associated with each (user, item) pair, such as citation counts, rating/vote on items or locations, and clicks on items. A high utility value indicates a strong association of the pair. In this work, we consider the problem of summarizing strong association for a large user-item matrix using a small summary size. Traditional techniques fail to distinguish user groups associated with different items (such as top- l item selection) or fail to focus on high utility (such as similarity- based subspace clustering and biclustering). We formulate a new problem, called Group Utility Maximization (GUM), to summarize the entire user population through k user groups and l items for each group; the goal is to maximize the total utility of selected items over all groups collectively. We show this problem is NP-hard even for l =1. We present two algorithms. One greedily finds the next group, called Greedy algorithm, and the other iteratively refines existing k groups, called k -max algorithm. Greedy algorithm provides the \\((1-\\frac{1}{e})\\) approximation guarantee for a nonnegative utility matrix, whereas k -max algorithm is more efficient for large datasets. We evaluate these algorithms on real-life datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4313437244",
    "type": "article"
  },
  {
    "title": "Fitting Imbalanced Uncertainties in Multi-output Time Series Forecasting",
    "doi": "https://doi.org/10.1145/3584704",
    "publication_date": "2023-02-16",
    "publication_year": 2023,
    "authors": "Jiezhu Cheng; Kaizhu Huang; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "We focus on multi-step ahead time series forecasting with the multi-output strategy. From the perspective of multi-task learning (MTL), we recognize imbalanced uncertainties between prediction tasks of different future time steps. Unexpectedly, trained by the standard summed Mean Squared Error (MSE) loss, existing multi-output forecasting models may suffer from performance drops due to the inconsistency between the loss function and the imbalance structure. To address this problem, we reformulate each prediction task as a distinct Gaussian Mixture Model (GMM) and derive a multi-level Gaussian mixture loss function to better fit imbalanced uncertainties in multi-output time series forecasting. Instead of using the two-step Expectation-Maximization (EM) algorithm, we apply the self-attention mechanism on the task-specific parameters to learn the correlations between different prediction tasks and generate the weight distribution for each GMM component. In this way, our method jointly optimizes the parameters of the forecasting model and the mixture model simultaneously in an end-to-end fashion, avoiding the need of two-step optimization. Experiments on three real-world datasets demonstrate the effectiveness of our multi-level Gaussian mixture loss compared to models trained with the standard summed MSE loss function. All the experimental data and source code are available at https://github.com/smallGum/GMM-FNN .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4321089625",
    "type": "article"
  },
  {
    "title": "Graph Neural Networks with Motisf-aware for Tenuous Subgraph Finding",
    "doi": "https://doi.org/10.1145/3589643",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Heli Sun; Miaomiao Sun; Liu Xue-chun; Linlin Zhu; Liang He; Xiaolin Jia; Yuan Chen",
    "corresponding_authors": "",
    "abstract": "Tenuous subgraph finding aims to detect a subgraph with few social interactions and weak relationships among nodes. Despite significant efforts made on this task, they are mostly carried out in view of graph-structured data. These methods depend on calculating the shortest path and need to enumerate all the paths between nodes, which suffer the combinatorial explosion. Moreover, they all lack the integration of neighborhood information. To this end, we propose a novel model named Graph Neural Network with Motif-aware for tenuous subgraph finding (GNNM), a neighborhood aggregation-based GNN framework that can capture the latent relationship between nodes. We design a GNN module to project nodes into a low-dimensional vector combining the higher-order correlation within nodes based on a motif-aware module. Then we design greedy algorithms in vector space to obtain a tenuous subgraph whose size is greater than a specified constraint. Particularly, considering that existing evaluation indicators cannot capture the latent friendship between nodes, we introduce a novel Potential Friend concept to measure the tenuity of a graph from a new perspective. Experimental results on the real-world and synthetic datasets demonstrate that our proposed method GNNM outperforms existing algorithms in efficiency and subgraph quality.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4362453183",
    "type": "article"
  },
  {
    "title": "Discrete Listwise Content-aware Recommendation",
    "doi": "https://doi.org/10.1145/3609334",
    "publication_date": "2023-07-14",
    "publication_year": 2023,
    "authors": "Fangyuan Luo; Jun Wu; Tao Wang",
    "corresponding_authors": "",
    "abstract": "To perform online inference efficiently, hashing techniques, devoted to encoding model parameters as binary codes, play a key role in reducing the computational cost of content-aware recommendation (CAR), particularly on devices with limited computation resource. However, current hashing methods for CAR fail to align their learning objectives (e.g., squared loss) with the ranking-based metrics (e.g., Normalized Discounted Cumulative Gain (NDCG)), resulting in suboptimal recommendation accuracy. In this article, we propose a novel ranking-based CAR hashing method based on Factorization Machine (FM), called Discrete Listwise FM (DLFM), for fast and accurate recommendation. Concretely, our DLFM is to optimize NDCG in the Hamming space for preserving the listwise user-item relationships. We devise an efficient algorithm to resolve the challenging DLFM problem, which can directly learn binary parameters in a relaxed continuous solution space, without additional quantization. Particularly, our theoretical analysis shows that the optimal solution to the relaxed continuous optimization problem is approximately the same as that of the original discrete optimization problem. Through extensive experiments on two real-world datasets, we show that DLFM consistently outperforms state-of-the-art hashing-based recommendation techniques.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4384343088",
    "type": "article"
  },
  {
    "title": "Fair and Private Data Preprocessing through Microaggregation",
    "doi": "https://doi.org/10.1145/3617377",
    "publication_date": "2023-08-24",
    "publication_year": 2023,
    "authors": "Vladimiro González-Zelaya; Julián Salas; David Megías; Paolo Missier",
    "corresponding_authors": "",
    "abstract": "Privacy protection for personal data and fairness in automated decisions are fundamental requirements for responsible Machine Learning. Both may be enforced through data preprocessing and share a common target: data should remain useful for a task, while becoming uninformative of the sensitive information. The intrinsic connection between privacy and fairness implies that modifications performed to guarantee one of these goals, may have an effect on the other, e.g., hiding a sensitive attribute from a classification algorithm might prevent a biased decision rule having such attribute as a criterion. This work resides at the intersection of algorithmic fairness and privacy. We show how the two goals are compatible, and may be simultaneously achieved, with a small loss in predictive performance. Our results are competitive with both state-of-the-art fairness correcting algorithms and hybrid privacy-fairness methods. Experiments were performed on three widely used benchmark datasets: Adult Income , COMPAS, and German Credit .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386136151",
    "type": "article"
  },
  {
    "title": "Attacking Shortest Paths by Cutting Edges",
    "doi": "https://doi.org/10.1145/3622941",
    "publication_date": "2023-09-11",
    "publication_year": 2023,
    "authors": "Benjamin A. Miller; Zohair Shafi; Wheeler Ruml; Yevgeniy Vorobeychik; Tina Eliassi‐Rad; Scott Alfeld",
    "corresponding_authors": "",
    "abstract": "Identifying shortest paths between nodes in a network is a common graph analysis problem that is important for many applications involving routing of resources. An adversary that can manipulate the graph structure could alter traffic patterns to gain some benefit (e.g., make more money by directing traffic to a toll road). This article presents the Force Path Cut problem, in which an adversary removes edges from a graph to make a particular path the shortest between its terminal nodes. We prove that the optimization version of this problem is APX-hard but introduce PATHATTACK , a polynomial-time approximation algorithm that guarantees a solution within a logarithmic factor of the optimal value. In addition, we introduce the Force Edge Cut and Force Node Cut problems, in which the adversary targets a particular edge or node, respectively, rather than an entire path. We derive a nonconvex optimization formulation for these problems and derive a heuristic algorithm that uses PATHATTACK as a subroutine. We demonstrate all of these algorithms on a diverse set of real and synthetic networks, illustrating where the proposed algorithms provide the greatest improvement over baseline methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386607418",
    "type": "article"
  },
  {
    "title": "A Multisource Data Fusion-based Heterogeneous Graph Attention Network for Competitor Prediction",
    "doi": "https://doi.org/10.1145/3625101",
    "publication_date": "2023-09-21",
    "publication_year": 2023,
    "authors": "Xiaoqing Ye; Yang Sun; Dun Liu; Tianrui Li",
    "corresponding_authors": "",
    "abstract": "Competitor identification is an essential component of corporate strategy. With the rapid development of artificial intelligence, various data-mining methodologies and frameworks have emerged to identify competitors. In general, the competitiveness among companies is determined by both market commonality and resource similarity. However, because resource information is more difficult to obtain than market information, existing studies primarily identify competitors via market commonality. To address this limitation, we introduce multisource company descriptions as well as heterogeneous business relationships, and we propose a novel method for simultaneously mining the market commonality and resource similarity. First, we use multisource company descriptions to represent companies and transform the heterogeneous business relationships into a heterogeneous business network. Then, we propose a novel multisource data fusion-based heterogeneous graph attention network (MHGAT) to learn the pairwise competitive relationships between companies. Specifically, a graph neural network-based model is proposed to learn the embeddings of companies by preserving their competition, and a multilevel attention framework is designed to integrate the embeddings from neighboring company level, heterogeneous relationship level, and multisource description level. Finally, experiments on a real-world dataset verify the effectiveness of our proposed MHGAT and demonstrate the usefulness of company descriptions and business relationships in competitor identification.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386929160",
    "type": "article"
  },
  {
    "title": "Maximizing the Diversity of Exposure in Online Social Networks by Identifying Users with Increased Susceptibility to Persuasion",
    "doi": "https://doi.org/10.1145/3625826",
    "publication_date": "2023-09-29",
    "publication_year": 2023,
    "authors": "Ahmad Zareie; Rizos Sakellariou",
    "corresponding_authors": "",
    "abstract": "Individuals may have a range of opinions on controversial topics. However, the ease of making friendships in online social networks tends to create groups of like-minded individuals, who propagate messages that reinforce existing opinions and ignore messages expressing opposite opinions. This creates a situation where there is a decrease in the diversity of messages to which users are exposed ( diversity of exposure ). This means that users do not easily get the chance to be exposed to messages containing alternative viewpoints; it is even more unlikely that they forward such messages to their friends. Increasing the chance that such messages are propagated implies that an individuals’ susceptibility to persuasion is increased, something that may ultimately increase the diversity of messages to which users are exposed. This article formulates a novel problem which aims to identify a small set of users for whom increasing susceptibility to persuasion maximizes the diversity of exposure of all users in the network. We study the properties of this problem and develop a method to find a solution with an approximation guarantee. For this, we first prove that the problem is neither submodular nor supermodular and then we develop submodular bounds for it. These bounds are used in the Sandwich framework to propose a method which approximates the solution using reverse sampling. The proposed method is validated using four real-world datasets. The obtained results demonstrate the superiority of the proposed method compared to baseline approaches.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387188357",
    "type": "article"
  },
  {
    "title": "EFMVFL: An Efficient and Flexible Multi-party Vertical Federated Learning without a Third Party",
    "doi": "https://doi.org/10.1145/3627993",
    "publication_date": "2023-10-17",
    "publication_year": 2023,
    "authors": "Yimin Huang; Wanwan Wang; Xingying Zhao; Yukun Wang; Xinyu Feng; Hao He; Ming Yao",
    "corresponding_authors": "",
    "abstract": "Federated learning (FL) is a machine learning setting which allows multiple participants collaboratively to train a model under the orchestration of a server without disclosing their local data. Vertical federated learning (VFL) is a special structure in FL. It handles the situation where participants have the same ID space but different feature spaces. In order to guarantee the security and privacy of the local data of each participant, homomorphic encryption (HE) is often used to transmit intermediate parameters or data during the training process. In most VFL frameworks, a trusted third-party server is necessary because the plaintexts of the parameters need to be revealed for the computation. However, it is hard to find such a credible entity in the real world. Existing methods for solving this problem are either communication-intensive or unsuitable for multi-party scenarios. By combining secret sharing (SS) and HE, we propose a novel VFL framework without any trusted third parties called EFMVFL. It allows intermediate parameters to be transmitted among multiple parties without revealing the plaintexts. EFMVFL is applicable to generalized linear models (GLMs) and supports flexible expansion to multiple participants. Extensive experiments under Logistic Regression and Poisson Regression show that our framework is outstanding in communication (reduced by 3.2×– 6.8×) and efficiency (accelerated by 1.6×– 3.1×).",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387708150",
    "type": "article"
  },
  {
    "title": "A Lightweight, Effective, and Efficient Model for Label Aggregation in Crowdsourcing",
    "doi": "https://doi.org/10.1145/3630102",
    "publication_date": "2023-10-26",
    "publication_year": 2023,
    "authors": "Yi Yang; Zhong‐Qiu Zhao; Gongqing Wu; Xingrui Zhuo; Qing Liu; Quan Bai; Weihua Li",
    "corresponding_authors": "",
    "abstract": "Due to the presence of noise in crowdsourced labels, label aggregation (LA) has become a standard procedure for post-processing these labels. LA methods estimate true labels from crowdsourced labels by modeling worker quality. However, most existing LA methods are iterative in nature. They require multiple passes through all crowdsourced labels, jointly and iteratively updating true labels and worker qualities until a termination condition is met. As a result, these methods are burdened with high space and time complexities, which restrict their applicability in scenarios where scalability and online aggregation are essential. Furthermore, defining a suitable termination condition for iterative algorithms can be challenging. In this article, we view LA as a dynamic system and represent it as a Dynamic Bayesian Network. From this dynamic model, we derive two lightweight and scalable algorithms: LA onepass and LA twopass . These algorithms can efficiently and effectively estimate worker qualities and true labels by traversing all labels at most twice, thereby eliminating the need for explicit termination conditions and multiple traversals over the crowdsourced labels. Due to their dynamic nature, the proposed algorithms are also capable of performing label aggregation online. We provide theoretical proof of the convergence property of the proposed algorithms and bound the error of the estimated worker qualities. Furthermore, we analyze the space and time complexities of our proposed algorithms, demonstrating their equivalence to those of majority voting. Through experiments conducted on 20 real-world datasets, we demonstrate that our proposed algorithms can effectively and efficiently aggregate labels in both offline and online settings, even though they traverse all labels at most twice. The code is on https://github.com/yyang318/LA_onepass .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387954182",
    "type": "article"
  },
  {
    "title": "Parameter-Agnostic Deep Graph Clustering",
    "doi": "https://doi.org/10.1145/3633783",
    "publication_date": "2023-11-27",
    "publication_year": 2023,
    "authors": "Han Zhao; Xu Yang; Cheng Deng",
    "corresponding_authors": "",
    "abstract": "Deep graph clustering, efficiently dividing nodes into multiple disjoint clusters in an unsupervised manner, has become a crucial tool for analyzing ubiquitous graph data. Existing methods have acquired impressive clustering effects by optimizing the clustering network under the parametric condition – predefining the true number of clusters ( K tr ). However, K tr is inaccessible in pure unsupervised scenarios, in which existing methods are incapable of inferring the number of clusters ( K ), causing limited feasibility. This paper proposes the first Parameter-Agnostic Deep Graph Clustering method (PADGC), which consists of two core modules: K -guidence clustering and topological-hierarchical inference, to infer K efficiently and gain impressive clustering predictions. Specifically, K -guidence clustering is employed to optimize the cluster assignments and discriminative embeddings in a mutual promotion manner under the latest updated K , even though K may deviate from K tr . In turn, such optimized cluster assignments are utilized to explore more accurate K in the topological-hierarchical inference, which can split the dispersive clusters and merge the coupled ones. In this way, these two modules are complementarily optimized until generating the final convergent K and discriminative cluster assignments. Extensive experiments on several benchmarks, including graphs and images, can demonstrate the superiority of our method. The mean values of our inferred K , in 11 out of 12 datasets, deviates from K tr by less than 1. Our method can also achieve competitive clustering effects with existing parametric deep graph clustering.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389057529",
    "type": "article"
  },
  {
    "title": "Local Overlapping Spatial-aware Community Detection",
    "doi": "https://doi.org/10.1145/3634707",
    "publication_date": "2023-11-27",
    "publication_year": 2023,
    "authors": "Ni Li; Hefei Xu; Yiwen Zhang; Wenjian Luo; Yingying Huang; Victor S. Sheng",
    "corresponding_authors": "",
    "abstract": "Local spatial-aware community detection refers to detecting a spatial-aware community for a given node using local information. A spatial-aware community means that nodes in the community are tightly connected in structure, and their locations are close to each other. Existing studies focus on detecting the local non-overlapping spatial-aware community, i.e., detecting a spatial-aware community containing the given node. However, many geosocial networks often contain overlapping spatial-aware communities. Therefore, we propose a local overlapping spatial-aware community detection (LOSCD) problem, which aims to detect all spatial-aware communities that contain a given node with local information. To address LOSCD problem, we design an algorithm based on Spatial Modularity and Edge Similarity, called SMES. SMES contains two processes: spatial expansion and structure detection. The spatial expansion process involves using spatial modularity to identify nodes that are spatially close, while the structural detection process employs edge similarity to identify nodes that are structurally close. Experimental results demonstrate that SMES outperforms comparison algorithms in terms of both structural and spatial cohesiveness.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389057927",
    "type": "article"
  },
  {
    "title": "Learning Hierarchical Task Structures for Few-shot Graph Classification",
    "doi": "https://doi.org/10.1145/3635473",
    "publication_date": "2023-12-04",
    "publication_year": 2023,
    "authors": "Song Wang; Yushun Dong; Xiao Huang; Chen Chen; Jundong Li",
    "corresponding_authors": "",
    "abstract": "The problem of few-shot graph classification targets at assigning class labels for graph samples, where only limited labeled graphs are provided for each class. To solve the problem brought by label scarcity, recent studies have proposed to adopt the prevalent few-shot learning framework to achieve fast adaptations to graph classes with limited labeled graphs. In particular, these studies typically propose to accumulate meta-knowledge across a large number of meta-training tasks, and then generalize such meta-knowledge to meta-test tasks sampled from a disjoint class set. Nevertheless, existing studies generally ignore the crucial task correlations among meta-training tasks and treat them independently. In fact, such task correlations can help promote the model generalization to meta-test tasks and result in better classification performance. On the other hand, it remains challenging to capture and utilize task correlations due to the complex components and interactions in meta-training tasks. To deal with this, we propose a novel few-shot graph classification framework FAITH to capture task correlations via learning a hierarchical task structure at different granularities. We further propose a task-specific classifier to incorporate the learned task correlations into the few-shot graph classification process. Moreover, we derive FAITH+, a variant of FAITH that can improve the sampling process for the hierarchical task structure. The extensive experiments on four prevalent graph datasets further demonstrate the superiority of FAITH and FAITH+ over other state-of-the-art baselines.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389310762",
    "type": "article"
  },
  {
    "title": "DEWP: Deep Expansion Learning for Wind Power Forecasting",
    "doi": "https://doi.org/10.1145/3637552",
    "publication_date": "2023-12-14",
    "publication_year": 2023,
    "authors": "Wei Fan; Yanjie Fu; Shun Zheng; Jiang Bian; Yuanchun Zhou; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "Wind is one kind of high-efficient, environmentally-friendly, and cost-effective energy source. Wind power, as one of the largest renewable energy in the world, has been playing a more and more important role in supplying electricity. Though growing dramatically in recent years, the amount of generated wind power can be directly or latently affected by multiple uncertain factors, such as wind speed, wind direction, temperatures, and so on. More importantly, there exist very complicated dependencies of the generated power on the latent composition of these multiple time-evolving variables, which are always ignored by existing works and thus largely hinder the prediction performances. To this end, we propose DEWP , a novel D eep E xpansion learning for W ind P ower forecasting framework to carefully model the complicated dependencies with adequate expressiveness. DEWP starts with a stack-by-stack architecture, where each stack is composed of (i) a variable expansion block that makes use of convolutional layers to capture dependencies among multiple variables; (ii) a time expansion block that applies Fourier series and backcast/forecast mechanism to learn temporal dependencies in sequential patterns. These two tailored blocks expand raw inputs into different latent feature spaces which can model different levels of dependencies of time-evolving sequential data. Moreover, we propose an inference block corresponding for each stack, which applies multi-head self-attentions to acquire attentive features and maps expanded latent representations into generated wind power. In addition, to make DEWP more expressive in handling deep neural architectures, we adapt doubly residue learning to process stack-by-stack outputs. Accurate wind power forecasting (WPF) is then better achieved through fine-grained outputs by continuously removing stack residues and accumulating useful stack forecasts. Finally, we present extensive experiments in the real-world WPF application on two datasets from two different turbines, in order to demonstrate the effectiveness of our approach.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389742565",
    "type": "article"
  },
  {
    "title": "Image Hash Layer Triggered CNN Framework for Wafer Map Failure Pattern Retrieval and Classification",
    "doi": "https://doi.org/10.1145/3638053",
    "publication_date": "2023-12-19",
    "publication_year": 2023,
    "authors": "Minghao Piao; Yi Sheng; Jinda Yan; Cheng Jin",
    "corresponding_authors": "",
    "abstract": "Recently, deep learning methods are often used in wafer map failure pattern classification. CNN requires less feature engineering but still needs preprocessing, e.g., denoising and resizing. Denoising is used to improve the quality of the input data, and resizing is used to transform the input into an identical size when the input data sizes are various. However, denoising and resizing may distort the original data information. Nevertheless, CNN-based applications are focusing on studying different feature map architectures and the input data manipulation is less attractive. In this study, we proposed an image hash layer triggered CNN framework for wafer map failure pattern retrieval and classification. The motivation and novelty are to design a CNN layer that can play as a resizing, information retrieval-preservation method in one step. The experiments proved that the proposed hash layer can retrieve the failure pattern information while maintaining the classification performance even though the input data size is decreased significantly. In the meantime, it can prevent overfitting, false negatives, and false positives, and save computing costs to a certain extent.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389950160",
    "type": "article"
  },
  {
    "title": "Distributional Learning for Network Alignment with Global Constraints",
    "doi": "https://doi.org/10.1145/3638056",
    "publication_date": "2023-12-20",
    "publication_year": 2023,
    "authors": "Hui Xu; Liyao Xiang; Xiaoying Gan; Luoyi Fu; Xinbing Wang; Chenghu Zhou",
    "corresponding_authors": "",
    "abstract": "Network alignment, pairing corresponding nodes across the source and target networks, plays an important role in many data mining tasks. Extensive studies focus on learning node embeddings across different networks in a unified space. However, these methods have not taken the large structural discrepancy between aligned nodes into account and, thus, are largely confined by the deterministic representations of nodes. In this work, we propose a novel network alignment framework highlighted by distributional learning and globally optimal alignment. By modeling the uncertainty of each node by Gaussian distribution, our framework builds similarity matrices on the Wasserstein distance between distributions and applies Sinkhorn operation, which learns the globally optimal mapping in an end-to-end fashion. We show that each integrated part of the framework contributes to the overall performance. Under a variety of experimental settings, our alignment framework shows superior accuracy and efficiency to the state-of-the-art.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390003026",
    "type": "article"
  },
  {
    "title": "<scp>ArieL</scp> : Adversarial Graph Contrastive Learning",
    "doi": "https://doi.org/10.1145/3638054",
    "publication_date": "2023-12-22",
    "publication_year": 2023,
    "authors": "Shengyu Feng; Baoyu Jing; Yada Zhu; Hanghang Tong",
    "corresponding_authors": "",
    "abstract": "Contrastive learning is an effective unsupervised method in graph representation learning, and the key component of contrastive learning lies in the construction of positive and negative samples. Previous methods usually utilize the proximity of nodes in the graph as the principle. Recently, the data-augmentation-based contrastive learning method has advanced to show great power in the visual domain, and some works extended this method from images to graphs. However, unlike the data augmentation on images, the data augmentation on graphs is far less intuitive and much harder to provide high-quality contrastive samples, which leaves much space for improvement. In this work, by introducing an adversarial graph view for data augmentation, we propose a simple but effective method, Adversarial Graph Contrastive Learning (ArieL), to extract informative contrastive samples within reasonable constraints. We develop a new technique called information regularization for stable training and use subgraph sampling for scalability. We generalize our method from node-level contrastive learning to the graph level by treating each graph instance as a super-node. ArieL consistently outperforms the current graph contrastive learning methods for both node-level and graph-level classification tasks on real-world datasets. We further demonstrate that ArieL is more robust in the face of adversarial attacks.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390134900",
    "type": "article"
  },
  {
    "title": "Information-aware Multi-view Outlier Detection",
    "doi": "https://doi.org/10.1145/3638354",
    "publication_date": "2023-12-22",
    "publication_year": 2023,
    "authors": "Jinrong Lai; Tong Wang; Chuan Chen; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "With the development of multi-view learning, multi-view outlier detection has received increasing attention in recent years. However, the current research still faces two challenges: (1) The current research lacks theoretical analysis tools for multi-view outliers. (2) Most current multi-view outlier detection algorithms are based on shallow structural assumptions of the data, such as cluster assumptions and subspace assumptions, thus they are not suitable for more complex data distributions. In addressing these two issues, this article proposes three occurrence mechanisms of multi-view outlier, which serve as foundational theoretical analysis tools for multi-view outliers. Utilizing proposed mechanisms, we analyze the impact of multi-view outliers and the information structure of multi-view data and validate our findings through experiments. Finally, we propose a novel algorithm referred to as Information-Aware Multi-View Outlier Detection (IAMOD). In contrast to other methods, IAMOD focuses on the information structure of multi-view data without relying on shallow structural assumptions. By learning a compact representation of the sample that is semantically rich and non-redundant, IAMOD can accurately identify multi-view outliers by comparing the consistency of the representations’ neighbors and views. Extensive experimental results demonstrate that our approach outperforms several state-of-the-art multi-view outlier detection methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390135198",
    "type": "article"
  },
  {
    "title": "Learning Global and Multi-granularity Local Representation with MLP for Sequential Recommendation",
    "doi": "https://doi.org/10.1145/3638562",
    "publication_date": "2023-12-26",
    "publication_year": 2023,
    "authors": "Chao Long; Huanhuan Yuan; Junhua Fang; Xuefeng Xian; Guanfeng Liu; Victor S. Sheng; Pengpeng Zhao",
    "corresponding_authors": "",
    "abstract": "Sequential recommendation aims to predict the next item of interest to users based on their historical behavior data. Usually, users’ global and local preferences jointly affect the final recommendation result in different ways. Most existing works use transformers to globally model sequences, which makes them face the dilemma of quadratic computational complexity when dealing with long sequences. Moreover, the scope setting of the user’s local preference is usually static and single, and cannot cover richer multi-level local semantics. To this end, we proposed a parallel architecture for capturing global representation and M ulti-granularity L ocal dependencies with M LP for sequential Rec ommendation ( MLM4Rec ). For global representation, we utilize modified MLP-Mixer to capture global information of user sequences due to its simplicity and efficiency. For local representation, we incorporate convolution into MLP and propose a multi-granularity local awareness mechanism for capturing richer local semantic information. Moreover, we introduced a weight pooling method to adaptively fuse local-global representations instead of directly concatenation. Our model has the advantages of low complexity and high efficiency thanks to its simple MLP structure. Experimental results on three public datasets demonstrate the effectiveness of our proposed model. Our code is available here 1 .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390226594",
    "type": "article"
  },
  {
    "title": "A Novel Neural Ensemble Architecture for On-the-fly Classification of Evolving Text Streams",
    "doi": "https://doi.org/10.1145/3639054",
    "publication_date": "2023-12-28",
    "publication_year": 2023,
    "authors": "Pouya Ghahramanian; Sepehr Bakhshi; Hamed Bonab; Fazlı Can",
    "corresponding_authors": "",
    "abstract": "We study on-the-fly classification of evolving text streams in which the relation between the input data and target labels changes over time—i.e., “concept drift.” These variations decrease the model’s performance, as predictions become less accurate over time and they necessitate a more adaptable system. While most studies focus on concept drift detection and handling with ensemble approaches, the application of neural models in this area is relatively less studied. We introduce Adaptive Neural Ensemble Network ( AdaNEN ), a novel ensemble-based neural approach, capable of handling concept drift in data streams. With our novel architecture, we address some of the problems neural models face when exploited for online adaptive learning environments. Most current studies address concept drift detection and handling in numerical streams, and the evolving text stream classification remains relatively unexplored. We hypothesize that the lack of public and large-scale experimental data could be one reason. To this end, we propose a method based on an existing approach for generating evolving text streams by introducing various types of concept drifts to real-world text datasets. We provide an extensive evaluation of our proposed approach using 12 state-of-the-art baselines and 13 datasets. We first evaluate concept drift handling capability of AdaNEN and the baseline models on evolving numerical streams; this aims to demonstrate the concept drift handling capabilities of our method on a general spectrum and motivate its use in evolving text streams. The models are then evaluated in evolving text stream classification. Our experimental results show that AdaNEN consistently outperforms the existing approaches in terms of predictive performance with conservative efficiency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390317672",
    "type": "article"
  },
  {
    "title": "X-distribution: Retraceable Power-law Exponent of Complex Networks",
    "doi": "https://doi.org/10.1145/3639413",
    "publication_date": "2023-12-30",
    "publication_year": 2023,
    "authors": "Pradumn Kumar Pandey; Aikta Arya; Akrati Saxena",
    "corresponding_authors": "",
    "abstract": "Network modeling has been explored extensively by means of theoretical analysis as well as numerical simulations for Network Reconstruction (NR). The network reconstruction problem requires the estimation of the power-law exponent (γ) of a given input network. Thus, the effectiveness of the NR solution depends on the accuracy of the calculation of γ. In this article, we re-examine the degree distribution-based estimation of γ, which is not very accurate due to approximations. We propose X -distribution, which is more accurate than degree distribution. Various state-of-the-art network models, including CPM, NRM, RefOrCite2, BA, CDPAM, and DMS, are considered for simulation purposes, and simulated results support the proposed claim. Further, we apply X -distribution over several real-world networks to calculate their power-law exponents, which differ from those calculated using respective degree distributions. It is observed that X -distributions exhibit more linearity (straight line) on the log-log scale than degree distributions. Thus, X -distribution is more suitable for the evaluation of power-law exponent using linear fitting (on the log-log scale). The MATLAB implementation of power-law exponent (γ) calculation using X -distribution for different network models and the real-world datasets used in our experiments are available at https://github.com/Aikta-Arya/X-distribution-Retraceable-Power-Law-Exponent-of-Complex-Networks.git .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390450624",
    "type": "article"
  },
  {
    "title": "Adaptive discriminant analysis for microarray-based classification",
    "doi": "https://doi.org/10.1145/1342320.1342325",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Yijuan Lu; Qi Tian; Jennifer Neary; Feng Liu; Yufeng Wang",
    "corresponding_authors": "",
    "abstract": "Microarray technology has generated enormous amounts of high-dimensional gene expression data, providing a unique platform for exploring gene regulatory networks. However, the curse of dimensionality plagues effort to analyze these high throughput data. Linear Discriminant Analysis (LDA) and Biased Discriminant Analysis (BDA) are two popular techniques for dimension reduction, which pay attention to different roles of the positive and negative samples in finding discriminating subspace. However, the drawbacks of these two methods are obvious: LDA has limited efficiency in classifying sample data from subclasses with different distributions, and BDA does not account for the underlying distribution of negative samples. In this paper, we propose a novel dimension reduction technique for microarray analysis: Adaptive Discriminant Analysis (ADA), which effectively exploits favorable attributes of both BDA and LDA and avoids their unfavorable ones. ADA can find a good discriminative subspace with adaptation to different sample distributions. It not only alleviates the problem of high dimensionality, but also enhances the classification performance in the subspace with naïve Bayes classifier. To learn the best model fitting the real scenario, boosted Adaptive Discriminant Analysis is further proposed. Extensive experiments on the yeast cell cycle regulation data set, and the expression data of the red blood cell cycle in malaria parasite Plasmodium falciparum demonstrate the superior performance of ADA and boosted ADA. We also present some putative genes of specific functional classes predicted by boosted ADA. Their potential functionality is confirmed by independent predictions based on Gene Ontology, demonstrating that ADA and boosted ADA are effective dimension reduction methods for microarray-based classification.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1970320222",
    "type": "article"
  },
  {
    "title": "Prioritized Relationship Analysis in Heterogeneous Information Networks",
    "doi": "https://doi.org/10.1145/3154401",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Jiongqian Liang; Deepak Ajwani; Patrick K. Nicholson; Alessandra Sala; Srinivasan Parthasarathy",
    "corresponding_authors": "",
    "abstract": "An increasing number of applications are modeled and analyzed in network form, where nodes represent entities of interest and edges represent interactions or relationships between entities. Commonly, such relationship analysis tools assume homogeneity in both node type and edge type. Recent research has sought to redress the assumption of homogeneity and focused on mining heterogeneous information networks (HINs) where both nodes and edges can be of different types. Building on such efforts, in this work, we articulate a novel approach for mining relationships across entities in such networks while accounting for user preference over relationship type and interestingness metric. We formalize the problem as a top- k lightest paths problem, contextualized in a real-world communication network, and seek to find the k most interesting path instances matching the preferred relationship type. Our solution, PROphetic HEuristic Algorithm for Path Searching (PRO-HEAPS), leverages a combination of novel graph preprocessing techniques, well-designed heuristics and the venerable A* search algorithm. We run our algorithm on real-world large-scale graphs and show that our algorithm significantly outperforms a wide variety of baseline approaches with speedups as large as 100X. To widen the range of applications, we also extend PRO-HEAPS to (i) support relationship analysis between two groups of entities and (ii) allow pattern path in the query to contain logical statements with operators AND, OR, NOT, and wild-card “.”. We run experiments using this generalized version of PRO-HEAPS and demonstrate that the advantage of PRO-HEAPS becomes even more pronounced for these general cases. Furthermore, we conduct a comprehensive analysis to study how the performance of PRO-HEAPS varies with respect to various attributes of the input HIN. We finally conduct a case study to demonstrate valuable applications of our algorithm.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2785197158",
    "type": "article"
  },
  {
    "title": "Event Analytics via Discriminant Tensor Factorization",
    "doi": "https://doi.org/10.1145/3184455",
    "publication_date": "2018-10-10",
    "publication_year": 2018,
    "authors": "Xidao Wen; Yu‐Ru Lin; Konstantinos Pelechrinis",
    "corresponding_authors": "",
    "abstract": "Analyzing the impact of disastrous events has been central to understanding and responding to crises. Traditionally, the assessment of disaster impact has primarily relied on the manual collection and analysis of surveys and questionnaires as well as the review of authority reports. This can be costly and time-consuming, whereas a timely assessment of an event’s impact is critical for crisis management and humanitarian operations. In this work, we formulate the impact discovery as the problem to identify the shared and discriminative subspace via tensor factorization due to the multi-dimensional nature of mobility data. Existing work in mining the shared and discriminative subspaces typically requires the predefined number of either type of them. In the context of event impact discovery, this could be impractical, especially for those unprecedented events. To overcome this, we propose a new framework, called “PairFac,” that jointly factorizes the multi-dimensional data to discover the latent mobility pattern along with its associated discriminative weight. This framework does not require splitting the shared and discriminative subspaces in advance and at the same time automatically captures the persistent and changing patterns from multi-dimensional behavioral data. Our work has important applications in crisis management and urban planning, which provides a timely assessment of impacts of major events in the urban environment.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2897753390",
    "type": "article"
  },
  {
    "title": "Fast Approximate Score Computation on Large-Scale Distributed Data for Learning Multinomial Bayesian Networks",
    "doi": "https://doi.org/10.1145/3301304",
    "publication_date": "2019-03-13",
    "publication_year": 2019,
    "authors": "Anas Katib; Praveen Rao; Kobus Barnard; Charles Kamhoua",
    "corresponding_authors": "",
    "abstract": "In this article, we focus on the problem of learning a Bayesian network over distributed data stored in a commodity cluster. Specifically, we address the challenge of computing the scoring function over distributed data in an efficient and scalable manner, which is a fundamental task during learning. While exact score computation can be done using the MapReduce-style computation, our goal is to compute approximate scores much faster with probabilistic error bounds and in a scalable manner. We propose a novel approach, which is designed to achieve the following: (a) decentralized score computation using the principle of gossiping; (b) lower resource consumption via a probabilistic approach for maintaining scores using the properties of a Markov chain; and (c) effective distribution of tasks during score computation (on large datasets) by synergistically combining well-known hashing techniques. We conduct theoretical analysis of our approach in terms of convergence speed of the statistics required for score computation, and memory and network bandwidth consumption. We also discuss how our approach is capable of efficiently recomputing scores when new data are available. We conducted a comprehensive evaluation of our approach and compared with the MapReduce-style computation using datasets of different characteristics on a 16-node cluster. When the MapReduce-style computation provided exact statistics for score computation, it was nearly 10 times slower than our approach. Although it ran faster on randomly sampled datasets than on the entire datasets, it performed worse than our approach in terms of accuracy. Our approach achieved high accuracy (below 6% average relative error) in estimating the statistics for approximate score computation on all the tested datasets. In conclusion, it provides a feasible tradeoff between computation time and accuracy for fast approximate score computation on large-scale distributed data.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2922366414",
    "type": "article"
  },
  {
    "title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation",
    "doi": "https://doi.org/10.1145/3365672",
    "publication_date": "2019-12-13",
    "publication_year": 2019,
    "authors": "Md Kamrul Hasan; Christopher Pal",
    "corresponding_authors": "",
    "abstract": "We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning metaparameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach is as follows: (1) more robust to outliers compared to the logistic and hinge losses; (2) outperforms comparable logistic and max margin models on larger scale benchmark problems; (3) when combined with Gaussian–Laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and (4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3004679435",
    "type": "article"
  },
  {
    "title": "Multiple Set Matching with Bloom Matrix and Bloom Vector",
    "doi": "https://doi.org/10.1145/3372409",
    "publication_date": "2020-02-09",
    "publication_year": 2020,
    "authors": "Francesco Concas; Pengfei Xu; Mohammad A. Hoque; Jiaheng Lu; Sasu Tarkoma",
    "corresponding_authors": "",
    "abstract": "Bloom Filter is a space-efficient probabilistic data structure for checking the membership of elements in a set. Given multiple sets, a standard Bloom Filter is not sufficient when looking for the items to which an element or a set of input elements belong. An example case is searching for documents with keywords in a large text corpus, which is essentially a multiple set matching problem where the input is single or multiple keywords, and the result is a set of possible candidate documents. This article solves the multiple set matching problem by proposing two efficient Bloom Multifilters called Bloom Matrix and Bloom Vector, which generalize the standard Bloom Filter. Both structures are space-efficient and answer queries with a set of identifiers for multiple set matching problems. The space efficiency can be optimized according to the distribution of labels among multiple sets: Uniform and Zipf. Bloom Vector efficiently exploits the Zipf distribution of data for further space reduction. Indeed, both structures are much more space-efficient compared with the state-of-the-art, Bloofi. The results also highlight that a L ookup operation on Bloom Matrix is significantly faster than on Bloom Vector and Bloofi.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3006091913",
    "type": "article"
  },
  {
    "title": "Enhanced Data Mining Technique to Measure Satisfaction Degree of Social Media Users of Xeljanz Drug",
    "doi": "https://doi.org/10.1145/3389433",
    "publication_date": "2020-05-13",
    "publication_year": 2020,
    "authors": "Mohammad Abdelaziz; Hazem M. El‐Bakry; Ahmed Abou Elfetouh; Amira Elzeiny",
    "corresponding_authors": "",
    "abstract": "In the recent times, social media has become important in the field of health care as a major resource of valuable health information. Social media can provide massive amounts of data in real-time through user interaction, and this data can be analysed to reflect the harms and benefits of treatment by using the personal health experiences of users to improve health outcomes. In this study, we propose an enhanced data mining framework for analysing user opinions on Twitter and on a health-care forum. The proposed framework measures the degree of satisfaction of consumers regarding the drug Xeljanz, which is used to treat rheumatoid arthritis. The proposed framework is based on seven steps distributed in two phases. The first phase involves aggregating data related to the drug Xeljanz. This data is pre-processed to produce a list of words with a term frequency-inverse document frequency score. The word list is then classified into the following three categories: positive, negative and neutral. The second phase involves modelling social media posts using network analysis, identifying sub-graphs, calculating average opinions and detecting influential users. The results showed 77.3% user satisfaction with Xeljanz. Positive opinions were especially pronounced among users who switched to Xeljanz based on advice from a physician. Negative opinions of Xeljanz typically pertained to the high cost of the drug.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3027814548",
    "type": "article"
  },
  {
    "title": "Time-Warped Sparse Non-negative Factorization for Functional Data Analysis",
    "doi": "https://doi.org/10.1145/3408313",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Chen Zhang; Steven C. H. Hoi; Fugee Tsung",
    "corresponding_authors": "",
    "abstract": "This article proposes a novel time-warped sparse non-negative factorization method for functional data analysis. The proposed method on the one hand guarantees the extracted basis functions and their coefficients to be positive and interpretable, and on the other hand is able to handle weakly correlated functions with different features. Furthermore, the method incorporates time warping into factorization and hence allows the extracted basis functions of different samples to have temporal deformations. An efficient framework of estimation algorithms is proposed based on a greedy variable selection approach. Numerical studies together with case studies on real-world data demonstrate the efficacy and applicability of the proposed methodology.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3091418863",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1217299",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4252653413",
    "type": "paratext"
  },
  {
    "title": "Active Sampling for Entity Matching with Guarantees",
    "doi": "https://doi.org/10.1145/2513092.2500490",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Kedar Bellare; Suresh Iyengar; Aditya Parameswaran; Vibhor Rastogi",
    "corresponding_authors": "",
    "abstract": "In entity matching, a fundamental issue while training a classifier to label pairs of entities as either duplicates or nonduplicates is the one of selecting informative training examples. Although active learning presents an attractive solution to this problem, previous approaches minimize the misclassification rate (0--1 loss) of the classifier, which is an unsuitable metric for entity matching due to class imbalance (i.e., many more nonduplicate pairs than duplicate pairs). To address this, a recent paper [Arasu et al. 2010] proposes to maximize recall of the classifier under the constraint that its precision should be greater than a specified threshold. However, the proposed technique requires the labels of all n input pairs in the worst case.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4205848058",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2133360",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Clustering data described by categorical attributes is a challenging task in data mining applications. Unlike numerical attributes, it is difficult to define a distance between pairs of values of a categorical attribute, since the values are not ...",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4237553951",
    "type": "paratext"
  },
  {
    "title": "Context-Based Evaluation of Dimensionality Reduction Algorithms—Experiments and Statistical Significance Analysis",
    "doi": "https://doi.org/10.1145/3428077",
    "publication_date": "2021-01-04",
    "publication_year": 2021,
    "authors": "Aindrila Ghosh; Mona Nashaat; James Miller; Shaikh Quader",
    "corresponding_authors": "",
    "abstract": "Dimensionality reduction is a commonly used technique in data analytics. Reducing the dimensionality of datasets helps not only with managing their analytical complexity but also with removing redundancy. Over the years, several such algorithms have been proposed with their aims ranging from generating simple linear projections to complex non-linear transformations of the input data. Subsequently, researchers have defined several quality metrics in order to evaluate the performances of different algorithms. Hence, given a plethora of dimensionality reduction algorithms and metrics for their quality analysis, there is a long-existing need for guidelines on how to select the most appropriate algorithm in a given scenario. In order to bridge this gap, in this article, we have compiled 12 state-of-the-art quality metrics and categorized them into 5 identified analytical contexts. Furthermore, we assessed 15 most popular dimensionality reduction algorithms on the chosen quality metrics using a large-scale and systematic experimental study. Later, using a set of robust non-parametric statistical tests, we assessed the generalizability of our evaluation on 40 real-world datasets. Finally, based on our results, we present practitioners’ guidelines for the selection of an appropriate dimensionally reduction algorithm in the present analytical contexts.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3119526997",
    "type": "article"
  },
  {
    "title": "Elastic Embedding through Graph Convolution-based Regression for Semi-supervised Classification",
    "doi": "https://doi.org/10.1145/3441456",
    "publication_date": "2021-03-26",
    "publication_year": 2021,
    "authors": "Fadi Dornaika",
    "corresponding_authors": "Fadi Dornaika",
    "abstract": "This article introduces a scheme for semi-supervised learning by estimating a flexible non-linear data representation that exploits Spectral Graph Convolutions structure. Structured data are exploited in order to determine non-linear and linear models. The introduced scheme takes advantage of data-driven graphs at two levels. First, it incorporates manifold smoothness that is naturally encoded by the graph itself. Second, the regression model is built on the convolved data samples that are derived from the data and their associated graph. The proposed semi-supervised embedding can tackle the problem of over-fitting on neighborhood structures for image data. The proposed Graph Convolution-based Semi-supervised Embedding paves the way to new theoretical and application perspectives related to the non-linear embedding. Indeed, building flexible models that adopt convolved data samples can enhance both the data representation and the final performance of the learning system. Several experiments are conducted on six image datasets for comparing the introduced scheme with some state-of-the-art semi-supervised approaches. This empirical evaluation shows the effectiveness of the proposed embedding scheme.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3144740033",
    "type": "article"
  },
  {
    "title": "A Layout-Based Classification Method for Visualizing Time-Varying Graphs",
    "doi": "https://doi.org/10.1145/3441301",
    "publication_date": "2021-03-26",
    "publication_year": 2021,
    "authors": "Yunzhe Wang; George Baciu; Chenhui Li",
    "corresponding_authors": "",
    "abstract": "Connectivity analysis between the components of large evolving systems can reveal significant patterns of interaction. The systems can be simulated by topological graph structures. However, such analysis becomes challenging on large and complex graphs. Tasks such as comparing, searching, and summarizing structures, are difficult due to the enormous number of calculations required. For time-varying graphs, the temporal dimension even intensifies the difficulty. In this article, we propose to reduce the complexity of analysis by focusing on subgraphs that are induced by closely related entities. To summarize the diverse structures of subgraphs, we build a supervised layout-based classification model. The main premise is that the graph structures can induce a unique appearance of the layout. In contrast to traditional graph theory-based and contemporary neural network-based methods of graph classification, our approach generates low costs and there is no need to learn informative graph representations. Combined with temporally stable visualizations, we can also facilitate the understanding of sub-structures and the tracking of graph evolution. The method is evaluated on two real-world datasets. The results show that our system is highly effective in carrying out visual-based analytics of large graphs.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3146089750",
    "type": "article"
  },
  {
    "title": "Online Sampling of Temporal Networks",
    "doi": "https://doi.org/10.1145/3442202",
    "publication_date": "2021-04-18",
    "publication_year": 2021,
    "authors": "Nesreen K. Ahmed; Nick Duffield; Ryan A. Rossi",
    "corresponding_authors": "",
    "abstract": "Temporal networks representing a stream of timestamped edges are seemingly ubiquitous in the real world. However, the massive size and continuous nature of these networks make them fundamentally challenging to analyze and leverage for descriptive and predictive modeling tasks. In this work, we propose a general framework for temporal network sampling with unbiased estimation. We develop online, single-pass sampling algorithms, and unbiased estimators for temporal network sampling. The proposed algorithms enable fast, accurate, and memory-efficient statistical estimation of temporal network patterns and properties. In addition, we propose a temporally decaying sampling algorithm with unbiased estimators for studying networks that evolve in continuous time, where the strength of links is a function of time, and the motif patterns are temporally weighted. In contrast to the prior notion of a △ t -temporal motif, the proposed formulation and algorithms for counting temporally weighted motifs are useful for forecasting tasks in networks such as predicting future links, or a future time-series variable of nodes and links. Finally, extensive experiments on a variety of temporal networks from different domains demonstrate the effectiveness of the proposed algorithms. A detailed ablation study is provided to understand the impact of the various components of the proposed framework.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3153875847",
    "type": "article"
  },
  {
    "title": "Understanding Persuasion Cascades in Online Product Rating Systems: Modeling, Analysis, and Inference",
    "doi": "https://doi.org/10.1145/3440887",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Hong Xie; Mingze Zhong; Yongkun Li; John C. S. Lui",
    "corresponding_authors": "",
    "abstract": "Online product rating systems have become an indispensable component for numerous web services such as Amazon, eBay, Google Play Store, and TripAdvisor. One functionality of such systems is to uncover the product quality via product ratings (or reviews) contributed by consumers. However, a well-known psychological phenomenon called “ message-based persuasion ” lead to “ biased ” product ratings in a cascading manner (we call this the persuasion cascade ). This article investigates: (1) How does the persuasion cascade influence the product quality estimation accuracy? (2) Given a real-world product rating dataset, how to infer the persuasion cascade and analyze it to draw practical insights? We first develop a mathematical model to capture key factors of a persuasion cascade. We formulate a high-order Markov chain to characterize the opinion dynamics of a persuasion cascade and prove the convergence of opinions. We further bound the product quality estimation error for a class of rating aggregation rules including the averaging scoring rule, via the matrix perturbation theory and the Chernoff bound. We also design a maximum likelihood algorithm to infer parameters of the persuasion cascade. We conduct experiments on both synthetic data and real-world data from Amazon and TripAdvisor. Experiment results show that our inference algorithm has a high accuracy. Furthermore, persuasion cascades notably exist, but the average scoring rule has a small product quality estimation error under practical scenarios.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3153943746",
    "type": "article"
  },
  {
    "title": "Network Embedding on Hierarchical Community Structure Network",
    "doi": "https://doi.org/10.1145/3434747",
    "publication_date": "2021-05-08",
    "publication_year": 2021,
    "authors": "Guojie Song; Yun Wang; Lun Du; Yi Li; Junshan Wang",
    "corresponding_authors": "",
    "abstract": "Network embedding is a method of learning a low-dimensional vector representation of network vertices under the condition of preserving different types of network properties. Previous studies mainly focus on preserving structural information of vertices at a particular scale, like neighbor information or community information, but cannot preserve the hierarchical community structure, which would enable the network to be easily analyzed at various scales. Inspired by the hierarchical structure of galaxies, we propose the Galaxy Network Embedding (GNE) model, which formulates an optimization problem with spherical constraints to describe the hierarchical community structure preserving network embedding. More specifically, we present an approach of embedding communities into a low-dimensional spherical surface, the center of which represents the parent community they belong to. Our experiments reveal that the representations from GNE preserve the hierarchical community structure and show advantages in several applications such as vertex multi-class classification, network visualization, and link prediction. The source code of GNE is available online.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3161406713",
    "type": "article"
  },
  {
    "title": "Hierarchical Concept-Driven Language Model",
    "doi": "https://doi.org/10.1145/3451167",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Yashen Wang; Huanhuan Zhang; Zhirun Liu; Qiang Zhou",
    "corresponding_authors": "",
    "abstract": "For guiding natural language generation, many semantic-driven methods have been proposed. While clearly improving the performance of the end-to-end training task, these existing semantic-driven methods still have clear limitations: for example, (i) they only utilize shallow semantic signals (e.g., from topic models) with only a single stochastic hidden layer in their data generation process, which suffer easily from noise (especially adapted for short-text etc.) and lack of interpretation; (ii) they ignore the sentence order and document context, as they treat each document as a bag of sentences, and fail to capture the long-distance dependencies and global semantic meaning of a document. To overcome these problems, we propose a novel semantic-driven language modeling framework, which is a method to learn a Hierarchical Language Model and a Recurrent Conceptualization-enhanced Gamma Belief Network, simultaneously. For scalable inference, we develop the auto-encoding Variational Recurrent Inference, allowing efficient end-to-end training and simultaneously capturing global semantics from a text corpus. Especially, this article introduces concept information derived from high-quality lexical knowledge graph Probase, which leverages strong interpretability and anti-nose capability for the proposed model. Moreover, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence concept dependence. Experiments conducted on several NLP tasks validate the superiority of the proposed approach, which could effectively infer meaningful hierarchical concept structure of document and hierarchical multi-scale structures of sequences, even compared with latest state-of-the-art Transformer-based models.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3162067818",
    "type": "article"
  },
  {
    "title": "Distributed Latent Dirichlet Allocation on Streams",
    "doi": "https://doi.org/10.1145/3451528",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Yunyan Guo; Jianzhong Li",
    "corresponding_authors": "",
    "abstract": "Latent Dirichlet Allocation (LDA) has been widely used for topic modeling, with applications spanning various areas such as natural language processing and information retrieval. While LDA on small and static datasets has been extensively studied, several real-world challenges are posed in practical scenarios where datasets are often huge and are gathered in a streaming fashion. As the state-of-the-art LDA algorithm on streams, Streaming Variational Bayes (SVB) introduced Bayesian updating to provide a streaming procedure. However, the utility of SVB is limited in applications since it ignored three challenges of processing real-world streams: topic evolution , data turbulence , and real-time inference . In this article, we propose a novel distributed LDA algorithm—referred to as StreamFed-LDA— to deal with challenges on streams. For topic modeling of streaming data, the ability to capture evolving topics is essential for practical online inference. To achieve this goal, StreamFed-LDA is based on a specialized framework that supports lifelong (continual) learning of evolving topics. On the other hand, data turbulence is commonly present in streams due to real-life events. In that case, the design of StreamFed-LDA allows the model to learn new characteristics from the most recent data while maintaining the historical information. On massive streaming data, it is difficult and crucial to provide real-time inference results. To increase the throughput and reduce the latency, StreamFed-LDA introduces additional techniques that substantially reduce both computation and communication costs in distributed systems. Experiments on four real-world datasets show that the proposed framework achieves significantly better performance of online inference compared with the baselines. At the same time, StreamFed-LDA also reduces the latency by orders of magnitudes in real-world datasets.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3184138430",
    "type": "article"
  },
  {
    "title": "MSIPA: Multi-Scale Interval Pattern-Aware Network for ICU Transfer Prediction",
    "doi": "https://doi.org/10.1145/3458284",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Wu Lee; Yuliang Shi; Hongfeng Sun; Lin Cheng; Kun Zhang; Xinjun Wang; Zhiyong Chen",
    "corresponding_authors": "",
    "abstract": "Accurate prediction of patients’ ICU transfer events is of great significance for improving ICU treatment efficiency. ICU transition prediction task based on Electronic Health Records (EHR) is a temporal mining task like many other health informatics mining tasks. In the EHR-based temporal mining task, existing approaches are usually unable to mine and exploit patterns used to improve model performance. This article proposes a network based on Interval Pattern-Aware, Multi-Scale Interval Pattern-Aware (MSIPA) network. MSIPA mines different interval patterns in temporal EHR data according to the short, medium, and long intervals. MSIPA utilizes the Scaled Dot-Product Attention mechanism to query the contexts corresponding to the three scale patterns. Furthermore, Transformer will use all three types of contextual information simultaneously for ICU transfer prediction. Extensive experiments on real-world data demonstrate that an MSIPA network outperforms state-of-the-art methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3185785441",
    "type": "article"
  },
  {
    "title": "High-Value Token-Blocking: Efficient Blocking Method for Record Linkage",
    "doi": "https://doi.org/10.1145/3450527",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Kevin O’Hare; Anna Jurek-Loughrey; Cassio P. de Campos",
    "corresponding_authors": "",
    "abstract": "Data integration is an important component of Big Data analytics. One of the key challenges in data integration is record linkage, that is, matching records that represent the same real-world entity. Because of computational costs, methods referred to as blocking are employed as a part of the record linkage pipeline in order to reduce the number of comparisons among records. In the past decade, a range of blocking techniques have been proposed. Real-world applications require approaches that can handle heterogeneous data sources and do not rely on labelled data. We propose high-value token-blocking (HVTB), a simple and efficient approach for blocking that is unsupervised and schema-agnostic, based on a crafted use of Term Frequency-Inverse Document Frequency. We compare HVTB with multiple methods and over a range of datasets, including a novel unstructured dataset composed of titles and abstracts of scientific papers. We thoroughly discuss results in terms of accuracy, use of computational resources, and different characteristics of datasets and records. The simplicity of HVTB yields fast computations and does not harm its accuracy when compared with existing approaches. It is shown to be significantly superior to other methods, suggesting that simpler methods for blocking should be considered before resorting to more sophisticated methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3186756857",
    "type": "article"
  },
  {
    "title": "New Multi-View Classification Method with Uncertain Data",
    "doi": "https://doi.org/10.1145/3458282",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Bo Liu; Haowen Zhong; Yanshan Xiao",
    "corresponding_authors": "",
    "abstract": "Multi-view classification aims at designing a multi-view learning strategy to train a classifier from multi-view data, which are easily collected in practice. Most of the existing works focus on multi-view classification by assuming the multi-view data are collected with precise information. However, we always collect the uncertain multi-view data due to the collection process is corrupted with noise in real-life application. In this case, this article proposes a novel approach, called uncertain multi-view learning with support vector machine (UMV-SVM) to cope with the problem of multi-view learning with uncertain data. The method first enforces the agreement among all the views to seek complementary information of multi-view data and takes the uncertainty of the multi-view data into consideration by modeling reachability area of the noise. Then it proposes an iterative framework to solve the proposed UMV-SVM model such that we can obtain the multi-view classifier for prediction. Extensive experiments on real-life datasets have shown that the proposed UMV-SVM can achieve a better performance for uncertain multi-view classification in comparison to the state-of-the-art multi-view classification methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3186798432",
    "type": "article"
  },
  {
    "title": "Hybrid Variational Autoencoder for Recommender Systems",
    "doi": "https://doi.org/10.1145/3470659",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Hangbin Zhang; Raymond K. Wong; Victor W. Chu",
    "corresponding_authors": "",
    "abstract": "E-commerce platforms heavily rely on automatic personalized recommender systems, e.g., collaborative filtering models, to improve customer experience. Some hybrid models have been proposed recently to address the deficiency of existing models. However, their performances drop significantly when the dataset is sparse. Most of the recent works failed to fully address this shortcoming. At most, some of them only tried to alleviate the problem by considering either user side or item side content information. In this article, we propose a novel recommender model called Hybrid Variational Autoencoder (HVAE) to improve the performance on sparse datasets. Different from the existing approaches, we encode both user and item information into a latent space for semantic relevance measurement. In parallel, we utilize collaborative filtering to find the implicit factors of users and items, and combine their outputs to deliver a hybrid solution. In addition, we compare the performance of Gaussian distribution and multinomial distribution in learning the representations of the textual data. Our experiment results show that HVAE is able to significantly outperform state-of-the-art models with robust performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3200876394",
    "type": "article"
  },
  {
    "title": "Balance-Subsampled Stable Prediction Across Unknown Test Data",
    "doi": "https://doi.org/10.1145/3477052",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Kun Kuang; Hengtao Zhang; Runze Wu; Fei Wu; Yueting Zhuang; Aijun Zhang",
    "corresponding_authors": "",
    "abstract": "In data mining and machine learning, it is commonly assumed that training and test data share the same population distribution. However, this assumption is often violated in practice because of the sample selection bias, which might induce the distribution shift from training data to test data. Such a model-agnostic distribution shift usually leads to prediction instability across unknown test data. This article proposes a novel balance-subsampled stable prediction (BSSP) algorithm based on the theory of fractional factorial design. It isolates the clear effect of each predictor from the confounding variables. A design-theoretic analysis shows that the proposed method can reduce the confounding effects among predictors induced by the distribution shift, improving both the accuracy of parameter estimation and the stability of prediction across unknown test data. Numerical experiments on synthetic and real-world datasets demonstrate that our BSSP algorithm can significantly outperform the baseline methods for stable prediction across unknown test data.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3210037261",
    "type": "article"
  },
  {
    "title": "Graph Community Infomax",
    "doi": "https://doi.org/10.1145/3480244",
    "publication_date": "2021-11-15",
    "publication_year": 2021,
    "authors": "Heli Sun; Li Yang; Bing Lv; Wujie Yan; Liang He; Shaojie Qiao; Jianbin Huang",
    "corresponding_authors": "",
    "abstract": "Graph representation learning aims at learning low-dimension representations for nodes in graphs, and has been proven very useful in several downstream tasks. In this article, we propose a new model, Graph Community Infomax (GCI), that can adversarial learn representations for nodes in attributed networks. Different from other adversarial network embedding models, which would assume that the data follow some prior distributions and generate fake examples, GCI utilizes the community information of networks, using nodes as positive(or real) examples and negative(or fake) examples at the same time. An autoencoder is applied to learn the embedding vectors for nodes and reconstruct the adjacency matrix, and a discriminator is used to maximize the mutual information between nodes and communities. Experiments on several real-world and synthetic networks have shown that GCI outperforms various network embedding methods on community detection tasks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3214213874",
    "type": "article"
  },
  {
    "title": "A Trajectory Evaluator by Sub-tracks for Detecting VOT-based Anomalous Trajectory",
    "doi": "https://doi.org/10.1145/3490032",
    "publication_date": "2022-01-08",
    "publication_year": 2022,
    "authors": "Fei Gao; Jia‐Da Li; Yisu Ge; Jianwen Shao; Shufang Lu; Libo Weng",
    "corresponding_authors": "",
    "abstract": "With the popularization of visual object tracking (VOT), more and more trajectory data are obtained and have begun to gain widespread attention in the fields of mobile robots, intelligent video surveillance, and the like. How to clean the anomalous trajectories hidden in the massive data has become one of the research hotspots. Anomalous trajectories should be detected and cleaned before the trajectory data can be effectively used. In this article, a Trajectory Evaluator by Sub-tracks (TES) for detecting VOT-based anomalous trajectory is proposed. Feature of Anomalousness is defined and described as the Eigenvector of classifier to filter Track Lets anomalous trajectory and IDentity Switch anomalous trajectory, which includes Feature of Anomalous Pose and Feature of Anomalous Sub-tracks (FAS). In the comparative experiments, TES achieves better results on different scenes than state-of-the-art methods. Moreover, FAS makes better performance than point flow, least square method fitting and Chebyshev Polynomial Fitting. It is verified that TES is more accurate and effective and is conducive to the sub-tracks trajectory data analysis.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4205320429",
    "type": "article"
  },
  {
    "title": "The Distance Function Optimization for the Near Neighbors-Based Classifiers",
    "doi": "https://doi.org/10.1145/3434769",
    "publication_date": "2022-02-24",
    "publication_year": 2022,
    "authors": "Marcel Jiřina; Said Krayem",
    "corresponding_authors": "",
    "abstract": "Based on the analysis of conditions for a good distance function we found four rules that should be fulfilled. Then, we introduce two new distance functions, a metric and a pseudometric one. We have tested how they fit for distance-based classifiers, especially for the IINC classifier. We rank distance functions according to several criteria and tests. Rankings depend not only on criteria or nature of the statistical test, but also whether it takes into account different difficulties of tasks or whether it considers all tasks as equally difficult. We have found that the new distance functions introduced belong among the four or five best out of 23 distance functions. We have tested them on 24 different tasks, using the mean, the median, the Friedman aligned test, and the Quade test. Our results show that a suitable distance function can improve behavior of distance-based classification rules.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4214749637",
    "type": "article"
  },
  {
    "title": "Stochastic Variational Optimization of a Hierarchical Dirichlet Process Latent Beta-Liouville Topic Model",
    "doi": "https://doi.org/10.1145/3502727",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Koffi Eddy Ihou; Manar Amayri; Nizar Bouguila",
    "corresponding_authors": "",
    "abstract": "In topic models, collections are organized as documents where they arise as mixtures over latent clusters called topics. A topic is a distribution over the vocabulary. In large-scale applications, parametric or finite topic mixture models such as LDA (latent Dirichlet allocation) and its variants are very restrictive in performance due to their reduced hypothesis space. In this article, we address the problem related to model selection and sharing ability of topics across multiple documents in standard parametric topic models. We propose as an alternative a BNP (Bayesian nonparametric) topic model where the HDP (hierarchical Dirichlet process) prior models documents topic mixtures through their multinomials on infinite simplex. We, therefore, propose asymmetric BL (Beta-Liouville) as a diffuse base measure at the corpus level DP (Dirichlet process) over a measurable space. This step illustrates the highly heterogeneous structure in the set of all topics that describes the corpus probability measure. For consistency in posterior inference and predictive distributions, we efficiently characterize random probability measures whose limits are the global and local DPs to approximate the HDP from the stick-breaking formulation with the GEM (Griffiths-Engen-McCloskey) random variables. Due to the diffuse measure with the BL prior as conjugate to the count data distribution, we obtain an improved version of the standard HDP that is usually based on symmetric Dirichlet (Dir). In addition, to improve coordinate ascent framework while taking advantage of its deterministic nature, our model implements an online optimization method based on stochastic, at document level, variational inference to accommodate fast topic learning when processing large collections of text documents with natural gradient. The high value in the predictive likelihood per document obtained when compared to the performance of its competitors is also consistent with the robustness of our fully asymmetric BL-based HDP. While insuring the predictive accuracy of the model using the probability of the held-out documents, we also added a combination of metrics such as the topic coherence and topic diversity to improve the quality and interpretability of the topics discovered. We also compared the performance of our model using these metrics against the standard symmetric LDA. We show that online HDP-LBLA (Latent BL Allocation)’s performance is the asymptote for parametric topic models. The accuracy in the results (improved predictive distributions of the held out) is a product of the model’s ability to efficiently characterize dependency between documents (topic correlation) as now they can easily share topics, resulting in a much robust and realistic compression algorithm for information modeling.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4220966911",
    "type": "article"
  },
  {
    "title": "Predicting a Person’s Next Activity Region with a Dynamic Region-Relation-Aware Graph Neural Network",
    "doi": "https://doi.org/10.1145/3529091",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Nengjun Zhu; Jian Cao; Xinjiang Lu; Chuanren Liu; Hao Liu; Yanyan Li; Xiangfeng Luo; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "The understanding of people’s inter-regional mobility behaviors, such as predicting the next activity region (AR) or uncovering the intentions for regional mobility, is of great value to public administration or business interests. While there are numerous studies on human mobility, these studies are mainly from a statistical view or study movement behaviors within a region. The work on individual-level inter-regional mobility behavior is limited. To this end, in this article, we propose a dynamic region-relation-aware graph neural network (DRRGNN) for exploring individual mobility behaviors over ARs. Specifically, we aim at developing models that can answer three questions: (1) Which regions are the ARs? (2) Which region will be the next AR, and (3) Why do people make this regional mobility? To achieve these tasks, we first propose a method to find out people’s ARs. Then, the designed model integrates a dynamic graph convolution network (DGCN) and a recurrent neural network (RNN) to depict the evolution of relations between ARs and mine the regional mobility patterns. In the learning process, the model further considers peoples’ profiles and visited point-of-interest (POIs). Finally, extensive experiments on two real-world datasets show that the proposed model can significantly improve accuracy for both the next AR prediction and mobility intention prediction.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4221054360",
    "type": "article"
  },
  {
    "title": "Detecting Anomalous Graphs in Labeled Multi-Graph Databases",
    "doi": "https://doi.org/10.1145/3533770",
    "publication_date": "2022-05-04",
    "publication_year": 2022,
    "authors": "Hung T. Nguyen; Pierre Jinghong Liang; Leman Akoglu",
    "corresponding_authors": "",
    "abstract": "Within a large database 𝒢 containing graphs with labeled nodes and directed, multi-edges; how can we detect the anomalous graphs? Most existing work are designed for plain (unlabeled) and/or simple (unweighted) graphs. We introduce CODEtect , the first approach that addresses the anomaly detection task for graph databases with such complex nature. To this end, it identifies a small representative set 𝒮 of structural patterns (i.e., node-labeled network motifs) that losslessly compress database 𝒢 as concisely as possible. Graphs that do not compress well are flagged as anomalous. CODEtect exhibits two novel building blocks: (i) a motif-based lossless graph encoding scheme, and (ii) fast memory-efficient search algorithms for 𝒮. We show the effectiveness of CODEtect on transaction graph databases from three different corporations and statistically similar synthetic datasets, where existing baselines adjusted for the task fall behind significantly, across different types of anomalies and performance metrics.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4225378533",
    "type": "article"
  },
  {
    "title": "DiVA: A Scalable, Interactive and Customizable Visual Analytics Platform for Information Diffusion on Large Networks",
    "doi": "https://doi.org/10.1145/3558771",
    "publication_date": "2022-08-24",
    "publication_year": 2022,
    "authors": "Dhruv Sehnan; Vasu Goel; Sarah Masud; Chhavi Jain; Vikram Goyal; Tanmoy Chakraborty",
    "corresponding_authors": "",
    "abstract": "With an increasing outreach of digital platforms in our lives, researchers have taken a keen interest in studying different facets of social interactions. Analyzing the spread of information ( aka diffusion) has brought forth multiple research areas such as modelling user engagement, determining emerging topics, forecasting the virality of online posts and predicting information cascades. Despite such ever-increasing interest, there remains a vacuum among easy-to-use interfaces for large-scale visualization of diffusion models. In this article, we introduce DiVA — Di ffusion V isualization and A nalysis, a tool that provides a scalable web interface and extendable APIs to analyze various diffusion trends on networks. DiVA uniquely offers support for simultaneous comparison of two competing diffusion models and even the comparison with the ground-truth results, which help develop a coherent understanding of real-world scenarios. Along with performing an exhaustive feature comparison and system evaluation of DiVA against publicly-available web interfaces for information diffusion, we conducted a user study to understand the strengths and limitations of DiVA . We noticed that evaluators had a seamless user experience, especially when analyzing diffusion on large networks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4293173627",
    "type": "article"
  },
  {
    "title": "SigGAN: Adversarial Model for Learning Signed Relationships in Networks",
    "doi": "https://doi.org/10.1145/3532610",
    "publication_date": "2022-05-20",
    "publication_year": 2022,
    "authors": "Roshni Chakraborty; Ritwika Das; Joydeep Chandra",
    "corresponding_authors": "",
    "abstract": "Signed link prediction in graphs is an important problem that has applications in diverse domains. It is a binary classification problem that predicts whether an edge between a pair of nodes is positive or negative. Existing approaches for link prediction in unsigned networks cannot be directly applied for signed link prediction due to their inherent differences. Furthermore, signed link prediction must consider the inherent characteristics of signed networks, such as structural balance theory. Recent signed link prediction approaches generate node representations using either generative models or discriminative models. Inspired by the recent success of Generative Adversarial Network (GAN) based models in several applications, we propose a GAN based model for signed networks, SigGAN. It considers the inherent characteristics of signed networks, such as integration of information from negative edges, high imbalance in number of positive and negative edges, and structural balance theory. Comparing the performance with state-of-the-art techniques on five real-world datasets validates the effectiveness of SigGAN.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4293249309",
    "type": "article"
  },
  {
    "title": "SPAP: Simultaneous Demand Prediction and Planning for Electric Vehicle Chargers in a New City",
    "doi": "https://doi.org/10.1145/3565577",
    "publication_date": "2022-10-04",
    "publication_year": 2022,
    "authors": "Yizong Wang; Dong Zhao; Yajie Ren; Desheng Zhang; Huadóng Ma",
    "corresponding_authors": "",
    "abstract": "For a new city that is committed to promoting Electric Vehicles (EVs), it is significant to plan the public charging infrastructure where charging demands are high. However, it is difficult to predict charging demands before the actual deployment of EV chargers for lack of operational data, resulting in a deadlock. A direct idea is to leverage the urban transfer learning paradigm to learn the knowledge from a source city, then exploit it to predict charging demands, and meanwhile determine locations and amounts of slow/fast chargers for charging stations in the target city. However, the demand prediction and charger planning depend on each other, and it is required to re-train the prediction model to eliminate the negative transfer between cities for each varied charger plan, leading to the unacceptable time complexity. To this end, we design an effective solution of S imultaneous Demand P rediction A nd P lanning ( SPAP ): discriminative features are extracted from multi-source data, and fed into an Attention-based Spatial-Temporal City Domain Adaptation Network ( AST-CDAN ) for cross-city demand prediction; a novel Transfer Iterative Optimization ( TIO ) algorithm is designed for charger planning by iteratively utilizing AST-CDAN and a charger plan fine-tuning algorithm. Extensive experiments on real-world datasets collected from three cities in China validate the effectiveness and efficiency of SPAP . Specially, SPAP improves at most 72.5% revenue compared with the real-world charger deployment.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4301395452",
    "type": "article"
  },
  {
    "title": "Lifelong Online Learning from Accumulated Knowledge",
    "doi": "https://doi.org/10.1145/3563947",
    "publication_date": "2022-10-17",
    "publication_year": 2022,
    "authors": "Changjian Shui; William Wang; Ihsen Hedhli; Chi Man Wong; Feng Wan; Boyu Wang; Christian Gagné",
    "corresponding_authors": "",
    "abstract": "In this article, we formulate lifelong learning as an online transfer learning procedure over consecutive tasks, where learning a given task depends on the accumulated knowledge. We propose a novel theoretical principled framework, lifelong online learning, where the learning process for each task is in an incremental manner. Specifically, our framework is composed of two-level predictions: the prediction information that is solely from the current task; and the prediction from the knowledge base by previous tasks. Moreover, this article tackled several fundamental challenges: arbitrary or even non-stationary task generation process, an unknown number of instances in each task, and constructing an efficient accumulated knowledge base. Notably, we provide a provable bound of the proposed algorithm, which offers insights on the how the accumulated knowledge improves the predictions. Finally, empirical evaluations on both synthetic and real datasets validate the effectiveness of the proposed algorithm.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4306407742",
    "type": "article"
  },
  {
    "title": "Visual Analysis of Brain Networks Using Sparse Regression Models",
    "doi": "https://doi.org/10.1145/3023363",
    "publication_date": "2018-02-06",
    "publication_year": 2018,
    "authors": "Lei Shi; Hanghang Tong; Madelaine Daianu; Feng Tian; Paul M. Thompson",
    "corresponding_authors": "",
    "abstract": "Studies of the human brain network are becoming increasingly popular in the fields of neuroscience, computer science, and neurology. Despite this rapidly growing line of research, gaps remain on the intersection of data analytics, interactive visual representation, and the human intelligence—all needed to advance our understanding of human brain networks. This article tackles this challenge by exploring the design space of visual analytics. We propose an integrated framework to orchestrate computational models with comprehensive data visualizations on the human brain network. The framework targets two fundamental tasks: the visual exploration of multi-label brain networks and the visual comparison among brain networks across different subject groups. During the first task, we propose a novel interactive user interface to visualize sets of labeled brain networks; in our second task, we introduce sparse regression models to select discriminative features from the brain network to facilitate the comparison. Through user studies and quantitative experiments, both methods are shown to greatly improve the visual comparison performance. Finally, real-world case studies with domain experts demonstrate the utility and effectiveness of our framework to analyze reconstructions of human brain connectivity maps. The perceptually optimized visualization design and the feature selection model calibration are shown to be the key to our significant findings.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2791982474",
    "type": "article"
  },
  {
    "title": "Housing Demand Estimation Based on Express Delivery Data",
    "doi": "https://doi.org/10.1145/3332522",
    "publication_date": "2019-07-26",
    "publication_year": 2019,
    "authors": "Qingyang Li; Zhiwen Yu; Bin Guo; Huang Xu; Xinjiang Lu",
    "corresponding_authors": "",
    "abstract": "Housing demand estimation is an important topic in the field of economic research. It is beneficial and helpful for various applications including real estate market regulation and urban planning, and therefore is crucial for both real estate investors and government administrators. Meanwhile, given the rapid development of the express industry, abundant useful information is embedded in express delivery records, which is helpful for researchers in profiling urban life patterns. The express delivery behaviors of the residents in a residential community can reflect the housing demand to some extent. Although housing demand has been analyzed in previous studies, its estimation has not been very good, and the subject remains under explored. To this end, in this article, we propose a systematic housing demand estimation method based on express delivery data. First, the express delivery records are aggregated on the community scale with the use of clustering methods, and the missing values in the records are completed. Then, various features are extracted from a less sparse dataset considering both the probability of residential mobility and the attractiveness of residential communities. In addition, given that the correlations between different districts can influence the performances of the inference model, the commonalities and differences of different districts are considered. After obtaining the features and correlations between different districts being obtained, the housing demand is estimated by using a multi-task learning method based on neural networks. The experimental results for real-world data show that the proposed model is effective at estimating the housing demand at the residential community level.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2966727702",
    "type": "article"
  },
  {
    "title": "In Search of a Stochastic Model for the E-News Reader",
    "doi": "https://doi.org/10.1145/3362695",
    "publication_date": "2019-11-13",
    "publication_year": 2019,
    "authors": "Bráulio M. Veloso; Renato Assunção; Anderson A. Ferreira; Nívio Ziviani",
    "corresponding_authors": "",
    "abstract": "E-news readers have increasingly at their disposal a broad set of news articles to read. Online newspaper sites use recommender systems to predict and to offer relevant articles to their users. Typically, these recommender systems do not leverage users’ reading behavior. If we know how the topics-reads change in a reading session, we may lead to fine-tuned recommendations, for example, after reading a certain number of sports items, it may be counter-productive to keep recommending other sports news. The motivation for this article is the assumption that understanding user behavior when reading successive online news articles can help in developing better recommender systems. We propose five categories of stochastic models to describe this behavior depending on how the previous reading history affects the future choices of topics. We instantiated these five classes with many different stochastic processes covering short-term memory, revealed-preference, cumulative advantage, and geometric sojourn models. Our empirical study is based on large datasets of E-news from two online newspapers. We collected data from more than 13 million users who generated more than 23 million reading sessions, each one composed by the successive clicks of the users on the posted news. We reduce each user session to the sequence of reading news topics. The models were fitted and compared using the Akaike Information Criterion and the Brier Score. We found that the best models are those in which the user moves through topics influenced only by their most recent readings. Our models were also better to predict the next reading than the recommender systems currently used in these journals showing that our models can improve user satisfaction.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3003404571",
    "type": "article"
  },
  {
    "title": "Sparse Graph Connectivity for Image Segmentation",
    "doi": "https://doi.org/10.1145/3397188",
    "publication_date": "2020-06-16",
    "publication_year": 2020,
    "authors": "Xiaofeng Zhu; Shichao Zhang; Jilian Zhang; Yonggang Li; Guangquan Lu; Yang Yang",
    "corresponding_authors": "",
    "abstract": "It has been demonstrated that the segmentation performance is highly dependent on both subspace preservation and graph connectivity. In the literature, the full connectivity method linearly represents each data point ( e.g., a pixel in one image) by all data points for achieving subspace preservation, while the sparse connectivity method was designed to linearly represent each data point by a set of data points for achieving graph connectivity. However, previous methods only focused on either subspace preservation or graph connectivity. In this article, we propose a Sparse Graph Connectivity (SGC) method for image segmentation to automatically learn the affinity matrix from the low-dimensional space of original data, which aims at simultaneously achieving subspace preservation and graph connectivity. To do this, the proposed SGC simultaneously learns a self-representation affinity matrix for subspace preservation and a sparse affinity matrix for graph connectivity, from the intrinsic low-dimensional feature space of high-dimensional original data. Meanwhile, the self-representation affinity matrix is pushed to be similar to the sparse affinity as well as be the final segmentation results. Experimental result on synthetic and real-image datasets showed that our SGC method achieved the best segmentation performance, compared to state-of-the-art segmentation methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3036051865",
    "type": "article"
  },
  {
    "title": "Scalable Spatial Scan Statistics for Trajectories",
    "doi": "https://doi.org/10.1145/3394046",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Michael E. Matheny; Dong Xie; Jeff M. Phillips",
    "corresponding_authors": "",
    "abstract": "We define several new models for how to define anomalous regions among enormous sets of trajectories. These are based on spatial scan statistics, and identify a geometric region which captures a subset of trajectories which are significantly different in a measured characteristic from the background population. The model definition depends on how much a geometric region is contributed to by some overlapping trajectory. This contribution can be the full trajectory, proportional to the length within the spatial region, or dependent on the flux across the boundary of that spatial region. Our methods are based on and significantly extend a recent two-level sampling approach which provides high accuracy at enormous scales of data. We support these new models and algorithms with extensive experiments on millions of trajectories and also theoretical guarantees.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3041990007",
    "type": "article"
  },
  {
    "title": "Exploiting User Preference and Mobile Peer Influence for Human Mobility Annotation",
    "doi": "https://doi.org/10.1145/3406600",
    "publication_date": "2020-09-28",
    "publication_year": 2020,
    "authors": "Renjun Hu; Yanchi Liu; Yanyan Li; Jingbo Zhou; Shuai Ma; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "Human mobility annotation aims to assign mobility records the corresponding visiting Point-of-Interests (POIs). It is one of the most fundamental problems for understanding human mobile behaviors. In literature, many efforts have been devoted to annotating mobility records in a pointwise or trajectory-wise manner. However, the user preference factor is not fully explored and, worse still, the mobile peer influence factor has never been integrated. To this end, in this article, we propose a novel framework, named JEPPI, to jointly exploit user preference and mobile peer influence to tackle the problem. In our JEPPI, we first unify the two distinct factors in a behavior-driven user-POI graph. This graph enables us to model user preference with user-POI visiting relationships, and model two types of mobile peer influence with co-location and co-visiting peer relationships, respectively. Moreover, we devise an equivalence-emphasizing metric to reduce redundancy in the second-order co-visiting peer influence. In addition, a mutual augmentation learning approach is proposed to preserve the latent structures of various factors exploited. Notably, our learning approach preserves all factors in a shared representation space such that user preference is learned with mobile peer influence being considered at the same time, and vice versa. In this way, the different factors are mutually augmented and semantically integrated to enhance human mobility annotation. Finally, using two large-scale real-world datasets, we conduct extensive experiments to demonstrate the superiority of our approach compared with the state-of-the-art annotation methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3089568126",
    "type": "article"
  },
  {
    "title": "iHypR",
    "doi": "https://doi.org/10.1145/2541268.2541269",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Sibel Adalı; Malik Magdon‐Ismail; Xiaohui Lu",
    "corresponding_authors": "",
    "abstract": "We present a new algorithm called iHypR for computing prominence of actors in social networks of collaborations. Our algorithm builds on the assumption that prominent actors collaborate on prominent objects, and prominent objects are naturally grouped into prominent clusters or groups (hyperedges in a graph). iHypR makes use of the relationships between actors, objects, and hyperedges to compute a global prominence score for the actors in the network. We do not assume the hyperedges are given in advance. Hyperedges computed by our method can perform as well or even better than “true” hyperedges. Our algorithm is customized for networks of collaborations, but it is generally applicable without further tuning. We show, through extensive experimentation with three real-life data sets and multiple external measures of prominence, that our algorithm outperforms existing well-known algorithms. Our work is the first to offer such an extensive evaluation. We show that unlike most existing algorithms, the performance is robust across multiple measures of performance. Further, we give a detailed study of the sensitivity of our algorithm to different data sets and the design choices within the algorithm that a user may wish to change. Our article illustrates the various trade-offs that must be considered in computing prominence in collaborative social networks.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1967656267",
    "type": "article"
  },
  {
    "title": "Exploiting fisher and fukunaga-koontz transforms in chernoff dimensionality reduction",
    "doi": "https://doi.org/10.1145/2499907.2499911",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Jing Peng; Guna Seetharaman; Wei Fan; Aparna S. Varde",
    "corresponding_authors": "",
    "abstract": "Knowledge discovery from big data demands effective representation of data. However, big data are often characterized by high dimensionality, which makes knowledge discovery more difficult. Many techniques for dimensionality reudction have been proposed, including well-known Fisher's Linear Discriminant Analysis (LDA). However, the Fisher criterion is incapable of dealing with heteroscedasticity in the data. A technique based on the Chernoff criterion for linear dimensionality reduction has been proposed that is capable of exploiting heteroscedastic information in the data. While the Chernoff criterion has been shown to outperform the Fisher's, a clear understanding of its exact behavior is lacking. In this article, we show precisely what can be expected from the Chernoff criterion. In particular, we show that the Chernoff criterion exploits the Fisher and Fukunaga-Koontz transforms in computing its linear discriminants. Furthermore, we show that a recently proposed decomposition of the data space into four subspaces is incomplete. We provide arguments on how to best enrich the decomposition of the data space in order to account for heteroscedasticity in the data. Finally, we provide experimental results validating our theoretical analysis.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2057495020",
    "type": "article"
  },
  {
    "title": "Density-Aware Clustering Based on Aggregated Heat Kernel and Its Transformation",
    "doi": "https://doi.org/10.1145/2700385",
    "publication_date": "2015-06-01",
    "publication_year": 2015,
    "authors": "Hao Huang; Shinjae Yoo; Dantong Yu; Hong Qin",
    "corresponding_authors": "",
    "abstract": "Current spectral clustering algorithms suffer from the sensitivity to existing noise and parameter scaling and may not be aware of different density distributions across clusters. If these problems are left untreated, the consequent clustering results cannot accurately represent true data patterns, in particular, for complex real-world datasets with heterogeneous densities. This article aims to solve these problems by proposing a diffusion-based Aggregated Heat Kernel (AHK) to improve the clustering stability, and a Local Density Affinity Transformation (LDAT) to correct the bias originating from different cluster densities. AHK statistically models the heat diffusion traces along the entire time scale, so it ensures robustness during the clustering process, while LDAT probabilistically reveals the local density of each instance and suppresses the local density bias in the affinity matrix. Our proposed framework integrates these two techniques systematically. As a result, it not only provides an advanced noise-resisting and density-aware spectral mapping to the original dataset but also demonstrates the stability during the processing of tuning the scaling parameter (which usually controls the range of neighborhood). Furthermore, our framework works well with the majority of similarity kernels, which ensures its applicability to many types of data and problem domains. The systematic experiments on different applications show that our proposed algorithm outperforms state-of-the-art clustering algorithms for the data with heterogeneous density distributions and achieves robust clustering performance with respect to tuning the scaling parameter and handling various levels and types of noise.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2219250615",
    "type": "article"
  },
  {
    "title": "Leveraging Neighbor Attributes for Classification in Sparsely Labeled Networks",
    "doi": "https://doi.org/10.1145/2898358",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Luke K. McDowell; David W. Aha",
    "corresponding_authors": "",
    "abstract": "Many analysis tasks involve linked nodes, such as people connected by friendship links. Research on link-based classification (LBC) has studied how to leverage these connections to improve classification accuracy. Most such prior research has assumed the provision of a densely labeled training network. Instead, this article studies the common and challenging case when LBC must use a single sparsely labeled network for both learning and inference, a case where existing methods often yield poor accuracy. To address this challenge, we introduce a novel method that enables prediction via “neighbor attributes,” which were briefly considered by early LBC work but then abandoned due to perceived problems. We then explain, using both extensive experiments and loss decomposition analysis, how using neighbor attributes often significantly improves accuracy. We further show that using appropriate semi-supervised learning (SSL) is essential to obtaining the best accuracy in this domain and that the gains of neighbor attributes remain across a range of SSL choices and data conditions. Finally, given the challenges of label sparsity for LBC and the impact of neighbor attributes, we show that multiple previous studies must be re-considered, including studies regarding the best model features, the impact of noisy attributes, and strategies for active learning.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2480368286",
    "type": "article"
  },
  {
    "title": "Fast Sampling for Time-Varying Determinantal Point Processes",
    "doi": "https://doi.org/10.1145/2943785",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Maoying Qiao; Richard Yi Da Xu; Wei Bian; Dacheng Tao",
    "corresponding_authors": "",
    "abstract": "Determinantal Point Processes (DPPs) are stochastic models which assign each subset of a base dataset with a probability proportional to the subset’s degree of diversity. It has been shown that DPPs are particularly appropriate in data subset selection and summarization (e.g., news display, video summarizations). DPPs prefer diverse subsets while other conventional models cannot offer. However, DPPs inference algorithms have a polynomial time complexity which makes it difficult to handle large and time-varying datasets, especially when real-time processing is required. To address this limitation, we developed a fast sampling algorithm for DPPs which takes advantage of the nature of some time-varying data (e.g., news corpora updating, communication network evolving), where the data changes between time stamps are relatively small. The proposed algorithm is built upon the simplification of marginal density functions over successive time stamps and the sequential Monte Carlo (SMC) sampling technique. Evaluations on both a real-world news dataset and the Enron Corpus confirm the efficiency of the proposed algorithm.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2494094731",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue of Best Papers in ACM SIGKDD 2014",
    "doi": "https://doi.org/10.1145/2936718",
    "publication_date": "2016-07-27",
    "publication_year": 2016,
    "authors": "Wei Wang; Jure Leskovec",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2496152636",
    "type": "article"
  },
  {
    "title": "Unsupervised Head--Modifier Detection in Search Queries",
    "doi": "https://doi.org/10.1145/2988235",
    "publication_date": "2016-12-26",
    "publication_year": 2016,
    "authors": "Zhongyuan Wang; Fang Wang; Haixun Wang; Zhirui Hu; Jun Yan; Fangtao Li; Ji-Rong Wen; Zhoujun Li",
    "corresponding_authors": "",
    "abstract": "Interpreting the user intent in search queries is a key task in query understanding. Query intent classification has been widely studied. In this article, we go one step further to understand the query from the view of head--modifier analysis. For example, given the query “popular iphone 5 smart cover,” instead of using coarse-grained semantic classes (e.g., find electronic product ), we interpret that “smart cover” is the head or the intent of the query and “iphone 5” is its modifier. Query head--modifier detection can help search engines to obtain particularly relevant content, which is also important for applications such as ads matching and query recommendation. We introduce an unsupervised semantic approach for query head--modifier detection. First, we mine a large number of instance level head--modifier pairs from search log. Then, we develop a conceptualization mechanism to generalize the instance level pairs to concept level. Finally, we derive weighted concept patterns that are concise, accurate, and have strong generalization power in head--modifier detection. The developed mechanism has been used in production for search relevance and ads matching. We use extensive experiment results to demonstrate the effectiveness of our approach.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2547396710",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2513092",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms, including classification, clustering, motif ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4253696321",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1921632",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Given large, multimillion-node graphs (e.g., Facebook, Web-crawls, etc.), how do they evolve over time? How are they connected? What are the central nodes and the outliers? In this article we define the Radius plot of a graph and show how it can answer ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4253701887",
    "type": "paratext"
  },
  {
    "title": "Fast Discovery of Group Lag Correlations in Streams",
    "doi": "https://doi.org/10.1145/1870096.1870101",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Yasushi Sakurai; Christos Faloutsos; Spiros Papadimitriou",
    "corresponding_authors": "",
    "abstract": "The study of data streams has received considerable attention in various communities (theory, databases, data mining, networking), due to several important applications, such as network analysis, sensor monitoring, financial data analysis, and moving object tracking. Our goal in this article is to monitor multiple numerical streams and determine which pairs are correlated with lags, as well as the value of each such lag. Lag correlations and anticorrelations are frequent and very interesting in practice. For example, a decrease in interest rates typically precedes an increase in house sales by a few months; higher amounts of fluoride in drinking water may lead to fewer dental cavities some years later. Other lag settings include network analysis, sensor monitoring, financial data analysis, and tracking of moving objects. Such data streams are often correlated or anticorrelated, but with unknown lag. We propose BRAID, a method of detecting lag correlations among data streams. BRAID can handle data streams of semi-infinite length incrementally, quickly, and with small resource consumption. However, BRAID requires space and time quadratic on a number of streams k . We also propose ThinBRAID, which is even faster than BRAID, requiring O ( k ) space and time per time tick. Our theoretical analysis shows that BRAID/ThinBRAID can estimate lag correlations with little or, often, with no error. Our experiments on real and realistic data show that BRAID and ThinBRAID detect the correct lag perfectly most of the time (the largest relative error was about 1%), while they are significantly faster (up to 40,000 times) than the naïve implementation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2089871757",
    "type": "article"
  },
  {
    "title": "Substantial improvements in the set-covering projection classifier CHIRP (composite hypercubes on iterated random projections)",
    "doi": "https://doi.org/10.1145/2382577.2382583",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Leland Wilkinson; Anushka Anand; Tommy Dang",
    "corresponding_authors": "",
    "abstract": "In Wilkinson et al. [2011] we introduced a new set-covering random projection classifier that achieved average error lower than that of other classifiers in the Weka platform. This classifier was based on an L ∞ norm distance function and exploited an iterative sequence of three stages (projecting, binning, and covering) to deal with the curse of dimensionality, computational complexity, and nonlinear separability. We now present substantial changes that improve robustness and reduce training and testing time by almost an order of magnitude without jeopardizing CHIRP's outstanding error performance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2139492896",
    "type": "article"
  },
  {
    "title": "Fast Connectivity Minimization on Large-Scale Networks",
    "doi": "https://doi.org/10.1145/3442342",
    "publication_date": "2021-05-03",
    "publication_year": 2021,
    "authors": "Chen Chen; Ruiyue Peng; Lei Ying; Hanghang Tong",
    "corresponding_authors": "",
    "abstract": "The connectivity of networks has been widely studied in many high-impact applications, ranging from immunization, critical infrastructure analysis, social network mining, to bioinformatic system studies. Regardless of the end application domains, connectivity minimization has always been a fundamental task to effectively control the functioning of the underlying system. The combinatorial nature of the connectivity minimization problem imposes an exponential computational complexity to find the optimal solution, which is intractable in large systems. To tackle the computational barrier, greedy algorithm is extensively used to ensure a near-optimal solution by exploiting the diminishing returns property of the problem. Despite the empirical success, the theoretical and algorithmic challenges of the problems still remain wide open. On the theoretical side, the intrinsic hardness and the approximability of the general connectivity minimization problem are still unknown except for a few special cases. On the algorithmic side, existing algorithms are hard to balance between the optimization quality and computational efficiency. In this article, we address the two challenges by (1) proving that the general connectivity minimization problem is NP-hard and <?TeX $1-1/e$?> is the best approximation ratio for any polynomial algorithms, and (2) proposing the algorithm CONTAIN and its variant CONTAIN + that can well balance optimization effectiveness and computational efficiency for eigen-function based connectivity minimization problems in large networks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3157991180",
    "type": "article"
  },
  {
    "title": "Efficient and High-Quality Seeded Graph Matching: Employing Higher-order Structural Information",
    "doi": "https://doi.org/10.1145/3442340",
    "publication_date": "2021-05-03",
    "publication_year": 2021,
    "authors": "Haida Zhang; Zengfeng Huang; Xuemin Lin; Zhe Lin; Wenjie Zhang; Ying Zhang",
    "corresponding_authors": "",
    "abstract": "Driven by many real applications, we study the problem of seeded graph matching. Given two graphs and , and a small set of pre-matched node pairs where and , the problem is to identify a matching between and growing from , such that each pair in the matching corresponds to the same underlying entity. Recent studies on efficient and effective seeded graph matching have drawn a great deal of attention and many popular methods are largely based on exploring the similarity between local structures to identify matching pairs. While these recent techniques work provably well on random graphs, their accuracy is low over many real networks. In this work, we propose to utilize higher-order neighboring information to improve the matching accuracy and efficiency. As a result, a new framework of seeded graph matching is proposed, which employs Personalized PageRank (PPR) to quantify the matching score of each node pair. To further boost the matching accuracy, we propose a novel postponing strategy, which postpones the selection of pairs that have competitors with similar matching scores. We show that the postpone strategy indeed significantly improves the matching accuracy. To improve the scalability of matching large graphs, we also propose efficient approximation techniques based on algorithms for computing PPR heavy hitters. Our comprehensive experimental studies on large-scale real datasets demonstrate that, compared with state-of-the-art approaches, our framework not only increases the precision and recall both by a significant margin but also achieves speed-up up to more than one order of magnitude.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3158808070",
    "type": "article"
  },
  {
    "title": "DeepDepict",
    "doi": "https://doi.org/10.1145/3446982",
    "publication_date": "2021-06-28",
    "publication_year": 2021,
    "authors": "Shaoyang Hao; Bin Guo; Hao Wang; Yunji Liang; Lina Yao; Qianru Wang; Zhiwen Yu",
    "corresponding_authors": "",
    "abstract": "In e-commerce platforms, the online descriptive information of products shows significant impacts on the purchase behaviors. To attract potential buyers for product promotion, numerous workers are employed to write the impressive product descriptions. The hand-crafted product descriptions are less-efficient with great labor costs and huge time consumption. Meanwhile, the generated product descriptions do not take consideration into the customization and the diversity to meet users’ interests. To address these problems, we propose one generic framework, namely DeepDepict, to automatically generate the information-rich and personalized product descriptive information. Specifically, DeepDepict leverages the graph attention to retrieve the product-related knowledge from external knowledge base to enrich the diversity of products, constructs the personalized lexicon to capture the linguistic traits of individuals for the personalization of product descriptions, and utilizes multiple pointer-generator network to fuse heterogeneous data from multi-sources to generate informative and personalized product descriptions. We conduct intensive experiments on one public dataset. The experimental results show that DeepDepict outperforms existing solutions in terms of description diversity, BLEU, and personalized degree with significant margin gain, and is able to generate product descriptions with comprehensive knowledge and personalized linguistic traits.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3176188030",
    "type": "article"
  },
  {
    "title": "Online and Distributed Robust Regressions with Extremely Noisy Labels",
    "doi": "https://doi.org/10.1145/3473038",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Shuo Lei; Xuchao Zhang; Liang Zhao; Arnold P. Boedihardjo; Chang‐Tien Lu",
    "corresponding_authors": "",
    "abstract": "In today’s era of big data, robust least-squares regression becomes a more challenging problem when considering the extremely corrupted labels along with explosive growth of datasets. Traditional robust methods can handle the noise but suffer from several challenges when applied in huge dataset including (1) computational infeasibility of handling an entire dataset at once, (2) existence of heterogeneously distributed corruption, and (3) difficulty in corruption estimation when data cannot be entirely loaded. This article proposes online and distributed robust regression approaches, both of which can concurrently address all the above challenges. Specifically, the distributed algorithm optimizes the regression coefficients of each data block via heuristic hard thresholding and combines all the estimates in a distributed robust consolidation. In addition, an online version of the distributed algorithm is proposed to incrementally update the existing estimates with new incoming data. Furthermore, a novel online robust regression method is proposed to estimate under a biased-batch corruption. We also prove that our algorithms benefit from strong robustness guarantees in terms of regression coefficient recovery with a constant upper bound on the error of state-of-the-art batch methods. Extensive experiments on synthetic and real datasets demonstrate that our approaches are superior to those of existing methods in effectiveness, with competitive efficiency.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3210528129",
    "type": "article"
  },
  {
    "title": "A Normalizing Flow-Based Co-Embedding Model for Attributed Networks",
    "doi": "https://doi.org/10.1145/3477049",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Shangsong Liang; Z. Ouyang; Zaiqiao Meng",
    "corresponding_authors": "",
    "abstract": "Network embedding is a technique that aims at inferring the low-dimensional representations of nodes in a semantic space. In this article, we study the problem of inferring the low-dimensional representations of both nodes and attributes for attributed networks in the same semantic space such that the affinity between a node and an attribute can be effectively measured. Intuitively, this problem can be addressed by simply utilizing existing variational auto-encoder (VAE) based network embedding algorithms. However, the variational posterior distribution in previous VAE based network embedding algorithms is often assumed and restricted to be a mean-field Gaussian distribution or other simple distribution families, which results in poor inference of the embeddings. To alleviate the above defect, we propose a novel VAE-based co-embedding method for attributed network, F-CAN, where posterior distributions are flexible, complex, and scalable distributions constructed through the normalizing flow. We evaluate our proposed models on a number of network tasks with several benchmark datasets. Experimental results demonstrate that there are clear improvements in the qualities of embeddings generated by our model to the state-of-the-art attributed network embedding methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3211043432",
    "type": "article"
  },
  {
    "title": "Spatial Prediction for Multivariate Non-Gaussian Data",
    "doi": "https://doi.org/10.1145/3022669",
    "publication_date": "2017-03-27",
    "publication_year": 2017,
    "authors": "Xutong Liu; Feng Chen; Yen-Cheng Lu; Chang‐Tien Lu",
    "corresponding_authors": "",
    "abstract": "With the ever increasing volume of geo-referenced datasets, there is a real need for better statistical estimation and prediction techniques for spatial analysis. Most existing approaches focus on predicting multivariate Gaussian spatial processes, but as the data may consist of non-Gaussian (or mixed type) variables, this creates two challenges: (1) how to accurately capture the dependencies among different data types, both Gaussian and non-Gaussian; and (2) how to efficiently predict multivariate non-Gaussian spatial processes. In this article, we propose a generic approach for predicting multiple response variables of mixed types. The proposed approach accurately captures cross-spatial dependencies among response variables and reduces the computational burden by projecting the spatial process to a lower dimensional space with knot-based techniques. Efficient approximations are provided to estimate posterior marginals of latent variables for the predictive process, and extensive experimental evaluations based on both simulation and real-life datasets are provided to demonstrate the effectiveness and efficiency of this new approach.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2602851708",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue on the Best Papers from KDD 2016",
    "doi": "https://doi.org/10.1145/3092689",
    "publication_date": "2017-06-29",
    "publication_year": 2017,
    "authors": "Charų C. Aggarwal",
    "corresponding_authors": "Charų C. Aggarwal",
    "abstract": "This issue contains the best papers from the ACM KDD Conference 2016. As is customary at KDD, special issue papers are invited only from the research track. The top-ranked papers from the KDD 2016 conference are included in this issue. This issue contains a total of six articles, which are from different areas of data mining. A brief description of these articles is also provided in this article.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2724503620",
    "type": "article"
  },
  {
    "title": "Data Stream Evolution Diagnosis Using Recursive Wavelet Density Estimators",
    "doi": "https://doi.org/10.1145/3106369",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Edgar S. García-Treviño; Muhammad Zaid Hameed; Javier Barria",
    "corresponding_authors": "",
    "abstract": "Data streams are a new class of data that is becoming pervasively important in a wide range of applications, ranging from sensor networks, environmental monitoring to finance. In this article, we propose a novel framework for the online diagnosis of evolution of multidimensional streaming data that incorporates Recursive Wavelet Density Estimators into the context of Velocity Density Estimation. In the proposed framework changes in streaming data are characterized by the use of local and global evolution coefficients . In addition, we propose for the analysis of changes in the correlation structure of the data a recursive implementation of the Pearson correlation coefficient using exponential discounting. Two visualization tools, namely temporal and spatial velocity profiles, are extended in the context of the proposed framework. These are the three main advantages of the proposed method over previous approaches: (1) the memory storage required is minimal and independent of any window size; (2) it has a significantly lower computational complexity; and (3) it makes possible the fast diagnosis of data evolution at all dimensions and at relevant combinations of dimensions with only one pass of the data. With the help of the four examples, we show the framework’s relevance in a change detection context and its potential capability for real world applications.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2785173328",
    "type": "article"
  },
  {
    "title": "Function-on-Function Regression with Mode-Sparsity Regularization",
    "doi": "https://doi.org/10.1145/3178113",
    "publication_date": "2018-03-23",
    "publication_year": 2018,
    "authors": "Pei Yang; Qi Tan; Jingrui He",
    "corresponding_authors": "",
    "abstract": "Functional data is ubiquitous in many domains, such as healthcare, social media, manufacturing process, sensor networks, and so on. The goal of function-on-function regression is to build a mapping from functional predictors to functional response. In this article, we propose a novel function-on-function regression model based on mode-sparsity regularization. The main idea is to represent the regression coefficient function between predictor and response as the double expansion of basis functions, and then use a mode-sparsity regularization to automatically filter out irrelevant basis functions for both predictors and responses. The proposed approach is further extended to the tensor version to accommodate multiple functional predictors. While allowing the dimensionality of the regression weight matrix or tensor to be relatively large, the mode-sparsity regularized model facilitates the multi-way shrinking of basis functions for each mode. The proposed mode-sparsity regularization covers a wide spectrum of sparse models for function-on-function regression. The resulting optimization problem is challenging due to the non-smooth property of the mode-sparsity regularization. We develop an efficient algorithm to solve the problem, which works in an iterative update fashion, and converges to the global optimum. Furthermore, we analyze the generalization performance of the proposed method and derive an upper bound for the consistency between the recovered function and the underlying true function. The effectiveness of the proposed approach is verified on benchmark functional datasets in various domains.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2789887307",
    "type": "article"
  },
  {
    "title": "GTΔ",
    "doi": "https://doi.org/10.1145/3183346",
    "publication_date": "2018-04-16",
    "publication_year": 2018,
    "authors": "Edward Toth; Sanjay Chawla",
    "corresponding_authors": "",
    "abstract": "Given a portfolio of stocks or a series of frames in a video how do we detect significant changes in a group of values for real-time applications? In this article, we formalize the problem of sequentially detecting temporal changes in a group of stochastic processes. As a solution to this particular problem, we propose the group temporal change (GTΔ) algorithm, a simple yet effective technique for the sequential detection of significant changes in a variety of statistical properties of a group over time. Due to the flexible framework of the GTΔ algorithm, a domain expert is able to select one or more statistical properties that they are interested in monitoring. The usefulness of our proposed algorithm is also demonstrated against state-of-the-art techniques on synthetically generated data as well as on two real-world applications; a portfolio of healthcare stocks over a 20 year period and a video monitoring the activity of our Sun.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2802253798",
    "type": "article"
  },
  {
    "title": "Robust Regression via Heuristic Corruption Thresholding and Its Adaptive Estimation Variation",
    "doi": "https://doi.org/10.1145/3314105",
    "publication_date": "2019-06-07",
    "publication_year": 2019,
    "authors": "Xuchao Zhang; Shuo Lei; Liang Zhao; Arnold P. Boedihardjo; Chang‐Tien Lu",
    "corresponding_authors": "",
    "abstract": "The presence of data noise and corruptions has recently invoked increasing attention on robust least-squares regression ( RLSR ), which addresses this fundamental problem that learns reliable regression coefficients when response variables can be arbitrarily corrupted. Until now, the following important challenges could not be handled concurrently: (1) rigorous recovery guarantee of regression coefficients, (2) difficulty in estimating the corruption ratio parameter, and (3) scaling to massive datasets. This article proposes a novel Robust regression algorithm via Heuristic Corruption Thresholding ( RHCT ) that concurrently addresses all the above challenges. Specifically, the algorithm alternately optimizes the regression coefficients and estimates the optimal uncorrupted set via heuristic thresholding without a pre-defined corruption ratio parameter until its convergence. Moreover, to improve the efficiency of corruption estimation in large-scale data, a Robust regression algorithm via Adaptive Corruption Thresholding ( RACT ) is proposed to determine the size of the uncorrupted set in a novel adaptive search method without iterating data samples exhaustively. In addition, we prove that our algorithms benefit from strong guarantees analogous to those of state-of-the-art methods in terms of convergence rates and recovery guarantees. Extensive experiments demonstrate that the effectiveness of our new methods is superior to that of existing methods in the recovery of both regression coefficients and uncorrupted sets, with very competitive efficiency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2952286880",
    "type": "article"
  },
  {
    "title": "Bayesian Model Selection Approach to Multiple Change-Points Detection with Non-Local Prior Distributions",
    "doi": "https://doi.org/10.1145/3340804",
    "publication_date": "2019-09-24",
    "publication_year": 2019,
    "authors": "Fei Jiang; Guosheng Yin; Francesca Dominici",
    "corresponding_authors": "",
    "abstract": "We propose a Bayesian model selection (BMS) boundary detection procedure using non-local prior distributions for a sequence of data with multiple systematic mean changes. By using the non-local priors in the BMS framework, the BMS method can effectively suppress the non-boundary spike points with large instantaneous changes. Further, we speedup the algorithm by reducing the multiple change points to a series of single change point detection problems. We establish the consistency of the estimated number and locations of the change points under various prior distributions. From both theoretical and numerical perspectives, we show that the non-local inverse moment prior leads to the fastest convergence rate in identifying the true change points on the boundaries. Extensive simulation studies are conducted to compare the BMS with existing methods, and our method is illustrated with application to the magnetic resonance imaging guided radiation therapy data.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2976806635",
    "type": "article"
  },
  {
    "title": "Budget-Constrained Real-Time Bidding Optimization",
    "doi": "https://doi.org/10.1145/3375393",
    "publication_date": "2020-02-09",
    "publication_year": 2020,
    "authors": "Chi-Chun Lin; Kun-Ta Chuang; Wush Wu; Ming-Syan Chen⋆",
    "corresponding_authors": "",
    "abstract": "In this article, we pursue a better solution for the promising problem, i.e., the bidding strategy design, in the real-time bidding (RTB) advertising (AD) environment. Under the budget constraint, the design of an optimal strategy for bidding on each incoming impression opportunity targets at acquiring as many clicks as possible during an AD campaign. State-of-the-art bidding algorithms rely on a single predictor, the clickthrough rate predictor, to calculate the bidding value for each impression. This provides reasonable performance if the predictor has appropriate accuracy in predicting the probability of user clicking. However, the classical methods usually fail to capture optimal results since the predictor accuracy is limited. We improve the situation by accomplishing an additional winning price predictor in the bidding process. In this article, an algorithm combining powers of multiple prediction models is developed. It emerges from an analogy to the online stochastic knapsack problem, and the efficiency of the algorithm is also theoretically analyzed. Experiments conducted on real world RTB datasets show that the proposed solution performs better with regard to both number of clicks achieved and effective cost per click in many different settings of budget constraints.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3006297623",
    "type": "article"
  },
  {
    "title": "Linearization of Dependency and Sampling for Participation-based Betweenness Centrality in Very Large B-hypergraphs",
    "doi": "https://doi.org/10.1145/3375399",
    "publication_date": "2020-03-13",
    "publication_year": 2020,
    "authors": "Kwang Hee Lee; Myoung Ho Kim",
    "corresponding_authors": "",
    "abstract": "A B-hypergraph consisting of nodes and directed hyperedges is a generalization of the directed graph. A directed hyperedge in the B-hypergraph represents a relation from a set of source nodes to a single destination node. We suggest one possible definition of betweenness centrality (BC) in B-hypergraphs, called Participation-based BC (PBC). A PBC score of a node is computed based on the number of the shortest paths in which the node participates. This score can be expressed in terms of dependency on the set of its outgoing hyperedges. In this article, we focus on developing efficient computation algorithms for PBC. We first present an algorithm called ePBC for computing exact PBC scores of nodes, which has a cubic-time complexity. This algorithm, however, can be used for only small-sized B-hypergraphs because of its cubic-time complexity, so we propose linearized PBC ( ℓ PBC) that is an approximation method of ePBC. ℓ PBC that comes with a guaranteed upper bound on its error, uses a linearization of dependency on a set of hyperedges. ℓ PBC improves the computing time of ePBC by an order of magnitude (i.e., it requires a quadratic time) while maintaining a high accuracy. ℓ PBC works well on small to medium-sized B-hypergraphs, but is not scalable enough for a very large B-hypergraph with more than a million hyperedges. To cope with such a very large B-hypergraph, we propose a very fast heuristic sampling-based method called sampling-based ℓ PBC (s ℓ PBC). We show through extensive experiments that ℓ PBC and s ℓ PBC can efficiently estimate PBC scores in various real-world B-hypergraphs with a reasonably good precision@ k . The experimental results show that s ℓ PBC works efficiently even for a very large B-hypergraph.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3011437903",
    "type": "article"
  },
  {
    "title": "MeSHProbeNet-P",
    "doi": "https://doi.org/10.1145/3421713",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Guangxu Xun; Kishlay Jha; Aidong Zhang",
    "corresponding_authors": "",
    "abstract": "Indexing biomedical research articles with Medical Subject Headings (MeSH) can greatly facilitate biomedical research and information retrieval. Currently MeSH indexing is performed by human experts. To alleviate the time consumption and monetary cost caused by manual indexing, many automatic MeSH indexing models have been developed, such as MeSHProbeNet, DeepMeSH, and NLM’s official model Medical Text Indexer. In this article, we propose an end-to-end framework, MeSHProbeNet-P, which extends MeSHProbeNet with personalizable MeSH probes. In MeSHProbeNet-P, each MeSH probe carries certain aspects of biomedical knowledge and extracts related information from input articles. MeSHProbeNet-P is able to automatically personalize its MeSH probes for different input articles to ensure that the current MeSH probes best fit the current input article and the most informative features can be extracted from the article. We demonstrate the effectiveness of MeSHProbeNet-P in a real-world large-scale MeSH indexing challenge. MeSHProbeNet-P won the first place in the first batch of Task A in the 2019 BioASQ challenge. The result on the first test set of the challenge is reported in this article. We also provide ablation studies to show the advantages of personalizable MeSH probes.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3111905973",
    "type": "article"
  },
  {
    "title": "Multi-Stage Network Embedding for Exploring Heterogeneous Edges",
    "doi": "https://doi.org/10.1145/3415157",
    "publication_date": "2020-12-07",
    "publication_year": 2020,
    "authors": "Hong Huang; Song Yu; Fanghua Ye; Xing Xie; Xuanhua Shi; Hai Jin",
    "corresponding_authors": "",
    "abstract": "The relationships between objects in a network are typically diverse and complex, leading to the heterogeneous edges with different semantic information. In this article, we focus on exploring the heterogeneous edges for network representation learning. By considering each relationship as a view that depicts a specific type of proximity between nodes, we propose a multi-stage non-negative matrix factorization (MNMF) model, committed to utilizing abundant information in multiple views to learn robust network representations. In fact, most existing network embedding methods are closely related to implicitly factorizing the complex proximity matrix. However, the approximation error is usually quite large, since a single low-rank matrix is insufficient to capture the original information. Through a multi-stage matrix factorization process motivated by gradient boosting, our MNMF model achieves lower approximation error. Meanwhile, the multi-stage structure of MNMF gives the feasibility of designing two kinds of non-negative matrix factorization (NMF) manners to preserve network information better. The united NMF aims to preserve the consensus information between different views, and the independent NMF aims to preserve unique information of each view. Concrete experimental results on realistic datasets indicate that our model outperforms three types of baselines in practical applications.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3112821822",
    "type": "article"
  },
  {
    "title": "Multi-label Deep Convolutional Transform Learning for Non-intrusive Load Monitoring",
    "doi": "https://doi.org/10.1145/3502729",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Shikha Singh; Émilie Chouzenoux; Giovanni Chierchia; Angshul Majumdar",
    "corresponding_authors": "",
    "abstract": "The objective of this letter is to propose a novel computational method to learn the state of an appliance (ON / OFF) given the aggregate power consumption recorded by the smart-meter. We formulate a multi-label classification problem where the classes correspond to the appliances. The proposed approach is based on our recently introduced framework of convolutional transform learning. We propose a deep supervised version of it relying on an original multi-label cost. Comparisons with state-of-the-art techniques show that our proposed method improves over the benchmarks on popular non-intrusive load monitoring datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4200630337",
    "type": "article"
  },
  {
    "title": "B <sub> <i>h</i> </sub> BF: A Bloom Filter Using B <sub> <i>h</i> </sub> Sequences for Multi-set Membership Query",
    "doi": "https://doi.org/10.1145/3502735",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Shuyu Pei; Kun Xie; Xin Wang; Gaogang Xie; Kenli Li; Wei Li; Yanbiao Li; Jigang Wen",
    "corresponding_authors": "",
    "abstract": "Multi-set membership query is a fundamental issue for network functions such as packet processing and state machines monitoring. Given the rigid query speed and memory requirements, it would be promising if a multi-set query algorithm can be designed based on Bloom filter (BF), a space-efficient probabilistic data structure. However, existing efforts on multi-set query based on BF suffer from at least one of the following drawbacks: low query speed, low query accuracy, limitation in only supporting insertion and query operations, or limitation in the set size. To address the issues, we design a novel B h sequence-based Bloom filter (B h BF) for multi-set query, which supports four operations: insertion, query, deletion, and update. In B h BF, the set ID is encoded as a code in a B h sequence. Exploiting good properties of B h sequences, we can correctly decode the BF cells to obtain the set IDs even when the number of hash collisions is high, which brings high query accuracy. In B h BF, we propose two strategies to further speed up the query speed and increase the query accuracy. On the theoretical side, we analyze the false positive and classification failure rate of our B h BF. Our results from extensive experiments over two real datasets demonstrate that B h BF significantly advances state-of-the-art multi-set query algorithms.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4220834278",
    "type": "article"
  },
  {
    "title": "Who will Win the Data Science Competition? Insights from KDD Cup 2019 and Beyond",
    "doi": "https://doi.org/10.1145/3511896",
    "publication_date": "2022-04-05",
    "publication_year": 2022,
    "authors": "Hao Liu; Qingyu Guo; Hengshu Zhu; Fuzhen Zhuang; Shenwen Yang; Dejing Dou; Hui Xiong",
    "corresponding_authors": "",
    "abstract": "Data science competitions are becoming increasingly popular for enterprises collecting advanced innovative solutions and allowing contestants to sharpen their data science skills. Most existing studies about data science competitions have a focus on improving task-specific data science techniques, such as algorithm design and parameter tuning. However, little effort has been made to understand the data science competition itself. To this end, in this article, we shed light on the team’s competition performance, and investigate the team’s evolving performance in the crowd-sourcing competitive innovation context. Specifically, we first acquire and construct multi-sourced datasets of various data science competitions, including the KDD Cup 2019 machine learning competition and beyond. Then, we conduct an empirical analysis to identify and quantify a rich set of features that are significantly correlated with teams’ future performances. By leveraging team’s rank as a proxy, we observe “the stronger, the stronger” rule; that is, top-ranked teams tend to keep their advantages and dominate weaker teams for the rest of the competition. Our results also confirm that teams with diversified backgrounds tend to achieve better performances. After that, we formulate the team’s future rank prediction problem and propose the Multi-Task Representation Learning (MTRL) framework to model both static features and dynamic features. Extensive experimental results on four real-world data science competitions demonstrate the team’s future performance can be well predicted by using MTRL. Finally, we envision our study will not only help competition organizers to understand the competition in a better way, but also provide strategic implications to contestants, such as guiding the team formation and designing the submission strategy.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4226196772",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1297332",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "How do we find a natural clustering of a real-world point set which contains an unknown number of clusters with different shapes, and which may be contaminated by noise? As most clustering algorithms were designed with certain assumptions (Gaussianity), ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4235901508",
    "type": "paratext"
  },
  {
    "title": "On Equivalence of Anomaly Detection Algorithms",
    "doi": "https://doi.org/10.1145/3536428",
    "publication_date": "2022-05-18",
    "publication_year": 2022,
    "authors": "Carlos Ivan Jerez; Jun Zhang; Marcia R. Silva",
    "corresponding_authors": "",
    "abstract": "In most domains, anomaly detection is typically cast as an unsupervised learning problem because of the infeasibility of labeling large datasets. In this setup, the evaluation and comparison of different anomaly detection algorithms is difficult. Although some work has been published in this field, they fail to account that different algorithms can detect different kinds of anomalies. More precisely, the literature on this topic has focused on defining criteria to determine which algorithm is better, while ignoring the fact that such criteria are meaningful only if the algorithms being compared are detecting the same kind of anomalies. Therefore, in this article, we propose an equivalence criterion for anomaly detection algorithms that measures to what degree two anomaly detection algorithms detect the same kind of anomalies. First, we lay out a set of desirable properties that such an equivalence criterion should have and why; second, we propose Gaussian Equivalence Criterion (GEC) as equivalence criterion and show mathematically that it has the desirable properties previously mentioned. Finally, we empirically validate these properties using a simulated and a real-world dataset. For the real-world dataset, we show how GEC can provide insight about the anomaly detection algorithms as well as the dataset.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4280644013",
    "type": "article"
  },
  {
    "title": "Interpretable Embedding and Visualization of Compressed Data",
    "doi": "https://doi.org/10.1145/3537901",
    "publication_date": "2022-05-18",
    "publication_year": 2022,
    "authors": "Nikolaos M. Freris; Ahmad Ajalloeian; Michail Vlachos",
    "corresponding_authors": "",
    "abstract": "Traditional embedding methodologies, also known as dimensionality reduction techniques, assume the availability of exact pairwise distances between the high-dimensional objects that will be embedded in a lower dimensionality. In this article, we propose an embedding that overcomes this limitation and can operate on pairwise distances that are represented as a range of lower and upper bounds. Such bounds are typically estimated when objects are compressed in a lossy manner, so our approach is highly applicable in the case of big compressed datasets. Our methodology can preserve multiple aspects of the original data relationships: distances, correlations, and object scores/ranks, whereas existing techniques typically preserve only distances. Comparative experiments with prevalent embedding methodologies (ISOMAP, t-SNE, MDS, UMAP) illustrate that our approach can provide fidelitous preservation of multiple object relationships, even in the presence of inexact distance information. Our visualization method is also easily interpretable.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4280650331",
    "type": "article"
  },
  {
    "title": "Generative Multi-Label Correlation Learning",
    "doi": "https://doi.org/10.1145/3538708",
    "publication_date": "2022-06-06",
    "publication_year": 2022,
    "authors": "Lichen Wang; Zhengming Ding; Kasey Lee; Seungju Han; Jae‐Joon Han; Changkyu Choi; Yun Fu",
    "corresponding_authors": "",
    "abstract": "In real-world applications, a single instance could have more than one label. To solve this task, multi-label learning methods emerged in recent years. It is a more challenging problem for many reasons, such as complex label correlation, long-tail label distribution, and data shortage. In general, overcoming these challenges and bettering learning performance could be achieved by utilizing more training samples and including label correlations. However, these solutions are expensive and inflexible. Large-scale, well-labeled datasets are difficult to obtain, and building label correlation maps requires task-specific semantic information as prior knowledge. To address these limitations, we propose a general and compact Multi-Label Correlation Learning (MUCO) framework. MUCO explicitly and effectively learns the latent label correlations by updating a label correlation tensor, which provides highly accurate and interpretable prediction results. In addition, a multi-label generative strategy is deployed to handle the long-tail label distribution challenge. It borrows the visual clues from limited samples and synthesizes more diverse samples. All networks in our model are optimized simultaneously. Extensive experiments illustrate the effectiveness and efficiency of MUCO. Ablation studies further prove the effectiveness of all the modules.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4281780077",
    "type": "article"
  },
  {
    "title": "Explainable Integration of Social Media Background in a Dynamic Neural Recommender",
    "doi": "https://doi.org/10.1145/3550279",
    "publication_date": "2022-07-26",
    "publication_year": 2022,
    "authors": "Yihong Zhang; Takahiro Hara",
    "corresponding_authors": "",
    "abstract": "Recommender systems nowadays are commonly deployed in e-commerce platforms to help customers making purchase decisions. Dynamic recommender considers not only static user-item interaction data, but the temporal information at the time of recommendation. Previous researches have suggested to incorporate social media as the temporal information in dynamic neural recommenders after transforming them into embeddings. While such an approach can potentially improve recommendation performance, the effectiveness is difficult to explain. In this article, we propose an explainable method to integrate social media in a dynamic neural recommender. Our method applies association rule mining, which can generate human-understandable behavior patterns from social media and e-commerce platforms. With real-world social media and e-commerce data, we show that the integration can improve accuracy by up to 14% while using the same data. Moreover, we can explain the positive cases by examining relevant association rules.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4287981199",
    "type": "article"
  },
  {
    "title": "Machine Learning-based Short-term Rainfall Prediction from Sky Data",
    "doi": "https://doi.org/10.1145/3502731",
    "publication_date": "2022-07-29",
    "publication_year": 2022,
    "authors": "Fu Jie Tey; Tin‐Yu Wu; Jiann-Liang Chen",
    "corresponding_authors": "",
    "abstract": "To predict rainfall, our proposed model architecture combines the Convolutional Neural Network (CNN), which uses the ResNet-152 pre-training model, with the Recurrent Neural Network (RNN), which uses the Long Short-term Memory Network (LSTM) layer, for model training. By encoding the cloud images through CNN, we extract the image feature vectors in the training process and train the vectors and meteorological data as the input of RNN. After training, the accuracy of the prediction model can reach up to 82%. The result has proven not only the outperformance of our proposed rainfall prediction method in terms of cost and prediction time, but also its accuracy and feasibility compared with general prediction methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4288690044",
    "type": "article"
  },
  {
    "title": "A Framework for Exploiting Local Information to Enhance Density Estimation of Data Streams",
    "doi": "https://doi.org/10.1145/2629618",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "Arnold P. Boedihardjo; Chang‐Tien Lu; Bingsheng Wang",
    "corresponding_authors": "",
    "abstract": "The Probability Density Function (PDF) is the fundamental data model for a variety of stream mining algorithms. Existing works apply the standard nonparametric Kernel Density Estimator (KDE) to approximate the PDF of data streams. As a result, the stream-based KDEs cannot accurately capture complex local density features. In this article, we propose the use of Local Region (LRs) to model local density information in univariate data streams. In-depth theoretical analyses are presented to justify the effectiveness of the LR-based KDE. Based on the analyses, we develop the General Local rEgion AlgorithM (GLEAM) to enhance the estimation quality of structurally complex univariate distributions for existing stream-based KDEs. A set of algorithmic optimizations is designed to improve the query throughput of GLEAM and to achieve its linear order computation. Additionally, a comprehensive suite of experiments was conducted to test the effectiveness and efficiency of GLEAM.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2034758081",
    "type": "article"
  },
  {
    "title": "Distributed Algorithms for Computing Very Large Thresholded Covariance Matrices",
    "doi": "https://doi.org/10.1145/2935750",
    "publication_date": "2016-11-19",
    "publication_year": 2016,
    "authors": "Zekai J. Gao; Chris Jermaine",
    "corresponding_authors": "",
    "abstract": "Computation of covariance matrices from observed data is an important problem, as such matrices are used in applications such as principal component analysis (PCA), linear discriminant analysis (LDA), and increasingly in the learning and application of probabilistic graphical models. However, computing an empirical covariance matrix is not always an easy problem. There are two key difficulties associated with computing such a matrix from a very high-dimensional dataset. The first problem is over-fitting. For a p -dimensional covariance matrix, there are p ( p − 1)/2 unique, off-diagonal entries in the empirical covariance matrix Ŝ for large p (say, p &gt; 10 5 ), the size n of the dataset is often much smaller than the number of covariances to compute. Over-fitting is a concern in any situation in which the number of parameters learned can greatly exceed the size of the dataset. Thus, there are strong theoretical reasons to expect that for high-dimensional data—even Gaussian data—the empirical covariance matrix is not a good estimate for the true covariance matrix underlying the generative process. The second problem is computational. Computing a covariance matrix takes O ( np 2 ) time. For large p (greater than 10,000) and n much greater than p , this is debilitating. In this article, we consider how both of these difficulties can be handled simultaneously. Specifically, a key regularization technique for high-dimensional covariance estimation is thresholding , in which the smallest or least significant entries in the covariance matrix are simply dropped and replaced with the value 0. This suggests an obvious way to address the computational difficulty as well: First, compute the identities of the K entries in the covariance matrix that are actually important in the sense that they will not be removed during thresholding, and then in a second step, compute the values of those entries. This can be done in O ( Kn ) time. If K ≪ p 2 and the identities of the important entries can be computed in reasonable time, then this is a big win. The key technical contribution of this article is the design and implementation of two different distributed algorithms for approximating the identities of the important entries quickly, using sampling. We have implemented these methods and tested them using an 800-core compute cluster. Experiments have been run using real datasets having millions of data points and up to 40, 000 dimensions. These experiments show that the proposed methods are both accurate and efficient.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2554071524",
    "type": "article"
  },
  {
    "title": "Efficient Version Space Algorithms for Human-in-the-Loop Model Development",
    "doi": "https://doi.org/10.1145/3637443",
    "publication_date": "2023-12-15",
    "publication_year": 2023,
    "authors": "Luciano Di Palma; Yanlei Diao; Anna Liu",
    "corresponding_authors": "",
    "abstract": "When active learning (AL) is applied to help users develop a model on a large dataset through interactively presenting data instances for labeling, existing AL techniques often suffer from two main drawbacks: First, to reach high accuracy they may require the user to label hundreds of data instances, which is an onerous task for the user. Second, retrieving the next instance to label from a large dataset can be time-consuming, making it incompatible with the interactive nature of the human exploration process. To address these issues, we introduce a novel version-space-based active learner for kernel classifiers, which possesses strong theoretical guarantees on performance and efficient implementation in time and space. In addition, by leveraging additional insights obtained in the user labeling process, we can factorize the version space to perform active learning in a set of subspaces, which further reduces the user labeling effort. Evaluation results show that our algorithms significantly outperform state-of-the-art version space strategies, as well as a recent factorization-aware algorithm, for model development over large datasets.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3207820302",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2630992",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Geoffrey I. Webb; Jilles Vreeken",
    "corresponding_authors": "",
    "abstract": "Nonnegative Matrix Factorization (NMF) has been one of the most widely used clustering techniques for exploratory data analysis. However, since each data point enters the objective function with squared residue error, a few outliers with large errors ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4236837939",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2582178",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Sybil accounts are fake identities created to unfairly increase the power or resources of a single malicious user. Researchers have long known about the existence of Sybil accounts in online communities such as file-sharing systems, but they have not ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4254689824",
    "type": "paratext"
  },
  {
    "title": "Three-way Preference Completion via Preference Graph",
    "doi": "https://doi.org/10.1145/3580368",
    "publication_date": "2023-01-30",
    "publication_year": 2023,
    "authors": "Lei Li; Zhiyuan Liu; Zan Zhang; Huanhuan Chen; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "With the personal partial rankings from agents over a subset of alternatives, the goal of preference completion is to infer the agent’s personalized preference over all alternatives including those the agent has not yet handled from uncertain preference of third parties. By combining the partial rankings of the target agent and the partial rankings from third parties to settle some disagreement with three-way preference completion, which includes a general strategy, an optimal strategy, and a pessimistic strategy, it forms the weighted preference graph. Technically, to settle the disagreement and obtain the completed preference of the target agent in the weighted preference graph, maximum likelihood estimation (MLE) under Mallows is proposed and validated theoretically by removing edges with the minimum weight in the weighted preference graph. However, it is not easy to locate the edges with the minimum weight efficiently in a big graph. Hence, an optimal MLE algorithm and three greedy MLE algorithms are proposed to process the MLE. Furthermore, these proposed algorithms are experimentally validated and compared with each other by both the synthetic dataset and the Flixter dataset.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4318477554",
    "type": "article"
  },
  {
    "title": "Modeling Regime Shifts in Multiple Time Series",
    "doi": "https://doi.org/10.1145/3592857",
    "publication_date": "2023-04-17",
    "publication_year": 2023,
    "authors": "Etienne Gael Tajeuna; Mohamed Bouguessa; Shengrui Wang",
    "corresponding_authors": "",
    "abstract": "We investigate the problem of discovering and modeling regime shifts in an ecosystem comprising multiple time series known as co-evolving time series. Regime shifts refer to the changing behaviors exhibited by series at different time intervals. Learning these changing behaviors is a key step toward time series forecasting. While advances have been made, existing methods suffer from one or more of the following shortcomings: (1) failure to take relationships between time series into consideration for discovering regimes in multiple time series; (2) lack of an effective approach that models time-dependent behaviors exhibited by series; (3) difficulties in handling data discontinuities which may be informative. Most of the existing methods are unable to handle all of these three issues in a unified framework. This, therefore, motivates our effort to devise a principled approach for modeling interactions and time-dependency in co-evolving time series. Specifically, we model an ecosystem of multiple time series by summarizing the heavy ensemble of time series into a lighter and more meaningful structure called a mapping grid . By using the mapping grid, our model first learns time series behavioral dependencies through a dynamic network representation, then learns the regime transition mechanism via a full time-dependent Cox regression model. The originality of our approach lies in modeling interactions between time series in regime identification and in modeling time-dependent regime transition probabilities, usually assumed to be static in existing work.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4366087542",
    "type": "article"
  },
  {
    "title": "Conditional Independence Test Based on Residual Similarity",
    "doi": "https://doi.org/10.1145/3593810",
    "publication_date": "2023-04-25",
    "publication_year": 2023,
    "authors": "Hao Zhang; Yewei Xia; Kun Zhang; Shuigeng Zhou; Jihong Guan",
    "corresponding_authors": "",
    "abstract": "Recently, many regression-based conditional independence (CI) test methods have been proposed to solve the problem of causal discovery. These methods provide alternatives to test CI of x,y given Z by first removing the information of the controlling set Z from x and y , and then testing the independence between the two residuals R x,Z and R y,Z . When the residuals are linearly uncorrelated, the independence test between them is nontrivial. With the ability to calculate inner product in high-dimensional space, kernel-based methods are usually used to achieve this goal, but they are considerably time-consuming. In this paper, we test the independence between two linear combinations under linear structural equation model. We show that the dependence between the two residuals can be captured by the difference between the similarity of R x,Z and R y,Z and that of R x,Z and R r ( R r is an independent copy of R y,Z ) in high-dimensional space. With this result, we provide a new way to test CI based on the similarity between residuals, which is called SCIT — the abbreviation of Similarity-based CI Testing. Furthermore, we develop two versions of the proposal, called Kernel-SCIT and Neural-SCIT, respectively. Kernel-SCIT calculates the similarity by using kernel functions, while Neural-SCIT approximates the upper bound of the similarity by using deep neural networks. In both algorithms, random permutation tests are performed to control Type I error rate. The proposed tests are evaluated on (conditional) independence test and causal discovery with both synthetic and real datasets. Experimental results show that Kernel-SCIT is simpler yet more efficient and effective than the typical existing kernel-based methods HSIC and KCIT in the cases of small sample size, and Neural-SCIT can significantly boost the performance of CI testing when sufficient samples are available. The source code is available at https://github.com/xyw5vplus1/SCIT .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4366985433",
    "type": "article"
  },
  {
    "title": "Community-Based Influence Maximization Using Network Embedding in Dynamic Heterogeneous Social Networks",
    "doi": "https://doi.org/10.1145/3594544",
    "publication_date": "2023-04-29",
    "publication_year": 2023,
    "authors": "Xi Qin; Cheng Zhong; Hai Xiang Lin",
    "corresponding_authors": "",
    "abstract": "Influence maximization (IM) is a very important issue in social network diffusion analysis. The topology of real social network is large-scale, dynamic, and heterogeneous. The heterogeneity, and continuous expansion and evolution of social network pose a challenge to find influential users. Existing IM algorithms usually assume that social networks are static or dynamic but homogeneous to simplify the complexity of the IM problem. We propose a community-based influence maximization algorithm using network embedding in dynamic heterogeneous social networks. We use DyHATR algorithm to obtain the propagation feature vectors of network nodes, and execute k -means cluster algorithm to transform the original network into a coarse granularity network (CGN). On CGN, we propose a community-based three-hop independent cascade model and construct the objective function of IM problem. We design a greedy heuristics algorithm to solve the IM problem with \\((1-\\frac{1}{e})-\\) approximation guarantee and use community structure to quickly identify seed users and estimate their influence value. Experimental results on real social networks demonstrated that compared with existing IM algorithms, our proposed algorithm had better comprehensive performance with respect to the influence value, more less execution time and memory consumption, and better scalability.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4367368181",
    "type": "article"
  },
  {
    "title": "An Information Theory Based Method for Quantifying the Predictability of Human Mobility",
    "doi": "https://doi.org/10.1145/3597500",
    "publication_date": "2023-05-23",
    "publication_year": 2023,
    "authors": "Zhiwen Yu; Minling Dang; Qi-long Wu; Liming Chen; Yujin Xie; Yu Wang; Bin Guo",
    "corresponding_authors": "",
    "abstract": "Research on human mobility drives the development of economy and society. How to predict when and where one will go accurately is one of the core research questions. Existing work is mainly concerned with performance of mobility prediction models. Since accuracy of predict models does not indicate whether or not one’s mobility is inherently easy to predict, there has not been a definite conclusion about that to what extent can our predictions of human mobility be accurate. To help solve this problem, we describe the formalized definition of predictability of human mobility, propose a model based on additive Markov chain to measure the probability of exploration, and further develop an information theory based method for quantifying the predictability considering exploration of human mobility. Then, we extend our method by using mutual information in order to measure the predictability considering external influencing factors, which has not been studied before. Experiments on simulation data and three real-world datasets show that our method yields a tighter upper bound on predictability of human mobility than previous work, and that predictability increased slightly when considering external factors such as weather and temperature.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4377824846",
    "type": "article"
  },
  {
    "title": "Open-World Graph Active Learning for Node Classification",
    "doi": "https://doi.org/10.1145/3607144",
    "publication_date": "2023-07-24",
    "publication_year": 2023,
    "authors": "Hui Xu; Liyao Xiang; Junjie Ou; Yuting Weng; Xinbing Wang; Chenghu Zhou",
    "corresponding_authors": "",
    "abstract": "The great power of Graph Neural Networks (GNNs) relies on a large number of labeled training data, but obtaining the labels can be costly in many cases. Graph Active Learning (GAL) is proposed to reduce such annotation costs, but the existing methods mainly focus on improving labeling efficiency with fixed classes, and are limited to handle the emergence of novel classes. We term the problem as Open-World Graph Active Learning (OWGAL) and propose a framework of the same name. The key is to recognize novel-class as well as informative nodes in a unified framework. Instead of a fully connected neural network classifier, OWGAL employs prototype learning and label propagation to assign high uncertainty scores to the targeted nodes in the representation and topology space, respectively. Weighted sampling further suppresses the impact of unimportant classes by weighing both the node and class importance. Experimental results on four large-scale datasets demonstrate that our framework achieves a substantial improvement of 5.97% to 16.57% on Macro-F1 over state-of-the-art methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385199400",
    "type": "article"
  },
  {
    "title": "DeepCPR: Deep Path Reasoning Using Sequence of User-Preferred Attributes for Conversational Recommendation",
    "doi": "https://doi.org/10.1145/3610775",
    "publication_date": "2023-07-25",
    "publication_year": 2023,
    "authors": "Huiting Liu; Yu Zhang; Peipei Li; Cheng Qian; Peng Zhao; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Conversational recommender systems (CRS) have garnered significant attention in academia and industry because of their ability to capture user preferences via system questions and user responses. Typically, in a CRS, reinforcement learning (RL) is utilized to determine the optimal timing for requesting attribute information or suggesting items. However, existing methods consider user-preferred attributes independently and ignore that attributes may be of different importance to the same user, in the attribute and item selection phases, which limits the accuracy and interpretability of CRS. Inspired by this, we propose deep conversational path reasoning (DeepCPR), which involves constructing a reasoning path on a graph with a series of user-favored attributes. It utilizes the attention mechanism to thoroughly examine the connections between these attributes and provide improved explanations for which attributes to inquire about or which items to recommend. In DeepCPR, two deep-learning-based modules are proposed to realize attribute and item selection. In the first module, the sequence of attributes confirmed by the user in conversation is encoded with a gated graph neural network to obtain the user’s long-term preference using a self-attention mechanism for the selection of candidate attributes. In the second module, a self-attention approach with more appropriate strategies is developed to dynamically select candidate items. In addition, to achieve fine-grained user preference modeling, a recurrent neural network is employed to aggregate the sequence of attributes that interact with the users. Numerous experimental evaluations conducted on four real CRS datasets show that the proposed method significantly outperforms existing advanced methods in terms of conversational recommendations.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385239589",
    "type": "article"
  },
  {
    "title": "Semi-Supervised Heterogeneous Graph Learning with Multi-Level Data Augmentation",
    "doi": "https://doi.org/10.1145/3608953",
    "publication_date": "2023-08-15",
    "publication_year": 2023,
    "authors": "Ying Chen; Siwei Qiang; Mingming Ha; Xiaolei Liu; Shaoshuai Li; Jiabi Tong; Lingfeng Yuan; Xiaobo Guo; Zhenfeng Zhu",
    "corresponding_authors": "",
    "abstract": "In recent years, semi-supervised graph learning with data augmentation (DA) has been the most commonly used and best-performing method to improve model robustness in sparse scenarios with few labeled samples. However, most existing DA methods are based on the homogeneous graph, but none are specific for the heterogeneous graph. Differing from the homogeneous graph, DA in the heterogeneous graph faces greater challenges: heterogeneity of information requires DA strategies to effectively handle heterogeneous relations, which considers the information contribution of different types of neighbors and edges to the target nodes. Furthermore, over-squashing of information is caused by the negative curvature formed by the non-uniformity distribution and the strong clustering in a complex graph. To address these challenges, this article presents a novel method named HG-MDA (Semi-Supervised Heterogeneous Graph Learning with Multi-Level Data Augmentation). For the problem of heterogeneity of information in DA, node and topology augmentation strategies are proposed for the characteristics of the heterogeneous graph. Additionally, meta-relation-based attention is applied as one of the indexes for selecting augmented nodes and edges. For the problem of over-squashing of information, triangle-based edge adding and removing are designed to alleviate the negative curvature and bring the gain of topology. Finally, the loss function consists of the cross-entropy loss for labeled data and the consistency regularization for unlabeled data. To effectively fuse the prediction results of various DA strategies, sharpening is used. Existing experiments on public datasets (i.e., ACM, DBLP, and OGB) and the industry dataset MB show that HG-MDA outperforms current SOTA models. Additionally, HG-MDA is applied to user identification in internet finance scenarios, helping the business to add 30% key users, and increase loans and balances by 3.6%, 11.1%, and 9.8%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385838249",
    "type": "article"
  },
  {
    "title": "History-enhanced and Uncertainty-aware Trajectory Recovery via Attentive Neural Network",
    "doi": "https://doi.org/10.1145/3615660",
    "publication_date": "2023-09-05",
    "publication_year": 2023,
    "authors": "Xia Tong; Yong Li; Yunhan Qi; Jie Feng; Fengli Xu; Funing Sun; Diansheng Guo; Depeng Jin",
    "corresponding_authors": "",
    "abstract": "A considerable amount of mobility data has been accumulated due to the proliferation of location-based services. Nevertheless, compared with mobility data from transportation systems like the GPS module in taxis, this kind of data is commonly sparse in terms of individual trajectories in the sense that users do not access mobile services and contribute their data all the time. Consequently, the sparsity inevitably weakens the practical value of the data even if it has a high user penetration rate. To solve this problem, we propose a novel attentional neural network-based model, named AttnMove, to densify individual trajectories by recovering unobserved locations at a fine-grained spatial-temporal resolution. To tackle the challenges posed by sparsity, we design various intra- and inter- trajectory attention mechanisms to better model the mobility regularity of users and fully exploit the periodical pattern from long-term history. In addition, to guarantee the robustness of the generated trajectories to avoid harming downstream applications, we also exploit the Bayesian approximate neural network to estimate the uncertainty of each imputation. As a result, locations generated by the model with high uncertainty will be excluded. We evaluate our model on two real-world datasets, and extensive results demonstrate the performance gain compared with the state-of-the-art methods. In-depth analyses of each design of our model have been conducted to understand their contribution. We also show that, by providing high-quality mobility data, our model can benefit a variety of mobility-oriented downstream applications.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386440668",
    "type": "article"
  },
  {
    "title": "From Asset Flow to Status, Action, and Intention Discovery: Early Malice Detection in Cryptocurrency",
    "doi": "https://doi.org/10.1145/3626102",
    "publication_date": "2023-09-28",
    "publication_year": 2023,
    "authors": "Ling Cheng; Feida Zhu; Yong Wang; Ruicheng Liang; Huiwen Liu",
    "corresponding_authors": "",
    "abstract": "Cryptocurrency has been subject to illicit activities probably more often than traditional financial assets due to the pseudo-anonymous nature of its transacting entities. An ideal detection model is expected to achieve all three critical properties of early detection, good interpretability, and versatility for various illicit activities. However, existing solutions cannot meet all these requirements, as most of them heavily rely on deep learning without interpretability and are only available for retrospective analysis of a specific illicit type. To tackle all these challenges, we propose Intention Monitor for early malice detection in Bitcoin, where the on-chain record data for a certain address are much scarcer than other cryptocurrency platforms. We first define asset transfer paths with the Decision Tree based feature Selection and Complement to build different feature sets for different malice types. Then, the Status/Action Proposal module and the Intention-VAE module generate the status, action, intent-snippet, and hidden intent-snippet embedding. With all these modules, our model is highly interpretable and can detect various illegal activities. Moreover, well-designed loss functions further enhance the prediction speed and the model’s interpretability. Extensive experiments on three real-world datasets demonstrate that our proposed algorithm outperforms the state-of-the-art methods. Furthermore, additional case studies justify that our model not only explains existing illicit patterns but also can find new suspicious characters.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387130600",
    "type": "article"
  },
  {
    "title": "Learning Entangled Interactions of Complex Causality via Self-Paced Contrastive Learning",
    "doi": "https://doi.org/10.1145/3632406",
    "publication_date": "2023-11-09",
    "publication_year": 2023,
    "authors": "Yunji Liang; Lei Liu; Luwen Huangfu; Sagar Samtani; Zhiwen Yu; Daniel Zeng",
    "corresponding_authors": "",
    "abstract": "Learning causality from large-scale text corpora is an important task with numerous applications—for example, in finance, biology, medicine, and scientific discovery. Prior studies have focused mainly on simple causality, which only includes one cause-effect pair. However, causality is notoriously difficult to understand and analyze because of multiple cause spans and their entangled interactions. To detect complex causality, we propose a self-paced contrastive learning model, namely N2NCause, to learn entangled interactions between multiple spans. Specifically, N2NCause introduces data enhancement operations to convert implicit expressions into explicit expressions with the most rational causal connectives for the synthesis of positive samples and to invert the directed connection between a cause-effect pair for the synthesis of negative samples. To learn the semantic dependency and causal direction of positive and negative samples, self-paced contrastive learning is proposed to learn the entangled interactions among spans, including the interaction direction and interaction field. We evaluated the performance of N2NCause in three cause-effect detection tasks. The experimental results show that, with the least data annotation efforts, N2NCause demonstrates competitive performance in detecting simple cause-effect relations, and it is superior to existing solutions for the detection of complex causality.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388539099",
    "type": "article"
  },
  {
    "title": "Sparse Grid Imputation Using Unpaired Imprecise Auxiliary Data: Theory and Application to PM2.5 Estimation",
    "doi": "https://doi.org/10.1145/3634751",
    "publication_date": "2023-11-27",
    "publication_year": 2023,
    "authors": "Ming-Chuan Yang; Guo-Wei Wong; Meng Chang Chen",
    "corresponding_authors": "",
    "abstract": "Sparse grid imputation (SGI) is a challenging problem, as its goal is to infer the values of the entire grid from a limited number of cells with values. Traditionally, the problem is solved using regression methods such as KNN and kriging, whereas in the real world, there is often extra information—usually imprecise—that can aid inference and yield better performance. In the SGI problem, in addition to the limited number of fixed grid cells with precise target domain values, there are contextual data and imprecise observations over the whole grid. To solve this problem, we propose a distribution estimation theory for the whole grid and realize the theory via the composition architecture of the Target-Embedding and the Contextual CycleGAN trained with contextual information and imprecise observations. Contextual CycleGAN is structured as two generator–discriminator pairs and uses different types of contextual loss to guide the training. We consider the real-world problem of fine-grained PM2.5 inference with realistic settings: a few (less than 1%) grid cells with precise PM2.5 data and all grid cells with contextual information concerning weather and imprecise observations from satellites and microsensors. The task is to infer reasonable values for all grid cells. As there is no ground truth for empty cells, out-of-sample mean squared error and Jensen–Shannon divergence measurements are used in the empirical study. The results show that Contextual CycleGAN supports the proposed theory and outperforms the methods used for comparison.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389046780",
    "type": "article"
  },
  {
    "title": "Hi-PART: Going Beyond Graph Pooling with Hierarchical Partition Tree for Graph-Level Representation Learning",
    "doi": "https://doi.org/10.1145/3636429",
    "publication_date": "2023-12-14",
    "publication_year": 2023,
    "authors": "Yuyang Ren; Haonan Zhang; Luoyi Fu; Shiyu Liang; Lei Zhou; Xinbing Wang; Xinde Cao; Fei Long; Chenghu Zhou",
    "corresponding_authors": "",
    "abstract": "Graph pooling refers to the operation that maps a set of node representations into a compact form for graph-level representation learning. However, existing graph pooling methods are limited by the power of the Weisfeiler–Lehman (WL) test in the performance of graph discrimination. In addition, these methods often suffer from hard adaptability to hyper-parameters and training instability. To address these issues, we propose Hi-PART, a simple yet effective graph neural network (GNN) framework with Hi erarchical Par tition T ree (HPT). In HPT, each layer is a partition of the graph with different levels of granularities that are going toward a finer grain from top to bottom. Such an exquisite structure allows us to quantify the graph structure information contained in HPT with the aid of structural information theory. Algorithmically, by employing GNNs to summarize node features into the graph feature based on HPT’s hierarchical structure, Hi-PART is able to adequately leverage the graph structure information and provably goes beyond the power of the WL test. Due to the separation of HPT optimization from graph representation learning, Hi-PART involves the height of HPT as the only extra hyper-parameter and enjoys higher training stability. Empirical results on graph classification benchmarks validate the superior expressive power and generalization ability of Hi-PART compared with state-of-the-art graph pooling approaches.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389764663",
    "type": "article"
  },
  {
    "title": "Supervised Clustering of Persian Handwritten Images Using Regularization and Dimension Reduction Methods",
    "doi": "https://doi.org/10.1145/3638060",
    "publication_date": "2023-12-20",
    "publication_year": 2023,
    "authors": "Sajedeh Moradnia; Mousa Golalizadeh",
    "corresponding_authors": "",
    "abstract": "Clustering, as a fundamental exploratory data technique, not only is used to discover patterns and structures in complex datasets but also is utilized to group variables in high-dimensional data analysis. Dimension reduction through clustering helps identify important variables and reduce data dimensions without losing significant information. High-dimensional image datasets, such as Persian handwritten images, have numerous pixels, making statistical inference difficult. Such high-dimensionality property pose challenges for analysis and processing, requiring specialized techniques like clustering to extract information. Incorporating response variable information enhances clustering analysis, transforming it into a supervised method. This article evaluates a supervised clustering approach using Ridge and Lasso penalties, comparing them in analyzing a real dataset while identifying important variables. We demonstrate that despite choosing a small number of variables as important variables, Lasso penalty performs relatively well in predicting the labels of new observations for this multi-class dataset.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390003022",
    "type": "article"
  },
  {
    "title": "HITS-based Propagation Paradigm for Graph Neural Networks",
    "doi": "https://doi.org/10.1145/3638779",
    "publication_date": "2023-12-30",
    "publication_year": 2023,
    "authors": "Mehak Khan; Gustavo Borges Moreno e Mello; Laurence Habib; Paal Engelstad; Anis Yazidi",
    "corresponding_authors": "",
    "abstract": "In this article, we present a new propagation paradigm based on the principle of Hyperlink-Induced Topic Search (HITS) algorithm. The HITS algorithm utilizes the concept of a “self-reinforcing” relationship of authority-hub. Using HITS, the centrality of nodes is determined via repeated updates of authority-hub scores that converge to a stationary distribution. Unlike PageRank-based propagation methods, which rely solely on the idea of authorities (in-links), HITS considers the relevance of both authorities (in-links) and hubs (out-links), thereby allowing for a more informative graph learning process. To segregate node prediction and propagation, we use a Multilayer Perceptron in combination with a HITS-based propagation approach and propose two models: HITS-GNN and HITS-GNN+. We provided additional validation of our models’ efficacy by performing an ablation study to assess the performance of authority-hub in independent models. Moreover, the effect of the main hyper-parameters and normalization is also analyzed to uncover how these techniques influence the performance of our models. Extensive experimental results indicate that the proposed approach significantly improves baseline methods on the graph (citation network) benchmark datasets by a decent margin for semi-supervised node classification, which can aid in predicting the categories (labels) of scientific articles not exclusively based on their content but also based on the type of articles they cite.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390444073",
    "type": "article"
  },
  {
    "title": "A Model for Information Growth in Collective Wisdom Processes",
    "doi": "https://doi.org/10.1145/2297456.2297458",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Sanmay Das; Malik Magdon‐Ismail",
    "corresponding_authors": "",
    "abstract": "Collaborative media such as wikis have become enormously successful venues for information creation. Articles accrue information through the asynchronous editing of users who arrive both seeking information and possibly able to contribute information. Most articles stabilize to high-quality, trusted sources of information representing the collective wisdom of all the users who edited the article. We propose a model for information growth which relies on two main observations: (i) as an article’s quality improves, it attracts visitors at a faster rate (a rich-get-richer phenomenon); and, simultaneously, (ii) the chances that a new visitor will improve the article drops (there is only so much that can be said about a particular topic). Our model is able to reproduce many features of the edit dynamics observed on Wikipedia; in particular, it captures the observed rise in the edit rate, followed by 1/ t decay. Despite differences in the media, we also document similar features in the comment rates for a segment of the LiveJournal blogosphere.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1972581688",
    "type": "article"
  },
  {
    "title": "Guest Editorial for Special Issue KDD’10",
    "doi": "https://doi.org/10.1145/2086737.2086738",
    "publication_date": "2012-01-31",
    "publication_year": 2012,
    "authors": "Charles Elkan; Yehuda Koren",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2051969758",
    "type": "editorial"
  },
  {
    "title": "Parallel Field Ranking",
    "doi": "https://doi.org/10.1145/2513092.2513096",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Ji Ming; Binbin Lin; Xiaofei He; Deng Cai; Jiawei Han",
    "corresponding_authors": "",
    "abstract": "Recently, ranking data with respect to the intrinsic geometric structure (manifold ranking) has received considerable attentions, with encouraging performance in many applications in pattern recognition, information retrieval and recommendation systems. Most of the existing manifold ranking methods focus on learning a ranking function that varies smoothly along the data manifold. However, beyond smoothness, a desirable ranking function should vary monotonically along the geodesics of the data manifold, such that the ranking order along the geodesics is preserved. In this article, we aim to learn a ranking function that varies linearly and therefore monotonically along the geodesics of the data manifold. Recent theoretical work shows that the gradient field of a linear function on the manifold has to be a parallel vector field. Therefore, we propose a novel ranking algorithm on the data manifolds, called Parallel Field Ranking. Specifically, we try to learn a ranking function and a vector field simultaneously. We require the vector field to be close to the gradient field of the ranking function, and the vector field to be as parallel as possible. Moreover, we require the value of the ranking function at the query point to be the highest, and then decrease linearly along the manifold. Experimental results on both synthetic data and real data demonstrate the effectiveness of our proposed algorithm.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2735152359",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2435209",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This work deals with the problem of classifying uncertain data. With this aim we introduce the Uncertain Nearest Neighbor (UNN) rule, which represents the generalization of the deterministic nearest neighbor rule to the case in which uncertain objects ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4230267798",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2086737",
    "publication_date": "2012-01-31",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose an online topic model for sequentially analyzing the time evolution of topics in document collections. Topics naturally evolve with multiple timescales. For example, some words may be used consistently over one hundred years, while other ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4232583492",
    "type": "paratext"
  },
  {
    "title": "Optimal Algebraic Breadth-First Search for Sparse Graphs",
    "doi": "https://doi.org/10.1145/3446216",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Paul Burkhardt",
    "corresponding_authors": "Paul Burkhardt",
    "abstract": "There has been a rise in the popularity of algebraic methods for graph algorithms given the development of the GraphBLAS library and other sparse matrix methods. An exemplar for these approaches is Breadth-First Search (BFS). The algebraic BFS algorithm is simply a recurrence of matrix-vector multiplications with the $n \\times n$ adjacency matrix, but the many redundant operations over nonzeros ultimately lead to suboptimal performance. Therefore an optimal algebraic BFS should be of keen interest especially if it is easily integrated with existing matrix methods. Current methods, notably in the GraphBLAS, use a Sparse Matrix masked-Sparse Vector (SpMmSpV) multiplication in which the input vector is kept in a sparse representation in each step of the BFS, and nonzeros in the vector are masked in subsequent steps. This has been an area of recent research in GraphBLAS and other libraries. While in theory these masking methods are asymptotically optimal on sparse graphs, many add work that leads to suboptimal runtime. We give a new optimal, algebraic BFS for sparse graphs, thus closing a gap in the literature. Our method multiplies progressively smaller submatrices of the adjacency matrix at each step. Let $n$ and $m$ refer to the number of vertices and edges, respectively. On a sparse graph, our method takes $O(n)$ algebraic operations as opposed to $O(m)$ operations needed by theoretically optimal sparse matrix approaches. Thus for sparse graphs it matches the bounds of the best-known sequential algorithm and on a Parallel Random Access Machine (PRAM) it is work-optimal. Our result holds for both directed and undirected graphs. Compared to a leading GraphBLAS library our method achieves up to 24x faster sequential time and for parallel computation it can be 17x faster on large graphs and 12x faster on large-diameter graphs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2948896230",
    "type": "article"
  },
  {
    "title": "Density Guarantee on Finding Multiple Subgraphs and Subtensors",
    "doi": "https://doi.org/10.1145/3446668",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Quang-Huy Duong; Heri Ramampiaro; Kjetil Nørvåg; Thu-Lan Dam",
    "corresponding_authors": "",
    "abstract": "Dense subregion (subgraph &amp; subtensor) detection is a well-studied area, with a wide range of applications, and numerous efficient approaches and algorithms have been proposed. Approximation approaches are commonly used for detecting dense subregions due to the complexity of the exact methods. Existing algorithms are generally efficient for dense subtensor and subgraph detection, and can perform well in many applications. However, most of the existing works utilize the state-or-the-art greedy 2-approximation algorithm to capably provide solutions with a loose theoretical density guarantee. The main drawback of most of these algorithms is that they can estimate only one subtensor, or subgraph, at a time, with a low guarantee on its density. While some methods can, on the other hand, estimate multiple subtensors, they can give a guarantee on the density with respect to the input tensor for the first estimated subsensor only. We address these drawbacks by providing both theoretical and practical solution for estimating multiple dense subtensors in tensor data and giving a higher lower bound of the density. In particular, we guarantee and prove a higher bound of the lower-bound density of the estimated subgraph and subtensors. We also propose a novel approach to show that there are multiple dense subtensors with a guarantee on its density that is greater than the lower bound used in the state-of-the-art algorithms. We evaluate our approach with extensive experiments on several real-world datasets, which demonstrates its efficiency and feasibility.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3160210882",
    "type": "article"
  },
  {
    "title": "Tiered Sampling",
    "doi": "https://doi.org/10.1145/3441299",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Lorenzo De Stefani; Erisa Terolli; Eli Upfal",
    "corresponding_authors": "",
    "abstract": "We introduce Tiered Sampling , a novel technique for estimating the count of sparse motifs in massive graphs whose edges are observed in a stream. Our technique requires only a single pass on the data and uses a memory of fixed size M , which can be magnitudes smaller than the number of edges. Our methods address the challenging task of counting sparse motifs—sub-graph patterns—that have a low probability of appearing in a sample of M edges in the graph, which is the maximum amount of data available to the algorithms in each step. To obtain an unbiased and low variance estimate of the count, we partition the available memory into tiers (layers) of reservoir samples. While the base layer is a standard reservoir sample of edges, other layers are reservoir samples of sub-structures of the desired motif. By storing more frequent sub-structures of the motif, we increase the probability of detecting an occurrence of the sparse motif we are counting, thus decreasing the variance and error of the estimate. While we focus on the designing and analysis of algorithms for counting 4-cliques, we present a method which allows generalizing Tiered Sampling to obtain high-quality estimates for the number of occurrence of any sub-graph of interest, while reducing the analysis effort due to specific properties of the pattern of interest. We present a complete analytical analysis and extensive experimental evaluation of our proposed method using both synthetic and real-world data. Our results demonstrate the advantage of our method in obtaining high-quality approximations for the number of 4 and 5-cliques for large graphs using a very limited amount of memory, significantly outperforming the single edge sample approach for counting sparse motifs in large scale graphs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3160724071",
    "type": "article"
  },
  {
    "title": "TKDD Special Issue SIGKDD 2009",
    "doi": "https://doi.org/10.1145/1857947.1857948",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Wei Wang",
    "corresponding_authors": "Wei Wang",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1987774268",
    "type": "article"
  },
  {
    "title": "Towards Automatic Construction of Multi-Network Models for Heterogeneous Multi-Task Learning",
    "doi": "https://doi.org/10.1145/3434748",
    "publication_date": "2021-03-05",
    "publication_year": 2021,
    "authors": "Unai Garciarena; Alexander Mendiburu; Roberto Santana",
    "corresponding_authors": "",
    "abstract": "Multi-task learning, as it is understood nowadays, consists of using one single model to carry out several similar tasks. From classifying hand-written characters of different alphabets to figuring out how to play several Atari games using reinforcement learning, multi-task models have been able to widen their performance range across different tasks, although these tasks are usually of a similar nature. In this work, we attempt to expand this range even further, by including heterogeneous tasks in a single learning procedure. To do so, we firstly formally define a multi-network model, identifying the necessary components and characteristics to allow different adaptations of said model depending on the tasks it is required to fulfill. Secondly, employing the formal definition as a starting point, we develop an illustrative model example consisting of three different tasks (classification, regression, and data sampling). The performance of this illustrative model is then analyzed, showing its capabilities. Motivated by the results of the analysis, we enumerate a set of open challenges and future research lines over which the full potential of the proposed model definition can be exploited.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3133616518",
    "type": "article"
  },
  {
    "title": "Search Efficient Binary Network Embedding",
    "doi": "https://doi.org/10.1145/3436892",
    "publication_date": "2021-05-08",
    "publication_year": 2021,
    "authors": "Daokun Zhang; Jie Yin; Xingquan Zhu; Chengqi Zhang",
    "corresponding_authors": "",
    "abstract": "Traditional network embedding primarily focuses on learning a continuous vector representation for each node, preserving network structure and/or node content information, such that off-the-shelf machine learning algorithms can be easily applied to the vector-format node representations for network analysis. However, the learned continuous vector representations are inefficient for large-scale similarity search, which often involves finding nearest neighbors measured by distance or similarity in a continuous vector space. In this article, we propose a search efficient binary network embedding algorithm called BinaryNE to learn a binary code for each node, by simultaneously modeling node context relations and node attribute relations through a three-layer neural network. BinaryNE learns binary node representations using a stochastic gradient descent-based online learning algorithm. The learned binary encoding not only reduces memory usage to represent each node, but also allows fast bit-wise comparisons to support faster node similarity search than using Euclidean or other distance measures. Extensive experiments and comparisons demonstrate that BinaryNE not only delivers more than 25 times faster search speed, but also provides comparable or better search quality than traditional continuous vector based network embedding methods. The binary codes learned by BinaryNE also render competitive performance on node classification and node clustering tasks. The source code of the BinaryNE algorithm is available at https://github.com/daokunzhang/BinaryNE.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3161383437",
    "type": "article"
  },
  {
    "title": "Self-Adaptive Skeleton Approaches to Detect Self-Organized Coalitions From Brain Functional Networks Through Probabilistic Mixture Models",
    "doi": "https://doi.org/10.1145/3447570",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Kai Liu; Hongbo Liu; Tomás Ward; Hua Wang; Yu Yang; Bo Zhang; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Detecting self-organized coalitions from functional networks is one of the most important ways to uncover functional mechanisms in the brain. Determining these raises well-known technical challenges in terms of scale imbalance, outliers and hard-examples. In this article, we propose a novel self-adaptive skeleton approach to detect coalitions through an approximation method based on probabilistic mixture models. The nodes in the networks are characterized in terms of robust k -order complete subgraphs ( k -clique ) as essential substructures. The k -clique enumeration algorithm quickly enumerates all k -cliques in a parallel manner for a given network. Then, the cliques, from max -clique down to min -clique, of each order k , are hierarchically embedded into a probabilistic mixture model. They are self-adapted to the corresponding structure density of coalitions in the brain functional networks through different order k . All the cliques are merged and evolved into robust skeletons to sustain each unbalanced coalition by eliminating outliers and separating overlaps. We call this the k -CLIque Merging Evolution (CLIME) algorithm. The experimental results illustrate that the proposed approaches are robust to density variation and coalition mixture and can enable the effective detection of coalitions from real brain functional networks. There exist potential cognitive functional relations between the regions of interest in the coalitions revealed by our methods, which suggests the approach can be usefully applied in neuroscientific studies.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3163763606",
    "type": "article"
  },
  {
    "title": "NeuSE: A Neural Snapshot Ensemble Method for Collaborative Filtering",
    "doi": "https://doi.org/10.1145/3450526",
    "publication_date": "2021-05-19",
    "publication_year": 2021,
    "authors": "Dongsheng Li; Haodong Liu; Chao Chen; Yingying Zhao; Stephen M. Chu; Bo Yang",
    "corresponding_authors": "",
    "abstract": "In collaborative filtering (CF) algorithms, the optimal models are usually learned by globally minimizing the empirical risks averaged over all the observed data. However, the global models are often obtained via a performance tradeoff among users/items, i.e., not all users/items are perfectly fitted by the global models due to the hard non-convex optimization problems in CF algorithms. Ensemble learning can address this issue by learning multiple diverse models but usually suffer from efficiency issue on large datasets or complex algorithms. In this article, we keep the intermediate models obtained during global model learning as the snapshot models, and then adaptively combine the snapshot models for individual user-item pairs using a memory network-based method. Empirical studies on three real-world datasets show that the proposed method can extensively and significantly improve the accuracy (up to 15.9% relatively) when applied to a variety of existing collaborative filtering methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3163797105",
    "type": "article"
  },
  {
    "title": "Context-Aware Semantic Annotation of Mobility Records",
    "doi": "https://doi.org/10.1145/3477048",
    "publication_date": "2021-10-22",
    "publication_year": 2021,
    "authors": "Huandong Wang; Yong Li; Junjie Lin; Hancheng Cao; Depeng Jin",
    "corresponding_authors": "",
    "abstract": "The wide adoption of mobile devices has provided us with a massive volume of human mobility records. However, a large portion of these records is unlabeled, i.e., only have GPS coordinates without semantic information (e.g., Point of Interest (POI)). To make those unlabeled records associate with more information for further applications, it is of great importance to annotate the original data with POIs information based on the external context. Nevertheless, semantic annotation of mobility records is challenging due to three aspects: the complex relationship among multiple domains of context, the sparsity of mobility records, and difficulties in balancing personal preference and crowd preference. To address these challenges, we propose CAP, a context-aware personalized semantic annotation model, where we use a Bayesian mixture model to model the complex relationship among five domains of context—location, time, POI category, personal preference, and crowd preference. We evaluate our model on two real-world datasets, and demonstrate that our proposed method significantly outperforms the state-of-the-art algorithms by over 11.8%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3208368897",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1409620",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4240642235",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1376815",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Attribute data and relationship data are two principal types of data, representing the intrinsic and extrinsic properties of entities. While attribute data have been the main source of data for cluster analysis, relationship data such as social networks ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4245826111",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1267066",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Prediction errors from a linear model tend to be larger when extrapolation is involved, particularly when the model is wrong. This article considers the problem of extrapolation and interpolation errors when a linear model tree is used for prediction. ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4253262926",
    "type": "paratext"
  },
  {
    "title": "Characterizing Directed and Undirected Networks via Multidimensional Walks with Jumps",
    "doi": "https://doi.org/10.1145/3299877",
    "publication_date": "2019-01-23",
    "publication_year": 2019,
    "authors": "Fabrício Murai; Bruno Ribeiro; Don Towlsey; Pinghui Wang",
    "corresponding_authors": "",
    "abstract": "Estimating distributions of node characteristics (labels) such as number of connections or citizenship of users in a social network via edge and node sampling is a vital part of the study of complex networks. Due to its low cost, sampling via a random walk (RW) has been proposed as an attractive solution to this task. Most RW methods assume either that the network is undirected or that walkers can traverse edges regardless of their direction. Some RW methods have been designed for directed networks where edges coming into a node are not directly observable. In this work, we propose Directed Unbiased Frontier Sampling (DUFS), a sampling method based on a large number of coordinated walkers, each starting from a node chosen uniformly at random. It applies to directed networks with invisible incoming edges because it constructs, in real time, an undirected graph consistent with the walkers trajectories, and its use of random jumps to prevent walkers from being trapped. DUFS generalizes previous RW methods and is suited for undirected networks and to directed networks regardless of in-edge visibility. We also propose an improved estimator of node label distribution that combines information from initial walker locations with subsequent RW observations. We evaluate DUFS, compare it to other RW methods, investigate the impact of its parameters on estimation accuracy and provide practical guidelines for choosing them. In estimating out-degree distributions, DUFS yields significantly better estimates of the head of the distribution than other methods, while matching or exceeding estimation accuracy of the tail. Last, we show that DUFS outperforms uniform sampling when estimating distributions of node labels of the top 10% largest degree nodes, even when sampling a node uniformly has the same cost as RW steps.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2604614786",
    "type": "article"
  },
  {
    "title": "Hybrid Crowd-Machine Wrapper Inference",
    "doi": "https://doi.org/10.1145/3344720",
    "publication_date": "2019-09-24",
    "publication_year": 2019,
    "authors": "Valter Crescenzi; Paolo Merialdo; Disheng Qiu",
    "corresponding_authors": "",
    "abstract": "Wrapper inference deals in generating programs to extract data from Web pages. Several supervised and unsupervised wrapper inference approaches have been proposed in the literature. On one hand, unsupervised approaches produce erratic wrappers: whenever the sources do not satisfy underlying assumptions of the inference algorithm, their accuracy is compromised. On the other hand, supervised approaches produce accurate wrappers, but since they need training data, their scalability is limited. The recent advent of crowdsourcing platforms has opened new opportunities for supervised approaches, as they make possible the production of large amounts of training data with the support of workers recruited online. Nevertheless, involving human workers has monetary costs. We present an original hybrid crowd-machine wrapper inference system that offers the benefits of both approaches exploiting the cooperation of crowd workers and unsupervised algorithms. Based on a principled probabilistic model that estimates the quality of wrappers, humans workers are recruited only when unsupervised wrapper induction algorithms are not able to produce sufficiently accurate solutions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2977125667",
    "type": "article"
  },
  {
    "title": "Mining Rank Data",
    "doi": "https://doi.org/10.1145/3363572",
    "publication_date": "2019-11-11",
    "publication_year": 2019,
    "authors": "Sascha Henzgen; Eyke Hüllermeier",
    "corresponding_authors": "",
    "abstract": "The problem of frequent pattern mining has been studied quite extensively for various types of data, including sets, sequences, and graphs. Somewhat surprisingly, another important type of data, namely rank data, has received very little attention in data mining so far. In this article, we therefore address the problem of mining rank data, that is, data in the form of rankings (total orders) of an underlying set of items. More specifically, two types of patterns are considered, namely frequent rankings and dependencies between such rankings in the form of association rules. Algorithms for mining frequent rankings and frequent closed rankings are proposed and tested experimentally, using both synthetic and real data.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2984554602",
    "type": "article"
  },
  {
    "title": "発見的符号化しきい値処理によるロバスト回帰とその適応推定変動【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Zhang Xuchao; Lei Shuo; Zhao Liang; P Boedihardjo Arnold; Lu Chang-Tien",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3163158638",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3178546",
    "publication_date": "2018-04-27",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Reputation systems have become an indispensable component of modern E-commerce systems, as they help buyers make informed decisions in choosing trustworthy sellers. To attract buyers and increase the transaction volume, sellers need to earn reasonably ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4247457900",
    "type": "paratext"
  },
  {
    "title": "diGRASS: <u>Di</u> rected <u>Gra</u> ph <u>S</u> pectral <u>S</u> parsification via Spectrum-Preserving Symmetrization",
    "doi": "https://doi.org/10.1145/3639568",
    "publication_date": "2024-01-04",
    "publication_year": 2024,
    "authors": "Ying Zhang; Zhiqiang Zhao; Zhuo Feng",
    "corresponding_authors": "",
    "abstract": "Recent spectral graph sparsification research aims to construct ultra-sparse subgraphs for preserving the original graph spectral (structural) properties, such as the first few Laplacian eigenvalues and eigenvectors, which has led to the development of a variety of nearly-linear time numerical and graph algorithms. However, there is very limited progress for spectral sparsification of directed graphs. In this work, we prove the existence of nearly-linear-sized spectral sparsifiers for directed graphs under certain conditions. Furthermore, we introduce a practically-efficient spectral algorithm (diGRASS) for sparsifying real-world, large-scale directed graphs leveraging spectral matrix perturbation analysis. The proposed method has been evaluated using a variety of directed graphs obtained from real-world applications, showing promising results for solving directed graph Laplacians, spectral partitioning of directed graphs, and approximately computing (personalized) PageRank vectors.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390575530",
    "type": "article"
  },
  {
    "title": "Multi-Task Learning with Sequential Dependence Toward Industrial Applications: A Systematic Formulation",
    "doi": "https://doi.org/10.1145/3640468",
    "publication_date": "2024-01-12",
    "publication_year": 2024,
    "authors": "Xiaobo Guo; Mingming Ha; Xuewen Tao; Shaoshuai Li; Youru Li; Zhenfeng Zhu; Zhiyong Shen; Li Ma",
    "corresponding_authors": "",
    "abstract": "Multi-task learning (MTL) is widely used in the online recommendation and financial services for multi-step conversion estimation, but current works often overlook the sequential dependence among tasks. In particular, sequential dependence multi-task learning (SDMTL) faces challenges in dealing with complex task correlations and extracting valuable information in real-world scenarios, leading to negative transfer and a deterioration in the performance. Herein, a systematic learning paradigm of the SDMTL problem is established for the first time, which applies to more general multi-step conversion scenarios with longer conversion paths or various task dependence relationships. Meanwhile, an SDMTL architecture, named Task-Aware Feature Extraction (TAFE), is designed to enable the dynamic task representation learning from a sample-wise view. TAFE selectively reconstructs the implicit shared information corresponding to each sample case and performs the explicit task-specific extraction under dependence constraints, which can avoid the negative transfer, resulting in more effective information sharing and joint representation learning. Extensive experiment results demonstrate the effectiveness and applicability of the proposed theoretical and implementation frameworks. Furthermore, the online evaluations at MYbank showed that TAFE had an average increase of 9.22% and 3.76% in various scenarios on the post-view click-through &amp; conversion rate (CTCVR) estimation task. Currently, TAFE is deployed in an online platform to provide various traffic services.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390815807",
    "type": "article"
  },
  {
    "title": "CoBjeason: Reasoning Covered Object in Image by Multi-Agent Collaboration Based on Informed Knowledge Graph",
    "doi": "https://doi.org/10.1145/3643565",
    "publication_date": "2024-01-26",
    "publication_year": 2024,
    "authors": "Huan Rong; Minfeng Qian; Tinghuai Ma; Di Jin; Victor S. Sheng",
    "corresponding_authors": "",
    "abstract": "Object detection is a widely studied problem in existing works. However, in this paper, we turn to a more challenging problem of “ Covered Object Reasoning ”, aimed at reasoning the category label of target object in the given image particularly when it has been totally covered (or invisible ). To resolve this problem, we propose CoBjeason to seize the opportunity when visual reasoning meets the knowledge graph, where “ empirical cognition ” on common visual contexts have been incorporated as knowledge graph to conduct reinforced multi-hop reasoning via two collaborative agents. Such two agents, for one thing, stand at the covered object (or unknown entity ) to observe the surrounding visual cues in the given image and gradually select entities and relations from the global gallery-level knowledge graph which contains entity-pairs frequently occurring across the entire image-collection, so as to infer the main structure of image-level knowledge graph forward expanded from the unknown entity . In turn, for another, based on the reasoned image-level knowledge graph, the semantic context among entities will be aggregated backward into unknown entity to select an appropriate entity from the global gallery-level knowledge graph as the reasoning result. Moreover, such two agents will collaborate with each other, securing that the above Forward &amp; Backward Reasoning will step towards the same destination of the higher performance on covered object reasoning. To our best knowledge, this is the first work on Covered Object Reasoning with Knowledge Graphs and reinforced Multi-Agent collaboration. Particularly, our study on Covered Object Reasoning and the proposed model CoBjeason could offer novel insights into more basic Computer Vision (CV) tasks, such as Semantic Segmentation with better understanding on the current scene when some objects are blurred or covered, Visual Question Answering with enhancement on the inference in more complicated visual context when some objects are covered or invisible, and Image Caption Generation with the augmentation on the richness of visual context for images containing partially visible objects. The improvement on the above basic CV tasks can further refine more complicated ones involved with nuanced visual interpretation like Autonomous Driving, where the recognition and reasoning on partially visible or covered object are critical. According to the experimental results, our proposed CoBjeason can achieve the best overall ranking performance on covered object reasoning compared with other models, meanwhile enjoying the advantage of lower “ exploration cost ”, with the insensitivity against the long-tail covered objects and the acceptable time complexity.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391262442",
    "type": "article"
  },
  {
    "title": "Scalable and Inductive Semi-supervised Classifier with Sample Weighting Based on Graph Topology",
    "doi": "https://doi.org/10.1145/3643645",
    "publication_date": "2024-01-30",
    "publication_year": 2024,
    "authors": "Fadi Dornaika; Z. Ibrahim; Alireza Bosaghzadeh",
    "corresponding_authors": "",
    "abstract": "Recently, graph-based semi-supervised learning (GSSL) has garnered significant interest in the realms of machine learning and pattern recognition. Although some of the proposed methods have made some progress, there are still some shortcomings that need to be overcome. There are three main limitations. First, the graphs used in these approaches are usually predefined regardless of the task at hand. Second, due to the use of graphs, almost all approaches are unable to process and consider data with a very large number of unlabeled samples. Thirdly, the imbalance of the topology of the samples is very often not taken into account. In particular, processing large datasets with GSSL might pose challenges in terms of computational resource feasibility. In this article, we present a scalable and inductive GSSL method. We broaden the scope of the graph topology imbalance paradigm to extensive databases. Second, we employ the calculated weights of the labeled sample for the label-matching term in the global objective function. This leads to a unified, scalable, semi-supervised learning model that allows simultaneous labeling of unlabeled data, projection of the feature space onto the labeling space, along with the graph matrix of anchors. In the proposed scheme, the integration of labels and features from anchors is applied for the adaptive construction of the anchor graph. Experimental results were performed on four large databases: NORB, RCV1, Covtype, and MNIST. These experiments demonstrate that the proposed method exhibits superior performance when compared to existing scalable semi-supervised learning models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391350686",
    "type": "article"
  },
  {
    "title": "A Taxonomy for Learning with Perturbation and Algorithms",
    "doi": "https://doi.org/10.1145/3644391",
    "publication_date": "2024-02-03",
    "publication_year": 2024,
    "authors": "Rujing Yao; Ou Wu",
    "corresponding_authors": "",
    "abstract": "Weighting strategy prevails in machine learning. For example, a common approach in robust machine learning is to exert low weights on samples which are likely to be noisy or quite hard. This study summarizes another less-explored strategy, namely, perturbation. Various incarnations of perturbation have been utilized but it has not been explicitly revealed. Learning with perturbation is called perturbation learning and a systematic taxonomy is constructed for it in this study. In our taxonomy, learning with perturbation is divided on the basis of the perturbation targets, directions, inference manners, and granularity levels. Many existing learning algorithms including some classical ones can be understood with the constructed taxonomy. Alternatively, these algorithms share the same component, namely, perturbation in their procedures. Furthermore, a family of new learning algorithms can be obtained by varying existing learning algorithms with our taxonomy. Specifically, three concrete new learning algorithms are proposed for robust machine learning. Extensive experiments on image classification and text sentiment analysis verify the effectiveness of the three new algorithms. Learning with perturbation can also be used in other various learning scenarios, such as imbalanced learning, clustering, regression, and so on.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391514742",
    "type": "article"
  },
  {
    "title": "On Breaking Truss-based and Core-based Communities",
    "doi": "https://doi.org/10.1145/3644077",
    "publication_date": "2024-02-15",
    "publication_year": 2024,
    "authors": "Huiping Chen; Alessio Conte; Roberto Grossi; Grigorios Loukides; Solon P. Pissis; Michelle Sweering",
    "corresponding_authors": "",
    "abstract": "We introduce the general problem of identifying a smallest edge subset of a given graph whose deletion makes the graph community-free. We consider this problem under two community notions that have attracted significant attention: k -truss and k -core. We also introduce a problem variant where the identified subset contains edges incident to a given set of nodes and ensures that these nodes are not contained in any community: k -truss or k -core, in our case. These problems are directly applicable in social networks: The identified edges can be hidden by users or sanitized from the output graph; or in communication networks: the identified edges correspond to vital network connections. We present a series of theoretical and practical results. On the theoretical side, we show through non-trivial reductions that the problems we introduce are NP-hard and, in fact, hard to approximate. For the k -truss-based problems, we also show exact exponential-time algorithms, as well as a non-trivial lower bound on the size of an optimal solution. On the practical side, we develop a series of heuristics that are sped up by efficient data structures that we propose for updating the truss or core decomposition under edge deletions. In addition, we develop an algorithm to compute the lower bound. Extensive experiments on 11 real-world and synthetic graphs show that our heuristics are effective, outperforming natural baselines, and also efficient (up to two orders of magnitude faster than a natural baseline), thanks to our data structures. Furthermore, we present a case study on a co-authorship network and experiments showing that the removal of edges identified by our heuristics does not substantially affect the clustering structure of the input graph. This work extends a KDD 2021 paper, providing new theoretical results as well as introducing core-based problems and algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391843025",
    "type": "article"
  },
  {
    "title": "MoMENt: Marked Point Processes with Memory-Enhanced Neural Networks for User Activity Modeling",
    "doi": "https://doi.org/10.1145/3649504",
    "publication_date": "2024-02-29",
    "publication_year": 2024,
    "authors": "Sherry Sahebi; Mengfan Yao; Siqian Zhao; Reza Feyzi-Behnagh",
    "corresponding_authors": "",
    "abstract": "Marked temporal point process models (MTPPs) aim to model event sequences and event markers (associated features) in continuous time. These models have been applied to various application domains where capturing event dynamics in continuous time is beneficial, such as education systems, social networks, and recommender systems. However, current MTPPs suffer from two major limitations, i.e., inefficient representation of event dynamic’s influence on marker distribution and losing fine-grained representation of historical marker distributions in the modeling. Motivated by these limitations, we propose a novel model called M arked P o int Processes with M emory- E nhanced N eural Ne t works (MoMENt) that can capture the bidirectional interrelations between markers and event dynamics while providing fine-grained marker representations. Specifically, MoMENt is constructed of two concurrent networks: Recurrent Activity Updater (RAU) to capture model event dynamics and Memory-Enhanced Marker Updater (MEMU) to represent markers. Both RAU and MEMU components are designed to update each other at every step to model the bidirectional influence of markers and event dynamics. To obtain a fine-grained representation of maker distributions, MEMU is devised with external memories that model detailed marker-level features with latent component vectors. Our extensive experiments on six real-world user interaction datasets demonstrate that MoMENt can accurately represent users’ activity dynamics, boosting time, type, and marker predictions, as well as recommendation performance up to 76.5%, 65.6%, 77.2%, and 57.7%, respectively, compared to baseline approaches. Furthermore, our case studies show the effectiveness of MoMENt in providing meaningful and fine-grained interpretations of user-system relations over time, e.g., how user choices influence their future preferences in the recommendation domain.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392295653",
    "type": "article"
  },
  {
    "title": "<scp>SsAG</scp> : Summarization and Sparsification of Attributed Graphs",
    "doi": "https://doi.org/10.1145/3651619",
    "publication_date": "2024-03-06",
    "publication_year": 2024,
    "authors": "Sarwan Ali; Muhammad Ahmad; Maham Anwer Beg; Imdadullah Khan; Safiullah Faizullah; Muhammad Asad Khan",
    "corresponding_authors": "",
    "abstract": "Graph summarization has become integral for managing and analyzing large-scale graphs in diverse real-world applications, including social networks, biological networks, and communication networks. Existing methods for graph summarization often face challenges, being either computationally expensive, limiting their applicability to large graphs, or lacking the incorporation of node attributes. In response, we introduce SsAG , an efficient and scalable lossy graph summarization method designed to preserve the essential structure of the original graph. SsAG computes a sparse representation (summary) of the input graph, accommodating graphs with node attributes. The summary is structured as a graph on supernodes (subsets of vertices of G ), where weighted superedges connect pairs of supernodes. The methodology focuses on constructing a summary graph with k supernodes, aiming to minimize the reconstruction error (the difference between the original graph and the graph reconstructed from the summary) while maximizing homogeneity with respect to the node attributes. The construction process involves iteratively merging pairs of nodes. To enhance computational efficiency, we derive a closed-form expression for efficiently computing the reconstruction error (RE) after merging a pair, enabling constant-time approximation of this score. We assign a weight to each supernode, quantifying their contribution to the score of pairs, and utilize a weighted sampling strategy to select the best pair for merging. Notably, a logarithmic-sized sample achieves a summary comparable in quality based on various measures. Additionally, we propose a sparsification step for the constructed summary, aiming to reduce storage costs to a specified target size with a marginal increase in RE. Empirical evaluations across diverse real-world graphs demonstrate that SsAG exhibits superior speed, being up to 17 × faster, while generating summaries of comparable quality. This work represents a significant advancement in the field, addressing computational challenges and showcasing the effectiveness of SsAG in graph summarization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392507855",
    "type": "article"
  },
  {
    "title": "FulBM: Fast Fully Batch Maintenance for Landmark-based 3-hop Cover Labeling",
    "doi": "https://doi.org/10.1145/3650035",
    "publication_date": "2024-03-15",
    "publication_year": 2024,
    "authors": "Wentai Zhang; E Haihong; Haoran Luo; Mingzhi Sun",
    "corresponding_authors": "",
    "abstract": "Landmark-based 3-hop cover labeling is a category of approaches for shortest distance/path queries on large-scale complex networks. It pre-computes an index offline to accelerate the online distance/path query. Most real-world graphs undergo rapid changes in topology, which makes index maintenance on dynamic graphs necessary. So far, the majority of index maintenance methods can handle only one edge update (either an addition or deletion) each time. To keep up with frequently changing graphs, we research the ful ly b atch m aintenance problem for the 3-hop cover labeling, and proposed the method called FulBM . FulBM is composed of two algorithms: InsBM and DelBM, which are designed to handle batch edge insertions and deletions, respectively. This separation is motivated by the insight that batch maintenance for edge insertions are much more time-efficient and the fact that most edge updates in the real world are incremental. Both InsBM and DelBM are equipped with well-designed pruning strategies to minimize the number of vertex accesses. We have conducted comprehensive experiments on both synthetic and real-world graphs to verify the efficiency of FulBM and its variants for weighted graphs. The results show that our methods achieve 5.5× to 228× speedup compared with the state-of-the-art method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392849793",
    "type": "article"
  },
  {
    "title": "A Dual Perspective Framework of Knowledge-correlation for Cross-domain Recommendation",
    "doi": "https://doi.org/10.1145/3652520",
    "publication_date": "2024-03-18",
    "publication_year": 2024,
    "authors": "Yuhan Wang; Qing Xie; Mengzi Tang; Lin Li; Jingling Yuan; Yongjian Liu",
    "corresponding_authors": "",
    "abstract": "Recommender System provides users with online services in a personalized way. The performance of traditional recommender systems may deteriorate because of problems such as cold-start and data sparsity. Cross-domain Recommendation System utilizes the richer information from auxiliary domains to guide the task in the target domain. However, direct knowledge transfer may lead to a negative impact due to data heterogeneity and feature mismatch between domains. In this article, we innovatively explore the cross-domain correlation from the perspectives of content semanticity and structural connectivity to fully exploit the information of Knowledge Graph. First, we adopt domain adaptation that automatically extracts transferable features to capture cross-domain semantic relations. Second, we devise a knowledge-aware graph neural network to explicitly model the high-order connectivity across domains. Third, we develop feature fusion strategies to combine the advantages of semantic and structural information. By simulating the cold-start scenario on two real-world datasets, the experimental results show that our proposed method has superior performance in accuracy and diversity compared with the SOTA methods. It demonstrates that our method can accurately predict users’ expressed preferences while exploring their potential diverse interests.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392911039",
    "type": "article"
  },
  {
    "title": "Enhancing Unsupervised Outlier Model Selection: A Study on IREOS Algorithms",
    "doi": "https://doi.org/10.1145/3653719",
    "publication_date": "2024-04-05",
    "publication_year": 2024,
    "authors": "Philipp Schlieper; H S Luft; Kai Klede; Christoph Strohmeyer; Bjoern M. Eskofier; Dario Zanca",
    "corresponding_authors": "",
    "abstract": "Outlier detection stands as a critical cornerstone in the field of data mining, with a wide range of applications spanning from fraud detection to network security. However, real-world scenarios often lack labeled data for training, necessitating unsupervised outlier detection methods. This study centers on Unsupervised Outlier Model Selection (UOMS), with a specific focus on the family of Internal, Relative Evaluation of Outlier Solutions (IREOS) algorithms. IREOS measures outlier candidate separability by evaluating multiple maximum-margin classifiers and, while effective, it is constrained by its high computational demands. We investigate the impact of several different separation methods in UOMS in terms of ranking quality and runtime. Surprisingly, our findings indicate that different separability measures have minimal impact on IREOS’ effectiveness. However, using linear separation methods within IREOS significantly reduces its computation time. These insights hold significance for real-world applications where efficient outlier detection is critical. In the context of this work, we provide the code for the IREOS algorithm and our separability techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4393979289",
    "type": "article"
  },
  {
    "title": "EML: Emotion-Aware Meta Learning for Cross-Event False Information Detection",
    "doi": "https://doi.org/10.1145/3661485",
    "publication_date": "2024-05-02",
    "publication_year": 2024,
    "authors": "Yinqiu Huang; Min Gao; Kai Shu; Chenghua Lin; Jia Wang; Wei Zhou",
    "corresponding_authors": "",
    "abstract": "Modern social media’s development has dramatically changed how people obtain information. However, the wide dissemination of various false information has severe detrimental effects. Accordingly, many deep learning-based methods have been proposed to detect false information and achieve promising results. However, these methods are unsuitable for new events due to the extremely limited labeled data and their discrepant data distribution to existing events. Domain adaptation methods have been proposed to mitigate these problems. However, their performance is suboptimal because they are not sensitive to new events due to they aim to align the domain information between existing events, and they hardly capture the fine-grained difference between real and fake claims by only using semantic information. Therefore, we propose a novel Emotion-aware Meta Learning (EML) approach for cross-event false information early detection, which deeply integrates emotions in meta learning to find event-sensitive initialization parameters that quickly adapt to new events. EML is non-trivial and faces three challenges: (1) How to effectively model semantic and emotional features to capture fine-grained differences? (2) How to reduce the impact of noise in meta learning based on semantic and emotional features? (3) How to detect the false information in a zero-shot detection scenario, i.e., no labeled data for new events? To tackle these challenges, firstly, we construct the emotion-aware meta tasks by selecting claims with similar and opposite emotions to the target claim other than usually used random sampling. Secondly, we propose a task weighting method and event-adaptation meta tasks to further improve the model’s robustness and generalization ability for detecting new events. Finally, we propose a weak label annotation method to extend EML to zero-shot detection according to the calculated labels’ confidence. Extensive experiments on real-world datasets show that the EML achieves superior performances on false information detection for new events.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396573707",
    "type": "article"
  },
  {
    "title": "FastHGNN: A New Sampling Technique for Learning with Hypergraph Neural Networks",
    "doi": "https://doi.org/10.1145/3663670",
    "publication_date": "2024-05-09",
    "publication_year": 2024,
    "authors": "Fengcheng Lu; Michael K. Ng",
    "corresponding_authors": "",
    "abstract": "Hypergraphs can represent higher-order relations among objects. Traditional hypergraph neural networks involve node-edge-node transform, leading to high computational cost and timing. The main aim of this article is to propose a new sampling technique for learning with hypergraph neural networks. The core idea is to design a layer-wise sampling scheme for nodes and hyperedges to approximate the original hypergraph convolution. We rewrite hypergraph convolution in the form of double integral and leverage Monte Carlo to achieve a discrete and consistent estimator. In addition, we use importance sampling and finally derive feasible probability mass functions for both nodes and hyperedges in consideration of variance reduction, based on some assumptions. Notably, the proposed sampling technique allows us to handle large-scale hypergraph learning, which is not feasible with traditional hypergraph neural networks. Experiment results demonstrate that our proposed model keeps a good balance between running time and prediction accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396779970",
    "type": "article"
  },
  {
    "title": "Attribute Diversity Aware Community Detection on Attributed Graphs Using Three-View Graph Attention Neural Networks",
    "doi": "https://doi.org/10.1145/3672081",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Yang Zhang; Ting Yu; Shengqiang Chi; Zhen Wang; Yue Gao; Ji Zhang; Tianshu Zhou",
    "corresponding_authors": "",
    "abstract": "Community detection is a fundamental yet important task for characterizing and understanding the structure of attributed graphs. Existing methods mainly focus on the structural tightness and attribute similarity among nodes in a community. However, grouping numerous semantically homogeneous nodes will result in information cocoons and thus reduce the robustness of community structure and the efficiency of node collaboration in real-world applications, such as recommendation systems and collaboration networks. Since nodes with closer connections tend to be more similar, finding communities with dense structures and diverse attributes poses great challenges to mining latent relationships between the graph structure and attribute distribution. To our best knowledge, very little research has been conducted to address this challenge. In this article, we propose a novel three-view graph attention neural networks (TvGANN) model to formally address the attribute diversity aware community detection problem. TvGANN reveals correlations between the graph structure and attributes distribution from the perspective of node organization, attribute co-occurrence, and the node-attribute interaction. It effectively captures structural features and attributes distribution by feeding a structural network and an attribute co-occurrence network into graph attention modules through the encoder–decoder framework. It also learns heterogeneous information by feeding a network into a meta-node attention module. Then, it fuzes the three modules and clusters the embedding representations through a Student's t -distribution approach, which iteratively refines the clustering results. The experiments show that our method not only improves the quality in dense community detection but also performs efficiently for attributed graphs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399576831",
    "type": "article"
  },
  {
    "title": "Structural Properties on Scale-Free Tree Network with an Ultra-Large Diameter",
    "doi": "https://doi.org/10.1145/3674146",
    "publication_date": "2024-06-20",
    "publication_year": 2024,
    "authors": "Fei Ma; Ping Wang",
    "corresponding_authors": "",
    "abstract": "Scale-free networks are prevalently observed in a great variety of complex systems, which triggers various researches relevant to networked models of such type. In this work, we propose a family of growth tree networks \\(\\mathcal{T}_{t}\\) , which turn out to be scale-free, in an iterative manner. As opposed to most of published tree models with scale-free feature, our tree networks have the power-law exponent \\(\\gamma=1{ + }\\ln 5/\\ln 2\\) that is obviously larger than \\(3\\) . At the same time, “small-world” property can not be found particularly because models \\(\\mathcal{T}_{t}\\) have an ultra-large diameter \\(D_{t}\\) (i.e., \\(D_{t}\\sim|\\mathcal{T}_{t}|^{\\ln 3/\\ln 5}\\) ) and a greater average shortest path length \\(\\langle\\mathcal{W}_{t}\\rangle\\) (namely, \\(\\langle\\mathcal{W}_{t}\\rangle\\sim|\\mathcal{T}_{t}|^{\\ln 3/\\ln 5}\\) ) where \\(|\\mathcal{T}_{t}|\\) represents vertex number. Next, we determine Pearson correlation coefficient and verify that networks \\(\\mathcal{T}_{t}\\) display disassortative mixing structure. In addition, we study random walks on tree networks \\(\\mathcal{T}_{t}\\) and derive exact solution to mean hitting time \\(\\langle\\mathcal{H}_{t}\\rangle\\) . The results suggest that the analytic formula for quantity \\(\\langle\\mathcal{H}_{t}\\rangle\\) as a function of vertex number \\(|\\mathcal{T}_{t}|\\) shows a power-law form, i.e., \\(\\langle\\mathcal{H}_{t}\\rangle\\sim|\\mathcal{T}_{t}|^{1+\\ln 3/\\ln 5}\\) . Accordingly, we execute extensive experimental simulations, and demonstrate that empirical analysis is in strong agreement with theoretical results. Lastly, we provide a guide to extend the proposed iterative manner in order to generate more general scale-free tree networks with large diameter.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399858001",
    "type": "article"
  },
  {
    "title": "Towards Faster Deep Graph Clustering via Efficient Graph Auto-Encoder",
    "doi": "https://doi.org/10.1145/3674983",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Shifei Ding; Benyu Wu; Ling Ding; Xiao Xu; Lili Guo; Hongmei Liao; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Deep graph clustering (DGC) has been a promising method for clustering graph data in recent years. However, existing research primarily focuses on optimizing clustering outcomes by improving the quality of embedded representations, resulting in slow-speed complex models. Additionally, these methods do not consider changes in node similarity and corresponding adjustments in the original structure during the iterative optimization process after updating node embeddings, which easily falls into the representation collapse issue. We introduce an Efficient Graph Auto-Encoder (EGAE) and a dynamic graph weight updating strategy to address these issues, forming the basis for our proposed Fast DGC (FastDGC) network. Specifically, we significantly reduce feature dimensions using a linear transformation that preserves the original node similarity. We then employ a single-layer graph convolutional filtering approximation to replace multiple layers of graph convolutional neural network, reducing computational complexity and parameter count. During iteration, we calculate the similarity between nodes using the linearly transformed features and periodically update the original graph structure to reduce edges with low similarity, thereby enhancing the learning of discriminative and cohesive representations. Theoretical analysis confirms that EGAE has lower computational complexity. Extensive experiments on standard datasets demonstrate that our proposed method improves clustering performance and achieves a speedup of 2–3 orders of magnitude compared to state-of-the-art methods, showcasing outstanding performance. The code for our model is available at https://github.com/Marigoldwu/FastDGC . Furthermore, we have organized a portion of the DGC code into a unified framework, available at https://github.com/Marigoldwu/A-Unified-Framework-for-Deep-Attribute-Graph-Clustering .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400112989",
    "type": "article"
  },
  {
    "title": "AGENDA: Predicting Trip Purposes with A New Graph Embedding Network and Active Domain Adaptation",
    "doi": "https://doi.org/10.1145/3677020",
    "publication_date": "2024-07-08",
    "publication_year": 2024,
    "authors": "Chengwu Liao; Chao Chen; W.-A. Zhang; Suiming Guo; Chao Liu",
    "corresponding_authors": "",
    "abstract": "Trip purpose is a meaningful aspect of travel behaviour for the understanding of urban mobility. However, it is non-trivial to automatically obtain trip purposes. On one hand, trip purposes are naturally diverse and complicated, but the available predictive data sources are limited in real-world scenarios. On the other hand, since trip purpose labeling is costly and the development levels of cities are unbalanced, it is infeasible to access large-scale labeled data in less developed cities to train advanced prediction models. To narrow the gaps, this article presents A new Graph Embedding Network and active Domain Adaptation based framework (AGENDA) that only requires open data sources and is capable of predicting in both label-rich cities and label-scarce cities. Specifically, in label-rich source cities, we first use the vehicle’s GPS trajectory and open POI check-ins to augment trip contexts. Then we establish a supervised graph embedding network with two attention mechanisms to extract the passenger’s latent activity semantics and a classifier to predict trip purpose. To enable the prediction in label-scarce target cities, we further devise an active domain adaptation framework, in which adversarial domain adaptation is used to transfer the source-learned knowledge, and active learning is used to integrate human intelligence in the model training. A group of experiments are conducted with real-world datasets in Beijing and Shanghai. Evaluation results demonstrate that the proposed framework significantly outperforms existing trip purpose prediction algorithms, and could make accurate trip purpose prediction in label-scarce cities with much fewer labeling efforts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400417464",
    "type": "article"
  },
  {
    "title": "MICCF: A Mutual Information Constrained Clustering Framework for Learning Clustering-Oriented Feature Representations",
    "doi": "https://doi.org/10.1145/3672402",
    "publication_date": "2024-07-08",
    "publication_year": 2024,
    "authors": "Hongyu Li; Lefei Zhang; Kehua Su; Wei Yu",
    "corresponding_authors": "",
    "abstract": "Deep clustering is a crucial task in machine learning and data mining that focuses on acquiring feature representations conducive to clustering. Previous research relies on self-supervised representation learning for general feature representations, such features may not be optimally suited for downstream clustering tasks. In this article, we introduce MICCF, a framework designed to bridge this gap and enhance clustering performance. MICCF enhances feature representations by combining mutual information constraints at different levels and employs an auxiliary alignment mutual information module for learning clustering-oriented features. To be specific, we propose a dual mutual information constraints module, incorporating minimal mutual information constraints at the feature level and maximal mutual information constraints at the instance level. This reduction in feature redundancy encourages the neural network to extract more discriminative features, while maximization ensures more unbiased and robust representations. To obtain clustering-oriented representations, the auxiliary alignment mutual information module utilizes pseudo-labels to maximize mutual information through a multi-classifier network, aligning features with the clustering task. The main network and the auxiliary module work in synergy to jointly optimize feature representations that are well-suited for the clustering task. We validate the effectiveness of our method through extensive experiments on six benchmark datasets. The results indicate that our method performs well in most scenarios, particularly on fine-grained datasets, where our approach effectively distinguishes subtle differences between closely related categories. Notably, our approach achieved a remarkable accuracy of 96.4% on the ImageNet-10 dataset, surpassing other comparison methods. The code is available at https://github.com/Li-Hyn/MICCF.git .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400428603",
    "type": "article"
  },
  {
    "title": "Large Scale K-Clustering",
    "doi": "https://doi.org/10.1145/3674508",
    "publication_date": "2024-07-20",
    "publication_year": 2024,
    "authors": "Konstantin Voevodski",
    "corresponding_authors": "Konstantin Voevodski",
    "abstract": "Large-scale learning algorithms are essential for modern data collections that may have billions of data points. Here, we study the design of parallel \\(k\\) -clustering algorithms, which include the \\(k\\) -median, \\(k\\) -medoids, and \\(k\\) -means clustering problems. We design efficient parallel algorithms for these problems and prove that they still compute constant-factor approximations to the optimal solution for stable clustering instances. In addition to our theoretic results, we present computational experiments that show that our \\(k\\) -median and \\(k\\) -means algorithms work well in practice—we are able to find better clusterings than state-of-the-art coreset constructions using samples of the same size.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400852077",
    "type": "article"
  },
  {
    "title": "Trustworthiness-Driven Graph Convolutional Networks for Signed Network Embedding",
    "doi": "https://doi.org/10.1145/3685279",
    "publication_date": "2024-08-06",
    "publication_year": 2024,
    "authors": "M. J. Kim; Yeon-Chang Lee; David Y. Kang; Sang‐Wook Kim",
    "corresponding_authors": "",
    "abstract": "The problem of representing nodes in a signed network as low-dimensional vectors, known as signed network embedding (SNE), has garnered considerable attention in recent years. While several SNE methods based on graph convolutional networks (GCNs) have been proposed for this problem, we point out that they significantly rely on the assumption that the decades-old balance theory always holds in the real-world. To address this limitation, we propose a novel GCN-based SNE approach, named as TrustSGCN, which corrects for incorrect embedding propagation in GCN by utilizing the trustworthiness on edge signs for high-order relationships inferred by the balance theory. The proposed approach consists of three modules: (M1) generation of each node’s extended ego-network; (M2) measurement of trustworthiness on edge signs; and (M3) trustworthiness-aware propagation of embeddings. Specifically, TrustSGCN leverages topological information to measure trustworthiness on edge sign for high-order relationships inferred by balance theory. It then considers structural properties inherent to an input network, such as the ratio of triads, to correct for incorrect embedding propagation. Furthermore, TrustSGCN learns the node embeddings by leveraging two well-known social theories, i.e., balance and status, to jointly preserve the edge sign and direction between nodes connected by existing edges in the embedding space. The experiments on six real-world signed network datasets demonstrate that TrustSGCN consistently outperforms six state-of-the-art GCN-based SNE methods. The code is available at https://github.com/kmj0792/TrustSGCN .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401379744",
    "type": "article"
  },
  {
    "title": "A Meta-Learning Approach to Mitigating the Estimation Bias of Q-Learning",
    "doi": "https://doi.org/10.1145/3688849",
    "publication_date": "2024-08-14",
    "publication_year": 2024,
    "authors": "Tao Tan; Hong Xie; Xiaoyu Shi; Mingsheng Shang",
    "corresponding_authors": "",
    "abstract": "It is a longstanding problem that Q-learning suffers from the overestimation bias. This issue originates from the fact that Q-learning uses the expectation of maximum Q-value to approximate the maximum expected Q-value. A number of algorithms, such as Double Q-learning, were proposed to address this problem by reducing the estimation of maximum Q-value, but this may lead to an underestimation bias. Note that this underestimation bias may have a larger performance penalty than the overestimation bias. Different from previous algorithms, this article studies this issue from a fresh perspective, i.e., meta-learning view, which leads to our Meta-Debias Q-learning. The main idea is to extract the maximum expected Q-value with meta-learning over multiple tasks to remove the estimation bias of maximum Q-value and help the agent choose the optimal action more accurately. However, there are two challenges: (1) How to automatically select suitable training tasks? (2) How to positively transfer the meta-knowledge from selected tasks to remove the estimation bias of maximum Q-value? To address the two challenges mentioned above, we quantify the similarity between the training tasks and the test task. This similarity enables us to select appropriate “partial” training tasks and helps the agent extract the maximum expected Q-value to remove the estimation bias. Extensive experiment results show that our Meta-Debias Q-learning outperforms SOTA baselines drastically in three evaluation indicators, i.e., maximum Q-value, policy, and reward. More specifically, our Meta-Debias Q-learning only underestimates \\(1.2*10^{-3}\\) than the maximum expected Q-value in the multi-armed bandit environment and only differs \\(5.04\\%-5\\%=0.04\\%\\) than the optimal policy in the two states MDP environment. In addition, we compare the uniform weight and our similarity weight. Experiment results reveal fundamental insights into why our proposed algorithm outperforms in the maximum Q-value, policy, and reward.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401576535",
    "type": "article"
  },
  {
    "title": "Graph Representation Learning Enhanced Semi-Supervised Feature Selection",
    "doi": "https://doi.org/10.1145/3689428",
    "publication_date": "2024-08-27",
    "publication_year": 2024,
    "authors": "Jun Tan; Zhifeng Qiu; Ning Gui",
    "corresponding_authors": "",
    "abstract": "Feature selection is a key step in machine learning by eliminating features that are not related to the modeling target to create reliable and interpretable models. By exploring the potential complex correlations among features of unlabeled data, recently introduced self-supervision-enhanced feature selection greatly reduces the reliance on the labeled samples. However, they are generally based on the autoencoder with sample-wise self-supervision, which can hardly exploit the relations among samples. To address this limitation, this article proposes graph representation learning enhanced semi-supervised feature selection (G-FS) which performs feature selection based on the discovery and exploitation of the non-Euclidean relations among features and samples by translating unlabeled “plain” tabular data into a bipartite graph. A self-supervised edge prediction task is designed to distill rich information on the graph into low-dimensional embeddings, which remove redundant features and noise. Guided by the condensed graph representation, we propose a batch attention feature weight generation mechanism that generates more robust weights according to batch-based selection patterns rather than individual samples. The results show that G-FS achieves significant performance edges in 14 datasets compared to twelve state-of-the-art baselines, including two recent self-supervised baselines. The source code is public available at https://github.com/Icannotnamemyselff/G-FS_Graph_enhacned_feature_selection .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401905921",
    "type": "article"
  },
  {
    "title": "Causal Discovery Using Weight-Based Conditional Independence Test",
    "doi": "https://doi.org/10.1145/3687467",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Zhaolong Ling; Bo Li; Yiwen Zhang; Peng Zhou; Xingyu Wu; Yue’e Huang; Kui Yu; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Conditional Independence (CI) tests play an essential role in causal discovery from observational data, enabling the measurement of independence between two nodes. However, traditional CI tests ignore the imbalanced occurrence probabilities of node values, which may affect the accuracy of determining independence between nodes. To address this problem, we first introduce a new concept of the Node-imbalance phenomenon to describe the imbalance of node values in the Bayesian network data and analyze the influence of the Node-imbalance phenomenon on the traditional CI tests, then we propose a Weight-Based Conditional Independence (WCI) test to improve the accuracy of CI tests in the presence of Node-imbalance. In the experiments, we verify that WCI effectively measures the dependency between nodes in the Node-imbalance phenomenon compared with the traditional independence tests, and the state-of-the-art causal discovery algorithms reduce the number of false causal orientations through WCI.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401944184",
    "type": "article"
  },
  {
    "title": "Fair Single Index Model",
    "doi": "https://doi.org/10.1145/3690646",
    "publication_date": "2024-08-30",
    "publication_year": 2024,
    "authors": "Yezheng Wang; Meng Ding; Jinhui Xu; Di Wang",
    "corresponding_authors": "",
    "abstract": "Single-index models (SIMs) have been widely used in various applications due to their simplicity and interpretability. However, despite the potential for SIMs to result in discriminatory outcomes based on sensitive attributes like gender, race, or ethnicity, the issue of fairness has not been thoroughly examined in recent studies on the topic. This paper aims to address these fairness concerns by proposing methods for building fair SIMs. Specifically, based on the definition of equal opportunity, we first provide a fairness definition for SIM. Next, we develop a unified fair SIM model and propose an efficient method to solve the fair SIM. Theoretically, we also show that our output is consistent in fairness. Finally, we conduct comprehensive experimental studies over 7 benchmark datasets and demonstrate that our fair SIM outperforms the other 8 baseline methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402048043",
    "type": "article"
  },
  {
    "title": "Hubness-Enabled Clustering and Recovery for Large-Scale Incomplete Multi-View Data",
    "doi": "https://doi.org/10.1145/3694689",
    "publication_date": "2024-09-04",
    "publication_year": 2024,
    "authors": "Xiao Yu; Hui Liu; Yan Zhang; Yuxiu Lin; Caiming Zhang",
    "corresponding_authors": "",
    "abstract": "Incomplete multi-view clustering has gained considerable attention in recent years due to the prevalence of incomplete multi-view data in real-world applications. However, existing methods often struggle to effectively deal with large-scale datasets, particularly those with a significant number of missing instances. To address these issues, we propose a novel method called Hubness-Enabled Clustering and Recovery for Large-Scale Incomplete Multi-View Data (HENRI). HENRI utilizes the consensus hubs of all views to identify informative anchors to handle large-scale incomplete datasets. Furthermore, it incorporates a novel sample-level fusion strategy that effectively integrates information from all views, leading to remarkable outcomes in both cluster formation and missing data reconstruction. HENRI demonstrates exceptional capability in capturing the underlying structures of the data and recovering missing information, even when faced with a significant number of instances with incomplete data in partial views. To validate its effectiveness, we conducted experiments on six complete datasets and 31 incomplete datasets, comparing against 11 baseline methods. The results are impressive, demonstrating the superior performance of HENRI over the state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402223909",
    "type": "article"
  },
  {
    "title": "Exploiting Pre-Trained Language Models for Black-Box Attack against Knowledge Graph Embeddings",
    "doi": "https://doi.org/10.1145/3688850",
    "publication_date": "2024-09-04",
    "publication_year": 2024,
    "authors": "Guangqian Yang; Lei Zhang; Yi Liu; Hongtao Xie; Zhendong Mao",
    "corresponding_authors": "",
    "abstract": "Despite the emerging research on adversarial attacks against knowledge graph embedding (KGE) models, most of them focus on white-box attack settings. However, white-box attacks are difficult to apply in practice compared to black-box attacks since they require access to model parameters that are unlikely to be provided. In this article, we propose a novel black-box attack method that only requires access to knowledge graph data, making it more realistic in real-world attack scenarios. Specifically, we utilize pre-trained language models (PLMs) to encode text features of the knowledge graphs, an aspect neglected by previous research. We then employ these encoded text features to identify the most influential triples for constructing corrupted triples for the attack. To improve the transferability of the attack, we further propose to fine-tune the PLM model by enriching triple embeddings with structure information. Extensive experiments conducted on two knowledge graph datasets illustrate the effectiveness of our proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402224130",
    "type": "article"
  },
  {
    "title": "A novel tree-based method for interpretable reinforcement learning",
    "doi": "https://doi.org/10.1145/3695464",
    "publication_date": "2024-09-09",
    "publication_year": 2024,
    "authors": "Yifan Li; Shuhan Qi; Xuan Wang; Jiajia Zhang; Lei Cui",
    "corresponding_authors": "",
    "abstract": "Deep reinforcement learning (DRL) has garnered remarkable success across various domains, propelled by advancements in deep learning (DL) technologies. However, the opacity of DL presents significant challenges, limiting the application of DRL in critical systems. In response, decision tree (DT)-based methods, known for their transparent decision-making mechanisms, have shown promise in making interpretable policies for decision-making problems. Existing methods often employ differential DTs to model RL policies and discretize them to conventional DTs for higher interpretability. Yet, this method leads to discrepancies between the trained differential DTs and the discretized DTs. To address this issue, we introduce Generative Consistent Trees (GCTs), a novel solution that circumvents the information loss typically associated with the argmax operation in prior research. By implementing a reparameterization technique to approximate the categorical distribution, GCTs ensure the consistencies between trained GCTs and discretized counterparts. Moreover, we have developed an imitation-learning-based framework for interpretable reinforcement learning. This framework is designed to train GCTs by efficiently mimicking expert policies. Our extensive experiments across multiple environments have validated the effectiveness of this approach, highlighting the potential of GCTs in enhancing the interpretability and applicability of DRL.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402354563",
    "type": "article"
  },
  {
    "title": "FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning",
    "doi": "https://doi.org/10.1145/3695256",
    "publication_date": "2024-09-09",
    "publication_year": 2024,
    "authors": "Sejoon Oh; Berk Ustun; Julian McAuley; Srijan Kumar",
    "corresponding_authors": "",
    "abstract": "Modern recommender systems may output considerably different recommendations due to small perturbations in the training data. Changes in the data from a single user will alter the recommendations as well as the recommendations of other users. In applications like healthcare, housing, and finance, this sensitivity can have adverse effects on user experience. We propose a method to stabilize a given recommender system against such perturbations. This is a challenging task due to (1) the lack of a “reference” rank list that can be used to anchor the outputs; and (2) the computational challenges in ensuring the stability of rank lists with respect to all possible perturbations of training data. Our method, FINEST, overcomes these challenges by obtaining reference rank lists from a given recommendation model and then fine-tuning the model under simulated perturbation scenarios with rank-preserving regularization on sampled items. Our experiments on real-world datasets demonstrate that FINEST can ensure that recommender models output stable recommendations under a wide range of different perturbations without compromising next-item prediction accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402354607",
    "type": "article"
  },
  {
    "title": "Multi-Label and Evolvable Dataset Preparation for Web-Based Object Detection",
    "doi": "https://doi.org/10.1145/3695465",
    "publication_date": "2024-09-09",
    "publication_year": 2024,
    "authors": "Shucheng Li; Jingzhou Zhu; Boyu Chang; Hao Wu; Fengyuan Xu; Sheng Zhong",
    "corresponding_authors": "",
    "abstract": "In this paper, we focus on the emerging field of web-based object detection, which has gained considerable attention due to its ability to utilize large amounts of web data for training, thus eliminating the need for labor-intensive manual annotations. However, the noisy and ever-evolving nature of web data poses challenges in preparing high-quality datasets for web-based object detection. To address these challenges, we propose a fully automatic dataset preparation method in this paper. Our proposed method incorporates a hierarchical clustering module that assigns multiple precise labels to each image. This module is based on our observation that web image data exhibits different distributions at varying granularities. Furthermore, an evolutionary relabeling module ensures the adaptability of both the prepared dataset and trained detection models to the ever-evolving web data. Extensive experiments demonstrate that our method outperforms other web-based methods, and achieves a comparable performance to those manually labeled benchmark datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402354630",
    "type": "article"
  },
  {
    "title": "Assessing the Spatial-Temporal Causal Impact of COVID-19 Related Policies on Epidemic Spread",
    "doi": "https://doi.org/10.1145/3697841",
    "publication_date": "2024-09-26",
    "publication_year": 2024,
    "authors": "Zhiwen Zhang; Hongjun Wang; Zipei Fan; Xuan Song; Ryosuke Shibasaki",
    "corresponding_authors": "",
    "abstract": "Analyzing the causal impact of various government-related policies on the epidemic spread is of critical importance. This paper aims to investigate the problem of assessing the causal effects of different COVID-19 related policies on the USA epidemic spread in different counties at any given time period, while eliminating biased interference from unobserved confounders (e.g., the vigilance of residents). However, the infection outcome of each region is influenced not only by its own confounding factors but also by policy interventions implemented in neighboring regions. Furthermore, the government policy index may exhibit a time-delay influence on outbreak dynamics. To this end, we implement observational data about different COVID-19 related policies (treatment) and outbreak dynamics (outcome) across different U.S. counties over time, and develop a causal framework that learns the representations of time-varying confounders to tackle the aforementioned issues. More specifically, we employ one recurrent structure to capture the accumulative effects stemming from the policy history and then utilize hypergraph neural network to model the interactions among spatial regions. Our experimental results demonstrate the effectiveness of the proposed framework in quantifying the causal impact of different policy types on epidemics. Compared with baseline methods, our assessment provides valuable insights for future policy-making endeavors.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402860315",
    "type": "article"
  },
  {
    "title": "Iterative Time Series Imputation by Maintaining Dependency Consistency",
    "doi": "https://doi.org/10.1145/3698107",
    "publication_date": "2024-09-29",
    "publication_year": 2024,
    "authors": "Hanwen Hu; Shiyou Qian; Dingyu Yang; Jian Cao; Guangtao Xue",
    "corresponding_authors": "",
    "abstract": "Data imputation is crucial in the analysis of incomplete time series, such as forecasting and classification, which involves learning dependencies among the observed values to infer missing ones. As there are no ground truths for missing values, the challenge of time series imputation lies in preventing the model from overfitting to spurious correlations. In this paper, we believe that ensuring dependency consistency between observed and imputed values in a sequence is paramount for data imputation. Based on this idea, we propose a model called IR 2 -Net 1 , which combines an incomplete representation mechanism ( IRM ) with an iterative reconstruction framework ( IRF ) to establish a closed-loop learning-validation imputation paradigm. Firstly, IRM facilitates the representation of dependencies in incomplete sequences while preserving their distributions and semantics, effectively preventing the model from capturing spurious correlations. Secondly, IRF enables the model to reconstruct identical complete sequences separately based on imputed and observed values, ensuring that the dependencies of imputed values remain consistent with those of the observed ones. We conduct experiments on four datasets and compare IR 2 -Net with seven state-of-the-art imputation models. The experiment results show that IR 2 -Net outperforms all the baselines by 4.1%-23.4% in terms of accuracy. Moreover, IRF and IRM are two general modules that can be easily integrated into two existing models, significantly enhancing their performance by 18.3%-42.0%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403004994",
    "type": "article"
  },
  {
    "title": "Promoting Machine Abilities of Discovering and Utilizing Knowledge in a Unified Zero-shot Learning Paradigm",
    "doi": "https://doi.org/10.1145/3700444",
    "publication_date": "2024-10-12",
    "publication_year": 2024,
    "authors": "Qingyang Mao; Zhi Li; Qi Liu; Likang Wu; Hefu Zhang; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "Knowledge discovery and utilization are two essential cognitive processes that enable humans to understand the world and extract new insights from their surroundings. These processes have motivated machine learning studies, particularly zero-shot learning (ZS), which seeks to identify unseen concepts through the use of side information. Previous ZS studies primarily focused on utilizing existing knowledge to infer unseen events, yet they overlook the crucial process of knowledge discovery and the integrated modeling of these knowledge-aware processes. In this study, we present a comprehensive ZS learning approach that explores and evaluates the machine's abilities of discovering and utilizing knowledge. More specifically, to emulate human-like knowledge discovery and utilization processes, we propose a novel visual-aware ZS knowledge graph completion task for evaluation, incorporating a traditional ZS image classification task. Technically, we develop a unified ZS learning paradigm named Cognitive Learner (CoLa) to foster the two knowledge-aware abilities. Including a knowledge representation learning (KRL) module and a knowledge adaptation (KA) module, CoLa adapts well to the two specified tasks with the corresponding data. Extensive experiments on large-scale datasets demonstrate CoLa models’ outstanding performance over compared methods in the two ZS tasks, illustrating their superior ability of discovering and utilizing knowledge.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403353453",
    "type": "article"
  },
  {
    "title": "GeoGail: A Model-based Imitation Learning Framework for Human Trajectory Synthesizing",
    "doi": "https://doi.org/10.1145/3699961",
    "publication_date": "2024-10-14",
    "publication_year": 2024,
    "authors": "Yuchen Wu; Huandong Wang; Changzheng Gao; Depeng Jin; Yong Li",
    "corresponding_authors": "",
    "abstract": "Synthesized human trajectories are crucial for a large number of applications. Existing solutions are mainly based on the generative adversarial network (GAN), which is limited due to the lack of modeling the human decision-making process. In this paper, we propose a novel imitation learning based method to synthesize human trajectories. This model utilizes a novel semantics-based interaction mechanism between the decision-making strategy and visitations to diverse geographical locations to model them in the semantic domain in a uniform manner. To augment the modeling ability to the real-world human decision-making policy, we propose a feature extraction model to extract the internal latent factors of variation of different individuals, and then propose a novel self-attention based policy net to capture the long-term correlation of mobility and decision-making patterns. Then, to better reward users’ mobility behavior, we propose a novel multi-scale reward net combined with mutual information to model the instant reward, long-term reward, and individual characteristics in a cohesive manner. Extensive experimental results on two real-world trajectory datasets show that our proposed model can synthesize the most high-quality trajectory data compared with six state-of-the-art baselines in terms of a number of key usability metrics, and can well support practical applications based on trajectory data, demonstrating its effectiveness. Furthermore, our proposed method can learn explainable knowledge automatically from data, including explainable statistical features of trajectories and statistical relation between decision-making policy and features.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403393746",
    "type": "article"
  },
  {
    "title": "Learnable Transform-Assisted Tensor Decomposition for Spatio-Irregular Multidimensional Data Recovery",
    "doi": "https://doi.org/10.1145/3701235",
    "publication_date": "2024-10-21",
    "publication_year": 2024,
    "authors": "Hao Zhang; Ting‐Zhu Huang; Xi-Le Zhao; Shuqin Zhang; J.L. Xie; Tai-Xiang Jiang; Michael K. Ng",
    "corresponding_authors": "",
    "abstract": "Tensor decompositions have been successfully applied to multidimensional data recovery. However, classical tensor decompositions are not suitable for emerging spatio-irregular multidimensional data (i.e., spatio-irregular tensor), whose spatial domain is non-rectangular, e.g., spatial transcriptomics data from bioinformatics and semantic units from computer vision. By using preprocessing (e.g., zero-padding or element-wise 0-1 weighting), the spatio-irregular tensor can be converted to a spatio-regular tensor and then classical tensor decompositions can be applied, but this strategy inevitably introduces bias information, leading to artifacts. How to design a tensor-based method suitable for emerging spatio-irregular tensors is an imperative challenge. To address this challenge, we propose a learnable transform-assisted tensor singular value decomposition (LTA-TSVD) for spatio-irregular tensor recovery, which allows us to leverage the intrinsic structure behind the spatio-irregular tensor. Specifically, we design a learnable transform to project the original spatio-irregular tensor into its latent spatio-regular tensor, and then the latent low-rank structure is captured by classical TSVD on the resulting regular tensor. Empowered by LTA-TSVD, we develop spatio-irregular low-rank tensor completion (SIR-LRTC) and spatio-irregular tensor robust principal component analysis (SIR-TRPCA) models for the spatio-irregular tensor imputation and denoising respectively, and we design corresponding solving algorithms with theoretical convergence. Extensive experiments including the spatial transcriptomics data imputation and hyperspectral image denoising show SIR-LRTC and SIR-TRPCA are superior performance to competing approaches and benefit downstream applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403602255",
    "type": "article"
  },
  {
    "title": "A General Concave Fairness Framework for Influence Maximization Based on Poverty Reward",
    "doi": "https://doi.org/10.1145/3701737",
    "publication_date": "2024-10-28",
    "publication_year": 2024,
    "authors": "Zhixiao Wang; Jiayu Zhao; Chengcheng Sun; Xiaobin Rui; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Influence maximization (IM) aims to find a group of influential nodes as initial spreaders to maximize the influence spread over a network. Yet, traditional IM algorithms have not been designed with fairness in mind, resulting in discrimination against some groups, like LGBTQ communities and racial minorities, etc. This issue has spurred research on Fair Influence Maximization (FIM). However, existing FIM studies come with some drawbacks. Firstly, most proposed notions of fairness for FIM cannot adjust the trade-off between fairness level and influence spread. Secondly, though a few specific notions of fairness allow such balancing, they are limited to a few specific concave functions, which may not be suitable for various real-world scenarios. Furthermore, none of them have studied the deep relations between the features of concave functions and the level of fairness. Thirdly, existing fairness metrics are limited to their corresponding concepts of fairness. Comparing the level of fairness across different algorithms using existing metrics can be challenging. To tackle the above problems, this paper first proposes a novel fairness notion named Poverty Reward (PR), which achieves fairness by rewarding the enrichment of groups with low utility. Based on PR, we further propose an algorithmic framework called Concave Fairness Framework (CFF) that allows any concave function that satisfies specific requirements. We also systematically clarify how fairness is improved by applying concave functions and provide an in-depth quantitative analysis of how to select appropriate concave functions for different utility distributions. Moreover, we propose the Reward of Fairness (RoF) metric that evaluates the disparity between groups. Based on RoF, an evaluation system is built to uniformly compare FIM algorithms from different fairness notions. Experiments in real-world datasets have demonstrated the validity of the CFF, as well as the proposed fairness notion.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403830166",
    "type": "article"
  },
  {
    "title": "Simplifying Distributed Neural Network Training on Massive Graphs: Randomized Partitions Improve Model Aggregation",
    "doi": "https://doi.org/10.1145/3701563",
    "publication_date": "2024-11-08",
    "publication_year": 2024,
    "authors": "Jiong Zhu; Aishwarya Reganti; Edward W Huang; Charles Dickens; Nikhil Rao; Karthik Subbian; Danai Koutra",
    "corresponding_authors": "",
    "abstract": "Distributed graph neural network (GNN) training facilitates learning on massive graphs that surpass the storage and computational capabilities of a single machine. Traditional distributed frameworks strive for performance parity with centralized training by maximally recovering cross-instance node dependencies, relying either on inter-instance communication or periodic fallback to centralized training. However, these processes create overhead and constrain the scalability of the framework. In this work, we propose a streamlined framework for distributed GNN training that eliminates these costly operations, yielding improved scalability, convergence speed, and performance over state-of-the-art approaches. Our framework (1) comprises independent trainers that asynchronously learn local models from locally available parts of the training graph and (2) synchronizes these local models only through periodic (time-based) model aggregation. Contrary to prevailing belief, our theoretical analysis shows that it is not essential to maximize the recovery of cross-instance node dependencies to achieve performance parity with centralized training. Instead, our framework leverages randomized assignment of nodes or super-nodes (i.e., collections of original nodes) to partition the training graph in order to enhance data uniformity and minimize discrepancies in gradient and loss function across instances. Experiments on social and e-commerce networks with up to 1.3 billion edges show that our proposed framework achieves state-of-the-art performance and 2.31 \\(\\times\\) speedup compared to the fastest baseline despite using less training data.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404185643",
    "type": "article"
  },
  {
    "title": "Modeling On-road Trajectories with Multi-task Learning",
    "doi": "https://doi.org/10.1145/3705005",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "Kaijun Liu; Sijie Ruan; Cheng Long; Liang Yu",
    "corresponding_authors": "",
    "abstract": "With the increasing popularity of GPS modules, there are various urban applications such as car navigation relying on trajectory data modeling. In this work, we study the problem of modeling on-road trajectories, which is to predict the next road segment given a partial GPS trajectory. Existing methods that model trajectories with Markov chain or recurrent neural network suffer from various issues, including limited capability of sequential modeling, insufficiency of incorporating the road network context, and lack of capturing the underlying semantics of trajectories. In this article, we propose a new trajectory modeling framework called Multi-task Modeling for Trajectories (MMTraj+), which avoids these issues. Specifically, MMTraj+ uses multi-head self-attention networks for sequential modeling, captures the overall road network as the context information for road segment embedding, and performs an auxiliary task of predicting the trajectory destination information (namely the ID and bearing angle) to better guide the main trajectory modeling task (controlled by a carefully designed gating mechanism). In addition, we tailor MMTraj+ for the cases where the destination information is known by dropping its auxiliary task of predicting the trajectory destination information. Extensive experiments conducted on real-world datasets demonstrate the superiority of the proposed method over the baseline methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404589653",
    "type": "article"
  },
  {
    "title": "Defending Federated Recommender Systems Against Untargeted Attacks: A Contribution-Aware Robust Aggregation Scheme",
    "doi": "https://doi.org/10.1145/3706112",
    "publication_date": "2024-11-28",
    "publication_year": 2024,
    "authors": "Ruicheng Liang; Yuanchun Jiang; Feida Zhu; Ling Cheng; Huiwen Liu",
    "corresponding_authors": "",
    "abstract": "Federated recommender systems (FedRSs) effectively tackle the trade-off between recommendation accuracy and privacy preservation. However, recent studies have revealed severe vulnerabilities in FedRSs, particularly against untargeted attacks seeking to undermine their overall performance. Defense methods employed in traditional recommender systems are not applicable to FedRSs, and existing robust aggregation schemes for other federated learning-based applications have proven ineffective in FedRSs. Building on the observation that malicious clients contribute negatively to the training process, we design a novel contribution-aware robust aggregation scheme to defend FedRSs against untargeted attacks, named contribution-aware Bayesian knowledge distillation aggregation (ConDA), comprising two key components for the defense. In the first contribution estimation component, we decentralize the estimation from the server side to the client side and propose an ensemble-based Shapley value to enable the efficient calculation of contributions, addressing the limitations of lacking auxiliary validation data and high computational complexity. In the second contribution-aware aggregation component, we merge the decentralized contributions via a majority voting mechanism and integrate the merged contributions into a Bayesian knowledge distillation aggregation scheme for robust aggregation, mitigating the impact of unreliable contributions induced by attacks. We evaluate the effectiveness and efficiency of ConDA on two real-world datasets from movie and music service providers. Through extensive experiments, we demonstrate the superiority of ConDA over the baseline robust aggregation schemes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404811019",
    "type": "article"
  },
  {
    "title": "Structure Amplification on Multi-layer Stochastic Block Models",
    "doi": "https://doi.org/10.1145/3706111",
    "publication_date": "2024-12-02",
    "publication_year": 2024,
    "authors": "Kun He; Xiaodong Xin; Jialu Bao; Meng Wang; Bart Selman; John E. Hopcroft",
    "corresponding_authors": "",
    "abstract": "Much of the complexity of social, biological, and engineering systems arises from the complicated interactions among the entities in the corresponding networks. A number of network analysis tools have been successfully used to discover latent structures termed communities in such networks. However, some communities with relatively weak structures can be difficult to uncover because they are obscured by other stronger connections. To cope with this situation, our previous work proposes an algorithm called HICODE to detect and amplify the dominant and hidden community structures. In this work, we conduct a comprehensive and systematic theoretical analysis on the impact of hidden community structure and the efficacy of the HICODE algorithm, as well as provide illustrations of the detection process and results. Specifically, we define a multi-layer stochastic block model, and use this model to explain why the existence of hidden structure makes the detection of dominant structure harder than equivalent random noises, which can also explain why many community detection algorithms only focusing on the dominant structure do not work well as expected. We then provide theoretical analysis that the iterative reducing methods could help to enhance the discovery of hidden structure as well as the dominant structure in the multi-layer stochastic block model for the two cases of accurate and inaccurate detection. Finally, visual simulations and experimental results are presented to show the process of HICODE algorithm and the impact of different number of layers on the detection quality.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404927952",
    "type": "article"
  },
  {
    "title": "AdaptSel: Adaptive Selection of Biased and Debiased Recommendation Models for Varying Test Environments",
    "doi": "https://doi.org/10.1145/3706637",
    "publication_date": "2024-12-05",
    "publication_year": 2024,
    "authors": "Zimu Wang; Hao Zou; Jiashuo Liu; Jiayun Wu; Pengfei Tian; Yue He; Peng Cui",
    "corresponding_authors": "",
    "abstract": "Recommendation systems are frequently challenged by pervasive biases in the training set that can compromise model effectiveness. To address this issue, various debiasing techniques have been developed to eliminate biases and produce debiased models. However, when encountering varying test environments, some data patterns manifested by the training data could be beneficial to the model’s performance. Completely removing biases may overlook the beneficial data patterns and consequently diminish recommendation accuracy. Thus, it is crucial to carefully integrate certain biases to optimize performance, while the ideal level of bias integration is highly dependent on the test environment. Moreover, these systems operate in dynamic scenarios where the test environments could vary, necessitating an adaptive integration strategy customized to the environment. Our research establishes that discrepancies in predictions of models can guide the selection of the most fitting model for specific situations. Building on this understanding, we present AdaptSel, a pioneering method for the adaptive selection of the superior model during the testing phase. Empirical evaluations substantiate the foundational assumptions of AdaptSel, accentuating its effectiveness in adaptively selecting the most suitable model for varying test environments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405071847",
    "type": "article"
  },
  {
    "title": "PREFER: A Pre-trained Model Recommendation Framework for Edge Computing Enabled Traffic Flow Prediction",
    "doi": "https://doi.org/10.1145/3707464",
    "publication_date": "2024-12-09",
    "publication_year": 2024,
    "authors": "Qiqi Cai; Jian Cao; Yirong Chen; Shiyou Qian; Liangxiao Yuan; Jie Wang",
    "corresponding_authors": "",
    "abstract": "The recent years have witnessed a surge in the development of traffic flow prediction methods, often deployed on cloud platforms to offer predictive services for entire transportation networks. However, the processes of training and executing a model for the entire traffic network are both time-consuming and computationally expensive. As a result, the utilization of edge servers for local sub-network prediction services has gained prominence. Nevertheless, training prediction models for numerous sub-networks within the extensive traffic network remains a time-intensive and computing resource-consuming task. To tackle this challenge, this paper introduces the Pre-trained model REcommendation Framework for traffic sub-nEtwork in tRaffic flow prediction (PREFER). PREFER trains a set of traffic flow prediction models on selected sub-networks, then recommends optimal pre-trained models for edge servers. The recommendation is specifically based on performance prediction, integrating neural collaborative filtering and traffic flow characteristics. Experiments conducted on real datasets reveal that the pre-trained models recommended by PREFER perform close to the actual optimal ones and significantly outperform existing recommendation algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405188788",
    "type": "article"
  },
  {
    "title": "Margin-aware Noise-robust Contrastive Learning for Partially View-aligned Problem",
    "doi": "https://doi.org/10.1145/3707646",
    "publication_date": "2024-12-09",
    "publication_year": 2024,
    "authors": "Yalan Qin; Nan Pu; Hanzhou Wu; Nicu Sebe",
    "corresponding_authors": "",
    "abstract": "In this paper, we study a challenging problem in contrastive learning when just a portion of data is aligned in multi-view dataset due to temporal, spatial, or spatio-temporal asynchronism across views. It is important to study partially view-aligned data since this type of data is common in real-world application and easily leads to data inconsistency among different views. Such a Partially View-aligned Problem (PVP) in contrastive learning has been relatively less touched so far, especially in downstream tasks, i.e., classification and clustering. In order to solve this problem, we introduce a flexible margin and propose margin-aware noise-robust contrastive learning to simultaneously identify the within-category counterparts from the other view of one data point based on the established cross-view correspondence and learn a shared representation. To be specific, the proposed learning framework is built on a novel margin-aware noise-robust contrastive loss. Since data pairs are used as input for the proposed margin-aware noise-robust contrastive learning, we build positive pairs according to the known correspondences and negative pairs in the manner of random sampling. Our margin-aware noise-robust contrastive learning framework is able to effectively reduce or remove the impacts caused by the possible existing noise for the constructed pairs in a margin-aware manner, i.e., false negative pairs leaded by random sampling in PVP. We relax the proposed margin-aware noise-robust contrastive loss and then give a detailed mathematical analysis for the effectiveness of our loss. As an instantiation, we construct an example under the proposed margin-aware noise-robust contrastive learning framework for validation in this work. To the best of our knowledge, this is the first attempt of extending contrastive learning to a margin-aware noise-robust version for dealing with PVP. We also enrich the learning paradigm when there is noise in the data. Extensive experiments on different datasets demonstrate the promising performance of the proposed method in the classification and clustering tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405189288",
    "type": "article"
  },
  {
    "title": "Efficient Latent-based Scoring Function Search for N-ary Relational Knowledge Bases",
    "doi": "https://doi.org/10.1145/3707644",
    "publication_date": "2024-12-11",
    "publication_year": 2024,
    "authors": "Shimin Di; Yongqi Zhang; Quanming Yao; Xiaofang Zhou; Lei Chen",
    "corresponding_authors": "",
    "abstract": "Designing a proper scoring function is the key to ensuring the excellent performance of knowledge base (KB) embedding. Recently, the scoring function search method introduces the automated machine learning technique to design the data-aware scoring function for the given binary relational data (a.k.a. knowledge graph, KG), which can consistently achieve good performance on different data sets. However, the current data-aware search method is still not as good as desired. First, the existing model can only search scoring functions on the given binary relational data, which is a special form of N-ary relational KBs. Second, observing that existing scoring functions can exhibit distinct performance on different semantic patterns, we are motivated to explore such semantics by searching pattern-aware scoring functions. Unfortunately, it is hard to extend existing search approaches to the scenarios of N-ary and pattern-aware due to the search efficiency and effectiveness issues. In this paper, we propose latent-based factors to model relational patterns and an efficient search algorithm on the N-ary scenario, i.e., efficient LA tent-based SCO ring function search for N-ary relational KBs (LASCO). The empirical results of LASCO on binary and N-ary relational data sets demonstrate that the proposed method can efficiently search pattern-aware scoring functions, and achieve better embedding performance than advanced baselines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405290548",
    "type": "article"
  },
  {
    "title": "Means of Hitting Times for Random Walks on Graphs: Connections, Computation, and Optimization",
    "doi": "https://doi.org/10.1145/3708561",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Haisong Xia; Wanyue Xu; Zuobai Zhang; Zhongzhi Zhang",
    "corresponding_authors": "",
    "abstract": "For random walks on graph \\(\\mathcal{G}\\) with \\(n\\) vertices and \\(m\\) edges, the mean hitting time \\(H_{j}\\) from a vertex chosen from the stationary distribution to vertex \\(j\\) measures the importance for \\(j\\) , while the Kemeny constant \\(\\mathcal{K}\\) is the mean hitting time from one vertex to another selected randomly according to the stationary distribution. In this article, we first establish a connection between the two quantities, representing \\(\\mathcal{K}\\) in terms of \\(H_{j}\\) for all vertices. We then develop an efficient algorithm estimating \\(H_{j}\\) for all vertices and \\(\\mathcal{K}\\) in nearly linear time of \\(m\\) . Moreover, we extend the centrality \\(H_{j}\\) of a single vertex to \\(H(S)\\) of a vertex set \\(S\\) , and establish a link between \\(H(S)\\) and some other quantities. We further study the NP-hard problem of selecting a group \\(S\\) of \\(k\\ll n\\) vertices with minimum \\(H(S)\\) , whose objective function is monotonic and supermodular. We finally propose two greedy algorithms approximately solving the problem. The former has an approximation factor \\((1-\\frac{k}{k-1}\\frac{1}{e})\\) and \\(O(kn^{3})\\) running time, while the latter returns a \\((1-\\frac{k}{k-1}\\frac{1}{e}-\\epsilon)\\) -approximation solution in nearly-linear time of \\(m\\) , for any parameter \\(0{\\lt}\\epsilon{\\lt}1\\) . Extensive experiment results validate the performance of our algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405561290",
    "type": "article"
  },
  {
    "title": "Discovering Time-Aware Hidden Dependencies with Personalized Graphical Structure in Electronic Health Records",
    "doi": "https://doi.org/10.1145/3709143",
    "publication_date": "2024-12-23",
    "publication_year": 2024,
    "authors": "Arya Hadizadeh Moghaddam; Mohsen Nayebi Kerdabadi; Bin Liu; Mei Liu; Zijun Yao",
    "corresponding_authors": "",
    "abstract": "Over the past decade, significant advancements in mining electronic health records (EHRs) have enabled a broad range of decision-support applications, and offered an unprecedented capacity for predicting critical events such as disease prognosis and mortality in healthcare. Despite the availability of comprehensive coding systems in EHRs (e.g., ICD-9), which are designed to record diverse information on diseases, procedures, and medications over time, the complex and dynamic dependencies among the recorded data are usually not captured. This limitation often hinders the contextual understanding of medical observations for effective EHR representation learning. Therefore, there is a compelling need to discover a hidden “EHR graph” that represents the medical relationship between the observed features according to a patient’s history. These hidden graphs consisting of the medical codes from the same visits can offer a comprehensive insight derived from disease-to-disease, disease-to-drug, and drug-to-drug dependencies. However, it is still unclear how to address the challenge that the dependencies may vary from patient to patient, and they can dynamically evolve from one visit to another. To this end, we propose Timeaware Personalized Graph Transformer (TPGT), a novel attention-based time-aware hidden graph model, that captures the personalized graphical structures among observed medical codes and summarizes the temporal code dependencies over time to improve patient representation for outcome prediction. Built upon an intra-visit and an inter-visit dual-attention mechanism to model patients’ EHR graphs, our model offers an interpretability of what diagnosis or medication in a patient’s history can interact, and how those interactions may change over time. We conduct extensive experiments on two real-world EHR datasets for different healthcare predictive tasks: acute kidney injury (AKI) prediction and ICU mortality prediction. The experimental results demonstrate a significant performance improvement of the proposed model over baselines through multi-aspect quantitative evaluation. Furthermore, we perform various qualitative studies to validate the interpretability of the model which highlights the application of the proposed method in the context of personalized medicine.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405706477",
    "type": "article"
  },
  {
    "title": "Ranking from Crowdsourced Pairwise Comparisons via Smoothed Riemannian Optimization",
    "doi": "https://doi.org/10.1145/3372407",
    "publication_date": "2020-02-09",
    "publication_year": 2020,
    "authors": "Jialin Dong; Kai Yang; Yuanming Shi",
    "corresponding_authors": "",
    "abstract": "Social Internet of Things has recently become a promising paradigm for augmenting the capability of humans and devices connected in the networks to provide services. In social Internet of Things network, crowdsourcing that collects the intelligence of the human crowd has served as a powerful tool for data acquisition and distributed computing. To support critical applications (e.g., a recommendation system and assessing the inequality of urban perception), in this article, we shall focus on the collaborative ranking problems for user preference prediction from crowdsourced pairwise comparisons. Based on the Bradley--Terry--Luce (BTL) model, a maximum likelihood estimation (MLE) is proposed via low-rank approach in order to estimate the underlying weight/score matrix, thereby predicting the ranking list for each user. A novel regularized formulation with the smoothed surrogate of elementwise infinity norm is proposed in order to address the unique challenge of the coupled the non-smooth elementwise infinity norm constraint and non-convex low-rank constraint in the MLE problem. We solve the resulting smoothed rank-constrained optimization problem via developing the Riemannian trust-region algorithm on quotient manifolds of fixed-rank matrices, which enjoys the superlinear convergence rate. The admirable performance and algorithmic advantages of the proposed method over the state-of-the-art algorithms are demonstrated via numerical results. Moreover, the proposed method outperforms state-of-the-art algorithms on large collaborative filtering datasets in both success rate of inferring preference and normalized discounted cumulative gain.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3027047177",
    "type": "article"
  },
  {
    "title": "End-to-End Continual Rare-Class Recognition with Emerging Novel Subclasses",
    "doi": "https://doi.org/10.1145/3399660",
    "publication_date": "2020-08-05",
    "publication_year": 2020,
    "authors": "Hung T. Nguyen; Xuejian Wang; Leman Akoglu",
    "corresponding_authors": "",
    "abstract": "Given a labeled dataset that contains a rare (or minority) class containing of-interest instances, as well as a large class of instances that are not of interest, how can we learn to recognize future of-interest instances over a continuous stream? The setting is different from traditional classification in that instances from novel minority subclasses might continually emerge over time—and hence is often referred as continual, life-long, or open-world classification. We introduce RaRecognize, which ( i ) estimates a general decision boundary between the rare class and the majority class, ( ii ) learns to recognize the individual rare subclasses that exist within the training data, as well as ( iii ) flags instances from previously unseen rare subclasses as newly emerging (i.e., novel). The learner in (i) is general in the sense that by construction it is dissimilar to the specialized learners in (ii) , thus distinguishes minority from the majority without overly tuning to what is only seen in the training data. Thanks to this generality, RaRecognize ignores all future instances that it labels as majority and recognizes the recurring as well as emerging rare subclasses only. This saves effort at test time as well as ensures that the model size grows moderately over time as it only maintains specialized minority learners. Overall, we build an end-to-end system which consists of (1) a representation learning component that transforms data instances into suitable vector inputs; (2) a continual classifier that labels incoming instances as majority (not of interest), rare recurrent, or rare emerging; and (3) a clustering component that groups the rare emerging instances into novel subclasses for expert vetting and model re-training. Through extensive experiments, we show that RaRecognize outperforms state-of-the art baselines on three real-world datasets that contain documents related to corporate-risk and (natural and man-made) disasters as rare classes.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3047269745",
    "type": "article"
  },
  {
    "title": "Finding Key Structures in MMORPG Graph with Hierarchical Graph Summarization",
    "doi": "https://doi.org/10.1145/3522691",
    "publication_date": "2022-03-18",
    "publication_year": 2022,
    "authors": "Jun-Gi Jang; Chaeheum Park; Changwon Jang; Geonsoo Kim; U Kang",
    "corresponding_authors": "",
    "abstract": "What are the key structures existing in a large real-world MMORPG (Massively Multiplayer Online Role-Playing Game) graph? How can we compactly summarize an MMORPG graph with hierarchical node labels, considering substructures at different levels of hierarchy? Recent MMORPGs generate complex interactions between entities inducing a heterogeneous graph where each entity has hierarchical labels. Succinctly summarizing a heterogeneous MMORPG graph is crucial to better understand its structure; however it is a challenging task since it needs to handle complex interactions and hierarchical labels efficiently. Although there exist few methods to summarize a large-scale graph, they do not deal with heterogeneous graphs with hierarchical node labels.We propose GSHL , a novel method that summarizes a heterogeneous graph with hierarchical labels. We formulate the encoding cost of hierarchical labels using MDL (Minimum Description Length). GSHL exploits the formulation to identify and segment subgraphs, and discovers compact and consistent structures in the graph. Experiments on a large real-world MMORPG graph with multi-million edges show that GSHL is a useful and scalable tool for summarizing the graph, finding important structures in the graph, and finding similar users.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3116471713",
    "type": "article"
  },
  {
    "title": "Improving Bandit Learning Via Heterogeneous Information Networks: Algorithms and Applications",
    "doi": "https://doi.org/10.1145/3522590",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Xiaoying Zhang; Hong Xie; John C. S. Lui",
    "corresponding_authors": "",
    "abstract": "Contextual bandit serves as an invaluable tool to balance the exploration vs. exploitation tradeoff in various applications such as online recommendation. In many applications, heterogeneous information networks (HINs) provide rich side information for contextual bandits, such as different types of attributes and relationships among users and items. In this article, we propose the first HIN-assisted contextual bandit framework, which utilizes a given HIN to assist contextual bandit learning. The proposed framework uses meta-paths in HIN to extract rich relations among users and items for the contextual bandit. The main challenge is how to leverage these relations, since users’ preference over items, the target of our online learning, are closely related to users’ preference over meta-paths. However, it is unknown which meta-path a user prefers more. Thus, both preferences are needed to be learned in an online fashion with exploration vs. exploitation tradeoff balanced. We propose the HIN-assisted upper confidence bound (HUCB) algorithm to address such a challenge. For each meta-path, the HUCB algorithm employs an independent base bandit algorithm to handle online item recommendations by leveraging the relationship captured in this meta-path. A bandit master is then employed to learn users’ preference over meta-paths to dynamically combine base bandit algorithms with a balance of exploration vs. exploitation tradeoff. We theoretically prove that the HUCB algorithm can achieve similar performance compared with the optimal algorithm where each user is served according to his true preference over meta-paths (assuming the optimal algorithm knows the preference). Moreover, we prove that the HUCB algorithm benefits from leveraging HIN in achieving a smaller regret upper bound than the baseline algorithm without leveraging HIN. Experimental results on a synthetic dataset, as well as real datasets from LastFM and Yelp demonstrate the fast learning speed of the HUCB algorithm.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4220803205",
    "type": "article"
  },
  {
    "title": "CausalSE: Understanding Varied Spatial Effects with Missing Data Towards Adding New Bike-sharing Stations",
    "doi": "https://doi.org/10.1145/3536427",
    "publication_date": "2022-05-17",
    "publication_year": 2022,
    "authors": "Qianru Wang; Bin Guo; Lu Cheng; Zhiwen Yu; Huan Liu",
    "corresponding_authors": "",
    "abstract": "To meet the growing bike-sharing demands and make people’s travel convenient, the companies need to add new stations at locations where demands exceed supply. Before making reliable decisions on adding new stations, it is required to understand the spatial effects of new stations on the station network. In this paper, we study the deployment of the new station by estimating its varied causal effects on the demands of nearby stations, e.g., how does adding a new station (treatment) causally influence the demands (outcome) of nearby stations? When working with observational data, we should control hidden confounders, which cause spurious relations between treatments and outcomes. However, previous studies use historical data of the individual unit (e.g., the station’s historical demands) to approximate its hidden confounders, which cannot deal with the lack of historical data for new stations. And the conventional methods overlook the differences between units, which cannot be applied to our problem. To overcome the challenges, we propose a novel model (CausalSE) to estimate the varied effects of new stations on nearby stations, which uses the shared knowledge (i.e., similar traveling patterns among stations) to approximate hidden confounders. Experimental results on real-world datasets show that CausalSE outperforms 6 state-of-the-art methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4280563532",
    "type": "article"
  },
  {
    "title": "GRASP: Scalable Graph Alignment by Spectral Corresponding Functions",
    "doi": "https://doi.org/10.1145/3561058",
    "publication_date": "2022-09-15",
    "publication_year": 2022,
    "authors": "Judith Hermanns; Konstantinos Skitsas; Anton Tsitsulin; Marina Munkhoeva; Alexander Kyster; Simon Daugaard Nielsen; Alex Bronstein; Davide Mottin; Panagiotis Karras",
    "corresponding_authors": "",
    "abstract": "What is the best way to match the nodes of two graphs? This graph alignment problem generalizes graph isomorphism and arises in applications from social network analysis to bioinformatics. Some solutions assume that auxiliary information on known matches or node or edge attributes is available, or utilize arbitrary graph features. Such methods fare poorly in the pure form of the problem, in which only graph structures are given. Other proposals translate the problem to one of aligning node embeddings, yet, by doing so, provide only a single-scale view of the graph. In this article, we transfer the shape-analysis concept of functional maps from the continuous to the discrete case, and treat the graph alignment problem as a special case of the problem of finding a mapping between functions on graphs. We present GRASP, a method that first establishes a correspondence between functions derived from Laplacian matrix eigenvectors, which capture multiscale structural characteristics, and then exploits this correspondence to align nodes. We enhance the basic form of GRASP by altering two of its components, namely the embedding method and the assignment procedure it employs, leveraging its modular, hence adaptable design. Our experimental study, featuring noise levels higher than anything used in previous studies, shows that the enhanced form of GRASP outperforms scalable state-of-the-art methods for graph alignment across noise levels and graph types, and performs competitively with respect to the best non-scalable ones. We include in our study another modular graph alignment algorithm, CONE, which is also adaptable thanks to its modular nature, and show it can manage graphs with skewed power-law degree distributions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4295943290",
    "type": "article"
  },
  {
    "title": "Effective and Scalable Manifold Ranking-Based Image Retrieval with Output Bound",
    "doi": "https://doi.org/10.1145/3565574",
    "publication_date": "2022-10-14",
    "publication_year": 2022,
    "authors": "Dandan Lin; Victor Junqiu Wei; Raymond Chi-Wing Wong",
    "corresponding_authors": "",
    "abstract": "Image retrieval keeps attracting a lot of attention from both academic and industry over past years due to its variety of useful applications. Due to the rapid growth of deep learning approaches, more better feature vectors of images could be discovered for improving image retrieval. However, most (if not all) existing deep learning approaches consider the similarity between two images locally without considering the similarity among a group of similar images globally , and thus could not return accurate results. In this article, we study the image retrieval with manifold ranking (MR) which considers both the local similarity and the global similarity, which could give more accurate results. However, existing best-known algorithms have one of the following issues: (1) they require to build a bulky index, (2) some of them do not have any theoretical bound on the output, and (3) some of them are time-consuming. Motivated by this, we propose two algorithms, namely Monte Carlo-based MR ( MCMR ) and MCMR+ , for image retrieval, which do not have the above issues. We are the first one to propose an index-free manifold ranking image retrieval with the output theoretical bound. More importantly, our algorithms give the first best-known time complexity result of \\(O(n \\log n)\\) where \\(n\\) is the total number of images in the database compared with the existing best-known result of \\(O(n^2)\\) in the literature of computing the exact top- \\(k\\) results with quality guarantee. Lastly, our experimental result shows that MCMR+ outperforms existing algorithms by up to four orders of magnitude in terms of query time.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4306174637",
    "type": "article"
  },
  {
    "title": "Ada-MIP: Adaptive Self-supervised Graph Representation Learning via Mutual Information and Proximity Optimization",
    "doi": "https://doi.org/10.1145/3568165",
    "publication_date": "2022-11-09",
    "publication_year": 2022,
    "authors": "Yuyang Ren; Haonan Zhang; Peng Yu; Luoyi Fu; Xinde Cao; Xinbing Wang; Guihai Chen; Fei Long; Chenghu Zhou",
    "corresponding_authors": "",
    "abstract": "Self-supervised graph-level representation learning has recently received considerable attention. Given varied input distributions, jointly learning graphs’ unique and common features is vital to downstream tasks. Inspired by graph contrastive learning (GCL), which targets maximizing the agreement between graph representations from different views, we propose an Ada ptive self-supervised framework, Ada-MIP, considering both M utual I nformation between views (unique features) and inter-graph P roximity (common features). Specifically, Ada-MIP learns graphs’ unique information through a learnable and probably injective augmenter, which can acquire more adaptive views compared to the augmentation strategies applied by existing GCL methods; to learn graphs’ common information, we employ graph kernels to calculate graphs’ proximity and learn graph representations among which the precomputed proximity is preserved. By sharing a global encoder, graphs’ unique and common information can be well integrated into the graph representations learned by Ada-MIP. Ada-MIP is also extendable to semi-supervised scenarios, with our experiments confirming its superior performance in both unsupervised and semi-supervised tasks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4308732418",
    "type": "article"
  },
  {
    "title": "Diffuse and Smooth: Beyond Truncated Receptive Field for Scalable and Adaptive Graph Representation Learning",
    "doi": "https://doi.org/10.1145/3572781",
    "publication_date": "2022-11-22",
    "publication_year": 2022,
    "authors": "Hui Tang; Xun Liang; Yuhui Guo; Xiangping Zheng; Bo Wu; Sensen Zhang; Zhiying Li",
    "corresponding_authors": "",
    "abstract": "As the scope of receptive field and the depth of Graph Neural Networks (GNNs) are two completely orthogonal aspects for graph learning, existing GNNs often have shallow layers with truncated-receptive field and far from achieving satisfactory performance. In this article, we follow the idea of decoupling graph convolution into propagation and transformation processes, which generates representations over a sequence of increasingly larger neighborhoods. Though this manner can enlarge the receptive field, it has two critical problems unsolved: how to find the suitable receptive field to avoid under-smoothing or over-smoothing? and how to balance different diffusion operators for better capturing the local and global dependencies? We tackle these challenges and propose a S calable, A daptive G raph C onvolutional N etworks ( SAGCN ) with Transformer architecture. Concretely, we propose a novel non-heuristic metric method that quickly finds the suitable number of diffusing iterations and produces smoothed local embeddings that enable the truncated receptive field to become scalable and independent of prior experience. Furthermore, we devise smooth2seq and diffusion-based position schemes introduced into Transformer architecture for better capturing local and global information among embeddings. Experimental results show that SAGCN enjoys high accuracy, scalability and efficiency on various open benchmarks and is competitive with other state-of-the-art competitors.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4309697865",
    "type": "article"
  },
  {
    "title": "Hierarchical Dense Pattern Detection in Tensors",
    "doi": "https://doi.org/10.1145/3577022",
    "publication_date": "2022-12-20",
    "publication_year": 2022,
    "authors": "Wenjie Feng; Shenghua Liu; Xueqi Cheng",
    "corresponding_authors": "",
    "abstract": "Dense subtensor detection gains remarkable success in spotting anomalies and fraudulent behaviors for multi-aspect data (i.e., tensors), like in social media and event streams. Existing methods detect the densest subtensors flatly and separately, with the underlying assumption that those subtensors are exclusive. However, many real-world tensors usually present hierarchical properties, e.g., the core-periphery structure and dynamic communities in networks. It is also unexplored how to fuse the prior knowledge into dense pattern detection to capture the local behavior. In this article, we propose CatchCore , a novel framework to efficiently find the hierarchical dense subtensors. We first design a unified metric for dense subtensor detection, which can be optimized with gradient-based methods. With the proposed metric, CatchCore detects hierarchical dense subtensors through the hierarchy-wise alternative optimization and finds local dense patterns concerning some items in a query manner. Finally, we utilize the minimum description length principle to measure the quality of detection results and select the optimal hierarchical dense subtensors. Extensive experiments on synthetic and real-world datasets demonstrate that CatchCore outperforms the top competitors in accuracy for detecting dense subtensors and anomaly patterns, like network attacks. Additionally, CatchCore successfully identifies a hierarchical researcher co-authorship group with intense interactions in the DBLP dataset; it can also capture core collaboration and multi-hop relations around some query objects. Meanwhile, CatchCore also scales linearly with all aspects of tensors.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4312037483",
    "type": "article"
  },
  {
    "title": "Correlation-aware Graph Data Augmentation with Implicit and Explicit Neighbors",
    "doi": "https://doi.org/10.1145/3638057",
    "publication_date": "2024-01-25",
    "publication_year": 2024,
    "authors": "Chuan-Wei Kuo; Boyu Chen; Wen-Chih Peng; Chih‐Chieh Hung; Hsin‐Ning Su",
    "corresponding_authors": "",
    "abstract": "In recent years, there has been a significant surge in commercial demand for citation graph-based tasks, such as patent analysis, social network analysis, and recommendation systems. Graph Neural Networks (GNNs) are widely used for these tasks due to their remarkable performance in capturing topological graph information. However, GNNs’ output results are highly dependent on the composition of local neighbors within the topological structure. To address this issue, we identify two types of neighbors in a citation graph: explicit neighbors based on the topological structure and implicit neighbors based on node features. Our primary motivation is to clearly define and visualize these neighbors, emphasizing their importance in enhancing graph neural network performance. We propose a Correlation-aware Network (CNet) to re-organize the citation graph and learn more valuable informative representations by leveraging these implicit and explicit neighbors. Our approach aims to improve graph data augmentation and classification performance, with the majority of our focus on stating the importance of using these neighbors, while also introducing a new graph data augmentation method. We compare CNet with state-of-the-art (SOTA) GNNs and other graph data augmentation approaches acting on GNNs. Extensive experiments demonstrate that CNet effectively extracts more valuable informative representations from the citation graph, significantly outperforming baselines. The code is available on public GitHub. 1",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391221823",
    "type": "article"
  },
  {
    "title": "Conditional Generative Adversarial Network for Early Classification of Longitudinal Datasets Using an Imputation Approach",
    "doi": "https://doi.org/10.1145/3644821",
    "publication_date": "2024-02-07",
    "publication_year": 2024,
    "authors": "Sharon Torao Pingi; Richi Nayak; Md Abul Bashar",
    "corresponding_authors": "",
    "abstract": "Early classification of longitudinal data remains an active area of research today. The complexity of these datasets and the high rates of missing data caused by irregular sampling present data-level challenges for the Early Longitudinal Data Classification (ELDC) problem. Coupled with the algorithmic challenge of optimising the opposing objectives of early classification (i.e., earliness and accuracy), ELDC becomes a non-trivial task. Inspired by the generative power and utility of the Generative Adversarial Network (GAN), we propose a novel context-conditional, longitudinal early classifier GAN (LEC-GAN). This model utilises informative missingness, static features and earlier observations to improve the ELDC objective. It achieves this by incorporating ELDC as an auxiliary task within an imputation optimization process. Our experiments on several datasets demonstrate that LEC-GAN outperforms all relevant baselines in terms of F1 scores while increasing the earliness of prediction.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391614405",
    "type": "article"
  },
  {
    "title": "On Mean-Optimal Robust Linear Discriminant Analysis",
    "doi": "https://doi.org/10.1145/3665500",
    "publication_date": "2024-05-21",
    "publication_year": 2024,
    "authors": "Xiangyu Li; Hua Wang",
    "corresponding_authors": "",
    "abstract": "Linear discriminant analysis (LDA) is widely used for dimensionality reduction under supervised learning settings. Traditional LDA objective aims to minimize the ratio of the squared Euclidean distances that may not perform optimally on noisy datasets. Multiple robust LDA objectives have been proposed to address this problem, but their implementations have two major limitations. One is that their mean calculations use the squared \\(\\ell_{2}\\) -norm distance to center the data, which is not valid when the objective depends on other distance functions. The second problem is that there is no generalized optimization algorithm to solve different robust LDA objectives. In addition, most existing algorithms can only guarantee that the solution is locally optimal rather than globally optimal. In this article, we review multiple robust loss functions and propose a new and generalized robust objective for LDA. Besides, to remove the mean value within data better, our objective uses an optimal way to center the data through learning. As one important algorithmic contribution, we derive an efficient iterative algorithm to optimize the resulting non-smooth and non-convex objective function. We theoretically prove that our solution algorithm guarantees that both the objective and the solution sequences converge to globally optimal solutions at a sub-linear convergence rate. The results of comprehensive experimental evaluations demonstrate the effectiveness of our new method, achieving significant improvements compared to the other competing methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4398178330",
    "type": "article"
  },
  {
    "title": "Concept Evolution Detecting over Feature Streams",
    "doi": "https://doi.org/10.1145/3678012",
    "publication_date": "2024-07-13",
    "publication_year": 2024,
    "authors": "Peng Zhou; Yufeng Guo; Haoran Yu; Yuanting Yan; Yanping Zhang; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "The explosion of data volume has gradually transformed big data processing from the static batch mode to the online streaming model. Streaming data can be divided into instance streams (feature space remains fixed while instances increase over time), feature streams (instance space is fixed while features arrive over time), or both. Generally, online streaming data learning has two main challenges: infinite length and concept changing. Recently, feature stream learning has received much attention. However, existing feature stream learning methods focus on feature selection or classification but ignore the concept changing over time. To the best of our knowledge, this is the first work that studies concept evolution detection over feature streams. Specifically, we first give the formal definition of concept evolution over feature streams, which include three different types: concept emerging, concept drift, and concept forgetting. Then, we design a novel framework to detect the concept evolution over feature streams that consists of a sliding window, an improved density peak-based clustering algorithm, and a weighted bipartite graph-based concept detecting method. Extensive experiments have been conducted on several synthetic and high-dimensional datasets to indicate our new method’s ability to cluster and detect concept evolution over feature streams.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400607216",
    "type": "article"
  },
  {
    "title": "Building Robust and Trustworthy HGNN Models: A Learnable Threshold Approach for Node Classification",
    "doi": "https://doi.org/10.1145/3707645",
    "publication_date": "2024-12-20",
    "publication_year": 2024,
    "authors": "Li Ma; Yongchao Liu; Xiaofeng Gao; Peng Zhang; Chuntao Hong",
    "corresponding_authors": "",
    "abstract": "Message passing scheme is a general idea for graph neural networks (GNNs) to learn node representations. During message passing, given a target node, we transform and aggregate the feature vectors of its neighbours and generate a representation vector for the target node. However, real-world graph data is usually constructed from complicated scenarios based on manually predefined rules; it is often the case that noisy information gets involved in message passing, thereby resulting in sub-optimal performance for GNNs and also impacting their trustworthiness and reliability. In this study, we present an effective learnable threshold technique that explicitly optimizes heterogeneous graph structure with the goal to maximize performance improvement of GNNs for downstream tasks. We give an explanation about the design of the learnable threshold and show the ability that our model can be applied to large-scale graphs. Experiments on seven datasets show that our model has a powerful ability to deal with homogeneous graphs with low homophily ratio and dense graphs. With the verification of robustness analysis, our model can resist the noisy information, which proves the robustness of our model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405674525",
    "type": "article"
  },
  {
    "title": "Do Anesthesiologists Know What They Are Doing? Mining a Surgical Time-Series Database to Correlate Expert Assessment with Outcomes",
    "doi": "https://doi.org/10.1145/2822897",
    "publication_date": "2016-02-09",
    "publication_year": 2016,
    "authors": "Risa B. Myers; John C. Frenzel; Joseph Ruiz; Christopher Jermaine",
    "corresponding_authors": "",
    "abstract": "Anesthesiologists are taught to carefully manage patient vital signs during surgery. Unfortunately, there is little empirical evidence that vital sign management, as currently practiced, is correlated with patient outcomes. We seek to validate or repudiate current clinical practice and determine whether or not clinician evaluation of surgical vital signs correlate with outcomes. Using a database of over 90,000 cases, we attempt to determine whether those cases that anesthesiologists would subjectively decide are “low quality” are more likely to result in negative outcomes. The problem reduces to one of multi-dimensional time-series classification. Our approach is to have a set of expert anesthesiologists independently label a small number of training cases, from which we build classifiers and label all 90,000 cases. We then use the labeling to search for correlation with outcomes and compare the prevalence of important 30-day outcomes between providers. To mimic the providers’ quality labels, we consider several standard classification methods, such as dynamic time warping in conjunction with a kNN classifier, as well as complexity invariant distance, and a regression based upon the feature extraction methods outlined by Mao et al. 2012 (using features such as time-series mean, standard deviation, skew, etc.). We also propose a new feature selection mechanism that learns a hidden Markov model to segment the time series; the fraction of time that each series spends in each state is used to label the series using a regression-based classifier. In the end, we obtain strong, empirical evidence that current best practice is correlated with reduced negative patient outcomes. We also learn that all of the experts were able to significantly separate cases by outcome, with higher prevalence of negative 30-day outcomes in the cases labeled as “low quality” for almost all of the outcomes investigated.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2299478735",
    "type": "article"
  },
  {
    "title": "Guest Editorial",
    "doi": "https://doi.org/10.1145/2912122",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Hanghang Tong; Fei Wang; Munmun De Choudhury; Zoran Obradović",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Guest Editorial: Special Issue on Connected Health at Big Data Era (BigChat): A TKDD Special Issue Authors: Hanghang Tong Arizona State University, Tempe, AZ Arizona State University, Tempe, AZView Profile , Fei Wang University of Connecticut, Storrs, CT University of Connecticut, Storrs, CTView Profile , Munmun De Choudhury Georgia Institute of Technology, Atlanta, GA Georgia Institute of Technology, Atlanta, GAView Profile , Zoran Obradovic Temple University, Philadelphia, PA Temple University, Philadelphia, PAView Profile Authors Info & Claims ACM Transactions on Knowledge Discovery from DataVolume 10Issue 4July 2016 Article No.: 37pp 1–4https://doi.org/10.1145/2912122Published:24 May 2016Publication History 1citation325DownloadsMetricsTotal Citations1Total Downloads325Last 12 Months27Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2396557329",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3017677",
    "publication_date": "2016-12-26",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Computation of covariance matrices from observed data is an important problem, as such matrices are used in applications such as principal component analysis (PCA), linear discriminant analysis (LDA), and increasingly in the learning and application of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231056360",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2835206",
    "publication_date": "2015-10-26",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Multiplayer Online Games (MOGs) like Defense of the Ancients and StarCraft II have attracted hundreds of millions of users who communicate, interact, and socialize with each other through gaming. In MOGs, rich social relationships emerge and can be used ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231107041",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2672614",
    "publication_date": "2014-11-17",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Exploring statistics of locally connected subgraph patterns (also known as network motifs) has helped researchers better understand the structure and function of biological and Online Social Networks (OSNs). Nowadays, the massive size of some critical ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234930439",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2936311",
    "publication_date": "2016-07-27",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "It is often crucial for manufacturers to decide what products to produce so that they can increase their market share in an increasingly fierce market. To decide which products to produce, manufacturers need to analyze the consumers’ requirements and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235873175",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2663598",
    "publication_date": "2014-10-28",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Unsupervised models can provide supplementary soft constraints to help classify new “target” data because similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237651322",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2786971",
    "publication_date": "2015-06-01",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Many Web services like Amazon, Epinions, and TripAdvisor provide historical product ratings so that users can evaluate the quality of products. Product ratings are important because they affect how well a product will be adopted by the market. The ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243367696",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2974720",
    "publication_date": "2016-08-27",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "It is an important yet challenging task for investors to determine the most suitable type of shop (e.g., restaurant, fashion) for a newly opened store. Traditional ways are predominantly field surveys and empirical estimation, which are not effective as ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244103679",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2737800",
    "publication_date": "2015-04-13",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We design a space-efficient algorithm that approximates the transitivity (global clustering coefficient) and total triangle count with only a single pass through a graph given as a stream of edges. Our procedure is based on the classic probabilistic ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250413203",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2888412",
    "publication_date": "2016-02-24",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Churners are users who stop using a given service after previously signing up. In the domain of telecommunications and video games, churners represent a loss of revenue as a user leaving indicates that they will no longer pay for the service. In the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250463621",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2630935",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Network sampling is integral to the analysis of social, information, and biological networks. Since many real-world networks are massive in size, continuously evolving, and/or distributed in nature, the network structure is often sampled in order to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252054504",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2808688",
    "publication_date": "2015-07-27",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "With the rapid growth of modern technologies, Internet has reached almost every corner of the world. As a result, it becomes more and more important to manage and mine information contained in Web pages in different languages. Traditional supervised ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253474790",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2663597",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Common-day applications of predictive models usually involve the full use of the available contextual information. When the operating context changes, one may fine-tune the by-default (incontextual) prediction or may even abstain from predicting a value ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255010503",
    "type": "paratext"
  },
  {
    "title": "Generating Artificial Outliers in the Absence of Genuine Ones — A Survey",
    "doi": "https://doi.org/10.1145/3447822",
    "publication_date": "2021-03-27",
    "publication_year": 2021,
    "authors": "Georg Steinbuß; Klemens Böhm",
    "corresponding_authors": "",
    "abstract": "By definition, outliers are rarely observed in reality, making them difficult to detect or analyze. Artificial outliers approximate such genuine outliers and can, for instance, help with the detection of genuine outliers or with benchmarking outlier-detection algorithms. The literature features different approaches to generate artificial outliers. However, systematic comparison of these approaches remains absent. This surveys and compares these approaches. We start by clarifying the terminology in the field, which varies from publication to publication, and we propose a general problem formulation. Our description of the connection of generating outliers to other research fields like experimental design or generative models frames the field of artificial outliers. Along with offering a concise description, we group the approaches by their general concepts and how they make use of genuine instances. An extensive experimental study reveals the differences between the generation approaches when ultimately being used for outlier detection. This survey shows that the existing approaches already cover a wide range of concepts underlying the generation, but also that the field still has potential for further development. Our experimental study does confirm the expectation that the quality of the generation approaches varies widely, for example, in terms of the dataset they are used on. Ultimately, to guide the choice of the generation approach in a specific context, we propose an appropriate general-decision process. In summary, this survey comprises, describes, and connects all relevant work regarding the generation of artificial outliers and may serve as a basis to guide further research in the field.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3149489965",
    "type": "article"
  },
  {
    "title": "Recurrent Coupled Topic Modeling over Sequential Documents",
    "doi": "https://doi.org/10.1145/3451530",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Jinjin Guo; Longbing Cao; Zhiguo Gong",
    "corresponding_authors": "",
    "abstract": "The abundant sequential documents such as online archival, social media, and news feeds are streamingly updated, where each chunk of documents is incorporated with smoothly evolving yet dependent topics. Such digital texts have attracted extensive research on dynamic topic modeling to infer hidden evolving topics and their temporal dependencies. However, most of the existing approaches focus on single-topic-thread evolution and ignore the fact that a current topic may be coupled with multiple relevant prior topics. In addition, these approaches also incur the intractable inference problem when inferring latent parameters, resulting in a high computational cost and performance degradation. In this work, we assume that a current topic evolves from all prior topics with corresponding coupling weights, forming the multi-topic-thread evolution . Our method models the dependencies between evolving topics and thoroughly encodes their complex multi-couplings across time steps. To conquer the intractable inference challenge, a new solution with a set of novel data augmentation techniques is proposed, which successfully discomposes the multi-couplings between evolving topics. A fully conjugate model is thus obtained to guarantee the effectiveness and efficiency of the inference technique. A novel Gibbs sampler with a backward–forward filter algorithm efficiently learns latent time-evolving parameters in a closed-form. In addition, the latent Indian Buffet Process compound distribution is exploited to automatically infer the overall topic number and customize the sparse topic proportions for each sequential document without bias. The proposed method is evaluated on both synthetic and real-world datasets against the competitive baselines, demonstrating its superiority over the baselines in terms of the low per-word perplexity, high coherent topics, and better document time prediction.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3184105828",
    "type": "article"
  },
  {
    "title": "A Synopsis Based Approach for Itemset Frequency Estimation over Massive Multi-Transaction Stream",
    "doi": "https://doi.org/10.1145/3465238",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Guangtao Wang; Gao Cong; Ying Zhang; Zhen Hai; Jieping Ye",
    "corresponding_authors": "",
    "abstract": "The streams where multiple transactions are associated with the same key are prevalent in practice, e.g., a customer has multiple shopping records arriving at different time. Itemset frequency estimation on such streams is very challenging since sampling based methods, such as the popularly used reservoir sampling, cannot be used. In this article, we propose a novel k -Minimum Value (KMV) synopsis based method to estimate the frequency of itemsets over multi-transaction streams. First, we extract the KMV synopses for each item from the stream. Then, we propose a novel estimator to estimate the frequency of an itemset over the KMV synopses. Comparing to the existing estimator, our method is not only more accurate and efficient to calculate but also follows the downward-closure property. These properties enable the incorporation of our new estimator with existing frequent itemset mining (FIM) algorithm (e.g., FP-Growth) to mine frequent itemsets over multi-transaction streams. To demonstrate this, we implement a KMV synopsis based FIM algorithm by integrating our estimator into existing FIM algorithms, and we prove it is capable of guaranteeing the accuracy of FIM with a bounded size of KMV synopsis. Experimental results on massive streams show our estimator can significantly improve on the accuracy for both estimating itemset frequency and FIM compared to the existing estimators.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3185630596",
    "type": "article"
  },
  {
    "title": "<tt>MCS+</tt> : An Efficient Algorithm for Crawling the Community Structure in Multiplex Networks",
    "doi": "https://doi.org/10.1145/3451527",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Ricky Laishram; Jeremy Wendt; Sucheta Soundarajan",
    "corresponding_authors": "",
    "abstract": "In this article, we consider the problem of crawling a multiplex network to identify the community structure of a layer-of-interest. A multiplex network is one where there are multiple types of relationships between the nodes. In many multiplex networks, some layers might be easier to explore (in terms of time, money etc.). We propose MCS+ , an algorithm that can use the information from the easier to explore layers to help in the exploration of a layer-of-interest that is expensive to explore. We consider the goal of exploration to be generating a sample that is representative of the communities in the complete layer-of-interest. This work has practical applications in areas such as exploration of dark (e.g., criminal) networks, online social networks, biological networks, and so on. For example, in a terrorist network, relationships such as phone records, e-mail records, and so on are easier to collect; in contrast, data on the face-to-face communications are much harder to collect, but also potentially more valuable. We perform extensive experimental evaluations on real-world networks, and we observe that MCS+ consistently outperforms the best baseline—the similarity of the sample that MCS+ generates to the real network is up to three times that of the best baseline in some networks. We also perform theoretical and experimental evaluations on the scalability of MCS+ to network properties, and find that it scales well with the budget, number of layers in the multiplex network, and the average degree in the original network.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3186735807",
    "type": "article"
  },
  {
    "title": "Con&amp;Net: A Cross-Network Anchor Link Discovery Method Based on Embedding Representation",
    "doi": "https://doi.org/10.1145/3469083",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Xueyuan Wang; Hongpo Zhang; Zongmin Wang; Yaqiong Qiao; Jiangtao Ma; Honghua Dai",
    "corresponding_authors": "",
    "abstract": "Cross-network anchor link discovery is an important research problem and has many applications in heterogeneous social network. Existing schemes of cross-network anchor link discovery can provide reasonable link discovery results, but the quality of these results depends on the features of the platform. Therefore, there is no theoretical guarantee to the stability. This article employs user embedding feature to model the relationship between cross-platform accounts, that is, the more similar the user embedding features are, the more similar the two accounts are. The similarity of user embedding features is determined by the distance of the user features in the latent space. Based on the user embedding features, this article proposes an embedding representation-based method Con&amp;Net(Content and Network) to solve cross-network anchor link discovery problem. Con&amp;Net combines the user’s profile features, user-generated content (UGC) features, and user’s social structure features to measure the similarity of two user accounts. Con&amp;Net first trains the user’s profile features to get profile embedding. Then it trains the network structure of the nodes to get structure embedding. It connects the two features through vector concatenating, and calculates the cosine similarity of the vector based on the embedding vector. This cosine similarity is used to measure the similarity of the user accounts. Finally, Con&amp;Net predicts the link based on similarity for account pairs across the two networks. A large number of experiments in Sina Weibo and Twitter networks show that the proposed method Con&amp;Net is better than state-of-the-art method. The area under the curve (AUC) value of the receiver operating characteristic (ROC) curve predicted by the anchor link is 11% higher than the baseline method, and Precision@30 is 25% higher than the baseline method.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3200124999",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue on Large-Scale Data Mining",
    "doi": "https://doi.org/10.1145/1921632.1921633",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Jimeng Sun; Yan Liu; Jie Tang; Chid Apte",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1965925416",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue ACM SIGKDD 2012",
    "doi": "https://doi.org/10.1145/2513092.2513093",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Deepak Agarwal; Rich Caruana; Jian Pei; Ke Wang",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2129667919",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2541268",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233466943",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1993077",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Document understanding techniques such as document clustering and multidocument summarization have been receiving much attention recently. Current document clustering methods usually represent the given collection of documents as a document-term matrix ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250388061",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2499907",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We study how links are formed in social networks. In particular, we focus on investigating how a reciprocal (two-way) link, the basic relationship in social networks, is developed from a parasocial (one-way) relationship and how the relationships ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254800227",
    "type": "paratext"
  },
  {
    "title": "ACM TKDD special issue ACM SIGKDD 2007 and ACM SIGKDD 2008",
    "doi": "https://doi.org/10.1145/1631162.1631163",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Heikki Mannila; Dimitrios Gunopulos",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2004927758",
    "type": "article"
  },
  {
    "title": "Special issue on best of SIGKDD 2011",
    "doi": "https://doi.org/10.1145/2382577.2382578",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Joydeep Ghosh; Padhraic Smyth; Andrew Tomkins; Rich Caruana",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2036702132",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2297456",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The scalability of learning algorithms has always been a central concern for data mining researchers, and nowadays, with the rapid increase in data storage capacities and availability, its importance has increased. To this end, sampling has been studied ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230305187",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2362383",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Dependency analysis is a typical approach for Bayesian network learning, which infers the structures of Bayesian networks by the results of a series of conditional independence (CI) tests. In practice, testing independence conditioning on large sets ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230363196",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1552303",
    "publication_date": "2009-07-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Background: We recently described “Author-ity,” a model for estimating the probability that two articles in MEDLINE, sharing the same author name, were written by the same individual. Features include shared title words, journal name, coauthors, medical ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235290497",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1514888",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236537456",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1839490",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The exponential number of possible subgraphs makes the problem of frequent subgraph mining a challenge. The set of maximal frequent subgraphs is much smaller to that of the set of frequent subgraphs providing ample scope for pruning. MARGIN is a maximal ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240198146",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1870096",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we study the problem of Web user profiling, which is aimed at finding, extracting, and fusing the “semantic”-based user profile from the Web. Previously, Web user profiling was often undertaken by creating a list of keywords for the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243019868",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1857947",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Behavioral targeting (BT) leverages historical user behavior to select the ads most relevant to users to display. The state-of-the-art of BT derives a linear Poisson regression model from fine-grained user behavioral data and predicts click-through rate ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244455438",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1754428",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248014501",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2382577",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Deemed “one of the top ten data mining mistakes”, leakage is the introduction of information about the data mining target that should not be legitimately available to mine from. In addition to our own industry experience with real-life projects, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250912640",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1460797",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we explore a novel sampling model, called feature preserved sampling (FPS) that sequentially generates a high-quality sample over sliding windows. The sampling quality we consider refers to the degree of consistency between the sample ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250943672",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1631162",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Interaction graphs are ubiquitous in many fields such as bioinformatics, sociology and physical sciences. There have been many studies in the literature targeted at studying and mining these graphs. However, almost all of them have studied these graphs ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251439510",
    "type": "paratext"
  },
  {
    "title": "Introduction to special issue ACM SIGKDD 2006",
    "doi": "https://doi.org/10.1145/1297332.1297333",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Roberto Bayardop; Kristin P. Bennett; Gautam Das; Dimitrios Gunopulos; Johannes Gehrke",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2025663195",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1324172",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232039869",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3058790",
    "publication_date": "2017-04-14",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The betweenness and closeness metrics are widely used metrics in many network analysis applications. Yet, they are expensive to compute. For that reason, making the betweenness and closeness centrality computations faster is an important and well-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234707576",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3119906",
    "publication_date": "2017-08-21",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This issue contains the best papers from the ACM KDD Conference 2016. As is customary at KDD, special issue papers are invited only from the research track. The top-ranked papers from the KDD 2016 conference are included in this issue. This issue ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244089792",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1342320",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "High-throughput biological screens are yielding ever-growing streams of information about multiple aspects of cellular activity. As more and more categories of datasets come online, there is a corresponding multitude of ways in which inferences can be ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248831732",
    "type": "paratext"
  },
  {
    "title": "DeltaShield: Information Theory for Human- Trafficking Detection",
    "doi": "https://doi.org/10.1145/3563040",
    "publication_date": "2023-02-07",
    "publication_year": 2023,
    "authors": "Catalina Vajiac; Meng-Chieh Lee; Aayushi Kulshrestha; Sacha Lévy; Namyong Park; Andreas Olligschlaeger; Cara Jones; Reihaneh Rabbany; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "Given a million escort advertisements, how can we spot near-duplicates? Such micro-clusters of ads are usually signals of human trafficking (HT). How can we summarize them to convince law enforcement to act? Spotting micro-clusters of near-duplicate documents is useful in multiple, additional settings, including spam-bot detection in Twitter ads, plagiarism, and more. We present InfoShield , which makes the following contributions: practical , being scalable and effective on real data; parameter-free and principled , requiring no user-defined parameters; interpretable , finding a document to be the cluster representative, highlighting all the common phrases, and automatically detecting “slots” (i.e., phrases that differ in every document); and generalizable , beating or matching domain-specific methods in Twitter bot detection and HT detection, respectively, as well as being language independent. Interpretability is particularly important for the anti-HT domain, where law enforcement must visually inspect ads. Our experiments on real data show that InfoShield correctly identifies Twitter bots with an F1 score over 90% and detects HT ads with 84% precision. Moreover, it is scalable, requiring about 8 hours for 4 million documents on a stock laptop. Our incremental version, DeltaShield , allows for fast, incremental updates, with minor loss of accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4319349118",
    "type": "article"
  },
  {
    "title": "Kernel Fisher Dictionary Transfer Learning",
    "doi": "https://doi.org/10.1145/3588575",
    "publication_date": "2023-03-22",
    "publication_year": 2023,
    "authors": "Linrui Shi; Zheng Zhang; Zizhu Fan; Chao Xi; Zhengming Li; Gaochang Wu",
    "corresponding_authors": "",
    "abstract": "Dictionary learning is an efficient knowledge representation method that can learn the essential features of data. Traditional dictionary learning methods are difficult to obtain nonlinear information when processing large-scale and high-dimensional datasets. While most dictionary learning algorithms are based on the assumption that the training data and test data have the same feature distribution, which is not always true in practical applications. To address the above problems, we propose the Kernel Fisher Dictionary Transfer Learning (KFDTL) algorithm. First, we map each sample to high-dimensional space through kernel mapping and use any dictionary learning algorithm to learn the essential features. Then, the feature-based transfer learning method is performed to predict the labels of the target samples. This method includes three main contributions: (1) KFDTL constructs a discriminative Fisher embedding model to make the same class samples have similar coding coefficients; (2) Based on the relationship between profiles and atoms, KFDTL constructs an adaptive model that adapts source domain samples to target domain samples; (3) The kernel method is used to efficiently solve nonlinear problems. Experiments on a large number of public image datasets have proved the effectiveness of the proposed method. The source code of the proposed method is available at https://github.com/zzfan3/KFDTL .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4353090787",
    "type": "article"
  },
  {
    "title": "Self-supervision for Tabular Data by Learning to Predict Additive Homoskedastic Gaussian Noise as Pretext",
    "doi": "https://doi.org/10.1145/3594720",
    "publication_date": "2023-05-01",
    "publication_year": 2023,
    "authors": "Tahir Syed; Behroz Mirza",
    "corresponding_authors": "",
    "abstract": "The lack of scalability of data annotation translates to the need to decrease dependency on labels. Self-supervision offers a solution with data training themselves. However, it has received relatively less attention on tabular data, data that drive a large proportion of business and application domains. This work, which we name the Statistical Self-Supervisor (SSS), proposes a method for self-supervision on tabular data by defining a continuous perturbation as pretext. It enables a neural network to learn representations by learning to predict the level of additive isotropic Gaussian noise added to inputs. The choice of the pretext transformation is motivated by intrinsic characteristics of a neural network fundamentally performing linear fits under the widely adopted assumption of Gaussianity in its fitting error and the preservation of locality of a data example on the data manifold in the presence of small random perturbations. The transform condenses information in the generated representations, making them better employable for further task-specific prediction as evidenced by performance improvement of the downstream classifier. To evaluate the persistence of performance under low-annotation settings, SSS is evaluated against different levels of label availability to the downstream classifier (1% to 100%) and benchmarked against self- and semi-supervised methods. At the most label-constrained, 1% setting, we report a maximum increase of at least 2.5% against the next-best semi-supervised competing method. We report an increase of more than 1.5% against self-supervised state of the art. Ablation studies also reveal that increasing label availability from 0% to 1% results in a maximum increase of up to 50% on either of the five performance metrics and up to 15% thereafter, indicating diminishing returns in additional annotation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4367605179",
    "type": "article"
  },
  {
    "title": "A Hybrid Continuous-Time Dynamic Graph Representation Learning Model by Exploring Both Temporal and Repetitive Information",
    "doi": "https://doi.org/10.1145/3596447",
    "publication_date": "2023-05-08",
    "publication_year": 2023,
    "authors": "Xiaona Li; Zhu Wang; Xindong Chen; Bin Guo; Zhiwen Yu",
    "corresponding_authors": "",
    "abstract": "Recently, dynamic graph representation learning has attracted more and more attention from both academic and industrial communities due to its capabilities of capturing different real-world phenomena. For a dynamic graph represented as a sequence of timestamped events, there are two kinds of evolutionary essences: temporal and repetitive information. At present, the temporal information of interactions (e.g., timestamps) have been deeply explored. However, as another vital nature of dynamic graphs, the repetitive information of interactions between two nodes is neglected, which may lead to inaccurate node representation. To address this issue, we propose a novel continuous-time dynamic graph representation learning model, which consists of a node-level-memory based module, a historical high-order neighborhood based vertical aggregation module and a repetitive-topological information based horizontal aggregation module. In particular, to characterize the evolving pattern of the repetitive information of interactions between a pair of nodes, we put forward a repetitive-interaction based attention mechanism to integrate the two key attributes (i.e., the content and the number of interactions) of repetitive interactions at different moments, based on the insight that the repetitive behaviors of nodes are widespread and essential. We conduct extensive experiments including future link prediction tasks (for transductive and inductive learning) and dynamic node classification task, and results on three real-life dynamic graph datasets demonstrate that the proposed method significantly outperforms state-of-the-art baselines, for both observed nodes and new ones.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4375865618",
    "type": "article"
  },
  {
    "title": "An Efficient Transfer Learning Method with Auxiliary Information",
    "doi": "https://doi.org/10.1145/3612930",
    "publication_date": "2023-08-09",
    "publication_year": 2023,
    "authors": "Bo Liu; Liangjiao Li; Yanshan Xiao; Kai Wang; Jian Hu; Junrui Liu; Qihang Chen; Ruiguang Huang",
    "corresponding_authors": "",
    "abstract": "Transfer learning (TL) is an information reuse learning tool, which can help us learn better classification effect than traditional single task learning, because transfer learning can share information within the task-to-task model. Most TL algorithms are studied in the field of data improvement, doing some data extraction and transformation. However, it ignores that existing the additional information to improve the model’s accuracy, like Universum samples in the training data with privileged information. In this article, we focus on considering prior data to improve the TL algorithm, and the additional features also called privileged information are incorporated into the learning to improve the learning paradigm. In addition, we also carry out the Universum samples which do not belong to any indicated categories into the transfer learning paradigm to improve the utilization of prior knowledge. We propose a new TL Model (PU-TLSVM), in which each task with corresponding privileged features and Universum data is considered in the proposed model, so as to apply tasks with a priori data to the training stage. Then, we use Lagrange duality theorem to optimize our model to obtain the optimal discriminant for target task classification. Finally, we make a lot of predictions and tests to compare the actual effectiveness of the proposed method with the previous methods. The experiment results indicate that the proposed method is more effective and robust than other baselines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4385708051",
    "type": "article"
  },
  {
    "title": "Cross-modal Multiple Granularity Interactive Fusion Network for Long Document Classification",
    "doi": "https://doi.org/10.1145/3631711",
    "publication_date": "2023-11-06",
    "publication_year": 2023,
    "authors": "Tengfei Liu; Yongli Hu; Junbin Gao; Yanfeng Sun; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Long Document Classification (LDC) has attracted great attention in Natural Language Processing and achieved considerable progress owing to the large-scale pre-trained language models. In spite of this, as a different problem from the traditional text classification, LDC is far from being settled. Long documents, such as news and articles, generally have more than thousands of words with complex structures. Moreover, compared with flat text, long documents usually contain multi-modal content of images, which provide rich information but not yet being utilized for classification. In this article, we propose a novel cross-modal method for long document classification, in which multiple granularity feature shifting networks are proposed to integrate the multi-scale text and visual features of long documents adaptively. Additionally, a multi-modal collaborative pooling block is proposed to eliminate redundant fine-grained text features and simultaneously reduce the computational complexity. To verify the effectiveness of the proposed model, we conduct experiments on the Food101 dataset and two constructed multi-modal long document datasets. The experimental results show that the proposed cross-modal method outperforms the single-modal text methods and defeats the state-of-the-art related multi-modal baselines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388412500",
    "type": "article"
  },
  {
    "title": "Causality-based fair multiple decision by response functions",
    "doi": "https://doi.org/10.1145/3632529",
    "publication_date": "2023-11-13",
    "publication_year": 2023,
    "authors": "Cong Su; Guoxian Yu; Yongqing Zheng; Jun Wang; Zhengtian Wu; Xiangliang Zhang; Carlotta Domeniconi",
    "corresponding_authors": "",
    "abstract": "A recent trend of fair machine learning is to build a decision model subjected to causality-based fairness requirements, which concern with the causality between sensitive attributes and decisions. Almost all (if not all) solutions focus on a single fair decision model and assume no hidden confounder to model causal effects in a too simplified way. However, multiple interdependent decision models are actually used and discrimination may transmit among them. The hidden confounder is another inescapable fact and causal effects cannot be computed from observational data in the unidentifiable situation. To address these problems, we propose a method called CMFL (Causality-based Multiple Fairness Learning). CMFL parameterizes the causal model by response-function variables, whose distributions capture the randomness of causal models. CMFL treats each classifier as a soft intervention to infer the post-intervention distribution, and combines the fairness constraints with the classification loss to train multiple decision classifiers. In this way, all classifiers can make approximately fair decisions. Experiments on synthetic and benchmark datasets confirm its effectiveness, the response-function variables can deal with the unidentifiable issue and hidden confounders.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388638809",
    "type": "article"
  },
  {
    "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
    "doi": "https://doi.org/10.1145/3636512",
    "publication_date": "2023-12-07",
    "publication_year": 2023,
    "authors": "Weiyao Zhu; Ou Wu; Fengguang Su; Yingjun Deng",
    "corresponding_authors": "",
    "abstract": "‘‘Easy/hard sample” is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389438972",
    "type": "article"
  },
  {
    "title": "A Three-Stage Transferable and Generative Crowdsourced Comment Integration Framework Based on Zero-and-Few-Shot Learning with Domain Distribution Alignment",
    "doi": "https://doi.org/10.1145/3636511",
    "publication_date": "2023-12-11",
    "publication_year": 2023,
    "authors": "Huan Rong; Xin Yu; Tinghuai Ma; Victor S. Sheng; Yang Zhou; Mznah Al‐Rodhaan",
    "corresponding_authors": "",
    "abstract": "Online shopping has become a crucial way to encourage daily consumption, where the User-generated, or crowdsourced product comments, can offer a broad range of feedback on e-commerce products. As a result, integrating critical opinions or major attitudes from the crowdsourced comments can provide valuable feedback for marketing strategy adjustment or product-quality monitoring. Unfortunately, the scarcity of annotated ground truth on the integrated comment, or the limited gold integration reference, has incurred the infeasibility of the regular supervised-learning-based comment integration. To resolve this problem, in this article, inspired by the principle of Transfer Learning, we propose a three-stage transferable and generative crowdsourced comment integration framework ( TTGCIF ) based on zero-and-few-shot learning with the support of domain distribution alignment. The proposed framework aims at generating abstractive integrated comment in target domain via the enhanced neural text generation model, by referring the available integration resource in related source domains, to avoid the exhausted effort on resource annotation devoted to the target domain. Specifically, at the first stage, to enhance the domain transferability, representations on the crowdsourced comments have been aligned up between the source and target domain, by minimizing the domain distribution discrepancy in the kernel space. At the second stage, Zero-shot comment integration mechanism has been adopted to deal with the dilemma that none of the gold integration reference may be available in target domain. In other words, taking the sample-level semantic prototype as input, the enhanced neural text generation model in TTGCIF is trained to learn data semantic association among different domains via semantic prototype transduction, so that the “ unlabeled ” crowdsourced comments in target domain can be associated with existing integration references in related source domains. At the third stage, based on the parameters trained at the second stage, fast domain adaptation mechanism in a Few-shot manner has also been adopted by seeking most potential parameters along the gradient direction constrained by instances across multiple source domains. In this way, parameters in TTGCIF can be sensitive to any alteration on training data, ensuring that even if only few annotated resource in target domain are available for “Fine-tune,” TTGCIF can still react promptly to achieve effective target domain adaptation. According to the experimental results, TTGCIF can achieve the best transferable product comment integration performance in target domain, with fast and stable domain adaption effect depending on no more than 10% annotated resource in target domain. More importantly, even if TTGCIF has not been fine-tuned on the target domain, yet by referring to the available integration resource in related source domains, the integrated comments generated by TTGCIF on the target domain are still superior to those generated by models already fine-tuned on the target domain.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389560762",
    "type": "article"
  },
  {
    "title": "Brief Announcement: On Augmented Graph Navigability",
    "doi": null,
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Pierre Fraigniaud; Emmanuelle Lebhar; Zvi Lotker",
    "corresponding_authors": "",
    "abstract": "It is known that graphs of doubling dimension O(loglogn) can be augmented to become navigable. We show that for doubling dimension >> log log n, an infinite family of graphs cannot be augmented to become navigable. Our proof uses a counting argument which enable us to consider any kind of augmentations. In particular we do not restrict our analysis to the case of symmetric distributions, nor to distributions for which the choice of the long range link at a node must be independent from the choices of long range links at other nodes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2567130773",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3181707",
    "publication_date": "2018-02-13",
    "publication_year": 2018,
    "authors": "Matthijs van Leeuwen; Polo Chau; Jilles Vreeken; Dafna Shahaf; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2789220918",
    "type": "editorial"
  },
  {
    "title": "Performance Bounds of Decentralized Search in Expert Networks for Query Answering",
    "doi": "https://doi.org/10.1145/3300230",
    "publication_date": "2019-03-13",
    "publication_year": 2019,
    "authors": "Liang Ma; Mudhakar Srivatsa; Derya Cansever; Xifeng Yan; Sue E. Kase; Michelle Vanni",
    "corresponding_authors": "",
    "abstract": "Expert networks are formed by a group of expert-professionals with different specialties to collaboratively resolve specific queries posted to the network. In such networks, when a query reaches an expert who does not have sufficient expertise, this query needs to be routed to other experts for further processing until it is completely solved; therefore, query answering efficiency is sensitive to the underlying query routing mechanism being used. Among all possible query routing mechanisms, decentralized search, operating purely on each expert’s local information without any knowledge of network global structure, represents the most basic and scalable routing mechanism, which is applicable to any network scenarios even in dynamic networks. However, there is still a lack of fundamental understanding of the efficiency of decentralized search in expert networks. In this regard, we investigate decentralized search by quantifying its performance under a variety of network settings. Our key findings reveal the existence of network conditions, under which decentralized search can achieve significantly short query routing paths (i.e., between O (log n ) and O (log 2 n ) hops, n : total number of experts in the network). Based on such theoretical foundation, we further study how the unique properties of decentralized search in expert networks are related to the anecdotal small-world phenomenon. In addition, we demonstrate that decentralized search is robust against estimation errors introduced by misinterpreting the required expertise levels. The developed performance bounds, confirmed by real datasets, are able to assist in predicting network performance and designing complex expert networks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2922186813",
    "type": "article"
  },
  {
    "title": "Addendum to the Special Issue on Interactive Data Exploration and Analytics (TKDD, Vol. 12, Iss. 1): Introduction by the Guest Editors",
    "doi": "https://doi.org/10.1145/3298786",
    "publication_date": "2019-01-23",
    "publication_year": 2019,
    "authors": "Matthijs van Leeuwen; Polo Chau; Jilles Vreeken; Dafna Shahaf; Christos Faloutsos",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2954805558",
    "type": "article"
  },
  {
    "title": "結果指向予測プロセス監視レビューとベンチマーク【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Teinemaa Irene; Dumas Marlon; Rosa Marcello La; Maggi Fabrizio Maria",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3162523501",
    "type": "article"
  },
  {
    "title": "質問応答のためのエキスパートネットワークにおける分散探索の性能限界【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Ma Liang; Mudhakar Srivatsa; Cansever Derya; Yan Xifeng; Kase Sue; Vanni Michelle",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3163788417",
    "type": "article"
  },
  {
    "title": "異常値検出への応用による多視点低ランク解析【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2018-01-01",
    "publication_year": 2018,
    "authors": "Li Sheng; Shao Ming; Fu Yun",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3171284353",
    "type": "article"
  },
  {
    "title": "大規模ネットワークにおける局所コミュニティ検出のためのKrylov部分空間近似【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Kun He; Pan Shi; Bindel David; E Hopcroft John",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3175910792",
    "type": "article"
  },
  {
    "title": "暗黙フィードバックを組み込んだトピックおよび社会的潜在因子による協調フィルタリング【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2018-01-01",
    "publication_year": 2018,
    "authors": "HU Guang-Neng; Xinyu Dai; Qiu Feng-Yu; Xia Rui; Li Tao; Huang Shu-Jian; Jiajun Chen",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3181353083",
    "type": "article"
  },
  {
    "title": "HIVE-COTEによる時系列分類 変換ベースアンサンブルの階層的投票集合【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2018-01-01",
    "publication_year": 2018,
    "authors": "Jason Lines; Taylor Sarah; Bagnall Anthony",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3181760951",
    "type": "article"
  },
  {
    "title": "異種情報ネットワークにおける優先関係分析【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2018-01-01",
    "publication_year": 2018,
    "authors": "Liang Jiongqian; Ajwani Deepak; K Nicholson Patrick; Alessandra Sala; Parthasarathy Srinivasan",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3192352892",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3208362",
    "publication_date": "2018-07-13",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Given a portfolio of stocks or a series of frames in a video how do we detect significant changes in a group of values for real-time applications? In this article, we formalize the problem of sequentially detecting temporal changes in a group of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230835635",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3366748",
    "publication_date": "2019-12-17",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Aspect category sentiment analysis (ACSA) is an underexploited subtask in aspect level sentiment analysis. It aims to identify the sentiment of predefined aspect categories. The main challenge in ACSA comes from the fact that the aspect category may not ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233329849",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3234931",
    "publication_date": "2018-07-20",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A recent experimental evaluation assessed 19 time series classification (TSC) algorithms and found that one was significantly more accurate than all others: the Flat Collective of Transformation-based Ensembles (Flat-COTE). Flat-COTE is an ensemble that ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236064823",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3375789",
    "publication_date": "2019-12-13",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237168558",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3331063",
    "publication_date": "2019-07-17",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "With the growing popularity of shared resources, large volumes of complex data of different types are collected automatically. Traditional data mining algorithms generally have problems and challenges including huge memory cost, low processing speed, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238071956",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3271478",
    "publication_date": "2018-10-17",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Clustering ensembles combine multiple partitions of data into a single clustering solution. It is an effective technique for improving the quality of clustering results. Current clustering ensemble algorithms are usually built on the pairwise agreements ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239340209",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3319626",
    "publication_date": "2019-06-04",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we focus on the problem of learning a Bayesian network over distributed data stored in a commodity cluster. Specifically, we address the challenge of computing the scoring function over distributed data in an efficient and scalable ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241111721",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3301280",
    "publication_date": "2019-01-29",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In many applications, an anomaly detection system presents the most anomalous data instance to a human analyst, who then must determine whether the instance is truly of interest (e.g., a threat in a security setting). Unfortunately, most anomaly ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243113592",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3178544",
    "publication_date": "2018-03-13",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Personality is a combination of all the attributes—behavioral, temperamental, emotional, and mental—that characterizes a unique individual. Ability to identify personalities of people has always been of great interest to the researchers due to its ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243523023",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3364623",
    "publication_date": "2019-10-12",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Air quality has gained much attention in recent years and is of great importance to protecting people’s health. Due to the influence of multiple factors, the limited air quality monitoring stations deployed in cities are unable to provide fine-grained ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248238336",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3343141",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The Sparse Representation Classifier, the Collaborative Representation Classifier (CRC), and the Two Phase Test Sample Sparse Representation (TPTSSR) classifier were introduced in recent times. All these frameworks are supervised and passive in the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255875962",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3178542",
    "publication_date": "2018-02-23",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Thanks to the capturing devices cost reduction and the advent of social networks, the size of image collections is becoming extremely huge. Many works in the literature have addressed the indexing of large image collections for search purposes. However, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256430253",
    "type": "paratext"
  },
  {
    "title": "Introduction to the Special Issue on the Best Papers from KDD 2018",
    "doi": "https://doi.org/10.1145/3407901",
    "publication_date": "2020-08-17",
    "publication_year": 2020,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3056538984",
    "type": "article"
  },
  {
    "title": "ブルーム行列とブルームベクトルによる多重集合マッチング【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2020-01-01",
    "publication_year": 2020,
    "authors": "Concas Francesco; Xu Pengfei; A Hoque Mohammad; Lu Jiaheng; Sasu Tarkoma",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3178063080",
    "type": "article"
  },
  {
    "title": "REMIAN実時間およびエラー耐性欠損値補完【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2020-01-01",
    "publication_year": 2020,
    "authors": "Ma Qian; Gu Yu; Lee Wang-Chien; Yu Ge; Liu Hongbo; Wu Xindong",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3193754798",
    "type": "article"
  },
  {
    "title": "TRACE: Travel Reinforcement Recommendation Based on Location-Aware Context Extraction",
    "doi": null,
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Zhe Fu; Li Yu; Xi Niu",
    "corresponding_authors": "",
    "abstract": "As the popularity of online travel platforms increases, users tend to make ad-hoc decisions on places to visit rather than preparing the detailed tour plans in advance. Under the situation of timeliness and uncertainty of users’ demand, how to integrate real-time context into a dynamic and personalized recommendations have become a key issue in travel recommender system. In this paper, by integrating the users’ historical preferences and real-time context, a location-aware recommender system called TRACE (Travel Reinforcement Recommendations Based on Location-Aware Context Extraction) is proposed. It captures users’ features based on location-aware context learning model, and makes dynamic recommendations based on reinforcement learning. Specifically, this research: (1) designs a travel reinforcing recommender system based on an Actor-Critic framework, which can dynamically track the user preference shifts and optimize the recommender system performance; (2) proposes a location-aware context learning model, which aims at extracting user context from real-time location and then calculating the impacts of nearby attractions on users’ preferences; and (3) conducts both offline and online experiments. Our proposed model achieves the best performance in both of the two experiments, which demonstrates that tracking the users’ preference shifts based on real-time location is valuable for improving the recommendation results.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3205606719",
    "type": "article"
  },
  {
    "title": "Uncovering the Local Hidden Community Structure in Social Networks",
    "doi": "https://doi.org/10.1145/3567597",
    "publication_date": "2022-10-17",
    "publication_year": 2022,
    "authors": "Meng Wang; Boyu Li; Kun He; John E. Hopcroft",
    "corresponding_authors": "",
    "abstract": "Hidden community is a useful concept proposed recently for social network analysis. Hidden communities indicate some weak communities whose most members also belong to other stronger dominant communities. Dominant communities could form a layer that partitions all the individuals of a network, and hidden communities could form other layer(s) underneath. These layers could be natural structures in the real-world networks like students grouped by major, minor, hometown, and so on. To handle the rapid growth of network scale, in this work, we explore the detection of hidden communities from the local perspective, and propose a new method that detects and boosts each layer iteratively on a subgraph sampled from the original network. We first expand the seed set from a single seed node based on our modified local spectral method and detect an initial dominant local community. Then we temporarily remove the members of this community as well as their connections to other nodes, and detect all the neighborhood communities in the remaining subgraph, including some “broken communities” that only contain a fraction of members in the original network. The local community and neighborhood communities form a dominant layer, and by reducing the edge weights inside these communities, we weaken this layer’s structure to reveal the hidden layers. Eventually, we repeat the whole process, and all communities containing the seed node can be detected and boosted iteratively. We theoretically show that our method can avoid some situations that a broken community and the local community are regarded as one community in the subgraph, leading to the inaccuracy of detection which can be caused by global hidden community detection methods. Extensive experiments show that our method could significantly outperform the state-of-the-art baselines designed for either global hidden community detection or multiple local community detection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4200634453",
    "type": "article"
  },
  {
    "title": "Evidence Transfer: Learning Improved Representations According to External Heterogeneous Task Outcomes",
    "doi": "https://doi.org/10.1145/3502732",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Athanasios Davvetas; Iraklis Klampanos; Spiros Skiadopoulos; Vangelis Karkaletsis",
    "corresponding_authors": "",
    "abstract": "Unsupervised representation learning tends to produce generic and reusable latent representations. However, these representations can often miss high-level features or semantic information, since they only observe the implicit properties of the dataset. On the other hand, supervised learning frameworks learn task-oriented latent representations that may not generalise in other tasks or domains. In this article, we introduce evidence transfer, a deep learning method that incorporates the outcomes of external tasks in the unsupervised learning process of an autoencoder. External task outcomes also referred to as categorical evidence, are represented by categorical variables, and are either directly or indirectly related to the primary dataset—in the most straightforward case they are the outcome of another task on the same dataset. Evidence transfer allows the manipulation of generic latent representations in order to include domain or task-specific knowledge that will aid their effectiveness in downstream tasks. Evidence transfer is robust against evidence of low quality and effective when introduced with related, corresponding, or meaningful evidence.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220737888",
    "type": "article"
  },
  {
    "title": "Online Learning Bipartite Matching with Non-stationary Distributions",
    "doi": "https://doi.org/10.1145/3502734",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Weirong Chen; Jiaqi Zheng; Haoyu Yu; Guihai Chen; Yixin Chen; Dongsheng Li",
    "corresponding_authors": "",
    "abstract": "Online bipartite matching has attracted wide interest since it can successfully model the popular online car-hailing problem and sharing economy. Existing works consider this problem under either adversary setting or i.i.d. setting. The former is too pessimistic to improve the performance in the general case; the latter is too optimistic to deal with the varying distribution of vertices. In this article, we initiate the study of the non-stationary online bipartite matching problem, which allows the distribution of vertices to vary with time and is more practical. We divide the non-stationary online bipartite matching problem into two subproblems, the matching problem and the selecting problem, and solve them individually. Combining Batch algorithms and deep Q-learning networks, we first construct a candidate algorithm set to solve the matching problem. For the selecting problem, we use a classical online learning algorithm, Exp3, as a selector algorithm and derive a theoretical bound. We further propose CDUCB as a selector algorithm by integrating distribution change detection into UCB. Rigorous theoretical analysis demonstrates that the performance of our proposed algorithms is no worse than that of any candidate algorithms in terms of competitive ratio. Finally, extensive experiments show that our proposed algorithms have much higher performance for the non-stationary online bipartite matching problem comparing to the state-of-the-art.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220981187",
    "type": "article"
  },
  {
    "title": "Toward Quality of Information Aware Distributed Machine Learning",
    "doi": "https://doi.org/10.1145/3522591",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Houping Xiao; Shiyu Wang",
    "corresponding_authors": "",
    "abstract": "In the era of big data, data are usually distributed across numerous connected computing and storage units (i.e., nodes or workers). Under such an environment, many machine learning problems can be reformulated as a consensus optimization problem, which consists of one objective and constraint terms splitting into N parts (each corresponds to a node). Such a problem can be solved efficiently in a distributed manner via Alternating Direction Method of Multipliers ( ADMM ). However, existing consensus optimization frameworks assume that every node has the same quality of information (QoI) , i.e., the data from all the nodes are equally informative for the estimation of global model parameters. As a consequence, they may lead to inaccurate estimates in the presence of nodes with low QoI. To overcome this challenge, in this article, we propose a novel consensus optimization framework for distributed machine-learning that incorporates the crucial metric, QoI. Theoretically, we prove that the convergence rate of the proposed framework is linear to the number of iterations, but has a tighter upper bound compared with ADMM . Experimentally, we show that the proposed framework is more efficient and effective than existing ADMM -based solutions on both synthetic and real-world datasets due to its faster convergence rate and higher accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4221117133",
    "type": "article"
  },
  {
    "title": "Spectral Ranking Regression",
    "doi": "https://doi.org/10.1145/3530693",
    "publication_date": "2022-04-12",
    "publication_year": 2022,
    "authors": "İlkay Yıldız; Jennifer Dy; Deniz Erdoğmuş; Susan Ostmo; J. Peter Campbell; Michael F. Chiang; Stratis Ioannidis",
    "corresponding_authors": "",
    "abstract": "We study the problem of ranking regression, in which a dataset of rankings is used to learn Plackett–Luce scores as functions of sample features. We propose a novel spectral algorithm to accelerate learning in ranking regression. Our main technical contribution is to show that the Plackett–Luce negative log-likelihood augmented with a proximal penalty has stationary points that satisfy the balance equations of a Markov Chain. This allows us to tackle the ranking regression problem via an efficient spectral algorithm by using the Alternating Directions Method of Multipliers (ADMM). ADMM separates the learning of scores and model parameters, and in turn, enables us to devise fast spectral algorithms for ranking regression via both shallow and deep neural network (DNN) models. For shallow models, our algorithms are up to 579 times faster than the Newton’s method. For DNN models, we extend the standard ADMM via a Kullback–Leibler proximal penalty and show that this is still amenable to fast inference via a spectral approach. Compared to a state-of-the-art siamese network, our resulting algorithms are up to 175 times faster and attain better predictions by up to 26% Top-1 Accuracy and 6% Kendall-Tau correlation over five real-life ranking datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4226145819",
    "type": "article"
  },
  {
    "title": "A Novel Graph Indexing Approach for Uncovering Potential COVID-19 Transmission Clusters",
    "doi": "https://doi.org/10.1145/3538492",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Xuliang Zhu; Xin Huang; Longxu Sun; Jiming Liu",
    "corresponding_authors": "",
    "abstract": "The COVID-19 pandemic has caused the society lockdowns and a large number of deaths in many countries. Potential transmission cluster discovery is to find all suspected users with infections, which is greatly needed to fast discover virus transmission chains so as to prevent an outbreak of COVID-19 as early as possible. In this article, we study the problem of potential transmission cluster discovery based on the spatio-temporal logs. Given a query of patient user q and a timestamp of confirmed infection t q , the problem is to find all potential infected users who have close social contacts to user q before time t q . We motivate and formulate the potential transmission cluster model, equipped with a detailed analysis of transmission cluster property and particular model usability. To identify potential clusters, one straightforward method is to compute all close contacts on-the-fly, which is simple but inefficient caused by scanning spatio-temporal logs many times. To accelerate the efficiency, we propose two indexing algorithms by constructing a multigraph index and an advanced BCG-index. Leveraging two well-designed techniques of spatio-temporal compression and graph partition on bipartite contact graphs, our BCG-index approach achieves a good balance of index construction and online query processing to fast discover potential transmission cluster. We theoretically analyze and compare the algorithm complexity of three proposed approaches. Extensive experiments on real-world check-in datasets and COVID-19 confirmed cases in the United States validate the effectiveness and efficiency of our potential transmission cluster model and algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4281398247",
    "type": "article"
  },
  {
    "title": "CrowdAtlas: Estimating Crowd Distribution within the Urban Rail Transit System",
    "doi": "https://doi.org/10.1145/3558521",
    "publication_date": "2022-08-23",
    "publication_year": 2022,
    "authors": "E Jinlong; Mo Li; Jianqiang Huang",
    "corresponding_authors": "",
    "abstract": "While urban rail transit systems are playing an increasingly important role in meeting the transportation demands of people, precise awareness of how the human crowd is distributed within such a system is highly necessary, which serves a range of important applications including emergency response, transit recommendation, and commercial valuation. Most rail transit systems are closed systems where once entered the passengers are free to move around all stations and are difficult to track. In this article, we attempt to estimate the crowd distribution based only on the tap-in and tap-out records of all the rail riders. Specifically, we study Singapore MRT (Mass Rapid Transit) as a vehicle and leverage EZ-Link transit card records to estimate the crowd distribution. Guided by a key observation that the passenger inflows and arrival flows at different MRT stations and time are spatio-temporally correlated due to behavioral consistency of MRT riders, we design and implement a machine learning-based solution, CrowdAtlas, that captures MRT riders’ transition probabilities among stations and across time, and based on that accurately estimates the crowd distribution within the MRT system. Our comprehensive performance evaluations with both trace-driven studies and real-world experiments in MRT disruption cases demonstrate the effectiveness of CrowdAtlas.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4292738201",
    "type": "article"
  },
  {
    "title": "The 8M Algorithm from Today’s Perspective",
    "doi": "https://doi.org/10.1145/3428078",
    "publication_date": "2021-01-09",
    "publication_year": 2021,
    "authors": "Radim Bělohlávek; Martin Trnečka",
    "corresponding_authors": "",
    "abstract": "We provide a detailed analysis and a first complete description of 8M—an old but virtually unknown algorithm for Boolean matrix factorization. Even though the algorithm uses a rather limited insight into the factorization problem from today’s perspective, we demonstrate that its performance is reasonably good compared to the currently available algorithms. Our analysis reveals that this is due to certain concepts employed by 8M that are not exploited by the current algorithms. We discuss the prospect of these concepts, utilize them to improve two well-known current factorization algorithms, and, furthermore, propose an improvement of 8M itself, which significantly enhances the performance of the original 8M. Our findings are illustrated by experimental evaluation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3120351787",
    "type": "article"
  },
  {
    "title": "Demarcating Endogenous and Exogenous Opinion Dynamics: An Experimental Design Approach",
    "doi": "https://doi.org/10.1145/3449361",
    "publication_date": "2021-06-28",
    "publication_year": 2021,
    "authors": "Paramita Koley; Avirup Saha; Sourangshu Bhattacharya; Niloy Ganguly; Abir De",
    "corresponding_authors": "",
    "abstract": "The networked opinion diffusion in online social networks is often governed by the two genres of opinions— endogenous opinions that are driven by the influence of social contacts among users, and exogenous opinions which are formed by external effects like news and feeds. Accurate demarcation of endogenous and exogenous messages offers an important cue to opinion modeling, thereby enhancing its predictive performance. In this article, we design a suite of unsupervised classification methods based on experimental design approaches, in which, we aim to select the subsets of events which minimize different measures of mean estimation error. In more detail, we first show that these subset selection tasks are NP-Hard. Then we show that the associated objective functions are weakly submodular, which allows us to cast efficient approximation algorithms with guarantees. Finally, we validate the efficacy of our proposal on various real-world datasets crawled from Twitter as well as diverse synthetic datasets. Our experiments range from validating prediction performance on unsanitized and sanitized events to checking the effect of selecting optimal subsets of various sizes. Through various experiments, we have found that our method offers a significant improvement in accuracy in terms of opinion forecasting, against several competitors.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3126810968",
    "type": "article"
  },
  {
    "title": "Unsupervised Subspace Extraction via Deep Kernelized Clustering",
    "doi": "https://doi.org/10.1145/3459082",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Gyoung S. Na; Hyunju Chang",
    "corresponding_authors": "",
    "abstract": "Feature extraction has been widely studied to find informative latent features and reduce the dimensionality of data. In particular, due to the difficulty in obtaining labeled data, unsupervised feature extraction has received much attention in data mining. However, widely used unsupervised feature extraction methods require side information about data or rigid assumptions on the latent feature space. Furthermore, most feature extraction methods require predefined dimensionality of the latent feature space,which should be manually tuned as a hyperparameter. In this article, we propose a new unsupervised feature extraction method called Unsupervised Subspace Extractor ( USE ), which does not require any side information and rigid assumptions on data. Furthermore, USE can find a subspace generated by a nonlinear combination of the input feature and automatically determine the optimal dimensionality of the subspace for the given nonlinear combination. The feature extraction process of USE is well justified mathematically, and we also empirically demonstrate the effectiveness of USE for several benchmark datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3183138758",
    "type": "article"
  },
  {
    "title": "Communication from the Editor-in-Chief: State of the ACM Transactions on Knowledge Discovery from Data",
    "doi": "https://doi.org/10.1145/3463950",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Charų C. Aggarwal",
    "corresponding_authors": "Charų C. Aggarwal",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3183217277",
    "type": "article"
  },
  {
    "title": "A Latent Variable Augmentation Method for Image Categorization with Insufficient Training Samples",
    "doi": "https://doi.org/10.1145/3451165",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Luyue Lin; Xin Zheng; Bo Liu; Wei Chen; Yanshan Xiao",
    "corresponding_authors": "",
    "abstract": "Over the past few years, we have made great progress in image categorization based on convolutional neural networks (CNNs). These CNNs are always trained based on a large-scale image data set; however, people may only have limited training samples for training CNN in the real-world applications. To solve this problem, one intuition is augmenting training samples. In this article, we propose an algorithm called Lavagan ( La tent V ariables A ugmentation Method based on G enerative A dversarial N ets) to improve the performance of CNN with insufficient training samples. The proposed Lavagan method is mainly composed of two tasks. The first task is that we augment a number latent variables (LVs) from a set of adaptive and constrained LVs distributions. In the second task, we take the augmented LVs into the training procedure of the image classifier. By taking these two tasks into account, we propose a uniform objective function to incorporate the two tasks into the learning. We then put forward an alternative two-play minimization game to minimize this uniform loss function such that we can obtain the predictive classifier. Moreover, based on Hoeffding’s Inequality and Chernoff Bounding method, we analyze the feasibility and efficiency of the proposed Lavagan method, which manifests that the LV augmentation method is able to improve the performance of Lavagan with insufficient training samples. Finally, the experiment has shown that the proposed Lavagan method is able to deliver more accurate performance than the existing state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3184990416",
    "type": "article"
  },
  {
    "title": "Incremental Multi-source Feature Learning and its Applications in Spatio-temporal Event Prediction",
    "doi": null,
    "publication_date": "2021-12-01",
    "publication_year": 2021,
    "authors": "Liang Zhao; Yuyang Gao; Jieping Ye; Feng Chen; Yanfang Ye; Chang‐Tien Lu; Naren Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3197973320",
    "type": "article"
  },
  {
    "title": "Assessing Large-Scale Power Relations among Locations from Mobility Data",
    "doi": "https://doi.org/10.1145/3470770",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Lucas Santos de Oliveira; Pedro O. S. Vaz de Melo; Aline Carneiro Viana",
    "corresponding_authors": "",
    "abstract": "The pervasiveness of smartphones has shaped our lives, social norms, and the structure that dictates human behavior. They now directly influence how individuals demand resources or interact with network services. From this scenario, identifying key locations in cities is fundamental for the investigation of human mobility and also for the understanding of social problems. In this context, we propose the first graph-based methodology in the literature to quantify the power of Point-of-Interests (POIs) over its vicinity by means of user mobility trajectories. Different from literature, we consider the flow of people in our analysis, instead of the number of neighbor POIs or their structural locations in the city. Thus, we modeled POI’s visits using the multiflow graph model where each POI is a node and the transitions of users among POIs are a weighted direct edge. Using this multiflow graph model, we compute the attract, support, and independence powers . The attract power and support power measure how many visits a POI gathers from and disseminate over its neighborhood, respectively. Moreover, the independence power captures the capacity of a POI to receive visitors independently from other POIs. We tested our methodology on well-known university campus mobility datasets and validated on Location-Based Social Networks (LBSNs) datasets from various cities around the world. Our findings show that in university campus: (i) buildings have low support power and attract power ; (ii) people tend to move over a few buildings and spend most of their time in the same building; and (iii) there is a slight dependence among buildings, even those with high independence power receive user visits from other buildings on campus. Globally, we reveal that (i) our metrics capture places that impact the number of visits in their neighborhood; (ii) cities in the same continent have similar independence patterns; and (iii) places with a high number of visitation and city central areas are the regions with the highest degree of independence.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3200303119",
    "type": "article"
  }
]