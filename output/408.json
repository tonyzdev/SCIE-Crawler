[
  {
    "title": "ViennaRNA Package 2.0",
    "doi": "https://doi.org/10.1186/1748-7188-6-26",
    "publication_date": "2011-11-24",
    "publication_year": 2011,
    "authors": "Ronny Lorenz; Stephan Wolf; Christian Höner zu Siederdissen; Hakim Tafer; Christoph Flamm; Peter F. Stadler; Ivo L. Hofacker",
    "corresponding_authors": "Ronny Lorenz",
    "abstract": "Secondary structure forms an important intermediate level of description of nucleic acids that encapsulates the dominating part of the folding energy, is often well conserved in evolution, and is routinely used as a basis to explain experimental findings. Based on carefully measured thermodynamic parameters, exact dynamic programming algorithms can be used to compute ground states, base pairing probabilities, as well as thermodynamic properties. The ViennaRNA Package has been a widely used compilation of RNA secondary structure related computer programs for nearly two decades. Major changes in the structure of the standard energy model, the Turner 2004 parameters, the pervasive use of multi-core CPUs, and an increasing number of algorithmic variants prompted a major technical overhaul of both the underlying RNAlib and the interactive user programs. New features include an expanded repertoire of tools to assess RNA-RNA interactions and restricted ensembles of structures, additional output information such as centroid structures and maximum expected accuracy structures derived from base pairing probabilities, or z-scores for locally stable secondary structures, and support for input in fasta format. Updates were implemented without compromising the computational efficiency of the core algorithms and ensuring compatibility with earlier versions. The ViennaRNA Package 2.0, supporting concurrent computations via OpenMP, can be downloaded from http://www.tbi.univie.ac.at/RNA .",
    "cited_by_count": 4601,
    "openalex_id": "https://openalex.org/W2086561953",
    "type": "article"
  },
  {
    "title": "Jane: a new tool for the cophylogeny reconstruction problem",
    "doi": "https://doi.org/10.1186/1748-7188-5-16",
    "publication_date": "2010-02-03",
    "publication_year": 2010,
    "authors": "Chris Conow; Daniel C. Fielder; Yaniv Ovadia; Ran Libeskind-Hadas",
    "corresponding_authors": "",
    "abstract": "This paper describes the theory and implementation of a new software tool, called Jane, for the study of historical associations. This problem arises in parasitology (associations of hosts and parasites), molecular systematics (associations of orderings and genes), and biogeography (associations of regions and orderings). The underlying problem is that of reconciling pairs of trees subject to biologically plausible events and costs associated with these events. Existing software tools for this problem have strengths and limitations, and the new Jane tool described here provides functionality that complements existing tools.The Jane software tool uses a polynomial time dynamic programming algorithm in conjunction with a genetic algorithm to find very good, and often optimal, solutions even for relatively large pairs of trees. The tool allows the user to provide rich timing information on both the host and parasite trees. In addition the user can limit host switch distance and specify multiple host switch costs by specifying regions in the host tree and costs for host switches between pairs of regions. Jane also provides a graphical user interface that allows the user to interactively experiment with modifications to the solutions found by the program.Jane is shown to be a useful tool for cophylogenetic reconstruction. Its functionality complements existing tools and it is therefore likely to be of use to researchers in the areas of parasitology, molecular systematics, and biogeography.",
    "cited_by_count": 384,
    "openalex_id": "https://openalex.org/W2144454528",
    "type": "article"
  },
  {
    "title": "Space-efficient and exact de Bruijn graph representation based on a Bloom filter",
    "doi": "https://doi.org/10.1186/1748-7188-8-22",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Rayan Chikhi; Guillaume Rizk",
    "corresponding_authors": "",
    "abstract": "The de Bruijn graph data structure is widely used in next-generation sequencing (NGS). Many programs, e.g. de novo assemblers, rely on in-memory representation of this graph. However, current techniques for representing the de Bruijn graph of a human genome require a large amount of memory (≥30 GB).We propose a new encoding of the de Bruijn graph, which occupies an order of magnitude less space than current representations. The encoding is based on a Bloom filter, with an additional structure to remove critical false positives.An assembly software implementing this structure, Minia, performed a complete de novo assembly of human genome short reads using 5.7 GB of memory in 23 hours.",
    "cited_by_count": 344,
    "openalex_id": "https://openalex.org/W2105656684",
    "type": "article"
  },
  {
    "title": "Identification of alternative topological domains in chromatin",
    "doi": "https://doi.org/10.1186/1748-7188-9-14",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Darya Filippova; Rob Patro; Geet Duggal; Carl Kingsford",
    "corresponding_authors": "",
    "abstract": "Chromosome conformation capture experiments have led to the discovery of dense, contiguous, megabase-sized topological domains that are similar across cell types and conserved across species. These domains are strongly correlated with a number of chromatin markers and have since been included in a number of analyses. However, functionally-relevant domains may exist at multiple length scales. We introduce a new and efficient algorithm that is able to capture persistent domains across various resolutions by adjusting a single scale parameter. The ensemble of domains we identify allows us to quantify the degree to which the domain structure is hierarchical as opposed to overlapping, and our analysis reveals a pronounced hierarchical structure in which larger stable domains tend to completely contain smaller domains. The identified novel domains are substantially different from domains reported previously and are highly enriched for insulating factor CTCF binding and histone marks at the boundaries.",
    "cited_by_count": 226,
    "openalex_id": "https://openalex.org/W2109949710",
    "type": "article"
  },
  {
    "title": "Partition function and base pairing probabilities of RNA heterodimers",
    "doi": "https://doi.org/10.1186/1748-7188-1-3",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Stephan Wolf; Hakim Tafer; Ulrike Mückstein; Christoph Flamm; Peter F. Stadler; Ivo L. Hofacker",
    "corresponding_authors": "",
    "abstract": "RNA has been recognized as a key player in cellular regulation in recent years. In many cases, non-coding RNAs exert their function by binding to other nucleic acids, as in the case of microRNAs and snoRNAs. The specificity of these interactions derives from the stability of inter-molecular base pairing. The accurate computational treatment of RNA-RNA binding therefore lies at the heart of target prediction algorithms. The standard dynamic programming algorithms for computing secondary structures of linear single-stranded RNA molecules are extended to the co-folding of two interacting RNAs. We present a program, RNAcofold, that computes the hybridization energy and base pairing pattern of a pair of interacting RNA molecules. In contrast to earlier approaches, complex internal structures in both RNAs are fully taken into account. RNAcofold supports the calculation of the minimum energy structure and of a complete set of suboptimal structures in an energy band above the ground state. Furthermore, it provides an extension of McCaskill's partition function algorithm to compute base pairing probabilities, realistic interaction energies, and equilibrium concentrations of duplex structures. RNAcofold is distributed as part of the Vienna RNA Package, http://www.tbi.univie.ac.at/RNA/ . Stephan H. Bernhart – berni@tbi.univie.ac.at",
    "cited_by_count": 303,
    "openalex_id": "https://openalex.org/W2129706781",
    "type": "article"
  },
  {
    "title": "DIALIGN-TX: greedy and progressive approaches for segment-based multiple sequence alignment",
    "doi": "https://doi.org/10.1186/1748-7188-3-6",
    "publication_date": "2008-05-27",
    "publication_year": 2008,
    "authors": "A R Subramanian; Michael Kaufmann; Burkhard Morgenstern",
    "corresponding_authors": "",
    "abstract": "DIALIGN-T is a reimplementation of the multiple-alignment program DIALIGN. Due to several algorithmic improvements, it produces significantly better alignments on locally and globally related sequence sets than previous versions of DIALIGN. However, like the original implementation of the program, DIALIGN-T uses a a straight-forward greedy approach to assemble multiple alignments from local pairwise sequence similarities. Such greedy approaches may be vulnerable to spurious random similarities and can therefore lead to suboptimal results. In this paper, we present DIALIGN-TX, a substantial improvement of DIALIGN-T that combines our previous greedy algorithm with a progressive alignment approach.Our new heuristic produces significantly better alignments, especially on globally related sequences, without increasing the CPU time and memory consumption exceedingly. The new method is based on a guide tree; to detect possible spurious sequence similarities, it employs a vertex-cover approximation on a conflict graph. We performed benchmarking tests on a large set of nucleic acid and protein sequences For protein benchmarks we used the benchmark database BALIBASE 3 and an updated release of the database IRMBASE 2 for assessing the quality on globally and locally related sequences, respectively. For alignment of nucleic acid sequences, we used BRAliBase II for global alignment and a newly developed database of locally related sequences called DIRM-BASE 1. IRMBASE 2 and DIRMBASE 1 are constructed by implanting highly conserved motives at random positions in long unalignable sequences.On BALIBASE3, our new program performs significantly better than the previous program DIALIGN-T and outperforms the popular global aligner CLUSTAL W, though it is still outperformed by programs that focus on global alignment like MAFFT, MUSCLE and T-COFFEE. On the locally related test sets in IRMBASE 2 and DIRM-BASE 1, our method outperforms all other programs while MAFFT E-INSi is the only method that comes close to the performance of DIALIGN-TX.",
    "cited_by_count": 235,
    "openalex_id": "https://openalex.org/W2143357251",
    "type": "article"
  },
  {
    "title": "Estimation of alternative splicing isoform frequencies from RNA-Seq data",
    "doi": "https://doi.org/10.1186/1748-7188-6-9",
    "publication_date": "2011-04-19",
    "publication_year": 2011,
    "authors": "Marius Nicolae; Serghei Mangul; Ion Măndoiu; Alex Zelikovsky",
    "corresponding_authors": "",
    "abstract": "Massively parallel whole transcriptome sequencing, commonly referred as RNA-Seq, is quickly becoming the technology of choice for gene expression profiling. However, due to the short read length delivered by current sequencing technologies, estimation of expression levels for alternative splicing gene isoforms remains challenging.In this paper we present a novel expectation-maximization algorithm for inference of isoform- and gene-specific expression levels from RNA-Seq data. Our algorithm, referred to as IsoEM, is based on disambiguating information provided by the distribution of insert sizes generated during sequencing library preparation, and takes advantage of base quality scores, strand and read pairing information when available. The open source Java implementation of IsoEM is freely available at http://dna.engr.uconn.edu/software/IsoEM/.Empirical experiments on both synthetic and real RNA-Seq datasets show that IsoEM has scalable running time and outperforms existing methods of isoform and gene expression level estimation. Simulation experiments confirm previous findings that, for a fixed sequencing cost, using reads longer than 25-36 bases does not necessarily lead to better accuracy for estimating expression levels of annotated isoforms and genes.",
    "cited_by_count": 187,
    "openalex_id": "https://openalex.org/W2093253451",
    "type": "article"
  },
  {
    "title": "Noisy: Identification of problematic columns in multiple sequence alignments",
    "doi": "https://doi.org/10.1186/1748-7188-3-7",
    "publication_date": "2008-06-24",
    "publication_year": 2008,
    "authors": "Andreas Dress; Christoph Flamm; Guido Fritzsch; Stefan Grünewald; Matthias Kruspe; Sonja J. Prohaska; Peter F. Stadler",
    "corresponding_authors": "",
    "abstract": "Abstract Motivation Sequence-based methods for phylogenetic reconstruction from (nucleic acid) sequence data are notoriously plagued by two effects: homoplasies and alignment errors. Large evolutionary distances imply a large number of homoplastic sites. As most protein-coding genes show dramatic variations in substitution rates that are not uncorrelated across the sequence, this often leads to a patchwork pattern of (i) phylogenetically informative and (ii) effectively randomized regions. In highly variable regions, furthermore, alignment errors accumulate resulting in sometimes misleading signals in phylogenetic reconstruction. Results We present here a method that, based on assessing the distribution of character states along a cyclic ordering of the taxa, allows the identification of phylogenetically uninformative homoplastic sites in a multiple sequence alignment. Removal of these sites appears to improve the performance of phylogenetic reconstruction algorithms as measured by various indices of \"tree quality\". In particular, we obtain more stable trees due to the exclusion of phylogenetically incompatible sites that most likely represent strongly randomized characters. Software The computer program noisy implements this approach. It can be employed to improving phylogenetic reconstruction capability with quite a considerable success rate whenever (1) the average bootstrap support obtained from the original alignment is low, and (2) there are sufficiently many taxa in the data set – at least, say, 12 to 15 taxa. The software can be obtained under the GNU Public License from http://www.bioinf.uni-leipzig.de/Software/noisy/ .",
    "cited_by_count": 161,
    "openalex_id": "https://openalex.org/W2127556561",
    "type": "article"
  },
  {
    "title": "Speeding up the Consensus Clustering methodology for microarray data analysis",
    "doi": "https://doi.org/10.1186/1748-7188-6-1",
    "publication_date": "2011-01-14",
    "publication_year": 2011,
    "authors": "Raffaele Giancarlo; Filippo Utro",
    "corresponding_authors": "",
    "abstract": "The inference of the number of clusters in a dataset, a fundamental problem in Statistics, Data Analysis and Classification, is usually addressed via internal validation measures. The stated problem is quite difficult, in particular for microarrays, since the inferred prediction must be sensible enough to capture the inherent biological structure in a dataset, e.g., functionally related genes. Despite the rich literature present in that area, the identification of an internal validation measure that is both fast and precise has proved to be elusive. In order to partially fill this gap, we propose a speed-up of Consensus (Consensus Clustering), a methodology whose purpose is the provision of a prediction of the number of clusters in a dataset, together with a dissimilarity matrix (the consensus matrix) that can be used by clustering algorithms. As detailed in the remainder of the paper, Consensus is a natural candidate for a speed-up.Since the time-precision performance of Consensus depends on two parameters, our first task is to show that a simple adjustment of the parameters is not enough to obtain a good precision-time trade-off. Our second task is to provide a fast approximation algorithm for Consensus. That is, the closely related algorithm FC (Fast Consensus) that would have the same precision as Consensus with a substantially better time performance. The performance of FC has been assessed via extensive experiments on twelve benchmark datasets that summarize key features of microarray applications, such as cancer studies, gene expression with up and down patterns, and a full spectrum of dimensionality up to over a thousand. Based on their outcome, compared with previous benchmarking results available in the literature, FC turns out to be among the fastest internal validation methods, while retaining the same outstanding precision of Consensus. Moreover, it also provides a consensus matrix that can be used as a dissimilarity matrix, guaranteeing the same performance as the corresponding matrix produced by Consensus. We have also experimented with the use of Consensus and FC in conjunction with NMF (Nonnegative Matrix Factorization), in order to identify the correct number of clusters in a dataset. Although NMF is an increasingly popular technique for biological data mining, our results are somewhat disappointing and complement quite well the state of the art about NMF, shedding further light on its merits and limitations.In summary, FC with a parameter setting that makes it robust with respect to small and medium-sized datasets, i.e, number of items to cluster in the hundreds and number of conditions up to a thousand, seems to be the internal validation measure of choice. Moreover, the technique we have developed here can be used in other contexts, in particular for the speed-up of stability-based validation measures.",
    "cited_by_count": 145,
    "openalex_id": "https://openalex.org/W2132922290",
    "type": "article"
  },
  {
    "title": "RNA folding with hard and soft constraints",
    "doi": "https://doi.org/10.1186/s13015-016-0070-z",
    "publication_date": "2016-04-23",
    "publication_year": 2016,
    "authors": "Ronny Lorenz; Ivo L. Hofacker; Peter F. Stadler",
    "corresponding_authors": "Ronny Lorenz",
    "abstract": "A large class of RNA secondary structure prediction programs uses an elaborate energy model grounded in extensive thermodynamic measurements and exact dynamic programming algorithms. External experimental evidence can be in principle be incorporated by means of hard constraints that restrict the search space or by means of soft constraints that distort the energy model. In particular recent advances in coupling chemical and enzymatic probing with sequencing techniques but also comparative approaches provide an increasing amount of experimental data to be combined with secondary structure prediction. Responding to the increasing needs for a versatile and user-friendly inclusion of external evidence into diverse flavors of RNA secondary structure prediction tools we implemented a generic layer of constraint handling into the ViennaRNA Package. It makes explicit use of the conceptual separation of the \"folding grammar\" defining the search space and the actual energy evaluation, which allows constraints to be interleaved in a natural way between recursion steps and evaluation of the standard energy function. The extension of the ViennaRNA Package provides a generic way to include diverse types of constraints into RNA folding algorithms. The computational overhead incurred is negligible in practice. A wide variety of application scenarios can be accommodated by the new framework, including the incorporation of structure probing data, non-standard base pairs and chemical modifications, as well as structure-dependent ligand binding.",
    "cited_by_count": 111,
    "openalex_id": "https://openalex.org/W2336017145",
    "type": "article"
  },
  {
    "title": "Assessing the efficiency of multiple sequence alignment programs",
    "doi": "https://doi.org/10.1186/1748-7188-9-4",
    "publication_date": "2014-03-06",
    "publication_year": 2014,
    "authors": "Fabiano Pais; Patrícia de Cássia Ruy; Guilherme Oliveira; Roney S. Coimbra",
    "corresponding_authors": "",
    "abstract": "Abstract Background Multiple sequence alignment (MSA) is an extremely useful tool for molecular and evolutionary biology and there are several programs and algorithms available for this purpose. Although previous studies have compared the alignment accuracy of different MSA programs, their computational time and memory usage have not been systematically evaluated. Given the unprecedented amount of data produced by next generation deep sequencing platforms, and increasing demand for large-scale data analysis, it is imperative to optimize the application of software. Therefore, a balance between alignment accuracy and computational cost has become a critical indicator of the most suitable MSA program. We compared both accuracy and cost of nine popular MSA programs, namely CLUSTALW, CLUSTAL OMEGA, DIALIGN-TX, MAFFT, MUSCLE, POA, Probalign, Probcons and T-Coffee, against the benchmark alignment dataset BAliBASE and discuss the relevance of some implementations embedded in each program’s algorithm. Accuracy of alignment was calculated with the two standard scoring functions provided by BAliBASE, the sum-of-pairs and total-column scores, and computational costs were determined by collecting peak memory usage and time of execution. Results Our results indicate that mostly the consistency-based programs Probcons, T-Coffee, Probalign and MAFFT outperformed the other programs in accuracy. Whenever sequences with large N/C terminal extensions were present in the BAliBASE suite, Probalign, MAFFT and also CLUSTAL OMEGA outperformed Probcons and T-Coffee. The drawback of these programs is that they are more memory-greedy and slower than POA, CLUSTALW, DIALIGN-TX, and MUSCLE. CLUSTALW and MUSCLE were the fastest programs, being CLUSTALW the least RAM memory demanding program. Conclusions Based on the results presented herein, all four programs Probcons, T-Coffee, Probalign and MAFFT are well recommended for better accuracy of multiple sequence alignments. T-Coffee and recent versions of MAFFT can deliver faster and reliable alignments, which are specially suited for larger datasets than those encountered in the BAliBASE suite, if multi-core computers are available. In fact, parallelization of alignments for multi-core computers should probably be addressed by more programs in a near future, which will certainly improve performance significantly.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2118716355",
    "type": "article"
  },
  {
    "title": "Fulgor: a fast and compact k-mer index for large-scale matching and color queries",
    "doi": "https://doi.org/10.1186/s13015-024-00251-9",
    "publication_date": "2024-01-22",
    "publication_year": 2024,
    "authors": "Jason Fan; Jamshed Khan; Noor Singh; Giulio Ermanno Pibiri; Rob Patro",
    "corresponding_authors": "",
    "abstract": "The problem of sequence identification or matching-determining the subset of reference sequences from a given collection that are likely to contain a short, queried nucleotide sequence-is relevant for many important tasks in Computational Biology, such as metagenomics and pangenome analysis. Due to the complex nature of such analyses and the large scale of the reference collections a resource-efficient solution to this problem is of utmost importance. This poses the threefold challenge of representing the reference collection with a data structure that is efficient to query, has light memory usage, and scales well to large collections. To solve this problem, we describe an efficient colored de Bruijn graph index, arising as the combination of a k-mer dictionary with a compressed inverted index. The proposed index takes full advantage of the fact that unitigs in the colored compacted de Bruijn graph are monochromatic (i.e., all k-mers in a unitig have the same set of references of origin, or color). Specifically, the unitigs are kept in the dictionary in color order, thereby allowing for the encoding of the map from k-mers to their colors in as little as 1 + o(1) bits per unitig. Hence, one color per unitig is stored in the index with almost no space/time overhead. By combining this property with simple but effective compression methods for integer lists, the index achieves very small space. We implement these methods in a tool called Fulgor, and conduct an extensive experimental analysis to demonstrate the improvement of our tool over previous solutions. For example, compared to Themisto-the strongest competitor in terms of index space vs. query time trade-off-Fulgor requires significantly less space (up to 43% less space for a collection of 150,000 Salmonella enterica genomes), is at least twice as fast for color queries, and is 2-6[Formula: see text] faster to construct.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4391099685",
    "type": "article"
  },
  {
    "title": "An enhanced RNA alignment benchmark for sequence alignment programs",
    "doi": "https://doi.org/10.1186/1748-7188-1-19",
    "publication_date": "2006-10-24",
    "publication_year": 2006,
    "authors": "Andreas Wilm; Indra Mainz; Gerhard Steger",
    "corresponding_authors": "",
    "abstract": "The performance of alignment programs is traditionally tested on sets of protein sequences, of which a reference alignment is known. Conclusions drawn from such protein benchmarks do not necessarily hold for the RNA alignment problem, as was demonstrated in the first RNA alignment benchmark published so far. For example, the twilight zone - the similarity range where alignment quality drops drastically - starts at 60 % for RNAs in comparison to 20 % for proteins. In this study we enhance the previous benchmark.The RNA sequence sets in the benchmark database are taken from an increased number of RNA families to avoid unintended impact by using only a few families. The size of sets varies from 2 to 15 sequences to assess the influence of the number of sequences on program performance. Alignment quality is scored by two measures: one takes into account only nucleotide matches, the other measures structural conservation. The performance order of parameters--like nucleotide substitution matrices and gap-costs--as well as of programs is rated by rank tests.Most sequence alignment programs perform equally well on RNA sequence sets with high sequence identity, that is with an average pairwise sequence identity (APSI) above 75 %. Parameters for gap-open and gap-extension have a large influence on alignment quality lower than APSI < or = 75 %; optimal parameter combinations are shown for several programs. The use of different 4 x 4 substitution matrices improved program performance only in some cases. The performance of iterative programs drastically increases with increasing sequence numbers and/or decreasing sequence identity, which makes them clearly superior to programs using a purely non-iterative, progressive approach. The best sequence alignment programs produce alignments of high quality down to APSI > 55 %; at lower APSI the use of sequence+structure alignment programs is recommended.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W2163798335",
    "type": "article"
  },
  {
    "title": "Efficient and accurate P-value computation for Position Weight Matrices",
    "doi": "https://doi.org/10.1186/1748-7188-2-15",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Hélène Touzet; Jean‐Stéphane Varré",
    "corresponding_authors": "",
    "abstract": "Position Weight Matrices (PWMs) are probabilistic representations of signals in sequences. They are widely used to model approximate patterns in DNA or in protein sequences. The usage of PWMs needs as a prerequisite to knowing the statistical significance of a word according to its score. This is done by defining the P-value of a score, which is the probability that the background model can achieve a score larger than or equal to the observed value. This gives rise to the following problem: Given a P-value, find the corresponding score threshold. Existing methods rely on dynamic programming or probability generating functions. For many examples of PWMs, they fail to give accurate results in a reasonable amount of time. The contribution of this paper is two fold. First, we study the theoretical complexity of the problem, and we prove that it is NP-hard. Then, we describe a novel algorithm that solves the P-value problem efficiently. The main idea is to use a series of discretized score distributions that improves the final result step by step until some convergence criterion is met. Moreover, the algorithm is capable of calculating the exact P-value without any error, even for matrices with non-integer coefficient values. The same approach is also used to devise an accurate algorithm for the reverse problem: finding the P-value for a given score. Both methods are implemented in a software called TFM-PVALUE, that is freely available. We have tested TFM-PVALUE on a large set of PWMs representing transcription factor binding sites. Experimental results show that it achieves better performance in terms of computational time and precision than existing tools.",
    "cited_by_count": 129,
    "openalex_id": "https://openalex.org/W2118215072",
    "type": "article"
  },
  {
    "title": "Sequence embedding for fast construction of guide trees for multiple sequence alignment",
    "doi": "https://doi.org/10.1186/1748-7188-5-21",
    "publication_date": "2010-05-14",
    "publication_year": 2010,
    "authors": "Gordon Blackshields; Fabian Sievers; Weifeng Shi; Andreas Wilm; Desmond G. Higgins",
    "corresponding_authors": "",
    "abstract": "The most widely used multiple sequence alignment methods require sequences to be clustered as an initial step. Most sequence clustering methods require a full distance matrix to be computed between all pairs of sequences. This requires memory and time proportional to N2 for N sequences. When N grows larger than 10,000 or so, this becomes increasingly prohibitive and can form a significant barrier to carrying out very large multiple alignments.In this paper, we have tested variations on a class of embedding methods that have been designed for clustering large numbers of complex objects where the individual distance calculations are expensive. These methods involve embedding the sequences in a space where the similarities within a set of sequences can be closely approximated without having to compute all pair-wise distances.We show how this approach greatly reduces computation time and memory requirements for clustering large numbers of sequences and demonstrate the quality of the clusterings by benchmarking them as guide trees for multiple alignment. Source code is available for download from http://www.clustal.org/mbed.tgz.",
    "cited_by_count": 118,
    "openalex_id": "https://openalex.org/W2115394533",
    "type": "article"
  },
  {
    "title": "Robinson-Foulds Supertrees",
    "doi": "https://doi.org/10.1186/1748-7188-5-18",
    "publication_date": "2010-02-24",
    "publication_year": 2010,
    "authors": "Mukul S. Bansal; J. Gordon Burleigh; Oliver Eulenstein; David Fernández‐Baca",
    "corresponding_authors": "",
    "abstract": "Supertree methods synthesize collections of small phylogenetic trees with incomplete taxon overlap into comprehensive trees, or supertrees, that include all taxa found in the input trees. Supertree methods based on the well established Robinson-Foulds (RF) distance have the potential to build supertrees that retain much information from the input trees. Specifically, the RF supertree problem seeks a binary supertree that minimizes the sum of the RF distances from the supertree to the input trees. Thus, an RF supertree is a supertree that is consistent with the largest number of clusters (or clades) from the input trees.We introduce efficient, local search based, hill-climbing heuristics for the intrinsically hard RF supertree problem on rooted trees. These heuristics use novel non-trivial algorithms for the SPR and TBR local search problems which improve on the time complexity of the best known (naïve) solutions by a factor of Theta(n) and Theta(n2) respectively (where n is the number of taxa, or leaves, in the supertree). We use an implementation of our new algorithms to examine the performance of the RF supertree method and compare it to matrix representation with parsimony (MRP) and the triplet supertree method using four supertree data sets. Not only did our RF heuristic provide fast estimates of RF supertrees in all data sets, but the RF supertrees also retained more of the information from the input trees (based on the RF distance) than the other supertree methods.Our heuristics for the RF supertree problem, based on our new local search algorithms, make it possible for the first time to estimate large supertrees by directly optimizing the RF distance from rooted input trees to the supertrees. This provides a new and fast method to build accurate supertrees. RF supertrees may also be useful for estimating majority-rule(-) supertrees, which are a generalization of majority-rule consensus trees.",
    "cited_by_count": 115,
    "openalex_id": "https://openalex.org/W1981037505",
    "type": "article"
  },
  {
    "title": "A weighted average difference method for detecting differentially expressed genes from microarray data",
    "doi": "https://doi.org/10.1186/1748-7188-3-8",
    "publication_date": "2008-06-26",
    "publication_year": 2008,
    "authors": "Koji Kadota; Yuji Nakai; Kentaro Shimizu",
    "corresponding_authors": "",
    "abstract": "Identification of differentially expressed genes (DEGs) under different experimental conditions is an important task in many microarray studies. However, choosing which method to use for a particular application is problematic because its performance depends on the evaluation metric, the dataset, and so on. In addition, when using the Affymetrix GeneChip(R) system, researchers must select a preprocessing algorithm from a number of competing algorithms such as MAS, RMA, and DFW, for obtaining expression-level measurements. To achieve optimal performance for detecting DEGs, a suitable combination of gene selection method and preprocessing algorithm needs to be selected for a given probe-level dataset.We introduce a new fold-change (FC)-based method, the weighted average difference method (WAD), for ranking DEGs. It uses the average difference and relative average signal intensity so that highly expressed genes are highly ranked on the average for the different conditions. The idea is based on our observation that known or potential marker genes (or proteins) tend to have high expression levels. We compared WAD with seven other methods; average difference (AD), FC, rank products (RP), moderated t statistic (modT), significance analysis of microarrays (samT), shrinkage t statistic (shrinkT), and intensity-based moderated t statistic (ibmT). The evaluation was performed using a total of 38 different binary (two-class) probe-level datasets: two artificial \"spike-in\" datasets and 36 real experimental datasets. The results indicate that WAD outperforms the other methods when sensitivity and specificity are considered simultaneously: the area under the receiver operating characteristic curve for WAD was the highest on average for the 38 datasets. The gene ranking for WAD was also the most consistent when subsets of top-ranked genes produced from three different preprocessed data (MAS, RMA, and DFW) were compared. Overall, WAD performed the best for MAS-preprocessed data and the FC-based methods (AD, WAD, FC, or RP) performed well for RMA and DFW-preprocessed data.WAD is a promising alternative to existing methods for ranking DEGs with two classes. Its high performance should increase researchers' confidence in microarray analyses.",
    "cited_by_count": 113,
    "openalex_id": "https://openalex.org/W2002084527",
    "type": "article"
  },
  {
    "title": "Data compression for sequencing data",
    "doi": "https://doi.org/10.1186/1748-7188-8-25",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Sebastian Deorowicz; Szymon Grabowski",
    "corresponding_authors": "Szymon Grabowski",
    "abstract": ": Post-Sanger sequencing methods produce tons of data, and there is a general agreement that the challenge to store and process them must be addressed with data compression. In this review we first answer the question \"why compression\" in a quantitative manner. Then we also answer the questions \"what\" and \"how\", by sketching the fundamental compression ideas, describing the main sequencing data types and formats, and comparing the specialized compression algorithms and tools. Finally, we go back to the question \"why compression\" and give other, perhaps surprising answers, demonstrating the pervasiveness of data compression techniques in computational biology.",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W2167943254",
    "type": "article"
  },
  {
    "title": "A normalization strategy for comparing tag count data",
    "doi": "https://doi.org/10.1186/1748-7188-7-5",
    "publication_date": "2012-04-05",
    "publication_year": 2012,
    "authors": "Koji Kadota; Tomoaki Nishiyama; Kentaro K. Shimizu",
    "corresponding_authors": "",
    "abstract": "High-throughput sequencing, such as ribonucleic acid sequencing (RNA-seq) and chromatin immunoprecipitation sequencing (ChIP-seq) analyses, enables various features of organisms to be compared through tag counts. Recent studies have demonstrated that the normalization step for RNA-seq data is critical for a more accurate subsequent analysis of differential gene expression. Development of a more robust normalization method is desirable for identifying the true difference in tag count data. We describe a strategy for normalizing tag count data, focusing on RNA-seq. The key concept is to remove data assigned as potential differentially expressed genes (DEGs) before calculating the normalization factor. Several R packages for identifying DEGs are currently available, and each package uses its own normalization method and gene ranking algorithm. We compared a total of eight package combinations: four R packages (edgeR, DESeq, baySeq, and NBPSeq) with their default normalization settings and with our normalization strategy. Many synthetic datasets under various scenarios were evaluated on the basis of the area under the curve (AUC) as a measure for both sensitivity and specificity. We found that packages using our strategy in the data normalization step overall performed well. This result was also observed for a real experimental dataset. Our results showed that the elimination of potential DEGs is essential for more accurate normalization of RNA-seq data. The concept of this normalization strategy can widely be applied to other types of tag count data and to microarray data.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2072532688",
    "type": "article"
  },
  {
    "title": "Maximum Parsimony on Phylogenetic networks",
    "doi": "https://doi.org/10.1186/1748-7188-7-9",
    "publication_date": "2012-05-02",
    "publication_year": 2012,
    "authors": "Lavanya Kannan; Ward C. Wheeler",
    "corresponding_authors": "",
    "abstract": "Phylogenetic networks are generalizations of phylogenetic trees, that are used to model evolutionary events in various contexts. Several different methods and criteria have been introduced for reconstructing phylogenetic trees. Maximum Parsimony is a character-based approach that infers a phylogenetic tree by minimizing the total number of evolutionary steps required to explain a given set of data assigned on the leaves. Exact solutions for optimizing parsimony scores on phylogenetic trees have been introduced in the past.In this paper, we define the parsimony score on networks as the sum of the substitution costs along all the edges of the network; and show that certain well-known algorithms that calculate the optimum parsimony score on trees, such as Sankoff and Fitch algorithms extend naturally for networks, barring conflicting assignments at the reticulate vertices. We provide heuristics for finding the optimum parsimony scores on networks. Our algorithms can be applied for any cost matrix that may contain unequal substitution costs of transforming between different characters along different edges of the network. We analyzed this for experimental data on 10 leaves or fewer with at most 2 reticulations and found that for almost all networks, the bounds returned by the heuristics matched with the exhaustively determined optimum parsimony scores.The parsimony score we define here does not directly reflect the cost of the best tree in the network that displays the evolution of the character. However, when searching for the most parsimonious network that describes a collection of characters, it becomes necessary to add additional cost considerations to prefer simpler structures, such as trees over networks. The parsimony score on a network that we describe here takes into account the substitution costs along the additional edges incident on each reticulate vertex, in addition to the substitution costs along the other edges which are common to all the branching patterns introduced by the reticulate vertices. Thus the score contains an in-built cost for the number of reticulate vertices in the network, and would provide a criterion that is comparable among all networks. Although the problem of finding the parsimony score on the network is believed to be computationally hard to solve, heuristics such as the ones described here would be beneficial in our efforts to find a most parsimonious network.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2102782866",
    "type": "article"
  },
  {
    "title": "A Partial Least Squares based algorithm for parsimonious variable selection",
    "doi": "https://doi.org/10.1186/1748-7188-6-27",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Tahir Mehmood; Harald Martens; Solve Sæbø; Jonas Warringer; Lars Snipen",
    "corresponding_authors": "",
    "abstract": "In genomics, a commonly encountered problem is to extract a subset of variables out of a large set of explanatory variables associated with one or several quantitative or qualitative response variables. An example is to identify associations between codon-usage and phylogeny based definitions of taxonomic groups at different taxonomic levels. Maximum understandability with the smallest number of selected variables, consistency of the selected variables, as well as variation of model performance on test data, are issues to be addressed for such problems.We present an algorithm balancing the parsimony and the predictive performance of a model. The algorithm is based on variable selection using reduced-rank Partial Least Squares with a regularized elimination. Allowing a marginal decrease in model performance results in a substantial decrease in the number of selected variables. This significantly improves the understandability of the model. Within the approach we have tested and compared three different criteria commonly used in the Partial Least Square modeling paradigm for variable selection; loading weights, regression coefficients and variable importance on projections. The algorithm is applied to a problem of identifying codon variations discriminating different bacterial taxa, which is of particular interest in classifying metagenomics samples. The results are compared with a classical forward selection algorithm, the much used Lasso algorithm as well as Soft-threshold Partial Least Squares variable selection.A regularized elimination algorithm based on Partial Least Squares produces results that increase understandability and consistency and reduces the classification error on test data compared to standard approaches.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2108619906",
    "type": "article"
  },
  {
    "title": "Bloom Filter Trie: an alignment-free and reference-free data structure for pan-genome storage",
    "doi": "https://doi.org/10.1186/s13015-016-0066-8",
    "publication_date": "2016-04-14",
    "publication_year": 2016,
    "authors": "Guillaume Holley; Roland Wittler; Jens Stoye",
    "corresponding_authors": "Guillaume Holley",
    "abstract": "High throughput sequencing technologies have become fast and cheap in the past years. As a result, large-scale projects started to sequence tens to several thousands of genomes per species, producing a high number of sequences sampled from each genome. Such a highly redundant collection of very similar sequences is called a pan-genome. It can be transformed into a set of sequences \"colored\" by the genomes to which they belong. A colored de Bruijn graph (C-DBG) extracts from the sequences all colored k-mers, strings of length k, and stores them in vertices.In this paper, we present an alignment-free, reference-free and incremental data structure for storing a pan-genome as a C-DBG: the bloom filter trie (BFT). The data structure allows to store and compress a set of colored k-mers, and also to efficiently traverse the graph. Bloom filter trie was used to index and query different pangenome datasets. Compared to another state-of-the-art data structure, BFT was up to two times faster to build while using about the same amount of main memory. For querying k-mers, BFT was about 52-66 times faster while using about 5.5-14.3 times less memory.We present a novel succinct data structure called the Bloom Filter Trie for indexing a pan-genome as a colored de Bruijn graph. The trie stores k-mers and their colors based on a new representation of vertices that compress and index shared substrings. Vertices use basic data structures for lightweight substrings storage as well as Bloom filters for efficient trie and graph traversals. Experimental results prove better performance compared to another state-of-the-art data structure.https://www.github.com/GuillaumeHolley/BloomFilterTrie.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2278452282",
    "type": "article"
  },
  {
    "title": "Gerbil: a fast and memory-efficient k-mer counter with GPU-support",
    "doi": "https://doi.org/10.1186/s13015-017-0097-9",
    "publication_date": "2017-03-31",
    "publication_year": 2017,
    "authors": "Marius Erbert; Steffen Rechner; Matthias Müller‐Hannemann",
    "corresponding_authors": "",
    "abstract": "A basic task in bioinformatics is the counting of k-mers in genome sequences. Existing k-mer counting tools are most often optimized for small k < 32 and suffer from excessive memory resource consumption or degrading performance for large k. However, given the technology trend towards long reads of next-generation sequencers, support for large k becomes increasingly important.We present the open source k-mer counting software Gerbil that has been designed for the efficient counting of k-mers for k ≥ 32. Our software is the result of an intensive process of algorithm engineering. It implements a two-step approach. In the first step, genome reads are loaded from disk and redistributed to temporary files. In a second step, the k-mers of each temporary file are counted via a hash table approach. In addition to its basic functionality, Gerbil can optionally use GPUs to accelerate the counting step. In a set of experiments with real-world genome data sets, we show that Gerbil is able to efficiently support both small and large k.While Gerbil's performance is comparable to existing state-of-the-art open source k-mer counting tools for small k < 32, it vastly outperforms its competitors for large k, thereby enabling new applications which require large values of k.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2597444305",
    "type": "article"
  },
  {
    "title": "Jabba: hybrid error correction for long sequencing reads",
    "doi": "https://doi.org/10.1186/s13015-016-0075-7",
    "publication_date": "2016-05-03",
    "publication_year": 2016,
    "authors": "Giles Miclotte; Mahdi Heydari; Piet Demeester; Stéphane Rombauts; Yves Van de Peer; Pieter Audenaert; Jan Fostier",
    "corresponding_authors": "",
    "abstract": "Third generation sequencing platforms produce longer reads with higher error rates than second generation technologies. While the improved read length can provide useful information for downstream analysis, underlying algorithms are challenged by the high error rate. Error correction methods in which accurate short reads are used to correct noisy long reads appear to be attractive to generate high-quality long reads. Methods that align short reads to long reads do not optimally use the information contained in the second generation data, and suffer from large runtimes. Recently, a new hybrid error correcting method has been proposed, where the second generation data is first assembled into a de Bruijn graph, on which the long reads are then aligned.In this context we present Jabba, a hybrid method to correct long third generation reads by mapping them on a corrected de Bruijn graph that was constructed from second generation data. Unique to our method is the use of a pseudo alignment approach with a seed-and-extend methodology, using maximal exact matches (MEMs) as seeds. In addition to benchmark results, certain theoretical results concerning the possibilities and limitations of the use of MEMs in the context of third generation reads are presented.Jabba produces highly reliable corrected reads: almost all corrected reads align to the reference, and these alignments have a very high identity. Many of the aligned reads are error-free. Additionally, Jabba corrects reads using a very low amount of CPU time. From this we conclude that pseudo alignment with MEMs is a fast and reliable method to map long highly erroneous sequences on a de Bruijn graph.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2346241034",
    "type": "article"
  },
  {
    "title": "Using cascading Bloom filters to improve the memory usage for de Brujin graphs",
    "doi": "https://doi.org/10.1186/1748-7188-9-2",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Kamil Salikhov; Gustavo Sacomoto; Grégory Kucherov",
    "corresponding_authors": "",
    "abstract": "De Brujin graphs are widely used in bioinformatics for processing next-generation sequencing data. Due to a very large size of NGS datasets, it is essential to represent de Bruijn graphs compactly, and several approaches to this problem have been proposed recently.In this work, we show how to reduce the memory required by the data structure of Chikhi and Rizk (WABI'12) that represents de Brujin graphs using Bloom filters. Our method requires 30% to 40% less memory with respect to their method, with insignificant impact on construction time. At the same time, our experiments showed a better query time compared to the method of Chikhi and Rizk.The proposed data structure constitutes, to our knowledge, currently the most efficient practical representation of de Bruijn graphs.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W2198888083",
    "type": "article"
  },
  {
    "title": "Jaccard index based similarity measure to compare transcription factor binding site models",
    "doi": "https://doi.org/10.1186/1748-7188-8-23",
    "publication_date": "2013-09-30",
    "publication_year": 2013,
    "authors": "Ilya E. Vorontsov; Ivan V. Kulakovskiy; Vsevolod J. Makeev",
    "corresponding_authors": "",
    "abstract": "Positional weight matrix (PWM) remains the most popular for quantification of transcription factor (TF) binding. PWM supplied with a score threshold defines a set of putative transcription factor binding sites (TFBS), thus providing a TFBS model.TF binding DNA fragments obtained by different experimental methods usually give similar but not identical PWMs. This is also common for different TFs from the same structural family. Thus it is often necessary to measure the similarity between PWMs. The popular tools compare PWMs directly using matrix elements. Yet, for log-odds PWMs, negative elements do not contribute to the scores of highly scoring TFBS and thus may be different without affecting the sets of the best recognized binding sites. Moreover, the two TFBS sets recognized by a given pair of PWMs can be more or less different depending on the score thresholds.We propose a practical approach for comparing two TFBS models, each consisting of a PWM and the respective scoring threshold. The proposed measure is a variant of the Jaccard index between two TFBS sets. The measure defines a metric space for TFBS models of all finite lengths. The algorithm can compare TFBS models constructed using substantially different approaches, like PWMs with raw positional counts and log-odds. We present the efficient software implementation: MACRO-APE (MAtrix CompaRisOn by Approximate P-value Estimation).MACRO-APE can be effectively used to compute the Jaccard index based similarity for two TFBS models. A two-pass scanning algorithm is presented to scan a given collection of PWMs for PWMs similar to a given query.MACRO-APE is implemented in ruby 1.9; software including source code and a manual is freely available at http://autosome.ru/macroape/ and in supplementary materials.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2136945519",
    "type": "article"
  },
  {
    "title": "NANUQ: a method for inferring species networks from gene trees under the coalescent model",
    "doi": "https://doi.org/10.1186/s13015-019-0159-2",
    "publication_date": "2019-12-01",
    "publication_year": 2019,
    "authors": "Elizabeth S. Allman; Hector Baños; John A. Rhodes",
    "corresponding_authors": "",
    "abstract": "Species networks generalize the notion of species trees to allow for hybridization or other lateral gene transfer. Under the network multispecies coalescent model, individual gene trees arising from a network can have any topology, but arise with frequencies dependent on the network structure and numerical parameters. We propose a new algorithm for statistical inference of a level-1 species network under this model, from data consisting of gene tree topologies, and provide the theoretical justification for it. The algorithm is based on an analysis of quartets displayed on gene trees, combining several statistical hypothesis tests with combinatorial ideas such as a quartet-based intertaxon distance appropriate to networks, the NeighborNet algorithm for circular split systems, and the Circular Network algorithm for constructing a splits graph.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2992645427",
    "type": "article"
  },
  {
    "title": "Estimating evolutionary distances between genomic sequences from spaced-word matches",
    "doi": "https://doi.org/10.1186/s13015-015-0032-x",
    "publication_date": "2015-02-10",
    "publication_year": 2015,
    "authors": "Burkhard Morgenstern; Bingyao Zhu; Sebastian Horwege; Chris André Leimeister",
    "corresponding_authors": "",
    "abstract": "Alignment-free methods are increasingly used to calculate evolutionary distances between DNA and protein sequences as a basis of phylogeny reconstruction. Most of these methods, however, use heuristic distance functions that are not based on any explicit model of molecular evolution. Herein, we propose a simple estimator d N of the evolutionary distance between two DNA sequences that is calculated from the number N of (spaced) word matches between them. We show that this distance function is more accurate than other distance measures that are used by alignment-free methods. In addition, we calculate the variance of the normalized number N of (spaced) word matches. We show that the variance of N is smaller for spaced words than for contiguous words, and that the variance is further reduced if our spaced-words approach is used with multiple patterns of 'match positions' and 'don't care positions'. Our software is available online and as downloadable source code at: http://spaced.gobics.de/.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2141865968",
    "type": "article"
  },
  {
    "title": "Prefix-free parsing for building big BWTs",
    "doi": "https://doi.org/10.1186/s13015-019-0148-5",
    "publication_date": "2019-05-24",
    "publication_year": 2019,
    "authors": "Christina Boucher; Travis Gagie; Alan Kuhnle; Ben Langmead; Giovanni Manzini; Taher Mun",
    "corresponding_authors": "",
    "abstract": "High-throughput sequencing technologies have led to explosive growth of genomic databases; one of which will soon reach hundreds of terabytes. For many applications we want to build and store indexes of these databases but constructing such indexes is a challenge. Fortunately, many of these genomic databases are highly-repetitive-a characteristic that can be exploited to ease the computation of the Burrows-Wheeler Transform (BWT), which underlies many popular indexes. In this paper, we introduce a preprocessing algorithm, referred to as",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2963653132",
    "type": "article"
  },
  {
    "title": "Fast characterization of segmental duplication structure in multiple genome assemblies",
    "doi": "https://doi.org/10.1186/s13015-022-00210-2",
    "publication_date": "2022-03-18",
    "publication_year": 2022,
    "authors": "Hamza Išerić; Can Alkan; Faraz Hach; Ibrahim Numanagić",
    "corresponding_authors": "Ibrahim Numanagić",
    "abstract": "The increasing availability of high-quality genome assemblies raised interest in the characterization of genomic architecture. Major architectural elements, such as common repeats and segmental duplications (SDs), increase genome plasticity that stimulates further evolution by changing the genomic structure and inventing new genes. Optimal computation of SDs within a genome requires quadratic-time local alignment algorithms that are impractical due to the size of most genomes. Additionally, to perform evolutionary analysis, one needs to characterize SDs in multiple genomes and find relations between those SDs and unique (non-duplicated) segments in other genomes. A naïve approach consisting of multiple sequence alignment would make the optimal solution to this problem even more impractical. Thus there is a need for fast and accurate algorithms to characterize SD structure in multiple genome assemblies to better understand the evolutionary forces that shaped the genomes of today.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W4221035067",
    "type": "article"
  },
  {
    "title": "A novel functional module detection algorithm for protein-protein interaction networks",
    "doi": "https://doi.org/10.1186/1748-7188-1-24",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Woochang Hwang; Young‐Rae Cho; Aidong Zhang; Murali Ramanathan",
    "corresponding_authors": "",
    "abstract": "The sparse connectivity of protein-protein interaction data sets makes identification of functional modules challenging. The purpose of this study is to critically evaluate a novel clustering technique for clustering and detecting functional modules in protein-protein interaction networks, termed STM.STM selects representative proteins for each cluster and iteratively refines clusters based on a combination of the signal transduced and graph topology. STM is found to be effective at detecting clusters with a diverse range of interaction structures that are significant on measures of biological relevance. The STM approach is compared to six competing approaches including the maximum clique, quasi-clique, minimum cut, betweeness cut and Markov Clustering (MCL) algorithms. The clusters obtained by each technique are compared for enrichment of biological function. STM generates larger clusters and the clusters identified have p-values that are approximately 125-fold better than the other methods on biological function. An important strength of STM is that the percentage of proteins that are discarded to create clusters is much lower than the other approaches.STM outperforms competing approaches and is capable of effectively detecting both densely and sparsely connected, biologically relevant functional modules with fewer discards.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W1759780789",
    "type": "article"
  },
  {
    "title": "Ranking differentially expressed genes from Affymetrix gene expression data: methods with reproducibility, sensitivity, and specificity",
    "doi": "https://doi.org/10.1186/1748-7188-4-7",
    "publication_date": "2009-04-22",
    "publication_year": 2009,
    "authors": "Koji Kadota; Yuji Nakai; Kentaro Shimizu",
    "corresponding_authors": "",
    "abstract": "To identify differentially expressed genes (DEGs) from microarray data, users of the Affymetrix GeneChip system need to select both a preprocessing algorithm to obtain expression-level measurements and a way of ranking genes to obtain the most plausible candidates. We recently recommended suitable combinations of a preprocessing algorithm and gene ranking method that can be used to identify DEGs with a higher level of sensitivity and specificity. However, in addition to these recommendations, researchers also want to know which combinations enhance reproducibility. We compared eight conventional methods for ranking genes: weighted average difference (WAD), average difference (AD), fold change (FC), rank products (RP), moderated t statistic (modT), significance analysis of microarrays (samT), shrinkage t statistic (shrinkT), and intensity-based moderated t statistic (ibmT) with six preprocessing algorithms (PLIER, VSN, FARMS, multi-mgMOS (mmgMOS), MBEI, and GCRMA). A total of 36 real experimental datasets was evaluated on the basis of the area under the receiver operating characteristic curve (AUC) as a measure for both sensitivity and specificity. We found that the RP method performed well for VSN-, FARMS-, MBEI-, and GCRMA-preprocessed data, and the WAD method performed well for mmgMOS-preprocessed data. Our analysis of the MicroArray Quality Control (MAQC) project's datasets showed that the FC-based gene ranking methods (WAD, AD, FC, and RP) had a higher level of reproducibility: The percentages of overlapping genes (POGs) across different sites for the FC-based methods were higher overall than those for the t-statistic-based methods (modT, samT, shrinkT, and ibmT). In particular, POG values for WAD were the highest overall among the FC-based methods irrespective of the choice of preprocessing algorithm. Our results demonstrate that to increase sensitivity, specificity, and reproducibility in microarray analyses, we need to select suitable combinations of preprocessing algorithms and gene ranking methods. We recommend the use of FC-based methods, in particular RP or WAD.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2060828805",
    "type": "article"
  },
  {
    "title": "Learning from positive examples when the negative class is undetermined- microRNA gene identification",
    "doi": "https://doi.org/10.1186/1748-7188-3-2",
    "publication_date": "2008-01-28",
    "publication_year": 2008,
    "authors": "Malik Yousef; Segun Jung; Louise C. Showe; Michael K. Showe",
    "corresponding_authors": "",
    "abstract": "The application of machine learning to classification problems that depend only on positive examples is gaining attention in the computational biology community. We and others have described the use of two-class machine learning to identify novel miRNAs. These methods require the generation of an artificial negative class. However, designation of the negative class can be problematic and if it is not properly done can affect the performance of the classifier dramatically and/or yield a biased estimate of performance. We present a study using one-class machine learning for microRNA (miRNA) discovery and compare one-class to two-class approaches using naïve Bayes and Support Vector Machines. These results are compared to published two-class miRNA prediction approaches. We also examine the ability of the one-class and two-class techniques to identify miRNAs in newly sequenced species.Of all methods tested, we found that 2-class naive Bayes and Support Vector Machines gave the best accuracy using our selected features and optimally chosen negative examples. One class methods showed average accuracies of 70-80% versus 90% for the two 2-class methods on the same feature sets. However, some one-class methods outperform some recently published two-class approaches with different selected features. Using the EBV genome as and external validation of the method we found one-class machine learning to work as well as or better than a two-class approach in identifying true miRNAs as well as predicting new miRNAs.One and two class methods can both give useful classification accuracies when the negative class is well characterized. The advantage of one class methods is that it eliminates guessing at the optimal features for the negative class when they are not well defined. In these cases one-class methods can be superior to two-class methods when the features which are chosen as representative of that positive class are well defined.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2063555458",
    "type": "article"
  },
  {
    "title": "Refining transcriptional regulatory networks using network evolutionary models and gene histories",
    "doi": "https://doi.org/10.1186/1748-7188-5-1",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Xiuwei Zhang; Bernard M. E. Moret",
    "corresponding_authors": "",
    "abstract": "Computational inference of transcriptional regulatory networks remains a challenging problem, in part due to the lack of strong network models. In this paper we present evolutionary approaches to improve the inference of regulatory networks for a family of organisms by developing an evolutionary model for these networks and taking advantage of established phylogenetic relationships among these organisms. In previous work, we used a simple evolutionary model and provided extensive simulation results showing that phylogenetic information, combined with such a model, could be used to gain significant improvements on the performance of current inference algorithms.In this paper, we extend the evolutionary model so as to take into account gene duplications and losses, which are viewed as major drivers in the evolution of regulatory networks. We show how to adapt our evolutionary approach to this new model and provide detailed simulation results, which show significant improvement on the reference network inference algorithms. Different evolutionary histories for gene duplications and losses are studied, showing that our adapted approach is feasible under a broad range of conditions. We also provide results on biological data (cis-regulatory modules for 12 species of Drosophila), confirming our simulation results.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2143907820",
    "type": "article"
  },
  {
    "title": "DeBi: Discovering Differentially Expressed Biclusters using a Frequent Itemset Approach",
    "doi": "https://doi.org/10.1186/1748-7188-6-18",
    "publication_date": "2011-06-23",
    "publication_year": 2011,
    "authors": "Akdes Serin; Martin Vingron",
    "corresponding_authors": "",
    "abstract": "The analysis of massive high throughput data via clustering algorithms is very important for elucidating gene functions in biological systems. However, traditional clustering methods have several drawbacks. Biclustering overcomes these limitations by grouping genes and samples simultaneously. It discovers subsets of genes that are co-expressed in certain samples. Recent studies showed that biclustering has a great potential in detecting marker genes that are associated with certain tissues or diseases. Several biclustering algorithms have been proposed. However, it is still a challenge to find biclusters that are significant based on biological validation measures. Besides that, there is a need for a biclustering algorithm that is capable of analyzing very large datasets in reasonable time.Here we present a fast biclustering algorithm called DeBi (Differentially Expressed BIclusters). The algorithm is based on a well known data mining approach called frequent itemset. It discovers maximum size homogeneous biclusters in which each gene is strongly associated with a subset of samples. We evaluate the performance of DeBi on a yeast dataset, on synthetic datasets and on human datasets.We demonstrate that the DeBi algorithm provides functionally more coherent gene sets compared to standard clustering or biclustering algorithms using biological validation measures such as Gene Ontology term and Transcription Factor Binding Site enrichment. We show that DeBi is a computationally efficient and powerful tool in analyzing large datasets. The method is also applicable on multiple gene expression datasets coming from different labs or platforms.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2021542402",
    "type": "article"
  },
  {
    "title": "MRL and SuperFine+MRL: new supertree methods",
    "doi": "https://doi.org/10.1186/1748-7188-7-3",
    "publication_date": "2012-01-26",
    "publication_year": 2012,
    "authors": "Nam Nguyen; Siavash Mirarab; Tandy Warnow",
    "corresponding_authors": "",
    "abstract": "Supertree methods combine trees on subsets of the full taxon set together to produce a tree on the entire set of taxa. Of the many supertree methods, the most popular is MRP (Matrix Representation with Parsimony), a method that operates by first encoding the input set of source trees by a large matrix (the \"MRP matrix\") over {0,1, ?}, and then running maximum parsimony heuristics on the MRP matrix. Experimental studies evaluating MRP in comparison to other supertree methods have established that for large datasets, MRP generally produces trees of equal or greater accuracy than other methods, and can run on larger datasets. A recent development in supertree methods is SuperFine+MRP, a method that combines MRP with a divide-and-conquer approach, and produces more accurate trees in less time than MRP. In this paper we consider a new approach for supertree estimation, called MRL (Matrix Representation with Likelihood). MRL begins with the same MRP matrix, but then analyzes the MRP matrix using heuristics (such as RAxML) for 2-state Maximum Likelihood.We compared MRP and SuperFine+MRP with MRL and SuperFine+MRL on simulated and biological datasets. We examined the MRP and MRL scores of each method on a wide range of datasets, as well as the resulting topological accuracy of the trees. Our experimental results show that MRL, coupled with a very good ML heuristic such as RAxML, produced more accurate trees than MRP, and MRL scores were more strongly correlated with topological accuracy than MRP scores.SuperFine+MRP, when based upon a good MP heuristic, such as TNT, produces among the best scores for both MRP and MRL, and is generally faster and more topologically accurate than other supertree methods we tested.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2040607633",
    "type": "article"
  },
  {
    "title": "Alignment-free phylogeny of whole genomes using underlying subwords",
    "doi": "https://doi.org/10.1186/1748-7188-7-34",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Matteo Comin; Davide Verzotto",
    "corresponding_authors": "",
    "abstract": "With the progress of modern sequencing technologies a large number of complete genomes are now available. Traditionally the comparison of two related genomes is carried out by sequence alignment. There are cases where these techniques cannot be applied, for example if two genomes do not share the same set of genes, or if they are not alignable to each other due to low sequence similarity, rearrangements and inversions, or more specifically to their lengths when the organisms belong to different species. For these cases the comparison of complete genomes can be carried out only with ad hoc methods that are usually called alignment-free methods.In this paper we propose a distance function based on subword compositions called Underlying Approach (UA). We prove that the matching statistics, a popular concept in the field of string algorithms able to capture the statistics of common words between two sequences, can be derived from a small set of \"independent\" subwords, namely the irredundant common subwords. We define a distance-like measure based on these subwords, such that each region of genomes contributes only once, thus avoiding to count shared subwords a multiple number of times. In a nutshell, this filter discards subwords occurring in regions covered by other more significant subwords.The Underlying Approach (UA) builds a scoring function based on this set of patterns, called underlying. We prove that this set is by construction linear in the size of input, without overlaps, and can be efficiently constructed. Results show the validity of our method in the reconstruction of phylogenetic trees, where the Underlying Approach outperforms the current state of the art methods. Moreover, we show that the accuracy of UA is achieved with a very small number of subwords, which in some cases carry meaningful biological information.http://www.dei.unipd.it/∼ciompin/main/underlying.html.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2039491180",
    "type": "article"
  },
  {
    "title": "BicPAM: Pattern-based biclustering for biomedical data analysis",
    "doi": "https://doi.org/10.1186/s13015-014-0027-z",
    "publication_date": "2014-12-01",
    "publication_year": 2014,
    "authors": "Rui Henriques; Sara C. Madeira",
    "corresponding_authors": "",
    "abstract": "Biclustering, the discovery of sets of objects with a coherent pattern across a subset of conditions, is a critical task to study a wide-set of biomedical problems, where molecular units or patients are meaningfully related with a set of properties. The challenging combinatorial nature of this task led to the development of approaches with restrictions on the allowed type, number and quality of biclusters. Contrasting, recent biclustering approaches relying on pattern mining methods can exhaustively discover flexible structures of robust biclusters. However, these approaches are only prepared to discover constant biclusters and their underlying contributions remain dispersed. The proposed BicPAM biclustering approach integrates existing principles made available by state-of-the-art pattern-based approaches with two new contributions. First, BicPAM is the first efficient attempt to exhaustively mine non-constant types of biclusters, including additive and multiplicative coherencies in the presence or absence of symmetries. Second, BicPAM provides strategies to effectively compose different biclustering structures and to handle arbitrary levels of noise inherent to data and with discretization procedures. Results show BicPAM's superiority against its peers and its ability to retrieve unique types of biclusters of interest, to efficiently deliver exhaustive solutions and to successfully recover planted biclusters in datasets with varying levels of missing values and noise. Its application over gene expression data leads to unique solutions with heightened biological relevance. BicPAM approaches integrate existing disperse efforts towards pattern-based biclustering and provides the first critical strategies to efficiently discover exhaustive solutions of biclusters with shifting, scaling and symmetric assumptions with varying quality and underlying structures. Additionally, BicPAM dynamically adapts its behavior to mine data with different levels of missing values and noise.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2100851406",
    "type": "article"
  },
  {
    "title": "Algorithmic approaches to protein-protein interaction site prediction",
    "doi": "https://doi.org/10.1186/s13015-015-0033-9",
    "publication_date": "2015-02-15",
    "publication_year": 2015,
    "authors": "Tristan Aumentado‐Armstrong; Bogdan Istrate; Robert A. Murgita",
    "corresponding_authors": "Robert A. Murgita",
    "abstract": "Interaction sites on protein surfaces mediate virtually all biological activities, and their identification holds promise for disease treatment and drug design. Novel algorithmic approaches for the prediction of these sites have been produced at a rapid rate, and the field has seen significant advancement over the past decade. However, the most current methods have not yet been reviewed in a systematic and comprehensive fashion. Herein, we describe the intricacies of the biological theory, datasets, and features required for modern protein-protein interaction site (PPIS) prediction, and present an integrative analysis of the state-of-the-art algorithms and their performance. First, the major sources of data used by predictors are reviewed, including training sets, evaluation sets, and methods for their procurement. Then, the features employed and their importance in the biological characterization of PPISs are explored. This is followed by a discussion of the methodologies adopted in contemporary prediction programs, as well as their relative performance on the datasets most recently used for evaluation. In addition, the potential utility that PPIS identification holds for rational drug design, hotspot prediction, and computational molecular docking is described. Finally, an analysis of the most promising areas for future development of the field is presented.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2163879391",
    "type": "article"
  },
  {
    "title": "The open-closed mod-minimizer algorithm",
    "doi": "https://doi.org/10.1186/s13015-025-00270-0",
    "publication_date": "2025-03-17",
    "publication_year": 2025,
    "authors": "Ragnar Groot Koerkamp; Daniel Liu; Giulio Ermanno Pibiri",
    "corresponding_authors": "Ragnar Groot Koerkamp; Giulio Ermanno Pibiri",
    "abstract": "Sampling algorithms that deterministically select a subset of $$k$$ -mers are an important building block in bioinformatics applications. For example, they are used to index large textual collections, like DNA, and to compare sequences quickly. In such applications, a sampling algorithm is required to select one $$k$$ -mer out of every window of w consecutive $$k$$ -mers. The folklore and most used scheme is the random minimizer that selects the smallest $$k$$ -mer in the window according to some random order. This scheme is remarkably simple and versatile, and has a density (expected fraction of selected $$k$$ -mers) of $$2/(w+1)$$ . In practice, lower density leads to faster methods and smaller indexes, and it turns out that the random minimizer is not the best one can do. Indeed, some schemes are known to approach optimal density 1/w when $$k \\rightarrow \\infty $$ , like the recently introduced mod-minimizer (Groot Koerkamp and Pibiri, WABI 2024). In this work, we study methods that achieve low density when $$k \\le w$$ . In this small-k regime, a practical method with provably better density than the random minimizer is the miniception (Zheng et al., Bioinformatics 2021). This method can be elegantly described as sampling the smallest closed sycnmer (Edgar, PeerJ 2021) in the window according to some random order. We show that extending the miniception to prefer sampling open syncmers yields much better density. This new method—the open-closed minimizer—offers improved density for small $$k \\le w$$ while being as fast to compute as the random minimizer. Compared to methods based on decycling sets, that achieve very low density in the small-k regime, our method has comparable density while being computationally simpler and intuitive. Furthermore, we extend the mod-minimizer to improve density of any scheme that works well for small k to also work well when $$k > w$$ is large. We hence obtain the open-closed mod-minimizer, a practical method that improves over the mod-minimizer for all k.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408530061",
    "type": "article"
  },
  {
    "title": "Unbiased anchors for reliable genome-wide synteny detection",
    "doi": "https://doi.org/10.1186/s13015-025-00275-9",
    "publication_date": "2025-04-05",
    "publication_year": 2025,
    "authors": "Karl K. Käther; Andreas Remmel; Steffen Lemke; Peter F. Stadler",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4409184210",
    "type": "article"
  },
  {
    "title": "Analysis of computational approaches for motif discovery",
    "doi": "https://doi.org/10.1186/1748-7188-1-8",
    "publication_date": "2006-05-19",
    "publication_year": 2006,
    "authors": "Nan Li; Martin Tompa",
    "corresponding_authors": "",
    "abstract": "Recently, we performed an assessment of 13 popular computational tools for discovery of transcription factor binding sites (M. Tompa, N. Li, et al., \"Assessing Computational Tools for the Discovery of Transcription Factor Binding Sites\", Nature Biotechnology, Jan. 2005). This paper contains follow-up analysis of the assessment results, and raises and discusses some important issues concerning the state of the art in motif discovery methods: 1. We categorize the objective functions used by existing tools, and design experiments to evaluate whether any of these objective functions is the right one to optimize. 2. We examine various features of the data sets that were used in the assessment, such as sequence length and motif degeneracy, and identify which features make data sets hard for current motif discovery tools. 3. We identify an important feature that has not yet been used by existing tools and propose a new objective function that incorporates this feature.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W1815199292",
    "type": "article"
  },
  {
    "title": "A polynomial time biclustering algorithm for finding approximate expression patterns in gene expression time series",
    "doi": "https://doi.org/10.1186/1748-7188-4-8",
    "publication_date": "2009-06-04",
    "publication_year": 2009,
    "authors": "Sara C. Madeira; Arlindo L. Oliveira",
    "corresponding_authors": "",
    "abstract": "The ability to monitor the change in expression patterns over time, and to observe the emergence of coherent temporal responses using gene expression time series, obtained from microarray experiments, is critical to advance our understanding of complex biological processes. In this context, biclustering algorithms have been recognized as an important tool for the discovery of local expression patterns, which are crucial to unravel potential regulatory mechanisms. Although most formulations of the biclustering problem are NP-hard, when working with time series expression data the interesting biclusters can be restricted to those with contiguous columns. This restriction leads to a tractable problem and enables the design of efficient biclustering algorithms able to identify all maximal contiguous column coherent biclusters. In this work, we propose e-CCC-Biclustering, a biclustering algorithm that finds and reports all maximal contiguous column coherent biclusters with approximate expression patterns in time polynomial in the size of the time series gene expression matrix. This polynomial time complexity is achieved by manipulating a discretized version of the original matrix using efficient string processing techniques. We also propose extensions to deal with missing values, discover anticorrelated and scaled expression patterns, and different ways to compute the errors allowed in the expression patterns. We propose a scoring criterion combining the statistical significance of expression patterns with a similarity measure between overlapping biclusters. We present results in real data showing the effectiveness of e-CCC-Biclustering and its relevance in the discovery of regulatory modules describing the transcriptomic expression patterns occurring in Saccharomyces cerevisiae in response to heat stress. In particular, the results show the advantage of considering approximate patterns when compared to state of the art methods that require exact matching of gene expression time series. The identification of co-regulated genes, involved in specific biological processes, remains one of the main avenues open to researchers studying gene regulatory networks. The ability of the proposed methodology to efficiently identify sets of genes with similar expression patterns is shown to be instrumental in the discovery of relevant biological phenomena, leading to more convincing evidence of specific regulatory mechanisms. A prototype implementation of the algorithm coded in Java together with the dataset and examples used in the paper is available in http://kdbio.inesc-id.pt/software/e-ccc-biclustering .",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2037400115",
    "type": "article"
  },
  {
    "title": "Accuracy of phylogeny reconstruction methods combining overlapping gene data sets",
    "doi": "https://doi.org/10.1186/1748-7188-5-37",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Anne Kupczok; Heiko A. Schmidt; Arndt von Haeseler",
    "corresponding_authors": "",
    "abstract": "Abstract Background The availability of many gene alignments with overlapping taxon sets raises the question of which strategy is the best to infer species phylogenies from multiple gene information. Methods and programs abound that use the gene alignment in different ways to reconstruct the species tree. In particular, different methods combine the original data at different points along the way from the underlying sequences to the final tree. Accordingly, they are classified into superalignment, supertree and medium-level approaches. Here, we present a simulation study to compare different methods from each of these three approaches. Results We observe that superalignment methods usually outperform the other approaches over a wide range of parameters including sparse data and gene-specific evolutionary parameters. In the presence of high incongruency among gene trees, however, other combination methods show better performance than the superalignment approach. Surprisingly, some supertree and medium-level methods exhibit, on average, worse results than a single gene phylogeny with complete taxon information. Conclusions For some methods, using the reconstructed gene tree as an estimation of the species tree is superior to the combination of incomplete information. Superalignment usually performs best since it is less susceptible to stochastic error. Supertree methods can outperform superalignment in the presence of gene-tree conflict.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2099549203",
    "type": "article"
  },
  {
    "title": "RNA Accessibility in cubic time",
    "doi": "https://doi.org/10.1186/1748-7188-6-3",
    "publication_date": "2011-03-09",
    "publication_year": 2011,
    "authors": "Stephan Wolf; Ullrike Mückstein; Ivo L. Hofacker",
    "corresponding_authors": "",
    "abstract": "The accessibility of RNA binding motifs controls the efficacy of many biological processes. Examples are the binding of miRNA, siRNA or bacterial sRNA to their respective targets. Similarly, the accessibility of the Shine-Dalgarno sequence is essential for translation to start in prokaryotes. Furthermore, many classes of RNA binding proteins require the binding site to be single-stranded.We introduce a way to compute the accessibility of all intervals within an RNA sequence in (n3) time. This improves on previous implementations where only intervals of one defined length were computed in the same time. While the algorithm is in the same efficiency class as sampling approaches, the results, especially if the probabilities get small, are much more exact.Our algorithm significantly speeds up methods for the prediction of RNA-RNA interactions and other applications that require the accessibility of RNA molecules. The algorithm is already available in the program RNAplfold of the ViennaRNA package.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2059051300",
    "type": "article"
  },
  {
    "title": "An automated stochastic approach to the identification of the protein specificity determinants and functional subfamilies",
    "doi": "https://doi.org/10.1186/1748-7188-5-29",
    "publication_date": "2010-07-15",
    "publication_year": 2010,
    "authors": "Pavel Mazin; Mikhail S. Gelfand; Andrey A. Mironov; A. B. Rakhmaninova; Anatoly R. Rubinov; Robert B. Russell; Olga V. Kalinina",
    "corresponding_authors": "",
    "abstract": "Recent progress in sequencing and 3 D structure determination techniques stimulated development of approaches aimed at more precise annotation of proteins, that is, prediction of exact specificity to a ligand or, more broadly, to a binding partner of any kind.We present a method, SDPclust, for identification of protein functional subfamilies coupled with prediction of specificity-determining positions (SDPs). SDPclust predicts specificity in a phylogeny-independent stochastic manner, which allows for the correct identification of the specificity for proteins that are separated on a phylogenetic tree, but still bind the same ligand. SDPclust is implemented as a Web-server http://bioinf.fbb.msu.ru/SDPfoxWeb/ and a stand-alone Java application available from the website.SDPclust performs a simultaneous identification of specificity determinants and specificity groups in a statistically robust and phylogeny-independent manner.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2102931666",
    "type": "article"
  },
  {
    "title": "Inverse folding of RNA pseudoknot structures",
    "doi": "https://doi.org/10.1186/1748-7188-5-27",
    "publication_date": "2010-06-23",
    "publication_year": 2010,
    "authors": "James Gao; Linda Y. M. Li; Christian M. Reidys",
    "corresponding_authors": "",
    "abstract": "RNA exhibits a variety of structural configurations. Here we consider a structure to be tantamount to the noncrossing Watson-Crick and G-U-base pairings (secondary structure) and additional cross-serial base pairs. These interactions are called pseudoknots and are observed across the whole spectrum of RNA functionalities. In the context of studying natural RNA structures, searching for new ribozymes and designing artificial RNA, it is of interest to find RNA sequences folding into a specific structure and to analyze their induced neutral networks. Since the established inverse folding algorithms, RNAinverse, RNA-SSD as well as INFO-RNA are limited to RNA secondary structures, we present in this paper the inverse folding algorithm Inv which can deal with 3-noncrossing, canonical pseudoknot structures.In this paper we present the inverse folding algorithm Inv. We give a detailed analysis of Inv, including pseudocodes. We show that Inv allows to design in particular 3-noncrossing nonplanar RNA pseudoknot 3-noncrossing RNA structures-a class which is difficult to construct via dynamic programming routines. Inv is freely available at http://www.combinatorics.cn/cbpc/inv.html.The algorithm Inv extends inverse folding capabilities to RNA pseudoknot structures. In comparison with RNAinverse it uses new ideas, for instance by considering sets of competing structures. As a result, Inv is not only able to find novel sequences even for RNA secondary structures, it does so in the context of competing structures that potentially exhibit cross-serial interactions.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2054167614",
    "type": "article"
  },
  {
    "title": "Comparative analysis of the quality of a global algorithm and a local algorithm for alignment of two sequences",
    "doi": "https://doi.org/10.1186/1748-7188-6-25",
    "publication_date": "2011-10-27",
    "publication_year": 2011,
    "authors": "Valery Polyanovsky; Mikhail Roytberg; V. G. Tumanyan",
    "corresponding_authors": "",
    "abstract": "Algorithms of sequence alignment are the key instruments for computer-assisted studies of biopolymers. Obviously, it is important to take into account the \"quality\" of the obtained alignments, i.e. how closely the algorithms manage to restore the \"gold standard\" alignment (GS-alignment), which superimposes positions originating from the same position in the common ancestor of the compared sequences. As an approximation of the GS-alignment, a 3D-alignment is commonly used not quite reasonably. Among the currently used algorithms of a pair-wise alignment, the best quality is achieved by using the algorithm of optimal alignment based on affine penalties for deletions (the Smith-Waterman algorithm). Nevertheless, the expedience of using local or global versions of the algorithm has not been studied.Using model series of amino acid sequence pairs, we studied the relative \"quality\" of results produced by local and global alignments versus (1) the relative length of similar parts of the sequences (their \"cores\") and their nonhomologous parts, and (2) relative positions of the core regions in the compared sequences. We obtained numerical values of the average quality (measured as accuracy and confidence) of the global alignment method and the local alignment method for evolutionary distances between homologous sequence parts from 30 to 240 PAM and for the core length making from 10% to 70% of the total length of the sequences for all possible positions of homologous sequence parts relative to the centers of the sequences.We revealed criteria allowing to specify conditions of preferred applicability for the local and the global alignment algorithms depending on positions and relative lengths of the cores and nonhomologous parts of the sequences to be aligned. It was demonstrated that when the core part of one sequence was positioned above the core of the other sequence, the global algorithm was more stable at longer evolutionary distances and larger nonhomologous parts than the local algorithm. On the contrary, when the cores were positioned asymmetrically, the local algorithm was more stable at longer evolutionary distances and larger nonhomologous parts than the global algorithm. This opens a possibility for creation of a combined method allowing generation of more accurate alignments.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W1991467930",
    "type": "article"
  },
  {
    "title": "An enhancement of binary particle swarm optimization for gene selection in classifying cancer classes",
    "doi": "https://doi.org/10.1186/1748-7188-8-15",
    "publication_date": "2013-04-24",
    "publication_year": 2013,
    "authors": "Mohd Saberi Mohamad; Sigeru Omatu; Safaai Deris; Michifumi Yoshioka; Afnizanfaizal Abdullah; Zuwairie Ibrahim",
    "corresponding_authors": "",
    "abstract": "Gene expression data could likely be a momentous help in the progress of proficient cancer diagnoses and classification platforms. Lately, many researchers analyze gene expression data using diverse computational intelligence methods, for selecting a small subset of informative genes from the data for cancer classification. Many computational methods face difficulties in selecting small subsets due to the small number of samples compared to the huge number of genes (high-dimension), irrelevant genes, and noisy genes. We propose an enhanced binary particle swarm optimization to perform the selection of small subsets of informative genes which is significant for cancer classification. Particle speed, rule, and modified sigmoid function are introduced in this proposed method to increase the probability of the bits in a particle’s position to be zero. The method was empirically applied to a suite of ten well-known benchmark gene expression data sets. The performance of the proposed method proved to be superior to other previous related works, including the conventional version of binary particle swarm optimization (BPSO) in terms of classification accuracy and the number of selected genes. The proposed method also requires lower computational time compared to BPSO.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2140725952",
    "type": "article"
  },
  {
    "title": "Coexpression and coregulation analysis of time-series gene expression data in estrogen-induced breast cancer cell",
    "doi": "https://doi.org/10.1186/1748-7188-8-9",
    "publication_date": "2013-03-23",
    "publication_year": 2013,
    "authors": "Anirban Bhar; Martin Haubrock; Anirban Mukhopadhyay; Ujjwal Maulik; Sanghamitra Bandyopadhyay; Edgar Wingender",
    "corresponding_authors": "",
    "abstract": "Estrogen is a chemical messenger that has an influence on many breast cancers as it helps cells to grow and divide. These cancers are often known as estrogen responsive cancers in which estrogen receptor occupies the surface of the cells. The successful treatment of breast cancers requires understanding gene expression, identifying of tumor markers, acquiring knowledge of cellular pathways, etc. In this paper we introduce our proposed triclustering algorithm δ-TRIMAX that aims to find genes that are coexpressed over subset of samples across a subset of time points. Here we introduce a novel mean-squared residue for such 3D dataset. Our proposed algorithm yields triclusters that have a mean-squared residue score below a threshold δ.We have applied our algorithm on one simulated dataset and one real-life dataset. The real-life dataset is a time-series dataset in estrogen induced breast cancer cell line. To establish the biological significance of genes belonging to resultant triclusters we have performed gene ontology, KEGG pathway and transcription factor binding site enrichment analysis. Additionally, we represent each resultant tricluster by computing its eigengene and verify whether its eigengene is also differentially expressed at early, middle and late estrogen responsive stages. We also identified hub-genes for each resultant triclusters and verified whether the hub-genes are found to be associated with breast cancer. Through our analysis CCL2, CD47, NFIB, BRD4, HPGD, CSNK1E, NPC1L1, PTEN, PTPN2 and ADAM9 are identified as hub-genes which are already known to be associated with breast cancer. The other genes that have also been identified as hub-genes might be associated with breast cancer or estrogen responsive elements. The TFBS enrichment analysis also reveals that transcription factor POU2F1 binds to the promoter region of ESR1 that encodes estrogen receptor α. Transcription factor E2F1 binds to the promoter regions of coexpressed genes MCM7, ANAPC1 and WEE1.Thus our integrative approach provides insights into breast cancer prognosis.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2116476540",
    "type": "article"
  },
  {
    "title": "The difficulty of protein structure alignment under the RMSD",
    "doi": "https://doi.org/10.1186/1748-7188-8-1",
    "publication_date": "2013-01-04",
    "publication_year": 2013,
    "authors": "Shuai Cheng Li",
    "corresponding_authors": "Shuai Cheng Li",
    "abstract": "Protein structure alignment is often modeled as the largest common point set (LCP) problem based on the Root Mean Square Deviation (RMSD), a measure commonly used to evaluate structural similarity. In the problem, each residue is represented by the coordinate of the Cα atom, and a structure is modeled as a sequence of 3D points. Out of two such sequences, one is to find two equal-sized subsequences of the maximum length, and a bijection between the points of the subsequences which gives an RMSD within a given threshold. The problem is considered to be difficult in terms of time complexity, but the reasons for its difficulty is not well-understood. Improving this time complexity is considered important in protein structure prediction and structural comparison, where the task of comparing very numerous structures is commonly encountered. To study why the LCP problem is difficult, we define a natural variant of the problem, called the minimum aligned distance (MAD). In the MAD problem, the length of the subsequences to obtain is specified in the input; and instead of fulfilling a threshold, the RMSD between the points of the two subsequences is to be minimized. Our results show that the difficulty of the two problems does not lie solely in the combinatorial complexity of finding the optimal subsequences, or in the task of superimposing the structures. By placing a limit on the distance between consecutive points, and assuming that the points are specified as integral values, we show that both problems are equally difficult, in the sense that they are reducible to each other. In this case, both problems can be exactly solved in polynomial time, although the time complexity remains high. We showed insights and techniques which we hope will lead to practical algorithms for the LCP problem for protein structures. The study identified two important factors in the problem's complexity: (1) The lack of a limit in the distance between the consecutive points of a structure; (2) The arbitrariness of the precision allowed in the input values. Both issues are of little practical concern for the purpose of protein structure alignment. When these factors are removed, the LCP problem is as hard as that of minimizing the RMSD (MAD problem), and can be solved exactly in polynomial time.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2126300961",
    "type": "article"
  },
  {
    "title": "A graph extension of the positional Burrows–Wheeler transform and its applications",
    "doi": "https://doi.org/10.1186/s13015-017-0109-9",
    "publication_date": "2017-07-11",
    "publication_year": 2017,
    "authors": "Adam M. Novak; Erik Garrison; Benedict Paten",
    "corresponding_authors": "",
    "abstract": "We present a generalization of the positional Burrows–Wheeler transform, or PBWT, to genome graphs, which we call the gPBWT. A genome graph is a collapsed representation of a set of genomes described as a graph. In a genome graph, a haplotype corresponds to a restricted form of walk. The gPBWT is a compressible representation of a set of these graph-encoded haplotypes that allows for efficient subhaplotype match queries. We give efficient algorithms for gPBWT construction and query operations. As a demonstration, we use the gPBWT to quickly count the number of haplotypes consistent with random walks in a genome graph, and with the paths taken by mapped reads; results suggest that haplotype consistency information can be practically incorporated into graph-based read mappers. We estimate that with the gPBWT of the order of 100,000 diploid genomes, including all forms structural variation, could be stored and made searchable for haplotype queries using a single large compute node.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2786984148",
    "type": "article"
  },
  {
    "title": "The link between orthology relations and gene trees: a correction perspective",
    "doi": "https://doi.org/10.1186/s13015-016-0067-7",
    "publication_date": "2016-04-15",
    "publication_year": 2016,
    "authors": "Manuel Lafond; Riccardo Dondi; Nadia El-Mabrouk",
    "corresponding_authors": "",
    "abstract": "While tree-oriented methods for inferring orthology and paralogy relations between genes are based on reconciling a gene tree with a species tree, many tree-free methods are also available (usually based on sequence similarity). Recently, the link between orthology relations and gene trees has been formally considered from the perspective of reconstructing phylogenies from orthology relations. In this paper, we consider this link from a correction point of view. Indeed, a gene tree induces a set of relations, but the converse is not always true: a set of relations is not necessarily in agreement with any gene tree. A natural question is thus how to minimally correct an infeasible set of relations. Another natural question, given a gene tree and a set of relations, is how to minimally correct a gene tree so that the resulting gene tree fits the set of relations.We consider four variants of relation and gene tree correction problems, and provide hardness results for all of them. More specifically, we show that it is NP-Hard to edit a minimum of set of relations to make them consistent with a given species tree. We also show that the problem of finding a maximum subset of genes that share consistent relations is hard to approximate. We then demonstrate that editing a gene tree to satisfy a given set of relations in a minimum way is NP-Hard, where \"minimum\" refers either to the number of modified relations depicted by the gene tree or the number of clades that are lost. We also discuss some of the algorithmic perspectives given these hardness results.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2339182468",
    "type": "article"
  },
  {
    "title": "Circular sequence comparison: algorithms and applications",
    "doi": "https://doi.org/10.1186/s13015-016-0076-6",
    "publication_date": "2016-05-10",
    "publication_year": 2016,
    "authors": "Roberto Grossi; Costas S. Iliopoulos; Robert Mercaş; Nadia Pisanti; Solon P. Pissis; Ahmad Retha; Fatima Vayani",
    "corresponding_authors": "",
    "abstract": "Sequence comparison is a fundamental step in many important tasks in bioinformatics; from phylogenetic reconstruction to the reconstruction of genomes. Traditional algorithms for measuring approximation in sequence comparison are based on the notions of distance or similarity, and are generally computed through sequence alignment techniques. As circular molecular structure is a common phenomenon in nature, a caveat of the adaptation of alignment techniques for circular sequence comparison is that they are computationally expensive, requiring from super-quadratic to cubic time in the length of the sequences. In this paper, we introduce a new distance measure based on q-grams, and show how it can be applied effectively and computed efficiently for circular sequence comparison. Experimental results, using real DNA, RNA, and protein sequences as well as synthetic data, demonstrate orders-of-magnitude superiority of our approach in terms of efficiency, while maintaining an accuracy very competitive to the state of the art.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2345982775",
    "type": "article"
  },
  {
    "title": "Double and multiple knockout simulations for genome-scale metabolic network reconstructions",
    "doi": "https://doi.org/10.1186/s13015-014-0028-y",
    "publication_date": "2015-01-08",
    "publication_year": 2015,
    "authors": "Yaron Goldstein; Alexander Bockmayr",
    "corresponding_authors": "",
    "abstract": "Constraint-based modeling of genome-scale metabolic network reconstructions has become a widely used approach in computational biology. Flux coupling analysis is a constraint-based method that analyses the impact of single reaction knockouts on other reactions in the network. We present an extension of flux coupling analysis for double and multiple gene or reaction knockouts, and develop corresponding algorithms for an in silico simulation. To evaluate our method, we perform a full single and double knockout analysis on a selection of genome-scale metabolic network reconstructions and compare the results. A prototype implementation of double knockout simulation is available at http://hoverboard.io/L4FC .",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W1983048808",
    "type": "article"
  },
  {
    "title": "BicNET: Flexible module discovery in large-scale biological networks using biclustering",
    "doi": "https://doi.org/10.1186/s13015-016-0074-8",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Rui Henriques; Sara C. Madeira",
    "corresponding_authors": "",
    "abstract": "Despite the recognized importance of module discovery in biological networks to enhance our understanding of complex biological systems, existing methods generally suffer from two major drawbacks. First, there is a focus on modules where biological entities are strongly connected, leading to the discovery of trivial/well-known modules and to the inaccurate exclusion of biological entities with subtler yet relevant roles. Second, there is a generalized intolerance towards different forms of noise, including uncertainty associated with less-studied biological entities (in the context of literature-driven networks) and experimental noise (in the context of data-driven networks). Although state-of-the-art biclustering algorithms are able to discover modules with varying coherency and robustness to noise, their application for the discovery of non-dense modules in biological networks has been poorly explored and it is further challenged by efficiency bottlenecks. This work proposes Biclustering NETworks (BicNET), a biclustering algorithm to discover non-trivial yet coherent modules in weighted biological networks with heightened efficiency. Three major contributions are provided. First, we motivate the relevance of discovering network modules given by constant, symmetric, plaid and order-preserving biclustering models. Second, we propose an algorithm to discover these modules and to robustly handle noisy and missing interactions. Finally, we provide new searches to tackle time and memory bottlenecks by effectively exploring the inherent structural sparsity of network data. Results in synthetic network data confirm the soundness, efficiency and superiority of BicNET. The application of BicNET on protein interaction and gene interaction networks from yeast, E. coli and Human reveals new modules with heightened biological significance. BicNET is, to our knowledge, the first method enabling the efficient unsupervised analysis of large-scale network data for the discovery of coherent modules with parameterizable homogeneity.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2405878133",
    "type": "article"
  },
  {
    "title": "A two-phase binning algorithm using l-mer frequency on groups of non-overlapping reads",
    "doi": "https://doi.org/10.1186/s13015-014-0030-4",
    "publication_date": "2015-01-15",
    "publication_year": 2015,
    "authors": "Le Van Vinh; Lang Tran; Lê Thanh Bình; Trần Văn Hoài",
    "corresponding_authors": "Lang Tran",
    "abstract": "Metagenomics is the study of genetic materials derived directly from complex microbial samples, instead of from culture. One of the crucial steps in metagenomic analysis, referred to as \"binning\", is to separate reads into clusters that represent genomes from closely related organisms. Among the existing binning methods, unsupervised methods base the classification on features extracted from reads, and especially taking advantage in case of the limitation of reference database availability. However, their performance, under various aspects, is still being investigated by recent theoretical and empirical studies. The one addressed in this paper is among those efforts to enhance the accuracy of the classification.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2137604316",
    "type": "article"
  },
  {
    "title": "On weighted k-mer dictionaries",
    "doi": "https://doi.org/10.1186/s13015-023-00226-2",
    "publication_date": "2023-06-17",
    "publication_year": 2023,
    "authors": "Giulio Ermanno Pibiri",
    "corresponding_authors": "Giulio Ermanno Pibiri",
    "abstract": "We consider the problem of representing a set of [Formula: see text]-mers and their abundance counts, or weights, in compressed space so that assessing membership and retrieving the weight of a [Formula: see text]-mer is efficient. The representation is called a weighted dictionary of [Formula: see text]-mers and finds application in numerous tasks in Bioinformatics that usually count [Formula: see text]-mers as a pre-processing step. In fact, [Formula: see text]-mer counting tools produce very large outputs that may result in a severe bottleneck for subsequent processing. In this work we extend the recently introduced SSHash dictionary (Pibiri in Bioinformatics 38:185-194, 2022) to also store compactly the weights of the [Formula: see text]-mers. From a technical perspective, we exploit the order of the [Formula: see text]-mers represented in SSHash to encode runs of weights, hence allowing much better compression than the empirical entropy of the weights. We study the problem of reducing the number of runs in the weights to improve compression even further and give an optimal algorithm for this problem. Lastly, we corroborate our findings with experiments on real-world datasets and comparison with competitive alternatives. Up to date, SSHash is the only [Formula: see text]-mer dictionary that is exact, weighted, associative, fast, and small.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4381046832",
    "type": "article"
  },
  {
    "title": "Decomposition of overlapping protein complexes: A graph theoretical method for analyzing static and dynamic protein associations",
    "doi": "https://doi.org/10.1186/1748-7188-1-7",
    "publication_date": "2006-04-26",
    "publication_year": 2006,
    "authors": "Elena Zotenko; Katia S. Guimarães; Raja Jothi; Teresa M. Przytycka",
    "corresponding_authors": "",
    "abstract": "Most cellular processes are carried out by multi-protein complexes, groups of proteins that bind together to perform a specific task. Some proteins form stable complexes, while other proteins form transient associations and are part of several complexes at different stages of a cellular process. A better understanding of this higher-order organization of proteins into overlapping complexes is an important step towards unveiling functional and evolutionary mechanisms behind biological networks.We propose a new method for identifying and representing overlapping protein complexes (or larger units called functional groups) within a protein interaction network. We develop a graph-theoretical framework that enables automatic construction of such representation. We illustrate the effectiveness of our method by applying it to TNFalpha/NF-kappaB and pheromone signaling pathways.The proposed representation helps in understanding the transitions between functional groups and allows for tracking a protein's path through a cascade of functional groups. Therefore, depending on the nature of the network, our representation is capable of elucidating temporal relations between functional groups. Our results show that the proposed method opens a new avenue for the analysis of protein interaction networks.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W1589640109",
    "type": "article"
  },
  {
    "title": "Multiple sequence alignment with user-defined anchor points",
    "doi": "https://doi.org/10.1186/1748-7188-1-6",
    "publication_date": "2006-04-19",
    "publication_year": 2006,
    "authors": "Burkhard Morgenstern; Sonja J. Prohaska; Dirk Pöhler; Peter F. Stadler",
    "corresponding_authors": "",
    "abstract": "Automated software tools for multiple alignment often fail to produce biologically meaningful results. In such situations, expert knowledge can help to improve the quality of alignments. Herein, we describe a semi-automatic version of the alignment program DIALIGN that can take pre-defined constraints into account. It is possible for the user to specify parts of the sequences that are assumed to be homologous and should therefore be aligned to each other. Our software program can use these sites as anchor points by creating a multiple alignment respecting these constraints. This way, our alignment method can produce alignments that are biologically more meaningful than alignments produced by fully automated procedures. As a demonstration of how our method works, we apply our approach to genomic sequences around the Hox gene cluster and to a set of DNA-binding proteins. As a by-product, we obtain insights about the performance of the greedy algorithm that our program uses for multiple alignment and about the underlying objective function. This information will be useful for the further development of DIALIGN. The described alignment approach has been integrated into the TRACKER software system.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2147996827",
    "type": "article"
  },
  {
    "title": "Finding coevolving amino acid residues using row and column weighting of mutual information and multi-dimensional amino acid representation",
    "doi": "https://doi.org/10.1186/1748-7188-2-12",
    "publication_date": "2007-10-03",
    "publication_year": 2007,
    "authors": "Rodrigo Gouveia‐Oliveira; Anders Gorm Pedersen",
    "corresponding_authors": "",
    "abstract": "Some amino acid residues functionally interact with each other. This interaction will result in an evolutionary co-variation between these residues - coevolution. Our goal is to find these coevolving residues.We present six new methods for detecting coevolving residues. Among other things, we suggest measures that are variants of Mutual Information, and measures that use a multidimensional representation of each residue in order to capture the physico-chemical similarities between amino acids. We created a benchmarking system, in silico, able to evaluate these methods through a wide range of realistic conditions. Finally, we use the combination of different methods as a way of improving performance.Our best method (Row and Column Weighed Mutual Information) has an estimated accuracy increase of 63% over Mutual Information. Furthermore, we show that the combination of different methods is efficient, and that the methods are quite sensitive to the different conditions tested.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2153819533",
    "type": "article"
  },
  {
    "title": "Metabolite-based clustering and visualization of mass spectrometry data using one-dimensional self-organizing maps",
    "doi": "https://doi.org/10.1186/1748-7188-3-9",
    "publication_date": "2008-06-26",
    "publication_year": 2008,
    "authors": "Peter Meinicke; Thomas Lingner; Alexander Kaever; Kirstin Feussner; Cornelia Göbel; Ivo Feußner; Petr Karlovský; Burkhard Morgenstern",
    "corresponding_authors": "",
    "abstract": "One of the goals of global metabolomic analysis is to identify metabolic markers that are hidden within a large background of data originating from high-throughput analytical measurements. Metabolite-based clustering is an unsupervised approach for marker identification based on grouping similar concentration profiles of putative metabolites. A major problem of this approach is that in general there is no prior information about an adequate number of clusters. We present an approach for data mining on metabolite intensity profiles as obtained from mass spectrometry measurements. We propose one-dimensional self-organizing maps for metabolite-based clustering and visualization of marker candidates. In a case study on the wound response of Arabidopsis thaliana, based on metabolite profile intensities from eight different experimental conditions, we show how the clustering and visualization capabilities can be used to identify relevant groups of markers. Our specialized realization of self-organizing maps is well-suitable to gain insight into complex pattern variation in a large set of metabolite profiles. In comparison to other methods our visualization approach facilitates the identification of interesting groups of metabolites by means of a convenient overview on relevant intensity patterns. In particular, the visualization effectively supports researchers in analyzing many putative clusters when the true number of biologically meaningful groups is unknown.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2135026494",
    "type": "article"
  },
  {
    "title": "On the optimality of the neighbor-joining algorithm",
    "doi": "https://doi.org/10.1186/1748-7188-3-5",
    "publication_date": "2008-04-30",
    "publication_year": 2008,
    "authors": "Kord Eickmeyer; Peter Huggins; Lior Pachter; Ruriko Yoshida",
    "corresponding_authors": "",
    "abstract": "The popular neighbor-joining (NJ) algorithm used in phylogenetics is a greedy algorithm for finding the balanced minimum evolution (BME) tree associated to a dissimilarity map. From this point of view, NJ is \"optimal\" when the algorithm outputs the tree which minimizes the balanced minimum evolution criterion. We use the fact that the NJ tree topology and the BME tree topology are determined by polyhedral subdivisions of the spaces of dissimilarity maps [equation; see text] to study the optimality of the neighbor-joining algorithm. In particular, we investigate and compare the polyhedral subdivisions for n </= 8. This requires the measurement of volumes of spherical polytopes in high dimension, which we obtain using a combination of Monte Carlo methods and polyhedral algorithms. Our results include a demonstration that highly unrelated trees can be co-optimal in BME reconstruction, and that NJ regions are not convex. We obtain the l2 radius for neighbor-joining for n = 5 and we conjecture that the ability of the neighbor-joining algorithm to recover the BME tree depends on the diameter of the BME tree.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2091748129",
    "type": "article"
  },
  {
    "title": "Fast prediction of RNA-RNA interaction",
    "doi": "https://doi.org/10.1186/1748-7188-5-5",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Raheleh Salari; Rolf Backofen; S. Cenk Şahinalp",
    "corresponding_authors": "",
    "abstract": "Regulatory antisense RNAs are a class of ncRNAs that regulate gene expression by prohibiting the translation of an mRNA by establishing stable interactions with a target sequence. There is great demand for efficient computational methods to predict the specific interaction between an ncRNA and its target mRNA(s). There are a number of algorithms in the literature which can predict a variety of such interactions--unfortunately at a very high computational cost. Although some existing target prediction approaches are much faster, they are specialized for interactions with a single binding site.In this paper we present a novel algorithm to accurately predict the minimum free energy structure of RNA-RNA interaction under the most general type of interactions studied in the literature. Moreover, we introduce a fast heuristic method to predict the specific (multiple) binding sites of two interacting RNAs.We verify the performance of our algorithms for joint structure and binding site prediction on a set of known interacting RNA pairs. Experimental results show our algorithms are highly accurate and outperform all competitive approaches.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2132081807",
    "type": "article"
  },
  {
    "title": "An experimental study of Quartets MaxCut and other supertree methods",
    "doi": "https://doi.org/10.1186/1748-7188-6-7",
    "publication_date": "2011-04-19",
    "publication_year": 2011,
    "authors": "M. Shel Swenson; Rahul Suri; C. Randal Linder; Tandy Warnow",
    "corresponding_authors": "",
    "abstract": "Supertree methods represent one of the major ways by which the Tree of Life can be estimated, but despite many recent algorithmic innovations, matrix representation with parsimony (MRP) remains the main algorithmic supertree method.We evaluated the performance of several supertree methods based upon the Quartets MaxCut (QMC) method of Snir and Rao and showed that two of these methods usually outperform MRP and five other supertree methods that we studied, under many realistic model conditions. However, the QMC-based methods have scalability issues that may limit their utility on large datasets. We also observed that taxon sampling impacted supertree accuracy, with poor results obtained when all of the source trees were only sparsely sampled. Finally, we showed that the popular optimality criterion of minimizing the total topological distance of the supertree to the source trees is only weakly correlated with supertree topological accuracy. Therefore evaluating supertree methods on biological datasets is problematic.Our results show that supertree methods that improve upon MRP are possible, and that an effort should be made to produce scalable and robust implementations of the most accurate supertree methods. Also, because topological accuracy depends upon taxon sampling strategies, attempts to construct very large phylogenetic trees using supertree methods should consider the selection of source tree datasets, as well as supertree methods. Finally, since supertree topological error is only weakly correlated with the supertree's topological distance to its source trees, development and testing of supertree methods presents methodological challenges.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2118058771",
    "type": "article"
  },
  {
    "title": "A tree-based method for the rapid screening of chemical fingerprints",
    "doi": "https://doi.org/10.1186/1748-7188-5-9",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Thomas G. Kristensen; Jesper Buus Nielsen; Christian NS Pedersen",
    "corresponding_authors": "",
    "abstract": "The fingerprint of a molecule is a bitstring based on its structure, constructed such that structurally similar molecules will have similar fingerprints. Molecular fingerprints can be used in an initial phase of drug development for identifying novel drug candidates by screening large databases for molecules with fingerprints similar to a query fingerprint. In this paper, we present a method which efficiently finds all fingerprints in a database with Tanimoto coefficient to the query fingerprint above a user defined threshold. The method is based on two novel data structures for rapid screening of large databases: the k D grid and the Multibit tree. The k D grid is based on splitting the fingerprints into k shorter bitstrings and utilising these to compute bounds on the similarity of the complete bitstrings. The Multibit tree uses hierarchical clustering and similarity within each cluster to compute similar bounds. We have implemented our method and tested it on a large real-world data set. Our experiments show that our method yields approximately a three-fold speed-up over previous methods. Using the novel k D grid and Multibit tree significantly reduce the time needed for searching databases of fingerprints. This will allow researchers to (1) perform more searches than previously possible and (2) to easily search large databases.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2118353525",
    "type": "article"
  },
  {
    "title": "Inferring species trees from incongruent multi-copy gene trees using the Robinson-Foulds distance",
    "doi": "https://doi.org/10.1186/1748-7188-8-28",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Ruchi Chaudhary; J. Gordon Burleigh; David Fernández‐Baca",
    "corresponding_authors": "",
    "abstract": "Constructing species trees from multi-copy gene trees remains a challenging problem in phylogenetics. One difficulty is that the underlying genes can be incongruent due to evolutionary processes such as gene duplication and loss, deep coalescence, or lateral gene transfer. Gene tree estimation errors may further exacerbate the difficulties of species tree estimation.We present a new approach for inferring species trees from incongruent multi-copy gene trees that is based on a generalization of the Robinson-Foulds (RF) distance measure to multi-labeled trees (mul-trees). We prove that it is NP-hard to compute the RF distance between two mul-trees; however, it is easy to calculate this distance between a mul-tree and a singly-labeled species tree. Motivated by this, we formulate the RF problem for mul-trees (MulRF) as follows: Given a collection of multi-copy gene trees, find a singly-labeled species tree that minimizes the total RF distance from the input mul-trees. We develop and implement a fast SPR-based heuristic algorithm for the NP-hard MulRF problem.We compare the performance of the MulRF method (available at http://genome.cs.iastate.edu/CBL/MulRF/) with several gene tree parsimony approaches using gene tree simulations that incorporate gene tree error, gene duplications and losses, and/or lateral transfer. The MulRF method produces more accurate species trees than gene tree parsimony approaches. We also demonstrate that the MulRF method infers in minutes a credible plant species tree from a collection of nearly 2,000 gene trees.Our new phylogenetic inference method, based on a generalized RF distance, makes it possible to quickly estimate species trees from large genomic data sets. Since the MulRF method, unlike gene tree parsimony, is based on a generic tree distance measure, it is appealing for analyses of genomic data sets, in which many processes such as deep coalescence, recombination, gene duplication and losses as well as phylogenetic error may contribute to gene tree discord. In experiments, the MulRF method estimated species trees accurately and quickly, demonstrating MulRF as an efficient alternative approach for phylogenetic inference from large-scale genomic data sets.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2126062220",
    "type": "article"
  },
  {
    "title": "Configurable pattern-based evolutionary biclustering of gene expression data",
    "doi": "https://doi.org/10.1186/1748-7188-8-4",
    "publication_date": "2013-02-23",
    "publication_year": 2013,
    "authors": "Beatriz Pontes; Raúl Giráldez; Jesús S. Aguilar–Ruiz",
    "corresponding_authors": "Beatriz Pontes",
    "abstract": "Biclustering algorithms for microarray data aim at discovering functionally related gene sets under different subsets of experimental conditions. Due to the problem complexity and the characteristics of microarray datasets, heuristic searches are usually used instead of exhaustive algorithms. Also, the comparison among different techniques is still a challenge. The obtained results vary in relevant features such as the number of genes or conditions, which makes it difficult to carry out a fair comparison. Moreover, existing approaches do not allow the user to specify any preferences on these properties. Here, we present the first biclustering algorithm in which it is possible to particularize several biclusters features in terms of different objectives. This can be done by tuning the specified features in the algorithm or also by incorporating new objectives into the search. Furthermore, our approach bases the bicluster evaluation in the use of expression patterns, being able to recognize both shifting and scaling patterns either simultaneously or not. Evolutionary computation has been chosen as the search strategy, naming thus our proposal Evo-Bexpa (Evo lutionary B iclustering based in Ex pression Pa tterns). We have conducted experiments on both synthetic and real datasets demonstrating Evo-Bexpa abilities to obtain meaningful biclusters. Synthetic experiments have been designed in order to compare Evo-Bexpa performance with other approaches when looking for perfect patterns. Experiments with four different real datasets also confirm the proper performing of our algorithm, whose results have been biologically validated through Gene Ontology.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2166252916",
    "type": "article"
  },
  {
    "title": "Pattern matching through Chaos Game Representation: bridging numerical and discrete data structures for biological sequence analysis",
    "doi": "https://doi.org/10.1186/1748-7188-7-10",
    "publication_date": "2012-05-02",
    "publication_year": 2012,
    "authors": "Susana Vinga; Alexandra M. Carvalho; Alexandre P. Francisco; Luís M. S.​Russo; Jonas S. Almeida",
    "corresponding_authors": "Susana Vinga",
    "abstract": "Chaos Game Representation (CGR) is an iterated function that bijectively maps discrete sequences into a continuous domain. As a result, discrete sequences can be object of statistical and topological analyses otherwise reserved to numerical systems. Characteristically, CGR coordinates of substrings sharing an L-long suffix will be located within 2-L distance of each other. In the two decades since its original proposal, CGR has been generalized beyond its original focus on genomic sequences and has been successfully applied to a wide range of problems in bioinformatics. This report explores the possibility that it can be further extended to approach algorithms that rely on discrete, graph-based representations.The exploratory analysis described here consisted of selecting foundational string problems and refactoring them using CGR-based algorithms. We found that CGR can take the role of suffix trees and emulate sophisticated string algorithms, efficiently solving exact and approximate string matching problems such as finding all palindromes and tandem repeats, and matching with mismatches. The common feature of these problems is that they use longest common extension (LCE) queries as subtasks of their procedures, which we show to have a constant time solution with CGR. Additionally, we show that CGR can be used as a rolling hash function within the Rabin-Karp algorithm.The analysis of biological sequences relies on algorithmic foundations facing mounting challenges, both logistic (performance) and analytical (lack of unifying mathematical framework). CGR is found to provide the latter and to promise the former: graph-based data structures for sequence analysis operations are entailed by numerical-based data structures produced by CGR maps, providing a unifying analytical framework for a diversity of pattern matching problems.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2111588838",
    "type": "article"
  },
  {
    "title": "HAlign-II: efficient ultra-large multiple sequence alignment and phylogenetic tree reconstruction with distributed and parallel computing",
    "doi": "https://doi.org/10.1186/s13015-017-0116-x",
    "publication_date": "2017-09-29",
    "publication_year": 2017,
    "authors": "Shixiang Wan; Quan Zou",
    "corresponding_authors": "",
    "abstract": "Multiple sequence alignment (MSA) plays a key role in biological sequence analyses, especially in phylogenetic tree construction. Extreme increase in next-generation sequencing results in shortage of efficient ultra-large biological sequence alignment approaches for coping with different sequence types.Distributed and parallel computing represents a crucial technique for accelerating ultra-large (e.g. files more than 1 GB) sequence analyses. Based on HAlign and Spark distributed computing system, we implement a highly cost-efficient and time-efficient HAlign-II tool to address ultra-large multiple biological sequence alignment and phylogenetic tree construction.The experiments in the DNA and protein large scale data sets, which are more than 1GB files, showed that HAlign II could save time and space. It outperformed the current software tools. HAlign-II can efficiently carry out MSA and construct phylogenetic trees with ultra-large numbers of biological sequences. HAlign-II shows extremely high memory efficiency and scales well with increases in computing resource.THAlign-II provides a user-friendly web server based on our distributed computing infrastructure. HAlign-II with open-source codes and datasets was established at http://lab.malab.cn/soft/halign.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2606325896",
    "type": "article"
  },
  {
    "title": "Biologically feasible gene trees, reconciliation maps and informative triples",
    "doi": "https://doi.org/10.1186/s13015-017-0114-z",
    "publication_date": "2017-08-29",
    "publication_year": 2017,
    "authors": "Marc Hellmuth",
    "corresponding_authors": "Marc Hellmuth",
    "abstract": "The history of gene families-which are equivalent to",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2734849906",
    "type": "article"
  },
  {
    "title": "EUCALYPT: efficient tree reconciliation enumerator",
    "doi": "https://doi.org/10.1186/s13015-014-0031-3",
    "publication_date": "2015-01-23",
    "publication_year": 2015,
    "authors": "Beatrice Donati; Christian Baudet; Blerina Sinaimeri; Pierluigi Crescenzi; Marie-France Sagot",
    "corresponding_authors": "",
    "abstract": "Phylogenetic tree reconciliation is the approach of choice for investigating the coevolution of sets of organisms such as hosts and parasites. It consists in a mapping between the parasite tree and the host tree using event-based maximum parsimony. Given a cost model for the events, many optimal reconciliations are however possible. Any further biological interpretation of them must therefore take this into account, making the capacity to enumerate all optimal solutions a crucial point. Only two algorithms currently exist that attempt such enumeration; in one case not all possible solutions are produced while in the other not all cost vectors are currently handled. The objective of this paper is two-fold. The first is to fill this gap, and the second is to test whether the number of solutions generally observed can be an issue in terms of interpretation. We present a polynomial-delay algorithm for enumerating all optimal reconciliations. We show that in general many solutions exist. We give an example where, for two pairs of host-parasite trees having each less than 41 leaves, the number of solutions is 5120, even when only time-feasible ones are kept. To facilitate their interpretation, those solutions are also classified in terms of how many of each event they contain. The number of different classes of solutions may thus be notably smaller than the number of solutions, yet they may remain high enough, in particular for the cases where losses have cost 0. In fact, depending on the cost vector, both numbers of solutions and of classes thereof may increase considerably. To further deal with this problem, we introduce and analyse a restricted version where host switches are allowed to happen only between species that are within some fixed distance along the host tree. This restriction allows us to reduce the number of time-feasible solutions while preserving the same optimal cost, as well as to find time-feasible solutions with a cost close to the optimal in the cases where no time-feasible solution is found. We present Eucalypt, a polynomial-delay algorithm for enumerating all optimal reconciliations which is freely available at http://eucalypt.gforge.inria.fr/ .",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W1986968252",
    "type": "article"
  },
  {
    "title": "Complexity and algorithms for copy-number evolution problems",
    "doi": "https://doi.org/10.1186/s13015-017-0103-2",
    "publication_date": "2017-05-16",
    "publication_year": 2017,
    "authors": "Mohammed El-Kebir; Benjamin J. Raphael; Ron Shamir; Roded Sharan; Simone Zaccaria; Meirav Zehavi; Ron Zeira",
    "corresponding_authors": "Ron Shamir",
    "abstract": "Cancer is an evolutionary process characterized by the accumulation of somatic mutations in a population of cells that form a tumor. One frequent type of mutations is copy number aberrations, which alter the number of copies of genomic regions. The number of copies of each position along a chromosome constitutes the chromosome's copy-number profile. Understanding how such profiles evolve in cancer can assist in both diagnosis and prognosis. We model the evolution of a tumor by segmental deletions and amplifications, and gauge distance from profile $$\\mathbf {a}$$ to $$\\mathbf {b}$$ by the minimum number of events needed to transform $$\\mathbf {a}$$ into $$\\mathbf {b}$$ . Given two profiles, our first problem aims to find a parental profile that minimizes the sum of distances to its children. Given k profiles, the second, more general problem, seeks a phylogenetic tree, whose k leaves are labeled by the k given profiles and whose internal vertices are labeled by ancestral profiles such that the sum of edge distances is minimum. For the former problem we give a pseudo-polynomial dynamic programming algorithm that is linear in the profile length, and an integer linear program formulation. For the latter problem we show it is NP-hard and give an integer linear program formulation that scales to practical problem instance sizes. We assess the efficiency and quality of our algorithms on simulated instances. https://github.com/raphael-group/CNT-ILP",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2614716373",
    "type": "article"
  },
  {
    "title": "Gene tree parsimony for incomplete gene trees: addressing true biological loss",
    "doi": "https://doi.org/10.1186/s13015-017-0120-1",
    "publication_date": "2018-01-19",
    "publication_year": 2018,
    "authors": "Md. Shamsuzzoha Bayzid; Tandy Warnow",
    "corresponding_authors": "Md. Shamsuzzoha Bayzid",
    "abstract": "Species tree estimation from gene trees can be complicated by gene duplication and loss, and \"gene tree parsimony\" (GTP) is one approach for estimating species trees from multiple gene trees. In its standard formulation, the objective is to find a species tree that minimizes the total number of gene duplications and losses with respect to the input set of gene trees. Although much is known about GTP, little is known about how to treat inputs containing some incomplete gene trees (i.e., gene trees lacking one or more of the species).We present new theory for GTP considering whether the incompleteness is due to gene birth and death (i.e., true biological loss) or taxon sampling, and present dynamic programming algorithms that can be used for an exact but exponential time solution for small numbers of taxa, or as a heuristic for larger numbers of taxa. We also prove that the \"standard\" calculations for duplications and losses exactly solve GTP when incompleteness results from taxon sampling, although they can be incorrect when incompleteness results from true biological loss. The software for the DP algorithm is freely available as open source code at https://github.com/smirarab/DynaDup.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2787155168",
    "type": "article"
  },
  {
    "title": "External memory BWT and LCP computation for sequence collections with applications",
    "doi": "https://doi.org/10.1186/s13015-019-0140-0",
    "publication_date": "2019-03-08",
    "publication_year": 2019,
    "authors": "Lavinia Egidi; Felipe A. Louza; Giovanni Manzini; Guilherme P. Telles",
    "corresponding_authors": "Felipe A. Louza",
    "abstract": "Sequencing technologies produce larger and larger collections of biosequences that have to be stored in compressed indices supporting fast search operations. Many compressed indices are based on the Burrows–Wheeler Transform (BWT) and the longest common prefix (LCP) array. Because of the sheer size of the input it is important to build these data structures in external memory and time using in the best possible way the available RAM. We propose a space-efficient algorithm to compute the BWT and LCP array for a collection of sequences in the external or semi-external memory setting. Our algorithm splits the input collection into subcollections sufficiently small that it can compute their BWT in RAM using an optimal linear time algorithm. Next, it merges the partial BWTs in external or semi-external memory and in the process it also computes the LCP values. Our algorithm can be modified to output two additional arrays that, combined with the BWT and LCP array, provide simple, scan-based, external memory algorithms for three well known problems in bioinformatics: the computation of maximal repeats, the all pairs suffix–prefix overlaps, and the construction of succinct de Bruijn graphs. We prove that our algorithm performs $${\\mathcal {O}}(n\\, \\mathsf {maxlcp})$$ sequential I/Os, where n is the total length of the collection and $$\\mathsf {maxlcp}$$ is the maximum LCP value. The experimental results show that our algorithm is only slightly slower than the state of the art for short sequences but it is up to 40 times faster for longer sequences or when the available RAM is at least equal to the size of the input.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2963995493",
    "type": "article"
  },
  {
    "title": "A multi-labeled tree dissimilarity measure for comparing “clonal trees” of tumor progression",
    "doi": "https://doi.org/10.1186/s13015-019-0152-9",
    "publication_date": "2019-07-27",
    "publication_year": 2019,
    "authors": "Nikolai Karpov; Salem Malikić; Md. Khaledur Rahman; S. Cenk Şahinalp",
    "corresponding_authors": "",
    "abstract": "We introduce a new dissimilarity measure between a pair of \"clonal trees\", each representing the progression and mutational heterogeneity of a tumor sample, constructed by the use of single cell or bulk high throughput sequencing data. In a clonal tree, each vertex represents a specific tumor clone, and is labeled with one or more mutations in a way that each mutation is assigned to the oldest clone that harbors it. Given two clonal trees, our multi-labeled tree dissimilarity (MLTD) measure is defined as the minimum number of mutation/label deletions, (empty) leaf deletions, and vertex (clonal) expansions, applied in any order, to convert each of the two trees to the maximum common tree. We show that the MLTD measure can be computed efficiently in polynomial time and it captures the similarity between trees of different clonal granularity well.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2965510019",
    "type": "article"
  },
  {
    "title": "Adjacency-constrained hierarchical clustering of a band similarity matrix with application to genomics",
    "doi": "https://doi.org/10.1186/s13015-019-0157-4",
    "publication_date": "2019-11-15",
    "publication_year": 2019,
    "authors": "Christophe Ambroise; Alia Dehman; Pierre Neuvial; Guillem Rigaill; Nathalie Villa‐Vialaneix",
    "corresponding_authors": "",
    "abstract": "Abstract Background Genomic data analyses such as Genome-Wide Association Studies (GWAS) or Hi-C studies are often faced with the problem of partitioning chromosomes into successive regions based on a similarity matrix of high-resolution, locus-level measurements. An intuitive way of doing this is to perform a modified Hierarchical Agglomerative Clustering (HAC), where only adjacent clusters (according to the ordering of positions within a chromosome) are allowed to be merged. But a major practical drawback of this method is its quadratic time and space complexity in the number of loci, which is typically of the order of $$10^4$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:math> to $$10^5$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:math> for each chromosome. Results By assuming that the similarity between physically distant objects is negligible, we are able to propose an implementation of adjacency-constrained HAC with quasi-linear complexity. This is achieved by pre-calculating specific sums of similarities, and storing candidate fusions in a min-heap. Our illustrations on GWAS and Hi-C datasets demonstrate the relevance of this assumption, and show that this method highlights biologically meaningful signals. Thanks to its small time and memory footprint, the method can be run on a standard laptop in minutes or even seconds. Availability and implementation Software and sample data are available as an package, adjclust , that can be downloaded from the Comprehensive R Archive Network (CRAN).",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2987747596",
    "type": "article"
  },
  {
    "title": "Binning long reads in metagenomics datasets using composition and coverage information",
    "doi": "https://doi.org/10.1186/s13015-022-00221-z",
    "publication_date": "2022-07-11",
    "publication_year": 2022,
    "authors": "Anuradha Wickramarachchi; Yu Lin",
    "corresponding_authors": "",
    "abstract": "Advancements in metagenomics sequencing allow the study of microbial communities directly from their environments. Metagenomics binning is a key step in the species characterisation of microbial communities. Next-generation sequencing reads are usually assembled into contigs for metagenomics binning mainly due to the limited information within short reads. Third-generation sequencing provides much longer reads that have lengths similar to the contigs assembled from short reads. However, existing contig-binning tools cannot be directly applied on long reads due to the absence of coverage information and the presence of high error rates. The few existing long-read binning tools either use only composition or use composition and coverage information separately. This may ignore bins that correspond to low-abundance species or erroneously split bins that correspond to species with non-uniform coverages. Here we present a reference-free binning approach, LRBinner, that combines composition and coverage information of complete long-read datasets. LRBinner also uses a distance-histogram-based clustering algorithm to extract clusters with varying sizes.The experimental results on both simulated and real datasets show that LRBinner achieves the best binning accuracy in most cases while handling the complete datasets without any sampling. Moreover, we show that binning reads using LRBinner prior to assembly reduces computational resources required for assembly while attaining satisfactory assembly qualities.LRBinner shows that deep-learning techniques can be used for effective feature aggregation to support the metagenomics binning of long reads. Furthermore, accurate binning of long reads supports improvements in metagenomics assembly, especially in complex datasets. Binning also helps to reduce the resources required for assembly. Source code for LRBinner is freely available at https://github.com/anuradhawick/LRBinner.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4285042422",
    "type": "article"
  },
  {
    "title": "Co-linear chaining on pangenome graphs",
    "doi": "https://doi.org/10.1186/s13015-024-00250-w",
    "publication_date": "2024-01-27",
    "publication_year": 2024,
    "authors": "Jyotshna Rajput; Ghanshyam Chandra; Chirag Jain",
    "corresponding_authors": "",
    "abstract": "Pangenome reference graphs are useful in genomics because they compactly represent the genetic diversity within a species, a capability that linear references lack. However, efficiently aligning sequences to these graphs with complex topology and cycles can be challenging. The seed-chain-extend based alignment algorithms use co-linear chaining as a standard technique to identify a good cluster of exact seed matches that can be combined to form an alignment. Recent works show how the co-linear chaining problem can be efficiently solved for acyclic pangenome graphs by exploiting their small width and how incorporating gap cost in the scoring function improves alignment accuracy. However, it remains open on how to effectively generalize these techniques for general pangenome graphs which contain cycles. Here we present the first practical formulation and an exact algorithm for co-linear chaining on cyclic pangenome graphs. We rigorously prove the correctness and computational complexity of the proposed algorithm. We evaluate the empirical performance of our algorithm by aligning simulated long reads from the human genome to a cyclic pangenome graph constructed from 95 publicly available haplotype-resolved human genome assemblies. While the existing heuristic-based algorithms are faster, the proposed algorithm provides a significant advantage in terms of accuracy. Implementation ( https://github.com/at-cg/PanAligner ).",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4391270274",
    "type": "article"
  },
  {
    "title": "NestedBD: Bayesian inference of phylogenetic trees from single-cell copy number profiles under a birth-death model",
    "doi": "https://doi.org/10.1186/s13015-024-00264-4",
    "publication_date": "2024-04-29",
    "publication_year": 2024,
    "authors": "Yushu Liu; Mohammadamin Edrisi; Zhi Yan; Huw A. Ogilvie; Luay Nakhleh",
    "corresponding_authors": "Yushu Liu",
    "abstract": "Abstract Copy number aberrations (CNAs) are ubiquitous in many types of cancer. Inferring CNAs from cancer genomic data could help shed light on the initiation, progression, and potential treatment of cancer. While such data have traditionally been available via “bulk sequencing,” the more recently introduced techniques for single-cell DNA sequencing (scDNAseq) provide the type of data that makes CNA inference possible at the single-cell resolution. We introduce a new birth-death evolutionary model of CNAs and a Bayesian method, NestedBD, for the inference of evolutionary trees (topologies and branch lengths with relative mutation rates) from single-cell data. We evaluated NestedBD’s performance using simulated data sets, benchmarking its accuracy against traditional phylogenetic tools as well as state-of-the-art methods. The results show that NestedBD infers more accurate topologies and branch lengths, and that the birth-death model can improve the accuracy of copy number estimation. And when applied to biological data sets, NestedBD infers plausible evolutionary histories of two colorectal cancer samples. NestedBD is available at https://github.com/Androstane/NestedBD .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4396213728",
    "type": "article"
  },
  {
    "title": "Fractional hitting sets for efficient multiset sketching",
    "doi": "https://doi.org/10.1186/s13015-024-00268-0",
    "publication_date": "2025-02-08",
    "publication_year": 2025,
    "authors": "Timothé Rouzé; Igor Martayan; Camille Marchet; Antoine Limasset",
    "corresponding_authors": "Timothé Rouzé",
    "abstract": "Abstract The exponential increase in publicly available sequencing data and genomic resources necessitates the development of highly efficient methods for data processing and analysis. Locality-sensitive hashing techniques have successfully transformed large datasets into smaller, more manageable sketches while maintaining comparability using metrics such as Jaccard and containment indices. However, fixed-size sketches encounter difficulties when applied to divergent datasets. Scalable sketching methods, such as , provide valuable solutions but still lack resource-efficient, tailored indexing. Our objective is to create lighter sketches with comparable results while enhancing efficiency. We introduce the concept of Fractional Hitting Sets, a generalization of Universal Hitting Sets, which cover a specified fraction of the k -mer space. In theory and practice, we demonstrate the feasibility of achieving such coverage with simple but highly efficient schemes. By encoding the covered k -mers as super- k -mers, we provide a space-efficient exact representation that also enables optimized comparisons. Our novel tool, , implements this scheme, and experimental results with real bacterial collections closely match our theoretical findings. In comparison to , achieves similar outcomes while utilizing an order of magnitude less space and memory and operating several times faster. This highlights the potential of our approach in addressing the challenges presented by the ever-expanding landscape of genomic data. is an open-source software and can be accessed at https://github.com/TimRouze/supersampler . The data required to reproduce the results presented in this manuscript is available at https://github.com/TimRouze/supersampler/experiments .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407269291",
    "type": "article"
  },
  {
    "title": "Mem-based pangenome indexing for k-mer queries",
    "doi": "https://doi.org/10.1186/s13015-025-00272-y",
    "publication_date": "2025-03-01",
    "publication_year": 2025,
    "authors": "Stephen Hwang; Nathaniel K. Brown; Omar Ahmed; Katharine M. Jenike; Sam Kovaka; Michael C. Schatz; Ben Langmead",
    "corresponding_authors": "",
    "abstract": "Abstract Pangenomes are growing in number and size, thanks to the prevalence of high-quality long-read assemblies. However, current methods for studying sequence composition and conservation within pangenomes have limitations. Methods based on graph pangenomes require a computationally expensive multiple-alignment step, which can leave out some variation. Indexes based on k -mers and de Bruijn graphs are limited to answering questions at a specific substring length k . We present Maximal Exact Match Ordered (MEMO), a pangenome indexing method based on maximal exact matches (MEMs) between sequences. A single MEMO index can handle arbitrary-length queries over pangenomic windows. MEMO enables both queries that test k -mer presence/absence (membership queries) and that count the number of genomes containing k -mers in a window (conservation queries). MEMO’s index for a pangenome of 89 human autosomal haplotypes fits in 2.04 GB, 8.8 $$\\times$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mo>×</mml:mo> </mml:math> smaller than a comparable KMC3 index and 11.4 $$\\times$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mo>×</mml:mo> </mml:math> smaller than a PanKmer index. MEMO indexes can be made smaller by sacrificing some counting resolution, with our decile-resolution HPRC index reaching 0.67 GB. MEMO can conduct a conservation query for 31-mers over the human leukocyte antigen locus in 13.89 s, 2.5 $$\\times$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mo>×</mml:mo> </mml:math> faster than other approaches. MEMO’s small index size, lack of k -mer length dependence, and efficient queries make it a flexible tool for studying and visualizing substring conservation in pangenomes.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408068966",
    "type": "article"
  },
  {
    "title": "NANUQ+: A divide-and-conquer approach to network estimation",
    "doi": "https://doi.org/10.1186/s13015-025-00274-w",
    "publication_date": "2025-07-25",
    "publication_year": 2025,
    "authors": "Elizabeth S. Allman; Hector Baños; John A. Rhodes; Kristina Wicke",
    "corresponding_authors": "Kristina Wicke",
    "abstract": "Inference of a species network from genomic data remains a difficult problem, with recent progress mostly limited to the level-1 case. However, inference of the Tree of Blobs of a network, showing only the network's cut edges, can be performed for any network by TINNiK, suggesting a divide-and-conquer approach to network inference where the tree's multifurcations are individually resolved to give more detailed structure. Here we develop a method, $$\\text {NANUQ}^+$$ , to quickly perform such a level-1 resolution. Viewed as part of the NANUQ pipeline for fast level-1 inference, this gives tools for both understanding when the level-1 assumption is likely to be met and for exploring all highly-supported resolutions to cycles.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4412708182",
    "type": "article"
  },
  {
    "title": "New journal: Algorithms for Molecular Biology",
    "doi": "https://doi.org/10.1186/1748-7188-1-1",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Burkhard Morgenstern; Peter F. Stadler",
    "corresponding_authors": "",
    "abstract": "This editorial announces Algorithms for Molecular Biology, a new online open access journal published by BioMed Central. By launching the first open access journal on algorithmic bioinformatics, we provide a forum for fast publication of high-quality research articles in this rapidly evolving field. Our journal will publish thoroughly peer-reviewed papers without length limitations covering all aspects of algorithmic data analysis in computatioal biology. Publications in Algorithms for Molecular Biology are easy to find, highly visible and tracked by organisations such as PubMed. An established online submission system makes a fast reviewing procedure possible and enables us to publish accepted papers without delay. All articles published in our journal are permanently archived by PubMed Central and other scientific archives. We are looking forward to receiving your contributions.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2099412910",
    "type": "editorial"
  },
  {
    "title": "Tracking cells in Life Cell Imaging videos using topological alignments",
    "doi": "https://doi.org/10.1186/1748-7188-4-10",
    "publication_date": "2009-07-16",
    "publication_year": 2009,
    "authors": "Axel Mosig; Stefan Jäger; Chaofeng Wang; Sumit Nath; Ilker Ersoy; Kannappan Palaniappan; Su‐Shing Chen",
    "corresponding_authors": "",
    "abstract": "With the increasing availability of live cell imaging technology, tracking cells and other moving objects in live cell videos has become a major challenge for bioimage informatics. An inherent problem for most cell tracking algorithms is over- or under-segmentation of cells - many algorithms tend to recognize one cell as several cells or vice versa.We propose to approach this problem through so-called topological alignments, which we apply to address the problem of linking segmentations of two consecutive frames in the video sequence. Starting from the output of a conventional segmentation procedure, we align pairs of consecutive frames through assigning sets of segments in one frame to sets of segments in the next frame. We achieve this through finding maximum weighted solutions to a generalized \"bipartite matching\" between two hierarchies of segments, where we derive weights from relative overlap scores of convex hulls of sets of segments. For solving the matching task, we rely on an integer linear program.Practical experiments demonstrate that the matching task can be solved efficiently in practice, and that our method is both effective and useful for tracking cells in data sets derived from a so-called Large Scale Digital Cell Analysis System (LSDCAS).The source code of the implementation is available for download from http://www.picb.ac.cn/patterns/Software/topaln.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2038922801",
    "type": "article"
  },
  {
    "title": "A simulation study comparing supertree and combined analysis methods using SMIDGen",
    "doi": "https://doi.org/10.1186/1748-7188-5-8",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "M. Shel Swenson; François Barbançon; Tandy Warnow; C. Randal Linder",
    "corresponding_authors": "",
    "abstract": "Supertree methods comprise one approach to reconstructing large molecular phylogenies given multi-marker datasets: trees are estimated on each marker and then combined into a tree (the \"supertree\") on the entire set of taxa. Supertrees can be constructed using various algorithmic techniques, with the most common being matrix representation with parsimony (MRP). When the data allow, the competing approach is a combined analysis (also known as a \"supermatrix\" or \"total evidence\" approach) whereby the different sequence data matrices for each of the different subsets of taxa are concatenated into a single supermatrix, and a tree is estimated on that supermatrix. In this paper, we describe an extensive simulation study we performed comparing two supertree methods, MRP and weighted MRP, to combined analysis methods on large model trees. A key contribution of this study is our novel simulation methodology (Super-Method Input Data Generator, or SMIDGen) that better reflects biological processes and the practices of systematists than earlier simulations. We show that combined analysis based upon maximum likelihood outperforms MRP and weighted MRP, giving especially big improvements when the largest subtree does not contain most of the taxa. This study demonstrates that MRP and weighted MRP produce distinctly less accurate trees than combined analyses for a given base method (maximum parsimony or maximum likelihood). Since there are situations in which combined analyses are not feasible, there is a clear need for better supertree methods. The source tree and combined datasets used in this study can be used to test other supertree and combined analysis methods.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2136538032",
    "type": "article"
  },
  {
    "title": "FlexSnap: Flexible Non-sequential Protein Structure Alignment",
    "doi": "https://doi.org/10.1186/1748-7188-5-12",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Saeed Salem; Mohammed J. Zaki; Chris Bystroff",
    "corresponding_authors": "",
    "abstract": "Proteins have evolved subject to energetic selection pressure for stability and flexibility. Structural similarity between proteins that have gone through conformational changes can be captured effectively if flexibility is considered. Topologically unrelated proteins that preserve secondary structure packing interactions can be detected if both flexibility and Sequential permutations are considered. We propose the FlexSnap algorithm for flexible non-topological protein structural alignment.The effectiveness of FlexSnap is demonstrated by measuring the agreement of its alignments with manually curated non-sequential structural alignments. FlexSnap showed competitive results against state-of-the-art algorithms, like DALI, SARF2, MultiProt, FlexProt, and FATCAT. Moreover on the DynDom dataset, FlexSnap reported longer alignments with smaller rmsd.We have introduced FlexSnap, a greedy chaining algorithm that reports both sequential and non-sequential alignments and allows twists (hinges). We assessed the quality of the FlexSnap alignments by measuring its agreements with manually curated non-sequential alignments. On the FlexProt dataset, FlexSnap was competitive to state-of-the-art flexible alignment methods. Moreover, we demonstrated the benefits of introducing hinges by showing significant improvements in the alignments reported by FlexSnap for the structure pairs for which rigid alignment methods reported alignments with either low coverage or large rmsd.An implementation of the FlexSnap algorithm will be made available online at http://www.cs.rpi.edu/~zaki/software/flexsnap.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2127868702",
    "type": "article"
  },
  {
    "title": "ReCoil - an algorithm for compression of extremely large datasets of dna data",
    "doi": "https://doi.org/10.1186/1748-7188-6-23",
    "publication_date": "2011-10-11",
    "publication_year": 2011,
    "authors": "Vladimir Yanovsky",
    "corresponding_authors": "Vladimir Yanovsky",
    "abstract": "The growing volume of generated DNA sequencing data makes the problem of its long term storage increasingly important. In this work we present ReCoil - an I/O efficient external memory algorithm designed for compression of very large collections of short reads DNA data. Typically each position of DNA sequence is covered by multiple reads of a short read dataset and our algorithm makes use of resulting redundancy to achieve high compression rate.While compression based on encoding mismatches between the dataset and a similar reference can yield high compression rate, good quality reference sequence may be unavailable. Instead, ReCoil's compression is based on encoding the differences between similar or overlapping reads. As such reads may appear at large distances from each other in the dataset and since random access memory is a limited resource, ReCoil is designed to work efficiently in external memory, leveraging high bandwidth of modern hard disk drives.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1965359119",
    "type": "article"
  },
  {
    "title": "Analysis of Metabolic Subnetworks by Flux Cone Projection",
    "doi": "https://doi.org/10.1186/1748-7188-7-17",
    "publication_date": "2012-05-29",
    "publication_year": 2012,
    "authors": "Sayed‐Amir Marashi; László Dávid; Alexander Bockmayr",
    "corresponding_authors": "Sayed‐Amir Marashi; Alexander Bockmayr",
    "abstract": "Analysis of elementary modes (EMs) is proven to be a powerful constraint-based method in the study of metabolic networks. However, enumeration of EMs is a hard computational task. Additionally, due to their large number, EMs cannot be simply used as an input for subsequent analysis. One possibility is to limit the analysis to a subset of interesting reactions. However, analysing an isolated subnetwork can result in finding incorrect EMs which are not part of any steady-state flux distribution of the original network. The ideal set to describe the reaction activity in a subnetwork would be the set of all EMs projected to the reactions of interest. Recently, the concept of \"elementary flux patterns\" (EFPs) has been proposed. Each EFP is a subset of the support (i.e., non-zero elements) of at least one EM. We introduce the concept of ProCEMs (Projected Cone Elementary Modes). The ProCEM set can be computed by projecting the flux cone onto a lower-dimensional subspace and enumerating the extreme rays of the projected cone. In contrast to EFPs, ProCEMs are not merely a set of reactions, but projected EMs. We additionally prove that the set of EFPs is included in the set of ProCEM supports. Finally, ProCEMs and EFPs are compared for studying substructures of biological networks. We introduce the concept of ProCEMs and recommend its use for the analysis of substructures of metabolic networks for which the set of EMs cannot be computed.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2121249893",
    "type": "article"
  },
  {
    "title": "DCJ-Indel sorting revisited",
    "doi": "https://doi.org/10.1186/1748-7188-8-6",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Phillip E. C. Compeau",
    "corresponding_authors": "Phillip E. C. Compeau",
    "abstract": "The introduction of the double cut and join operation (DCJ) caused a flurry of research into the study of multichromosomal rearrangements. However, little of this work has incorporated indels (i.e., insertions and deletions of chromosomes and chromosomal intervals) into the calculation of genomic distance functions, with the exception of Braga et al., who provided a linear time algorithm for the problem of DCJ-indel sorting. Although their algorithm only takes linear time, its derivation is lengthy and depends on a large number of possible cases.We note the simple idea that a deletion of a chromosomal interval can be viewed as a DCJ that creates a new circular chromosome. This framework will allow us to amortize indels as DCJs, which in turn permits the application of the classical breakpoint graph to obtain a simplified indel model that still solves the problem of DCJ-indel sorting in linear time via a more concise formulation that relies on the simpler problem of DCJ sorting. Furthermore, we can extend this result to fully characterize the solution space of DCJ-indel sorting.Encoding indels as DCJ operations offers a new insight into why the problem of DCJ-indel sorting is not ultimately any more difficult than that of sorting by DCJs alone. There is still room for research in this area, most notably the problem of sorting when the cost of indels is allowed to vary with respect to the cost of a DCJ and we demand a minimum cost transformation of one genome into another.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2139679291",
    "type": "article"
  },
  {
    "title": "Fast algorithms for approximate circular string matching",
    "doi": "https://doi.org/10.1186/1748-7188-9-9",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Carl Barton; Costas S. Iliopoulos; Solon P. Pissis",
    "corresponding_authors": "",
    "abstract": "Circular string matching is a problem which naturally arises in many biological contexts. It consists in finding all occurrences of the rotations of a pattern of length m in a text of length n. There exist optimal average-case algorithms for exact circular string matching. Approximate circular string matching is a rather undeveloped area. In this article, we present a suboptimal average-case algorithm for exact circular string matching requiring time . Based on our solution for the exact case, we present two fast average-case algorithms for approximate circular string matching with k-mismatches, under the Hamming distance model, requiring time for moderate values of k, that is . We show how the same results can be easily obtained under the edit distance model. The presented algorithms are also implemented as library functions. Experimental results demonstrate that the functions provided in this library accelerate the computations by more than three orders of magnitude compared to a naïve approach. We present two fast average-case algorithms for approximate circular string matching with k-mismatches; and show that they also perform very well in practice. The importance of our contribution is underlined by the fact that the provided functions may be seamlessly integrated into any biological pipeline. The source code of the library is freely available at http://www.inf.kcl.ac.uk/research/projects/asmf/ .",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2044074710",
    "type": "article"
  },
  {
    "title": "A minimum-labeling approach for reconstructing protein networks across multiple conditions",
    "doi": "https://doi.org/10.1186/1748-7188-9-1",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Arnon Mazza; Irit Gat‐Viks; Hesso Farhan; Roded Sharan",
    "corresponding_authors": "",
    "abstract": "The sheer amounts of biological data that are generated in recent years have driven the development of network analysis tools to facilitate the interpretation and representation of these data. A fundamental challenge in this domain is the reconstruction of a protein-protein subnetwork that underlies a process of interest from a genome-wide screen of associated genes. Despite intense work in this area, current algorithmic approaches are largely limited to analyzing a single screen and are, thus, unable to account for information on condition-specific genes, or reveal the dynamics (over time or condition) of the process in question. We propose a novel formulation for the problem of network reconstruction from multiple-condition data and devise an efficient integer program solution for it. We apply our algorithm to analyze the response to influenza infection and ER export regulation in humans. By comparing to an extant, single-condition tool we demonstrate the power of our new approach in integrating data from multiple conditions in a compact and coherent manner, capturing the dynamics of the underlying processes.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2102231637",
    "type": "article"
  },
  {
    "title": "Inferring interaction type in gene regulatory networks using co-expression data",
    "doi": "https://doi.org/10.1186/s13015-015-0054-4",
    "publication_date": "2015-07-07",
    "publication_year": 2015,
    "authors": "Pegah Khosravi; Vahid Gazestani; Leila Pirhaji; Brian K. Law; Mehdi Sadeghi; Bahram Goliaei; Gary D. Bader",
    "corresponding_authors": "Pegah Khosravi; Gary D. Bader",
    "abstract": "Knowledge of interaction types in biological networks is important for understanding the functional organization of the cell. Currently information-based approaches are widely used for inferring gene regulatory interactions from genomics data, such as gene expression profiles; however, these approaches do not provide evidence about the regulation type (positive or negative sign) of the interaction.This paper describes a novel algorithm, \"Signing of Regulatory Networks\" (SIREN), which can infer the regulatory type of interactions in a known gene regulatory network (GRN) given corresponding genome-wide gene expression data. To assess our new approach, we applied it to three different benchmark gene regulatory networks, including Escherichia coli, prostate cancer, and an in silico constructed network. Our new method has approximately 68, 70, and 100 percent accuracy, respectively, for these networks. To showcase the utility of SIREN algorithm, we used it to predict previously unknown regulation types for 454 interactions related to the prostate cancer GRN.SIREN is an efficient algorithm with low computational complexity; hence, it is applicable to large biological networks. It can serve as a complementary approach for a wide range of network reconstruction methods that do not provide information about the interaction type.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2117719890",
    "type": "article"
  },
  {
    "title": "Instability in progressive multiple sequence alignment algorithms",
    "doi": "https://doi.org/10.1186/s13015-015-0057-1",
    "publication_date": "2015-10-08",
    "publication_year": 2015,
    "authors": "Kieran Boyce; Fabian Sievers; Desmond G. Higgins",
    "corresponding_authors": "",
    "abstract": "Progressive alignment is the standard approach used to align large numbers of sequences. As with all heuristics, this involves a tradeoff between alignment accuracy and computation time.We examine this tradeoff and find that, because of a loss of information in the early steps of the approach, the alignments generated by the most common multiple sequence alignment programs are inherently unstable, and simply reversing the order of the sequences in the input file will cause a different alignment to be generated. Although this effect is more obvious with larger numbers of sequences, it can also be seen with data sets in the order of one hundred sequences. We also outline the means to determine the number of sequences in a data set beyond which the probability of instability will become more pronounced.This has major ramifications for both the designers of large-scale multiple sequence alignment algorithms, and for the users of these alignments.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W1962255460",
    "type": "article"
  },
  {
    "title": "MissMax: alignment-free sequence comparison with mismatches through filtering and heuristics",
    "doi": "https://doi.org/10.1186/s13015-016-0072-x",
    "publication_date": "2016-04-21",
    "publication_year": 2016,
    "authors": "Cinzia Pizzi",
    "corresponding_authors": "Cinzia Pizzi",
    "abstract": "Measuring sequence similarity is central for many problems in bioinformatics. In several contexts alignment-free techniques based on exact occurrences of substrings are faster, but also less accurate, than alignment-based approaches. Recently, several studies attempted to bridge the accuracy gap with the introduction of approximate matches in the definition of composition-based similarity measures.In this work we present MissMax, an exact algorithm for the computation of the longest common substring with mismatches between each suffix of a sequence x and a sequence y. This collection of statistics is useful for the computation of two similarity measures: the longest and the average common substring with k mismatches. As a further contribution we provide a \"relaxed\" version of MissMax that does not guarantee the exact solution, but it is faster in practice and still very precise.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2339926834",
    "type": "article"
  },
  {
    "title": "SNPs detection by eBWT positional clustering",
    "doi": "https://doi.org/10.1186/s13015-019-0137-8",
    "publication_date": "2019-02-06",
    "publication_year": 2019,
    "authors": "Nicola Prezza; Nadia Pisanti; Marinella Sciortino; Giovanna Rosone",
    "corresponding_authors": "",
    "abstract": "Sequencing technologies keep on turning cheaper and faster, thus putting a growing pressure for data structures designed to efficiently store raw data, and possibly perform analysis therein. In this view, there is a growing interest in alignment-free and reference-free variants calling methods that only make use of (suitably indexed) raw reads data.We develop the positional clustering theory that (i) describes how the extended Burrows-Wheeler Transform (eBWT) of a collection of reads tends to cluster together bases that cover the same genome position (ii) predicts the size of such clusters, and (iii) exhibits an elegant and precise LCP array based procedure to locate such clusters in the eBWT. Based on this theory, we designed and implemented an alignment-free and reference-free SNPs calling method, and we devised a consequent SNPs calling pipeline. Experiments on both synthetic and real data show that SNPs can be detected with a simple scan of the eBWT and LCP arrays as, in accordance with our theoretical framework, they are within clusters in the eBWT of the reads. Finally, our tool intrinsically performs a reference-free evaluation of its accuracy by returning the coverage of each SNP.Based on the results of the experiments on synthetic and real data, we conclude that the positional clustering framework can be effectively used for the problem of identifying SNPs, and it appears to be a promising approach for calling other type of variants directly on raw sequencing data.The software ebwt2snp is freely available for academic use at: https://github.com/nicolaprezza/ebwt2snp.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2920034498",
    "type": "article"
  },
  {
    "title": "BiC2PAM: constraint-guided biclustering for biological data analysis with domain knowledge",
    "doi": "https://doi.org/10.1186/s13015-016-0085-5",
    "publication_date": "2016-09-14",
    "publication_year": 2016,
    "authors": "Rui Henriques; Sara C. Madeira",
    "corresponding_authors": "",
    "abstract": "Biclustering has been largely used in biological data analysis, enabling the discovery of putative functional modules from omic and network data. Despite the recognized importance of incorporating domain knowledge to guide biclustering and guarantee a focus on relevant and non-trivial biclusters, this possibility has not yet been comprehensively addressed. This results from the fact that the majority of existing algorithms are only able to deliver sub-optimal solutions with restrictive assumptions on the structure, coherency and quality of biclustering solutions, thus preventing the up-front satisfaction of knowledge-driven constraints. Interestingly, in recent years, a clearer understanding of the synergies between pattern mining and biclustering gave rise to a new class of algorithms, termed as pattern-based biclustering algorithms. These algorithms, able to efficiently discover flexible biclustering solutions with optimality guarantees, are thus positioned as good candidates for knowledge incorporation. In this context, this work aims to bridge the current lack of solid views on the use of background knowledge to guide (pattern-based) biclustering tasks. This work extends (pattern-based) biclustering algorithms to guarantee the satisfiability of constraints derived from background knowledge and to effectively explore efficiency gains from their incorporation. In this context, we first show the relevance of constraints with succinct, (anti-)monotone and convertible properties for the analysis of expression data and biological networks. We further show how pattern-based biclustering algorithms can be adapted to effectively prune of the search space in the presence of such constraints, as well as be guided in the presence of biological annotations. Relying on these contributions, we propose BiClustering with Constraints using PAttern Mining (BiC2PAM), an extension of BicPAM and BicNET biclustering algorithms. Experimental results on biological data demonstrate the importance of incorporating knowledge within biclustering to foster efficiency and enable the discovery of non-trivial biclusters with heightened biological relevance. This work provides the first comprehensive view and sound algorithm for biclustering biological data with constraints derived from user expectations, knowledge repositories and/or literature.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2519170897",
    "type": "article"
  },
  {
    "title": "The paralog-to-contig assignment problem: high quality gene models from fragmented assemblies",
    "doi": "https://doi.org/10.1186/s13015-016-0063-y",
    "publication_date": "2016-02-24",
    "publication_year": 2016,
    "authors": "Henrike Indrischek; Nicolas Wieseke; Peter F. Stadler; Sonja J. Prohaska",
    "corresponding_authors": "",
    "abstract": "The accurate annotation of genes in newly sequenced genomes remains a challenge. Although sophisticated comparative pipelines are available, computationally derived gene models are often less than perfect. This is particularly true when multiple similar paralogs are present. The issue is aggravated further when genomes are assembled only at a preliminary draft level to contigs or short scaffolds. However, these genomes deliver valuable information for studying gene families. High accuracy models of protein coding genes are needed in particular for phylogenetics and for the analysis of gene family histories. We present a pipeline, ExonMatchSolver, that is designed to help the user to produce and curate high quality models of the protein-coding part of genes. The tool in particular tackles the problem of identifying those coding exon groups that belong to the same paralogous genes in a fragmented genome assembly. This paralog-to-contig assignment problem is shown to be NP-complete. It is phrased and solved as an Integer Linear Programming problem. The ExonMatchSolver-pipeline can be employed to build highly accurate models of protein coding genes even when spanning several genomic fragments. This sets the stage for a better understanding of the evolutionary history within particular gene families which possess a large number of paralogs and in which frequent gene duplication events occurred.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2285253122",
    "type": "article"
  },
  {
    "title": "Automatic layout and visualization of biclusters",
    "doi": "https://doi.org/10.1186/1748-7188-1-15",
    "publication_date": "2006-09-04",
    "publication_year": 2006,
    "authors": "Gregory Grothaus; Adeel Mufti; T. M. Murali",
    "corresponding_authors": "",
    "abstract": "Abstract Background Biclustering has emerged as a powerful algorithmic tool for analyzing measurements of gene expression. A number of different methods have emerged for computing biclusters in gene expression data. Many of these algorithms may output a very large number of biclusters with varying degrees of overlap. There are no systematic methods that create a two-dimensional layout of the computed biclusters and display overlaps between them. Results We develop a novel algorithm for laying out biclusters in a two-dimensional matrix whose rows (respectively, columns) are rows (respectively, columns) of the original dataset. We display each bicluster as a contiguous submatrix in the layout. We allow the layout to have repeated rows and/or columns from the original matrix as required, but we seek a layout of the smallest size. We also develop a web-based search interface for the user to query the genes and samples of interest and visualise the layout of biclusters matching the queries. Conclusion We demonstrate the usefulness of our approach on gene expression data for two types of leukaemia and on protein-DNA binding data for two growth conditions in Saccharomyces cerevisiae . The software implementing the layout algorithm is available at http://bioinformatics.cs.vt.edu/~murali/papers/bivoc .",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2105685244",
    "type": "article"
  },
  {
    "title": "EXMOTIF: efficient structured motif extraction",
    "doi": "https://doi.org/10.1186/1748-7188-1-21",
    "publication_date": "2006-11-16",
    "publication_year": 2006,
    "authors": "Yongqiang Zhang; Mohammed J. Zaki",
    "corresponding_authors": "",
    "abstract": "Extracting motifs from sequences is a mainstay of bioinformatics. We look at the problem of mining structured motifs, which allow variable length gaps between simple motif components. We propose an efficient algorithm, called EXMOTIF, that given some sequence(s), and a structured motif template, extracts all frequent structured motifs that have quorum q. Potential applications of our method include the extraction of single/composite regulatory binding sites in DNA sequences. EXMOTIF is efficient in terms of both time and space and is shown empirically to outperform RISO, a state-of-the-art algorithm. It is also successful in finding potential single/composite transcription factor binding sites. EXMOTIF is a useful and efficient tool in discovering structured motifs, especially in DNA sequences. The algorithm is available as open-source at: http://www.cs.rpi.edu/~zaki/software/exMotif/ .",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2125246942",
    "type": "article"
  },
  {
    "title": "Consistency of the Neighbor-Net Algorithm",
    "doi": "https://doi.org/10.1186/1748-7188-2-8",
    "publication_date": "2007-06-28",
    "publication_year": 2007,
    "authors": "David Bryant; Vincent Moulton; Andreas Spillner",
    "corresponding_authors": "",
    "abstract": "Neighbor-Net is a novel method for phylogenetic analysis that is currently being widely used in areas such as virology, bacteriology, and plant evolution. Given an input distance matrix, Neighbor-Net produces a phylogenetic network, a generalization of an evolutionary or phylogenetic tree which allows the graphical representation of conflicting phylogenetic signals.In general, any network construction method should not depict more conflict than is found in the data, and, when the data is fitted well by a tree, the method should return a network that is close to this tree. In this paper we provide a formal proof that Neighbor-Net satisfies both of these requirements so that, in particular, Neighbor-Net is statistically consistent on circular distances.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2125056339",
    "type": "article"
  },
  {
    "title": "Exact p-value calculation for heterotypic clusters of regulatory motifs and its application in computational annotation of cis-regulatory modules",
    "doi": "https://doi.org/10.1186/1748-7188-2-13",
    "publication_date": "2007-10-10",
    "publication_year": 2007,
    "authors": "Valentina Boeva; Julien Clément; Mireille Régnier; Mikhail Roytberg; Vsevolod J. Makeev",
    "corresponding_authors": "Valentina Boeva",
    "abstract": "cis-Regulatory modules (CRMs) of eukaryotic genes often contain multiple binding sites for transcription factors. The phenomenon that binding sites form clusters in CRMs is exploited in many algorithms to locate CRMs in a genome. This gives rise to the problem of calculating the statistical significance of the event that multiple sites, recognized by different factors, would be found simultaneously in a text of a fixed length. The main difficulty comes from overlapping occurrences of motifs. So far, no tools have been developed allowing the computation of p-values for simultaneous occurrences of different motifs which can overlap. We developed and implemented an algorithm computing the p-value that s different motifs occur respectively k1, ..., k s or more times, possibly overlapping, in a random text. Motifs can be represented with a majority of popular motif models, but in all cases, without indels. Zero or first order Markov chains can be adopted as a model for the random text. The computational tool was tested on the set of cis-regulatory modules involved in D. melanogaster early development, for which there exists an annotation of binding sites for transcription factors. Our test allowed us to correctly identify transcription factors cooperatively/competitively binding to DNA. The algorithm that precisely computes the probability of simultaneous motif occurrences is inspired by the Aho-Corasick automaton and employs a prefix tree together with a transition function. The algorithm runs with the O(n|Σ|(m| | + K|σ| K ) ∏ i k i ) time complexity, where n is the length of the text, |Σ| is the alphabet size, m is the maximal motif length, | | is the total number of words in motifs, K is the order of Markov model, and k i is the number of occurrences of the i th motif. The primary objective of the program is to assess the likelihood that a given DNA segment is CRM regulated with a known set of regulatory factors. In addition, the program can also be used to select the appropriate threshold for PWM scanning. Another application is assessing similarity of different motifs. Project web page, stand-alone version and documentation can be found at http://bioinform.genetika.ru/AhoPro/",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2099800150",
    "type": "article"
  },
  {
    "title": "PhyloScan: identification of transcription factor binding sites using cross-species evidence",
    "doi": "https://doi.org/10.1186/1748-7188-2-1",
    "publication_date": "2007-01-23",
    "publication_year": 2007,
    "authors": "C. Steven Carmack; Lee Ann McCue; Lee A. Newberg; Charles E. Lawrence",
    "corresponding_authors": "",
    "abstract": "Abstract Background When transcription factor binding sites are known for a particular transcription factor, it is possible to construct a motif model that can be used to scan sequences for additional sites. However, few statistically significant sites are revealed when a transcription factor binding site motif model is used to scan a genome-scale database. Methods We have developed a scanning algorithm, PhyloScan, which combines evidence from matching sites found in orthologous data from several related species with evidence from multiple sites within an intergenic region, to better detect regulons. The orthologous sequence data may be multiply aligned, unaligned, or a combination of aligned and unaligned. In aligned data, PhyloScan statistically accounts for the phylogenetic dependence of the species contributing data to the alignment and, in unaligned data, the evidence for sites is combined assuming phylogenetic independence of the species. The statistical significance of the gene predictions is calculated directly, without employing training sets. Results In a test of our methodology on synthetic data modeled on seven Enterobacteriales , four Vibrionales , and three Pasteurellales species, PhyloScan produces better sensitivity and specificity than MONKEY, an advanced scanning approach that also searches a genome for transcription factor binding sites using phylogenetic information. The application of the algorithm to real sequence data from seven Enterobacteriales species identifies novel Crp and PurR transcription factor binding sites, thus providing several new potential sites for these transcription factors. These sites enable targeted experimental validation and thus further delineation of the Crp and PurR regulons in E. coli . Conclusion Better sensitivity and specificity can be achieved through a combination of (1) using mixed alignable and non-alignable sequence data and (2) combining evidence from multiple sites within an intergenic region.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W1861050171",
    "type": "article"
  },
  {
    "title": "Gravitation field algorithm and its application in gene cluster",
    "doi": "https://doi.org/10.1186/1748-7188-5-32",
    "publication_date": "2010-09-20",
    "publication_year": 2010,
    "authors": "Ming Zheng; Guixia Liu; Chunguang Zhou; Yanchun Liang; Yan Wang",
    "corresponding_authors": "",
    "abstract": "Abstract Background Searching optima is one of the most challenging tasks in clustering genes from available experimental data or given functions. SA, GA, PSO and other similar efficient global optimization methods are used by biotechnologists. All these algorithms are based on the imitation of natural phenomena. Results This paper proposes a novel searching optimization algorithm called Gravitation Field Algorithm (GFA) which is derived from the famous astronomy theory Solar Nebular Disk Model (SNDM) of planetary formation. GFA simulates the Gravitation field and outperforms GA and SA in some multimodal functions optimization problem. And GFA also can be used in the forms of unimodal functions. GFA clusters the dataset well from the Gene Expression Omnibus. Conclusions The mathematical proof demonstrates that GFA could be convergent in the global optimum by probability 1 in three conditions for one independent variable mass functions. In addition to these results, the fundamental optimization concept in this paper is used to analyze how SA and GA affect the global search and the inherent defects in SA and GA. Some results and source code (in Matlab) are publicly available at http://ccst.jlu.edu.cn/CSBG/GFA .",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2113710106",
    "type": "article"
  },
  {
    "title": "Wrapper-based selection of genetic features in genome-wide association studies through fast matrix operations",
    "doi": "https://doi.org/10.1186/1748-7188-7-11",
    "publication_date": "2012-05-02",
    "publication_year": 2012,
    "authors": "Tapio Pahikkala; Sebastian Okser; Antti Airola; Tapio Salakoski; Tero Aittokallio",
    "corresponding_authors": "Tapio Pahikkala",
    "abstract": "Through the wealth of information contained within them, genome-wide association studies (GWAS) have the potential to provide researchers with a systematic means of associating genetic variants with a wide variety of disease phenotypes. Due to the limitations of approaches that have analyzed single variants one at a time, it has been proposed that the genetic basis of these disorders could be determined through detailed analysis of the genetic variants themselves and in conjunction with one another. The construction of models that account for these subsets of variants requires methodologies that generate predictions based on the total risk of a particular group of polymorphisms. However, due to the excessive number of variants, constructing these types of models has so far been computationally infeasible. We have implemented an algorithm, known as greedy RLS, that we use to perform the first known wrapper-based feature selection on the genome-wide level. The running time of greedy RLS grows linearly in the number of training examples, the number of features in the original data set, and the number of selected features. This speed is achieved through computational short-cuts based on matrix calculus. Since the memory consumption in present-day computers can form an even tighter bottleneck than running time, we also developed a space efficient variation of greedy RLS which trades running time for memory. These approaches are then compared to traditional wrapper-based feature selection implementations based on support vector machines (SVM) to reveal the relative speed-up and to assess the feasibility of the new algorithm. As a proof of concept, we apply greedy RLS to the Hypertension – UK National Blood Service WTCCC dataset and select the most predictive variants using 3-fold external cross-validation in less than 26 minutes on a high-end desktop. On this dataset, we also show that greedy RLS has a better classification performance on independent test data than a classifier trained using features selected by a statistical p-value-based filter, which is currently the most popular approach for constructing predictive models in GWAS. Greedy RLS is the first known implementation of a machine learning based method with the capability to conduct a wrapper-based feature selection on an entire GWAS containing several thousand examples and over 400,000 variants. In our experiments, greedy RLS selected a highly predictive subset of genetic variants in a fraction of the time spent by wrapper-based selection methods used together with SVM classifiers. The proposed algorithms are freely available as part of the RLScore software library at http://users.utu.fi/aatapa/RLScore/ .",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2112917490",
    "type": "article"
  },
  {
    "title": "QTL/microarray approach using pathway information",
    "doi": "https://doi.org/10.1186/1748-7188-7-1",
    "publication_date": "2012-01-15",
    "publication_year": 2012,
    "authors": "Hirokazu Matsuda; Yukio Taniguchi; Hiroaki Iwaisaki",
    "corresponding_authors": "Hirokazu Matsuda",
    "abstract": "A combined quantitative trait loci (QTL) and microarray-based approach is commonly used to find differentially expressed genes which are then identified based on the known function of a gene in the biological process governing the trait of interest. However, a low cutoff value in individual gene analyses may result in many genes with moderate but meaningful changes in expression being missed.We modified a gene set analysis to identify intersection sets with significantly affected expression for which the changes in the individual gene sets are less significant. The gene expression profiles in liver tissues of four strains of mice from publicly available microarray sources were analyzed to detect trait-associated pathways using information on the QTL regions of blood concentrations of high density lipoproteins (HDL) cholesterol and insulin-like growth factor 1 (IGF-1). Several metabolic pathways related to HDL levels, including lipid metabolism, ABC transporters and cytochrome P450 pathways were detected for HDL QTL regions. Most of the pathways identified for the IGF-1 phenotype were signal transduction pathways associated with biological processes for IGF-1's regulation.We have developed a method of identifying pathways associated with a quantitative trait using information on QTL. Our approach provides insights into genotype-phenotype relations at the level of biological pathways which may help to elucidate the genetic architecture underlying variation in phenotypic traits.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2152766058",
    "type": "article"
  },
  {
    "title": "Tree-average distances on certain phylogenetic networks have their weights uniquely determined",
    "doi": "https://doi.org/10.1186/1748-7188-7-13",
    "publication_date": "2012-05-15",
    "publication_year": 2012,
    "authors": "Stephen J. Willson",
    "corresponding_authors": "Stephen J. Willson",
    "abstract": "A phylogenetic network N has vertices corresponding to species and arcs corresponding to direct genetic inheritance from the species at the tail to the species at the head. Measurements of DNA are often made on species in the leaf set, and one seeks to infer properties of the network, possibly including the graph itself. In the case of phylogenetic trees, distances between extant species are frequently used to infer the phylogenetic trees by methods such as neighbor-joining. This paper proposes a tree-average distance for networks more general than trees. The notion requires a weight on each arc measuring the genetic change along the arc. For each displayed tree the distance between two leaves is the sum of the weights along the path joining them. At a hybrid vertex, each character is inherited from one of its parents. We will assume that for each hybrid there is a probability that the inheritance of a character is from a specified parent. Assume that the inheritance events at different hybrids are independent. Then for each displayed tree there will be a probability that the inheritance of a given character follows the tree; this probability may be interpreted as the probability of the tree. The tree-average distance between the leaves is defined to be the expected value of their distance in the displayed trees. For a class of rooted networks that includes rooted trees, it is shown that the weights and the probabilities at each hybrid vertex can be calculated given the network and the tree-average distances between the leaves. Hence these weights and probabilities are uniquely determined. The hypotheses on the networks include that hybrid vertices have indegree exactly 2 and that vertices that are not leaves have a tree-child.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2161897726",
    "type": "article"
  },
  {
    "title": "On the family-free DCJ distance and similarity",
    "doi": "https://doi.org/10.1186/s13015-015-0041-9",
    "publication_date": "2015-04-01",
    "publication_year": 2015,
    "authors": "Fábio V. Martinez; Pedro Feijão; Marília Braga; Jens Stoye",
    "corresponding_authors": "",
    "abstract": "Structural variation in genomes can be revealed by many (dis)similarity measures. Rearrangement operations, such as the so called double-cut-and-join (DCJ), are large-scale mutations that can create complex changes and produce such variations in genomes. A basic task in comparative genomics is to find the rearrangement distance between two given genomes, i.e., the minimum number of rearragement operations that transform one given genome into another one. In a family-based setting, genes are grouped into gene families and efficient algorithms have already been presented to compute the DCJ distance between two given genomes. In this work we propose the problem of computing the DCJ distance of two given genomes without prior gene family assignment, directly using the pairwise similarities between genes. We prove that this new family-free DCJ distance problem is APX-hard and provide an integer linear program to its solution. We also study a family-free DCJ similarity and prove that its computation is NP-hard.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1986644389",
    "type": "article"
  },
  {
    "title": "A graph modification approach for finding core–periphery structures in protein interaction networks",
    "doi": "https://doi.org/10.1186/s13015-015-0043-7",
    "publication_date": "2015-05-01",
    "publication_year": 2015,
    "authors": "Sharon Bruckner; Falk Hüffner; Christian Komusiewicz",
    "corresponding_authors": "",
    "abstract": "The core-periphery model for protein interaction (PPI) networks assumes that protein complexes in these networks consist of a dense core and a possibly sparse periphery that is adjacent to vertices in the core of the complex. In this work, we aim at uncovering a global core-periphery structure for a given PPI network. We propose two exact graph-theoretic formulations for this task, which aim to fit the input network to a hypothetical ground truth network by a minimum number of edge modifications. In one model each cluster has its own periphery, and in the other the periphery is shared. We first analyze both models from a theoretical point of view, showing their NP-hardness. Then, we devise efficient exact and heuristic algorithms for both models and finally perform an evaluation on subnetworks of the S. cerevisiae PPI network.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2138881347",
    "type": "article"
  },
  {
    "title": "Segmentor3IsBack: an R package for the fast and exact segmentation of Seq-data",
    "doi": "https://doi.org/10.1186/1748-7188-9-6",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Alice Cleynen; Michel Koskas; Émilie Lebarbier; Guillem Rigaill; Stéphane Robin",
    "corresponding_authors": "Alice Cleynen",
    "abstract": "Change point problems arise in many genomic analyses such as the detection of copy number variations or the detection of transcribed regions. The expanding Next Generation Sequencing technologies now allow to locate change points at the nucleotide resolution. Because of its complexity which is almost linear in the sequence length when the maximal number of segments is constant, and as its performance had been acknowledged for microarrays, we propose to use the Pruned Dynamic Programming algorithm for Seq-experiment outputs. This requires the adaptation of the algorithm to the negative binomial distribution with which we model the data. We show that if the dispersion in the signal is known, the PDP algorithm can be used, and we provide an estimator for this dispersion. We describe a compression framework which reduces the time complexity without modifying the accuracy of the segmentation. We propose to estimate the number of segments via a penalized likelihood criterion. We illustrate the performance of the proposed methodology on RNA-Seq data. We illustrate the results of our approach on a real dataset and show its good performance. Our algorithm is available as an R package on the CRAN repository.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2161789913",
    "type": "article"
  },
  {
    "title": "Fair evaluation of global network aligners",
    "doi": "https://doi.org/10.1186/s13015-015-0050-8",
    "publication_date": "2015-06-09",
    "publication_year": 2015,
    "authors": "Joseph Crawford; Yihan Sun; Tijana Milenković",
    "corresponding_authors": "",
    "abstract": "Analogous to genomic sequence alignment, biological network alignment identifies conserved regions between networks of different species. Then, function can be transferred from well- to poorly-annotated species between aligned network regions. Network alignment typically encompasses two algorithmic components: node cost function (NCF), which measures similarities between nodes in different networks, and alignment strategy (AS), which uses these similarities to rapidly identify high-scoring alignments. Different methods use both different NCFs and different ASs. Thus, it is unclear whether the superiority of a method comes from its NCF, its AS, or both. We already showed on state-of-the-art methods, MI-GRAAL and IsoRankN, that combining NCF of one method and AS of another method can give a new superior method. Here, we evaluate MI-GRAAL against a newer approach, GHOST, by mixing-and-matching the methods' NCFs and ASs to potentially further improve alignment quality. While doing so, we approach important questions that have not been asked systematically thus far. First, we ask how much of the NCF information should come from protein sequence data compared to network topology data. Existing methods determine this parameter more-less arbitrarily, which could affect alignment quality. Second, when topological information is used in NCF, we ask how large the size of the neighborhoods of the compared nodes should be. Existing methods assume that the larger the neighborhood size, the better.Our findings are as follows. MI-GRAAL's NCF is superior to GHOST's NCF, while the performance of the methods' ASs is data-dependent. Thus, for data on which GHOST's AS is superior to MI-GRAAL's AS, the combination of MI-GRAAL's NCF and GHOST's AS represents a new superior method. Also, which amount of sequence information is used within NCF does not affect alignment quality, while the inclusion of topological information is crucial for producing good alignments. Finally, larger neighborhood sizes are preferred, but often, it is the second largest size that is superior. Using this size instead of the largest one would decrease computational complexity.Taken together, our results represent general recommendations for a fair evaluation of network alignment methods and in particular of two-stage NCF-AS approaches.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W1779639821",
    "type": "article"
  },
  {
    "title": "A multiobjective approach for identifying protein complexes and studying their association in multiple disorders",
    "doi": "https://doi.org/10.1186/s13015-015-0056-2",
    "publication_date": "2015-08-08",
    "publication_year": 2015,
    "authors": "Sanghamitra Bandyopadhyay; Sumanta Ray; Anirban Mukhopadhyay; Ujjwal Maulik",
    "corresponding_authors": "",
    "abstract": "Detecting protein complexes within protein–protein interaction (PPI) networks is a major step toward the analysis of biological processes and pathways. Identification and characterization of protein complexes in PPI network is an ongoing challenge. Several high-throughput experimental techniques provide substantial number of PPIs which are widely utilized for compiling the PPI network of a species. Here we focus on detecting human protein complexes by developing a multiobjective framework. For this large human PPI network is partitioned into modules which serves as protein complex. For building the objective functions we have utilized topological properties of PPI network and biological properties based on Gene Ontology semantic similarity. The proposed method is compared with that of some state-of-the-art algorithms in the context of different performance metrics. For the purpose of biological validation of our predicted complexes we have also employed a Gene Ontology and pathway based analysis here. Additionally, we have performed an analysis to associate resulting protein complexes with 22 key disease classes. Two bipartite networks are created to clearly visualize the association of identified protein complexes with the disorder classes. Here, we present the task of identifying protein complexes as a multiobjective optimization problem. Identified protein complexes are found to be associated with several disorders classes like 'Cancer', 'Endocrine' and 'Multiple'. This analysis uncovers some new relationships between disorders and predicted complexes that may take a potential role in the prediction of multi target drugs.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W1934405236",
    "type": "article"
  },
  {
    "title": "Invariants and Other Structural Properties of Biochemical Models as a Constraint Satisfaction Problem",
    "doi": "https://doi.org/10.1186/1748-7188-7-15",
    "publication_date": "2012-05-29",
    "publication_year": 2012,
    "authors": "Sylvain Soliman",
    "corresponding_authors": "Sylvain Soliman",
    "abstract": "We present a way to compute the minimal semi-positive invariants of a Petri net representing a biological reaction system, as resolution of a Constraint Satisfaction Problem. The use of Petri nets to manipulate Systems Biology models and make available a variety of tools is quite old, and recently analyses based on invariant computation for biological models have become more and more frequent, for instance in the context of module decomposition. In our case, this analysis brings both qualitative and quantitative information on the models, in the form of conservation laws, consistency checking, etc. thanks to finite domain constraint programming. It is noticeable that some of the most recent optimizations of standard invariant computation techniques in Petri nets correspond to well-known techniques in constraint solving, like symmetry-breaking. Moreover, we show that the simple and natural encoding proposed is not only efficient but also flexible enough to encompass sub/sur-invariants, siphons/traps, etc., i.e., other Petri net structural properties that lead to supplementary insight on the dynamics of the biochemical system under study. A simple implementation based on GNU-Prolog's finite domain solver, and including symmetry detection and breaking, was incorporated into the BIOCHAM modelling environment and in the independent tool Nicotine. Some illustrative examples and benchmarks are provided.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2102643016",
    "type": "article"
  },
  {
    "title": "Reconciliation and local gene tree rearrangement can be of mutual profit",
    "doi": "https://doi.org/10.1186/1748-7188-8-12",
    "publication_date": "2013-04-08",
    "publication_year": 2013,
    "authors": "Thi Hau Nguyen; Vincent Ranwez; Stéphanie Pointet; Anne-Muriel Arigon Chifolleau; Jean‐Philippe Doyon; Vincent Berry",
    "corresponding_authors": "",
    "abstract": "Abstract Background Reconciliation methods compare gene trees and species trees to recover evolutionary events such as duplications, transfers and losses explaining the history and composition of genomes. It is well-known that gene trees inferred from molecular sequences can be partly erroneous due to incorrect sequence alignments as well as phylogenetic reconstruction artifacts such as long branch attraction. In practice, this leads reconciliation methods to overestimate the number of evolutionary events. Several methods have been proposed to circumvent this problem, by collapsing the unsupported edges and then resolving the obtained multifurcating nodes, or by directly rearranging the binary gene trees. Yet these methods have been defined for models of evolution accounting only for duplications and losses, i.e. can not be applied to handle prokaryotic gene families. Results We propose a reconciliation method accounting for gene duplications, losses and horizontal transfers, that specifically takes into account the uncertainties in gene trees by rearranging their weakly supported edges. Rearrangements are performed on edges having a low confidence value, and are accepted whenever they improve the reconciliation cost. We prove useful properties on the dynamic programming matrix used to compute reconciliations, which allows to speed-up the tree space exploration when rearrangements are generated by Nearest Neighbor Interchanges (NNI) edit operations. Experiments on synthetic data show that gene trees modified by such NNI rearrangements are closer to the correct simulated trees and lead to better event predictions on average. Experiments on real data demonstrate that the proposed method leads to a decrease in the reconciliation cost and the number of inferred events. Finally on a dataset of 30 k gene families, this reconciliation method shows a ranking of prokaryotic phyla by transfer rates identical to that proposed by a different approach dedicated to transfer detection [BMCBIOINF 11:324, 2010, PNAS 109(13):4962–4967, 2012]. Conclusions Prokaryotic gene trees can now be reconciled with their species phylogeny while accounting for the uncertainty of the gene tree. More accurate and more precise reconciliations are obtained with respect to previous parsimony algorithms not accounting for such uncertainties [LNCS 6398:93–108, 2010, BIOINF 28(12): i283–i291, 2012]. A software implementing the method is freely available at http://www.atgc-montpellier.fr/Mowgli/ .",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2105669285",
    "type": "article"
  },
  {
    "title": "Clustering of reads with alignment-free measures and quality values",
    "doi": "https://doi.org/10.1186/s13015-014-0029-x",
    "publication_date": "2015-01-27",
    "publication_year": 2015,
    "authors": "Matteo Comin; Andrea Leoni; Michele Schimd",
    "corresponding_authors": "",
    "abstract": "The data volume generated by Next-Generation Sequencing (NGS) technologies is growing at a pace that is now challenging the storage and data processing capacities of modern computer systems. In this context an important aspect is the reduction of data complexity by collapsing redundant reads in a single cluster to improve the run time, memory requirements, and quality of post-processing steps like assembly and error correction. Several alignment-free measures, based on k-mers counts, have been used to cluster reads. Quality scores produced by NGS platforms are fundamental for various analysis of NGS data like reads mapping and error detection. Moreover future-generation sequencing platforms will produce long reads but with a large number of erroneous bases (up to 15 %).In this scenario it will be fundamental to exploit quality value information within the alignment-free framework. To the best of our knowledge this is the first study that incorporates quality value information and k-mers counts, in the context of alignment-free measures, for the comparison of reads data. Based on this principles, in this paper we present a family of alignment-free measures called D (q) -type. A set of experiments on simulated and real reads data confirms that the new measures are superior to other classical alignment-free statistics, especially when erroneous reads are considered. Also results on de novo assembly and metagenomic reads classification show that the introduction of quality values improves over standard alignment-free measures. These statistics are implemented in a software called QCluster (http://www.dei.unipd.it/~ciompin/main/qcluster.html).",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2142167651",
    "type": "article"
  },
  {
    "title": "Algorithms for computing the double cut and join distance on both gene order and intergenic sizes",
    "doi": "https://doi.org/10.1186/s13015-017-0107-y",
    "publication_date": "2017-06-05",
    "publication_year": 2017,
    "authors": "Guillaume Fertin; Géraldine Jean; Éric Tannier",
    "corresponding_authors": "",
    "abstract": "Combinatorial works on genome rearrangements have so far ignored the influence of intergene sizes, i.e. the number of nucleotides between consecutive genes, although it was recently shown decisive for the accuracy of inference methods (Biller et al. in Genome Biol Evol 8:1427–39, 2016; Biller et al. in Beckmann A, Bienvenu L, Jonoska N, editors. Proceedings of Pursuit of the Universal-12th conference on computability in Europe, CiE 2016, Lecture notes in computer science, vol 9709, Paris, France, June 27–July 1, 2016. Berlin: Springer, p. 35–44, 2016). In this line, we define a new genome rearrangement model called wDCJ, a generalization of the well-known double cut and join (or DCJ) operation that modifies both the gene order and the intergene size distribution of a genome. We first provide a generic formula for the wDCJ distance between two genomes, and show that computing this distance is strongly NP-complete. We then propose an approximation algorithm of ratio 4/3, and two exact ones: a fixed-parameter tractable (FPT) algorithm and an integer linear programming (ILP) formulation. We provide theoretical and empirical bounds on the expected growth of the parameter at the center of our FPT and ILP algorithms, assuming a probabilistic model of evolution under wDCJ, which shows that both these algorithms should run reasonably fast in practice.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2622515803",
    "type": "article"
  },
  {
    "title": "Algorithms for detecting and analysing autocatalytic sets",
    "doi": "https://doi.org/10.1186/s13015-015-0042-8",
    "publication_date": "2015-04-27",
    "publication_year": 2015,
    "authors": "Wim Hordijk; Joshua I Smith; Mike Steel",
    "corresponding_authors": "Wim Hordijk",
    "abstract": "Autocatalytic sets are considered to be fundamental to the origin of life. Prior theoretical and computational work on the existence and properties of these sets has relied on a fast algorithm for detectingself-sustaining autocatalytic sets in chemical reaction systems. Here, we introduce and apply a modified version and several extensions of the basic algorithm: (i) a modification aimed at reducing the number of calls to the computationally most expensive part of the algorithm, (ii) the application of a previously introduced extension of the basic algorithm to sample the smallest possible autocatalytic sets within a reaction network, and the application of a statistical test which provides a probable lower bound on the number of such smallest sets, (iii) the introduction and application of another extension of the basic algorithm to detect autocatalytic sets in a reaction system where molecules can also inhibit (as well as catalyse) reactions, (iv) a further, more abstract, extension of the theory behind searching for autocatalytic sets.(i) The modified algorithm outperforms the original one in the number of calls to the computationally most expensive procedure, which, in some cases also leads to a significant improvement in overall running time, (ii) our statistical test provides strong support for the existence of very large numbers (even millions) of minimal autocatalytic sets in a well-studied polymer model, where these minimal sets share about half of their reactions on average, (iii) \"uninhibited\" autocatalytic sets can be found in reaction systems that allow inhibition, but their number and sizes depend on the level of inhibition relative to the level of catalysis.(i) Improvements in the overall running time when searching for autocatalytic sets can potentially be obtained by using a modified version of the algorithm, (ii) the existence of large numbers of minimal autocatalytic sets can have important consequences for the possible evolvability of autocatalytic sets, (iii) inhibition can be efficiently dealt with as long as the total number of inhibitors is small.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2150962838",
    "type": "article"
  },
  {
    "title": "Approximating the correction of weighted and unweighted orthology and paralogy relations",
    "doi": "https://doi.org/10.1186/s13015-017-0096-x",
    "publication_date": "2017-03-11",
    "publication_year": 2017,
    "authors": "Riccardo Dondi; Manuel Lafond; Nadia El-Mabrouk",
    "corresponding_authors": "Riccardo Dondi",
    "abstract": "Given a gene family, the relations between genes (orthology/paralogy), are represented by a relation graph, where edges connect pairs of orthologous genes and \"missing\" edges represent paralogs. While a gene tree directly induces a relation graph, the converse is not always true. Indeed, a relation graph is not necessarily \"satisfiable\", i.e. does not necessarily correspond to a gene tree. And even if that holds, it may not be \"consistent\", i.e. the tree may not represent a true history in agreement with a species tree. Previous studies have addressed the problem of correcting a relation graph for satisfiability and consistency. Here we consider the weighted version of the problem, where a degree of confidence is assigned to each orthology or paralogy relation. We also consider a maximization variant of the unweighted version of the problem.We provide complexity and algorithmic results for the approximation of the considered problems. We show that minimizing the correction of a weighted graph does not admit a constant factor approximation algorithm assuming the unique game conjecture, and we give an n-approximation algorithm, n being the number of vertices in the graph. We also provide polynomial time approximation schemes for the maximization variant for unweighted graphs.We provided complexity and algorithmic results for variants of the problem of correcting a relation graph for satisfiability and consistency. For the maximization variants we were able to design polynomial time approximation schemes, while for the weighted minimization variants we were able to provide the first inapproximability results.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2592818772",
    "type": "article"
  },
  {
    "title": "A hybrid parameter estimation algorithm for beta mixtures and applications to methylation state classification",
    "doi": "https://doi.org/10.1186/s13015-017-0112-1",
    "publication_date": "2017-08-18",
    "publication_year": 2017,
    "authors": "Christopher Schröder; Sven Rahmann",
    "corresponding_authors": "",
    "abstract": "Mixtures of beta distributions are a flexible tool for modeling data with values on the unit interval, such as methylation levels. However, maximum likelihood parameter estimation with beta distributions suffers from problems because of singularities in the log-likelihood function if some observations take the values 0 or 1.While ad-hoc corrections have been proposed to mitigate this problem, we propose a different approach to parameter estimation for beta mixtures where such problems do not arise in the first place. Our algorithm combines latent variables with the method of moments instead of maximum likelihood, which has computational advantages over the popular EM algorithm.As an application, we demonstrate that methylation state classification is more accurate when using adaptive thresholds from beta mixtures than non-adaptive thresholds on observed methylation levels. We also demonstrate that we can accurately infer the number of mixture components.The hybrid algorithm between likelihood-based component un-mixing and moment-based parameter estimation is a robust and efficient method for beta mixture estimation. We provide an implementation of the method (\"betamix\") as open source software under the MIT license.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2771671371",
    "type": "article"
  },
  {
    "title": "Atom mapping with constraint programming",
    "doi": "https://doi.org/10.1186/s13015-014-0023-3",
    "publication_date": "2014-11-28",
    "publication_year": 2014,
    "authors": "Martin Mann; Feras Nahar; Norah Schnorr; Rolf Backofen; Peter F. Stadler; Christoph Flamm",
    "corresponding_authors": "",
    "abstract": "Chemical reactions are rearrangements of chemical bonds. Each atom in an educt molecule thus appears again in a specific position of one of the reaction products. This bijection between educt and product atoms is not reported by chemical reaction databases, however, so that the \"Atom Mapping Problem\" of finding this bijection is left as an important computational task for many practical applications in computational chemistry and systems biology. Elementary chemical reactions feature a cyclic imaginary transition state (ITS) that imposes additional restrictions on the bijection between educt and product atoms that are not taken into account by previous approaches. We demonstrate that Constraint Programming is well-suited to solving the Atom Mapping Problem in this setting. The performance of our approach is evaluated for a manually curated subset of chemical reactions from the KEGG database featuring various ITS cycle layouts and reaction mechanisms.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2116802997",
    "type": "article"
  },
  {
    "title": "Identification of donor splice sites using support vector machine: a computational approach based on positional, compositional and dependency features",
    "doi": "https://doi.org/10.1186/s13015-016-0078-4",
    "publication_date": "2016-06-01",
    "publication_year": 2016,
    "authors": "Prabina Kumar Meher; Tanmaya Kumar Sahu; A. R. Rao; S. D. Wahi",
    "corresponding_authors": "",
    "abstract": "Identification of splice sites is essential for annotation of genes. Though existing approaches have achieved an acceptable level of accuracy, still there is a need for further improvement. Besides, most of the approaches are species-specific and hence it is required to develop approaches compatible across species. Each splice site sequence was transformed into a numeric vector of length 49, out of which four were positional, four were dependency and 41 were compositional features. Using the transformed vectors as input, prediction was made through support vector machine. Using balanced training set, the proposed approach achieved area under ROC curve (AUC-ROC) of 96.05, 96.96, 96.95, 96.24 % and area under PR curve (AUC-PR) of 97.64, 97.89, 97.91, 97.90 %, while tested on human, cattle, fish and worm datasets respectively. On the other hand, AUC-ROC of 97.21, 97.45, 97.41, 98.06 % and AUC-PR of 93.24, 93.34, 93.38, 92.29 % were obtained, while imbalanced training datasets were used. The proposed approach was found comparable with state-of-art splice site prediction approaches, while compared using the bench mark NN269 dataset and other datasets. The proposed approach achieved consistent accuracy across different species as well as found comparable with the existing approaches. Thus, we believe that the proposed approach can be used as a complementary method to the existing methods for the prediction of splice sites. A web server named as 'HSplice' has also been developed based on the proposed approach for easy prediction of 5′ splice sites by the users and is freely available at http://cabgrid.res.in:8080/HSplice .",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2461601381",
    "type": "article"
  },
  {
    "title": "On avoided words, absent words, and their application to biological sequence analysis",
    "doi": "https://doi.org/10.1186/s13015-017-0094-z",
    "publication_date": "2017-03-13",
    "publication_year": 2017,
    "authors": "Yannis Almirantis; Panagiotis Charalampopoulos; Jia Gao; Costas S. Iliopoulos; Manal Mohamed; Solon P. Pissis; Dimitris Polychronopoulos",
    "corresponding_authors": "",
    "abstract": "The deviation of the observed frequency of a word w from its expected frequency in a given sequence x is used to determine whether or not the word is avoided. This concept is particularly useful in DNA linguistic analysis. The value of the deviation of w, denoted by [Formula: see text], effectively characterises the extent of a word by its edge contrast in the context in which it occurs. A word w of length [Formula: see text] is a [Formula: see text]-avoided word in x if [Formula: see text], for a given threshold [Formula: see text]. Notice that such a word may be completely absent from x. Hence, computing all such words naïvely can be a very time-consuming procedure, in particular for large k.In this article, we propose an [Formula: see text]-time and [Formula: see text]-space algorithm to compute all [Formula: see text]-avoided words of length k in a given sequence of length n over a fixed-sized alphabet. We also present a time-optimal [Formula: see text]-time algorithm to compute all [Formula: see text]-avoided words (of any length) in a sequence of length n over an integer alphabet of size [Formula: see text]. In addition, we provide a tight asymptotic upper bound for the number of [Formula: see text]-avoided words over an integer alphabet and the expected length of the longest one. We make available an implementation of our algorithm. Experimental results, using both real and synthetic data, show the efficiency and applicability of our implementation in biological sequence analysis.The systematic search for avoided words is particularly useful for biological sequence analysis. We present a linear-time and linear-space algorithm for the computation of avoided words of length k in a given sequence x. We suggest a modification to this algorithm so that it computes all avoided words of x, irrespective of their length, within the same time complexity. We also present combinatorial results with regards to avoided words and absent words.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2596908583",
    "type": "article"
  },
  {
    "title": "Alignment- and reference-free phylogenomics with colored de Bruijn graphs",
    "doi": "https://doi.org/10.1186/s13015-020-00164-3",
    "publication_date": "2020-04-07",
    "publication_year": 2020,
    "authors": "Roland Wittler",
    "corresponding_authors": "Roland Wittler",
    "abstract": "The increasing amount of available genome sequence data enables large-scale comparative studies. A common task is the inference of phylogenies-a challenging task if close reference sequences are not available, genome sequences are incompletely assembled, or the high number of genomes precludes multiple sequence alignment in reasonable time.We present a new whole-genome based approach to infer phylogenies that is alignment- and reference-free. In contrast to other methods, it does not rely on pairwise comparisons to determine distances to infer edges in a tree. Instead, a colored de Bruijn graph is constructed, and information on common subsequences is extracted to infer phylogenetic splits.The introduced new methodology for large-scale phylogenomics shows high potential. Application to different datasets confirms robustness of the approach. A comparison to other state-of-the-art whole-genome based methods indicates comparable or higher accuracy and efficiency.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2979069693",
    "type": "article"
  },
  {
    "title": "Fast lightweight accurate xenograft sorting",
    "doi": "https://doi.org/10.1186/s13015-021-00181-w",
    "publication_date": "2021-04-02",
    "publication_year": 2021,
    "authors": "Jens Zentgraf; Sven Rahmann",
    "corresponding_authors": "Sven Rahmann",
    "abstract": "Abstract Motivation With an increasing number of patient-derived xenograft (PDX) models being created and subsequently sequenced to study tumor heterogeneity and to guide therapy decisions, there is a similarly increasing need for methods to separate reads originating from the graft (human) tumor and reads originating from the host species’ (mouse) surrounding tissue. Two kinds of methods are in use: On the one hand, alignment-based tools require that reads are mapped and aligned (by an external mapper/aligner) to the host and graft genomes separately first; the tool itself then processes the resulting alignments and quality metrics (typically BAM files) to assign each read or read pair. On the other hand, alignment-free tools work directly on the raw read data (typically FASTQ files). Recent studies compare different approaches and tools, with varying results. Results We show that alignment-free methods for xenograft sorting are superior concerning CPU time usage and equivalent in accuracy. We improve upon the state of the art sorting by presenting a fast lightweight approach based on three-way bucketed quotiented Cuckoo hashing. Our hash table requires memory comparable to an FM index typically used for read alignment and less than other alignment-free approaches. It allows extremely fast lookups and uses less CPU time than other alignment-free methods and alignment-based methods at similar accuracy. Several engineering steps (e.g., shortcuts for unsuccessful lookups, software prefetching) improve the performance even further. Availability Our software xengsort is available under the MIT license at http://gitlab.com/genomeinformatics/xengsort . It is written in numba-compiled Python and comes with sample Snakemake workflows for hash table construction and dataset processing.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3148116804",
    "type": "article"
  },
  {
    "title": "Improving metagenomic binning results with overlapped bins using assembly graphs",
    "doi": "https://doi.org/10.1186/s13015-021-00185-6",
    "publication_date": "2021-05-04",
    "publication_year": 2021,
    "authors": "Vijini Mallawaarachchi; Anuradha Wickramarachchi; Yu Lin",
    "corresponding_authors": "",
    "abstract": "Abstract Background Metagenomic sequencing allows us to study the structure, diversity and ecology in microbial communities without the necessity of obtaining pure cultures. In many metagenomics studies, the reads obtained from metagenomics sequencing are first assembled into longer contigs and these contigs are then binned into clusters of contigs where contigs in a cluster are expected to come from the same species. As different species may share common sequences in their genomes, one assembled contig may belong to multiple species. However, existing tools for binning contigs only support non-overlapped binning, i.e., each contig is assigned to at most one bin (species). Results In this paper, we introduce GraphBin2 which refines the binning results obtained from existing tools and, more importantly, is able to assign contigs to multiple bins. GraphBin2 uses the connectivity and coverage information from assembly graphs to adjust existing binning results on contigs and to infer contigs shared by multiple species. Experimental results on both simulated and real datasets demonstrate that GraphBin2 not only improves binning results of existing tools but also supports to assign contigs to multiple bins. Conclusion GraphBin2 incorporates the coverage information into the assembly graph to refine the binning results obtained from existing binning tools. GraphBin2 also enables the detection of contigs that may belong to multiple species. We show that GraphBin2 outperforms its predecessor GraphBin on both simulated and real datasets. GraphBin2 is freely available at https://github.com/Vini2/GraphBin2 .",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3159917788",
    "type": "article"
  },
  {
    "title": "Inverse bifurcation analysis: application to simple gene systems",
    "doi": "https://doi.org/10.1186/1748-7188-1-11",
    "publication_date": "2006-07-21",
    "publication_year": 2006,
    "authors": "James Lu; Heinz W. Engl; Peter Schuster",
    "corresponding_authors": "",
    "abstract": "Bifurcation analysis has proven to be a powerful method for understanding the qualitative behavior of gene regulatory networks. In addition to the more traditional forward problem of determining the mapping from parameter space to the space of model behavior, the inverse problem of determining model parameters to result in certain desired properties of the bifurcation diagram provides an attractive methodology for addressing important biological problems. These include understanding how the robustness of qualitative behavior arises from system design as well as providing a way to engineer biological networks with qualitative properties. We demonstrate that certain inverse bifurcation problems of biological interest may be cast as optimization problems involving minimal distances of reference parameter sets to bifurcation manifolds. This formulation allows for an iterative solution procedure based on performing a sequence of eigen-system computations and one-parameter continuations of solutions, the latter being a standard capability in existing numerical bifurcation software. As applications of the proposed method, we show that the problem of maximizing regions of a given qualitative behavior as well as the reverse engineering of bistable gene switches can be modelled and efficiently solved.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2124849619",
    "type": "article"
  },
  {
    "title": "A combinatorial optimization approach for diverse motif finding applications",
    "doi": "https://doi.org/10.1186/1748-7188-1-13",
    "publication_date": "2006-08-17",
    "publication_year": 2006,
    "authors": "Elena Zaslavsky; Mona Singh",
    "corresponding_authors": "",
    "abstract": "Discovering approximately repeated patterns, or motifs, in biological sequences is an important and widely-studied problem in computational molecular biology. Most frequently, motif finding applications arise when identifying shared regulatory signals within DNA sequences or shared functional and structural elements within protein sequences. Due to the diversity of contexts in which motif finding is applied, several variations of the problem are commonly studied.We introduce a versatile combinatorial optimization framework for motif finding that couples graph pruning techniques with a novel integer linear programming formulation. Our approach is flexible and robust enough to model several variants of the motif finding problem, including those incorporating substitution matrices and phylogenetic distances. Additionally, we give an approach for determining statistical significance of uncovered motifs. In testing on numerous DNA and protein datasets, we demonstrate that our approach typically identifies statistically significant motifs corresponding to either known motifs or other motifs of high conservation. Moreover, in most cases, our approach finds provably optimal solutions to the underlying optimization problem.Our results demonstrate that a combined graph theoretic and mathematical programming approach can be the basis for effective and powerful techniques for diverse motif finding applications.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2152210981",
    "type": "article"
  },
  {
    "title": "Local sequence alignments statistics: deviations from Gumbel statistics in the rare-event tail",
    "doi": "https://doi.org/10.1186/1748-7188-2-9",
    "publication_date": "2007-07-11",
    "publication_year": 2007,
    "authors": "S. Wolfsheimer; Bernd Burghardt; Alexander K. Hartmann",
    "corresponding_authors": "",
    "abstract": "The optimal score for ungapped local alignments of infinitely long random sequences is known to follow a Gumbel extreme value distribution. Less is known about the important case, where gaps are allowed. For this case, the distribution is only known empirically in the high-probability region, which is biologically less relevant. We provide a method to obtain numerically the biologically relevant rare-event tail of the distribution. The method, which has been outlined in an earlier work, is based on generating the sequences with a parametrized probability distribution, which is biased with respect to the original biological one, in the framework of Metropolis Coupled Markov Chain Monte Carlo. Here, we first present the approach in detail and evaluate the convergence of the algorithm by considering a simple test case. In the earlier work, the method was just applied to one single example case. Therefore, we consider here a large set of parameters: We study the distributions for protein alignment with different substitution matrices (BLOSUM62 and PAM250) and affine gap costs with different parameter values. In the logarithmic phase (large gap costs) it was previously assumed that the Gumbel form still holds, hence the Gumbel distribution is usually used when evaluating p-values in databases. Here we show that for all cases, provided that the sequences are not too long (L > 400), a \"modified\" Gumbel distribution, i.e. a Gumbel distribution with an additional Gaussian factor is suitable to describe the data. We also provide a \"scaling analysis\" of the parameters used in the modified Gumbel distribution. Furthermore, via a comparison with BLAST parameters, we show that significance estimations change considerably when using the true distributions as presented here. Finally, we study also the distribution of the sum statistics of the k best alignments. Our results show that the statistics of gapped and ungapped local alignments deviates significantly from Gumbel in the rare-event tail. We provide a Gaussian correction to the distribution and an analysis of its scaling behavior for several different scoring parameter sets, which are commonly used to search protein data bases. The case of sum statistics of k best alignments is included.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2092073894",
    "type": "article"
  },
  {
    "title": "\"Hook\"-calibration of GeneChip-microarrays: Theory and algorithm",
    "doi": "https://doi.org/10.1186/1748-7188-3-12",
    "publication_date": "2008-08-29",
    "publication_year": 2008,
    "authors": "Hans Binder; Stephan Preibisch",
    "corresponding_authors": "Hans Binder",
    "abstract": ": The improvement of microarray calibration methods is an essential prerequisite for quantitative expression analysis. This issue requires the formulation of an appropriate model describing the basic relationship between the probe intensity and the specific transcript concentration in a complex environment of competing interactions, the estimation of the magnitude these effects and their correction using the intensity information of a given chip and, finally the development of practicable algorithms which judge the quality of a particular hybridization and estimate the expression degree from the intensity values.: We present the so-called hook-calibration method which co-processes the log-difference (delta) and -sum (sigma) of the perfect match (PM) and mismatch (MM) probe-intensities. The MM probes are utilized as an internal reference which is subjected to the same hybridization law as the PM, however with modified characteristics. After sequence-specific affinity correction the method fits the Langmuir-adsorption model to the smoothed delta-versus-sigma plot. The geometrical dimensions of this so-called hook-curve characterize the particular hybridization in terms of simple geometric parameters which provide information about the mean non-specific background intensity, the saturation value, the mean PM/MM-sensitivity gain and the fraction of absent probes. This graphical summary spans a metrics system for expression estimates in natural units such as the mean binding constants and the occupancy of the probe spots. The method is single-chip based, i.e. it separately uses the intensities for each selected chip.: The hook-method corrects the raw intensities for the non-specific background hybridization in a sequence-specific manner, for the potential saturation of the probe-spots with bound transcripts and for the sequence-specific binding of specific transcripts. The obtained chip characteristics in combination with the sensitivity corrected probe-intensity values provide expression estimates scaled in natural units which are given by the binding constants of the particular hybridization.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2157031069",
    "type": "article"
  },
  {
    "title": "Differential co-expression framework to quantify goodness of biclusters and compare biclustering algorithms",
    "doi": "https://doi.org/10.1186/1748-7188-5-23",
    "publication_date": "2010-05-28",
    "publication_year": 2010,
    "authors": "Burton Kuan Hui Chia; R. Krishna Murthy Karuturi",
    "corresponding_authors": "",
    "abstract": "Biclustering is an important analysis procedure to understand the biological mechanisms from microarray gene expression data. Several algorithms have been proposed to identify biclusters, but very little effort was made to compare the performance of different algorithms on real datasets and combine the resultant biclusters into one unified ranking.In this paper we propose differential co-expression framework and a differential co-expression scoring function to objectively quantify quality or goodness of a bicluster of genes based on the observation that genes in a bicluster are co-expressed in the conditions belonged to the bicluster and not co-expressed in the other conditions. Furthermore, we propose a scoring function to stratify biclusters into three types of co-expression. We used the proposed scoring functions to understand the performance and behavior of the four well established biclustering algorithms on six real datasets from different domains by combining their output into one unified ranking.Differential co-expression framework is useful to provide quantitative and objective assessment of the goodness of biclusters of co-expressed genes and performance of biclustering algorithms in identifying co-expression biclusters. It also helps to combine the biclusters output by different algorithms into one unified ranking i.e. meta-biclustering.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2011794262",
    "type": "article"
  },
  {
    "title": "iTriplet, a rule-based nucleic acid sequence motif finder",
    "doi": "https://doi.org/10.1186/1748-7188-4-14",
    "publication_date": "2009-10-29",
    "publication_year": 2009,
    "authors": "Eric S. Ho; Christopher Jakubowski; Samuel Gunderson",
    "corresponding_authors": "",
    "abstract": "Abstract Background With the advent of high throughput sequencing techniques, large amounts of sequencing data are readily available for analysis. Natural biological signals are intrinsically highly variable making their complete identification a computationally challenging problem. Many attempts in using statistical or combinatorial approaches have been made with great success in the past. However, identifying highly degenerate and long (&gt;20 nucleotides) motifs still remains an unmet challenge as high degeneracy will diminish statistical significance of biological signals and increasing motif size will cause combinatorial explosion. In this report, we present a novel rule-based method that is focused on finding degenerate and long motifs. Our proposed method, named iTriplet, avoids costly enumeration present in existing combinatorial methods and is amenable to parallel processing. Results We have conducted a comprehensive assessment on the performance and sensitivity-specificity of iTriplet in analyzing artificial and real biological sequences in various genomic regions. The results show that iTriplet is able to solve challenging cases. Furthermore we have confirmed the utility of iTriplet by showing it accurately predicts polyA-site-related motifs using a dual Luciferase reporter assay. Conclusion iTriplet is a novel rule-based combinatorial or enumerative motif finding method that is able to process highly degenerate and long motifs that have resisted analysis by other methods. In addition, iTriplet is distinguished from other methods of the same family by its parallelizability, which allows it to leverage the power of today's readily available high-performance computing systems.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2163713628",
    "type": "article"
  },
  {
    "title": "A simple, practical and complete O -time Algorithm for RNA folding using the Four-Russians Speedup",
    "doi": "https://doi.org/10.1186/1748-7188-5-13",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Yelena Frid; Dan Gusfield",
    "corresponding_authors": "",
    "abstract": "The problem of computationally predicting the secondary structure (or folding) of RNA molecules was first introduced more than thirty years ago and yet continues to be an area of active research and development. The basic RNA-folding problem of finding a maximum cardinality, non-crossing, matching of complimentary nucleotides in an RNA sequence of length n, has an O(n3)-time dynamic programming solution that is widely applied. It is known that an o(n3) worst-case time solution is possible, but the published and suggested methods are complex and have not been established to be practical. Significant practical improvements to the original dynamic programming method have been introduced, but they retain the O(n3) worst-case time bound when n is the only problem-parameter used in the bound. Surprisingly, the most widely-used, general technique to achieve a worst-case (and often practical) speed up of dynamic programming, the Four-Russians technique, has not been previously applied to the RNA-folding problem. This is perhaps due to technical issues in adapting the technique to RNA-folding.In this paper, we give a simple, complete, and practical Four-Russians algorithm for the basic RNA-folding problem, achieving a worst-case time-bound of O(n3/log(n)).We show that this time-bound can also be obtained for richer nucleotide matching scoring-schemes, and that the method achieves consistent speed-ups in practice. The contribution is both theoretical and practical, since the basic RNA-folding problem is often solved multiple times in the inner-loop of more complex algorithms, and for long RNA molecules in the study of RNA virus genomes.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2141607439",
    "type": "article"
  },
  {
    "title": "Fractal MapReduce decomposition of sequence alignment",
    "doi": "https://doi.org/10.1186/1748-7188-7-12",
    "publication_date": "2012-05-02",
    "publication_year": 2012,
    "authors": "Jonas S. Almeida; Alexander Grüneberg; Wolfgang Maaß; Susana Vinga",
    "corresponding_authors": "",
    "abstract": "The dramatic fall in the cost of genomic sequencing, and the increasing convenience of distributed cloud computing resources, positions the MapReduce coding pattern as a cornerstone of scalable bioinformatics algorithm development. In some cases an algorithm will find a natural distribution via use of map functions to process vectorized components, followed by a reduce of aggregate intermediate results. However, for some data analysis procedures such as sequence analysis, a more fundamental reformulation may be required.In this report we describe a solution to sequence comparison that can be thoroughly decomposed into multiple rounds of map and reduce operations. The route taken makes use of iterated maps, a fractal analysis technique, that has been found to provide a \"alignment-free\" solution to sequence analysis and comparison. That is, a solution that does not require dynamic programming, relying on a numeric Chaos Game Representation (CGR) data structure. This claim is demonstrated in this report by calculating the length of the longest similar segment by inspecting only the USM coordinates of two analogous units: with no resort to dynamic programming.The procedure described is an attempt at extreme decomposition and parallelization of sequence alignment in anticipation of a volume of genomic sequence data that cannot be met by current algorithmic frameworks. The solution found is delivered with a browser-based application (webApp), highlighting the browser's emergence as an environment for high performance distributed computing.Public distribution of accompanying software library with open source and version control at http://usm.github.com. Also available as a webApp through Google Chrome's WebStore http://chrome.google.com/webstore: search with \"usm\".",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2027378283",
    "type": "article"
  },
  {
    "title": "Adaptive efficient compression of genomes",
    "doi": "https://doi.org/10.1186/1748-7188-7-30",
    "publication_date": "2012-11-12",
    "publication_year": 2012,
    "authors": "Sebastian Wandelt; Ulf Leser",
    "corresponding_authors": "",
    "abstract": ": Modern high-throughput sequencing technologies are able to generate DNA sequences at an ever increasing rate. In parallel to the decreasing experimental time and cost necessary to produce DNA sequences, computational requirements for analysis and storage of the sequences are steeply increasing. Compression is a key technology to deal with this challenge. Recently, referential compression schemes, storing only the differences between a to-be-compressed input and a known reference sequence, gained a lot of interest in this field. However, memory requirements of the current algorithms are high and run times often are slow. In this paper, we propose an adaptive, parallel and highly efficient referential sequence compression method which allows fine-tuning of the trade-off between required memory and compression speed. When using 12 MB of memory, our method is for human genomes on-par with the best previous algorithms in terms of compression ratio (400:1) and compression speed. In contrast, it compresses a complete human genome in just 11 seconds when provided with 9 GB of main memory, which is almost three times faster than the best competitor while using less main memory.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2115431946",
    "type": "article"
  },
  {
    "title": "Computing evolutionary distinctiveness indices in large scale analysis",
    "doi": "https://doi.org/10.1186/1748-7188-7-6",
    "publication_date": "2012-04-13",
    "publication_year": 2012,
    "authors": "Iain Martyn; Tyler S. Kuhn; Arne Ø. Mooers; Vincent Moulton; Andreas Spillner",
    "corresponding_authors": "",
    "abstract": "We present optimal linear time algorithms for computing the Shapley values and 'heightened evolutionary distinctiveness' (HED) scores for the set of taxa in a phylogenetic tree. We demonstrate the efficiency of these new algorithms by applying them to a set of 10,000 reasonable 5139-species mammal trees. This is the first time these indices have been computed on such a large taxon and we contrast our finding with an ad-hoc index for mammals, fair proportion (FP), used by the Zoological Society of London's EDGE programme. Our empirical results follow expectations. In particular, the Shapley values are very strongly correlated with the FP scores, but provide a higher weight to the few monotremes that comprise the sister to all other mammals. We also find that the HED score, which measures a species' unique contribution to future subsets as function of the probability that close relatives will go extinct, is very sensitive to the estimated probabilities. When they are low, HED scores are less than FP scores, and approach the simple measure of a species' age. Deviations (like the Solendon genus of the West Indies) occur when sister species are both at high risk of extinction and their clade roots deep in the tree. Conversely, when endangered species have higher probabilities of being lost, HED scores can be greater than FP scores and species like the African elephant Loxondonta africana, the two solendons and the thumbless bat Furipterus horrens can move up the rankings. We suggest that conservation attention be applied to such species that carry genetic responsibility for imperiled close relatives. We also briefly discuss extensions of Shapley values and HED scores that are possible with the algorithms presented here.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2104168759",
    "type": "article"
  },
  {
    "title": "Best hits of 11110110111: model-free selection and parameter-free sensitivity calculation of spaced seeds",
    "doi": "https://doi.org/10.1186/s13015-017-0092-1",
    "publication_date": "2017-02-14",
    "publication_year": 2017,
    "authors": "Laurent Noé",
    "corresponding_authors": "Laurent Noé",
    "abstract": "Spaced seeds, also named gapped q-grams, gapped k-mers, spaced q-grams, have been proven to be more sensitive than contiguous seeds (contiguous q-grams, contiguous k-mers) in nucleic and amino-acid sequences analysis. Initially proposed to detect sequence similarities and to anchor sequence alignments, spaced seeds have more recently been applied in several alignment-free related methods. Unfortunately, spaced seeds need to be initially designed. This task is known to be time-consuming due to the number of spaced seed candidates. Moreover, it can be altered by a set of arbitrary chosen parameters from the probabilistic alignment models used. In this general context, Dominant seeds have been introduced by Mak and Benson (Bioinformatics 25:302-308, 2009) on the Bernoulli model, in order to reduce the number of spaced seed candidates that are further processed in a parameter-free calculation of the sensitivity.We expand the scope of work of Mak and Benson on single and multiple seeds by considering the Hit Integration model of Chung and Park (BMC Bioinform 11:31, 2010), demonstrate that the same dominance definition can be applied, and that a parameter-free study can be performed without any significant additional cost. We also consider two new discrete models, namely the Heaviside and the Dirac models, where lossless seeds can be integrated. From a theoretical standpoint, we establish a generic framework on all the proposed models, by applying a counting semi-ring to quickly compute large polynomial coefficients needed by the dominance filter. From a practical standpoint, we confirm that dominant seeds reduce the set of, either single seeds to thoroughly analyse, or multiple seeds to store. Moreover, in http://bioinfo.cristal.univ-lille.fr/yass/iedera_dominance, we provide a full list of spaced seeds computed on the four aforementioned models, with one (continuous) parameter left free for each model, and with several (discrete) alignment lengths.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2588746379",
    "type": "article"
  },
  {
    "title": "Fast phylogenetic inference from typing data",
    "doi": "https://doi.org/10.1186/s13015-017-0119-7",
    "publication_date": "2018-02-15",
    "publication_year": 2018,
    "authors": "João André Carriço; Maxime Crochemore; Alexandre P. Francisco; Solon P. Pissis; Bruno Ribeiro-Gonçalves; Cátia Vaz",
    "corresponding_authors": "",
    "abstract": "Microbial typing methods are commonly used to study the relatedness of bacterial strains. Sequence-based typing methods are a gold standard for epidemiological surveillance due to the inherent portability of sequence and allelic profile data, fast analysis times and their capacity to create common nomenclatures for strains or clones. This led to development of several novel methods and several databases being made available for many microbial species. With the mainstream use of High Throughput Sequencing, the amount of data being accumulated in these databases is huge, storing thousands of different profiles. On the other hand, computing genetic evolutionary distances among a set of typing profiles or taxa dominates the running time of many phylogenetic inference methods. It is important also to note that most of genetic evolution distance definitions rely, even if indirectly, on computing the pairwise Hamming distance among sequences or profiles. We propose here an average-case linear-time algorithm to compute pairwise Hamming distances among a set of taxa under a given Hamming distance threshold. This article includes both a theoretical analysis and extensive experimental results concerning the proposed algorithm. We further show how this algorithm can be successfully integrated into a well known phylogenetic inference method, and how it can be used to speedup querying local phylogenetic patterns over large typing databases.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2789665539",
    "type": "article"
  },
  {
    "title": "A representation of a compressed de Bruijn graph for pan-genome analysis that enables search",
    "doi": "https://doi.org/10.1186/s13015-016-0083-7",
    "publication_date": "2016-07-18",
    "publication_year": 2016,
    "authors": "Timo Beller; Enno Ohlebusch",
    "corresponding_authors": "",
    "abstract": "Recently, Marcus et al. (Bioinformatics 30:3476–83, 2014) proposed to use a compressed de Bruijn graph to describe the relationship between the genomes of many individuals/strains of the same or closely related species. They devised an $$O(n\\log g)$$ time algorithm called splitMEM that constructs this graph directly (i.e., without using the uncompressed de Bruijn graph) based on a suffix tree, where n is the total length of the genomes and g is the length of the longest genome. Baier et al. (Bioinformatics 32:497–504, 2016) improved their result. In this paper, we propose a new space-efficient representation of the compressed de Bruijn graph that adds the possibility to search for a pattern (e.g. an allele—a variant form of a gene) within the pan-genome. The ability to search within the pan-genome graph is of utmost importance and is a design goal of pan-genome data structures.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2272733715",
    "type": "article"
  },
  {
    "title": "Phylogeny reconstruction based on the length distribution of k-mismatch common substrings",
    "doi": "https://doi.org/10.1186/s13015-017-0118-8",
    "publication_date": "2017-12-01",
    "publication_year": 2017,
    "authors": "Burkhard Morgenstern; Svenja Schöbel; Chris-André Leimeister",
    "corresponding_authors": "Burkhard Morgenstern",
    "abstract": "Various approaches to alignment-free sequence comparison are based on the length of exact or inexact word matches between pairs of input sequences. Haubold et al. (J Comput Biol 16:1487–1500, 2009) showed how the average number of substitutions per position between two DNA sequences can be estimated based on the average length of exact common substrings. In this paper, we study the length distribution of k-mismatch common substrings between two sequences. We show that the number of substitutions per position can be accurately estimated from the position of a local maximum in the length distribution of their k-mismatch common substrings.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2775498959",
    "type": "article"
  },
  {
    "title": "Outlier detection in BLAST hits",
    "doi": "https://doi.org/10.1186/s13015-018-0126-3",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Nidhi Shah; Stephen F. Altschul; Mihai Pop",
    "corresponding_authors": "",
    "abstract": "An important task in a metagenomic analysis is the assignment of taxonomic labels to sequences in a sample. Most widely used methods for taxonomy assignment compare a sequence in the sample to a database of known sequences. Many approaches use the best BLAST hit(s) to assign the taxonomic label. However, it is known that the best BLAST hit may not always correspond to the best taxonomic match. An alternative approach involves phylogenetic methods, which take into account alignments and a model of evolution in order to more accurately define the taxonomic origin of sequences. Similarity-search based methods typically run faster than phylogenetic methods and work well when the organisms in the sample are well represented in the database. In contrast, phylogenetic methods have the capability to identify new organisms in a sample but are computationally quite expensive.We propose a two-step approach for metagenomic taxon identification; i.e., use a rapid method that accurately classifies sequences using a reference database (this is a filtering step) and then use a more complex phylogenetic method for the sequences that were unclassified in the previous step. In this work, we explore whether and when using top BLAST hit(s) yields a correct taxonomic label. We develop a method to detect outliers among BLAST hits in order to separate the phylogenetically most closely related matches from matches to sequences from more distantly related organisms. We used modified BILD (Bayesian Integral Log-Odds) scores, a multiple-alignment scoring function, to define the outliers within a subset of top BLAST hits and assign taxonomic labels. We compared the accuracy of our method to the RDP classifier and show that our method yields fewer misclassifications while properly classifying organisms that are not present in the database. Finally, we evaluated the use of our method as a pre-processing step before more expensive phylogenetic analyses (in our case TIPP) in the context of real 16S rRNA datasets.Our experiments make a good case for using a two-step approach for accurate taxonomic assignment. We show that our method can be used as a filtering step before using phylogenetic methods and provides a way to interpret BLAST results using more information than provided by E-values and bit-scores alone.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2799036102",
    "type": "article"
  },
  {
    "title": "Time-consistent reconciliation maps and forbidden time travel",
    "doi": "https://doi.org/10.1186/s13015-018-0121-8",
    "publication_date": "2018-02-06",
    "publication_year": 2018,
    "authors": "Nikolai Nøjgaard; Manuela Geiß; Daniel Merkle; Peter F. Stadler; Nicolas Wieseke; Marc Hellmuth",
    "corresponding_authors": "Marc Hellmuth",
    "abstract": "Background: In the absence of horizontal gene transfer it is possible to reconstruct the history of gene families from empirically determined orthology relations, which are equivalent to event-labeled gene trees. Knowledge of the event labels considerably simplifies the problem of reconciling a gene tree T with a species trees S, relative to the reconciliation problem without prior knowledge of the event types. It is well-known that optimal reconciliations in the unlabeled case may violate time-consistency and thus are not biologically feasible. Here we investigate the mathematical structure of the event labeled reconciliation problem with horizontal transfer. Results: We investigate the issue of time-consistency for the event-labeled version of the reconciliation problem, provide a convenient axiomatic framework, and derive a complete characterization of time-consistent reconciliations. This characterization depends on certain weak conditions on the event-labeled gene trees that reflect conditions under which evolutionary events are observable at least in principle. We give an [Formula: see text]-time algorithm to decide whether a time-consistent reconciliation map exists. It does not require the construction of explicit timing maps, but relies entirely on the comparably easy task of checking whether a small auxiliary graph is acyclic. The algorithms are implemented in C++ using the boost graph library and are freely available at https://github.com/Nojgaard/tc-recon. Significance: The combinatorial characterization of time consistency and thus biologically feasible reconciliation is an important step towards the inference of gene family histories with horizontal transfer from orthology data, i.e., without presupposed gene and species trees. The fast algorithm to decide time consistency is useful in a broader context because it constitutes an attractive component for all tools that address tree reconciliation problems.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2762409740",
    "type": "article"
  },
  {
    "title": "From pairs of most similar sequences to phylogenetic best matches",
    "doi": "https://doi.org/10.1186/s13015-020-00165-2",
    "publication_date": "2020-04-09",
    "publication_year": 2020,
    "authors": "Peter F. Stadler; Manuela Geiß; David Schaller; Alitzel López Sánchez; Marcos González Laffitte; Dulce I. Valdivia; Marc Hellmuth; Maribel Hernández-Rosales",
    "corresponding_authors": "Peter F. Stadler",
    "abstract": "Many of the commonly used methods for orthology detection start from mutually most similar pairs of genes (reciprocal best hits) as an approximation for evolutionary most closely related pairs of genes (reciprocal best matches). This approximation of best matches by best hits becomes exact for ultrametric dissimilarities, i.e., under the Molecular Clock Hypothesis. It fails, however, whenever there are large lineage specific rate variations among paralogous genes. In practice, this introduces a high level of noise into the input data for best-hit-based orthology detection methods.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3017197234",
    "type": "article"
  },
  {
    "title": "Mono-valent salt corrections for RNA secondary structures in the ViennaRNA package",
    "doi": "https://doi.org/10.1186/s13015-023-00236-0",
    "publication_date": "2023-07-29",
    "publication_year": 2023,
    "authors": "Hua-Ting Yao; Ronny Lorenz; Ivo L. Hofacker; Peter F. Stadler",
    "corresponding_authors": "Hua-Ting Yao; Peter F. Stadler",
    "abstract": "RNA features a highly negatively charged phosphate backbone that attracts a cloud of counter-ions that reduce the electrostatic repulsion in a concentration dependent manner. Ion concentrations thus have a large influence on folding and stability of RNA structures. Despite their well-documented effects, salt effects are not handled consistently by currently available secondary structure prediction algorithms. Combining Debye-Hückel potentials for line charges and Manning's counter-ion condensation theory, Einert et al. (Biophys J 100: 2745-2753, 2011) modeled the energetic contributions of monovalent cations on loops and helices.The model of Einert et al. is adapted to match the structure of the dynamic programming recursion of RNA secondary structure prediction algorithms. An empirical term describing the salt dependence of the duplex initiation energy is added to improve co-folding predictions for two or more RNA strands. The slightly modified model is implemented in the ViennaRNA package in such way that only the energy parameters but not the algorithmic structure is affected. A comparison with data from the literature show that predicted free energies and melting temperatures are in reasonable agreement with experiments.The new feature in the ViennaRNA package makes it possible to study effects of salt concentrations on RNA folding in a systematic manner. Strictly speaking, the model pertains only to mono-valent cations, and thus covers the most important parameter, i.e., the NaCl concentration. It remains a question for future research to what extent unspecific effects of bi- and tri-valent cations can be approximated in a similar manner.Corrections for the concentration of monovalent cations are available in the ViennaRNA package starting from version 2.6.0.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4385380567",
    "type": "article"
  },
  {
    "title": "Pfp-fm: an accelerated FM-index",
    "doi": "https://doi.org/10.1186/s13015-024-00260-8",
    "publication_date": "2024-04-10",
    "publication_year": 2024,
    "authors": "Aaron Hong; Marco Antônio Oliva; Dominik Köppl; Hideo Bannai; Christina Boucher; Travis Gagie",
    "corresponding_authors": "",
    "abstract": "Abstract FM-indexes are crucial data structures in DNA alignment, but searching with them usually takes at least one random access per character in the query pattern. Ferragina and Fischer [1] observed in 2007 that word-based indexes often use fewer random accesses than character-based indexes, and thus support faster searches. Since DNA lacks natural word-boundaries, however, it is necessary to parse it somehow before applying word-based FM-indexing. In 2022, Deng et al. [2] proposed parsing genomic data by induced suffix sorting, and showed that the resulting word-based FM-indexes support faster counting queries than standard FM-indexes when patterns are a few thousand characters or longer. In this paper we show that using prefix-free parsing—which takes parameters that let us tune the average length of the phrases—instead of induced suffix sorting, gives a significant speedup for patterns of only a few hundred characters. We implement our method and demonstrate it is between 3 and 18 times faster than competing methods on queries to GRCh38, and is consistently faster on queries made to 25,000, 50,000 and 100,000 SARS-CoV-2 genomes. Hence, it seems our method accelerates the performance of count over all state-of-the-art methods with a moderate increase in the memory. The source code for $$\\texttt {PFP-FM}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>PFP</mml:mi> <mml:mo>-</mml:mo> <mml:mi>FM</mml:mi> </mml:mrow> </mml:math> is available at https://github.com/AaronHong1024/afm .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4394688386",
    "type": "article"
  },
  {
    "title": "ESKEMAP: exact sketch-based read mapping",
    "doi": "https://doi.org/10.1186/s13015-024-00261-7",
    "publication_date": "2024-05-04",
    "publication_year": 2024,
    "authors": "Tizian Schulz; Paul Medvedev",
    "corresponding_authors": "Tizian Schulz; Paul Medvedev",
    "abstract": "Abstract Background Given a sequencing read, the broad goal of read mapping is to find the location(s) in the reference genome that have a “similar sequence”. Traditionally, “similar sequence” was defined as having a high alignment score and read mappers were viewed as heuristic solutions to this well-defined problem. For sketch-based mappers, however, there has not been a problem formulation to capture what problem an exact sketch-based mapping algorithm should solve. Moreover, there is no sketch-based method that can find all possible mapping positions for a read above a certain score threshold. Results In this paper, we formulate the problem of read mapping at the level of sequence sketches. We give an exact dynamic programming algorithm that finds all hits above a given similarity threshold. It runs in $$\\mathcal {O} (|t| + |p| + \\ell ^2)$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mo>+</mml:mo> <mml:mo>|</mml:mo> <mml:mi>p</mml:mi> <mml:mo>|</mml:mo> <mml:mo>+</mml:mo> <mml:msup> <mml:mi>ℓ</mml:mi> <mml:mn>2</mml:mn> </mml:msup> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> time and $$\\mathcal {O} (\\ell \\log \\ell )$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>log</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> space, where | t | is the number of $$k$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>k</mml:mi> </mml:math> -mers inside the sketch of the reference, | p | is the number of $$k$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>k</mml:mi> </mml:math> -mers inside the read’s sketch and $$\\ell$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>ℓ</mml:mi> </mml:math> is the number of times that $$k$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>k</mml:mi> </mml:math> -mers from the pattern sketch occur in the sketch of the text. We evaluate our algorithm’s performance in mapping long reads to the T2T assembly of human chromosome Y, where ampliconic regions make it desirable to find all good mapping positions. For an equivalent level of precision as minimap2, the recall of our algorithm is 0.88, compared to only 0.76 of minimap2.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4396638317",
    "type": "article"
  },
  {
    "title": "TINNiK: inference of the tree of blobs of a species network under the coalescent model",
    "doi": "https://doi.org/10.1186/s13015-024-00266-2",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Elizabeth S. Allman; Hector Baños; Jonathan Mitchell; John A. Rhodes",
    "corresponding_authors": "",
    "abstract": "The tree of blobs of a species network shows only the tree-like aspects of relationships of taxa on a network, omitting information on network substructures where hybridization or other types of lateral transfer of genetic information occur. By isolating such regions of a network, inference of the tree of blobs can serve as a starting point for a more detailed investigation, or indicate the limit of what may be inferrable without additional assumptions. Building on our theoretical work on the identifiability of the tree of blobs from gene quartet distributions under the Network Multispecies Coalescent model, we develop an algorithm, TINNiK, for statistically consistent tree of blobs inference. We provide examples of its application to both simulated and empirical datasets, utilizing an implementation in the MSCquartets 2.0 R package.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4404064679",
    "type": "article"
  },
  {
    "title": "Effective p-value computations using Finite Markov Chain Imbedding (FMCI): application to local score and to pattern statistics",
    "doi": "https://doi.org/10.1186/1748-7188-1-5",
    "publication_date": "2006-04-07",
    "publication_year": 2006,
    "authors": "Grégory Nuel",
    "corresponding_authors": "Grégory Nuel",
    "abstract": "The technique of Finite Markov Chain Imbedding (FMCI) is a classical approach to complex combinatorial problems related to sequences. In order to get efficient algorithms, it is known that such approaches need to be first rewritten using recursive relations. We propose here to give here a general recursive algorithms allowing to compute in a numerically stable manner exact Cumulative Distribution Function (CDF) or complementary CDF (CCDF). These algorithms are then applied in two particular cases: the local score of one sequence and pattern statistics. In both cases, asymptotic developments are derived. For the local score, our new approach allows for the very first time to compute exact p-values for a practical study (finding hydrophobic segments in a protein database) where only approximations were available before. In this study, the asymptotic approximations appear to be completely unreliable for 99.5% of the considered sequences. Concerning the pattern statistics, the new FMCI algorithms dramatically outperform the previous ones as they are more reliable, easier to implement, faster and with lower memory requirements.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2159507861",
    "type": "article"
  },
  {
    "title": "RNAstrand: reading direction of structured RNAs in multiple sequence alignments",
    "doi": "https://doi.org/10.1186/1748-7188-2-6",
    "publication_date": "2007-05-31",
    "publication_year": 2007,
    "authors": "Kristin Reiche; Peter F. Stadler",
    "corresponding_authors": "",
    "abstract": "Genome-wide screens for structured ncRNA genes in mammals, urochordates, and nematodes have predicted thousands of putative ncRNA genes and other structured RNA motifs. A prerequisite for their functional annotation is to determine the reading direction with high precision. While folding energies of an RNA and its reverse complement are similar, the differences are sufficient at least in conjunction with substitution patterns to discriminate between structured RNAs and their complements. We present here a support vector machine that reliably classifies the reading direction of a structured RNA from a multiple sequence alignment and provides a considerable improvement in classification accuracy over previous approaches. RNAstrand is freely available as a stand-alone tool from http://www.bioinf.uni-leipzig.de/Software/RNAstrand and is also included in the latest release of RNAz, a part of the Vienna RNA Package.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2137042854",
    "type": "article"
  },
  {
    "title": "Exact distribution of a pattern in a set of random sequences generated by a Markov source: applications to biological data",
    "doi": "https://doi.org/10.1186/1748-7188-5-15",
    "publication_date": "2010-01-26",
    "publication_year": 2010,
    "authors": "Grégory Nuel; Leslie Regad; Juliette Martin; Anne‐Claude Camproux",
    "corresponding_authors": "Grégory Nuel",
    "abstract": "In bioinformatics it is common to search for a pattern of interest in a potentially large set of rather short sequences (upstream gene regions, proteins, exons, etc.). Although many methodological approaches allow practitioners to compute the distribution of a pattern count in a random sequence generated by a Markov source, no specific developments have taken into account the counting of occurrences in a set of independent sequences. We aim to address this problem by deriving efficient approaches and algorithms to perform these computations both for low and high complexity patterns in the framework of homogeneous or heterogeneous Markov models.The latest advances in the field allowed us to use a technique of optimal Markov chain embedding based on deterministic finite automata to introduce three innovative algorithms. Algorithm 1 is the only one able to deal with heterogeneous models. It also permits to avoid any product of convolution of the pattern distribution in individual sequences. When working with homogeneous models, Algorithm 2 yields a dramatic reduction in the complexity by taking advantage of previous computations to obtain moment generating functions efficiently. In the particular case of low or moderate complexity patterns, Algorithm 3 exploits power computation and binary decomposition to further reduce the time complexity to a logarithmic scale. All these algorithms and their relative interest in comparison with existing ones were then tested and discussed on a toy-example and three biological data sets: structural patterns in protein loop structures, PROSITE signatures in a bacterial proteome, and transcription factors in upstream gene regions. On these data sets, we also compared our exact approaches to the tempting approximation that consists in concatenating the sequences in the data set into a single sequence.Our algorithms prove to be effective and able to handle real data sets with multiple sequences, as well as biological patterns of interest, even when the latter display a high complexity (PROSITE signatures for example). In addition, these exact algorithms allow us to avoid the edge effect observed under the single sequence approximation, which leads to erroneous results, especially when the marginal distribution of the model displays a slow convergence toward the stationary distribution. We end up with a discussion on our method and on its potential improvements.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2146231163",
    "type": "article"
  },
  {
    "title": "Module detection in complex networks using integer optimisation",
    "doi": "https://doi.org/10.1186/1748-7188-5-36",
    "publication_date": "2010-11-12",
    "publication_year": 2010,
    "authors": "Gang Xu; Laura Bennett; Lazaros G. Papageorgiou; Sophia Tsoka",
    "corresponding_authors": "",
    "abstract": "Abstract Background The detection of modules or community structure is widely used to reveal the underlying properties of complex networks in biology, as well as physical and social sciences. Since the adoption of modularity as a measure of network topological properties, several methodologies for the discovery of community structure based on modularity maximisation have been developed. However, satisfactory partitions of large graphs with modest computational resources are particularly challenging due to the NP-hard nature of the related optimisation problem. Furthermore, it has been suggested that optimising the modularity metric can reach a resolution limit whereby the algorithm fails to detect smaller communities than a specific size in large networks. Results We present a novel solution approach to identify community structure in large complex networks and address resolution limitations in module detection. The proposed algorithm employs modularity to express network community structure and it is based on mixed integer optimisation models. The solution procedure is extended through an iterative procedure to diminish effects that tend to agglomerate smaller modules (resolution limitations). Conclusions A comprehensive comparative analysis of methodologies for module detection based on modularity maximisation shows that our approach outperforms previously reported methods. Furthermore, in contrast to previous reports, we propose a strategy to handle resolution limitations in modularity maximisation. Overall, we illustrate ways to improve existing methodologies for community structure identification so as to increase its efficiency and applicability.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2120230599",
    "type": "article"
  },
  {
    "title": "Phylogenetic comparative assembly",
    "doi": "https://doi.org/10.1186/1748-7188-5-3",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Peter Husemann; Jens Stoye",
    "corresponding_authors": "",
    "abstract": "Recent high throughput sequencing technologies are capable of generating a huge amount of data for bacterial genome sequencing projects. Although current sequence assemblers successfully merge the overlapping reads, often several contigs remain which cannot be assembled any further. It is still costly and time consuming to close all the gaps in order to acquire the whole genomic sequence. Here we propose an algorithm that takes several related genomes and their phylogenetic relationships into account to create a graph that contains the likelihood for each pair of contigs to be adjacent. Subsequently, this graph can be used to compute a layout graph that shows the most promising contig adjacencies in order to aid biologists in finishing the complete genomic sequence. The layout graph shows unique contig orderings where possible, and the best alternatives where necessary. Our new algorithm for contig ordering uses sequence similarity as well as phylogenetic information to estimate adjacencies of contigs. An evaluation of our implementation shows that it performs better than recent approaches while being much faster at the same time.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2152465654",
    "type": "article"
  },
  {
    "title": "A robust approach based on Weibull distribution for clustering gene expression data",
    "doi": "https://doi.org/10.1186/1748-7188-6-14",
    "publication_date": "2011-05-31",
    "publication_year": 2011,
    "authors": "Huakun Wang; Zhenzhen Wang; Xia Li; Binsheng Gong; Lixin Feng; Ying Zhou",
    "corresponding_authors": "",
    "abstract": "Abstract Background Clustering is a widely used technique for analysis of gene expression data. Most clustering methods group genes based on the distances, while few methods group genes according to the similarities of the distributions of the gene expression levels. Furthermore, as the biological annotation resources accumulated, an increasing number of genes have been annotated into functional categories. As a result, evaluating the performance of clustering methods in terms of the functional consistency of the resulting clusters is of great interest. Results In this paper, we proposed the WDCM (Weibull Distribution-based Clustering Method), a robust approach for clustering gene expression data, in which the gene expressions of individual genes are considered as the random variables following unique Weibull distributions. Our WDCM is based on the concept that the genes with similar expression profiles have similar distribution parameters, and thus the genes are clustered via the Weibull distribution parameters. We used the WDCM to cluster three cancer gene expression data sets from the lung cancer, B-cell follicular lymphoma and bladder carcinoma and obtained well-clustered results. We compared the performance of WDCM with k-means and Self Organizing Map (SOM) using functional annotation information given by the Gene Ontology (GO). The results showed that the functional annotation ratios of WDCM are higher than those of the other methods. We also utilized the external measure Adjusted Rand Index to validate the performance of the WDCM. The comparative results demonstrate that the WDCM provides the better clustering performance compared to k-means and SOM algorithms. The merit of the proposed WDCM is that it can be applied to cluster incomplete gene expression data without imputing the missing values. Moreover, the robustness of WDCM is also evaluated on the incomplete data sets. Conclusions The results demonstrate that our WDCM produces clusters with more consistent functional annotations than the other methods. The WDCM is also verified to be robust and is capable of clustering gene expression data containing a small quantity of missing values.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2105801586",
    "type": "article"
  },
  {
    "title": "Reducing the worst case running times of a family of RNA and CFG problems, using Valiant’s approach",
    "doi": "https://doi.org/10.1186/1748-7188-6-20",
    "publication_date": "2011-08-18",
    "publication_year": 2011,
    "authors": "Shay Zakov; Dekel Tsur; Michal Ziv-Ukelson",
    "corresponding_authors": "",
    "abstract": "Abstract Background RNA secondary structure prediction is a mainstream bioinformatic domain, and is key to computational analysis of functional RNA. In more than 30 years, much research has been devoted to defining different variants of RNA structure prediction problems, and to developing techniques for improving prediction quality. Nevertheless, most of the algorithms in this field follow a similar dynamic programming approach as that presented by Nussinov and Jacobson in the late 70's, which typically yields cubic worst case running time algorithms. Recently, some algorithmic approaches were applied to improve the complexity of these algorithms, motivated by new discoveries in the RNA domain and by the need to efficiently analyze the increasing amount of accumulated genome-wide data. Results We study Valiant's classical algorithm for Context Free Grammar recognition in sub-cubic time, and extract features that are common to problems on which Valiant's approach can be applied. Based on this, we describe several problem templates, and formulate generic algorithms that use Valiant's technique and can be applied to all problems which abide by these templates, including many problems within the world of RNA Secondary Structures and Context Free Grammars. Conclusions The algorithms presented in this paper improve the theoretical asymptotic worst case running time bounds for a large family of important problems. It is also possible that the suggested techniques could be applied to yield a practical speedup for these problems. For some of the problems (such as computing the RNA partition function and base-pair binding probabilities), the presented techniques are the only ones which are currently known for reducing the asymptotic running time bounds of the standard algorithms.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1996951465",
    "type": "article"
  },
  {
    "title": "A novel method for identifying disease associated protein complexes based on functional similarity protein complex networks",
    "doi": "https://doi.org/10.1186/s13015-015-0044-6",
    "publication_date": "2015-04-27",
    "publication_year": 2015,
    "authors": "Duc‐Hau Le",
    "corresponding_authors": "Duc‐Hau Le",
    "abstract": "Protein complexes formed by non-covalent interaction among proteins play important roles in cellular functions. Computational and purification methods have been used to identify many protein complexes and their cellular functions. However, their roles in terms of causing disease have not been well discovered yet. There exist only a few studies for the identification of disease-associated protein complexes. However, they mostly utilize complicated heterogeneous networks which are constructed based on an out-of-date database of phenotype similarity network collected from literature. In addition, they only apply for diseases for which tissue-specific data exist.In this study, we propose a method to identify novel disease-protein complex associations. First, we introduce a framework to construct functional similarity protein complex networks where two protein complexes are functionally connected by either shared protein elements, shared annotating GO terms or based on protein interactions between elements in each protein complex. Second, we propose a simple but effective neighborhood-based algorithm, which yields a local similarity measure, to rank disease candidate protein complexes.Comparing the predictive performance of our proposed algorithm with that of two state-of-the-art network propagation algorithms including one we used in our previous study, we found that it performed statistically significantly better than that of these two algorithms for all the constructed functional similarity protein complex networks. In addition, it ran about 32 times faster than these two algorithms. Moreover, our proposed method always achieved high performance in terms of AUC values irrespective of the ways to construct the functional similarity protein complex networks and the used algorithms. The performance of our method was also higher than that reported in some existing methods which were based on complicated heterogeneous networks. Finally, we also tested our method with prostate cancer and selected the top 100 highly ranked candidate protein complexes. Interestingly, 69 of them were evidenced since at least one of their protein elements are known to be associated with prostate cancer.Our proposed method, including the framework to construct functional similarity protein complex networks and the neighborhood-based algorithm on these networks, could be used for identification of novel disease-protein complex associations.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1979799211",
    "type": "article"
  },
  {
    "title": "Playing hide and seek with repeats in local and global de novo transcriptome assembly of short RNA-seq reads",
    "doi": "https://doi.org/10.1186/s13015-017-0091-2",
    "publication_date": "2017-02-22",
    "publication_year": 2017,
    "authors": "Leandro Lima; Blerina Sinaimeri; Gustavo Sacomoto; Hélène Lopez-Maestre; Camille Marchet; Vincent Mièle; Marie-France Sagot; Vincent Lacroix",
    "corresponding_authors": "Leandro Lima",
    "abstract": "The main challenge in de novo genome assembly of DNA-seq data is certainly to deal with repeats that are longer than the reads. In de novo transcriptome assembly of RNA-seq reads, on the other hand, this problem has been underestimated so far. Even though we have fewer and shorter repeated sequences in transcriptomics, they do create ambiguities and confuse assemblers if not addressed properly. Most transcriptome assemblers of short reads are based on de Bruijn graphs (DBG) and have no clear and explicit model for repeats in RNA-seq data, relying instead on heuristics to deal with them. The results of this work are threefold. First, we introduce a formal model for representing high copy-number and low-divergence repeats in RNA-seq data and exploit its properties to infer a combinatorial characteristic of repeat-associated subgraphs. We show that the problem of identifying such subgraphs in a DBG is NP-complete. Second, we show that in the specific case of local assembly of alternative splicing (AS) events, we can implicitly avoid such subgraphs, and we present an efficient algorithm to enumerate AS events that are not included in repeats. Using simulated data, we show that this strategy is significantly more sensitive and precise than the previous version of KisSplice (Sacomoto et al. in WABI, pp 99–111, 1), Trinity (Grabherr et al. in Nat Biotechnol 29(7):644–652, 2), and Oases (Schulz et al. in Bioinformatics 28(8):1086–1092, 3), for the specific task of calling AS events. Third, we turn our focus to full-length transcriptome assembly, and we show that exploring the topology of DBGs can improve de novo transcriptome evaluation methods. Based on the observation that repeats create complicated regions in a DBG, and when assemblers try to traverse these regions, they can infer erroneous transcripts, we propose a measure to flag transcripts traversing such troublesome regions, thereby giving a confidence level for each transcript. The originality of our work when compared to other transcriptome evaluation methods is that we use only the topology of the DBG, and not read nor coverage information. We show that our simple method gives better results than Rsem-Eval (Li et al. in Genome Biol 15(12):553, 4) and TransRate (Smith-Unna et al. in Genome Res 26(8):1134–1144, 5) on both real and simulated datasets for detecting chimeras, and therefore is able to capture assembly errors missed by these methods.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2589448906",
    "type": "article"
  },
  {
    "title": "Reconciling multiple genes trees via segmental duplications and losses",
    "doi": "https://doi.org/10.1186/s13015-019-0139-6",
    "publication_date": "2019-03-20",
    "publication_year": 2019,
    "authors": "Riccardo Dondi; Manuel Lafond; Céline Scornavacca",
    "corresponding_authors": "",
    "abstract": "Reconciling gene trees with a species tree is a fundamental problem to understand the evolution of gene families. Many existing approaches reconcile each gene tree independently. However, it is well-known that the evolution of gene families is interconnected. In this paper, we extend a previous approach to reconcile a set of gene trees with a species tree based on segmental macro-evolutionary events, where segmental duplication events and losses are associated with cost $$\\delta $$ and $$\\lambda $$ , respectively. We show that the problem is polynomial-time solvable when $$\\delta \\le \\lambda $$ (via LCA-mapping), while if $$\\delta > \\lambda $$ the problem is NP-hard, even when $$\\lambda = 0$$ and a single gene tree is given, solving a long standing open problem on the complexity of multi-gene reconciliation. On the positive side, we give a fixed-parameter algorithm for the problem, where the parameters are $$\\delta /\\lambda $$ and the number d of segmental duplications, of time complexity $$O\\left(\\lceil \\frac{\\delta }{\\lambda } \\rceil ^{d} \\cdot n \\cdot \\frac{\\delta }{\\lambda }\\right)$$ . Finally, we demonstrate the usefulness of this algorithm on two previously studied real datasets: we first show that our method can be used to confirm or raise doubt on hypothetical segmental duplications on a set of 16 eukaryotes, then show how we can detect whole genome duplications in yeast genomes.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2940808208",
    "type": "article"
  },
  {
    "title": "Statistically consistent divide-and-conquer pipelines for phylogeny estimation using NJMerge",
    "doi": "https://doi.org/10.1186/s13015-019-0151-x",
    "publication_date": "2019-07-19",
    "publication_year": 2019,
    "authors": "Erin K. Molloy; Tandy Warnow",
    "corresponding_authors": "",
    "abstract": "Divide-and-conquer methods, which divide the species set into overlapping subsets, construct a tree on each subset, and then combine the subset trees using a supertree method, provide a key algorithmic framework for boosting the scalability of phylogeny estimation methods to large datasets. Yet the use of supertree methods, which typically attempt to solve NP-hard optimization problems, limits the scalability of such approaches. In this paper, we introduce a divide-and-conquer approach that does not require supertree estimation: we divide the species set into pairwise disjoint subsets, construct a tree on each subset using a base method, and then combine the subset trees using a distance matrix. For this merger step, we present a new method, called NJMerge, which is a polynomial-time extension of Neighbor Joining (NJ); thus, NJMerge can be viewed either as a method for improving traditional NJ or as a method for scaling the base method to larger datasets. We prove that NJMerge can be used to create divide-and-conquer pipelines that are statistically consistent under some models of evolution. We also report the results of an extensive simulation study evaluating NJMerge on multi-locus datasets with up to 1000 species. We found that NJMerge sometimes improved the accuracy of traditional NJ and substantially reduced the running time of three popular species tree methods (ASTRAL-III, SVDquartets, and “concatenation” using RAxML) without sacrificing accuracy. Finally, although NJMerge can fail to return a tree, in our experiments, NJMerge failed on only 11 out of 2560 test cases. Theoretical and empirical results suggest that NJMerge is a valuable technique for large-scale phylogeny estimation, especially when computational resources are limited. NJMerge is freely available on Github ( http://github.com/ekmolloy/njmerge ).",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2963132458",
    "type": "article"
  },
  {
    "title": "Implications of non-uniqueness in phylogenetic deconvolution of bulk DNA samples of tumors",
    "doi": "https://doi.org/10.1186/s13015-019-0155-6",
    "publication_date": "2019-09-03",
    "publication_year": 2019,
    "authors": "Yuanyuan Qi; Dikshant Pradhan; Mohammed El-Kebir",
    "corresponding_authors": "",
    "abstract": "Tumors exhibit extensive intra-tumor heterogeneity, the presence of groups of cellular populations with distinct sets of somatic mutations. This heterogeneity is the result of an evolutionary process, described by a phylogenetic tree. In addition to enabling clinicians to devise patient-specific treatment plans, phylogenetic trees of tumors enable researchers to decipher the mechanisms of tumorigenesis and metastasis. However, the problem of reconstructing a phylogenetic tree T given bulk sequencing data from a tumor is more complicated than the classic phylogeny inference problem. Rather than observing the leaves of T directly, we are given mutation frequencies that are the result of mixtures of the leaves of T. The majority of current tumor phylogeny inference methods employ the perfect phylogeny evolutionary model. The underlying Perfect Phylogeny Mixture (PPM) combinatorial problem typically has multiple solutions.We prove that determining the exact number of solutions to the PPM problem is #P-complete and hard to approximate within a constant factor. Moreover, we show that sampling solutions uniformly at random is hard as well. On the positive side, we provide a polynomial-time computable upper bound on the number of solutions and introduce a simple rejection-sampling based scheme that works well for small instances. Using simulated and real data, we identify factors that contribute to and counteract non-uniqueness of solutions. In addition, we study the sampling performance of current methods, identifying significant biases.Awareness of non-uniqueness of solutions to the PPM problem is key to drawing accurate conclusions in downstream analyses based on tumor phylogenies. This work provides the theoretical foundations for non-uniqueness of solutions in tumor phylogeny inference from bulk DNA samples.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2971560633",
    "type": "article"
  },
  {
    "title": "Pareto optimization in algebraic dynamic programming",
    "doi": "https://doi.org/10.1186/s13015-015-0051-7",
    "publication_date": "2015-07-06",
    "publication_year": 2015,
    "authors": "Cédric Saule; Robert Giegerich",
    "corresponding_authors": "",
    "abstract": "Pareto optimization combines independent objectives by computing the Pareto front of its search space, defined as the set of all solutions for which no other candidate solution scores better under all objectives. This gives, in a precise sense, better information than an artificial amalgamation of different scores into a single objective, but is more costly to compute. Pareto optimization naturally occurs with genetic algorithms, albeit in a heuristic fashion. Non-heuristic Pareto optimization so far has been used only with a few applications in bioinformatics. We study exact Pareto optimization for two objectives in a dynamic programming framework. We define a binary Pareto product operator [Formula: see text] on arbitrary scoring schemes. Independent of a particular algorithm, we prove that for two scoring schemes A and B used in dynamic programming, the scoring scheme [Formula: see text] correctly performs Pareto optimization over the same search space. We study different implementations of the Pareto operator with respect to their asymptotic and empirical efficiency. Without artificial amalgamation of objectives, and with no heuristics involved, Pareto optimization is faster than computing the same number of answers separately for each objective. For RNA structure prediction under the minimum free energy versus the maximum expected accuracy model, we show that the empirical size of the Pareto front remains within reasonable bounds. Pareto optimization lends itself to the comparative investigation of the behavior of two alternative scoring schemes for the same purpose. For the above scoring schemes, we observe that the Pareto front can be seen as a composition of a few macrostates, each consisting of several microstates that differ in the same limited way. We also study the relationship between abstract shape analysis and the Pareto front, and find that they extract information of a different nature from the folding space and can be meaningfully combined.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1960535846",
    "type": "article"
  },
  {
    "title": "Sorting signed permutations by short operations",
    "doi": "https://doi.org/10.1186/s13015-015-0040-x",
    "publication_date": "2015-03-24",
    "publication_year": 2015,
    "authors": "Gustavo Rodrigues Galvão; Orlando Lee; Zanoni Dias",
    "corresponding_authors": "",
    "abstract": "During evolution, global mutations may alter the order and the orientation of the genes in a genome. Such mutations are referred to as rearrangement events, or simply operations. In unichromosomal genomes, the most common operations are reversals, which are responsible for reversing the order and orientation of a sequence of genes, and transpositions, which are responsible for switching the location of two contiguous portions of a genome. The problem of computing the minimum sequence of operations that transforms one genome into another – which is equivalent to the problem of sorting a permutation into the identity permutation – is a well-studied problem that finds application in comparative genomics. There are a number of works concerning this problem in the literature, but they generally do not take into account the length of the operations (i.e. the number of genes affected by the operations). Since it has been observed that short operations are prevalent in the evolution of some species, algorithms that efficiently solve this problem in the special case of short operations are of interest. In this paper, we investigate the problem of sorting a signed permutation by short operations. More precisely, we study four flavors of this problem: (i) the problem of sorting a signed permutation by reversals of length at most 2; (ii) the problem of sorting a signed permutation by reversals of length at most 3; (iii) the problem of sorting a signed permutation by reversals and transpositions of length at most 2; and (iv) the problem of sorting a signed permutation by reversals and transpositions of length at most 3. We present polynomial-time solutions for problems (i) and (iii), a 5-approximation for problem (ii), and a 3-approximation for problem (iv). Moreover, we show that the expected approximation ratio of the 5-approximation algorithm is not greater than 3 for random signed permutations with more than 12 elements. Finally, we present experimental results that show that the approximation ratios of the approximation algorithms cannot be smaller than 3. In particular, this means that the approximation ratio of the 3-approximation algorithm is tight.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2082505684",
    "type": "article"
  },
  {
    "title": "A constraint solving approach to model reduction by tropical equilibration",
    "doi": "https://doi.org/10.1186/s13015-014-0024-2",
    "publication_date": "2014-12-01",
    "publication_year": 2014,
    "authors": "Sylvain Soliman; François Fages; Ovidiu Radulescu",
    "corresponding_authors": "",
    "abstract": "Model reduction is a central topic in systems biology and dynamical systems theory, for reducing the complexity of detailed models, finding important parameters, and developing multi-scale models for instance. While singular perturbation theory is a standard mathematical tool to analyze the different time scales of a dynamical system and decompose the system accordingly, tropical methods provide a simple algebraic framework to perform these analyses systematically in polynomial systems. The crux of these methods is in the computation of tropical equilibrations. In this paper we show that constraint-based methods, using reified constraints for expressing the equilibration conditions, make it possible to numerically solve non-linear tropical equilibration problems, out of reach of standard computation methods. We illustrate this approach first with the detailed reduction of a simple biochemical mechanism, the Michaelis-Menten enzymatic reaction model, and second, with large-scale performance figures obtained on the http://biomodels.net repository.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2171851694",
    "type": "article"
  },
  {
    "title": "Enumeration of minimal stoichiometric precursor sets in metabolic networks",
    "doi": "https://doi.org/10.1186/s13015-016-0087-3",
    "publication_date": "2016-09-19",
    "publication_year": 2016,
    "authors": "Ricardo Andrade; Martin Wannagat; Cecília C. Klein; Vicente Acuña; Alberto Marchetti-Spaccamela; Paulo V. Milreu; Leen Stougie; Marie-France Sagot",
    "corresponding_authors": "",
    "abstract": "What an organism needs at least from its environment to produce a set of metabolites, e.g. target(s) of interest and/or biomass, has been called a minimal precursor set. Early approaches to enumerate all minimal precursor sets took into account only the topology of the metabolic network (topological precursor sets). Due to cycles and the stoichiometric values of the reactions, it is often not possible to produce the target(s) from a topological precursor set in the sense that there is no feasible flux. Although considering the stoichiometry makes the problem harder, it enables to obtain biologically reasonable precursor sets that we call stoichiometric. Recently a method to enumerate all minimal stoichiometric precursor sets was proposed in the literature. The relationship between topological and stoichiometric precursor sets had however not yet been studied. Such relationship between topological and stoichiometric precursor sets is highlighted. We also present two algorithms that enumerate all minimal stoichiometric precursor sets. The first one is of theoretical interest only and is based on the above mentioned relationship. The second approach solves a series of mixed integer linear programming problems. We compared the computed minimal precursor sets to experimentally obtained growth media of several Escherichia coli strains using genome-scale metabolic networks. The results show that the second approach efficiently enumerates minimal precursor sets taking stoichiometry into account, and allows for broad in silico studies of strains or species interactions that may help to understand e.g. pathotype and niche-specific metabolic capabilities. sasita is written in Java, uses cplex as LP solver and can be downloaded together with all networks and input files used in this paper at http://sasita.gforge.inria.fr/ .",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2522250770",
    "type": "article"
  },
  {
    "title": "Faster algorithms for RNA-folding using the Four-Russians method",
    "doi": "https://doi.org/10.1186/1748-7188-9-5",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Balaji Venkatachalam; Dan Gusfield; Yelena Frid",
    "corresponding_authors": "Balaji Venkatachalam",
    "abstract": "The secondary structure that maximizes the number of non-crossing matchings between complimentary bases of an RNA sequence of length n can be computed in O(n3) time using Nussinov's dynamic programming algorithm. The Four-Russians method is a technique that reduces the running time for certain dynamic programming algorithms by a multiplicative factor after a preprocessing step where solutions to all smaller subproblems of a fixed size are exhaustively enumerated and solved. Frid and Gusfield designed an O(n3logn) algorithm for RNA folding using the Four-Russians technique. In their algorithm the preprocessing is interleaved with the algorithm computation.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2774679324",
    "type": "article"
  },
  {
    "title": "SMOTIF: efficient structured pattern and profile motif search",
    "doi": "https://doi.org/10.1186/1748-7188-1-22",
    "publication_date": "2006-11-21",
    "publication_year": 2006,
    "authors": "Yongqiang Zhang; Mohammed J. Zaki",
    "corresponding_authors": "",
    "abstract": "A structured motif allows variable length gaps between several components, where each component is a simple motif, which allows either no gaps or only fixed length gaps. The motif can either be represented as a pattern or a profile (also called positional weight matrix). We propose an efficient algorithm, called SMOTIF, to solve the structured motif search problem, i.e., given one or more sequences and a structured motif, SMOTIF searches the sequences for all occurrences of the motif. Potential applications include searching for long terminal repeat (LTR) retrotransposons and composite regulatory binding sites in DNA sequences. SMOTIF can search for both pattern and profile motifs, and it is efficient in terms of both time and space; it outperforms SMARTFINDER, a state-of-the-art algorithm for structured motif search. Experimental results show that SMOTIF is about 7 times faster and consumes 100 times less memory than SMARTFINDER. It can effectively search for LTR retrotransposons and is well suited to searching for motifs with long range gaps. It is also successful in finding potential composite transcription factor binding sites. SMOTIF is a useful and efficient tool in searching for structured pattern and profile motifs. The algorithm is available as open-source at: http://www.cs.rpi.edu/~zaki/software/sMotif/ .",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2139306337",
    "type": "article"
  },
  {
    "title": "\"Hook\"-calibration of GeneChip-microarrays: Chip characteristics and expression measures",
    "doi": "https://doi.org/10.1186/1748-7188-3-11",
    "publication_date": "2008-08-29",
    "publication_year": 2008,
    "authors": "Hans Binder; Knut Krohn; Stephan Preibisch",
    "corresponding_authors": "Hans Binder",
    "abstract": "Microarray experiments rely on several critical steps that may introduce biases and uncertainty in downstream analyses. These steps include mRNA sample extraction, amplification and labelling, hybridization, and scanning causing chip-specific systematic variations on the raw intensity level. Also the chosen array-type and the up-to-dateness of the genomic information probed on the chip affect the quality of the expression measures. In the accompanying publication we presented theory and algorithm of the so-called hook method which aims at correcting expression data for systematic biases using a series of new chip characteristics.In this publication we summarize the essential chip characteristics provided by this method, analyze special benchmark experiments to estimate transcript related expression measures and illustrate the potency of the method to detect and to quantify the quality of a particular hybridization. It is shown that our single-chip approach provides expression measures responding linearly on changes of the transcript concentration over three orders of magnitude. In addition, the method calculates a detection call judging the relation between the signal and the detection limit of the particular measurement. The performance of the method in the context of different chip generations and probe set assignments is illustrated. The hook method characterizes the RNA-quality in terms of the 3'/5'-amplification bias and the sample-specific calling rate. We show that the proper judgement of these effects requires the disentanglement of non-specific and specific hybridization which, otherwise, can lead to misinterpretations of expression changes. The consequences of modifying probe/target interactions by either changing the labelling protocol or by substituting RNA by DNA targets are demonstrated.The single-chip based hook-method provides accurate expression estimates and chip-summary characteristics using the natural metrics given by the hybridization reaction with the potency to develop new standards for microarray quality control and calibration.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2146948904",
    "type": "article"
  },
  {
    "title": "Fast algorithms for computing sequence distances by exhaustive substring composition",
    "doi": "https://doi.org/10.1186/1748-7188-3-13",
    "publication_date": "2008-10-28",
    "publication_year": 2008,
    "authors": "Alberto Apostolico; Olgert Denas",
    "corresponding_authors": "",
    "abstract": "The increasing throughput of sequencing raises growing needs for methods of sequence analysis and comparison on a genomic scale, notably, in connection with phylogenetic tree reconstruction. Such needs are hardly fulfilled by the more traditional measures of sequence similarity and distance, like string edit and gene rearrangement, due to a mixture of epistemological and computational problems. Alternative measures, based on the subword composition of sequences, have emerged in recent years and proved to be both fast and effective in a variety of tested cases. The common denominator of such measures is an underlying information theoretic notion of relative compressibility. Their viability depends critically on computational cost. The present paper describes as a paradigm the extension and efficient implementation of one of the methods in this class. The method is based on the comparison of the frequencies of all subwords in the two input sequences, where frequencies are suitably adjusted to take into account the statistical background.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2105378773",
    "type": "article"
  },
  {
    "title": "HuMiTar: A sequence-based method for prediction of human microRNA targets",
    "doi": "https://doi.org/10.1186/1748-7188-3-16",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "Jishou Ruan; Hanzhe Chen; Lukasz Kurgan; Ke Chen; Chunsheng Kang; Peiyu Pu",
    "corresponding_authors": "",
    "abstract": "MicroRNAs (miRs) are small noncoding RNAs that bind to complementary/partially complementary sites in the 3' untranslated regions of target genes to regulate protein production of the target transcript and to induce mRNA degradation or mRNA cleavage. The ability to perform accurate, high-throughput identification of physiologically active miR targets would enable functional characterization of individual miRs. Current target prediction methods include traditional approaches that are based on specific base-pairing rules in the miR's seed region and implementation of cross-species conservation of the target site, and machine learning (ML) methods that explore patterns that contrast true and false miR-mRNA duplexes. However, in the case of the traditional methods research shows that some seed region matches that are conserved are false positives and that some of the experimentally validated target sites are not conserved.We present HuMiTar, a computational method for identifying common targets of miRs, which is based on a scoring function that considers base-pairing for both seed and non-seed positions for human miR-mRNA duplexes. Our design shows that certain non-seed miR nucleotides, such as 14, 18, 13, 11, and 17, are characterized by a strong bias towards formation of Watson-Crick pairing. We contrasted HuMiTar with several representative competing methods on two sets of human miR targets and a set of ten glioblastoma oncogenes. Comparison with the two best performing traditional methods, PicTar and TargetScanS, and a representative ML method that considers the non-seed positions, NBmiRTar, shows that HuMiTar predictions include majority of the predictions of the other three methods. At the same time, the proposed method is also capable of finding more true positive targets as a trade-off for an increased number of predictions. Genome-wide predictions show that the proposed method is characterized by 1.99 signal-to-noise ratio and linear, with respect to the length of the mRNA sequence, computational complexity. The ROC analysis shows that HuMiTar obtains results comparable with PicTar, which are characterized by high true positive rates that are coupled with moderate values of false positive rates.The proposed HuMiTar method constitutes a step towards providing an efficient model for studying translational gene regulation by miRs.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2133004689",
    "type": "article"
  },
  {
    "title": "An image processing approach to computing distances between RNA secondary structures dot plots",
    "doi": "https://doi.org/10.1186/1748-7188-4-4",
    "publication_date": "2009-02-09",
    "publication_year": 2009,
    "authors": "Tor Ivry; Shahar Michal; Assaf Avihoo; Guillermo Sapiro; Danny Barash",
    "corresponding_authors": "",
    "abstract": "Abstract Background Computing the distance between two RNA secondary structures can contribute in understanding the functional relationship between them. When used repeatedly, such a procedure may lead to finding a query RNA structure of interest in a database of structures. Several methods are available for computing distances between RNAs represented as strings or graphs, but none utilize the RNA representation with dot plots. Since dot plots are essentially digital images, there is a clear motivation to devise an algorithm for computing the distance between dot plots based on image processing methods. Results We have developed a new metric dubbed 'DoPloCompare', which compares two RNA structures. The method is based on comparing dot plot diagrams that represent the secondary structures. When analyzing two diagrams and motivated by image processing, the distance is based on a combination of histogram correlations and a geometrical distance measure. We introduce, describe, and illustrate the procedure by two applications that utilize this metric on RNA sequences. The first application is the RNA design problem, where the goal is to find the nucleotide sequence for a given secondary structure. Examples where our proposed distance measure outperforms others are given. The second application locates peculiar point mutations that induce significant structural alternations relative to the wild type predicted secondary structure. The approach reported in the past to solve this problem was tested on several RNA sequences with known secondary structures to affirm their prediction, as well as on a data set of ribosomal pieces. These pieces were computationally cut from a ribosome for which an experimentally derived secondary structure is available, and on each piece the prediction conveys similarity to the experimental result. Our newly proposed distance measure shows benefit in this problem as well when compared to standard methods used for assessing the distance similarity between two RNA secondary structures. Conclusion Inspired by image processing and the dot plot representation for RNA secondary structure, we have managed to provide a conceptually new and potentially beneficial metric for comparing two RNA secondary structures. We illustrated our approach on the RNA design problem, as well as on an application that utilizes the distance measure to detect conformational rearranging point mutations in an RNA sequence.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2013805110",
    "type": "article"
  },
  {
    "title": "Periodic pattern detection in sparse boolean sequences",
    "doi": "https://doi.org/10.1186/1748-7188-5-31",
    "publication_date": "2010-09-10",
    "publication_year": 2010,
    "authors": "Ivan Junier; Joan Hérisson; François Képès",
    "corresponding_authors": "",
    "abstract": "The specific position of functionally related genes along the DNA has been shown to reflect the interplay between chromosome structure and genetic regulation. By investigating the statistical properties of the distances separating such genes, several studies have highlighted various periodic trends. In many cases, however, groups built up from co-functional or co-regulated genes are small and contain wrong information (data contamination) so that the statistics is poorly exploitable. In addition, gene positions are not expected to satisfy a perfectly ordered pattern along the DNA. Within this scope, we present an algorithm that aims to highlight periodic patterns in sparse boolean sequences, i.e. sequences of the type 010011011010... where the ratio of the number of 1's (denoting here the transcription start of a gene) to 0's is small.The algorithm is particularly robust with respect to strong signal distortions such as the addition of 1's at arbitrary positions (contaminated data), the deletion of existing 1's in the sequence (missing data) and the presence of disorder in the position of the 1's (noise). This robustness property stems from an appropriate exploitation of the remarkable alignment properties of periodic points in solenoidal coordinates.The efficiency of the algorithm is demonstrated in situations where standard Fourier-based spectral methods are poorly adapted. We also show how the proposed framework allows to identify the 1's that participate in the periodic trends, i.e. how the framework allows to allocate a positional score to genes, in the same spirit of the sequence score. The software is available for public use at http://www.issb.genopole.fr/MEGA/Softwares/iSSB_SolenoidalApplication.zip.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2150888175",
    "type": "article"
  },
  {
    "title": "Sparsification of RNA structure prediction including pseudoknots",
    "doi": "https://doi.org/10.1186/1748-7188-5-39",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Mathias Möhl; Raheleh Salari; Sebastian Will; Rolf Backofen; S. Cenk Şahinalp",
    "corresponding_authors": "",
    "abstract": "Although many RNA molecules contain pseudoknots, computational prediction of pseudoknotted RNA structure is still in its infancy due to high running time and space consumption implied by the dynamic programming formulations of the problem.In this paper, we introduce sparsification to significantly speedup the dynamic programming approaches for pseudoknotted RNA structure prediction, which also lower the space requirements. Although sparsification has been applied to a number of RNA-related structure prediction problems in the past few years, we provide the first application of sparsification to pseudoknotted RNA structure prediction specifically and to handling gapped fragments more generally - which has a much more complex recursive structure than other problems to which sparsification has been applied. We analyse how to sparsify four pseudoknot structure prediction algorithms, among those the most general method available (the Rivas-Eddy algorithm) and the fastest one (Reeder-Giegerich algorithm). In all algorithms the number of \"candidate\" substructures to be considered is reduced.Our experimental results on the sparsified Reeder-Giegerich algorithm suggest a linear speedup over the unsparsified implementation.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1978336838",
    "type": "article"
  },
  {
    "title": "Grammatical-Restrained Hidden Conditional Random Fields for Bioinformatics applications",
    "doi": "https://doi.org/10.1186/1748-7188-4-13",
    "publication_date": "2009-10-22",
    "publication_year": 2009,
    "authors": "Piero Fariselli; Castrense Savojardo; Pier Luigi Martelli; Rita Casadio",
    "corresponding_authors": "",
    "abstract": "Discriminative models are designed to naturally address classification tasks. However, some applications require the inclusion of grammar rules, and in these cases generative models, such as Hidden Markov Models (HMMs) and Stochastic Grammars, are routinely applied. We introduce Grammatical-Restrained Hidden Conditional Random Fields (GRHCRFs) as an extension of Hidden Conditional Random Fields (HCRFs). GRHCRFs while preserving the discriminative character of HCRFs, can assign labels in agreement with the production rules of a defined grammar. The main GRHCRF novelty is the possibility of including in HCRFs prior knowledge of the problem by means of a defined grammar. Our current implementation allows regular grammar rules. We test our GRHCRF on a typical biosequence labeling problem: the prediction of the topology of Prokaryotic outer-membrane proteins. We show that in a typical biosequence labeling problem the GRHCRF performs better than CRF models of the same complexity, indicating that GRHCRFs can be useful tools for biosequence analysis applications. GRHCRF software is available under GPLv3 licence at the website http://www.biocomp.unibo.it/~savojard/biocrf-0.9.tar.gz",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2145631315",
    "type": "article"
  },
  {
    "title": "Scoring function to predict solubility mutagenesis",
    "doi": "https://doi.org/10.1186/1748-7188-5-33",
    "publication_date": "2010-10-07",
    "publication_year": 2010,
    "authors": "Ye Tian; C. Deutsch; Bala Krishnamoorthy",
    "corresponding_authors": "",
    "abstract": "Abstract Background Mutagenesis is commonly used to engineer proteins with desirable properties not present in the wild type (WT) protein, such as increased or decreased stability, reactivity, or solubility. Experimentalists often have to choose a small subset of mutations from a large number of candidates to obtain the desired change, and computational techniques are invaluable to make the choices. While several such methods have been proposed to predict stability and reactivity mutagenesis, solubility has not received much attention. Results We use concepts from computational geometry to define a three body scoring function that predicts the change in protein solubility due to mutations. The scoring function captures both sequence and structure information. By exploring the literature, we have assembled a substantial database of 137 single- and multiple-point solubility mutations. Our database is the largest such collection with structural information known so far. We optimize the scoring function using linear programming (LP) methods to derive its weights based on training. Starting with default values of 1, we find weights in the range [0,2] so that predictions of increase or decrease in solubility are optimized. We compare the LP method to the standard machine learning techniques of support vector machines (SVM) and the Lasso. Using statistics for leave-one-out (LOO), 10-fold, and 3-fold cross validations (CV) for training and prediction, we demonstrate that the LP method performs the best overall. For the LOOCV, the LP method has an overall accuracy of 81%. Availability Executables of programs, tables of weights, and datasets of mutants are available from the following web page: http://www.wsu.edu/~kbala/OptSolMut.html .",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2139762959",
    "type": "article"
  },
  {
    "title": "WordCluster: detecting clusters of DNA words and genomic elements",
    "doi": "https://doi.org/10.1186/1748-7188-6-2",
    "publication_date": "2011-01-24",
    "publication_year": 2011,
    "authors": "Michael Hackenberg; Pedro Carpena; Pedro Bernaola‐Galván; Guillermo Barturen; A. Marti ́–Alganza; José Luis Tejera Oliver",
    "corresponding_authors": "",
    "abstract": "Many k-mers (or DNA words) and genomic elements are known to be spatially clustered in the genome. Well established examples are the genes, TFBSs, CpG dinucleotides, microRNA genes and ultra-conserved non-coding regions. Currently, no algorithm exists to find these clusters in a statistically comprehensible way. The detection of clustering often relies on densities and sliding-window approaches or arbitrarily chosen distance thresholds.We introduce here an algorithm to detect clusters of DNA words (k-mers), or any other genomic element, based on the distance between consecutive copies and an assigned statistical significance. We implemented the method into a web server connected to a MySQL backend, which also determines the co-localization with gene annotations. We demonstrate the usefulness of this approach by detecting the clusters of CAG/CTG (cytosine contexts that can be methylated in undifferentiated cells), showing that the degree of methylation vary drastically between inside and outside of the clusters. As another example, we used WordCluster to search for statistically significant clusters of olfactory receptor (OR) genes in the human genome.WordCluster seems to predict biological meaningful clusters of DNA words (k-mers) and genomic entities. The implementation of the method into a web server is available at http://bioinfo2.ugr.es/wordCluster/wordCluster.php including additional features like the detection of co-localization with gene regions or the annotation enrichment tool for functional analysis of overlapped genes.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2098188217",
    "type": "article"
  },
  {
    "title": "Predicting direct protein interactions from affinity purification mass spectrometry data",
    "doi": "https://doi.org/10.1186/1748-7188-5-34",
    "publication_date": "2010-10-29",
    "publication_year": 2010,
    "authors": "Ethan DH Kim; Ashish Sabharwal; Adrian Vetta; Mathieu Blanchette",
    "corresponding_authors": "",
    "abstract": "Affinity purification followed by mass spectrometry identification (AP-MS) is an increasingly popular approach to observe protein-protein interactions (PPI) in vivo. One drawback of AP-MS, however, is that it is prone to detecting indirect interactions mixed with direct physical interactions. Therefore, the ability to distinguish direct interactions from indirect ones is of much interest.We first propose a simple probabilistic model for the interactions captured by AP-MS experiments, under which the problem of separating direct interactions from indirect ones is formulated. Then, given idealized quantitative AP-MS data, we study the problem of identifying the most likely set of direct interactions that produced the observed data. We address this challenging graph theoretical problem by first characterizing signatures that can identify weakly connected nodes as well as dense regions of the network. The rest of the direct PPI network is then inferred using a genetic algorithm.Our algorithm shows good performance on both simulated and biological networks with very high sensitivity and specificity. Then the algorithm is used to predict direct interactions from a set of AP-MS PPI data from yeast, and its performance is measured against a high-quality interaction dataset.As the sensitivity of AP-MS pipeline improves, the fraction of indirect interactions detected will also increase, thereby making the ability to distinguish them even more desirable. Despite the simplicity of our model for indirect interactions, our method provides a good performance on the test networks.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2110150937",
    "type": "article"
  },
  {
    "title": "Separating metagenomic short reads into genomes via clustering",
    "doi": "https://doi.org/10.1186/1748-7188-7-27",
    "publication_date": "2012-09-26",
    "publication_year": 2012,
    "authors": "Olga Tanaseichuk; James Borneman; Tao Jiang",
    "corresponding_authors": "",
    "abstract": "The metagenomics approach allows the simultaneous sequencing of all genomes in an environmental sample. This results in high complexity datasets, where in addition to repeats and sequencing errors, the number of genomes and their abundance ratios are unknown. Recently developed next-generation sequencing (NGS) technologies significantly improve the sequencing efficiency and cost. On the other hand, they result in shorter reads, which makes the separation of reads from different species harder. Among the existing computational tools for metagenomic analysis, there are similarity-based methods that use reference databases to align reads and composition-based methods that use composition patterns (i.e., frequencies of short words or l-mers) to cluster reads. Similarity-based methods are unable to classify reads from unknown species without close references (which constitute the majority of reads). Since composition patterns are preserved only in significantly large fragments, composition-based tools cannot be used for very short reads, which becomes a significant limitation with the development of NGS. A recently proposed algorithm, AbundanceBin, introduced another method that bins reads based on predicted abundances of the genomes sequenced. However, it does not separate reads from genomes of similar abundance levels.In this work, we present a two-phase heuristic algorithm for separating short paired-end reads from different genomes in a metagenomic dataset. We use the observation that most of the l-mers belong to unique genomes when l is sufficiently large. The first phase of the algorithm results in clusters of l-mers each of which belongs to one genome. During the second phase, clusters are merged based on l-mer repeat information. These final clusters are used to assign reads. The algorithm could handle very short reads and sequencing errors. It is initially designed for genomes with similar abundance levels and then extended to handle arbitrary abundance ratios. The software can be download for free at http://www.cs.ucr.edu/∼tanaseio/toss.htm.Our tests on a large number of simulated metagenomic datasets concerning species at various phylogenetic distances demonstrate that genomes can be separated if the number of common repeats is smaller than the number of genome-specific repeats. For such genomes, our method can separate NGS reads with a high precision and sensitivity.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2155563453",
    "type": "article"
  },
  {
    "title": "A new, fast algorithm for detecting protein coevolution using maximum compatible cliques",
    "doi": "https://doi.org/10.1186/1748-7188-6-17",
    "publication_date": "2011-06-14",
    "publication_year": 2011,
    "authors": "Alex Rodionov; Alexandr Bezginov; Jonathan Rose; Elisabeth R.M. Tillier",
    "corresponding_authors": "Alex Rodionov; Elisabeth R.M. Tillier",
    "abstract": "The MatrixMatchMaker algorithm was recently introduced to detect the similarity between phylogenetic trees and thus the coevolution between proteins. MMM finds the largest common submatrices between pairs of phylogenetic distance matrices, and has numerous advantages over existing methods of coevolution detection. However, these advantages came at the cost of a very long execution time.In this paper, we show that the problem of finding the maximum submatrix reduces to a multiple maximum clique subproblem on a graph of protein pairs. This allowed us to develop a new algorithm and program implementation, MMMvII, which achieved more than 600× speedup with comparable accuracy to the original MMM.MMMvII will thus allow for more more extensive and intricate analyses of coevolution.An implementation of the MMMvII algorithm is available at: http://www.uhnresearch.ca/labs/tillier/MMMWEBvII/MMMWEBvII.php.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2158241520",
    "type": "article"
  },
  {
    "title": "Gene tree correction for reconciliation and species tree inference",
    "doi": "https://doi.org/10.1186/1748-7188-7-31",
    "publication_date": "2012-11-20",
    "publication_year": 2012,
    "authors": "Krister M. Swenson; Andrea Doroftei; Nadia El-Mabrouk",
    "corresponding_authors": "Krister M. Swenson",
    "abstract": "Reconciliation is the commonly used method for inferring the evolutionary scenario for a gene family. It consists in \"embedding\" inferred gene trees into a known species tree, revealing the evolution of the gene family by duplications and losses. When a species tree is not known, a natural algorithmic problem is to infer a species tree from a set of gene trees, such that the corresponding reconciliation minimizes the number of duplications and/or losses. The main drawback of reconciliation is that the inferred evolutionary scenario is strongly dependent on the considered gene trees, as few misplaced leaves may lead to a completely different history, with significantly more duplications and losses.In this paper, we take advantage of certain gene trees' properties in order to preprocess them for reconciliation or species tree inference. We flag certain duplication vertices of a gene tree, the \"non-apparent duplication\" (NAD) vertices, as resulting from the misplacement of leaves. In the case of species tree inference, we develop a polynomial-time heuristic for removing the minimum number of species leading to a set of gene trees that exhibit no NAD vertices with respect to at least one species tree. In the case of reconciliation, we consider the optimization problem of removing the minimum number of leaves or species leading to a tree without any NAD vertex. We develop a polynomial-time algorithm that is exact for two special classes of gene trees, and show a good performance on simulated data sets in the general case.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2165484374",
    "type": "article"
  },
  {
    "title": "ANMM4CBR: a case-based reasoning method for gene expression data classification",
    "doi": "https://doi.org/10.1186/1748-7188-5-14",
    "publication_date": "2010-01-06",
    "publication_year": 2010,
    "authors": "Bangpeng Yao; Shao Li",
    "corresponding_authors": "",
    "abstract": "Accurate classification of microarray data is critical for successful clinical diagnosis and treatment. The \"curse of dimensionality\" problem and noise in the data, however, undermines the performance of many algorithms. In order to obtain a robust classifier, a novel Additive Nonparametric Margin Maximum for Case-Based Reasoning (ANMM4CBR) method is proposed in this article. ANMM4CBR employs a case-based reasoning (CBR) method for classification. CBR is a suitable paradigm for microarray analysis, where the rules that define the domain knowledge are difficult to obtain because usually only a small number of training samples are available. Moreover, in order to select the most informative genes, we propose to perform feature selection via additively optimizing a nonparametric margin maximum criterion, which is defined based on gene pre-selection and sample clustering. Our feature selection method is very robust to noise in the data. The effectiveness of our method is demonstrated on both simulated and real data sets. We show that the ANMM4CBR method performs better than some state-of-the-art methods such as support vector machine (SVM) and k nearest neighbor (k NN), especially when the data contains a high level of noise. The source code is attached as an additional file of this paper.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2168535963",
    "type": "article"
  },
  {
    "title": "Finding driver pathways in cancer: models and algorithms",
    "doi": "https://doi.org/10.1186/1748-7188-7-23",
    "publication_date": "2012-09-06",
    "publication_year": 2012,
    "authors": "Fabio Vandin; Eli Upfal; Benjamin J. Raphael",
    "corresponding_authors": "Fabio Vandin",
    "abstract": "Cancer sequencing projects are now measuring somatic mutations in large numbers of cancer genomes. A key challenge in interpreting these data is to distinguish driver mutations, mutations important for cancer development, from passenger mutations that have accumulated in somatic cells but without functional consequences. A common approach to identify genes harboring driver mutations is a single gene test that identifies individual genes that are recurrently mutated in a significant number of cancer genomes. However, the power of this test is reduced by: (1) the necessity of estimating the background mutation rate (BMR) for each gene; (2) the mutational heterogeneity in most cancers meaning that groups of genes (e.g. pathways), rather than single genes, are the primary target of mutations. We investigate the problem of discovering driver pathways, groups of genes containing driver mutations, directly from cancer mutation data and without prior knowledge of pathways or other interactions between genes. We introduce two generative models of somatic mutations in cancer and study the algorithmic complexity of discovering driver pathways in both models. We show that a single gene test for driver genes is highly sensitive to the estimate of the BMR. In contrast, we show that an algorithmic approach that maximizes a straightforward measure of the mutational properties of a driver pathway successfully discovers these groups of genes without an estimate of the BMR. Moreover, this approach is also successful in the case when the observed frequencies of passenger and driver mutations are indistinguishable, a situation where single gene tests fail. Accurate estimation of the BMR is a challenging task. Thus, methods that do not require an estimate of the BMR, such as the ones we provide here, can give increased power for the discovery of driver genes.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2179763962",
    "type": "article"
  },
  {
    "title": "Constructing majority-rule supertrees",
    "doi": "https://doi.org/10.1186/1748-7188-5-2",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Jianrong Dong; David Fernández‐Baca; FR McMorris",
    "corresponding_authors": "",
    "abstract": "Supertree methods combine the phylogenetic information from multiple partially-overlapping trees into a larger phylogenetic tree called a supertree. Several supertree construction methods have been proposed to date, but most of these are not designed with any specific properties in mind. Recently, Cotton and Wilkinson proposed extensions of the majority-rule consensus tree method to the supertree setting that inherit many of the appealing properties of the former. We study a variant of one of Cotton and Wilkinson's methods, called majority-rule (+) supertrees. After proving that a key underlying problem for constructing majority-rule (+) supertrees is NP-hard, we develop a polynomial-size exact integer linear programming formulation of the problem. We then present a data reduction heuristic that identifies smaller subproblems that can be solved independently. While this technique is not guaranteed to produce optimal solutions, it can achieve substantial problem-size reduction. Finally, we report on a computational study of our approach on various real data sets, including the 121-taxon, 7-tree Seabirds data set of Kennedy and Page. The results indicate that our exact method is computationally feasible for moderately large inputs. For larger inputs, our data reduction heuristic makes it feasible to tackle problems that are well beyond the range of the basic integer programming approach. Comparisons between the results obtained by our heuristic and exact solutions indicate that the heuristic produces good answers. Our results also suggest that the majority-rule (+) approach, in both its basic form and with data reduction, yields biologically meaningful phylogenies.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2732829080",
    "type": "article"
  },
  {
    "title": "Distributional fold change test – a statistical approach for detecting differential expression in microarray experiments",
    "doi": "https://doi.org/10.1186/1748-7188-7-29",
    "publication_date": "2012-11-02",
    "publication_year": 2012,
    "authors": "Vadim Farztdinov; Fionnuala A. McDyer",
    "corresponding_authors": "",
    "abstract": "Because of the large volume of data and the intrinsic variation of data intensity observed in microarray experiments, different statistical methods have been used to systematically extract biological information and to quantify the associated uncertainty. The simplest method to identify differentially expressed genes is to evaluate the ratio of average intensities in two different conditions and consider all genes that differ by more than an arbitrary cut-off value to be differentially expressed. This filtering approach is not a statistical test and there is no associated value that can indicate the level of confidence in the designation of genes as differentially expressed or not differentially expressed. At the same time the fold change by itself provide valuable information and it is important to find unambiguous ways of using this information in expression data treatment. A new method of finding differentially expressed genes, called distributional fold change (DFC) test is introduced. The method is based on an analysis of the intensity distribution of all microarray probe sets mapped to a three dimensional feature space composed of average expression level, average difference of gene expression and total variance. The proposed method allows one to rank each feature based on the signal-to-noise ratio and to ascertain for each feature the confidence level and power for being differentially expressed. The performance of the new method was evaluated using the total and partial area under receiver operating curves and tested on 11 data sets from Gene Omnibus Database with independently verified differentially expressed genes and compared with the t-test and shrinkage t-test. Overall the DFC test performed the best – on average it had higher sensitivity and partial AUC and its elevation was most prominent in the low range of differentially expressed features, typical for formalin-fixed paraffin-embedded sample sets. The distributional fold change test is an effective method for finding and ranking differentially expressed probesets on microarrays. The application of this test is advantageous to data sets using formalin-fixed paraffin-embedded samples or other systems where degradation effects diminish the applicability of correlation adjusted methods to the whole feature set.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2131515457",
    "type": "article"
  },
  {
    "title": "Ultrametric networks: a new tool for phylogenetic analysis",
    "doi": "https://doi.org/10.1186/1748-7188-8-7",
    "publication_date": "2013-03-05",
    "publication_year": 2013,
    "authors": "Alberto Apostolico; Matteo Comin; Andres Dress; Laxmi Parida",
    "corresponding_authors": "",
    "abstract": "Abstract Background The large majority of optimization problems related to the inference of distance‐based trees used in phylogenetic analysis and classification is known to be intractable. One noted exception is found within the realm of ultrametric distances. The introduction of ultrametric trees in phylogeny was inspired by a model of evolution driven by the postulate of a molecular clock, now dismissed, whereby phylogeny could be represented by a weighted tree in which the sum of the weights of the edges separating any given leaf from the root is the same for all leaves. Both, molecular clocks and rooted ultrametric trees, fell out of fashion as credible representations of evolutionary change. At the same time, ultrametric dendrograms have shown good potential for purposes of classification in so far as they have proven to provide good approximations for additive trees. Most of these approximations are still intractable, but the problem of finding the nearest ultrametric distance matrix to a given distance matrix with respect to the L ∞ distance has been long known to be solvable in polynomial time, the solution being incarnated in any minimum spanning tree for the weighted graph subtending to the matrix. Results This paper expands this subdominant ultrametric perspective by studying ultrametric networks , consisting of the collection of all edges involved in some minimum spanning tree. It is shown that, for a graph with n vertices, the construction of such a network can be carried out by a simple algorithm in optimal time O ( n 2 ) which is faster by a factor of n than the direct adaptation of the classical O ( n 3 ) paradigm by Warshall for computing the transitive closure of a graph. This algorithm, called UltraNet, will be shown to be easily adapted to compute relaxed networks and to support the introduction of artificial points to reduce the maximum distance between vertices in a pair. Finally, a few experiments will be discussed to demonstrate the applicability of subdominant ultrametric networks. Availability http://www.dei.unipd.it/~ciompin/main/Ultranet/Ultranet.html",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2143221644",
    "type": "article"
  },
  {
    "title": "Generalized enhanced suffix array construction in external memory",
    "doi": "https://doi.org/10.1186/s13015-017-0117-9",
    "publication_date": "2017-12-01",
    "publication_year": 2017,
    "authors": "Felipe A. Louza; Guilherme P. Telles; Steve Hoffmann; Cristina D. A. Ciferri",
    "corresponding_authors": "Cristina D. A. Ciferri",
    "abstract": "Suffix arrays, augmented by additional data structures, allow solving efficiently many string processing problems. The external memory construction of the generalized suffix array for a string collection is a fundamental task when the size of the input collection or the data structure exceeds the available internal memory.In this article we present and analyze [Formula: see text] [introduced in CPM (External memory generalized suffix and [Formula: see text] arrays construction. In: Proceedings of CPM. pp 201-10, 2013)], the first external memory algorithm to construct generalized suffix arrays augmented with the longest common prefix array for a string collection. Our algorithm relies on a combination of buffers, induced sorting and a heap to avoid direct string comparisons. We performed experiments that covered different aspects of our algorithm, including running time, efficiency, external memory access, internal phases and the influence of different optimization strategies. On real datasets of size up to 24 GB and using 2 GB of internal memory, [Formula: see text] showed a competitive performance when compared to [Formula: see text] and [Formula: see text], which are efficient algorithms for a single string according to the related literature. We also show the effect of disk caching managed by the operating system on our algorithm.The proposed algorithm was validated through performance tests using real datasets from different domains, in various combinations, and showed a competitive performance. Our algorithm can also construct the generalized Burrows-Wheeler transform of a string collection with no additional cost except by the output time.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2774498188",
    "type": "article"
  },
  {
    "title": "Prediction of plant promoters based on hexamers and random triplet pair analysis",
    "doi": "https://doi.org/10.1186/1748-7188-6-19",
    "publication_date": "2011-06-28",
    "publication_year": 2011,
    "authors": "AKM Azad; Saima Shahid; Nasimul Noman; Hyunju Lee",
    "corresponding_authors": "",
    "abstract": "Abstract Background With an increasing number of plant genome sequences, it has become important to develop a robust computational method for detecting plant promoters. Although a wide variety of programs are currently available, prediction accuracy of these still requires further improvement. The limitations of these methods can be addressed by selecting appropriate features for distinguishing promoters and non-promoters. Methods In this study, we proposed two feature selection approaches based on hexamer sequences: the Frequency Distribution Analyzed Feature Selection Algorithm (FDAFSA) and the Random Triplet Pair Feature Selecting Genetic Algorithm (RTPFSGA). In FDAFSA, adjacent triplet-pairs (hexamer sequences) were selected based on the difference in the frequency of hexamers between promoters and non-promoters. In RTPFSGA, random triplet-pairs (RTPs) were selected by exploiting a genetic algorithm that distinguishes frequencies of non-adjacent triplet pairs between promoters and non-promoters. Then, a support vector machine (SVM), a nonlinear machine-learning algorithm, was used to classify promoters and non-promoters by combining these two feature selection approaches. We referred to this novel algorithm as PromoBot. Results Promoter sequences were collected from the PlantProm database. Non-promoter sequences were collected from plant mRNA, rRNA, and tRNA of PlantGDB and plant miRNA of miRBase. Then, in order to validate the proposed algorithm, we applied a 5-fold cross validation test. Training data sets were used to select features based on FDAFSA and RTPFSGA, and these features were used to train the SVM. We achieved 89% sensitivity and 86% specificity. Conclusions We compared our PromoBot algorithm to five other algorithms. It was found that the sensitivity and specificity of PromoBot performed well (or even better) with the algorithms tested. These results show that the two proposed feature selection methods based on hexamer frequencies and random triplet-pair could be successfully incorporated into a supervised machine learning method in promoter classification problem. As such, we expect that PromoBot can be used to help identify new plant promoters. Source codes and analysis results of this work could be provided upon request.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2098442797",
    "type": "article"
  },
  {
    "title": "MoDock: A multi-objective strategy improves the accuracy for molecular docking",
    "doi": "https://doi.org/10.1186/s13015-015-0034-8",
    "publication_date": "2015-02-17",
    "publication_year": 2015,
    "authors": "Junfeng Gu; Xu Yang; Ling Kang; Jinying Wu; Xicheng Wang",
    "corresponding_authors": "",
    "abstract": "As a main method of structure-based virtual screening, molecular docking is the most widely used in practice. However, the non-ideal efficacy of scoring functions is thought as the biggest barrier which hinders the improvement of the molecular docking method.A new multi-objective strategy for molecular docking, named as MoDock, is presented to further improve the docking accuracy with available scoring functions. Instead of simple combination of multiple objectives with fixed weight factors, an aggregate function is adopted to approximate the real solution of the original multi-objective and multi-constraint problem, which will simultaneously smooth the energy surface of the combined scoring functions. Then, method of centers and genetic algorithm are used to find the optimal solution. Tests of MoDock against the GOLD test data set reveal the multi-objective strategy improves the docking accuracy over the individual scoring functions. Meanwhile, a 70% ratio of the good docking solutions with the RMSD value below 1.0 Å outperforms other 6 commonly used docking programs, even with a flexible receptor docking program included.The results show MoDock is an effective strategy to overcome the deviations brought by single scoring function, and improves the prediction power of molecular docking.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2127358457",
    "type": "article"
  },
  {
    "title": "Fast local fragment chaining using sum-of-pair gap costs",
    "doi": "https://doi.org/10.1186/1748-7188-6-4",
    "publication_date": "2011-03-18",
    "publication_year": 2011,
    "authors": "Christian Otto; Steve Hoffmann; Jan Gorodkin; Peter F. Stadler",
    "corresponding_authors": "Peter F. Stadler",
    "abstract": "Background: Fast seed-based alignment heuristics such as BLAST and BLAT have become indispensable tools in comparative genomics for all studies aiming at the evolutionary relations of proteins, genes, and non-coding RNAs. This is true in particular for the large mammalian genomes. The sensitivity and specificity of these tools, however, crucially depend on parameters such as seed sizes or maximum expectation values. In settings that require high sensitivity the amount of short local match fragments easily becomes intractable. Then, fragment chaining is a powerful leverage to quickly connect, score, and rank the fragments to improve the specificity. Results: Here we present a fast and flexible fragment chainer that for the first time also supports a sum-of-pair gap cost model. This model has proven to achieve a higher accuracy and sensitivity in its own field of application. Due to a highly time-efficient index structure our method outperforms the only existing tool for fragment chaining under the linear gap cost model. It can easily be applied to the output generated by alignment tools such as segemehl or BLAST. As an example we consider homology-based searches for human and mouse snoRNAs demonstrating that a highly sensitive BLAST search with subsequent chaining is an attractive option. The sum-of pair gap costs provide a substantial advantage is this context. Conclusions: Chaining of short match fragments helps to quickly and accurately identify regions of homology that may not be found using local alignment heuristics alone. By providing both the linear and the sum-of-pair gap cost model, a wider range of application can be covered. The software clasp is available at http://www.bioinf.uni-leipzig.de/Software/clasp/.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2160175656",
    "type": "article"
  },
  {
    "title": "Automated partial atomic charge assignment for drug-like molecules: a fast knapsack approach",
    "doi": "https://doi.org/10.1186/s13015-019-0138-7",
    "publication_date": "2019-02-05",
    "publication_year": 2019,
    "authors": "Martin S. Engler; Bertrand Caron; Lourens Veen; Daan P. Geerke; Alan E. Mark; Gunnar W. Klau",
    "corresponding_authors": "",
    "abstract": "A key factor in computational drug design is the consistency and reliability with which intermolecular interactions between a wide variety of molecules can be described. Here we present a procedure to efficiently, reliably and automatically assign partial atomic charges to atoms based on known distributions. We formally introduce the molecular charge assignment problem, where the task is to select a charge from a set of candidate charges for every atom of a given query molecule. Charges are accompanied by a score that depends on their observed frequency in similar neighbourhoods (chemical environments) in a database of previously parameterised molecules. The aim is to assign the charges such that the total charge equals a known target charge within a margin of error while maximizing the sum of the charge scores. We show that the problem is a variant of the well-studied multiple-choice knapsack problem and thus weakly $$\\mathcal {NP}$$ -complete. We propose solutions based on Integer Linear Programming and a pseudo-polynomial time Dynamic Programming algorithm. We demonstrate that the results obtained for novel molecules not included in the database are comparable to the ones obtained performing explicit charge calculations while decreasing the time to determine partial charges for a molecule from hours or even days to below a second. Our software is openly available.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2918225422",
    "type": "article"
  },
  {
    "title": "An efficient algorithm for protein structure comparison using elastic shape analysis",
    "doi": "https://doi.org/10.1186/s13015-016-0089-1",
    "publication_date": "2016-09-29",
    "publication_year": 2016,
    "authors": "Sudhir Srivastava; S. B. Lal; Dwijesh Chandra Mishra; U. B. Angadi; Krishna Kumar Chaturvedi; S. N.; Anil Rai",
    "corresponding_authors": "",
    "abstract": "Protein structure comparison play important role in in silico functional prediction of a new protein. It is also used for understanding the evolutionary relationships among proteins. A variety of methods have been proposed in literature for comparing protein structures but they have their own limitations in terms of accuracy and complexity with respect to computational time and space. There is a need to improve the computational complexity in comparison/alignment of proteins through incorporation of important biological and structural properties in the existing techniques.An efficient algorithm has been developed for comparing protein structures using elastic shape analysis in which the sequence of 3D coordinates atoms of protein structures supplemented by additional auxiliary information from side-chain properties are incorporated. The protein structure is represented by a special function called square-root velocity function. Furthermore, singular value decomposition and dynamic programming have been employed for optimal rotation and optimal matching of the proteins, respectively. Also, geodesic distance has been calculated and used as the dissimilarity score between two protein structures. The performance of the developed algorithm is tested and found to be more efficient, i.e., running time reduced by 80-90 % without compromising accuracy of comparison when compared with the existing methods. Source codes for different functions have been developed in R. Also, user friendly web-based application called ProtSComp has been developed using above algorithm for comparing protein 3D structures and is accessible free.The methodology and algorithm developed in this study is taking considerably less computational time without loss of accuracy (Table 2). The proposed algorithm is considering different criteria of representing protein structures using 3D coordinates of atoms and inclusion of residue wise molecular properties as auxiliary information.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2524951041",
    "type": "article"
  },
  {
    "title": "Finding all maximal perfect haplotype blocks in linear time",
    "doi": "https://doi.org/10.1186/s13015-020-0163-6",
    "publication_date": "2020-02-10",
    "publication_year": 2020,
    "authors": "Jarno Alanko; Hideo Bannai; Bastien Cazaux; Pierre Peterlongo; Jens Stoye",
    "corresponding_authors": "",
    "abstract": "Abstract Recent large-scale community sequencing efforts allow at an unprecedented level of detail the identification of genomic regions that show signatures of natural selection. Traditional methods for identifying such regions from individuals’ haplotype data, however, require excessive computing times and therefore are not applicable to current datasets. In 2019, Cunha et al. (Advances in bioinformatics and computational biology: 11th Brazilian symposium on bioinformatics, BSB 2018, Niterói, Brazil, October 30 - November 1, 2018, Proceedings, 2018. 10.1007/978-3-030-01722-4_3 ) suggested the maximal perfect haplotype block as a very simple combinatorial pattern, forming the basis of a new method to perform rapid genome-wide selection scans. The algorithm they presented for identifying these blocks, however, had a worst-case running time quadratic in the genome length. It was posed as an open problem whether an optimal, linear-time algorithm exists. In this paper we give two algorithms that achieve this time bound, one conceptually very simple one using suffix trees and a second one using the positional Burrows–Wheeler Transform, that is very efficient also in practice.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2962871655",
    "type": "article"
  },
  {
    "title": "Bayesian optimization with evolutionary and structure-based regularization for directed protein evolution",
    "doi": "https://doi.org/10.1186/s13015-021-00195-4",
    "publication_date": "2021-07-01",
    "publication_year": 2021,
    "authors": "Trevor S. Frisby; Christopher J. Langmead",
    "corresponding_authors": "",
    "abstract": "Abstract Background Directed evolution (DE) is a technique for protein engineering that involves iterative rounds of mutagenesis and screening to search for sequences that optimize a given property, such as binding affinity to a specified target. Unfortunately, the underlying optimization problem is under-determined, and so mutations introduced to improve the specified property may come at the expense of unmeasured, but nevertheless important properties (ex. solubility, thermostability, etc). We address this issue by formulating DE as a regularized Bayesian optimization problem where the regularization term reflects evolutionary or structure-based constraints. Results We applied our approach to DE to three representative proteins, GB1, BRCA1, and SARS-CoV-2 Spike, and evaluated both evolutionary and structure-based regularization terms. The results of these experiments demonstrate that: (i) structure-based regularization usually leads to better designs (and never hurts), compared to the unregularized setting; (ii) evolutionary-based regularization tends to be least effective; and (iii) regularization leads to better designs because it effectively focuses the search in certain areas of sequence space, making better use of the experimental budget. Additionally, like previous work in Machine learning assisted DE, we find that our approach significantly reduces the experimental burden of DE, relative to model-free methods. Conclusion Introducing regularization into a Bayesian ML-assisted DE framework alters the exploratory patterns of the underlying optimization routine, and can shift variant selections towards those with a range of targeted and desirable properties. In particular, we find that structure-based regularization often improves variant selection compared to unregularized approaches, and never hurts.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3173337413",
    "type": "article"
  },
  {
    "title": "A new 1.375-approximation algorithm for sorting by transpositions",
    "doi": "https://doi.org/10.1186/s13015-022-00205-z",
    "publication_date": "2022-01-15",
    "publication_year": 2022,
    "authors": "Luiz Augusto G. Silva; Luís Kowada; Noraí R. Rocco; Maria Emília M. T. Walter",
    "corresponding_authors": "",
    "abstract": "Abstract Background sorting by transpositions (SBT) is a classical problem in genome rearrangements. In 2012, SBT was proven to be $$\\mathcal {NP}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>NP</mml:mi></mml:math> -hard and the best approximation algorithm with a 1.375 ratio was proposed in 2006 by Elias and Hartman (EH algorithm). Their algorithm employs simplification , a technique used to transform an input permutation $$\\pi$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>π</mml:mi></mml:math> into a simple permutation $${\\hat{\\pi }}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mover><mml:mi>π</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math> , presumably easier to handle with. The permutation $${\\hat{\\pi }}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mover><mml:mi>π</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math> is obtained by inserting new symbols into $$\\pi$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>π</mml:mi></mml:math> in a way that the lower bound of the transposition distance of $$\\pi$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>π</mml:mi></mml:math> is kept on $${\\hat{\\pi }}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mover><mml:mi>π</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math> . The simplification is guaranteed to keep the lower bound, not the transposition distance. A sequence of operations sorting $${\\hat{\\pi }}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mover><mml:mi>π</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math> can be mimicked to sort $$\\pi$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>π</mml:mi></mml:math> . Results and conclusions First, using an algebraic approach, we propose a new upper bound for the transposition distance, which holds for all $$S_n$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math> . Next, motivated by a problem identified in the EH algorithm, which causes it, in scenarios involving how the input permutation is simplified, to require one extra transposition above the 1.375-approximation ratio, we propose a new approximation algorithm to solve SBT ensuring the 1.375-approximation ratio for all $$S_n$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math> . We implemented our algorithm and EH’s. Regarding the implementation of the EH algorithm, two other issues were identified and needed to be fixed. We tested both algorithms against all permutations of size n , $$2\\le n \\le 12$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mn>2</mml:mn><mml:mo>≤</mml:mo><mml:mi>n</mml:mi><mml:mo>≤</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math> . The results show that the EH algorithm exceeds the approximation ratio of 1.375 for permutations with a size greater than 7. The percentage of computed distances that are equal to transposition distance, computed by the implemented algorithms are also compared with others available in the literature. Finally, we investigate the performance of both implementations on longer permutations of maximum length 500. From the experiments, we conclude that maximum and the average distances computed by our algorithm are a little better than the ones computed by the EH algorithm and the running times of both algorithms are similar, despite the time complexity of our algorithm being higher.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3210690626",
    "type": "article"
  },
  {
    "title": "Parsimonious Clone Tree Integration in cancer",
    "doi": "https://doi.org/10.1186/s13015-022-00209-9",
    "publication_date": "2022-03-14",
    "publication_year": 2022,
    "authors": "Palash Sashittal; Simone Zaccaria; Mohammed El-Kebir",
    "corresponding_authors": "",
    "abstract": "Every tumor is composed of heterogeneous clones, each corresponding to a distinct subpopulation of cells that accumulated different types of somatic mutations, ranging from single-nucleotide variants (SNVs) to copy-number aberrations (CNAs). As the analysis of this intra-tumor heterogeneity has important clinical applications, several computational methods have been introduced to identify clones from DNA sequencing data. However, due to technological and methodological limitations, current analyses are restricted to identifying tumor clones only based on either SNVs or CNAs, preventing a comprehensive characterization of a tumor's clonal composition.To overcome these challenges, we formulate the identification of clones in terms of both SNVs and CNAs as a integration problem while accounting for uncertainty in the input SNV and CNA proportions. We thus characterize the computational complexity of this problem and we introduce PACTION (PArsimonious Clone Tree integratION), an algorithm that solves the problem using a mixed integer linear programming formulation. On simulated data, we show that tumor clones can be identified reliably, especially when further taking into account the ancestral relationships that can be inferred from the input SNVs and CNAs. On 49 tumor samples from 10 prostate cancer patients, our integration approach provides a higher resolution view of tumor evolution than previous studies.PACTION is an accurate and fast method that reconstructs clonal architecture of cancer tumors by integrating SNV and CNA clones inferred using existing methods.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4220655002",
    "type": "article"
  },
  {
    "title": "Fast calculation of the quartet distance between trees of arbitrary degrees",
    "doi": "https://doi.org/10.1186/1748-7188-1-16",
    "publication_date": "2006-09-25",
    "publication_year": 2006,
    "authors": "Chris Christiansen; Thomas Mailund; Christian NS Pedersen; Martin Randers; Martin Stig Stissing",
    "corresponding_authors": "Thomas Mailund",
    "abstract": "Abstract Background A number of algorithms have been developed for calculating the quartet distance between two evolutionary trees on the same set of species. The quartet distance is the number of quartets – sub-trees induced by four leaves – that differs between the trees. Mostly, these algorithms are restricted to work on binary trees, but recently we have developed algorithms that work on trees of arbitrary degree. Results We present a fast algorithm for computing the quartet distance between trees of arbitrary degree. Given input trees T and T' , the algorithm runs in time O ( n + | V |·| V' | min{ id , id' }) and space O ( n + | V |·| V' |), where n is the number of leaves in the two trees, V and V are the non-leaf nodes in T and T' , respectively, and id and id' are the maximal number of non-leaf nodes adjacent to a non-leaf node in T and T' , respectively. The fastest algorithms previously published for arbitrary degree trees run in O ( n 3 ) (independent of the degree of the tree) and O (| V |·| V' |· id · id' ), respectively. We experimentally compare the algorithm with existing algorithms for computing the quartet distance for general trees. Conclusion We present a new algorithm for computing the quartet distance between two trees of arbitrary degree. The new algorithm improves the asymptotic running time for computing the quartet distance, compared to previous methods, and experimental results indicate that the new method also performs significantly better in practice.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2127063840",
    "type": "article"
  },
  {
    "title": "Mining, compressing and classifying with extensible motifs",
    "doi": "https://doi.org/10.1186/1748-7188-1-4",
    "publication_date": "2006-03-23",
    "publication_year": 2006,
    "authors": "Alberto Apostolico; Matteo Comin; Laxmi Parida",
    "corresponding_authors": "",
    "abstract": "Motif patterns of maximal saturation emerged originally in contexts of pattern discovery in biomolecular sequences and have recently proven a valuable notion also in the design of data compression schemes. Informally, a motif is a string of intermittently solid and wild characters that recurs more or less frequently in an input sequence or family of sequences. Motif discovery techniques and tools tend to be computationally imposing, however, special classes of \"rigid\" motifs have been identified of which the discovery is affordable in low polynomial time. In the present work, \"extensible\" motifs are considered such that each sequence of gaps comes endowed with some elasticity, whereby the same pattern may be stretched to fit segments of the source that match all the solid characters but are otherwise of different lengths. A few applications of this notion are then described. In applications of data compression by textual substitution, extensible motifs are seen to bring savings on the size of the codebook, and hence to improve compression. In germane contexts, in which compressibility is used in its dual role as a basis for structural inference and classification, extensible motifs are seen to support unsupervised classification and phylogeny reconstruction. Off-line compression based on extensible motifs can be used advantageously to compress and classify biological sequences.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2145494382",
    "type": "article"
  },
  {
    "title": "Reconstructing phylogenies from noisy quartets in polynomial time with a high success probability",
    "doi": "https://doi.org/10.1186/1748-7188-3-1",
    "publication_date": "2008-01-24",
    "publication_year": 2008,
    "authors": "Gang Wu; Ming‐Yang Kao; Guohui Lin; Jia-Huai You",
    "corresponding_authors": "",
    "abstract": "In recent years, quartet-based phylogeny reconstruction methods have received considerable attentions in the computational biology community. Traditionally, the accuracy of a phylogeny reconstruction method is measured by simulations on synthetic datasets with known \"true\" phylogenies, while little theoretical analysis has been done. In this paper, we present a new model-based approach to measuring the accuracy of a quartet-based phylogeny reconstruction method. Under this model, we propose three efficient algorithms to reconstruct the \"true\" phylogeny with a high success probability.The first algorithm can reconstruct the \"true\" phylogeny from the input quartet topology set without quartet errors in O(n2) time by querying at most (n - 4) log(n - 1) quartet topologies, where n is the number of the taxa. When the input quartet topology set contains errors, the second algorithm can reconstruct the \"true\" phylogeny with a probability approximately 1 - p in O(n4 log n) time, where p is the probability for a quartet topology being an error. This probability is improved by the third algorithm to approximately [equation; see text], where [equation, see text], with running time of O(n5), which is at least 0.984 when p < 0.05.The three proposed algorithms are mathematically guaranteed to reconstruct the \"true\" phylogeny with a high success probability. The experimental results showed that the third algorithm produced phylogenies with a higher probability than its aforementioned theoretical lower bound and outperformed some existing phylogeny reconstruction methods in both speed and accuracy.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2089039157",
    "type": "article"
  },
  {
    "title": "Auto-validating von Neumann rejection sampling from small phylogenetic tree spaces",
    "doi": "https://doi.org/10.1186/1748-7188-4-1",
    "publication_date": "2009-01-07",
    "publication_year": 2009,
    "authors": "Raazesh Sainudiin; A. York",
    "corresponding_authors": "",
    "abstract": "In phylogenetic inference one is interested in obtaining samples from the posterior distribution over the tree space on the basis of some observed DNA sequence data. One of the simplest sampling methods is the rejection sampler due to von Neumann. Here we introduce an auto-validating version of the rejection sampler, via interval analysis, to rigorously draw samples from posterior distributions over small phylogenetic tree spaces. The posterior samples from the auto-validating sampler are used to rigorously (i) estimate posterior probabilities for different rooted topologies based on mitochondrial DNA from human, chimpanzee and gorilla, (ii) conduct a non-parametric test of rate variation between protein-coding and tRNA-coding sites from three primates and (iii) obtain a posterior estimate of the human-neanderthal divergence time. This solves the open problem of rigorously drawing independent and identically distributed samples from the posterior distribution over rooted and unrooted small tree spaces (3 or 4 taxa) based on any multiply-aligned sequence data.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1988496827",
    "type": "article"
  },
  {
    "title": "A scoring matrix approach to detecting miRNA target sites",
    "doi": "https://doi.org/10.1186/1748-7188-3-3",
    "publication_date": "2008-03-31",
    "publication_year": 2008,
    "authors": "Simon Moxon; Vincent Moulton; Jan T. Kim",
    "corresponding_authors": "",
    "abstract": "Experimental identification of microRNA (miRNA) targets is a difficult and time consuming process. As a consequence several computational prediction methods have been devised in order to predict targets for follow up experimental validation. Current computational target prediction methods use only the miRNA sequence as input. With an increasing number of experimentally validated targets becoming available, utilising this additional information in the search for further targets may help to improve the specificity of computational methods for target site prediction.We introduce a generic target prediction method, the Stacking Binding Matrix (SBM) that uses both information about the miRNA as well as experimentally validated target sequences in the search for candidate target sequences. We demonstrate the utility of our method by applying it to both animal and plant data sets and compare it with miRanda, a commonly used target prediction method.We show that SBM can be applied to target prediction in both plants and animals and performs well in terms of sensitivity and specificity. Open source code implementing the SBM method, together with documentation and examples are freely available for download from the address in the Availability and Requirements section.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1994809794",
    "type": "article"
  },
  {
    "title": "Lossless filter for multiple repeats with bounded edit distance",
    "doi": "https://doi.org/10.1186/1748-7188-4-3",
    "publication_date": "2009-01-30",
    "publication_year": 2009,
    "authors": "Pierre Peterlongo; Gustavo Akio Tominaga Sacomoto; Alair Pereira do Lago; Nadia Pisanti; Marie-France Sagot",
    "corresponding_authors": "Pierre Peterlongo",
    "abstract": "Identifying local similarity between two or more sequences, or identifying repeats occurring at least twice in a sequence, is an essential part in the analysis of biological sequences and of their phylogenetic relationship. Finding such fragments while allowing for a certain number of insertions, deletions, and substitutions, is however known to be a computationally expensive task, and consequently exact methods can usually not be applied in practice.The filter TUIUIU that we introduce in this paper provides a possible solution to this problem. It can be used as a preprocessing step to any multiple alignment or repeats inference method, eliminating a possibly large fraction of the input that is guaranteed not to contain any approximate repeat. It consists in the verification of several strong necessary conditions that can be checked in a fast way. We implemented three versions of the filter. The first is simply a straightforward extension to the case of multiple sequences of an application of conditions already existing in the literature. The second uses a stronger condition which, as our results show, enable to filter sensibly more with negligible (if any) additional time. The third version uses an additional condition and pushes the sensibility of the filter even further with a non negligible additional time in many circumstances; our experiments show that it is particularly useful with large error rates. The latter version was applied as a preprocessing of a multiple alignment tool, obtaining an overall time (filter plus alignment) on average 63 and at best 530 times smaller than before (direct alignment), with in most cases a better quality alignment.To the best of our knowledge, TUIUIU is the first filter designed for multiple repeats and for dealing with error rates greater than 10% of the repeats length.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2066846592",
    "type": "article"
  },
  {
    "title": "Optimal selection of epitopes for TXP-immunoaffinity mass spectrometry",
    "doi": "https://doi.org/10.1186/1748-7188-5-28",
    "publication_date": "2010-06-25",
    "publication_year": 2010,
    "authors": "Hannes Planatscher; Jochen Supper; Oliver Poetz; Dieter Stoll; Thomas Joos; Markus F. Templin; Andreas Zell",
    "corresponding_authors": "",
    "abstract": "Mass spectrometry (MS) based protein profiling has become one of the key technologies in biomedical research and biomarker discovery. One bottleneck in MS-based protein analysis is sample preparation and an efficient fractionation step to reduce the complexity of the biological samples, which are too complex to be analyzed directly with MS. Sample preparation strategies that reduce the complexity of tryptic digests by using immunoaffinity based methods have shown to lead to a substantial increase in throughput and sensitivity in the proteomic mass spectrometry approach. The limitation of using such immunoaffinity-based approaches is the availability of the appropriate peptide specific capture antibodies. Recent developments in these approaches, where subsets of peptides with short identical terminal sequences can be enriched using antibodies directed against short terminal epitopes, promise a significant gain in efficiency.We show that the minimal set of terminal epitopes for the coverage of a target protein list can be found by the formulation as a set cover problem, preceded by a filtering pipeline for the exclusion of peptides and target epitopes with undesirable properties.For small datasets (a few hundred proteins) it is possible to solve the problem to optimality with moderate computational effort using commercial or free solvers. Larger datasets, like full proteomes require the use of heuristics.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2098420351",
    "type": "article"
  },
  {
    "title": "Bootstrapping phylogenies inferred from rearrangement data",
    "doi": "https://doi.org/10.1186/1748-7188-7-21",
    "publication_date": "2012-08-29",
    "publication_year": 2012,
    "authors": "Yu Lin; Vaibhav Rajan; Bernard M. E. Moret",
    "corresponding_authors": "",
    "abstract": "Abstract Background Large-scale sequencing of genomes has enabled the inference of phylogenies based on the evolution of genomic architecture, under such events as rearrangements, duplications, and losses. Many evolutionary models and associated algorithms have been designed over the last few years and have found use in comparative genomics and phylogenetic inference. However, the assessment of phylogenies built from such data has not been properly addressed to date. The standard method used in sequence-based phylogenetic inference is the bootstrap, but it relies on a large number of homologous characters that can be resampled; yet in the case of rearrangements, the entire genome is a single character. Alternatives such as the jackknife suffer from the same problem, while likelihood tests cannot be applied in the absence of well established probabilistic models. Results We present a new approach to the assessment of distance-based phylogenetic inference from whole-genome data; our approach combines features of the jackknife and the bootstrap and remains nonparametric. For each feature of our method, we give an equivalent feature in the sequence-based framework; we also present the results of extensive experimental testing, in both sequence-based and genome-based frameworks. Through the feature-by-feature comparison and the experimental results, we show that our bootstrapping approach is on par with the classic phylogenetic bootstrap used in sequence-based reconstruction, and we establish the clear superiority of the classic bootstrap for sequence data and of our corresponding new approach for rearrangement data over proposed variants. Finally, we test our approach on a small dataset of mammalian genomes, verifying that the support values match current thinking about the respective branches. Conclusions Our method is the first to provide a standard of assessment to match that of the classic phylogenetic bootstrap for aligned sequences. Its support values follow a similar scale and its receiver-operating characteristics are nearly identical, indicating that it provides similar levels of sensitivity and specificity. Thus our assessment method makes it possible to conduct phylogenetic analyses on whole genomes with the same degree of confidence as for analyses on aligned sequences. Extensions to search-based inference methods such as maximum parsimony and maximum likelihood are possible, but remain to be thoroughly tested.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2124321327",
    "type": "article"
  },
  {
    "title": "Invariant based quartet puzzling",
    "doi": "https://doi.org/10.1186/1748-7188-7-35",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Joseph Rusinko; Brian Hipp",
    "corresponding_authors": "",
    "abstract": "First proposed by Cavender and Felsenstein, and Lake, invariant based algorithms for phylogenetic reconstruction were widely dismissed by practicing biologists because invariants were perceived to have limited accuracy in constructing trees based on DNA sequences of reasonable length. Recent developments by algebraic geometers have led to the construction of lists of invariants which have been demonstrated to be more accurate on small sequences, but were limited in that they could only be used for trees with small numbers of taxa. We have developed and tested an invariant based quartet puzzling algorithm which is accurate and efficient for biologically reasonable data sets.We found that our algorithm outperforms Maximum Likelihood based quartet puzzling on data sets simulated with low to medium evolutionary rates. For faster rates of evolution, invariant based quartet puzzling is reasonable but less effective than maximum likelihood based puzzling.This is a proof of concept algorithm which is not intended to replace existing reconstruction algorithms. Rather, the conclusion is that when seeking solutions to a new wave of phylogenetic problems (super tree algorithms, gene vs. species tree, mixture models), invariant based methods should be considered. This article demonstrates that invariants are a practical, reasonable and flexible source for reconstruction techniques.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2126955332",
    "type": "article"
  },
  {
    "title": "Resolving spatial inconsistencies in chromosome conformation measurements",
    "doi": "https://doi.org/10.1186/1748-7188-8-8",
    "publication_date": "2013-03-09",
    "publication_year": 2013,
    "authors": "Geet Duggal; Rob Patro; Emre Sefer; Hao Wang; Darya Filippova; Samir Khuller; Carl Kingsford",
    "corresponding_authors": "",
    "abstract": "Chromosome structure is closely related to its function and Chromosome Conformation Capture (3C) is a widely used technique for exploring spatial properties of chromosomes. 3C interaction frequencies are usually associated with spatial distances. However, the raw data from 3C experiments is an aggregation of interactions from many cells, and the spatial distances of any given interaction are uncertain.We introduce a new method for filtering 3C interactions that selects subsets of interactions that obey metric constraints of various strictness. We demonstrate that, although the problem is computationally hard, near-optimal results are often attainable in practice using well-designed heuristics and approximation algorithms. Further, we show that, compared with a standard technique, this metric filtering approach leads to (a) subgraphs with higher statistical significance, (b) lower embedding error, (c) lower sensitivity to initial conditions of the embedding algorithm, and (d) structures with better agreement with light microscopy measurements. Our filtering scheme is applicable for a strict frequency-to-distance mapping and a more relaxed mapping from frequency to a range of distances.Our filtering method for 3C data considers both metric consistency and statistical confidence simultaneously resulting in lower-error embeddings that are biologically more plausible.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2134695249",
    "type": "article"
  },
  {
    "title": "Parsimonious reconstruction of network evolution",
    "doi": "https://doi.org/10.1186/1748-7188-7-25",
    "publication_date": "2012-09-19",
    "publication_year": 2012,
    "authors": "Rob Patro; Emre Sefer; Justin Malin; Guillaume Marçais; Saket Navlakha; Carl Kingsford",
    "corresponding_authors": "Rob Patro",
    "abstract": "Understanding the evolution of biological networks can provide insight into how their modular structure arises and how they are affected by environmental changes. One approach to studying the evolution of these networks is to reconstruct plausible common ancestors of present-day networks, allowing us to analyze how the topological properties change over time and to posit mechanisms that drive the networks' evolution. Further, putative ancestral networks can be used to help solve other difficult problems in computational biology, such as network alignment.We introduce a combinatorial framework for encoding network histories, and we give a fast procedure that, given a set of gene duplication histories, in practice finds network histories with close to the minimum number of interaction gain or loss events to explain the observed present-day networks. In contrast to previous studies, our method does not require knowing the relative ordering of unrelated duplication events. Results on simulated histories and real biological networks both suggest that common ancestral networks can be accurately reconstructed using this parsimony approach. A software package implementing our method is available under the Apache 2.0 license at http://cbcb.umd.edu/kingsford-group/parana.Our parsimony-based approach to ancestral network reconstruction is both efficient and accurate. We show that considering a larger set of potential ancestral interactions by not assuming a relative ordering of unrelated duplication events can lead to improved ancestral network inference.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2149367002",
    "type": "article"
  },
  {
    "title": "A priori assessment of data quality in molecular phylogenetics",
    "doi": "https://doi.org/10.1186/s13015-014-0022-4",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Bernhard Misof; Karen Meusemann; Björn M. von Reumont; Patrick Kück; Sonja J. Prohaska; Peter F. Stadler",
    "corresponding_authors": "",
    "abstract": "Sets of sequence data used in phylogenetic analysis are often plagued by both random noise and systematic biases. Since the commonly used methods of phylogenetic reconstruction are designed to produce trees it is an important task to evaluate these trees a posteriori. Preferably, however, one would like to assess the suitability of the input data for phylogenetic analysis a priori and, if possible, obtain information on how to prune the data sets to improve the quality of phylogenetic reconstruction without introducing unwarranted biases. In the last few years several different approaches, algorithms, and software tools have been proposed for this purpose. Here we provide an overview of the state of the art and briefly discuss the most pressing open problems.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1989914127",
    "type": "article"
  },
  {
    "title": "A sub-cubic time algorithm for computing the quartet distance between two general trees",
    "doi": "https://doi.org/10.1186/1748-7188-6-15",
    "publication_date": "2011-06-03",
    "publication_year": 2011,
    "authors": "Jesper Kresten Nielsen; Anders K. Kristensen; Thomas Mailund; Christian NS Pedersen",
    "corresponding_authors": "Jesper Kresten Nielsen",
    "abstract": "Background When inferring phylogenetic trees different algorithms may give different trees. To study such effects a measure for the distance between two trees is useful. Quartet distance is one such measure, and is the number of quartet topologies that differ between two trees. Results We have derived a new algorithm for computing the quartet distance between a pair of general trees, i.e. trees where inner nodes can have any degree ≥ 3. The time and space complexity of our algorithm is sub-cubic in the number of leaves and does not depend on the degree of the inner nodes. This makes it the fastest algorithm so far for computing the quartet distance between general trees independent of the degree of the inner nodes. Conclusions We have implemented our algorithm and two of the best competitors. Our new algorithm is significantly faster than the competition and seems to run in close to quadratic time in practice.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2009222636",
    "type": "article"
  },
  {
    "title": "LocARNAscan: Incorporating thermodynamic stability in sequence and structure-based RNA homology search",
    "doi": "https://doi.org/10.1186/1748-7188-8-14",
    "publication_date": "2013-04-20",
    "publication_year": 2013,
    "authors": "Sebastian Will; Michael Siebauer; Steffen Heyne; Jan Engelhardt; Peter F. Stadler; Kristin Reiche; Rolf Backofen",
    "corresponding_authors": "Peter F. Stadler; Kristin Reiche; Rolf Backofen",
    "abstract": "The search for distant homologs has become an import issue in genome annotation. A particular difficulty is posed by divergent homologs that have lost recognizable sequence similarity. This same problem also arises in the recognition of novel members of large classes of RNAs such as snoRNAs or microRNAs that consist of families unrelated by common descent. Current homology search tools for structured RNAs are either based entirely on sequence similarity (such as blast or hmmer) or combine sequence and secondary structure. The most prominent example of the latter class of tools is Infernal. Alternatives are descriptor-based methods. In most practical applications published to-date, however, the information contained in covariance models or manually prescribed search patterns is dominated by sequence information. Here we ask two related questions: (1) Is secondary structure alone informative for homology search and the detection of novel members of RNA classes? (2) To what extent is the thermodynamic propensity of the target sequence to fold into the correct secondary structure helpful for this task?Sequence-structure alignment can be used as an alternative search strategy. In this scenario, the query consists of a base pairing probability matrix, which can be derived either from a single sequence or from a multiple alignment representing a set of known representatives. Sequence information can be optionally added to the query. The target sequence is pre-processed to obtain local base pairing probabilities. As a search engine we devised a semi-global scanning variant of LocARNA's algorithm for sequence-structure alignment. The LocARNAscan tool is optimized for speed and low memory consumption. In benchmarking experiments on artificial data we observe that the inclusion of thermodynamic stability is helpful, albeit only in a regime of extremely low sequence information in the query. We observe, furthermore, that the sensitivity is bounded in particular by the limited accuracy of the predicted local structures of the target sequence.Although we demonstrate that a purely structure-based homology search is feasible in principle, it is unlikely to outperform tools such as Infernal in most application scenarios, where a substantial amount of sequence information is typically available. The LocARNAscan approach will profit, however, from high throughput methods to determine RNA secondary structure. In transcriptome-wide applications, such methods will provide accurate structure annotations on the target side.Source code of the free software LocARNAscan 1.0 and supplementary data are available at http://www.bioinf.uni-leipzig.de/Software/LocARNAscan.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2161534813",
    "type": "article"
  },
  {
    "title": "A polynomial time algorithm for calculating the probability of a ranked gene tree given a species tree",
    "doi": "https://doi.org/10.1186/1748-7188-7-7",
    "publication_date": "2012-04-30",
    "publication_year": 2012,
    "authors": "Tanja Stadler; J. H. Degnan",
    "corresponding_authors": "Tanja Stadler",
    "abstract": "The ancestries of genes form gene trees which do not necessarily have the same topology as the species tree due to incomplete lineage sorting. Available algorithms determining the probability of a gene tree given a species tree require exponential computational runtime. In this paper, we provide a polynomial time algorithm to calculate the probability of a ranked gene tree topology for a given species tree, where a ranked tree topology is a tree topology with the internal vertices being ordered. The probability of a gene tree topology can thus be calculated in polynomial time if the number of orderings of the internal vertices is a polynomial number. However, the complexity of calculating the probability of a gene tree topology with an exponential number of rankings for a given species tree remains unknown. Polynomial algorithms for calculating ranked gene tree probabilities may become useful in developing methodology to infer species trees based on a collection of gene trees, leading to a more accurate reconstruction of ancestral species relationships.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2163239648",
    "type": "article"
  },
  {
    "title": "ASP-based method for the enumeration of attractors in non-deterministic synchronous and asynchronous multi-valued networks",
    "doi": "https://doi.org/10.1186/s13015-017-0111-2",
    "publication_date": "2017-08-15",
    "publication_year": 2017,
    "authors": "Emna Ben Abdallah; Maxime Folschette; Olivier Roux; Morgan Magnin",
    "corresponding_authors": "",
    "abstract": "This paper addresses the problem of finding attractors in biological regulatory networks. We focus here on non-deterministic synchronous and asynchronous multi-valued networks, modeled using automata networks (AN). AN is a general and well-suited formalism to study complex interactions between different components (genes, proteins,...). An attractor is a minimal trap domain, that is, a part of the state-transition graph that cannot be escaped. Such structures are terminal components of the dynamics and take the form of steady states (singleton) or complex compositions of cycles (non-singleton). Studying the effect of a disease or a mutation on an organism requires finding the attractors in the model to understand the long-term behaviors. We present a computational logical method based on answer set programming (ASP) to identify all attractors. Performed without any network reduction, the method can be applied on any dynamical semantics. In this paper, we present the two most widespread non-deterministic semantics: the asynchronous and the synchronous updating modes. The logical approach goes through a complete enumeration of the states of the network in order to find the attractors without the necessity to construct the whole state-transition graph. We realize extensive computational experiments which show good performance and fit the expected theoretical results in the literature. The originality of our approach lies on the exhaustive enumeration of all possible (sets of) states verifying the properties of an attractor thanks to the use of ASP. Our method is applied to non-deterministic semantics in two different schemes (asynchronous and synchronous). The merits of our methods are illustrated by applying them to biological examples of various sizes and comparing the results with some existing approaches. It turns out that our approach succeeds to exhaustively enumerate on a desktop computer, in a large model (100 components), all existing attractors up to a given size (20 states). This size is only limited by memory and computation time.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2748594262",
    "type": "article"
  },
  {
    "title": "A reconstruction problem for a class of phylogenetic networks with lateral gene transfers",
    "doi": "https://doi.org/10.1186/s13015-015-0059-z",
    "publication_date": "2015-12-01",
    "publication_year": 2015,
    "authors": "Gabriel Cardona; Joan Carles Pons; Francesc Rosselló",
    "corresponding_authors": "",
    "abstract": "Lateral, or Horizontal, Gene Transfers are a type of asymmetric evolutionary events where genetic material is transferred from one species to another. In this paper we consider LGT networks, a general model of phylogenetic networks with lateral gene transfers which consist, roughly, of a principal rooted tree with its leaves labelled on a set of taxa, and a set of extra secondary arcs between nodes in this tree representing lateral gene transfers. An LGT network gives rise in a natural way to a principal phylogenetic subtree and a set of secondary phylogenetic subtrees, which, roughly, represent, respectively, the main line of evolution of most genes and the secondary lines of evolution through lateral gene transfers.We introduce a set of simple conditions on an LGT network that guarantee that its principal and secondary phylogenetic subtrees are pairwise different and that these subtrees determine, up to isomorphism, the LGT network. We then give an algorithm that, given a set of pairwise different phylogenetic trees [Formula: see text] on the same set of taxa, outputs, when it exists, the LGT network that satisfies these conditions and such that its principal phylogenetic tree is [Formula: see text] and its secondary phylogenetic trees are [Formula: see text].",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2184238867",
    "type": "article"
  },
  {
    "title": "Approximating the DCJ distance of balanced genomes in linear time",
    "doi": "https://doi.org/10.1186/s13015-017-0095-y",
    "publication_date": "2017-03-09",
    "publication_year": 2017,
    "authors": "Diego P. Rubert; Pedro Feijão; Marília D. V. Braga; Jens Stoye; Fábio V. Martinez",
    "corresponding_authors": "",
    "abstract": "Rearrangements are large-scale mutations in genomes, responsible for complex changes and structural variations. Most rearrangements that modify the organization of a genome can be represented by the double cut and join (DCJ) operation. Given two balanced genomes, i.e., two genomes that have exactly the same number of occurrences of each gene in each genome, we are interested in the problem of computing the rearrangement distance between them, i.e., finding the minimum number of DCJ operations that transform one genome into the other. This problem is known to be NP-hard.We propose a linear time approximation algorithm with approximation factor O(k) for the DCJ distance problem, where k is the maximum number of occurrences of any gene in the input genomes. Our algorithm works for linear and circular unichromosomal balanced genomes and uses as an intermediate step an O(k)-approximation for the minimum common string partition problem, which is closely related to the DCJ distance problem.Experiments on simulated data sets show that our approximation algorithm is very competitive both in efficiency and in quality of the solutions.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2593661024",
    "type": "article"
  },
  {
    "title": "OCTAL: Optimal Completion of gene trees in polynomial time",
    "doi": "https://doi.org/10.1186/s13015-018-0124-5",
    "publication_date": "2018-03-15",
    "publication_year": 2018,
    "authors": "Sarah Christensen; Erin K. Molloy; Pranjal Vachaspati; Tandy Warnow",
    "corresponding_authors": "Sarah Christensen",
    "abstract": "For a combination of reasons (including data generation protocols, approaches to taxon and gene sampling, and gene birth and loss), estimated gene trees are often incomplete, meaning that they do not contain all of the species of interest. As incomplete gene trees can impact downstream analyses, accurate completion of gene trees is desirable.We introduce the Optimal Tree Completion problem, a general optimization problem that involves completing an unrooted binary tree (i.e., adding missing leaves) so as to minimize its distance from a reference tree on a superset of the leaves. We present OCTAL, an algorithm that finds an optimal solution to this problem when the distance between trees is defined using the Robinson-Foulds (RF) distance, and we prove that OCTAL runs in [Formula: see text] time, where n is the total number of species. We report on a simulation study in which gene trees can differ from the species tree due to incomplete lineage sorting, and estimated gene trees are completed using OCTAL with a reference tree based on a species tree estimated from the multi-locus dataset. OCTAL produces completed gene trees that are closer to the true gene trees than an existing heuristic approach in ASTRAL-II, but the accuracy of a completed gene tree computed by OCTAL depends on how topologically similar the reference tree (typically an estimated species tree) is to the true gene tree.OCTAL is a useful technique for adding missing taxa to incomplete gene trees and provides good accuracy under a wide range of model conditions. However, results show that OCTAL's accuracy can be reduced when incomplete lineage sorting is high, as the reference tree can be far from the true gene tree. Hence, this study suggests that OCTAL would benefit from using other types of reference trees instead of species trees when there are large topological distances between true gene trees and species trees.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2799249604",
    "type": "article"
  },
  {
    "title": "Split-inducing indels in phylogenomic analysis",
    "doi": "https://doi.org/10.1186/s13015-018-0130-7",
    "publication_date": "2018-07-16",
    "publication_year": 2018,
    "authors": "Alexander Donath; Peter F. Stadler",
    "corresponding_authors": "Alexander Donath",
    "abstract": "Most phylogenetic studies using molecular data treat gaps in multiple sequence alignments as missing data or even completely exclude alignment columns that contain gaps. Here we show that gap patterns in large-scale, genome-wide alignments are themselves phylogenetically informative and can be used to infer reliable phylogenies provided the gap data are properly filtered to reduce noise introduced by the alignment method. We introduce here the notion of split-inducing indels (splids) that define an approximate bipartition of the taxon set. We show both in simulated data and in case studies on real-life data that splids can be efficiently extracted from phylogenomic data sets. Suitably processed gap patterns extracted from genome-wide alignment provide a surprisingly clear phylogenetic signal and an allow the inference of accurate phylogenetic trees.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2883371511",
    "type": "article"
  },
  {
    "title": "Semi-nonparametric modeling of topological domain formation from epigenetic data",
    "doi": "https://doi.org/10.1186/s13015-019-0142-y",
    "publication_date": "2019-03-05",
    "publication_year": 2019,
    "authors": "Emre Sefer; Carl Kingsford",
    "corresponding_authors": "Emre Sefer",
    "abstract": "Hi-C experiments capturing the 3D genome architecture have led to the discovery of topologically-associated domains (TADs) that form an important part of the 3D genome organization and appear to play a role in gene regulation and other functions. Several histone modifications have been independently associated with TAD formation, but their combinatorial effects on domain formation remain poorly understood at a global scale.We propose a convex semi-nonparametric approach called nTDP based on Bernstein polynomials to explore the joint effects of histone markers on TAD formation as well as predict TADs solely from the histone data. We find a small subset of modifications to be predictive of TADs across species. By inferring TADs using our trained model, we are able to predict TADs across different species and cell types, without the use of Hi-C data, suggesting their effect is conserved. This work provides the first comprehensive joint model of the effect of histone markers on domain formation.Our approach, nTDP, can form the basis of a unified, explanatory model of the relationship between epigenetic marks and topological domain structures. It can be used to predict domain boundaries for cell types, species, and conditions for which no Hi-C data is available. The model may also be of use for improving Hi-C-based domain finders.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2924256311",
    "type": "article"
  },
  {
    "title": "Evolution through segmental duplications and losses: a Super-Reconciliation approach",
    "doi": "https://doi.org/10.1186/s13015-020-00171-4",
    "publication_date": "2020-05-26",
    "publication_year": 2020,
    "authors": "Mattéo Delabre; Nadia El-Mabrouk; Katharina T. Huber; Manuel Lafond; Vincent Moulton; Emmanuel Noutahi; Miguel Sautié Castellanos",
    "corresponding_authors": "",
    "abstract": "The classical gene and species tree reconciliation, used to infer the history of gene gain and loss explaining the evolution of gene families, assumes an independent evolution for each family. While this assumption is reasonable for genes that are far apart in the genome, it is not appropriate for genes grouped into syntenic blocks, which are more plausibly the result of a concerted evolution. Here, we introduce the Super-Reconciliation problem which consists in inferring a history of segmental duplication and loss events (involving a set of neighboring genes) leading to a set of present-day syntenies from a single ancestral one. In other words, we extend the traditional Duplication-Loss reconciliation problem of a single gene tree, to a set of trees, accounting for segmental duplications and losses. Existency of a Super-Reconciliation depends on individual gene tree consistency. In addition, ignoring rearrangements implies that existency also depends on gene order consistency. We first show that the problem of reconstructing a most parsimonious Super-Reconciliation, if any, is NP-hard and give an exact exponential-time algorithm to solve it. Alternatively, we show that accounting for rearrangements in the evolutionary model, but still only minimizing segmental duplication and loss events, leads to an exact polynomial-time algorithm. We finally assess time efficiency of the former exponential time algorithm for the Duplication-Loss model on simulated datasets, and give a proof of concept on the opioid receptor genes.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2976303698",
    "type": "article"
  },
  {
    "title": "gsufsort: constructing suffix arrays, LCP arrays and BWTs for string collections",
    "doi": "https://doi.org/10.1186/s13015-020-00177-y",
    "publication_date": "2020-09-22",
    "publication_year": 2020,
    "authors": "Felipe A. Louza; Guilherme P. Telles; Simon Gog; Nicola Prezza; Giovanna Rosone",
    "corresponding_authors": "Felipe A. Louza; Giovanna Rosone",
    "abstract": "The construction of a suffix array for a collection of strings is a fundamental task in Bioinformatics and in many other applications that process strings. Related data structures, as the Longest Common Prefix array, the Burrows-Wheeler transform, and the document array, are often needed to accompany the suffix array to efficiently solve a wide variety of problems. While several algorithms have been proposed to construct the suffix array for a single string, less emphasis has been put on algorithms to construct suffix arrays for string collections.In this paper we introduce gsufsort, an open source software for constructing the suffix array and related data indexing structures for a string collection with N symbols in O(N) time. Our tool is written in ANSI/C and is based on the algorithm gSACA-K (Louza et al. in Theor Comput Sci 678:22-39, 2017), the fastest algorithm to construct suffix arrays for string collections. The tool supports large fasta, fastq and text files with multiple strings as input. Experiments have shown very good performance on different types of strings.gsufsort is a fast, portable, and lightweight tool for constructing the suffix array and additional data structures for string collections.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3089043377",
    "type": "article"
  },
  {
    "title": "Models and algorithms for genome rearrangement with positional constraints",
    "doi": "https://doi.org/10.1186/s13015-016-0065-9",
    "publication_date": "2016-05-17",
    "publication_year": 2016,
    "authors": "Krister M. Swenson; Pijus Simonaitis; Mathieu Blanchette",
    "corresponding_authors": "Krister M. Swenson",
    "abstract": "Traditionally, the merit of a rearrangement scenario between two gene orders has been measured based on a parsimony criteria alone; two scenarios with the same number of rearrangements are considered equally good. In this paper, we acknowledge that each rearrangement has a certain likelihood of occurring based on biological constraints, e.g. physical proximity of the DNA segments implicated or repetitive sequences. We propose optimization problems with the objective of maximizing overall likelihood, by weighting the rearrangements. We study a binary weight function suitable to the representation of sets of genome positions that are most likely to have swapped adjacencies. We give a polynomial-time algorithm for the problem of finding a minimum weight double cut and join scenario among all minimum length scenarios. In the process we solve an optimization problem on colored noncrossing partitions, which is a generalization of the Maximum Independent Set problem on circle graphs. We introduce a model for weighting genome rearrangements and show that under simple yet reasonable conditions, a fundamental distance can be computed in polynomial time. This is achieved by solving a generalization of the Maximum Independent Set problem on circle graphs. Several variants of the problem are also mentioned.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2405206615",
    "type": "article"
  },
  {
    "title": "Disk compression of k-mer sets",
    "doi": "https://doi.org/10.1186/s13015-021-00192-7",
    "publication_date": "2021-06-21",
    "publication_year": 2021,
    "authors": "Amatur Rahman; Rayan Chikhi; Paul Medvedev",
    "corresponding_authors": "",
    "abstract": "Abstract K-mer based methods have become prevalent in many areas of bioinformatics. In applications such as database search, they often work with large multi-terabyte-sized datasets. Storing such large datasets is a detriment to tool developers, tool users, and reproducibility efforts. General purpose compressors like gzip, or those designed for read data, are sub-optimal because they do not take into account the specific redundancy pattern in k-mer sets. In our earlier work (Rahman and Medvedev, RECOMB 2020), we presented an algorithm UST-Compress that uses a spectrum-preserving string set representation to compress a set of k-mers to disk. In this paper, we present two improved methods for disk compression of k-mer sets, called ESS-Compress and ESS-Tip-Compress. They use a more relaxed notion of string set representation to further remove redundancy from the representation of UST-Compress. We explore their behavior both theoretically and on real data. We show that they improve the compression sizes achieved by UST-Compress by up to 27 percent, across a breadth of datasets. We also derive lower bounds on how well this type of compression strategy can hope to do.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3175899557",
    "type": "article"
  },
  {
    "title": "INGOT-DR: an interpretable classifier for predicting drug resistance in M. tuberculosis",
    "doi": "https://doi.org/10.1186/s13015-021-00198-1",
    "publication_date": "2021-08-10",
    "publication_year": 2021,
    "authors": "Hooman Zabeti; Nick Dexter; Amir Safari; Nafiseh Sedaghat; Maxwell W. Libbrecht; Leonid Chindelevitch",
    "corresponding_authors": "Hooman Zabeti",
    "abstract": "Prediction of drug resistance and identification of its mechanisms in bacteria such as Mycobacterium tuberculosis, the etiological agent of tuberculosis, is a challenging problem. Solving this problem requires a transparent, accurate, and flexible predictive model. The methods currently used for this purpose rarely satisfy all of these criteria. On the one hand, approaches based on testing strains against a catalogue of previously identified mutations often yield poor predictive performance; on the other hand, machine learning techniques typically have higher predictive accuracy, but often lack interpretability and may learn patterns that produce accurate predictions for the wrong reasons. Current interpretable methods may either exhibit a lower accuracy or lack the flexibility needed to generalize them to previously unseen data.In this paper we propose a novel technique, inspired by group testing and Boolean compressed sensing, which yields highly accurate predictions, interpretable results, and is flexible enough to be optimized for various evaluation metrics at the same time.We test the predictive accuracy of our approach on five first-line and seven second-line antibiotics used for treating tuberculosis. We find that it has a higher or comparable accuracy to that of commonly used machine learning models, and is able to identify variants in genes with previously reported association to drug resistance. Our method is intrinsically interpretable, and can be customized for different evaluation metrics. Our implementation is available at github.com/hoomanzabeti/INGOT_DR and can be installed via The Python Package Index (Pypi) under ingotdr. This package is also compatible with most of the tools in the Scikit-learn machine learning library.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3192846055",
    "type": "article"
  },
  {
    "title": "A novel method for inference of acyclic chemical compounds with bounded branch-height based on artificial neural networks and integer programming",
    "doi": "https://doi.org/10.1186/s13015-021-00197-2",
    "publication_date": "2021-08-14",
    "publication_year": 2021,
    "authors": "Naveed Ahmed Azam; Jianshen Zhu; Yanming Sun; Yu Shi; Aleksandar Shurbevski; Liang Zhao; Hiroshi Nagamochi; Tatsuya Akutsu",
    "corresponding_authors": "Tatsuya Akutsu",
    "abstract": "Abstract Analysis of chemical graphs is becoming a major research topic in computational molecular biology due to its potential applications to drug design. One of the major approaches in such a study is inverse quantitative structure activity/property relationship (inverse QSAR/QSPR) analysis, which is to infer chemical structures from given chemical activities/properties. Recently, a novel two-phase framework has been proposed for inverse QSAR/QSPR, where in the first phase an artificial neural network (ANN) is used to construct a prediction function. In the second phase, a mixed integer linear program (MILP) formulated on the trained ANN and a graph search algorithm are used to infer desired chemical structures. The framework has been applied to the case of chemical compounds with cycle index up to 2 so far. The computational results conducted on instances with n non-hydrogen atoms show that a feature vector can be inferred by solving an MILP for up to $$n=40$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math> , whereas graphs can be enumerated for up to $$n=15$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math> . When applied to the case of chemical acyclic graphs, the maximum computable diameter of a chemical structure was up to 8. In this paper, we introduce a new characterization of graph structure, called “branch-height” based on which a new MILP formulation and a new graph search algorithm are designed for chemical acyclic graphs. The results of computational experiments using such chemical properties as octanol/water partition coefficient, boiling point and heat of combustion suggest that the proposed method can infer chemical acyclic graphs with around $$n=50$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math> and diameter 30.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3194348808",
    "type": "article"
  },
  {
    "title": "Space-efficient representation of genomic k-mer count tables",
    "doi": "https://doi.org/10.1186/s13015-022-00212-0",
    "publication_date": "2022-03-21",
    "publication_year": 2022,
    "authors": "Yoshihiro Shibuya; Djamal Belazzougui; Grégory Kucherov",
    "corresponding_authors": "Grégory Kucherov",
    "abstract": "k-mer counting is a common task in bioinformatic pipelines, with many dedicated tools available. Many of these tools produce in output k-mer count tables containing both k-mers and counts, easily reaching tens of GB. Furthermore, such tables do not support efficient random-access queries in general.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4221076494",
    "type": "article"
  },
  {
    "title": "Pangenomic genotyping with the marker array",
    "doi": "https://doi.org/10.1186/s13015-023-00225-3",
    "publication_date": "2023-05-05",
    "publication_year": 2023,
    "authors": "Taher Mun; Kavya Vaddadi; Ben Langmead",
    "corresponding_authors": "",
    "abstract": "We present a new method and software tool called rowbowt that applies a pangenome index to the problem of inferring genotypes from short-read sequencing data. The method uses a novel indexing structure called the marker array. Using the marker array, we can genotype variants with respect from large panels like the 1000 Genomes Project while reducing the reference bias that results when aligning to a single linear reference. rowbowt can infer accurate genotypes in less time and memory compared to existing graph-based methods. The method is implemented in the open source software tool rowbowt available at https://github.com/alshai/rowbowt .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4372333204",
    "type": "article"
  },
  {
    "title": "Constructing phylogenetic networks via cherry picking and machine learning",
    "doi": "https://doi.org/10.1186/s13015-023-00233-3",
    "publication_date": "2023-09-16",
    "publication_year": 2023,
    "authors": "Giulia Bernardini; Leo van Iersel; Esther Julien; Leen Stougie",
    "corresponding_authors": "",
    "abstract": "Combining a set of phylogenetic trees into a single phylogenetic network that explains all of them is a fundamental challenge in evolutionary studies. Existing methods are computationally expensive and can either handle only small numbers of phylogenetic trees or are limited to severely restricted classes of networks.In this paper, we apply the recently-introduced theoretical framework of cherry picking to design a class of efficient heuristics that are guaranteed to produce a network containing each of the input trees, for practical-size datasets consisting of binary trees. Some of the heuristics in this framework are based on the design and training of a machine learning model that captures essential information on the structure of the input trees and guides the algorithms towards better solutions. We also propose simple and fast randomised heuristics that prove to be very effective when run multiple times.Unlike the existing exact methods, our heuristics are applicable to datasets of practical size, and the experimental study we conducted on both simulated and real data shows that these solutions are qualitatively good, always within some small constant factor from the optimum. Moreover, our machine-learned heuristics are one of the first applications of machine learning to phylogenetics and show its promise.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4386800720",
    "type": "article"
  },
  {
    "title": "Investigating the complexity of the double distance problems",
    "doi": "https://doi.org/10.1186/s13015-023-00246-y",
    "publication_date": "2024-01-04",
    "publication_year": 2024,
    "authors": "Marília D. V. Braga; Leonie R. Brockmann; Katharina Klerx; Jens Stoye",
    "corresponding_authors": "",
    "abstract": "Abstract Background Two genomes $$\\mathbb {A}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>A</mml:mi> </mml:math> and $$\\mathbb {B}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>B</mml:mi> </mml:math> over the same set of gene families form a canonical pair when each of them has exactly one gene from each family. Denote by $$n_*$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msub> <mml:mi>n</mml:mi> <mml:mrow> <mml:mrow/> <mml:mo>∗</mml:mo> </mml:mrow> </mml:msub> </mml:math> the number of common families of $$\\mathbb {A}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>A</mml:mi> </mml:math> and $$\\mathbb {B}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>B</mml:mi> </mml:math> . Different distances of canonical genomes can be derived from a structure called breakpoint graph , which represents the relation between the two given genomes as a collection of cycles of even length and paths. Let $$c_i$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msub> <mml:mi>c</mml:mi> <mml:mi>i</mml:mi> </mml:msub> </mml:math> and $$p_j$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msub> <mml:mi>p</mml:mi> <mml:mi>j</mml:mi> </mml:msub> </mml:math> be respectively the numbers of cycles of length i and of paths of length j in the breakpoint graph of genomes $$\\mathbb {A}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>A</mml:mi> </mml:math> and $$\\mathbb {B}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>B</mml:mi> </mml:math> . Then, the breakpoint distance of $$\\mathbb {A}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>A</mml:mi> </mml:math> and $$\\mathbb {B}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>B</mml:mi> </mml:math> is equal to $$n_*-\\left( c_2+\\frac{p_0}{2}\\right)$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:msub> <mml:mi>n</mml:mi> <mml:mrow> <mml:mrow/> <mml:mo>∗</mml:mo> </mml:mrow> </mml:msub> <mml:mo>-</mml:mo> <mml:mfenced> <mml:msub> <mml:mi>c</mml:mi> <mml:mn>2</mml:mn> </mml:msub> <mml:mo>+</mml:mo> <mml:mfrac> <mml:msub> <mml:mi>p</mml:mi> <mml:mn>0</mml:mn> </mml:msub> <mml:mn>2</mml:mn> </mml:mfrac> </mml:mfenced> </mml:mrow> </mml:math> . Similarly, when the considered rearrangements are those modeled by the double-cut-and-join (DCJ) operation, the rearrangement distance of $$\\mathbb {A}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>A</mml:mi> </mml:math> and $$\\mathbb {B}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>B</mml:mi> </mml:math> is $$n_*-\\left( c+\\frac{p_e }{2}\\right)$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:msub> <mml:mi>n</mml:mi> <mml:mrow> <mml:mrow/> <mml:mo>∗</mml:mo> </mml:mrow> </mml:msub> <mml:mo>-</mml:mo> <mml:mfenced> <mml:mi>c</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac> <mml:msub> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi> </mml:msub> <mml:mn>2</mml:mn> </mml:mfrac> </mml:mfenced> </mml:mrow> </mml:math> , where c is the total number of cycles and $$p_e$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msub> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi> </mml:msub> </mml:math> is the total number of paths of even length. Motivation The distance formulation is a basic unit for several other combinatorial problems related to genome evolution and ancestral reconstruction, such as median or double distance . Interestingly, both median and double distance problems can be solved in polynomial time for the breakpoint distance, while they are NP-hard for the rearrangement distance. One way of exploring the complexity space between these two extremes is to consider a $$\\sigma _k$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msub> <mml:mi>σ</mml:mi> <mml:mi>k</mml:mi> </mml:msub> </mml:math> distance, defined to be $$n_*-\\left( c_2+c_4+\\ldots +c_k+\\frac{p_0+p_2+\\ldots +p_{k-2}}{2}\\right)$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:msub> <mml:mi>n</mml:mi> <mml:mrow> <mml:mrow/> <mml:mo>∗</mml:mo> </mml:mrow> </mml:msub> <mml:mo>-</mml:mo> <mml:mfenced> <mml:msub> <mml:mi>c</mml:mi> <mml:mn>2</mml:mn> </mml:msub> <mml:mo>+</mml:mo> <mml:msub> <mml:mi>c</mml:mi> <mml:mn>4</mml:mn> </mml:msub> <mml:mo>+</mml:mo> <mml:mo>…</mml:mo> <mml:mo>+</mml:mo> <mml:msub> <mml:mi>c</mml:mi> <mml:mi>k</mml:mi> </mml:msub> <mml:mo>+</mml:mo> <mml:mfrac> <mml:mrow> <mml:msub> <mml:mi>p</mml:mi> <mml:mn>0</mml:mn> </mml:msub> <mml:mo>+</mml:mo> <mml:msub> <mml:mi>p</mml:mi> <mml:mn>2</mml:mn> </mml:msub> <mml:mo>+</mml:mo> <mml:mo>…</mml:mo> <mml:mo>+</mml:mo> <mml:msub> <mml:mi>p</mml:mi> <mml:mrow> <mml:mi>k</mml:mi> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> </mml:mrow> </mml:msub> </mml:mrow> <mml:mn>2</mml:mn> </mml:mfrac> </mml:mfenced> </mml:mrow> </mml:math> , and increasingly investigate the complexities of median and double distance for the $$\\sigma _4$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msub> <mml:mi>σ</mml:mi> <mml:mn>4</mml:mn> </mml:msub> </mml:math> distance, then the $$\\sigma _6$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msub> <mml:mi>σ</mml:mi> <mml:mn>6</mml:mn> </mml:msub> </mml:math> distance, and so on. Results While for the median much effort was done in our and in other research groups but no progress was obtained even for the $$\\sigma _4$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msub> <mml:mi>σ</mml:mi> <mml:mn>4</mml:mn> </mml:msub> </mml:math> distance, for solving the double distance under $$\\sigma _4$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msub> <mml:mi>σ</mml:mi> <mml:mn>4</mml:mn> </mml:msub> </mml:math> and $$\\sigma _6$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msub> <mml:mi>σ</mml:mi> <mml:mn>6</mml:mn> </mml:msub> </mml:math> distances we could devise linear time algorithms, which we present here.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390586052",
    "type": "article"
  },
  {
    "title": "Recombinations, chains and caps: resolving problems with the DCJ-indel model",
    "doi": "https://doi.org/10.1186/s13015-024-00253-7",
    "publication_date": "2024-02-27",
    "publication_year": 2024,
    "authors": "Leonard Bohnenkämper",
    "corresponding_authors": "Leonard Bohnenkämper",
    "abstract": "Abstract One of the most fundamental problems in genome rearrangement studies is the (genomic) distance problem. It is typically formulated as finding the minimum number of rearrangements under a model that are needed to transform one genome into the other. A powerful multi-chromosomal model is the Double Cut and Join (DCJ) model.While the DCJ model is not able to deal with some situations that occur in practice, like duplicated or lost regions, it was extended over time to handle these cases. First, it was extended to the DCJ-indel model, solving the issue of lost markers. Later ILP-solutions for so called natural genomes , in which each genomic region may occur an arbitrary number of times, were developed, enabling in theory to solve the distance problem for any pair of genomes. However, some theoretical and practical issues remained unsolved. On the theoretical side of things, there exist two disparate views of the DCJ-indel model, motivated in the same way, but with different conceptualizations that could not be reconciled so far. On the practical side, while ILP solutions for natural genomes typically perform well on telomere to telomere resolved genomes, they have been shown in recent years to quickly loose performance on genomes with a large number of contigs or linear chromosomes. This has been linked to a particular technique, namely capping . Simply put, capping circularizes linear chromosomes by concatenating them during solving time, increasing the solution space of the ILP superexponentially. Recently, we introduced a new conceptualization of the DCJ-indel model within the context of another rearrangement problem. In this manuscript, we will apply this new conceptualization to the distance problem. In doing this, we uncover the relation between the disparate conceptualizations of the DCJ-indel model. We are also able to derive an ILP solution to the distance problem that does not rely on capping. This solution significantly improves upon the performance of previous solutions on genomes with high numbers of contigs while still solving the problem exactly and being competitive in performance otherwise. We demonstrate the performance advantage on simulated genomes as well as showing its practical usefulness in an analysis of 11 Drosophila genomes.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392191045",
    "type": "article"
  },
  {
    "title": "Space-efficient computation of k-mer dictionaries for large values of k",
    "doi": "https://doi.org/10.1186/s13015-024-00259-1",
    "publication_date": "2024-04-05",
    "publication_year": 2024,
    "authors": "Diego Díaz-Domínguez; Miika Leinonen; Leena Salmela",
    "corresponding_authors": "Diego Díaz-Domínguez",
    "abstract": "Abstract Computing k -mer frequencies in a collection of reads is a common procedure in many genomic applications. Several state-of-the-art k -mer counters rely on hash tables to carry out this task but they are often optimised for small k as a hash table keeping keys explicitly (i.e., k -mer sequences) takes $$O(N\\frac{k}{w})$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mfrac> <mml:mi>k</mml:mi> <mml:mi>w</mml:mi> </mml:mfrac> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> computer words, N being the number of distinct k -mers and w the computer word size, which is impractical for long values of k . This space usage is an important limitation as analysis of long and accurate HiFi sequencing reads can require larger values of k . We propose , a space-efficient hash table for k -mers using $$O(N+u\\frac{k}{w})$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>+</mml:mo> <mml:mi>u</mml:mi> <mml:mfrac> <mml:mi>k</mml:mi> <mml:mi>w</mml:mi> </mml:mfrac> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> words of space, where u is the number of reads. Our framework exploits the fact that consecutive k -mers overlap by $$k-1$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>k</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> </mml:mrow> </mml:math> symbols. Thus, we only store the last symbol of a k -mer and a pointer within the hash table to a previous one, which we can use to recover the remaining $$k-1$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>k</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> </mml:mrow> </mml:math> symbols. We adapt to compute canonical k -mers as well. This variant also uses pointers within the hash table to save space but requires more work to decode the k -mers. Specifically, it takes $$O(\\sigma ^{k})$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msup> <mml:mi>σ</mml:mi> <mml:mi>k</mml:mi> </mml:msup> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> time in the worst case, $$\\sigma$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>σ</mml:mi> </mml:math> being the DNA alphabet, but our experiments show this is hardly ever the case. The canonical variant does not improve our theoretical results but greatly reduces space usage in practice while keeping a competitive performance to get the k -mers and their frequencies. We compare canonical to a regular hash table storing canonical k -mers explicitly as keys and show that our method uses up to five times less space while being less than 1.5 times slower. We also show that canonical uses significantly less memory than state-of-the-art k -mer counters when they do not resort to disk to keep intermediate results.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4393989706",
    "type": "article"
  },
  {
    "title": "Evaluating deterministic motif significance measures in protein databases",
    "doi": "https://doi.org/10.1186/1748-7188-2-16",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Pedro G. Ferreira; Paulo J. Azevedo",
    "corresponding_authors": "",
    "abstract": "Assessing the outcome of motif mining algorithms is an essential task, as the number of reported motifs can be very large. Significance measures play a central role in automatically ranking those motifs, and therefore alleviating the analysis work. Spotting the most interesting and relevant motifs is then dependent on the choice of the right measures. The combined use of several measures may provide more robust results. However caution has to be taken in order to avoid spurious evaluations.From the set of conducted experiments, it was verified that several of the selected significance measures show a very similar behavior in a wide range of situations therefore providing redundant information. Some measures have proved to be more appropriate to rank highly conserved motifs, while others are more appropriate for weakly conserved ones. Support appears as a very important feature to be considered for correct motif ranking. We observed that not all the measures are suitable for situations with poorly balanced class information, like for instance, when positive data is significantly less than negative data. Finally, a visualization scheme was proposed that, when several measures are applied, enables an easy identification of high scoring motifs.In this work we have surveyed and categorized 14 significance measures for pattern evaluation. Their ability to rank three types of deterministic motifs was evaluated. Measures were applied in different testing conditions, where relations were identified. This study provides some pertinent insights on the choice of the right set of significance measures for the evaluation of deterministic motifs extracted from protein databases.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2122099933",
    "type": "article"
  },
  {
    "title": "Reconciling taxonomy and phylogenetic inference: formalism and algorithms for describing discord and inferring taxonomic roots",
    "doi": "https://doi.org/10.1186/1748-7188-7-8",
    "publication_date": "2012-05-02",
    "publication_year": 2012,
    "authors": "F. A. Matsen; Aaron Gallagher",
    "corresponding_authors": "",
    "abstract": "Although taxonomy is often used informally to evaluate the results of phylogenetic inference and the root of phylogenetic trees, algorithmic methods to do so are lacking.In this paper we formalize these procedures and develop algorithms to solve the relevant problems. In particular, we introduce a new algorithm that solves a \"subcoloring\" problem to express the difference between a taxonomy and a phylogeny at a given rank. This algorithm improves upon the current best algorithm in terms of asymptotic complexity for the parameter regime of interest; we also describe a branch-and-bound algorithm that saves orders of magnitude in computation on real data sets. We also develop a formalism and an algorithm for rooting phylogenetic trees according to a taxonomy.The algorithms in this paper, and the associated freely-available software, will help biologists better use and understand taxonomically labeled phylogenetic trees.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2137015563",
    "type": "article"
  },
  {
    "title": "WildSpan: mining structured motifs from protein sequences",
    "doi": "https://doi.org/10.1186/1748-7188-6-6",
    "publication_date": "2011-03-31",
    "publication_year": 2011,
    "authors": "Chen‐Ming Hsu; Chien‐Yu Chen; Baw‐Jhiune Liu",
    "corresponding_authors": "",
    "abstract": "Abstract Background Automatic extraction of motifs from biological sequences is an important research problem in study of molecular biology. For proteins, it is desired to discover sequence motifs containing a large number of wildcard symbols, as the residues associated with functional sites are usually largely separated in sequences. Discovering such patterns is time-consuming because abundant combinations exist when long gaps (a gap consists of one or more successive wildcards) are considered. Mining algorithms often employ constraints to narrow down the search space in order to increase efficiency. However, improper constraint models might degrade the sensitivity and specificity of the motifs discovered by computational methods. We previously proposed a new constraint model to handle large wildcard regions for discovering functional motifs of proteins. The patterns that satisfy the proposed constraint model are called W-patterns. A W-pattern is a structured motif that groups motif symbols into pattern blocks interleaved with large irregular gaps. Considering large gaps reflects the fact that functional residues are not always from a single region of protein sequences, and restricting motif symbols into clusters corresponds to the observation that short motifs are frequently present within protein families. To efficiently discover W-patterns for large-scale sequence annotation and function prediction, this paper first formally introduces the problem to solve and proposes an algorithm named WildSpan (sequential pattern mining across large wildcard regions) that incorporates several pruning strategies to largely reduce the mining cost. Results WildSpan is shown to efficiently find W-patterns containing conserved residues that are far separated in sequences. We conducted experiments with two mining strategies, protein-based and family-based mining, to evaluate the usefulness of W-patterns and performance of WildSpan. The protein-based mining mode of WildSpan is developed for discovering functional regions of a single protein by referring to a set of related sequences (e.g. its homologues). The discovered W-patterns are used to characterize the protein sequence and the results are compared with the conserved positions identified by multiple sequence alignment (MSA). The family-based mining mode of WildSpan is developed for extracting sequence signatures for a group of related proteins (e.g. a protein family) for protein function classification. In this situation, the discovered W-patterns are compared with PROSITE patterns as well as the patterns generated by three existing methods performing the similar task. Finally, analysis on execution time of running WildSpan reveals that the proposed pruning strategy is effective in improving the scalability of the proposed algorithm. Conclusions The mining results conducted in this study reveal that WildSpan is efficient and effective in discovering functional signatures of proteins directly from sequences. The proposed pruning strategy is effective in improving the scalability of WildSpan. It is demonstrated in this study that the W-patterns discovered by WildSpan provides useful information in characterizing protein sequences. The WildSpan executable and open source codes are available on the web ( http://biominer.csie.cyu.edu.tw/wildspan ).",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2091572769",
    "type": "article"
  },
  {
    "title": "Ubiquity of synonymity: almost all large binary trees are not uniquely identified by their spectra or their immanantal polynomials",
    "doi": "https://doi.org/10.1186/1748-7188-7-14",
    "publication_date": "2012-05-21",
    "publication_year": 2012,
    "authors": "F. A. Matsen; Steven N. Evans",
    "corresponding_authors": "F. A. Matsen",
    "abstract": "There are several common ways to encode a tree as a matrix, such as the adjacency matrix, the Laplacian matrix (that is, the infinitesimal generator of the natural random walk), and the matrix of pairwise distances between leaves. Such representations involve a specific labeling of the vertices or at least the leaves, and so it is natural to attempt to identify trees by some feature of the associated matrices that is invariant under relabeling. An obvious candidate is the spectrum of eigenvalues (or, equivalently, the characteristic polynomial). We show for any of these choices of matrix that the fraction of binary trees with a unique spectrum goes to zero as the number of leaves goes to infinity. We investigate the rate of convergence of the above fraction to zero using numerical methods. For the adjacency and Laplacian matrices, we show that the a priori more informative immanantal polynomials have no greater power to distinguish between trees. Our results show that a generic large binary tree is highly unlikely to be identified uniquely by common spectral invariants.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2158093499",
    "type": "article"
  },
  {
    "title": "TS-AMIR: a topology string alignment method for intensive rapid protein structure comparison",
    "doi": "https://doi.org/10.1186/1748-7188-7-4",
    "publication_date": "2012-02-15",
    "publication_year": 2012,
    "authors": "Jafar Razmara; Safaai Deris; Sepideh Parvizpour",
    "corresponding_authors": "",
    "abstract": "In structural biology, similarity analysis of protein structure is a crucial step in studying the relationship between proteins. Despite the considerable number of techniques that have been explored within the past two decades, the development of new alternative methods is still an active research area due to the need for high performance tools. In this paper, we present TS-AMIR, a Topology String Alignment Method for Intensive Rapid comparison of protein structures. The proposed method works in two stages: In the first stage, the method generates a topology string based on the geometric details of secondary structure elements, and then, utilizes an n-gram modelling technique over entropy concept to capture similarities in these strings. This initial correspondence map between secondary structure elements is submitted to the second stage in order to obtain the alignment at the residue level. Applying the Kabsch method, a heuristic step-by-step algorithm is adopted in the second stage to align the residues, resulting in an optimal rotation matrix and minimized RMSD. The performance of the method was assessed in different information retrieval tests and the results were compared with those of CE and TM-align, representing two geometrical tools, and YAKUSA, 3D-BLAST and SARST as three representatives of linear encoding schemes. It is shown that the method obtains a high running speed similar to that of the linear encoding schemes. In addition, the method runs about 800 and 7200 times faster than TM-align and CE respectively, while maintaining a competitive accuracy with TM-align and CE. The experimental results demonstrate that linear encoding techniques are capable of reaching the same high degree of accuracy as that achieved by geometrical methods, while generally running hundreds of times faster than conventional programs.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2172156899",
    "type": "article"
  },
  {
    "title": "Recursive algorithms for phylogenetic tree counting",
    "doi": "https://doi.org/10.1186/1748-7188-8-26",
    "publication_date": "2013-10-28",
    "publication_year": 2013,
    "authors": "Alexandra Gavryushkina; David Welch; Alexei J. Drummond",
    "corresponding_authors": "Alexandra Gavryushkina",
    "abstract": "In Bayesian phylogenetic inference we are interested in distributions over a space of trees. The number of trees in a tree space is an important characteristic of the space and is useful for specifying prior distributions. When all samples come from the same time point and no prior information available on divergence times, the tree counting problem is easy. However, when fossil evidence is used in the inference to constrain the tree or data are sampled serially, new tree spaces arise and counting the number of trees is more difficult.We describe an algorithm that is polynomial in the number of sampled individuals for counting of resolutions of a constraint tree assuming that the number of constraints is fixed. We generalise this algorithm to counting resolutions of a fully ranked constraint tree. We describe a quadratic algorithm for counting the number of possible fully ranked trees on n sampled individuals. We introduce a new type of tree, called a fully ranked tree with sampled ancestors, and describe a cubic time algorithm for counting the number of such trees on n sampled individuals.These algorithms should be employed for Bayesian Markov chain Monte Carlo inference when fossil data are included or data are serially sampled.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2034530888",
    "type": "article"
  },
  {
    "title": "Simplified sequence-based method for ATP-binding prediction using contextual local evolutionary conservation",
    "doi": "https://doi.org/10.1186/1748-7188-9-7",
    "publication_date": "2014-03-11",
    "publication_year": 2014,
    "authors": "Chun Fang; Tamotsu Noguchi; Hayato Yamana",
    "corresponding_authors": "",
    "abstract": "Identifying ligand-binding sites is a key step to annotate the protein functions and to find applications in drug design. Now, many sequence-based methods adopted various predicted results from other classifiers, such as predicted secondary structure, predicted solvent accessibility and predicted disorder probabilities, to combine with position-specific scoring matrix (PSSM) as input for binding sites prediction. These predicted features not only easily result in high-dimensional feature space, but also greatly increased the complexity of algorithms. Moreover, the performances of these predictors are also largely influenced by the other classifiers.In order to verify that conservation is the most powerful attribute in identifying ligand-binding sites, and to show the importance of revising PSSM to match the detailed conservation pattern of functional site in prediction, we have analyzed the Adenosine-5'-triphosphate (ATP) ligand as an example, and proposed a simple method for ATP-binding sites prediction, named as CLCLpred (Contextual Local evolutionary Conservation-based method for Ligand-binding prediction). Our method employed no predicted results from other classifiers as input; all used features were extracted from PSSM only. We tested our method on 2 separate data sets. Experimental results showed that, comparing with other 9 existing methods on the same data sets, our method achieved the best performance.This study demonstrates that: 1) exploiting the signal from the detailed conservation pattern of residues will largely facilitate the prediction of protein functional sites; and 2) the local evolutionary conservation enables accurate prediction of ATP-binding sites directly from protein sequence.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2124452245",
    "type": "article"
  },
  {
    "title": "Optimal computation of all tandem repeats in a weighted sequence",
    "doi": "https://doi.org/10.1186/s13015-014-0021-5",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Carl Barton; Costas S. Iliopoulos; Solon P. Pissis",
    "corresponding_authors": "Carl Barton",
    "abstract": "Tandem duplication, in the context of molecular biology, occurs as a result of mutational events in which an original segment of DNA is converted into a sequence of individual copies. More formally, a repetition or tandem repeat in a string of letters consists of exact concatenations of identical factors of the string. Biologists are interested in approximate tandem repeats and not necessarily only in exact tandem repeats. A weighted sequence is a string in which a set of letters may occur at each position with respective probabilities of occurrence. It naturally arises in many biological contexts and provides a method to realise the approximation among distinct adjacent occurrences of the same DNA segment.Crochemore's repetitions algorithm, also referred to as Crochemore's partitioning algorithm, was introduced in 1981, and was the first optimal [Formula: see text]-time algorithm to compute all repetitions in a string of length n. In this article, we present a novel variant of Crochemore's partitioning algorithm for weighted sequences, which requires optimal [Formula: see text] time, thus improving on the best known [Formula: see text]-time algorithm (Zhang et al., 2013) for computing all repetitions in a weighted sequence of length n.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2152202044",
    "type": "article"
  },
  {
    "title": "Heuristic shortest hyperpaths in cell signaling hypergraphs",
    "doi": "https://doi.org/10.1186/s13015-022-00217-9",
    "publication_date": "2022-05-26",
    "publication_year": 2022,
    "authors": "Spencer Krieger; John Kececioglu",
    "corresponding_authors": "",
    "abstract": "Cell signaling pathways, which are a series of reactions that start at receptors and end at transcription factors, are basic to systems biology. Properly modeling the reactions in such pathways requires directed hypergraphs, where an edge is now directed between two sets of vertices. Inferring a pathway by the most parsimonious series of reactions corresponds to finding a shortest hyperpath in a directed hypergraph, which is NP-complete. The current state-of-the-art for shortest hyperpaths in cell signaling hypergraphs solves a mixed-integer linear program to find an optimal hyperpath that is restricted to be acyclic, and offers no efficiency guarantees.We present, for the first time, a heuristic for general shortest hyperpaths that properly handles cycles, and is guaranteed to be efficient. We show the heuristic finds provably optimal hyperpaths for the class of singleton-tail hypergraphs, and also give a practical algorithm for tractably generating all source-sink hyperpaths. The accuracy of the heuristic is demonstrated through comprehensive experiments on all source-sink instances from the standard NCI-PID and Reactome pathway databases, which show it finds a hyperpath that matches the state-of-the-art mixed-integer linear program on over 99% of all instances that are acyclic. On instances where only cyclic hyperpaths exist, the heuristic surpasses the state-of-the-art, which finds no solution; on every such cyclic instance, enumerating all source-sink hyperpaths shows the solution found by the heuristic was in fact optimal.The new shortest hyperpath heuristic is both fast and accurate. This makes finding source-sink hyperpaths, which in general may contain cycles, now practical for real cell signaling networks.Source code for the hyperpath heuristic in a new tool we call Hhugin (as well as for hyperpath enumeration, and all dataset instances) is available free for non-commercial use at http://hhugin.cs.arizona.edu.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4281567003",
    "type": "article"
  },
  {
    "title": "$$\\textsc {McDag}$$: indexing maximal common subsequences for k strings",
    "doi": "https://doi.org/10.1186/s13015-025-00271-z",
    "publication_date": "2025-04-19",
    "publication_year": 2025,
    "authors": "Giovanni Buzzega; Alessio Conte; Roberto Grossi; Giulia Punzi",
    "corresponding_authors": "",
    "abstract": "Abstract Analyzing and comparing sequences of symbols is among the most fundamental problems in computer science, possibly even more so in bioinformatics. Maximal Common Subsequences (MCSs), i.e., inclusion-maximal sequences of non-contiguous symbols common to two or more strings, have only recently received attention in this area, despite being a basic notion and a natural generalization of more common tools like Longest Common Substrings/Subsequences. In this paper we simplify and engineer recent advancements in MCSs into a practical tool called $$\\textsc {McDag}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>M</mml:mi> <mml:mstyle> <mml:mi>C</mml:mi> <mml:mi>D</mml:mi> <mml:mi>A</mml:mi> <mml:mi>G</mml:mi> </mml:mstyle> </mml:mrow> </mml:math> , the first publicly available tool that can index MCSs of real genomic data, and show that its definition can be generalized to multiple strings. We demonstrate that our tool can index pairs of sequences exceeding 10,000 base pairs within minutes, utilizing only 4-7% more than the minimum required nodes. For three or more sequences, we observe experimentally that the minimum index may exhibit a significant increase in the number of nodes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409599792",
    "type": "article"
  },
  {
    "title": "Anchorage accurately assembles anchor-flanked synthetic long reads",
    "doi": "https://doi.org/10.1186/s13015-025-00288-4",
    "publication_date": "2025-07-06",
    "publication_year": 2025,
    "authors": "Xiaofei Carl Zang; Xiang Li; Kyle Metcalfe; Tuval Ben-Yehezkel; Ryan Kelley; Mingfu Shao",
    "corresponding_authors": "",
    "abstract": "Modern sequencing technologies allow for the addition of short-sequence tags, known as anchors, to both ends of a captured molecule. Anchors are useful in assembling the full-length sequence of a captured molecule as they can be used to accurately determine the endpoints. One representative of such anchor-enabled technology is LoopSeq Solo, a synthetic long read (SLR) sequencing protocol. LoopSeq Solo also achieves ultra-high sequencing depth and high purity of short reads covering the entire captured molecule. Despite the availability of many assembly methods, constructing full-length sequence from these anchor-enabled, ultra-high coverage sequencing data remains challenging due to the complexity of the underlying assembly graphs and the lack of specific algorithms leveraging anchors. We present Anchorage, a novel assembler that performs anchor-guided assembly for ultra-high-depth sequencing data. Anchorage starts with a kmer-based approach for precise estimation of molecule lengths. It then formulates the assembly problem as finding an optimal path that connects the two nodes determined by anchors in the underlying compact de Bruijn graph. The optimality is defined as maximizing the weight of the smallest node while matching the estimated sequence length. Anchorage uses a modified dynamic programming algorithm to efficiently find the optimal path. Through both simulations and real data, we show that Anchorage outperforms existing assembly methods, particularly in the presence of sequencing artifacts. Anchorage fills the gap in assembling anchor-enabled data. We anticipate its broad use as anchor-enabled sequencing technologies become prevalent. Anchorage is freely available at https://github.com/Shao-Group/anchorage ; the scripts and documents that can reproduce all experiments in this manuscript are available at https://github.com/Shao-Group/anchorage-test .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412057961",
    "type": "article"
  },
  {
    "title": "Swiftly identifying strongly unique k-mers",
    "doi": "https://doi.org/10.1186/s13015-025-00286-6",
    "publication_date": "2025-07-13",
    "publication_year": 2025,
    "authors": "Jens Zentgraf; Sven Rahmann",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412372943",
    "type": "article"
  },
  {
    "title": "The path-label reconciliation (PLR) dissimilarity measure for gene trees",
    "doi": "https://doi.org/10.1186/s13015-025-00284-8",
    "publication_date": "2025-08-19",
    "publication_year": 2025,
    "authors": "Alitzel López Sánchez; José Antonio Ramírez-Rafael; Alejandro Flores-Lamas; Maribel Hernández-Rosales; Manuel Lafond",
    "corresponding_authors": "",
    "abstract": "Abstract Background In this study, we investigate the problem of comparing gene trees reconciled with the same species tree using a novel semi-metric, called the Path-Label Reconciliation (PLR) dissimilarity measure. This approach not only quantifies differences in the topology of reconciled gene trees, but also considers discrepancies in predicted ancestral gene-species maps and speciation/duplication events, offering a refinement of existing metrics such as Robinson-Foulds (RF) and their labeled extensions LRF and ELRF. A tunable parameter $$\\alpha$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>α</mml:mi> </mml:math> also allows users to adjust the balance between its species map and event labeling components. Our contributions We show that PLR can be computed in linear time and that it is a semi-metric. We also discuss the diameters of reconciled gene tree measures, which are important in practice for normalization, and provide initial bounds on PLR, LRF, and ELRF. To validate PLR, we simulate reconciliations and perform comparisons with LRF and ELRF. The results show that PLR provides a more evenly distributed range of distances, making it less susceptible to overestimating differences in the presence of small topological changes, while at the same time being computationally efficient. We also apply our measure to evaluate the set of possible rootings of gene trees against a gold standard, and demonstrate that our measure is better at distinguishing one best gene tree among multiple candidates. Furthermore, our findings suggest that the theoretical diameter is rarely reached in practice. The PLR measure advances phylogenetic reconciliation by combining theoretical rigor with practical applicability. Future research will refine its mathematical properties, explore its performance on different types of trees, and integrate it with existing bioinformatics tools for large-scale evolutionary analyses. The implementation of the PLR distance is available in the open-source PyPI package : https://pypi.org/project/parle/ .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413308490",
    "type": "article"
  },
  {
    "title": "Exclusive functional signatures for gene annotation with vast OpenOrd layout",
    "doi": "https://doi.org/10.1186/s13015-025-00290-w",
    "publication_date": "2025-09-29",
    "publication_year": 2025,
    "authors": "Gleb S. Buzanov; Vsevolod J. Makeev",
    "corresponding_authors": "Gleb S. Buzanov; Vsevolod J. Makeev",
    "abstract": "A biological study can produce a limited number of marker genes, not large enough to be used in gene set enrichment analysis. Here we suggest VOL-Gene, a graph-based algorithm that partitions all genes into non-overlapping classes of functionally related genes, thus assigning a single function to each gene. To this end, many functional signatures are combined into a single weighted graph, which is partitioned into cliques. For a poorly annotated marker gene, our approach fetches a number of genes that belong to the same class, some of which can be well annotated and are likely to take part in the same biological process.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414629815",
    "type": "article"
  },
  {
    "title": "Breaking the hierarchy - a new cluster selection mechanism for hierarchical clustering methods",
    "doi": "https://doi.org/10.1186/1748-7188-4-12",
    "publication_date": "2009-10-19",
    "publication_year": 2009,
    "authors": "László A Zahoránszky; Gyula Y. Katona; Péter Hári; András Málnási‐Csizmadia; Katharina A. Zweig; Gergely Zahoránszky-Köhalmi",
    "corresponding_authors": "",
    "abstract": "Hierarchical clustering methods like Ward's method have been used since decades to understand biological and chemical data sets. In order to get a partition of the data set, it is necessary to choose an optimal level of the hierarchy by a so-called level selection algorithm. In 2005, a new kind of hierarchical clustering method was introduced by Palla et al. that differs in two ways from Ward's method: it can be used on data on which no full similarity matrix is defined and it can produce overlapping clusters, i.e., allow for multiple membership of items in clusters. These features are optimal for biological and chemical data sets but until now no level selection algorithm has been published for this method.In this article we provide a general selection scheme, the level independent clustering selection method, called LInCS. With it, clusters can be selected from any level in quadratic time with respect to the number of clusters. Since hierarchically clustered data is not necessarily associated with a similarity measure, the selection is based on a graph theoretic notion of cohesive clusters. We present results of our method on two data sets, a set of drug like molecules and set of protein-protein interaction (PPI) data. In both cases the method provides a clustering with very good sensitivity and specificity values according to a given reference clustering. Moreover, we can show for the PPI data set that our graph theoretic cohesiveness measure indeed chooses biologically homogeneous clusters and disregards inhomogeneous ones in most cases. We finally discuss how the method can be generalized to other hierarchical clustering methods to allow for a level independent cluster selection.Using our new cluster selection method together with the method by Palla et al. provides a new interesting clustering mechanism that allows to compute overlapping clusters, which is especially valuable for biological and chemical data sets.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2027263440",
    "type": "article"
  },
  {
    "title": "Back-translation for discovering distant protein homologies in the presence of frameshift mutations",
    "doi": "https://doi.org/10.1186/1748-7188-5-6",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Marta Gîrdea; Laurent Noé; Grégory Kucherov",
    "corresponding_authors": "",
    "abstract": "Frameshift mutations in protein-coding DNA sequences produce a drastic change in the resulting protein sequence, which prevents classic protein alignment methods from revealing the proteins' common origin. Moreover, when a large number of substitutions are additionally involved in the divergence, the homology detection becomes difficult even at the DNA level.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2096816098",
    "type": "article"
  },
  {
    "title": "Hierarchical folding of multiple sequence alignments for the prediction of structures and RNA-RNA interactions",
    "doi": "https://doi.org/10.1186/1748-7188-5-22",
    "publication_date": "2010-05-21",
    "publication_year": 2010,
    "authors": "Stefan E. Seemann; Andreas S. Richter; Jan Gorodkin; Rolf Backofen",
    "corresponding_authors": "",
    "abstract": "Abstract Background Many regulatory non-coding RNAs (ncRNAs) function through complementary binding with mRNAs or other ncRNAs, e.g ., microRNAs, snoRNAs and bacterial sRNAs. Predicting these RNA interactions is essential for functional studies of putative ncRNAs or for the design of artificial RNAs. Many ncRNAs show clear signs of undergoing compensating base changes over evolutionary time. Here, we postulate that a non-negligible part of the existing RNA-RNA interactions contain preserved but covarying patterns of interactions. Methods We present a novel method that takes compensating base changes across the binding sites into account. The algorithm works in two steps on two pre-generated multiple alignments. In the first step, individual base pairs with high reliability are found using the algorithm, which includes evolutionary and thermodynamic properties. In step two (where high reliability base pairs from step one are constrained as unpaired), the principle of cofolding is combined with hierarchical folding. The final prediction of intra - and inter -molecular base pairs consists of the reliabilities computed from the constrained expected accuracy scoring, which is an extended version of that used for individual multiple alignments. Results We derived a rather extensive algorithm. One of the advantages of our approach (in contrast to other RNA-RNA interaction prediction methods) is the application of covariance detection and prediction of pseudoknots between intra - and inter -molecular base pairs. As a proof of concept, we show an example and discuss the strengths and weaknesses of the approach.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2107728696",
    "type": "article"
  },
  {
    "title": "Mimosa: Mixture model of co-expression to detect modulators of regulatory interaction",
    "doi": "https://doi.org/10.1186/1748-7188-5-4",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Matthew E.B. Hansen; Logan J. Everett; Larry N. Singh; Sridhar Hannenhalli",
    "corresponding_authors": "",
    "abstract": "Functionally related genes tend to be correlated in their expression patterns across multiple conditions and/or tissue-types. Thus co-expression networks are often used to investigate functional groups of genes. In particular, when one of the genes is a transcription factor (TF), the co-expression-based interaction is interpreted, with caution, as a direct regulatory interaction. However, any particular TF, and more importantly, any particular regulatory interaction, is likely to be active only in a subset of experimental conditions. Moreover, the subset of expression samples where the regulatory interaction holds may be marked by presence or absence of a modifier gene, such as an enzyme that post-translationally modifies the TF. Such subtlety of regulatory interactions is overlooked when one computes an overall expression correlation.Here we present a novel mixture modeling approach where a TF-Gene pair is presumed to be significantly correlated (with unknown coefficient) in an (unknown) subset of expression samples. The parameters of the model are estimated using a Maximum Likelihood approach. The estimated mixture of expression samples is then mined to identify genes potentially modulating the TF-Gene interaction. We have validated our approach using synthetic data and on four biological cases in cow, yeast, and humans.While limited in some ways, as discussed, the work represents a novel approach to mine expression data and detect potential modulators of regulatory interactions.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2123170032",
    "type": "article"
  },
  {
    "title": "Random generation of RNA secondary structures according to native distributions",
    "doi": "https://doi.org/10.1186/1748-7188-6-24",
    "publication_date": "2011-10-12",
    "publication_year": 2011,
    "authors": "Markus E. Nebel; Anika Scheid; Frank Weinberg",
    "corresponding_authors": "",
    "abstract": "Abstract Background Random biological sequences are a topic of great interest in genome analysis since, according to a powerful paradigm, they represent the background noise from which the actual biological information must differentiate. Accordingly, the generation of random sequences has been investigated for a long time. Similarly, random object of a more complicated structure like RNA molecules or proteins are of interest. Results In this article, we present a new general framework for deriving algorithms for the non-uniform random generation of combinatorial objects according to the encoding and probability distribution implied by a stochastic context-free grammar. Briefly, the framework extends on the well-known recursive method for (uniform) random generation and uses the popular framework of admissible specifications of combinatorial classes, introducing weighted combinatorial classes to allow for the non-uniform generation by means of unranking. This framework is used to derive an algorithm for the generation of RNA secondary structures of a given fixed size. We address the random generation of these structures according to a realistic distribution obtained from real-life data by using a very detailed context-free grammar (that models the class of RNA secondary structures by distinguishing between all known motifs in RNA structure). Compared to well-known sampling approaches used in several structure prediction tools (such as SFold) ours has two major advantages: Firstly, after a preprocessing step in time <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>O</mml:mi> <mml:mrow> <mml:mo>(</mml:mo> <mml:mrow> <mml:msup> <mml:mrow> <mml:mi>n</mml:mi> </mml:mrow> <mml:mrow> <mml:mn>2</mml:mn> </mml:mrow> </mml:msup> </mml:mrow> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> for the computation of all weighted class sizes needed, with our approach a set of m random secondary structures of a given structure size n can be computed in worst-case time complexity <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>O</mml:mi> <mml:mfenced> <mml:mrow> <mml:mi>m</mml:mi> <mml:mo>⋅</mml:mo> <mml:mi>n</mml:mi> <mml:mo>⋅</mml:mo> <mml:mo>log</mml:mo> <mml:mrow> <mml:mo>(</mml:mo> <mml:mrow> <mml:mi>n</mml:mi> </mml:mrow> <mml:mo>)</mml:mo> </mml:mrow> </mml:mrow> </mml:mfenced> </mml:mrow> </mml:math> while other algorithms typically have a runtime in <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>O</mml:mi> <mml:mrow> <mml:mo>(</mml:mo> <mml:mrow> <mml:mi>m</mml:mi> <mml:mo>⋅</mml:mo> <mml:msup> <mml:mrow> <mml:mi>n</mml:mi> </mml:mrow> <mml:mrow> <mml:mn>2</mml:mn> </mml:mrow> </mml:msup> </mml:mrow> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> . Secondly, our approach works with integer arithmetic only which is faster and saves us from all the discomforting details of using floating point arithmetic with logarithmized probabilities. Conclusion A number of experimental results shows that our random generation method produces realistic output, at least with respect to the appearance of the different structural motifs. The algorithm is available as a webservice at http://wwwagak.cs.uni-kl.de/NonUniRandGen and can be used for generating random secondary structures of any specified RNA type. A link to download an implementation of our method (in Wolfram Mathematica) can be found there, too.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2125585709",
    "type": "article"
  },
  {
    "title": "Regmex: a statistical tool for exploring motifs in ranked sequence lists from genomics experiments",
    "doi": "https://doi.org/10.1186/s13015-018-0135-2",
    "publication_date": "2018-12-01",
    "publication_year": 2018,
    "authors": "Morten Muhlig Nielsen; Paula Tataru; Tobias Madsen; Asger Hobolth; Jakob Skou Pedersen",
    "corresponding_authors": "Morten Muhlig Nielsen",
    "abstract": "Background: Motif analysis methods have long been central for studying biological function of nucleotide sequences. Functional genomics experiments extend their potential. They typically generate sequence lists ranked by an experimentally acquired functional property such as gene expression or protein binding affinity. Current motif discovery tools suffer from limitations in searching large motif spaces, and thus more complex motifs may not be included. There is thus a need for motif analysis methods that are tailored for analyzing specific complex motifs motivated by biological questions and hypotheses rather than acting as a screen based motif finding tool. Methods: We present Regmex (REGular expression Motif EXplorer), which offers several methods to identify overrepresented motifs in ranked lists of sequences. Regmex uses regular expressions to define motifs or families of motifs and embedded Markov models to calculate exact p-values for motif observations in sequences. Biases in motif distributions across ranked sequence lists are evaluated using random walks, Brownian bridges, or modified rank based statistics. A modular setup and fast analytic p value evaluations make Regmex applicable to diverse and potentially large-scale motif analysis problems. Results: We demonstrate use cases of combined motifs on simulated data and on expression data from micro RNA transfection experiments. We confirm previously obtained results and demonstrate the usability of Regmex to test a specific hypothesis about the relative location of microRNA seed sites and U-rich motifs. We further compare the tool with an existing motif discovery tool and show increased sensitivity. Conclusions: Regmex is a useful and flexible tool to analyze motif hypotheses that relates to large data sets in functional genomics. The method is available as an R package (https://github.com/muhligs/regmex).",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2905076306",
    "type": "article"
  },
  {
    "title": "Efficient unfolding pattern recognition in single molecule force spectroscopy data",
    "doi": "https://doi.org/10.1186/1748-7188-6-16",
    "publication_date": "2011-06-06",
    "publication_year": 2011,
    "authors": "Bill Andreopoulos; Dirk Labudde",
    "corresponding_authors": "",
    "abstract": "Single-molecule force spectroscopy (SMFS) is a technique that measures the force necessary to unfold a protein. SMFS experiments generate Force-Distance (F-D) curves. A statistical analysis of a set of F-D curves reveals different unfolding pathways. Information on protein structure, conformation, functional states, and inter- and intra-molecular interactions can be derived. In the present work, we propose a pattern recognition algorithm and apply our algorithm to datasets from SMFS experiments on the membrane protein bacterioRhodopsin (bR). We discuss the unfolding pathways found in bR, which are characterised by main peaks and side peaks. A main peak is the result of the pairwise unfolding of the transmembrane helices. In contrast, a side peak is an unfolding event in the alpha-helix or other secondary structural element. The algorithm is capable of detecting side peaks along with main peaks. Therefore, we can detect the individual unfolding pathway as the sequence of events labeled with their occurrences and co-occurrences special to bR's unfolding pathway. We find that side peaks do not co-occur with one another in curves as frequently as main peaks do, which may imply a synergistic effect occurring between helices. While main peaks co-occur as pairs in at least 50% of curves, the side peaks co-occur with one another in less than 10% of curves. Moreover, the algorithm runtime scales well as the dataset size increases. Our algorithm satisfies the requirements of an automated methodology that combines high accuracy with efficiency in analyzing SMFS datasets. The algorithm tackles the force spectroscopy analysis bottleneck leading to more consistent and reproducible results.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2098784730",
    "type": "article"
  },
  {
    "title": "The space of phylogenetic mixtures for equivariant models",
    "doi": "https://doi.org/10.1186/1748-7188-7-33",
    "publication_date": "2012-11-28",
    "publication_year": 2012,
    "authors": "Marta Casanellas; Jesús Fernández-Sánchez; Anna Kedzierska",
    "corresponding_authors": "",
    "abstract": "The selection of an evolutionary model to best fit given molecular data is usually a heuristic choice. In his seminal book, J. Felsenstein suggested that certain linear equations satisfied by the expected probabilities of patterns observed at the leaves of a phylogenetic tree could be used for model selection. It remained an open question, however, whether these equations were sufficient to fully characterize the evolutionary model under consideration.Here we prove that, for most equivariant models of evolution, the space of distributions satisfying these linear equations coincides with the space of distributions arising from mixtures of trees. In other words, we prove that the evolution of an observed multiple sequence alignment can be modeled by a mixture of phylogenetic trees under an equivariant evolutionary model if and only if the distribution of patterns at its columns satisfies the linear equations mentioned above. Moreover, we provide a set of linearly independent equations defining this space of phylogenetic mixtures for each equivariant model and for any number of taxa. Lastly, we use these results to perform a study of identifiability of phylogenetic mixtures.The space of phylogenetic mixtures under equivariant models is a linear space that fully characterizes the evolutionary model. We provide an explicit algorithm to obtain the equations defining these spaces for a number of models and taxa. Its implementation has proved to be a powerful tool for model selection.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2111696995",
    "type": "article"
  },
  {
    "title": "DCJ-indel and DCJ-substitution distances with distinct operation costs",
    "doi": "https://doi.org/10.1186/1748-7188-8-21",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Poly H. da Silva; Raphael C. S. Machado; Simone Dantas; Marília Braga",
    "corresponding_authors": "",
    "abstract": "Classical approaches to compute the genomic distance are usually limited to genomes with the same content and take into consideration only rearrangements that change the organization of the genome (i.e. positions and orientation of pieces of DNA, number and type of chromosomes, etc.), such as inversions, translocations, fusions and fissions. These operations are generically represented by the double-cut and join (DCJ) operation. The distance between two genomes, in terms of number of DCJ operations, can be computed in linear time. In order to handle genomes with distinct contents, also insertions and deletions of fragments of DNA - named indels - must be allowed. More powerful than an indel is a substitution of a fragment of DNA by another fragment of DNA. Indels and substitutions are called content-modifying operations. It has been shown that both the DCJ-indel and the DCJ-substitution distances can also be computed in linear time, assuming that the same cost is assigned to any DCJ or content-modifying operation.In the present study we extend the DCJ-indel and the DCJ-substitution models, considering that the content-modifying cost is distinct from and upper bounded by the DCJ cost, and show that the distance in both models can still be computed in linear time. Although the triangular inequality can be disrupted in both models, we also show how to efficiently fix this problem a posteriori.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2122511759",
    "type": "article"
  },
  {
    "title": "Coordinate systems for supergenomes",
    "doi": "https://doi.org/10.1186/s13015-018-0133-4",
    "publication_date": "2018-09-24",
    "publication_year": 2018,
    "authors": "Fabian Gärtner; Christian Höner zu Siederdissen; Lydia Müller; Peter F. Stadler",
    "corresponding_authors": "Fabian Gärtner",
    "abstract": "Genome sequences and genome annotation data have become available at ever increasing rates in response to the rapid progress in sequencing technologies. As a consequence the demand for methods supporting comparative, evolutionary analysis is also growing. In particular, efficient tools to visualize-omics data simultaneously for multiple species are sorely lacking. A first and crucial step in this direction is the construction of a common coordinate system. Since genomes not only differ by rearrangements but also by large insertions, deletions, and duplications, the use of a single reference genome is insufficient, in particular when the number of species becomes large. The computational problem then becomes to determine an order and orientations of optimal local alignments that are as co-linear as possible with all the genome sequences. We first review the most prominent approaches to model the problem formally and then proceed to showing that it can be phrased as a particular variant of the Betweenness Problem. It is NP hard in general. As exact solutions are beyond reach for the problem sizes of practical interest, we introduce a collection of heuristic simplifiers to resolve ordering conflicts. Benchmarks on real-life data ranging from bacterial to fly genomes demonstrate the feasibility of computing good common coordinate systems.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2892773221",
    "type": "article"
  },
  {
    "title": "Probabilistic grammatical model for helix‐helix contact site classification",
    "doi": "https://doi.org/10.1186/1748-7188-8-31",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Witold Dyrka; Jean‐Christophe Nebel; Małgorzata Kotulska",
    "corresponding_authors": "",
    "abstract": "Hidden Markov Models power many state-of-the-art tools in the field of protein bioinformatics. While excelling in their tasks, these methods of protein analysis do not convey directly information on medium- and long-range residue-residue interactions. This requires an expressive power of at least context-free grammars. However, application of more powerful grammar formalisms to protein analysis has been surprisingly limited.In this work, we present a probabilistic grammatical framework for problem-specific protein languages and apply it to classification of transmembrane helix-helix pairs configurations. The core of the model consists of a probabilistic context-free grammar, automatically inferred by a genetic algorithm from only a generic set of expert-based rules and positive training samples. The model was applied to produce sequence based descriptors of four classes of transmembrane helix-helix contact site configurations. The highest performance of the classifiers reached AUCROC of 0.70. The analysis of grammar parse trees revealed the ability of representing structural features of helix-helix contact sites.We demonstrated that our probabilistic context-free framework for analysis of protein sequences outperforms the state of the art in the task of helix-helix contact site classification. However, this is achieved without necessarily requiring modeling long range dependencies between interacting residues. A significant feature of our approach is that grammar rules and parse trees are human-readable. Thus they could provide biologically meaningful information for molecular biologists.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2107143624",
    "type": "article"
  },
  {
    "title": "MSARC: Multiple sequence alignment by residue clustering",
    "doi": "https://doi.org/10.1186/1748-7188-9-12",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Michał Modzelewski; Norbert Dojer",
    "corresponding_authors": "",
    "abstract": "Progressive methods offer efficient and reasonably good solutions to the multiple sequence alignment problem. However, resulting alignments are biased by guide-trees, especially for relatively distant sequences. We propose MSARC, a new graph-clustering based algorithm that aligns sequence sets without guide-trees. Experiments on the BAliBASE dataset show that MSARC achieves alignment quality similar to the best progressive methods. Furthermore, MSARC outperforms them on sequence sets whose evolutionary distances are difficult to represent by a phylogenetic tree. These datasets are most exposed to the guide-tree bias of alignments. MSARC is available at http://bioputer.mimuw.edu.pl/msarc",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2123250224",
    "type": "article"
  },
  {
    "title": "Interpretation and approximation tools for big, dense Markov chain transition matrices in population genetics",
    "doi": "https://doi.org/10.1186/s13015-015-0061-5",
    "publication_date": "2015-12-01",
    "publication_year": 2015,
    "authors": "Katja Reichel; Valentin Bahier; Cédric Midoux; Nicolas Parisey; Jean‐Pierre Masson; Solenn Stoeckel",
    "corresponding_authors": "",
    "abstract": "Markov chains are a common framework for individual-based state and time discrete models in evolution. Though they played an important role in the development of basic population genetic theory, the analysis of more complex evolutionary scenarios typically involves approximation with other types of models. As the number of states increases, the big, dense transition matrices involved become increasingly unwieldy. However, advances in computational technology continue to reduce the challenges of \"big data\", thus giving new potential to state-rich Markov chains in theoretical population genetics.Using a population genetic model based on genotype frequencies as an example, we propose a set of methods to assist in the computation and interpretation of big, dense Markov chain transition matrices. With the help of network analysis, we demonstrate how they can be transformed into clear and easily interpretable graphs, providing a new perspective even on the classic case of a randomly mating, finite population with mutation. Moreover, we describe an algorithm to save computer memory by substituting the original matrix with a sparse approximate while preserving its mathematically important properties, including a closely corresponding dominant (normalized) eigenvector. A global sensitivity analysis of the approximation results in our example shows that size reduction of more than 90 % is possible without significantly affecting the basic model results. Sample implementations of our methods are collected in the Python module mamoth.Our methods help to make stochastic population genetic models involving big, dense transition matrices computationally feasible. Our visualization techniques provide new ways to explore such models and concisely present the results. Thus, our methods will contribute to establish state-rich Markov chains as a valuable supplement to the diversity of population genetic models currently employed, providing interesting new details about evolution e.g. under non-standard reproductive systems such as partial clonality.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2196928942",
    "type": "article"
  },
  {
    "title": "Sparse RNA folding revisited: space-efficient minimum free energy structure prediction",
    "doi": "https://doi.org/10.1186/s13015-016-0071-y",
    "publication_date": "2016-04-23",
    "publication_year": 2016,
    "authors": "Sebastian Will; Hosna Jabbari",
    "corresponding_authors": "Sebastian Will",
    "abstract": "RNA secondary structure prediction by energy minimization is the central computational tool for the analysis of structural non-coding RNAs and their interactions. Sparsification has been successfully applied to improve the time efficiency of various structure prediction algorithms while guaranteeing the same result; however, for many such folding problems, space efficiency is of even greater concern, particularly for long RNA sequences. So far, space-efficient sparsified RNA folding with fold reconstruction was solved only for simple base-pair-based pseudo-energy models. Here, we revisit the problem of space-efficient free energy minimization. Whereas the space-efficient minimization of the free energy has been sketched before, the reconstruction of the optimum structure has not even been discussed. We show that this reconstruction is not possible in trivial extension of the method for simple energy models. Then, we present the time- and space-efficient sparsified free energy minimization algorithm SparseMFEFold that guarantees MFE structure prediction. In particular, this novel algorithm provides efficient fold reconstruction based on dynamically garbage-collected trace arrows. The complexity of our algorithm depends on two parameters, the number of candidates Z and the number of trace arrows T; both are bounded by $$n^2$$ , but are typically much smaller. The time complexity of RNA folding is reduced from $$O(n^3)$$ to $$O(n^2+nZ)$$ ; the space complexity, from $$O(n^2)$$ to $$O(n + T + Z)$$ . Our empirical results show more than 80 % space savings over RNAfold [Vienna RNA package] on the long RNAs from the RNA STRAND database (≥2500 bases). The presented technique is intentionally generalizable to complex prediction algorithms; due to their high space demands, algorithms like pseudoknot prediction and RNA–RNA-interaction prediction are expected to profit even stronger than \"standard\" MFE folding. SparseMFEFold is free software, available at http://www.bioinf.uni-leipzig.de/~will/Software/SparseMFEFold .",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2339509753",
    "type": "review"
  },
  {
    "title": "A Bayesian inference method for the analysis of transcriptional regulatory networks in metagenomic data",
    "doi": "https://doi.org/10.1186/s13015-016-0082-8",
    "publication_date": "2016-07-08",
    "publication_year": 2016,
    "authors": "Elizabeth T. Hobbs; Talmo Pereira; Patrick O’Neill; Ivan Erill",
    "corresponding_authors": "",
    "abstract": "Metagenomics enables the analysis of bacterial population composition and the study of emergent population features, such as shared metabolic pathways. Recently, we have shown that metagenomics datasets can be leveraged to characterize population-wide transcriptional regulatory networks, or meta-regulons, providing insights into how bacterial populations respond collectively to specific triggers. Here we formalize a Bayesian inference framework to analyze the composition of transcriptional regulatory networks in metagenomes by determining the probability of regulation of orthologous gene sequences. We assess the performance of this approach on synthetic datasets and we validate it by analyzing the copper-homeostasis network of Firmicutes species in the human gut microbiome. Assessment on synthetic datasets shows that our method provides a robust and interpretable metric for assessing putative regulation by a transcription factor on sets of promoter sequences mapping to an orthologous gene cluster. The inference framework integrates the regulatory contribution of secondary sites and can discern false positives arising from multiple instances of a clonal sequence. Posterior probabilities for orthologous gene clusters decline sharply when less than 20 % of mapped promoters have binding sites, but we introduce a sensitivity adjustment procedure to speed up computation that enhances regulation assessment in heterogeneous ortholog clusters. Analysis of the copper-homeostasis regulon governed by CsoR in the human gut microbiome Firmicutes reveals that CsoR controls itself and copper-translocating P-type ATPases, but not CopZ-type copper chaperones. Our analysis also indicates that CsoR frequently targets promoters with dual CsoR-binding sites, suggesting that it exploits higher-order binding conformations to fine-tune its activity. We introduce and validate a method for the analysis of transcriptional regulatory networks from metagenomic data that enables inference of meta-regulons in a systematic and interpretable way. Validation of this method on the CsoR meta-regulon of gut microbiome Firmicutes illustrates the usefulness of the approach, revealing novel properties of the copper-homeostasis network in poorly characterized bacterial species and putting forward evidence of new mechanisms of DNA binding for this transcriptional regulator. Our approach will enable the comparative analysis of regulatory networks across metagenomes, yielding novel insights into the evolution of transcriptional regulatory networks.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2465757697",
    "type": "article"
  },
  {
    "title": "The Bourque distances for mutation trees of cancers",
    "doi": "https://doi.org/10.1186/s13015-021-00188-3",
    "publication_date": "2021-06-10",
    "publication_year": 2021,
    "authors": "Katharina Jahn; Niko Beerenwinkel; Louxin Zhang",
    "corresponding_authors": "Louxin Zhang",
    "abstract": "Abstract Background Mutation trees are rooted trees in which nodes are of arbitrary degree and labeled with a mutation set. These trees, also referred to as clonal trees, are used in computational oncology to represent the mutational history of tumours. Classical tree metrics such as the popular Robinson–Foulds distance are of limited use for the comparison of mutation trees. One reason is that mutation trees inferred with different methods or for different patients often contain different sets of mutation labels. Results We generalize the Robinson–Foulds distance into a set of distance metrics called Bourque distances for comparing mutation trees. We show the basic version of the Bourque distance for mutation trees can be computed in linear time. We also make a connection between the Robinson–Foulds distance and the nearest neighbor interchange distance.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3172516756",
    "type": "article"
  },
  {
    "title": "Approximation algorithm for rearrangement distances considering repeated genes and intergenic regions",
    "doi": "https://doi.org/10.1186/s13015-021-00200-w",
    "publication_date": "2021-10-13",
    "publication_year": 2021,
    "authors": "Gabriel Siqueira; Alexsandro Oliveira Alexandrino; Andre Rodrigues Oliveira; Zanoni Dias",
    "corresponding_authors": "Gabriel Siqueira",
    "abstract": "The rearrangement distance is a method to compare genomes of different species. Such distance is the number of rearrangement events necessary to transform one genome into another. Two commonly studied events are the transposition, which exchanges two consecutive blocks of the genome, and the reversal, which reverts a block of the genome. When dealing with such problems, seminal works represented genomes as sequences of genes without repetition. More realistic models started to consider gene repetition or the presence of intergenic regions, sequences of nucleotides between genes and in the extremities of the genome. This work explores the transposition and reversal events applied in a genome representation considering both gene repetition and intergenic regions. We define two problems called Minimum Common Intergenic String Partition and Reverse Minimum Common Intergenic String Partition. Using a relation with these two problems, we show a [Formula: see text]-approximation for the Intergenic Transposition Distance, the Intergenic Reversal Distance, and the Intergenic Reversal and Transposition Distance problems, where k is the maximum number of copies of a gene in the genomes. Our practical experiments on simulated genomes show that the use of partitions improves the estimates for the distances.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3205950683",
    "type": "article"
  },
  {
    "title": "Adding hydrogen atoms to molecular models via fragment superimposition",
    "doi": "https://doi.org/10.1186/s13015-022-00215-x",
    "publication_date": "2022-03-29",
    "publication_year": 2022,
    "authors": "Patrick Kunzmann; Jacob Marcel Anter; Kay Hamacher",
    "corresponding_authors": "Patrick Kunzmann",
    "abstract": "Most experimentally determined structures of biomolecules lack annotated hydrogen positions due to their low electron density. However, thorough structure analysis and simulations require knowledge about the positions of hydrogen atoms. Existing methods for their prediction are either limited to a certain range of molecules or only work effectively on small compounds.We present a novel algorithm that compiles fragments of molecules with known hydrogen atom positions into a library. Using this library the method is able to predict hydrogen positions for molecules with similar moieties. We show that the method is able to accurately assign hydrogen atoms to most organic compounds including biomacromolecules, if a sufficiently large library is used.We bundled the algorithm into the open-source Python package and command line program Hydride. Since usually no additional parametrization is necessary for the problem at hand, the software works out-of-box for a wide range of molecular systems usually within a few seconds of computation time. Hence, we believe that Hydride could be a valuable tool for structural biologists and biophysicists alike.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4220998561",
    "type": "article"
  },
  {
    "title": "Eulertigs: minimum plain text representation of k-mer sets without repetitions in linear time",
    "doi": "https://doi.org/10.1186/s13015-023-00227-1",
    "publication_date": "2023-07-04",
    "publication_year": 2023,
    "authors": "Sebastian Schmidt; Jarno Alanko",
    "corresponding_authors": "",
    "abstract": "Abstract A fundamental operation in computational genomics is to reduce the input sequences to their constituent k -mers. For maximum performance of downstream applications it is important to store the k -mers in small space, while keeping the representation easy and efficient to use (i.e. without k -mer repetitions and in plain text). Recently, heuristics were presented to compute a near-minimum such representation. We present an algorithm to compute a minimum representation in optimal (linear) time and use it to evaluate the existing heuristics. Our algorithm first constructs the de Bruijn graph in linear time and then uses a Eulerian-cycle-based algorithm to compute the minimum representation, in time linear in the size of the output.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4383186537",
    "type": "article"
  },
  {
    "title": "Weighted ASTRID: fast and accurate species trees from weighted internode distances",
    "doi": "https://doi.org/10.1186/s13015-023-00230-6",
    "publication_date": "2023-07-19",
    "publication_year": 2023,
    "authors": "Baqiao Liu; Tandy Warnow",
    "corresponding_authors": "",
    "abstract": "Species tree estimation is a basic step in many biological research projects, but is complicated by the fact that gene trees can differ from the species tree due to processes such as incomplete lineage sorting (ILS), gene duplication and loss (GDL), and horizontal gene transfer (HGT), which can cause different regions within the genome to have different evolutionary histories (i.e., \"gene tree heterogeneity\"). One approach to estimating species trees in the presence of gene tree heterogeneity resulting from ILS operates by computing trees on each genomic region (i.e., computing \"gene trees\") and then using these gene trees to define a matrix of average internode distances, where the internode distance in a tree T between two species x and y is the number of nodes in T between the leaves corresponding to x and y. Given such a matrix, a tree can then be computed using methods such as neighbor joining. Methods such as ASTRID and NJst (which use this basic approach) are provably statistically consistent, very fast (low degree polynomial time) and have had high accuracy under many conditions that makes them competitive with other popular species tree estimation methods. In this study, inspired by the very recent work of weighted ASTRAL, we present weighted ASTRID, a variant of ASTRID that takes the branch uncertainty on the gene trees into account in the internode distance.Our experimental study evaluating weighted ASTRID typically shows improvements in accuracy compared to the original (unweighted) ASTRID, and shows competitive accuracy against weighted ASTRAL, the state of the art. Our re-implementation of ASTRID also improves the runtime, with marked improvements on large datasets.Weighted ASTRID is a new and very fast method for species tree estimation that typically improves upon ASTRID and has comparable accuracy to weighted ASTRAL, while remaining much faster. Weighted ASTRID is available at https://github.com/RuneBlaze/internode .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4384819927",
    "type": "article"
  },
  {
    "title": "A topology-marginal composite likelihood via a generalized phylogenetic pruning algorithm",
    "doi": "https://doi.org/10.1186/s13015-023-00235-1",
    "publication_date": "2023-07-31",
    "publication_year": 2023,
    "authors": "Seong-Hwan Jun; Hassan Nasif; Chris Jennings-Shaffer; David H Rich; Anna Kooperberg; Mathieu Fourment; Cheng Zhang; Marc A. Suchard; F. A. Matsen",
    "corresponding_authors": "F. A. Matsen",
    "abstract": "Abstract Bayesian phylogenetics is a computationally challenging inferential problem. Classical methods are based on random-walk Markov chain Monte Carlo (MCMC), where random proposals are made on the tree parameter and the continuous parameters simultaneously. Variational phylogenetics is a promising alternative to MCMC, in which one fits an approximating distribution to the unnormalized phylogenetic posterior. Previous work fit this variational approximation using stochastic gradient descent, which is the canonical way of fitting general variational approximations. However, phylogenetic trees are special structures, giving opportunities for efficient computation. In this paper we describe a new algorithm that directly generalizes the Felsenstein pruning algorithm (a.k.a. sum-product algorithm) to compute a composite-like likelihood by marginalizing out ancestral states and subtrees simultaneously. We show the utility of this algorithm by rapidly making point estimates for branch lengths of a multi-tree phylogenetic model. These estimates accord with a long MCMC run and with estimates obtained using a variational method, but are much faster to obtain. Thus, although generalized pruning does not lead to a variational algorithm as such, we believe that it will form a useful starting point for variational inference.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4385416016",
    "type": "article"
  },
  {
    "title": "RNA inverse folding can be solved in linear time for structures without isolated stacks or base pairs",
    "doi": "https://doi.org/10.1186/s13015-025-00278-6",
    "publication_date": "2025-10-24",
    "publication_year": 2025,
    "authors": "Théo Boury; Laurent Bulteau; Yann Ponty",
    "corresponding_authors": "",
    "abstract": "Abstract Inverse folding is a classic instance of negative RNA design which consists in finding a sequence that uniquely folds into a target secondary structure with respect to energy minimization. A breakthrough result of Bonnet et al. shows that, even in simple base pairs-based (BP) models, the decision version of a mildly constrained version of inverse folding is NP-hard. In this work, we show that inverse folding can be solved in linear time for a large collection of targets, including every structure that contains no isolated BP and no isolated stack (or, equivalently, when all helices consist of $$3^{+}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msup> <mml:mn>3</mml:mn> <mml:mo>+</mml:mo> </mml:msup> </mml:math> base pairs). For structures featuring shorter helices, our linear algorithm is no longer guaranteed to produce a solution, but still does so for a large proportion of instances. Our approach introduces a notion of modulo m -separability, generalizing a property pioneered by Hales et al . Separability is a sufficient condition for the existence of a solution to the inverse folding problem. We show that, for any input secondary structure of length n , a modulo m -separated sequence can be produced in time $$\\mathcal {O}(n\\,m\\, 2^m)$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mspace/> <mml:mi>m</mml:mi> <mml:mspace/> <mml:msup> <mml:mn>2</mml:mn> <mml:mi>m</mml:mi> </mml:msup> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> anytime such a sequence exists. Meanwhile, we show that any structure consisting of $$3^{+}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:msup> <mml:mn>3</mml:mn> <mml:mo>+</mml:mo> </mml:msup> </mml:math> base pairs is either trivially non-designable, or always admits a modulo-2 separated solution. Solution sequences can thus be produced in linear time, and even be uniformly generated within the set of modulo-2 separable sequences.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399114806",
    "type": "article"
  },
  {
    "title": "Finding high posterior density phylogenies by systematically extending a directed acyclic graph",
    "doi": "https://doi.org/10.1186/s13015-025-00273-x",
    "publication_date": "2025-02-28",
    "publication_year": 2025,
    "authors": "Chris Jennings-Shaffer; David H Rich; Matthew Macaulay; Michael D. Karcher; Tanvi Ganapathy; Shosuke Kiami; Anna Kooperberg; Cheng Zhang; Marc A. Suchard; F. A. Matsen",
    "corresponding_authors": "F. A. Matsen",
    "abstract": "Bayesian phylogenetics typically estimates a posterior distribution, or aspects thereof, using Markov chain Monte Carlo methods. These methods integrate over tree space by applying local rearrangements to move a tree through its space as a random walk. Previous work explored the possibility of replacing this random walk with a systematic search, but was quickly overwhelmed by the large number of probable trees in the posterior distribution. In this paper we develop methods to sidestep this problem using a recently introduced structure called the subsplit directed acyclic graph (sDAG). This structure can represent many trees at once, and local rearrangements of trees translate to methods of enlarging the sDAG. Here we propose two methods of introducing, ranking, and selecting local rearrangements on sDAGs to produce a collection of trees with high posterior density. One of these methods successfully recovers the set of high posterior density trees across a range of data sets. However, we find that a simpler strategy of aggregating trees into an sDAG in fact is computationally faster and returns a higher fraction of probable trees.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404449992",
    "type": "article"
  },
  {
    "title": "Estimating similarity and distance using FracMinHash",
    "doi": "https://doi.org/10.1186/s13015-025-00276-8",
    "publication_date": "2025-05-15",
    "publication_year": 2025,
    "authors": "Mahmudur Rahman Hera; David Koslicki",
    "corresponding_authors": "",
    "abstract": "Abstract Motivation The increasing number and volume of genomic and metagenomic data necessitates scalable and robust computational models for precise analysis. Sketching techniques utilizing $$k$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>k</mml:mi> </mml:math> -mers from a biological sample have proven to be useful for large-scale analyses. In recent years, FracMinHash has emerged as a popular sketching technique and has been used in several useful applications. Recent studies on FracMinHash proved unbiased estimators for the containment and Jaccard indices. However, theoretical investigations for other metrics are still lacking. Theoretical contributions In this paper, we present a theoretical framework for estimating similarity/distance metrics by using FracMinHash sketches, when the metric is expressible in a certain form. We establish conditions under which such an estimation is sound and recommend a minimum scale factor s for accurate results. Experimental evidence supports our theoretical findings. Practical contributions We also present , a fast and efficient FracMinHash sketch generator program. is the fastest known FracMinHash sketch generator, delivering accurate and precise results for cosine similarity estimation on real data. is also the first parallel tool for this task, allowing for speeding up sketch generation using multiple CPU cores – an option lacking in existing serialized tools. We show that by computing FracMinHash sketches using , we can estimate pairwise similarity speedily and accurately on real data. is freely available here: https://github.com/KoslickiLab/frac-kmc/",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410398037",
    "type": "article"
  },
  {
    "title": "AlfaPang: alignment free algorithm for pangenome graph construction",
    "doi": "https://doi.org/10.1186/s13015-025-00277-7",
    "publication_date": "2025-05-15",
    "publication_year": 2025,
    "authors": "Adam Cicherski; A. Lisiecka; Norbert Dojer",
    "corresponding_authors": "",
    "abstract": "Abstract The success of pangenome-based approaches to genomics analysis depends largely on the existence of efficient methods for constructing pangenome graphs that are applicable to large genome collections. In the current paper we present AlfaPang, a new pangenome graph building algorithm. AlfaPang is based on a novel alignment-free approach that allows to construct pangenome graphs using significantly less computational resources than state-of-the-art tools. The code of AlfaPang is freely available at https://github.com/AdamCicherski/AlfaPang .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410406894",
    "type": "article"
  },
  {
    "title": "Sama: a contig assembler with correctness guarantee",
    "doi": "https://doi.org/10.1186/s13015-025-00280-y",
    "publication_date": "2025-06-03",
    "publication_year": 2025,
    "authors": "Leena Salmela",
    "corresponding_authors": "Leena Salmela",
    "abstract": "In genome assembly the task is to reconstruct a genome based on sequencing reads. Current practical methods are based on heuristics which are hard to analyse and thus such analysis is not readily available. We present a model for estimating the probability of misassembly at each position of a de Bruijn graph based assembly. Unlike previous work, our model also takes into account missing data. We apply our model to produce contigs with correctness guarantee and correctness estimates for each position in the contigs. Our experiments show that when the coverage of k-mers is high enough, our method produces contigs with similar contiguity characteristics as state-of-the-art assemblers which are based on heuristic correction of the de Bruijn graph. Our model may have further applications in downstream analysis of contigs or in any analysis working directly on the de Bruijn graph.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410994480",
    "type": "article"
  },
  {
    "title": "Reconstructing rearrangement phylogenies of natural genomes",
    "doi": "https://doi.org/10.1186/s13015-025-00279-5",
    "publication_date": "2025-06-07",
    "publication_year": 2025,
    "authors": "Leonard Bohnenkämper; Jens Stoye; Daniel Doerr",
    "corresponding_authors": "",
    "abstract": "Abstract Background We study the classical problem of inferring ancestral genomes from a set of extant genomes under a given phylogeny, known as the Small Parsimony Problem (SPP). Genomes are represented as sequences of oriented markers, organized in one or more linear or circular chromosomes. Any marker may appear in several copies, without restriction on orientation or genomic location, known as the natural genomes model. Evolutionary events along the branches of the phylogeny encompass large scale rearrangements, including segmental inversions, translocations, gain and loss (DCJ-indel model). Even under simpler rearrangement models, such as the classical breakpoint model without duplicates, the SPP is computationally intractable. Nevertheless, the SPP for natural genomes under the DCJ-indel model has been studied recently, with limited success. Methods Building on prior work, we present a highly optimized ILP that is able to solve the SPP for sufficiently small phylogenies and gene families. A notable improvement w.r.t. the previous result is an optimized way of handling both circular and linear chromosomes. This is especially relevant to the SPP, since the chromosomal structure of ancestral genomes is unknown and the solution space for this chromosomal structure is typically large. Results We benchmark our method on simulated and real data. On simulated phylogenies we observe a considerable performance improvement on problems that include linear chromosomes. And even when the ground truth contains only one circular chromosome per genome, our method outperforms its predecessor due to its optimized handling of the solution space. The practical advantage becomes also visible in an analysis of seven Anopheles taxa.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411111758",
    "type": "article"
  },
  {
    "title": "Faster computation of left-bounded shortest unique substrings",
    "doi": "https://doi.org/10.1186/s13015-025-00287-5",
    "publication_date": "2025-06-20",
    "publication_year": 2025,
    "authors": "Larissa L M Aguiar; Felipe A. Louza",
    "corresponding_authors": "",
    "abstract": "Finding shortest unique substrings (SUS) is a fundamental problem in string processing with applications in bioinformatics. In this paper, we present an algorithm for solving a variant of the SUS problem, the left-bounded shortest unique substrings (LSUS). This variant is particularly important in applications such as PCR primer design. Our algorithm runs in O(n) time using 2n memory words plus n bytes for an input string of length n. Experimental results with real and artificial datasets show that our algorithm is the fastest alternative in practice, being two times faster (on the average) than related works, while using a similar peak memory footprint.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411473267",
    "type": "article"
  },
  {
    "title": "b-move: faster lossless approximate pattern matching in a run-length compressed index",
    "doi": "https://doi.org/10.1186/s13015-025-00281-x",
    "publication_date": "2025-08-12",
    "publication_year": 2025,
    "authors": "Lore Depuydt; Luca Renders; Simon Van de Vyver; Lennart Veys; Travis Gagie; Jan Fostier",
    "corresponding_authors": "",
    "abstract": "Due to the increasing availability of high-quality genome sequences, pan-genomes are gradually replacing single consensus reference genomes in many bioinformatics pipelines to better capture genetic diversity. Traditional bioinformatics tools using the FM-index face memory limitations with such large genome collections. Recent advancements in run-length compressed indices like Gagie et al.'s r-index and Nishimoto and Tabei's move structure, alleviate memory constraints but focus primarily on backward search for MEM-finding. Arakawa et al.'s br-index initiates complete approximate pattern matching using bidirectional search in run-length compressed space, but with significant computational overhead due to complex memory access patterns. We introduce b-move, a novel bidirectional extension of the move structure, enabling fast, cache-efficient, lossless approximate pattern matching in run-length compressed space. It achieves bidirectional character extensions up to 7 times faster than the br-index, closing the performance gap with FM-index-based alternatives. For locating occurrences, b-move performs ϕ and ϕ-1 operations up to 7 times faster than the br-index. At the same time, it maintains the favorable memory characteristics of the br-index, for example, all available complete E. coli genomes on NCBI's RefSeq collection can be compiled into a b-move index that fits into the RAM of a typical laptop. b-move proves practical and scalable for pan-genome indexing and querying. We provide a C++ implementation of b-move, supporting efficient lossless approximate pattern matching including locate functionality, available at https://github.com/biointec/b-move under the AGPL-3.0 license.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413110219",
    "type": "article"
  },
  {
    "title": "Finding maximum common contractions between phylogenetic networks",
    "doi": "https://doi.org/10.1186/s13015-025-00283-9",
    "publication_date": "2025-10-01",
    "publication_year": 2025,
    "authors": "Bertrand Marchand; Nadia Tahiri; Shohreh Golpaigani Fard; Olivier Tremblay-Savard; Manuel Lafond",
    "corresponding_authors": "Bertrand Marchand",
    "abstract": "In this paper, we lay the groundwork on the comparison of phylogenetic networks based on edge contractions and expansions as edit operations, as originally proposed by Robinson and Foulds to compare trees. We prove that these operations connect the space of all phylogenetic networks on the same set of leaves, even if we forbid contractions that create cycles. This allows to define an operational distance on this space, as the minimum number of contractions and expansions required to transform one network into another. We highlight the difference between this distance and the computation of the maximum common contraction between two networks. Given its ability to outline a common structure between them, which can provide valuable biological insights, we study the algorithmic aspects of the latter. We first prove that computing a maximum common contraction between two networks is NP-hard, even when the maximum degree, the size of the common contraction, or the number of leaves is bounded. We also provide lower bounds to the problem based on the Exponential-Time Hypothesis. Nonetheless, we do provide a polynomial-time algorithm for weakly galled trees, a generalization of galled trees.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414725606",
    "type": "article"
  },
  {
    "title": "Orthology and near-cographs in the context of phylogenetic networks",
    "doi": "https://doi.org/10.1186/s13015-025-00285-7",
    "publication_date": "2025-10-02",
    "publication_year": 2025,
    "authors": "Anna Lindeberg; Guillaume E. Scholz; Nicolas Wieseke; Marc Hellmuth",
    "corresponding_authors": "",
    "abstract": "Abstract Orthologous genes, which arise through speciation, play a key role in comparative genomics and functional inference. In particular, graph-based methods allow for the inference of orthology estimates without prior knowledge of the underlying gene or species trees. This results in orthology graphs, where each vertex represents a gene, and an edge exists between two vertices if the corresponding genes are estimated to be orthologs. Orthology graphs inferred under a tree-like evolutionary model must be cographs. However, real-world data often deviate from this property, either due to noise in the data, errors in inference methods or, simply, because evolution follows a network-like rather than a tree-like process. The latter, in particular, raises the question of whether and how orthology graphs can be derived from or, equivalently, are explained by phylogenetic networks. In this work, we study the constraints imposed on orthology graphs when the underlying evolutionary history follows a phylogenetic network instead of a tree. We show that any orthology graph can be represented by a sufficiently complex level-k network. However, such networks lack biologically meaningful constraints. In contrast, level-1 networks provide a simpler explanation, and we establish characterizations for level-1 explainable orthology graphs, i.e., those derived from level-1 evolutionary histories. To this end, we employ modular decomposition, a classical technique for studying graph structures. Specifically, an arbitrary graph is level-1 explainable if and only if each primitive subgraph is a near-cograph (a graph in which the removal of a single vertex results in a cograph). Additionally, we present a linear-time algorithm to recognize level-1 explainable orthology graphs and to construct a level-1 network that explains them, if such a network exists. Finally, we demonstrate the close relationship of level-1 explainable orthology graphs to the substitution operation, weakly chordal and perfect graphs, as well as graphs with twin-width at most 2.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414746528",
    "type": "article"
  },
  {
    "title": "CHSMiner: a GUI tool to identify chromosomal homologous segments",
    "doi": "https://doi.org/10.1186/1748-7188-4-2",
    "publication_date": "2009-01-15",
    "publication_year": 2009,
    "authors": "Zhen Wang; Guohui Ding; Zhonghao Yu; Lei Liu; Yixue Li",
    "corresponding_authors": "",
    "abstract": "Abstract Background The identification of chromosomal homologous segments (CHS) within and between genomes is essential for comparative genomics. Various processes including insertion/deletion and inversion could cause the degeneration of CHSs. Results Here we present a Java software CHSMiner that detects CHSs based on shared gene content alone. It implements fast greedy search algorithm and rigorous statistical validation, and its friendly graphical interface allows interactive visualization of the results. We tested the software on both simulated and biological realistic data and compared its performance with similar existing software and data source. Conclusion CHSMiner is characterized by its integrated workflow, fast speed and convenient usage. It will be useful for both experimentalists and bioinformaticians interested in the structure and evolution of genomes.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2031992856",
    "type": "article"
  },
  {
    "title": "A markov classification model for metabolic pathways",
    "doi": "https://doi.org/10.1186/1748-7188-5-10",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Timothy Hancock; Hiroshi Mamitsuka",
    "corresponding_authors": "",
    "abstract": "This paper considers the problem of identifying pathways through metabolic networks that relate to a specific biological response. Our proposed model, HME3M, first identifies frequently traversed network paths using a Markov mixture model. Then by employing a hierarchical mixture of experts, separate classifiers are built using information specific to each path and combined into an ensemble prediction for the response. We compared the performance of HME3M with logistic regression and support vector machines (SVM) for both simulated pathways and on two metabolic networks, glycolysis and the pentose phosphate pathway for Arabidopsis thaliana. We use AltGenExpress microarray data and focus on the pathway differences in the developmental stages and stress responses of Arabidopsis. The results clearly show that HME3M outperformed the comparison methods in the presence of increasing network complexity and pathway noise. Furthermore an analysis of the paths identified by HME3M for each metabolic network confirmed known biological responses of Arabidopsis. This paper clearly shows HME3M to be an accurate and robust method for classifying metabolic pathways. HME3M is shown to outperform all comparison methods and further is capable of identifying known biologically active pathways within microarray data.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2171040830",
    "type": "article"
  },
  {
    "title": "Linear-time protein 3-D structure searching with insertions and deletions",
    "doi": "https://doi.org/10.1186/1748-7188-5-7",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Tetsuo Shibuya; Jesper Jansson; Kunihiko Sadakane",
    "corresponding_authors": "Tetsuo Shibuya",
    "abstract": "Two biomolecular 3-D structures are said to be similar if the RMSD (root mean square deviation) between the two molecules' sequences of 3-D coordinates is less than or equal to some given constant bound. Tools for searching for similar structures in biomolecular 3-D structure databases are becoming increasingly important in the structural biology of the post-genomic era. We consider an important, fundamental problem of reporting all substructures in a 3-D structure database of chain molecules (such as proteins) which are similar to a given query 3-D structure, with consideration of indels (i.e., insertions and deletions). This problem has been believed to be very difficult but its exact computational complexity has not been known. In this paper, we first prove that the problem in unbounded dimensions is NP-hard. We then propose a new algorithm that dramatically improves the average-case time complexity of the problem in 3-D in case the number of indels k is bounded by a constant. Our algorithm solves the above problem for a query of size m and a database of size N in average-case O(N) time, whereas the time complexity of the previously best algorithm was O(Nmk+1). Our results show that although the problem of searching for similar structures in a database based on the RMSD measure with indels is NP-hard in the case of unbounded dimensions, it can be solved in 3-D by a simple average-case linear time algorithm when the number of indels is bounded by a constant.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2785935484",
    "type": "article"
  },
  {
    "title": "Linear time minimum segmentation enables scalable founder reconstruction",
    "doi": "https://doi.org/10.1186/s13015-019-0147-6",
    "publication_date": "2019-05-17",
    "publication_year": 2019,
    "authors": "Tuukka Norri; Bastien Cazaux; Dmitry Kosolobov; Veli Mäkinen",
    "corresponding_authors": "",
    "abstract": "We study a preprocessing routine relevant in pan-genomic analyses: consider a set of aligned haplotype sequences of complete human chromosomes. Due to the enormous size of such data, one would like to represent this input set with a few founder sequences that retain as well as possible the contiguities of the original sequences. Such a smaller set gives a scalable way to exploit pan-genomic information in further analyses (e.g. read alignment and variant calling). Optimizing the founder set is an NP-hard problem, but there is a segmentation formulation that can be solved in polynomial time, defined as follows. Given a threshold L and a set $${\\mathcal {R}} = \\{R_1, \\ldots , R_m\\}$$ of m strings (haplotype sequences), each having length n, the minimum segmentation problem for founder reconstruction is to partition [1, n] into set P of disjoint segments such that each segment $$[a,b] \\in P$$ has length at least L and the number $$d(a,b)=|\\{R_i[a,b] :1\\le i \\le m\\}|$$ of distinct substrings at segment [a, b] is minimized over $$[a,b] \\in P$$ . The distinct substrings in the segments represent founder blocks that can be concatenated to form $$\\max \\{ d(a,b) :[a,b] \\in P \\}$$ founder sequences representing the original $${\\mathcal {R}}$$ such that crossovers happen only at segment boundaries. We give an O(mn) time (i.e. linear time in the input size) algorithm to solve the minimum segmentation problem for founder reconstruction, improving over an earlier $$O(mn^2)$$ . Our improvement enables to apply the formulation on an input of thousands of complete human chromosomes. We implemented the new algorithm and give experimental evidence on its practicality. The implementation is available in https://github.com/tsnorri/founder-sequences .",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2946853996",
    "type": "article"
  },
  {
    "title": "Detecting conserved protein complexes using a dividing-and-matching algorithm and unequally lenient criteria for network comparison",
    "doi": "https://doi.org/10.1186/s13015-015-0053-5",
    "publication_date": "2015-06-29",
    "publication_year": 2015,
    "authors": "Wei Peng; Jianxin Wang; Fang‐Xiang Wu; Yi Pan",
    "corresponding_authors": "",
    "abstract": "The increase of protein-protein interaction (PPI) data of different species makes it possible to identify common subnetworks (conserved protein complexes) across species via local alignment of their PPI networks, which benefits us to study biological evolution. Local alignment algorithms compare PPI network of different species at both protein sequence and network structure levels. For computational and biological reasons, it is hard to find common subnetworks with strict similar topology from two input PPI networks. Consequently some methods introduce less strict criteria for topological similarity. However those methods fail to consider the differences of the two input networks and adopt equally lenient criteria on them. In this work, a new dividing-and-matching-based method, namely UEDAMAlign is proposed to detect conserved protein complexes. This method firstly uses known protein complexes or computational methods to divide one of the two input PPI networks into subnetworks and then maps the proteins in these subnetworks to the other PPI network to get their homologous proteins. After that, UEDAMAlign conducts unequally lenient criteria on the two input networks to find common connected components from the proteins in the subnetworks and their homologous proteins in the other network. We carry out network alignments between S. cerevisiae and D. melanogaster, H. sapiens and D. melanogaster, respectively. Comparisons are made between other six existing methods and UEDAMAlign. The experimental results show that UEDAMAlign outperforms other existing methods in recovering conserved protein complexes that both match well with known protein complexes and have similar functions.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1039758599",
    "type": "article"
  },
  {
    "title": "RNA-RNA interaction prediction using genetic algorithm",
    "doi": "https://doi.org/10.1186/1748-7188-9-17",
    "publication_date": "2014-06-29",
    "publication_year": 2014,
    "authors": "Soheila Montaseri; Fatemeh Zare‐Mirakabad; Nasrollah Moghadam Charkari",
    "corresponding_authors": "Fatemeh Zare‐Mirakabad",
    "abstract": "RNA-RNA interaction plays an important role in the regulation of gene expression and cell development. In this process, an RNA molecule prohibits the translation of another RNA molecule by establishing stable interactions with it. In the RNA-RNA interaction prediction problem, two RNA sequences are given as inputs and the goal is to find the optimal secondary structure of two RNAs and between them. Some different algorithms have been proposed to predict RNA-RNA interaction structure. However, most of them suffer from high computational time.In this paper, we introduce a novel genetic algorithm called GRNAs to predict the RNA-RNA interaction. The proposed algorithm is performed on some standard datasets with appropriate accuracy and lower time complexity in comparison to the other state-of-the-art algorithms. In the proposed algorithm, each individual is a secondary structure of two interacting RNAs. The minimum free energy is considered as a fitness function for each individual. In each generation, the algorithm is converged to find the optimal secondary structure (minimum free energy structure) of two interacting RNAs by using crossover and mutation operations.This algorithm is properly employed for joint secondary structure prediction. The results achieved on a set of known interacting RNA pairs are compared with the other related algorithms and the effectiveness and validity of the proposed algorithm have been demonstrated. It has been shown that time complexity of the algorithm in each iteration is as efficient as the other approaches.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2133906676",
    "type": "article"
  },
  {
    "title": "Gene Ontology consistent protein function prediction: the FALCON algorithm applied to six eukaryotic genomes",
    "doi": "https://doi.org/10.1186/1748-7188-8-10",
    "publication_date": "2013-03-27",
    "publication_year": 2013,
    "authors": "Yiannis Kourmpetis; Aalt D. J. van Dijk; Cajo J. F. ter Braak",
    "corresponding_authors": "",
    "abstract": ": Gene Ontology (GO) is a hierarchical vocabulary for the description of biological functions and locations, often employed by computational methods for protein function prediction. Due to the structure of GO, function predictions can be self- contradictory. For example, a protein may be predicted to belong to a detailed functional class, but not in a broader class that, due to the vocabulary structure, includes the predicted one.We present a novel discrete optimization algorithm called Functional Annotation with Labeling CONsistency (FALCON) that resolves such contradictions. The GO is modeled as a discrete Bayesian Network. For any given input of GO term membership probabilities, the algorithm returns the most probable GO term assignments that are in accordance with the Gene Ontology structure. The optimization is done using the Differential Evolution algorithm. Performance is evaluated on simulated and also real data from Arabidopsis thaliana showing improvement compared to related approaches. We finally applied the FALCON algorithm to obtain genome-wide function predictions for six eukaryotic species based on data provided by the CAFA (Critical Assessment of Function Annotation) project.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2134781696",
    "type": "article"
  },
  {
    "title": "GAML: genome assembly by maximum likelihood",
    "doi": "https://doi.org/10.1186/s13015-015-0052-6",
    "publication_date": "2015-06-03",
    "publication_year": 2015,
    "authors": "Vladimír Boža; Broňa Brejová; Tomáš Vinař",
    "corresponding_authors": "",
    "abstract": "Resolution of repeats and scaffolding of shorter contigs are critical parts of genome assembly. Modern assemblers usually perform such steps by heuristics, often tailored to a particular technology for producing paired or long reads. We propose a new framework that allows systematic combination of diverse sequencing datasets into a single assembly. We achieve this by searching for an assembly with the maximum likelihood in a probabilistic model capturing error rate, insert lengths, and other characteristics of the sequencing technology used to produce each dataset. We have implemented a prototype genome assembler GAML that can use any combination of insert sizes with Illumina or 454 reads, as well as PacBio reads. Our experiments show that we can assemble short genomes with N50 sizes and error rates comparable to ALLPATHS-LG or Cerulean. While ALLPATHS-LG and Cerulean require each a specific combination of datasets, GAML works on any combination. We have introduced a new probabilistic approach to genome assembly and demonstrated that this approach can lead to superior results when used to combine diverse set of datasets from different sequencing technologies. Data and software is available at http://compbio.fmph.uniba.sk/gaml .",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2148048486",
    "type": "article"
  },
  {
    "title": "Efficient procedures for the numerical simulation of mid-size RNA kinetics",
    "doi": "https://doi.org/10.1186/1748-7188-7-24",
    "publication_date": "2012-09-07",
    "publication_year": 2012,
    "authors": "Iddo Aviram; Ilia Veltman; Alexander Churkin; Danny Barash",
    "corresponding_authors": "Danny Barash",
    "abstract": "Methods for simulating the kinetic folding of RNAs by numerically solving the chemical master equation have been developed since the late 90's, notably the programs Kinfold and Treekin with Barriers that are available in the Vienna RNA package. Our goal is to formulate extensions to the algorithms used, starting from the Gillespie algorithm, that will allow numerical simulations of mid-size (~ 60-150 nt) RNA kinetics in some practical cases where numerous distributions of folding times are desired. These extensions can contribute to analyses and predictions of RNA folding in biologically significant problems.By describing in a particular way the reduction of numerical simulations of RNA folding kinetics into the Gillespie stochastic simulation algorithm for chemical reactions, it is possible to formulate extensions to the basic algorithm that will exploit memoization and parallelism for efficient computations. These can be used to advance forward from the small examples demonstrated to larger examples of biological interest.The implementation that is described and used for the Gillespie algorithm is freely available by contacting the authors, noting that the efficient procedures suggested may also be applicable along with Vienna's Kinfold.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2153766974",
    "type": "article"
  },
  {
    "title": "MORPH-PRO: a novel algorithm and web server for protein morphing",
    "doi": "https://doi.org/10.1186/1748-7188-8-19",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Natalie Castellana; Andrey Lushnikov; Piotr Rotkiewicz; Natasha Sefcovic; Pavel A. Pevzner; Adam Godzik; Kira Vyatkina",
    "corresponding_authors": "",
    "abstract": "Proteins are known to be dynamic in nature, changing from one conformation to another while performing vital cellular tasks. It is important to understand these movements in order to better understand protein function. At the same time, experimental techniques provide us with only single snapshots of the whole ensemble of available conformations. Computational protein morphing provides a visualization of a protein structure transitioning from one conformation to another by producing a series of intermediate conformations.We present a novel, efficient morphing algorithm, Morph-Pro based on linear interpolation. We also show that apart from visualization, morphing can be used to provide plausible intermediate structures. We test this by using the intermediate structures of a c-Jun N-terminal kinase (JNK1) conformational change in a virtual docking experiment. The structures are shown to dock with higher score to known JNK1-binding ligands than structures solved using X-Ray crystallography. This experiment demonstrates the potential applications of the intermediate structures in modeling or virtual screening efforts.Visualization of protein conformational changes is important for characterization of protein function. Furthermore, the intermediate structures produced by our algorithm are good approximations to true structures. We believe there is great potential for these computationally predicted structures in protein-ligand docking experiments and virtual screening. The Morph-Pro web server can be accessed at http://morph-pro.bioinf.spbau.ru.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2171605109",
    "type": "article"
  },
  {
    "title": "Partially local three-way alignments and the sequence signatures of mitochondrial genome rearrangements",
    "doi": "https://doi.org/10.1186/s13015-017-0113-0",
    "publication_date": "2017-08-23",
    "publication_year": 2017,
    "authors": "Marwa Al Arab; Matthias Bernt; Christian Höner zu Siederdissen; Kifah Tout; Peter F. Stadler",
    "corresponding_authors": "Marwa Al Arab",
    "abstract": "Genomic DNA frequently undergoes rearrangement of the gene order that can be localized by comparing the two DNA sequences. In mitochondrial genomes different mechanisms are likely at work, at least some of which involve the duplication of sequence around the location of the apparent breakpoints. We hypothesize that these different mechanisms of genome rearrangement leave distinctive sequence footprints. In order to study such effects it is important to locate the breakpoint positions with precision.We define a partially local sequence alignment problem that assumes that following a rearrangement of a sequence F, two fragments L, and R are produced that may exactly fit together to match F, leave a gap of deleted DNA between L and R, or overlap with each other. We show that this alignment problem can be solved by dynamic programming in cubic space and time. We apply the new method to evaluate rearrangements of animal mitogenomes and find that a surprisingly large fraction of these events involved local sequence duplications.The partially local sequence alignment method is an effective way to investigate the mechanism of genomic rearrangement events. While applied here only to mitogenomes there is no reason why the method could not be used to also consider rearrangements in nuclear genomes.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2749451544",
    "type": "article"
  },
  {
    "title": "Differentially mutated subnetworks discovery",
    "doi": "https://doi.org/10.1186/s13015-019-0146-7",
    "publication_date": "2019-03-30",
    "publication_year": 2019,
    "authors": "Morteza Chalabi Hajkarim; Eli Upfal; Fabio Vandin",
    "corresponding_authors": "Fabio Vandin",
    "abstract": "We study the problem of identifying differentially mutated subnetworks of a large gene–gene interaction network, that is, subnetworks that display a significant difference in mutation frequency in two sets of cancer samples. We formally define the associated computational problem and show that the problem is NP-hard. We propose a novel and efficient algorithm, called DAMOKLE, to identify differentially mutated subnetworks given genome-wide mutation data for two sets of cancer samples. We prove that DAMOKLE identifies subnetworks with statistically significant difference in mutation frequency when the data comes from a reasonable generative model, provided enough samples are available. We test DAMOKLE on simulated and real data, showing that DAMOKLE does indeed find subnetworks with significant differences in mutation frequency and that it provides novel insights into the molecular mechanisms of the disease not revealed by standard methods.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2942397809",
    "type": "article"
  },
  {
    "title": "Reconstruction of time-consistent species trees",
    "doi": "https://doi.org/10.1186/s13015-020-00175-0",
    "publication_date": "2020-08-20",
    "publication_year": 2020,
    "authors": "Manuel Lafond; Marc Hellmuth",
    "corresponding_authors": "",
    "abstract": "Abstract Background The history of gene families—which are equivalent to event-labeled gene trees—can to some extent be reconstructed from empirically estimated evolutionary event-relations containing pairs of orthologous, paralogous or xenologous genes. The question then arises as whether inferred event-labeled gene trees are “biologically feasible” which is the case if one can find a species tree with which the gene tree can be reconciled in a time-consistent way. Results In this contribution, we consider event-labeled gene trees that contain speciations, duplications as well as horizontal gene transfer (HGT) and we assume that the species tree is unknown. Although many problems become NP-hard as soon as HGT and time-consistency are involved, we show, in contrast, that the problem of finding a time-consistent species tree for a given event-labeled gene can be solved in polynomial-time. We provide a cubic-time algorithm to decide whether a “time-consistent” species tree for a given event-labeled gene tree exists and, in the affirmative case, to construct the species tree within the same time-complexity.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3080790984",
    "type": "article"
  },
  {
    "title": "On the number of genomic pacemakers: a geometric approach",
    "doi": "https://doi.org/10.1186/s13015-014-0026-0",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Sagi Snir",
    "corresponding_authors": "Sagi Snir",
    "abstract": "The universal pacemaker (UPM) model extends the classical molecular clock (MC) model, by allowing each gene, in addition to its individual intrinsic rate as in the MC, to accelerate or decelerate according to the universal pacemaker. Under UPM, the relative evolutionary rates of all genes remain nearly constant whereas the absolute rates can change arbitrarily. It was shown on several taxa groups spanning the entire tree of life that the UPM model describes the evolutionary process better than the MC model. In this work we provide a natural generalization to the UPM model that we denote multiple pacemakers (MPM). Under the MPM model every gene is still affected by a single pacemaker, however the number of pacemakers is not confined to one. Such a model induces a partition over the gene set where all the genes in one part are affected by the same pacemaker and task is to identify the pacemaker partition, or in other words, finding for each gene its associated pacemaker. We devise a novel heuristic procedure, relying on statistical and geometrical tools, to solve the problem and demonstrate by simulation that this approach can cope satisfactorily with considerable noise and realistic problem sizes. We applied this procedure to a set of over 2000 genes in 100 prokaryotes and demonstrated the significant existence of two pacemakers.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2116130550",
    "type": "article"
  },
  {
    "title": "Computing the skewness of the phylogenetic mean pairwise distance in linear time",
    "doi": "https://doi.org/10.1186/1748-7188-9-15",
    "publication_date": "2014-06-14",
    "publication_year": 2014,
    "authors": "Constantinos Tsirogiannis; Brody Sandel",
    "corresponding_authors": "",
    "abstract": "The phylogenetic Mean Pairwise Distance (MPD) is one of the most popular measures for computing the phylogenetic distance between a given group of species. More specifically, for a phylogenetic tree and for a set of species R represented by a subset of the leaf nodes of , the MPD of R is equal to the average cost of all possible simple paths in that connect pairs of nodes in R. Among other phylogenetic measures, the MPD is used as a tool for deciding if the species of a given group R are closely related. To do this, it is important to compute not only the value of the MPD for this group but also the expectation, the variance, and the skewness of this metric. Although efficient algorithms have been developed for computing the expectation and the variance the MPD, there has been no approach so far for computing the skewness of this measure. In the present work we describe how to compute the skewness of the MPD on a tree optimally, in Θ(n) time; here n is the size of the tree . So far this is the first result that leads to an exact, let alone efficient, computation of the skewness for any popular phylogenetic distance measure. Moreover, we show how we can compute in Θ(n) time several interesting quantities in , that can be possibly used as building blocks for computing efficiently the skewness of other phylogenetic measures. The optimal computation of the skewness of the MPD that is outlined in this work provides one more tool for studying the phylogenetic relatedness of species in large phylogenetic trees. Until now this has been infeasible, given that traditional techniques for computing the skewness are inefficient and based on inexact resampling.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2119776447",
    "type": "article"
  },
  {
    "title": "An online peak extraction algorithm for ion mobility spectrometry data",
    "doi": "https://doi.org/10.1186/s13015-015-0045-5",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Dominik Kopczynski; Sven Rahmann",
    "corresponding_authors": "",
    "abstract": "Ion mobility (IM) spectrometry (IMS), coupled with multi-capillary columns (MCCs), has been gaining importance for biotechnological and medical applications because of its ability to detect and quantify volatile organic compounds (VOC) at low concentrations in the air or in exhaled breath at ambient pressure and temperature. Ongoing miniaturization of spectrometers creates the need for reliable data analysis on-the-fly in small embedded low-power devices. We present the first fully automated online peak extraction method for MCC/IMS measurements consisting of several thousand individual spectra. Each individual spectrum is processed as it arrives, removing the need to store the measurement before starting the analysis, as is currently the state of the art. Thus the analysis device can be an inexpensive low-power system such as the Raspberry Pi. The key idea is to extract one-dimensional peak models (with four parameters) from each spectrum and then merge these into peak chains and finally two-dimensional peak models. We describe the different algorithmic steps in detail and evaluate the online method against state-of-the-art peak extraction methods.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2125573977",
    "type": "article"
  },
  {
    "title": "Protein (multi-)location prediction: using location inter-dependencies in a probabilistic framework",
    "doi": "https://doi.org/10.1186/1748-7188-9-8",
    "publication_date": "2014-03-19",
    "publication_year": 2014,
    "authors": "Ramanuja Simha; Hagit Shatkay",
    "corresponding_authors": "",
    "abstract": "Knowing the location of a protein within the cell is important for understanding its function, role in biological processes, and potential use as a drug target. Much progress has been made in developing computational methods that predict single locations for proteins. Most such methods are based on the over-simplifying assumption that proteins localize to a single location. However, it has been shown that proteins localize to multiple locations. While a few recent systems attempt to predict multiple locations of proteins, their performance leaves much room for improvement. Moreover, they typically treat locations as independent and do not attempt to utilize possible inter-dependencies among locations. Our hypothesis is that directly incorporating inter-dependencies among locations into both the classifier-learning and the prediction process can improve location prediction performance.We present a new method and a preliminary system we have developed that directly incorporates inter-dependencies among locations into the location-prediction process of multiply-localized proteins. Our method is based on a collection of Bayesian network classifiers, where each classifier is used to predict a single location. Learning the structure of each Bayesian network classifier takes into account inter-dependencies among locations, and the prediction process uses estimates involving multiple locations. We evaluate our system on a dataset of single- and multi-localized proteins (the most comprehensive protein multi-localization dataset currently available, derived from the DBMLoc dataset). Our results, obtained by incorporating inter-dependencies, are significantly higher than those obtained by classifiers that do not use inter-dependencies. The performance of our system on multi-localized proteins is comparable to a top performing system (YLoc+), without being restricted only to location-combinations present in the training set.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2177760071",
    "type": "article"
  },
  {
    "title": "Designing minimal microbial strains of desired functionality using a genetic algorithm",
    "doi": "https://doi.org/10.1186/s13015-015-0060-6",
    "publication_date": "2015-12-01",
    "publication_year": 2015,
    "authors": "Govind Nair; Christian Jungreuthmayer; Michael Hanscho; Jürgen Zanghellini",
    "corresponding_authors": "",
    "abstract": "The rational, in silico prediction of gene-knockouts to turn organisms into efficient cell factories is an essential and computationally challenging task in metabolic engineering. Elementary flux mode analysis in combination with constraint minimal cut sets is a particularly powerful method to identify optimal engineering targets, which will force an organism into the desired metabolic state. Given an engineering objective, it is theoretically possible, although computationally impractical, to find the best minimal intervention strategies. We developed a genetic algorithm (GA-MCS) to quickly find many (near) optimal intervention strategies while overcoming the above mentioned computational burden. We tested our algorithm on Escherichia coli metabolic networks of three different sizes to find intervention strategies satisfying three different engineering objectives. We show that GA-MCS finds all practically relevant targets for any (non)-linear engineering objective. Our algorithm also found solutions comparable to previously published results. We show that for large networks optimal solutions are found within a fraction of the time used for a complete enumeration.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2213285950",
    "type": "article"
  },
  {
    "title": "An exact algorithm for finding cancer driver somatic genome alterations: the weighted mutually exclusive maximum set cover problem",
    "doi": "https://doi.org/10.1186/s13015-016-0073-9",
    "publication_date": "2016-05-04",
    "publication_year": 2016,
    "authors": "Songjian Lu; Gunasheil Mandava; Gaibo Yan; Xinghua Lu",
    "corresponding_authors": "Songjian Lu",
    "abstract": "The mutual exclusivity of somatic genome alterations (SGAs), such as somatic mutations and copy number alterations, is an important observation of tumors and is widely used to search for cancer signaling pathways or SGAs related to tumor development. However, one problem with current methods that use mutual exclusivity is that they are not signal-based; another problem is that they use heuristic algorithms to handle the NP-hard problems, which cannot guarantee to find the optimal solutions of their models.In this study, we propose a novel signal-based method that utilizes the intrinsic relationship between SGAs on signaling pathways and expression changes of downstream genes regulated by pathways to identify cancer signaling pathways using the mutually exclusive property. We also present a relatively efficient exact algorithm that can guarantee to obtain the optimal solution of the new computational model.We have applied our new model and exact algorithm to the breast cancer data. The results reveal that our new approach increases the capability of finding better solutions in the application of cancer research. Our new exact algorithm has a time complexity of [Formula: see text](Note: Following the recent convention, we use a star * to represent that the polynomial part of the time complexity is neglected), which has solved the NP-hard problem of our model efficiently.Our new method and algorithm can discover the true causes behind the phenotypes, such as what SGA events lead to abnormality of the cell cycle or make the cell metastasis lose control in tumors; thus, it identifies the target candidates for precision (or target) therapeutics.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2347032301",
    "type": "article"
  },
  {
    "title": "Dollo-CDP: a polynomial-time algorithm for the clade-constrained large Dollo parsimony problem",
    "doi": "https://doi.org/10.1186/s13015-023-00249-9",
    "publication_date": "2024-01-08",
    "publication_year": 2024,
    "authors": "Jun Yan Dai; Tobias Rubel; Yunheng Han; Erin K. Molloy",
    "corresponding_authors": "",
    "abstract": "Abstract The last decade of phylogenetics has seen the development of many methods that leverage constraints plus dynamic programming. The goal of this algorithmic technique is to produce a phylogeny that is optimal with respect to some objective function and that lies within a constrained version of tree space. The popular species tree estimation method ASTRAL, for example, returns a tree that (1) maximizes the quartet score computed with respect to the input gene trees and that (2) draws its branches (bipartitions) from the input constraint set. This technique has yet to be used for parsimony problems where the input are binary characters, sometimes with missing values. Here, we introduce the clade-constrained character parsimony problem and present an algorithm that solves this problem for the Dollo criterion score in $$O(|\\Sigma |^{3.726}(n+k) + |\\Sigma |^{1.726}nk)$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mo>|</mml:mo> <mml:mi>Σ</mml:mi> <mml:msup> <mml:mo>|</mml:mo> <mml:mrow> <mml:mn>3.726</mml:mn> </mml:mrow> </mml:msup> <mml:mrow> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo> </mml:mrow> <mml:mo>+</mml:mo> <mml:mo>|</mml:mo> <mml:mi>Σ</mml:mi> <mml:msup> <mml:mo>|</mml:mo> <mml:mrow> <mml:mn>1.726</mml:mn> </mml:mrow> </mml:msup> <mml:mi>n</mml:mi> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> time, where n is the number of leaves, k is the number of characters, and $$\\Sigma$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>Σ</mml:mi> </mml:math> is the set of clades used as constraints. Dollo parsimony, which requires traits/mutations to be gained at most once but allows them to be lost any number of times, is widely used for tumor phylogenetics as well as species phylogenetics, for example analyses of low-homoplasy retroelement insertions across the vertebrate tree of life. This motivated us to implement our algorithm in a software package, called Dollo-CDP, and evaluate its utility for analyzing retroelement insertion presence / absence patterns for bats, birds, toothed whales as well as simulated data. Our results show that Dollo-CDP can improve upon heuristic search from a single starting tree, often recovering a better scoring tree. Moreover, Dollo-CDP scales to data sets with much larger numbers of taxa than branch-and-bound while still having an optimality guarantee, albeit a more restricted one. Lastly, we show that our algorithm for Dollo parsimony can easily be adapted to Camin-Sokal parsimony but not Fitch parsimony.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390675502",
    "type": "article"
  },
  {
    "title": "Unifying duplication episode clustering and gene-species mapping inference",
    "doi": "https://doi.org/10.1186/s13015-024-00252-8",
    "publication_date": "2024-02-14",
    "publication_year": 2024,
    "authors": "Paweł Górecki; Natalia Rutecka; Agnieszka Mykowiecka; Jarosław Paszek",
    "corresponding_authors": "Paweł Górecki",
    "abstract": "Abstract We present a novel problem, called MetaEC, which aims to infer gene-species assignments in a collection of partially leaf-labeled gene trees labels by minimizing the size of duplication episode clustering (EC). This problem is particularly relevant in metagenomics, where incomplete data often poses a challenge in the accurate reconstruction of gene histories. To solve MetaEC, we propose a polynomial time dynamic programming (DP) formulation that verifies the existence of a set of duplication episodes from a predefined set of episode candidates. In addition, we design a method to infer distributions of gene-species mappings. We then demonstrate how to use DP to design an algorithm that solves MetaEC. Although the algorithm is exponential in the worst case, we introduce a heuristic modification of the algorithm that provides a solution with the knowledge that it is exact. To evaluate our method, we perform two computational experiments on simulated and empirical data containing whole genome duplication events, showing that our algorithm is able to accurately infer the corresponding events.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391821711",
    "type": "article"
  },
  {
    "title": "SparseRNAfolD: optimized sparse RNA pseudoknot-free folding with dangle consideration",
    "doi": "https://doi.org/10.1186/s13015-024-00256-4",
    "publication_date": "2024-03-03",
    "publication_year": 2024,
    "authors": "Mateo Gray; Sebastian Will; Hosna Jabbari",
    "corresponding_authors": "",
    "abstract": "Abstract Motivation Computational RNA secondary structure prediction by free energy minimization is indispensable for analyzing structural RNAs and their interactions. These methods find the structure with the minimum free energy (MFE) among exponentially many possible structures and have a restrictive time and space complexity ( $$O(n^3)$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msup> <mml:mi>n</mml:mi> <mml:mn>3</mml:mn> </mml:msup> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> time and $$O(n^2)$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msup> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn> </mml:msup> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> space for pseudoknot-free structures) for longer RNA sequences. Furthermore, accurate free energy calculations, including dangle contributions can be difficult and costly to implement, particularly when optimizing for time and space requirements. Results Here we introduce a fast and efficient sparsified MFE pseudoknot-free structure prediction algorithm, SparseRNAFolD, that utilizes an accurate energy model that accounts for dangle contributions. While the sparsification technique was previously employed to improve the time and space complexity of a pseudoknot-free structure prediction method with a realistic energy model, SparseMFEFold, it was not extended to include dangle contributions due to the complexity of computation. This may come at the cost of prediction accuracy. In this work, we compare three different sparsified implementations for dangle contributions and provide pros and cons of each method. As well, we compare our algorithm to LinearFold, a linear time and space algorithm, where we find that in practice, SparseRNAFolD has lower memory consumption across all lengths of sequence and a faster time for lengths up to 1000 bases. Conclusion Our SparseRNAFolD algorithm is an MFE-based algorithm that guarantees optimality of result and employs the most general energy model, including dangle contributions. We provide a basis for applying dangles to sparsified recursion in a pseudoknot-free model that has the potential to be extended to pseudoknots.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392343346",
    "type": "article"
  },
  {
    "title": "Suffix sorting via matching statistics",
    "doi": "https://doi.org/10.1186/s13015-023-00245-z",
    "publication_date": "2024-03-12",
    "publication_year": 2024,
    "authors": "Zsuzsanna Lipták; Francesco Masillo; Simon J. Puglisi",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392717017",
    "type": "article"
  },
  {
    "title": "Infrared: a declarative tree decomposition-powered framework for bioinformatics",
    "doi": "https://doi.org/10.1186/s13015-024-00258-2",
    "publication_date": "2024-03-16",
    "publication_year": 2024,
    "authors": "Hua-Ting Yao; Bertrand Marchand; Sarah J. Berkemer; Yann Ponty; Sebastian Will",
    "corresponding_authors": "",
    "abstract": "Many bioinformatics problems can be approached as optimization or controlled sampling tasks, and solved exactly and efficiently using Dynamic Programming (DP). However, such exact methods are typically tailored towards specific settings, complex to develop, and hard to implement and adapt to problem variations.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392885478",
    "type": "article"
  },
  {
    "title": "On the maximal cliques in c-max-tolerance graphs and their application in clustering molecular sequences",
    "doi": "https://doi.org/10.1186/1748-7188-1-9",
    "publication_date": "2006-05-31",
    "publication_year": 2006,
    "authors": "Katharina Lehmann; Michael Kaufmann; Stephan Steigele; Kay Nieselt",
    "corresponding_authors": "",
    "abstract": "Abstract Given a set S of n locally aligned sequences, it is a needed prerequisite to partition it into groups of very similar sequences to facilitate subsequent computations, such as the generation of a phylogenetic tree. This article introduces a new method of clustering which partitions S into subsets such that the overlap of each pair of sequences within a subset is at least a given percentage c of the lengths of the two sequences. We show that this problem can be reduced to finding all maximal cliques in a special kind of max-tolerance graph which we call a c-max-tolerance graph. Previously we have shown that finding all maximal cliques in general max-tolerance graphs can be done efficiently in O ( n 3 + out ). Here, using a new kind of sweep-line algorithm, we show that the restriction to c -max-tolerance graphs yields a better runtime of O ( n 2 log n + out ). Furthermore, we present another algorithm which is much easier to implement, and though theoretically slower than the first one, is still running in polynomial time. We then experimentally analyze the number and structure of all maximal cliques in a c -max-tolerance graph, depending on the chosen c -value. We apply our simple algorithm to artificial and biological data and we show that this implementation is much faster than the well-known application Cliquer. By introducing a new heuristic that uses the set of all maximal cliques to partition S , we finally show that the computed partition gives a reasonable clustering for biological data sets.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2122731139",
    "type": "article"
  },
  {
    "title": "Transcriptional regulatory network discovery via multiple method integration: application to e. coli K12",
    "doi": "https://doi.org/10.1186/1748-7188-2-2",
    "publication_date": "2007-03-30",
    "publication_year": 2007,
    "authors": "Jingjun Sun; Kağan Tuncay; Alaa Abi Haidar; Lisa Ensman; Frank Stanley; Michael Trelinski; P. Ortoleva",
    "corresponding_authors": "",
    "abstract": "Transcriptional regulatory network (TRN) discovery from one method (e.g. microarray analysis, gene ontology, phylogenic similarity) does not seem feasible due to lack of sufficient information, resulting in the construction of spurious or incomplete TRNs. We develop a methodology, TRND, that integrates a preliminary TRN, microarray data, gene ontology and phylogenic similarity to accurately discover TRNs and apply the method to E. coli K12. The approach can easily be extended to include other methodologies. Although gene ontology and phylogenic similarity have been used in the context of gene-gene networks, we show that more information can be extracted when gene-gene scores are transformed to gene-transcription factor (TF) scores using a preliminary TRN. This seems to be preferable over the construction of gene-gene interaction networks in light of the observed fact that gene expression and activity of a TF made of a component encoded by that gene is often out of phase. TRND multi-method integration is found to be facilitated by the use of a Bayesian framework for each method derived from its individual scoring measure and a training set of gene/TF regulatory interactions. The TRNs we construct are in better agreement with microarray data. The number of gene/TF interactions we discover is actually double that of existing networks.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2008746869",
    "type": "article"
  },
  {
    "title": "Protein sequence and structure alignments within one framework",
    "doi": "https://doi.org/10.1186/1748-7188-3-4",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Gundolf Schenk; Thomas A. Margraf; Andrew E. Torda",
    "corresponding_authors": "",
    "abstract": "Protein structure alignments are usually based on very different techniques to sequence alignments. We propose a method which treats sequence, structure and even combined sequence + structure in a single framework. Using a probabilistic approach, we calculate a similarity measure which can be applied to fragments containing only protein sequence, structure or both simultaneously.Proof-of-concept results are given for the different problems. For sequence alignments, the methodology is no better than conventional methods. For structure alignments, the techniques are very fast, reliable and tolerant of a range of alignment parameters. Combined sequence and structure alignments may provide a more reliable alignment for pairs of proteins where pure structural alignments can be misled by repetitive elements or apparent symmetries.The probabilistic framework has an elegance in principle, merging sequence and structure descriptors into a single framework. It has a practical use in fast structural alignments and a potential use in finding those examples where sequence and structural similarities apparently disagree.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2142801660",
    "type": "article"
  },
  {
    "title": "Evolving DNA motifs to predict GeneChip probe performance",
    "doi": "https://doi.org/10.1186/1748-7188-4-6",
    "publication_date": "2009-03-19",
    "publication_year": 2009,
    "authors": "WB Langdon; AP Harrison",
    "corresponding_authors": "",
    "abstract": "Affymetrix High Density Oligonuclotide Arrays (HDONA) simultaneously measure expression of thousands of genes using millions of probes. We use correlations between measurements for the same gene across 6685 human tissue samples from NCBI's GEO database to indicated the quality of individual HG-U133A probes. Low correlation indicates a poor probe. Regular expressions can be automatically created from a Backus-Naur form (BNF) context-free grammar using strongly typed genetic programming. The automatically produced motif is better at predicting poor DNA sequences than an existing human generated RE, suggesting runs of Cytosine and Guanine and mixtures should all be avoided.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2090157266",
    "type": "article"
  },
  {
    "title": "Syntenator: Multiple gene order alignments with a gene-specific scoring function",
    "doi": "https://doi.org/10.1186/1748-7188-3-14",
    "publication_date": "2008-11-06",
    "publication_year": 2008,
    "authors": "Christian Rödelsperger; Christoph Dieterich",
    "corresponding_authors": "",
    "abstract": "Identification of homologous regions or conserved syntenies across genomes is one crucial step in comparative genomics. This task is usually performed by genome alignment softwares like WABA or blastz. In case of conserved syntenies, such regions are defined as conserved gene orders. On the gene order level, homologous regions can even be found between distantly related genomes, which do not align on the nucleotide sequence level.We present a novel approach to identify regions of conserved synteny across multiple genomes. Syntenator represents genomes and alignments thereof as partial order graphs (POGs). These POGs are aligned by a dynamic programming approach employing a gene-specific scoring function. The scoring function reflects the level of protein sequence similarity for each possible gene pair. Our method consistently defines larger homologous regions in pairwise gene order alignments than nucleotide-level comparisons. Our method is superior to methods that work on predefined homology gene sets (as implemented in Blockfinder). Syntenator successfully reproduces 80% of the EnsEMBL man-mouse conserved syntenic blocks. The full potential of our method becomes visible by comparing remotely related genomes and multiple genomes. Gene order alignments potentially resolve up to 75% of the EnsEMBL 1:many orthology relations and 27% of the many:many orthology relations.We propose Syntenator as a software solution to reliably infer conserved syntenies among distantly related genomes. The software is available from http://www2.tuebingen.mpg.de/abt4/plone.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2170992251",
    "type": "article"
  },
  {
    "title": "A safe and complete algorithm for metagenomic assembly",
    "doi": "https://doi.org/10.1186/s13015-018-0122-7",
    "publication_date": "2018-02-07",
    "publication_year": 2018,
    "authors": "Nidia Obscura Acosta; Veli Mäkinen; Alexandru I. Tomescu",
    "corresponding_authors": "",
    "abstract": "Reconstructing the genome of a species from short fragments is one of the oldest bioinformatics problems. Metagenomic assembly is a variant of the problem asking to reconstruct the circular genomes of all bacterial species present in a sequencing sample. This problem can be naturally formulated as finding a collection of circular walks of a directed graph G that together cover all nodes, or edges, of G.We address this problem with the \"safe and complete\" framework of Tomescu and Medvedev (Research in computational Molecular biology-20th annual conference, RECOMB 9649:152-163, 2016). An algorithm is called safe if it returns only those walks (also called safe) that appear as subwalk in all metagenomic assembly solutions for G. A safe algorithm is called complete if it returns all safe walks of G.We give graph-theoretic characterizations of the safe walks of G, and a safe and complete algorithm finding all safe walks of G. In the node-covering case, our algorithm runs in time [Formula: see text], and in the edge-covering case it runs in time [Formula: see text]; n and m denote the number of nodes and edges, respectively, of G. This algorithm constitutes the first theoretical tight upper bound on what can be safely assembled from metagenomic reads using this problem formulation.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2792957821",
    "type": "article"
  },
  {
    "title": "Unrooted unordered homeomorphic subtree alignment of RNA trees",
    "doi": "https://doi.org/10.1186/1748-7188-8-13",
    "publication_date": "2013-04-16",
    "publication_year": 2013,
    "authors": "Nimrod Milo; Shay Zakov; Erez Katzenelson; Eitan Bachmat; Yefim Dinitz; Michal Ziv-Ukelson",
    "corresponding_authors": "",
    "abstract": ": We generalize some current approaches for RNA tree alignment, which are traditionally confined to ordered rooted mappings, to also consider unordered unrooted mappings. We define the Homeomorphic Subtree Alignment problem (HSA), and present a new algorithm which applies to several modes, combining global or local, ordered or unordered, and rooted or unrooted tree alignments. Our algorithm generalizes previous algorithms that either solved the problem in an asymmetric manner, or were restricted to the rooted and/or ordered cases. Focusing here on the most general unrooted unordered case, we show that for input trees T and S, our algorithm has an O(nTnS + min(dT,dS)LTLS) time complexity, where nT,LT and dT are the number of nodes, the number of leaves, and the maximum node degree in T, respectively (satisfying dT ≤ LT ≤ nT), and similarly for nS,LS and dS with respect to the tree S. This improves the time complexity of previous algorithms for less general variants of the problem.In order to obtain this time bound for HSA, we developed new algorithms for a generalized variant of the Min-Cost Bipartite Matching problem (MCM), as well as to two derivatives of this problem, entitled All-Cavity-MCM and All-Pairs-Cavity-MCM. For two input sets of size n and m, where n ≤ m, MCM and both its cavity derivatives are solved in O(n3 + nm) time, without the usage of priority queues (e.g. Fibonacci heaps) or other complex data structures. This gives the first cubic time algorithm for All-Pairs-Cavity-MCM, and improves the running times of MCM and All-Cavity-MCM problems in the unbalanced case where n ≪ m.We implemented the algorithm (in all modes mentioned above) as a graphical software tool which computes and displays similarities between secondary structures of RNA given as input, and employed it to a preliminary experiment in which we ran all-against-all inter-family pairwise alignments of RNAse P and Hammerhead RNA family members, exposing new similarities which could not be detected by the traditional rooted ordered alignment approaches. The results demonstrate that our approach can be used to expose structural similarity between some RNAs with higher sensitivity than the traditional rooted ordered alignment approaches. Source code and web-interface for our tool can be found in http://www.cs.bgu.ac.il/\\~negevcb/FRUUT.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2128379289",
    "type": "article"
  },
  {
    "title": "Efficient algorithms for the discovery of gapped factors",
    "doi": "https://doi.org/10.1186/1748-7188-6-5",
    "publication_date": "2011-03-23",
    "publication_year": 2011,
    "authors": "Alberto Apostolico; Cinzia Pizzi; Esko Ukkonen",
    "corresponding_authors": "",
    "abstract": "The discovery of surprisingly frequent patterns is of paramount interest in bioinformatics and computational biology. Among the patterns considered, those consisting of pairs of solid words that co-occur within a prescribed maximum distance -or gapped factors- emerge in a variety of contexts of DNA and protein sequence analysis. A few algorithms and tools have been developed in connection with specific formulations of the problem, however, none can handle comprehensively each of the multiple ways in which the distance between the two terms in a pair may be defined.This paper presents efficient algorithms and tools for the extraction of all pairs of words up to an arbitrarily large length that co-occur surprisingly often in close proximity within a sequence. Whereas the number of such pairs in a sequence of n characters can be Θ(n4), it is shown that an exhaustive discovery process can be carried out in O(n2) or O(n3), depending on the way distance is measured. This is made possible by a prudent combination of properties of pattern maximality and monotonicity of scores, which lead to reduce the number of word pairs to be weighed explicitly, while still producing also the scores attained by any of the pairs not explicitly considered. We applied our approach to the discovery of spaced dyads in DNA sequences.Experiments on biological datasets prove that the method is effective and much faster than exhaustive enumeration of candidate patterns. Software is available freely by academic users via the web interface at http://bcb.dei.unipd.it:8080/dyweb.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2131881581",
    "type": "article"
  },
  {
    "title": "A mixed integer linear programming model to reconstruct phylogenies from single nucleotide polymorphism haplotypes under the maximum parsimony criterion",
    "doi": "https://doi.org/10.1186/1748-7188-8-3",
    "publication_date": "2013-01-23",
    "publication_year": 2013,
    "authors": "Daniele Catanzaro; R. Ravi; Russell Schwartz",
    "corresponding_authors": "Daniele Catanzaro",
    "abstract": "Phylogeny estimation from aligned haplotype sequences has attracted more and more attention in the recent years due to its importance in analysis of many fine-scale genetic data. Its application fields range from medical research, to drug discovery, to epidemiology, to population dynamics. The literature on molecular phylogenetics proposes a number of criteria for selecting a phylogeny from among plausible alternatives. Usually, such criteria can be expressed by means of objective functions, and the phylogenies that optimize them are referred to as optimal. One of the most important estimation criteria is the parsimony which states that the optimal phylogeny T∗for a set H of n haplotype sequences over a common set of variable loci is the one that satisfies the following requirements: (i) it has the shortest length and (ii) it is such that, for each pair of distinct haplotypes hi,hj∈H, the sum of the edge weights belonging to the path from hi to hj in T∗ is not smaller than the observed number of changes between hi and hj. Finding the most parsimonious phylogeny for H involves solving an optimization problem, called the Most Parsimonious Phylogeny Estimation Problem (MPPEP), which is NP-hard in many of its versions.In this article we investigate a recent version of the MPPEP that arises when input data consist of single nucleotide polymorphism haplotypes extracted from a population of individuals on a common genomic region. Specifically, we explore the prospects for improving on the implicit enumeration strategy of implicit enumeration strategy used in previous work using a novel problem formulation and a series of strengthening valid inequalities and preliminary symmetry breaking constraints to more precisely bound the solution space and accelerate implicit enumeration of possible optimal phylogenies. We present the basic formulation and then introduce a series of provable valid constraints to reduce the solution space. We then prove that these constraints can often lead to significant reductions in the gap between the optimal solution and its non-integral linear programming bound relative to the prior art as well as often substantially faster processing of moderately hard problem instances.We provide an indication of the conditions under which such an optimal enumeration approach is likely to be feasible, suggesting that these strategies are usable for relatively large numbers of taxa, although with stricter limits on numbers of variable sites. The work thus provides methodology suitable for provably optimal solution of some harder instances that resist all prior approaches.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2159065207",
    "type": "article"
  },
  {
    "title": "FSH: fast spaced seed hashing exploiting adjacent hashes",
    "doi": "https://doi.org/10.1186/s13015-018-0125-4",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Samuele Girotto; Matteo Comin; Cinzia Pizzi",
    "corresponding_authors": "",
    "abstract": "Patterns with wildcards in specified positions, namely spaced seeds, are increasingly used instead of k-mers in many bioinformatics applications that require indexing, querying and rapid similarity search, as they can provide better sensitivity. Many of these applications require to compute the hashing of each position in the input sequences with respect to the given spaced seed, or to multiple spaced seeds. While the hashing of k-mers can be rapidly computed by exploiting the large overlap between consecutive k-mers, spaced seeds hashing is usually computed from scratch for each position in the input sequence, thus resulting in slower processing. The method proposed in this paper, fast spaced-seed hashing (FSH), exploits the similarity of the hash values of spaced seeds computed at adjacent positions in the input sequence. In our experiments we compute the hash for each positions of metagenomics reads from several datasets, with respect to different spaced seeds. We also propose a generalized version of the algorithm for the simultaneous computation of multiple spaced seeds hashing. In the experiments, our algorithm can compute the hashing values of spaced seeds with a speedup, with respect to the traditional approach, between 1.6 $$\\times$$ to 5.3 $$\\times$$ , depending on the structure of the spaced seed. Spaced seed hashing is a routine task for several bioinformatics application. FSH allows to perform this task efficiently and raise the question of whether other hashing can be exploited to further improve the speed up. This has the potential of major impact in the field, making spaced seed applications not only accurate, but also faster and more efficient. The software FSH is freely available for academic use at: https://bitbucket.org/samu661/fsh/overview .",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2799170772",
    "type": "article"
  },
  {
    "title": "Constrained incremental tree building: new absolute fast converging phylogeny estimation methods with improved scalability and accuracy",
    "doi": "https://doi.org/10.1186/s13015-019-0136-9",
    "publication_date": "2019-02-06",
    "publication_year": 2019,
    "authors": "Qiuyi Zhang; Satish Rao; Tandy Warnow",
    "corresponding_authors": "Tandy Warnow",
    "abstract": "Absolute fast converging (AFC) phylogeny estimation methods are ones that have been proven to recover the true tree with high probability given sequences whose lengths are polynomial in the number of number of leaves in the tree (once the shortest and longest branch weights are fixed). While there has been a large literature on AFC methods, the best in terms of empirical performance was DCMNJ, published in SODA 2001. The main empirical advantage of DCMNJ over other AFC methods is its use of neighbor joining (NJ) to construct trees on smaller taxon subsets, which are then combined into a tree on the full set of species using a supertree method; in contrast, the other AFC methods in essence depend on quartet trees that are computed independently of each other, which reduces accuracy compared to neighbor joining. However, DCMNJ is unlikely to scale to large datasets due to its reliance on supertree methods, as no current supertree methods are able to scale to large datasets with high accuracy.In this study we present a new approach to large-scale phylogeny estimation that shares some of the features of DCMNJ but bypasses the use of supertree methods. We prove that this new approach is AFC and uses polynomial time and space. Furthermore, we describe variations on this basic approach that can be used with leaf-disjoint constraint trees (computed using methods such as maximum likelihood) to produce other methods that are likely to provide even better accuracy. Thus, we present a new generalizable technique for large-scale tree estimation that is designed to improve scalability for phylogeny estimation methods to ultra-large datasets, and that can be used in a variety of settings (including tree estimation from unaligned sequences, and species tree estimation from gene trees).",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2920593524",
    "type": "article"
  },
  {
    "title": "Kohdista: an efficient method to index and query possible Rmap alignments",
    "doi": "https://doi.org/10.1186/s13015-019-0160-9",
    "publication_date": "2019-12-01",
    "publication_year": 2019,
    "authors": "Martin D. Muggli; Simon J. Puglisi; Christina Boucher",
    "corresponding_authors": "Christina Boucher",
    "abstract": "Abstract Background Genome-wide optical maps are ordered high-resolution restriction maps that give the position of occurrence of restriction cut sites corresponding to one or more restriction enzymes. These genome-wide optical maps are assembled using an overlap-layout-consensus approach using raw optical map data, which are referred to as Rmaps. Due to the high error-rate of Rmap data, finding the overlap between Rmaps remains challenging. Results We present K ohdista , which is an index-based algorithm for finding pairwise alignments between single molecule maps ( Rmaps ). The novelty of our approach is the formulation of the alignment problem as automaton path matching, and the application of modern index-based data structures. In particular, we combine the use of the Generalized Compressed Suffix Array (GCSA) index with the wavelet tree in order to build K ohdista . We validate K ohdista on simulated E. coli data, showing the approach successfully finds alignments between Rmaps simulated from overlapping genomic regions. Conclusion we demonstrate K ohdista is the only method that is capable of finding a significant number of high quality pairwise Rmap alignments for large eukaryote organisms in reasonable time.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2994746706",
    "type": "article"
  },
  {
    "title": "Protein docking with predicted constraints",
    "doi": "https://doi.org/10.1186/s13015-015-0036-6",
    "publication_date": "2015-02-19",
    "publication_year": 2015,
    "authors": "Ludwig Krippahl; Pedro Barahona",
    "corresponding_authors": "",
    "abstract": "This paper presents a constraint-based method for improving protein docking results. Efficient constraint propagation cuts over 95% of the search time for finding the configurations with the largest contact surface, provided a contact is specified between two amino acid residues. This makes it possible to scan a large number of potentially correct constraints, lowering the requirements for useful contact predictions. While other approaches are very dependent on accurate contact predictions, ours requires only that at least one correct contact be retained in a set of, for example, one hundred constraints to test. It is this feature that makes it feasible to use readily available sequence data to predict specific potential contacts. Although such prediction is too inaccurate for most purposes, we demonstrate with a Naïve Bayes Classifier that it is accurate enough to more than double the average number of acceptable models retained during the crucial filtering stage of protein docking when combined with our constrained docking algorithm. All software developed in this work is freely available as part of the Open Chemera Library.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2150944048",
    "type": "article"
  },
  {
    "title": "An improved Four-Russians method and sparsified Four-Russians algorithm for RNA folding",
    "doi": "https://doi.org/10.1186/s13015-016-0081-9",
    "publication_date": "2016-08-05",
    "publication_year": 2016,
    "authors": "Yelena Frid; Dan Gusfield",
    "corresponding_authors": "",
    "abstract": "The basic RNA secondary structure prediction problem or single sequence folding problem (SSF) was solved 35 years ago by a now well-known [Formula: see text]-time dynamic programming method. Recently three methodologies-Valiant, Four-Russians, and Sparsification-have been applied to speedup RNA secondary structure prediction. The sparsification method exploits two properties of the input: the number of subsequence Z with the endpoints belonging to the optimal folding set and the maximum number base-pairs L. These sparsity properties satisfy [Formula: see text] and [Formula: see text], and the method reduces the algorithmic running time to O(LZ). While the Four-Russians method utilizes tabling partial results.In this paper, we explore three different algorithmic speedups. We first expand the reformulate the single sequence folding Four-Russians [Formula: see text]-time algorithm, to utilize an on-demand lookup table. Second, we create a framework that combines the fastest Sparsification and new fastest on-demand Four-Russians methods. This combined method has worst-case running time of [Formula: see text], where [Formula: see text] and [Formula: see text]. Third we update the Four-Russians formulation to achieve an on-demand [Formula: see text]-time parallel algorithm. This then leads to an asymptotic speedup of [Formula: see text] where [Formula: see text] and [Formula: see text] the number of subsequence with the endpoint j belonging to the optimal folding set.The on-demand formulation not only removes all extraneous computation and allows us to incorporate more realistic scoring schemes, but leads us to take advantage of the sparsity properties. Through asymptotic analysis and empirical testing on the base-pair maximization variant and a more biologically informative scoring scheme, we show that this Sparse Four-Russians framework is able to achieve a speedup on every problem instance, that is asymptotically never worse, and empirically better than achieved by the minimum of the two methods alone.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2498116915",
    "type": "article"
  },
  {
    "title": "Embedding gene trees into phylogenetic networks by conflict resolution algorithms",
    "doi": "https://doi.org/10.1186/s13015-022-00218-8",
    "publication_date": "2022-05-19",
    "publication_year": 2022,
    "authors": "Marcin Wawerka; Dawid Dąbkowski; Natalia Rutecka; Agnieszka Mykowiecka; Paweł Górecki",
    "corresponding_authors": "Marcin Wawerka",
    "abstract": "Phylogenetic networks are mathematical models of evolutionary processes involving reticulate events such as hybridization, recombination, or horizontal gene transfer. One of the crucial notions in phylogenetic network modelling is displayed tree, which is obtained from a network by removing a set of reticulation edges. Displayed trees may represent an evolutionary history of a gene family if the evolution is shaped by reticulation events.We address the problem of inferring an optimal tree displayed by a network, given a gene tree G and a tree-child network N, under the deep coalescence and duplication costs. We propose an O(mn)-time dynamic programming algorithm (DP) to compute a lower bound of the optimal displayed tree cost, where m and n are the sizes of G and N, respectively. In addition, our algorithm can verify whether the solution is exact. Moreover, it provides a set of reticulation edges corresponding to the obtained cost. If the cost is exact, the set induces an optimal displayed tree. Otherwise, the set contains pairs of conflicting edges, i.e., edges sharing a reticulation node. Next, we show a conflict resolution algorithm that requires [Formula: see text] invocations of DP in the worst case, where r is the number of reticulations. We propose a similar [Formula: see text]-time algorithm for level-k tree-child networks and a branch and bound solution to compute lower and upper bounds of optimal costs. We also extend the algorithms to a broader class of phylogenetic networks. Based on simulated data, the average runtime is [Formula: see text] under the deep-coalescence cost and [Formula: see text] under the duplication cost.Despite exponential complexity in the worst case, our algorithms perform significantly well on empirical and simulated datasets, due to the strategy of resolving internal dissimilarities between gene trees and networks. Therefore, the algorithms are efficient alternatives to enumeration strategies commonly proposed in the literature and enable analyses of complex networks with dozens of reticulations.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4280636520",
    "type": "article"
  },
  {
    "title": "Automated design of dynamic programming schemes for RNA folding with pseudoknots",
    "doi": "https://doi.org/10.1186/s13015-023-00229-z",
    "publication_date": "2023-12-01",
    "publication_year": 2023,
    "authors": "Bertrand Marchand; Sebastian Will; Sarah J. Berkemer; Yann Ponty; Laurent Bulteau",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4281485009",
    "type": "article"
  },
  {
    "title": "Finding the region of pseudo-periodic tandem repeats in biological sequences",
    "doi": "https://doi.org/10.1186/1748-7188-1-2",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Xiaowen Liu; Lusheng Wang",
    "corresponding_authors": "",
    "abstract": "The genomes of many species are dominated by short sequences repeated consecutively. It is estimated that over 10% of the human genome consists of tandemly repeated sequences. Finding repeated regions in long sequences is important in sequence analysis. We develop a software, LocRepeat, that finds regions of pseudo-periodic repeats in a long sequence. We use the definition of Li et al. [1] for the pseudo-periodic partition of a region and extend the algorithm that can select the repeated region from a given long sequence and give the pseudo-periodic partition of the region. LocRepeat is available at http://www.cs.cityu.edu.hk/~lwang/software/LocRepeat",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2106523371",
    "type": "article"
  },
  {
    "title": "A phylogenetic generalized hidden Markov model for predicting alternatively spliced exons",
    "doi": "https://doi.org/10.1186/1748-7188-1-14",
    "publication_date": "2006-08-25",
    "publication_year": 2006,
    "authors": "Jonathan Allen; Steven L. Salzberg",
    "corresponding_authors": "Jonathan Allen",
    "abstract": "An important challenge in eukaryotic gene prediction is accurate identification of alternatively spliced exons. Functional transcripts can go undetected in gene expression studies when alternative splicing only occurs under specific biological conditions. Non-expression based computational methods support identification of rarely expressed transcripts. A non-expression based statistical method is presented to annotate alternatively spliced exons using a single genome sequence and evidence from cross-species sequence conservation. The computational method is implemented in the program ExAlt and an analysis of prediction accuracy is given for Drosophila melanogaster. ExAlt identifies the structure of most alternatively spliced exons in the test set and cross-species sequence conservation is shown to improve the precision of predictions. The software package is available to run on Drosophila genomes to search for new cases of alternative splicing.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2123759220",
    "type": "article"
  },
  {
    "title": "Computing distribution of scale independent motifs in biological sequences",
    "doi": "https://doi.org/10.1186/1748-7188-1-18",
    "publication_date": "2006-10-18",
    "publication_year": 2006,
    "authors": "Jonas S. Almeida; Susana Vinga",
    "corresponding_authors": "",
    "abstract": "The use of Chaos Game Representation (CGR) or its generalization, Universal Sequence Maps (USM), to describe the distribution of biological sequences has been found objectionable because of the fractal structure of that coordinate system. Consequently, the investigation of distribution of symbolic motifs at multiple scales is hampered by an inexact association between distance and sequence dissimilarity. A solution to this problem could unleash the use of iterative maps as phase-state representation of sequences where its statistical properties can be conveniently investigated. In this study a family of kernel density functions is described that accommodates the fractal nature of iterative function representations of symbolic sequences and, consequently, enables the exact investigation of sequence motifs of arbitrary lengths in that scale-independent representation. Furthermore, the proposed kernel density includes both Markovian succession and currently used alignment-free sequence dissimilarity metrics as special solutions. Therefore, the fractal kernel described is in fact a generalization that provides a common framework for a diverse suite of sequence analysis techniques.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2134325994",
    "type": "article"
  },
  {
    "title": "A spatio-temporal mining approach towards summarizing and analyzing protein folding trajectories",
    "doi": "https://doi.org/10.1186/1748-7188-2-3",
    "publication_date": "2007-04-04",
    "publication_year": 2007,
    "authors": "Hui Yang; Srinivasan Parthasarathy; Duygu Ucar",
    "corresponding_authors": "",
    "abstract": "Understanding the protein folding mechanism remains a grand challenge in structural biology. In the past several years, computational theories in molecular dynamics have been employed to shed light on the folding process. Coupled with high computing power and large scale storage, researchers now can computationally simulate the protein folding process in atomistic details at femtosecond temporal resolution. Such simulation often produces a large number of folding trajectories, each consisting of a series of 3D conformations of the protein under study. As a result, effectively managing and analyzing such trajectories is becoming increasingly important. In this article, we present a spatio-temporal mining approach to analyze protein folding trajectories. It exploits the simplicity of contact maps, while also integrating 3D structural information in the analysis. It characterizes the dynamic folding process by first identifying spatio-temporal association patterns in contact maps, then studying how such patterns evolve along a folding trajectory. We demonstrate that such patterns can be leveraged to summarize folding trajectories, and to facilitate the detection and ordering of important folding events along a folding path. We also show that such patterns can be used to identify a consensus partial folding pathway across multiple folding trajectories. Furthermore, we argue that such patterns can capture both local and global structural topology in a 3D protein conformation, thereby facilitating effective structural comparison amongst conformations. We apply this approach to analyze the folding trajectories of two small synthetic proteins-BBA5 and GSGS (or Beta3S). We show that this approach is promising towards addressing the above issues, namely, folding trajectory summarization, folding events detection and ordering, and consensus partial folding pathway identification across trajectories.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2010483041",
    "type": "article"
  },
  {
    "title": "Data Mining in Bioinformatics (BIOKDD)",
    "doi": "https://doi.org/10.1186/1748-7188-2-4",
    "publication_date": "2007-04-11",
    "publication_year": 2007,
    "authors": "Mohammed J. Zaki; George Karypis; Jiong Yang",
    "corresponding_authors": "",
    "abstract": "Data Mining is the process of automatic discovery of novel and understandable models and patterns from large amounts of data. Bioinformatics is the science of storing, analyzing, and utilizing information from biological data such as sequences, molecules, gene expressions, and pathways. Development of novel data mining methods will play a fundamental role in understanding these rapidly expanding sources of biological data.\r\n\r\nData mining approaches seem ideally suited for bioinformatics, which is data-rich, but lacks a comprehensive theory of life's organization at the molecular level. The extensive databases of biological information create both challenges and opportunities for developing novel data mining methods. The 6th Workshop on Data Mining in Bioinformatics (BIOKDD) was held on August 20th, 2006, Philadelphia, PA, USA, in conjunction with the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. The goal of the workshop was to encourage KDD researchers to take on the numerous challenges that Bioinformatics offers. The BIOKDD workshops have been held annually in conjunction with the ACM SIGKDD Conferences, since 2001. Additional information about BIOKDD can be obtained online [1].\r\n\r\nFive revised and expanded papers were selected from the BIOKDD workshop, out of a total of 18 submissions, to appear in Algorithms for Molecular Biology (AMB). These papers underwent another round of external reviewing prior to being accepted for AMB. An overview of each paper is given below. In the paper titled Automatic Layout and Visualization of Biclusters, Gregory A. Grothaus, Adeel Mufti and T. M. Murali [2], present a novel method to display biclusters mined from gene expression data. The approach allows querying and visual exploration of the clusters/sub-matrices. The software is also available as open-source.\r\n\r\nIn ExMotif: Efficient Structured Motif Extraction, Yongqiang Zhang and Mohammed J. Zaki [3], describe a new algorithm called EXMOTIF to extract frequent motifs from DNA sequences. The method can mine structured motifs and profiles which have variable gaps between different elements. The demonstrate the efficiency of the method compared to state-of-the-art methods, and also demonstrate an application in mining composite transcription factor binding sites.\r\n\r\nIn the paper Refining Motifs by Improving Information Content Scores using Neighborhood Profile Search, Chandan K. Reddy, Yao-Chung Weng and Hsiao-Dong Chiang [4], show how one can refine the profile motifs discovered via Expectation Maximization and Gibbs Sampling based methods. They search the neighborhood regions of the initial alignments to obtain locally optimal solutions, which improve the information content of the discovered profiles.\r\n\r\nIn their paper, A Novel Functional Module Detection Algorithm for Protein-Protein Interaction Networks, Woochang Hwang, Young-Rae Cho, Aidong Zhang and Murali Ramanathan [5], describe the unexpected properties of the protein-protein interaction (PPI) networks and their use in a clustering method to detect biologically relevant functional modules. They propose a new method called STM (signal transduction model) to detect the PPI modules, and compare it with previous approaches to demonstrate its effectiveness in discovering large and arbitrary shaped clusters.\r\n\r\nIn A Spatio-temporal Mining Approach towards Summarizing and Analyzing Protein Folding Trajectories, Hui Yang, Srinivasan Parthasarathy and Duygu Ucar [6], describe a method to mine protein folding molecular dynamics simulations datasets. They describe a spatio-temporal association discovery approach to mine protein folding trajectories, to identify critical events and common pathways.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2064634797",
    "type": "article"
  },
  {
    "title": "GOGOT: a method for the identification of differentially expressed fragments from cDNA-AFLP data",
    "doi": "https://doi.org/10.1186/1748-7188-2-5",
    "publication_date": "2007-05-30",
    "publication_year": 2007,
    "authors": "Koji Kadota; Ryoko Araki; Yuji Nakai; Masumi Abe",
    "corresponding_authors": "",
    "abstract": "One-dimensional (1-D) electrophoretic data obtained using the cDNA-AFLP method have attracted great interest for the identification of differentially expressed transcript-derived fragments (TDFs). However, high-throughput analysis of the cDNA-AFLP data is currently limited by the need for labor-intensive visual evaluation of multiple electropherograms. We would like to have high-throughput ways of identifying such TDFs.We describe a method, GOGOT, which automatically detects the differentially expressed TDFs in a set of time-course electropherograms. Analysis by GOGOT is conducted as follows: correction of fragment lengths of TDFs, alignment of identical TDFs across different electropherograms, normalization of peak heights, and identification of differentially expressed TDFs using a special statistic. The output of the analysis is a highly reduced list of differentially expressed TDFs. Visual evaluation confirmed that the peak alignment was performed perfectly for the TDFs by virtue of the correction of peak fragment lengths before alignment in step 1. The validity of the automated ranking of TDFs by the special statistic was confirmed by the visual evaluation of a third party.GOGOT is useful for the automated detection of differentially expressed TDFs from cDNA-AFLP temporal electrophoretic data. The current algorithm may be applied to other electrophoretic data and temporal microarray data.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2098702528",
    "type": "article"
  },
  {
    "title": "AlignMiner: a Web-based tool for detection of divergent regions in multiple sequence alignments of conserved sequences",
    "doi": "https://doi.org/10.1186/1748-7188-5-24",
    "publication_date": "2010-06-02",
    "publication_year": 2010,
    "authors": "Darío Guerrero; Rocí­o Bautista; David P. Villalobos; Francisco R. Cantón; M. Gonzalo Claros",
    "corresponding_authors": "",
    "abstract": "Multiple sequence alignments are used to study gene or protein function, phylogenetic relations, genome evolution hypotheses and even gene polymorphisms. Virtually without exception, all available tools focus on conserved segments or residues. Small divergent regions, however, are biologically important for specific quantitative polymerase chain reaction, genotyping, molecular markers and preparation of specific antibodies, and yet have received little attention. As a consequence, they must be selected empirically by the researcher. AlignMiner has been developed to fill this gap in bioinformatic analyses.AlignMiner is a Web-based application for detection of conserved and divergent regions in alignments of conserved sequences, focusing particularly on divergence. It accepts alignments (protein or nucleic acid) obtained using any of a variety of algorithms, which does not appear to have a significant impact on the final results. AlignMiner uses different scoring methods for assessing conserved/divergent regions, Entropy being the method that provides the highest number of regions with the greatest length, and Weighted being the most restrictive. Conserved/divergent regions can be generated either with respect to the consensus sequence or to one master sequence. The resulting data are presented in a graphical interface developed in AJAX, which provides remarkable user interaction capabilities. Users do not need to wait until execution is complete and can.even inspect their results on a different computer. Data can be downloaded onto a user disk, in standard formats. In silico and experimental proof-of-concept cases have shown that AlignMiner can be successfully used to designing specific polymerase chain reaction primers as well as potential epitopes for antibodies. Primer design is assisted by a module that deploys several oligonucleotide parameters for designing primers \"on the fly\".AlignMiner can be used to reliably detect divergent regions via several scoring methods that provide different levels of selectivity. Its predictions have been verified by experimental means. Hence, it is expected that its usage will save researchers' time and ensure an objective selection of the best-possible divergent region when closely related sequences are analysed. AlignMiner is freely available at http://www.scbi.uma.es/alignminer.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2164335278",
    "type": "article"
  },
  {
    "title": "Fast structure similarity searches among protein models: efficient clustering of protein fragments",
    "doi": "https://doi.org/10.1186/1748-7188-7-16",
    "publication_date": "2012-05-29",
    "publication_year": 2012,
    "authors": "Federico Fogolari; Alessandra Corazza; Paolo Viglino; Gennaro Esposito",
    "corresponding_authors": "",
    "abstract": "For many predictive applications a large number of models is generated and later clustered in subsets based on structure similarity. In most clustering algorithms an all-vs-all root mean square deviation (RMSD) comparison is performed. Most of the time is typically spent on comparison of non-similar structures. For sets with more than, say, 10,000 models this procedure is very time-consuming and alternative faster algorithms, restricting comparisons only to most similar structures would be useful. We exploit the inverse triangle inequality on the RMSD between two structures given the RMSDs with a third structure. The lower bound on RMSD may be used, when restricting the search of similarity to a reasonably low RMSD threshold value, to speed up similarity searches significantly. Tests are performed on large sets of decoys which are widely used as test cases for predictive methods, with a speed-up of up to 100 times with respect to all-vs-all comparison depending on the set and parameters used. Sample applications are shown. The algorithm presented here allows fast comparison of large data sets of structures with limited memory requirements. As an example of application we present clustering of more than 100000 fragments of length 5 from the top500H dataset into few hundred representative fragments. A more realistic scenario is provided by the search of similarity within the very large decoy sets used for the tests. Other applications regard filtering nearly-indentical conformation in selected CASP9 datasets and clustering molecular dynamics snapshots. A linux executable and a Perl script with examples are given in the supplementary material (Additional file 1). The source code is available upon request from the authors.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2106245871",
    "type": "article"
  },
  {
    "title": "Towards a practical O(n logn) phylogeny algorithm",
    "doi": "https://doi.org/10.1186/1748-7188-7-32",
    "publication_date": "2012-11-26",
    "publication_year": 2012,
    "authors": "Jakub Truszkowski; Yanqi Hao; Daniel G. Brown",
    "corresponding_authors": "",
    "abstract": ": Recently, we have identified a randomized quartet phylogeny algorithm that has O(nlogn) runtime with high probability, which is asymptotically optimal. Our algorithm has high probability of returning the correct phylogeny when quartet errors are independent and occur with known probability, and when the algorithm uses a guide tree on O(loglogn) taxa that is correct with high probability. In practice, none of these assumptions is correct: quartet errors are positively correlated and occur with unknown probability, and the guide tree is often error prone. Here, we bring our work out of the purely theoretical setting. We present a variety of extensions which, while only slowing the algorithm down by a constant factor, make its performance nearly comparable to that of Neighbour Joining , which requires Θ(n3) runtime in existing implementations. Our results suggest a new direction for quartet-based phylogenetic reconstruction that may yield striking speed improvements at minimal accuracy cost. An early prototype implementation of our software is available at http://www.cs.uwaterloo.ca/jmtruszk/qtree.tar.gz.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2110922051",
    "type": "article"
  },
  {
    "title": "Direct vs 2-stage approaches to structured motif finding",
    "doi": "https://doi.org/10.1186/1748-7188-7-20",
    "publication_date": "2012-08-21",
    "publication_year": 2012,
    "authors": "Maria Federico; Mauro Leoncini; Manuela Montangero; Paolo Valente",
    "corresponding_authors": "Mauro Leoncini",
    "abstract": "Abstract Background The notion of DNA motif is a mathematical abstraction used to model regions of the DNA (known as Transcription Factor Binding Sites , or TFBSs ) that are bound by a given Transcription Factor to regulate gene expression or repression. In turn, DNA structured motifs are a mathematical counterpart that models sets of TFBSs that work in concert in the gene regulations processes of higher eukaryotic organisms. Typically, a structured motif is composed of an ordered set of isolated (or simple ) motifs, separated by a variable, but somewhat constrained number of “irrelevant” base-pairs. Discovering structured motifs in a set of DNA sequences is a computationally hard problem that has been addressed by a number of authors using either a direct approach, or via the preliminary identification and successive combination of simple motifs. Results We describe a computational tool, named SISMA, for the de-novo discovery of structured motifs in a set of DNA sequences. SISMA is an exact, enumerative algorithm, meaning that it finds all the motifs conforming to the specifications. It does so in two stages: first it discovers all the possible component simple motifs, then combines them in a way that respects the given constraints. We developed SISMA mainly with the aim of understanding the potential benefits of such a 2-stage approach w.r.t. direct methods. In fact, no 2-stage software was available for the general problem of structured motif discovery, but only a few tools that solved restricted versions of the problem. We evaluated SISMA against other published tools on a comprehensive benchmark made of both synthetic and real biological datasets. In a significant number of cases, SISMA outperformed the competitors, exhibiting a good performance also in most of the cases in which it was inferior. Conclusions A reflection on the results obtained lead us to conclude that a 2-stage approach can be implemented with many advantages over direct approaches. Some of these have to do with greater modularity, ease of parallelization, and the possibility to perform adaptive searches of structured motifs. As another consideration, we noted that most hard instances for SISMA were easy to detect in advance. In these cases one may initially opt for a direct method; or, as a viable alternative in most laboratories, one could run both direct and 2-stage tools in parallel, halting the computations when the first halts.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2111676148",
    "type": "article"
  },
  {
    "title": "Multi-membership gene regulation in pathway based microarray analysis",
    "doi": "https://doi.org/10.1186/1748-7188-6-22",
    "publication_date": "2011-09-22",
    "publication_year": 2011,
    "authors": "Stelios Pavlidis; Annette Payne; Stephen Swift",
    "corresponding_authors": "",
    "abstract": "Gene expression analysis has been intensively researched for more than a decade. Recently, there has been elevated interest in the integration of microarray data analysis with other types of biological knowledge in a holistic analytical approach. We propose a methodology that can be facilitated for pathway based microarray data analysis, based on the observation that a substantial proportion of genes present in biochemical pathway databases are members of a number of distinct pathways. Our methodology aims towards establishing the state of individual pathways, by identifying those truly affected by the experimental conditions based on the behaviour of such genes. For that purpose it considers all the pathways in which a gene participates and the general census of gene expression per pathway.We utilise hill climbing, simulated annealing and a genetic algorithm to analyse the consistency of the produced results, through the application of fuzzy adjusted rand indexes and hamming distance. All algorithms produce highly consistent genes to pathways allocations, revealing the contribution of genes to pathway functionality, in agreement with current pathway state visualisation techniques, with the simulated annealing search proving slightly superior in terms of efficiency.We show that the expression values of genes, which are members of a number of biochemical pathways or modules, are the net effect of the contribution of each gene to these biochemical processes. We show that by manipulating the pathway and module contribution of such genes to follow underlying trends we can interpret microarray results centred on the behaviour of these genes.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2118654675",
    "type": "article"
  },
  {
    "title": "Incompatible quartets, triplets, and characters",
    "doi": "https://doi.org/10.1186/1748-7188-8-11",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Brad Shutters; Sudheer Vakati; David Fernández‐Baca",
    "corresponding_authors": "Brad Shutters",
    "abstract": "We study a long standing conjecture on the necessary and sufficient conditions for the compatibility of multi-state characters: There exists a function f(r) such that, for any set C of r-state characters, C is compatible if and only if every subset of f(r) characters of C is compatible. We show that for every r≥2, there exists an incompatible set C of Ω(r2)r-state characters such that every proper subset of C is compatible. This improves the previous lower bound of f(r)≥r given by Meacham (1983), and f(4)≥5 given by Habib and To (2011). For the case when r=3, Lam, Gusfield and Sridhar (2011) recently showed that f(3)=3. We give an independent proof of this result and completely characterize the sets of pairwise compatible 3-state characters by a single forbidden intersection pattern. Our lower bound on f(r) is proven via a result on quartet compatibility that may be of independent interest: For every n≥4, there exists an incompatible set Q of Ω(n2) quartets over n labels such that every proper subset of Q is compatible. We show that such a set of quartets can have size at most 3 when n=5, and at most O(n3) for arbitrary n. We contrast our results on quartets with the case of rooted triplets: For every n≥3, if R is an incompatible set of more than n−1 triplets over n labels, then some proper subset of R is incompatible. We show this bound is tight by exhibiting, for every n≥3, a set of n−1 triplets over n taxa such that R is incompatible, but every proper subset of R is compatible.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2150911766",
    "type": "article"
  },
  {
    "title": "Identification of bifurcation transitions in biological regulatory networks using Answer-Set Programming",
    "doi": "https://doi.org/10.1186/s13015-017-0110-3",
    "publication_date": "2017-07-20",
    "publication_year": 2017,
    "authors": "Louis Fippo Fitime; Olivier Roux; Carito Guziołowski; Loïc Paulevé",
    "corresponding_authors": "",
    "abstract": "Numerous cellular differentiation processes can be captured using discrete qualitative models of biological regulatory networks. These models describe the temporal evolution of the state of the network subject to different competing transitions, potentially leading the system to different attractors. This paper focusses on the formal identification of states and transitions that are crucial for preserving or pre-empting the reachability of a given behaviour.In the context of non-deterministic automata networks, we propose a static identification of so-called bifurcations, i.e., transitions after which a given goal is no longer reachable. Such transitions are naturally good candidates for controlling the occurrence of the goal, notably by modulating their propensity. Our method combines Answer-Set Programming with static analysis of reachability properties to provide an under-approximation of all the existing bifurcations.We illustrate our discrete bifurcation analysis on several models of biological systems, for which we identify transitions which impact the reachability of given long-term behaviour. In particular, we apply our implementation on a regulatory network among hundreds of biological species, supporting the scalability of our approach.Our method allows a formal and scalable identification of transitions which are responsible for the lost of capability to reach a given state. It can be applied to any asynchronous automata networks, which encompass Boolean and multi-valued models. An implementation is provided as part of the Pint software, available at http://loicpauleve.name/pint.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2738925829",
    "type": "article"
  },
  {
    "title": "Exploiting bounded signal flow for graph orientation based on cause–effect pairs",
    "doi": "https://doi.org/10.1186/1748-7188-6-21",
    "publication_date": "2011-08-25",
    "publication_year": 2011,
    "authors": "Britta Dorn; Falk Hüffner; Dominikus Krüger; Rolf Niedermeier; Johannes Uhlmann",
    "corresponding_authors": "Falk Hüffner",
    "abstract": "Abstract Background We consider the following problem: Given an undirected network and a set of sender–receiver pairs, direct all edges such that the maximum number of \"signal flows\" defined by the pairs can be routed respecting edge directions. This problem has applications in understanding protein interaction based cell regulation mechanisms. Since this problem is NP-hard, research so far concentrated on polynomial-time approximation algorithms and tractable special cases. Results We take the viewpoint of parameterized algorithmics and examine several parameters related to the maximum signal flow over vertices or edges. We provide several fixed-parameter tractability results, and in one case a sharp complexity dichotomy between a linear-time solvable case and a slightly more general NP-hard case. We examine the value of these parameters for several real-world network instances. Conclusions Several biologically relevant special cases of the NP-hard problem can be solved to optimality. In this way, parameterized analysis yields both deeper insight into the computational complexity and practical solving strategies.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2809436691",
    "type": "article"
  },
  {
    "title": "Kermit: linkage map guided long read assembly",
    "doi": "https://doi.org/10.1186/s13015-019-0143-x",
    "publication_date": "2019-03-20",
    "publication_year": 2019,
    "authors": "Riku Walve; Pasi Rastas; Leena Salmela",
    "corresponding_authors": "",
    "abstract": "With long reads getting even longer and cheaper, large scale sequencing projects can be accomplished without short reads at an affordable cost. Due to the high error rates and less mature tools, de novo assembly of long reads is still challenging and often results in a large collection of contigs. Dense linkage maps are collections of markers whose location on the genome is approximately known. Therefore they provide long range information that has the potential to greatly aid in de novo assembly. Previously linkage maps have been used to detect misassemblies and to manually order contigs. However, no fully automated tools exist to incorporate linkage maps in assembly but instead large amounts of manual labour is needed to order the contigs into chromosomes. We formulate the genome assembly problem in the presence of linkage maps and present the first method for guided genome assembly using linkage maps. Our method is based on an additional cleaning step added to the assembly. We show that it can simplify the underlying assembly graph, resulting in more contiguous assemblies and reducing the amount of misassemblies when compared to de novo assembly. We present the first method to integrate linkage maps directly into genome assembly. With a modest increase in runtime, our method improves contiguity and correctness of genome assembly.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2923920794",
    "type": "article"
  },
  {
    "title": "Natural family-free genomic distance",
    "doi": "https://doi.org/10.1186/s13015-021-00183-8",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Diego P. Rubert; Fábio V. Martinez; Marília D. V. Braga",
    "corresponding_authors": "Marília D. V. Braga",
    "abstract": "A classical problem in comparative genomics is to compute the rearrangement distance, that is the minimum number of large-scale rearrangements required to transform a given genome into another given genome. The traditional approaches in this area are family-based, i.e., require the classification of DNA fragments of both genomes into families. Furthermore, the most elementary family-based models, which are able to compute distances in polynomial time, restrict the families to occur at most once in each genome. In contrast, the distance computation in models that allow multifamilies (i.e., families with multiple occurrences) is NP-hard. Very recently, Bohnenkämper et al. (J Comput Biol 28:410-431, 2021) proposed an ILP formulation for computing the genomic distance of genomes with multifamilies, allowing structural rearrangements, represented by the generic double cut and join (DCJ) operation, and content-modifying insertions and deletions of DNA segments. This ILP is very efficient, but must maximize a matching of the genes in each multifamily, in order to prevent the free lunch artifact that would otherwise let empty or almost empty matchings give smaller distances.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3082723548",
    "type": "article"
  },
  {
    "title": "Quantifying steric hindrance and topological obstruction to protein structure superposition",
    "doi": "https://doi.org/10.1186/s13015-020-00180-3",
    "publication_date": "2021-02-27",
    "publication_year": 2021,
    "authors": "Peter Røgen",
    "corresponding_authors": "Peter Røgen",
    "abstract": "Background: In computational structural biology, structure comparison is fundamental for our understanding of proteins. Structure comparison is, e.g., algorithmically the starting point for computational studies of structural evolution and it guides our efforts to predict protein structures from their amino acid sequences. Most methods for structural alignment of protein structures optimize the distances between aligned and superimposed residue pairs, i.e., the distances traveled by the aligned and superimposed residues during linear interpolation. Considering such a linear interpolation, these methods do not differentiate if there is room for the interpolation, if it causes steric clashes, or more severely, if it changes the topology of the compared protein backbone curves. Results: To distinguish such cases, we analyze the linear interpolation between two aligned and superimposed backbones. We quantify the amount of steric clashes and find all self-intersections in a linear backbone interpolation. To determine if the self-intersections alter the protein’s backbone curve significantly or not, we present a path-finding algorithm that checks if there exists a self-avoiding path in a neighborhood of the linear interpolation. A new path is constructed by altering the linear interpolation using a novel interpretation of Reidemeister moves from knot theory working on three-dimensional curves rather than on knot diagrams. Either the algorithm finds a self-avoiding path or it returns a smallest set of essential self-intersections. Each of these indicates a significant difference between the folds of the aligned protein structures. As expected, we find at least one essential self-intersection separating most unknotted structures from a knotted structure, and we find even larger motions in proteins connected by obstruction free linear interpolations. We also find examples of homologous proteins that are differently threaded, and we find many distinct folds connected by longer but simple deformations. TM-align is one of the most restrictive alignment programs. With standard parameters, it only aligns residues superimposed within 5 Ångström distance. We find 42165 topological obstructions between aligned parts in 142068 TM-alignments. Thus, this restrictive alignment procedure still allows topological dissimilarity of the aligned parts. Conclusions: Based on the data we conclude that our program ProteinAlignmentObstruction provides significant additional information to alignment scores based solely on distances between aligned and superimposed residue pairs.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3134923564",
    "type": "article"
  },
  {
    "title": "An improved approximation algorithm for the reversal and transposition distance considering gene order and intergenic sizes",
    "doi": "https://doi.org/10.1186/s13015-021-00203-7",
    "publication_date": "2021-12-01",
    "publication_year": 2021,
    "authors": "Klairton Lima Brito; Andre Rodrigues Oliveira; Alexsandro Oliveira Alexandrino; Ulisses Dias; Zanoni Dias",
    "corresponding_authors": "Klairton Lima Brito",
    "abstract": "In the comparative genomics field, one of the goals is to estimate a sequence of genetic changes capable of transforming a genome into another. Genome rearrangement events are mutations that can alter the genetic content or the arrangement of elements from the genome. Reversal and transposition are two of the most studied genome rearrangement events. A reversal inverts a segment of a genome while a transposition swaps two consecutive segments. Initial studies in the area considered only the order of the genes. Recent works have incorporated other genetic information in the model. In particular, the information regarding the size of intergenic regions, which are structures between each pair of genes and in the extremities of a linear genome.In this work, we investigate the SORTING BY INTERGENIC REVERSALS AND TRANSPOSITIONS problem on genomes sharing the same set of genes, considering the cases where the orientation of genes is known and unknown. Besides, we explored a variant of the problem, which generalizes the transposition event. As a result, we present an approximation algorithm that guarantees an approximation factor of 4 for both cases considering the reversal and transposition (classic definition) events, an improvement from the 4.5-approximation previously known for the scenario where the orientation of the genes is unknown. We also present a 3-approximation algorithm by incorporating the generalized transposition event, and we propose a greedy strategy to improve the performance of the algorithms. We performed practical tests adopting simulated data which indicated that the algorithms, in both cases, tend to perform better when compared with the best-known algorithms for the problem. Lastly, we conducted experiments using real genomes to demonstrate the applicability of the algorithms.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4200239892",
    "type": "article"
  },
  {
    "title": "An optimized FM-index library for nucleotide and amino acid search",
    "doi": "https://doi.org/10.1186/s13015-021-00204-6",
    "publication_date": "2021-12-01",
    "publication_year": 2021,
    "authors": "Tim Anderson; Travis J. Wheeler",
    "corresponding_authors": "",
    "abstract": "Abstract Background Pattern matching is a key step in a variety of biological sequence analysis pipelines. The FM-index is a compressed data structure for pattern matching, with search run time that is independent of the length of the database text. Implementation of the FM-index is reasonably complicated, so that increased adoption will be aided by the availability of a fast and flexible FM-index library. Results We present AvxWindowedFMindex (AWFM-index), a lightweight, open-source, thread-parallel FM-index library written in C that is optimized for indexing nucleotide and amino acid sequences. AWFM-index introduces a new approach to storing FM-index data in a strided bit-vector format that enables extremely efficient computation of the FM-index occurrence function via AVX2 bitwise instructions, and combines this with optional on-disk storage of the index’s suffix array and a cache-efficient lookup table for partial k-mer searches. The AWFM-index performs exact match count and locate queries faster than SeqAn3’s FM-index implementation across a range of comparable memory footprints. When optimized for speed, AWFM-index is $$\\sim $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mo>∼</mml:mo> </mml:math> 2–4x faster than SeqAn3 for nucleotide search, and $$\\sim $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mo>∼</mml:mo> </mml:math> 2–6x faster for amino acid search; it is also $$\\sim $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mo>∼</mml:mo> </mml:math> 4x faster with similar memory footprint when storing the suffix array in on-disk SSD storage. Conclusions AWFM-index is easy to incorporate into bioinformatics software, offers run-time performance parameterization, and provides clients with FM-index functionality at both a high-level (count or locate all instances of a query string) and low-level (step-wise control of the FM-index backward-search process). The open-source library is available for download at https://github.com/TravisWheelerLab/AvxWindowFmIndex.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4225665859",
    "type": "article"
  },
  {
    "title": "Analysis of pattern overlaps and exact computation of P-values of pattern occurrences numbers: case of Hidden Markov Models",
    "doi": "https://doi.org/10.1186/s13015-014-0025-1",
    "publication_date": "2014-12-01",
    "publication_year": 2014,
    "authors": "Mireille Régnier; Е.И. Фурлетова; Victor Yakovlev; Mikhail Roytberg",
    "corresponding_authors": "Mireille Régnier; Е.И. Фурлетова",
    "abstract": "Finding new functional fragments in biological sequences is a challenging problem. Methods addressing this problem commonly search for clusters of pattern occurrences that are statistically significant. A measure of statistical significance is the P-value of a number of pattern occurrences, i.e. the probability to find at least S occurrences of words from a pattern in a random text of length N generated according to a given probability model. All words of the pattern are supposed to be of same length. We present a novel algorithm SufPref that computes an exact P-value for Hidden Markov models (HMM). The algorithm is based on recursive equations on text sets related to pattern occurrences; the equations can be used for any probability model. The algorithm inductively traverses a specific data structure, an overlap graph. The nodes of the graph are associated with the overlaps of words from . The edges are associated to the prefix and suffix relations between overlaps. An originality of our data structure is that pattern need not be explicitly represented in nodes or leaves. The algorithm relies on the Cartesian product of the overlap graph and the graph of HMM states; this approach is analogous to the automaton approach from JBCB 4: 553-569. The gain in size of SufPref data structure leads to significant improvements in space and time complexity compared to existent algorithms. The algorithm SufPref was implemented as a C++ program; the program can be used both as Web-server and a stand alone program for Linux and Windows. The program interface admits special formats to describe probability models of various types (HMM, Bernoulli, Markov); a pattern can be described with a list of words, a PSSM, a degenerate pattern or a word and a number of mismatches. It is available at http://server2.lpm.org.ru/bio/online/sf/ . The program was applied to compare sensitivity and specificity of methods for TFBS prediction based on P-values computed for Bernoulli models, Markov models of orders one and two and HMMs. The experiments show that the methods have approximately the same qualities.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2111670884",
    "type": "article"
  },
  {
    "title": "Probabilistic approaches to alignment with tandem repeats",
    "doi": "https://doi.org/10.1186/1748-7188-9-3",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Michal Nánási; Tomáš Vinař; Broňa Brejová",
    "corresponding_authors": "",
    "abstract": "Short tandem repeats are ubiquitous in genomic sequences and due to their complex evolutionary history pose a challenge for sequence alignment tools. To better account for the presence of tandem repeats in pairwise sequence alignments, we propose a simple tractable pair hidden Markov model that explicitly models their presence. Using the framework of gain functions, we design several optimization criteria for decoding this model and describe resulting decoding algorithms, ranging from the traditional Viterbi and posterior decoding to block-based decoding algorithms tailored to our model. We compare the accuracy of individual decoding algorithms on simulated and real data and find that our approach is superior to the classical three-state pair HMM. Our study illustrates versatility of pair hidden Markov models coupled with appropriate decoding criteria as a modeling tool for capturing complex sequence features.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2114073930",
    "type": "article"
  },
  {
    "title": "Mutual enrichment in ranked lists and the statistical assessment of position weight matrix motifs",
    "doi": "https://doi.org/10.1186/1748-7188-9-11",
    "publication_date": "2014-04-05",
    "publication_year": 2014,
    "authors": "Limor Leibovich; Zohar Yakhini",
    "corresponding_authors": "",
    "abstract": "Statistics in ranked lists is useful in analysing molecular biology measurement data, such as differential expression, resulting in ranked lists of genes, or ChIP-Seq, which yields ranked lists of genomic sequences. State of the art methods study fixed motifs in ranked lists of sequences. More flexible models such as position weight matrix (PWM) motifs are more challenging in this context, partially because it is not clear how to avoid the use of arbitrary thresholds. To assess the enrichment of a PWM motif in a ranked list we use a second ranking on the same set of elements induced by the PWM. Possible orders of one ranked list relative to another can be modelled as permutations. Due to sample space complexity, it is difficult to accurately characterize tail distributions in the group of permutations. In this paper we develop tight upper bounds on tail distributions of the size of the intersection of the top parts of two uniformly and independently drawn permutations. We further demonstrate advantages of this approach using our software implementation, mmHG-Finder, which is publicly available, to study PWM motifs in several datasets. In addition to validating known motifs, we found GC-rich strings to be enriched amongst the promoter sequences of long non-coding RNAs that are specifically expressed in thyroid and prostate tissue samples and observed a statistical association with tissue specific CpG hypo-methylation. We develop tight bounds that can be calculated in polynomial time. We demonstrate utility of mutual enrichment in motif search and assess performance for synthetic and biological datasets. We suggest that thyroid and prostate-specific long non-coding RNAs are regulated by transcription factors that bind GC-rich sequences, such as EGR1, SP1 and E2F3. We further suggest that this regulation is associated with DNA hypo-methylation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2132909884",
    "type": "article"
  },
  {
    "title": "An integrative approach for a network based meta-analysis of viral RNAi screens",
    "doi": "https://doi.org/10.1186/s13015-015-0035-7",
    "publication_date": "2015-02-12",
    "publication_year": 2015,
    "authors": "Sandeep Amberkar; Lars Kaderali",
    "corresponding_authors": "",
    "abstract": "Big data is becoming ubiquitous in biology, and poses significant challenges in data analysis and interpretation. RNAi screening has become a workhorse of functional genomics, and has been applied, for example, to identify host factors involved in infection for a panel of different viruses. However, the analysis of data resulting from such screens is difficult, with often low overlap between hit lists, even when comparing screens targeting the same virus. This makes it a major challenge to select interesting candidates for further detailed, mechanistic experimental characterization.To address this problem we propose an integrative bioinformatics pipeline that allows for a network based meta-analysis of viral high-throughput RNAi screens. Initially, we collate a human protein interaction network from various public repositories, which is then subjected to unsupervised clustering to determine functional modules. Modules that are significantly enriched with host dependency factors (HDFs) and/or host restriction factors (HRFs) are then filtered based on network topology and semantic similarity measures. Modules passing all these criteria are finally interpreted for their biological significance using enrichment analysis, and interesting candidate genes can be selected from the modules.We apply our approach to seven screens targeting three different viruses, and compare results with other published meta-analyses of viral RNAi screens. We recover key hit genes, and identify additional candidates from the screens. While we demonstrate the application of the approach using viral RNAi data, the method is generally applicable to identify underlying mechanisms from hit lists derived from high-throughput experimental data, and to select a small number of most promising genes for further mechanistic studies.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2158784229",
    "type": "article"
  },
  {
    "title": "An algorithm to parse segment packing in predicted protein contact maps",
    "doi": "https://doi.org/10.1186/s13015-016-0080-x",
    "publication_date": "2016-06-17",
    "publication_year": 2016,
    "authors": "William R. Taylor",
    "corresponding_authors": "William R. Taylor",
    "abstract": "The analysis of correlation in alignments generates a matrix of predicted contacts between positions in the structure and while these can arise for many reasons, the simplest explanation is that the pair of residues are in contact in a three-dimensional structure and are affecting each others selection pressure. To analyse these data, A dynamic programming algorithm was developed for parsing secondary structure interactions in predicted contact maps.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2439539979",
    "type": "article"
  },
  {
    "title": "Efficient privacy-preserving variable-length substring match for genome sequence",
    "doi": "https://doi.org/10.1186/s13015-022-00211-1",
    "publication_date": "2022-04-26",
    "publication_year": 2022,
    "authors": "Yoshiki Nakagawa; Satsuya Ohata; Kana Shimizu",
    "corresponding_authors": "Kana Shimizu",
    "abstract": "The development of a privacy-preserving technology is important for accelerating genome data sharing. This study proposes an algorithm that securely searches a variable-length substring match between a query and a database sequence. Our concept hinges on a technique that efficiently applies FM-index for a secret-sharing scheme. More precisely, we developed an algorithm that can achieve a secure table lookup in such a way that [Formula: see text] is computed for a given depth of recursion where [Formula: see text] is an initial position, and V is a vector. We used the secure table lookup for vectors created based on FM-index. The notable feature of the secure table lookup is that time, communication, and round complexities are not dependent on the table length N, after the query input. Therefore, a substring match by reference to the FM-index-based table can also be conducted independently against the database length, and the entire search time is dramatically improved compared to previous approaches. We conducted an experiment using a human genome sequence with the length of 10 million as the database and a query with the length of 100 and found that the query response time of our protocol was at least three orders of magnitude faster than a non-indexed database search protocol under the realistic computation/network environment.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4225249012",
    "type": "article"
  },
  {
    "title": "Reconstructing protein structure from solvent exposure using tabu search",
    "doi": "https://doi.org/10.1186/1748-7188-1-20",
    "publication_date": "2006-10-27",
    "publication_year": 2006,
    "authors": "Martin Paluszewski; Thomas Hamelryck; Paweł Winter",
    "corresponding_authors": "",
    "abstract": "A new, promising solvent exposure measure, called half-sphere-exposure (HSE), has recently been proposed. Here, we study the reconstruction of a protein's Calpha trace solely from structure-derived HSE information. This problem is of relevance for de novo structure prediction using predicted HSE measure. For comparison, we also consider the well-established contact number (CN) measure. We define energy functions based on the HSE- or CN-vectors and minimize them using two conformational search heuristics: Monte Carlo simulation (MCS) and tabu search (TS). While MCS has been the dominant conformational search heuristic in literature, TS has been applied only a few times. To discretize the conformational space, we use lattice models with various complexity.The proposed TS heuristic with a novel tabu definition generally performs better than MCS for this problem. Our experiments show that, at least for small proteins (up to 35 amino acids), it is possible to reconstruct the protein backbone solely from the HSE or CN information. In general, the HSE measure leads to better models than the CN measure, as judged by the RMSD and the angle correlation with the native structure. The angle correlation, a measure of structural similarity, evaluates whether equivalent residues in two structures have the same general orientation. Our results indicate that the HSE measure is potentially very useful to represent solvent exposure in protein structure prediction, design and simulation.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2154386652",
    "type": "article"
  },
  {
    "title": "Modeling genetic imprinting effects of DNA sequences with multilocus polymorphism data",
    "doi": "https://doi.org/10.1186/1748-7188-4-11",
    "publication_date": "2009-08-11",
    "publication_year": 2009,
    "authors": "Sheron Wen; Chenguang Wang; Arthur Berg; Yao Li; Myron M Chang; Roger B. Fillingim; Margaret R. Wallace; Roland Staud; Lee M. Kaplan; Rongling Wu",
    "corresponding_authors": "Rongling Wu",
    "abstract": "Single nucleotide polymorphisms (SNPs) represent the most widespread type of DNA sequence variation in the human genome and they have recently emerged as valuable genetic markers for revealing the genetic architecture of complex traits in terms of nucleotide combination and sequence. Here, we extend an algorithmic model for the haplotype analysis of SNPs to estimate the effects of genetic imprinting expressed at the DNA sequence level. The model provides a general procedure for identifying the number and types of optimal DNA sequence variants that are expressed differently due to their parental origin. The model is used to analyze a genetic data set collected from a pain genetics project. We find that DNA haplotype GAC from three SNPs, OPRKG36T (with two alleles G and T), OPRKA843G (with alleles A and G), and OPRKC846T (with alleles C and T), at the kappa-opioid receptor, triggers a significant effect on pain sensitivity, but with expression significantly depending on the parent from which it is inherited (p = 0.008). With a tremendous advance in SNP identification and automated screening, the model founded on haplotype discovery and statistical inference may provide a useful tool for genetic analysis of any quantitative trait with complex inheritance.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2123559913",
    "type": "article"
  },
  {
    "title": "Efficient edit distance with duplications and contractions",
    "doi": "https://doi.org/10.1186/1748-7188-8-27",
    "publication_date": "2013-10-29",
    "publication_year": 2013,
    "authors": "Tamar Pinhas; Shay Zakov; Dekel Tsur; Michal Ziv-Ukelson",
    "corresponding_authors": "",
    "abstract": ": We propose three algorithms for string edit distance with duplications and contractions. These include an efficient general algorithm and two improvements which apply under certain constraints on the cost function. The new algorithms solve a more general problem variant and obtain better time complexities with respect to previous algorithms. Our general algorithm is based on min-plus multiplication of square matrices and has time and space complexities of O (|Σ|MP (n)) and O (|Σ|n2), respectively, where |Σ| is the alphabet size, n is the length of the strings, and MP (n) is the time bound for the computation of min-plus matrix multiplication of two n × n matrices (currently, MP(n)=On3log3lognlog2n due to an algorithm by Chan).For integer cost functions, the running time is further improved to O|Σ|n3log2n. In addition, this variant of the algorithm is online, in the sense that the input strings may be given letter by letter, and its time complexity bounds the processing time of the first n given letters. This acceleration is based on our efficient matrix-vector min-plus multiplication algorithm, intended for matrices and vectors for which differences between adjacent entries are from a finite integer interval D. Choosing a constant 1log|D|n<λ<1, the algorithm preprocesses an n × n matrix in On2+λ|D| time and On2+λ|D|λ2log|D|2n space. Then, it may multiply the matrix with any given n-length vector in On2λ2log|D|2n time. Under some discreteness assumptions, this matrix-vector min-plus multiplication algorithm applies to several problems from the domains of context-free grammar parsing and RNA folding and, in particular, implies the asymptotically fastest On3log2n time algorithm for single-strand RNA folding with discrete cost functions.Finally, assuming a different constraint on the cost function, we present another version of the algorithm that exploits the run-length encoding of the strings and runs in O|Σ|nMP(ñ)ñ time and O(|Σ|nñ) space, where ñ is the length of the run-length encoding of the strings.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2012746331",
    "type": "article"
  },
  {
    "title": "Accelerating calculations of RNA secondary structure partition functions using GPUs",
    "doi": "https://doi.org/10.1186/1748-7188-8-29",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Harry A. Stern; David H. Mathews",
    "corresponding_authors": "David H. Mathews",
    "abstract": "RNA performs many diverse functions in the cell in addition to its role as a messenger of genetic information. These functions depend on its ability to fold to a unique three-dimensional structure determined by the sequence. The conformation of RNA is in part determined by its secondary structure, or the particular set of contacts between pairs of complementary bases. Prediction of the secondary structure of RNA from its sequence is therefore of great interest, but can be computationally expensive. In this work we accelerate computations of base-pair probababilities using parallel graphics processing units (GPUs).Calculation of the probabilities of base pairs in RNA secondary structures using nearest-neighbor standard free energy change parameters has been implemented using CUDA to run on hardware with multiprocessor GPUs. A modified set of recursions was introduced, which reduces memory usage by about 25%. GPUs are fastest in single precision, and for some hardware, restricted to single precision. This may introduce significant roundoff error. However, deviations in base-pair probabilities calculated using single precision were found to be negligible compared to those resulting from shifting the nearest-neighbor parameters by a random amount of magnitude similar to their experimental uncertainties. For large sequences running on our particular hardware, the GPU implementation reduces execution time by a factor of close to 60 compared with an optimized serial implementation, and by a factor of 116 compared with the original code.Using GPUs can greatly accelerate computation of RNA secondary structure partition functions, allowing calculation of base-pair probabilities for large sequences in a reasonable amount of time, with a negligible compromise in accuracy due to working in single precision. The source code is integrated into the RNAstructure software package and available for download at http://rna.urmc.rochester.edu.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2163199234",
    "type": "article"
  },
  {
    "title": "Efficient algorithms for training the parameters of hidden Markov models using stochastic expectation maximization (EM) training and Viterbi training",
    "doi": "https://doi.org/10.1186/1748-7188-5-38",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Tin Yin Lam; Irmtraud M. Meyer",
    "corresponding_authors": "Irmtraud M. Meyer",
    "abstract": "Hidden Markov models are widely employed by numerous bioinformatics programs used today. Applications range widely from comparative gene prediction to time-series analyses of micro-array data. The parameters of the underlying models need to be adjusted for specific data sets, for example the genome of a particular species, in order to maximize the prediction accuracy. Computationally efficient algorithms for parameter training are thus key to maximizing the usability of a wide range of bioinformatics applications.We introduce two computationally efficient training algorithms, one for Viterbi training and one for stochastic expectation maximization (EM) training, which render the memory requirements independent of the sequence length. Unlike the existing algorithms for Viterbi and stochastic EM training which require a two-step procedure, our two new algorithms require only one step and scan the input sequence in only one direction. We also implement these two new algorithms and the already published linear-memory algorithm for EM training into the hidden Markov model compiler HMM-CONVERTER and examine their respective practical merits for three small example models.Bioinformatics applications employing hidden Markov models can use the two algorithms in order to make Viterbi training and stochastic EM training more computationally efficient. Using these algorithms, parameter training can thus be attempted for more complex models and longer training sequences. The two new algorithms have the added advantage of being easier to implement than the corresponding default algorithms for Viterbi training and stochastic EM training.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2165398449",
    "type": "article"
  },
  {
    "title": "On the computational complexity of the maximum parsimony reconciliation problem in the duplication-loss-coalescence model",
    "doi": "https://doi.org/10.1186/s13015-017-0098-8",
    "publication_date": "2017-03-14",
    "publication_year": 2017,
    "authors": "Daniel Bork; Ricson Cheng; Jincheng Wang; Jean Sung; Ran Libeskind-Hadas",
    "corresponding_authors": "",
    "abstract": "Phylogenetic tree reconciliation is a widely-used method for inferring the evolutionary histories of genes and species. In the duplication-loss-coalescence (DLC) model, we seek a reconciliation that explains the incongruence between a gene and species tree using gene duplication, loss, and deep coalescence events. In the maximum parsimony framework, costs are associated with these event types and a reconciliation is sought that minimizes the total cost of the events required to map the gene tree onto the species tree.We show that this problem is NP-hard even for the special case of minimizing the number of duplications. We then show that the problem is APX-hard when both duplications and losses are considered, implying that no polynomial-time approximation scheme can exist for the problem unless P = NP.These intractability results are likely to guide future research on algorithmic aspects of the DLC-reconciliation problem.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2595167122",
    "type": "article"
  },
  {
    "title": "An efficient algorithm for testing the compatibility of phylogenies with nested taxa",
    "doi": "https://doi.org/10.1186/s13015-017-0099-7",
    "publication_date": "2017-03-16",
    "publication_year": 2017,
    "authors": "Yun Deng; David Fernández‐Baca",
    "corresponding_authors": "",
    "abstract": "Semi-labeled trees generalize ordinary phylogenetic trees, allowing internal nodes to be labeled by higher-order taxa. Taxonomies are examples of semi-labeled trees. Suppose we are given collection [Formula: see text] of semi-labeled trees over various subsets of a set of taxa. The ancestral compatibility problem asks whether there is a semi-labeled tree that respects the clusterings and the ancestor/descendant relationships implied by the trees in [Formula: see text]. The running time and space usage of the best previous algorithm for testing ancestral compatibility depend on the degrees of the nodes in the trees in [Formula: see text].We give a algorithm for the ancestral compatibility problem that runs in [Formula: see text] time and uses [Formula: see text] space, where [Formula: see text] is the total number of nodes and edges in the trees in [Formula: see text].Taxonomies enable researchers to expand greatly the taxonomic coverage of their phylogenetic analyses. The running time of our method does not depend on the degrees of the nodes in the trees in [Formula: see text]. This characteristic is important when taxonomies-which can have nodes of high degree-are used.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2596982519",
    "type": "article"
  },
  {
    "title": "StreAM- $$T_g$$ T g : algorithms for analyzing coarse grained RNA dynamics based on Markov models of connectivity-graphs",
    "doi": "https://doi.org/10.1186/s13015-017-0105-0",
    "publication_date": "2017-05-30",
    "publication_year": 2017,
    "authors": "Sven Jäger; Benjamin Schiller; Philipp Babel; Malte Blumenroth; Thorsten Strufe; Kay Hamacher",
    "corresponding_authors": "Sven Jäger",
    "abstract": "In this work, we present a new coarse grained representation of RNA dynamics. It is based on adjacency matrices and their interactions patterns obtained from molecular dynamics simulations. RNA molecules are well-suited for this representation due to their composition which is mainly modular and assessable by the secondary structure alone. These interactions can be represented as adjacency matrices of k nucleotides. Based on those, we define transitions between states as changes in the adjacency matrices which form Markovian dynamics. The intense computational demand for deriving the transition probability matrices prompted us to develop StreAM- $$T_g$$ , a stream-based algorithm for generating such Markov models of k-vertex adjacency matrices representing the RNA. We benchmark StreAM- $$T_g$$ (a) for random and RNA unit sphere dynamic graphs (b) for the robustness of our method against different parameters. Moreover, we address a riboswitch design problem by applying StreAM- $$T_g$$ on six long term molecular dynamics simulation of a synthetic tetracycline dependent riboswitch (500 ns) in combination with five different antibiotics. The proposed algorithm performs well on large simulated as well as real world dynamic graphs. Additionally, StreAM- $$T_g$$ provides insights into nucleotide based RNA dynamics in comparison to conventional metrics like the root-mean square fluctuation. In the light of experimental data our results show important design opportunities for the riboswitch.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2616999455",
    "type": "article"
  },
  {
    "title": "Finding local genome rearrangements",
    "doi": "https://doi.org/10.1186/s13015-018-0127-2",
    "publication_date": "2018-05-04",
    "publication_year": 2018,
    "authors": "Pijus Simonaitis; Krister M. Swenson",
    "corresponding_authors": "",
    "abstract": "The double cut and join (DCJ) model of genome rearrangement is well studied due to its mathematical simplicity and power to account for the many events that transform gene order. These studies have mostly been devoted to the understanding of minimum length scenarios transforming one genome into another. In this paper we search instead for rearrangement scenarios that minimize the number of rearrangements whose breakpoints are unlikely due to some biological criteria. One such criterion has recently become accessible due to the advent of the Hi-C experiment, facilitating the study of 3D spacial distance between breakpoint regions.We establish a link between the minimum number of unlikely rearrangements required by a scenario and the problem of finding a maximum edge-disjoint cycle packing on a certain transformed version of the adjacency graph. This link leads to a 3/2-approximation as well as an exact integer linear programming formulation for our problem, which we prove to be NP-complete. We also present experimental results on fruit flies, showing that Hi-C data is informative when used as a criterion for rearrangements.A new variant of the weighted DCJ distance problem is addressed that ignores scenario length in its objective function. A solution to this problem provides a lower bound on the number of unlikely moves necessary when transforming one gene order into another. This lower bound aids in the study of rearrangement scenarios with respect to chromatin structure, and could eventually be used in the design of a fixed parameter algorithm with a more general objective function.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2804961397",
    "type": "article"
  },
  {
    "title": "Super short operations on both gene order and intergenic sizes",
    "doi": "https://doi.org/10.1186/s13015-019-0156-5",
    "publication_date": "2019-11-05",
    "publication_year": 2019,
    "authors": "Andre Rodrigues Oliveira; Géraldine Jean; Guillaume Fertin; Ulisses Dias; Zanoni Dias",
    "corresponding_authors": "",
    "abstract": "The evolutionary distance between two genomes can be estimated by computing a minimum length sequence of operations, called genome rearrangements, that transform one genome into another. Usually, a genome is modeled as an ordered sequence of genes, and most of the studies in the genome rearrangement literature consist in shaping biological scenarios into mathematical models. For instance, allowing different genome rearrangements operations at the same time, adding constraints to these rearrangements (e.g., each rearrangement can affect at most a given number of genes), considering that a rearrangement implies a cost depending on its length rather than a unit cost, etc. Most of the works, however, have overlooked some important features inside genomes, such as the presence of sequences of nucleotides between genes, called intergenic regions.In this work, we investigate the problem of computing the distance between two genomes, taking into account both gene order and intergenic sizes. The genome rearrangement operations we consider here are constrained types of reversals and transpositions, called super short reversals (SSRs) and super short transpositions (SSTs), which affect up to two (consecutive) genes. We denote by super short operations (SSOs) any SSR or SST. We show 3-approximation algorithms when the orientation of the genes is not considered when we allow SSRs, SSTs, or SSOs, and 5-approximation algorithms when considering the orientation for either SSRs or SSOs. We also show that these algorithms improve their approximation factors when the input permutation has a higher number of inversions, where the approximation factor decreases from 3 to either 2 or 1.5, and from 5 to either 3 or 2.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2985875441",
    "type": "article"
  },
  {
    "title": "Non-parametric correction of estimated gene trees using TRACTION",
    "doi": "https://doi.org/10.1186/s13015-019-0161-8",
    "publication_date": "2020-01-04",
    "publication_year": 2020,
    "authors": "Sarah Christensen; Erin K. Molloy; Pranjal Vachaspati; Ananya Yammanuru; Tandy Warnow",
    "corresponding_authors": "",
    "abstract": "Abstract Motivation Estimated gene trees are often inaccurate, due to insufficient phylogenetic signal in the single gene alignment, among other causes. Gene tree correction aims to improve the accuracy of an estimated gene tree by using computational techniques along with auxiliary information, such as a reference species tree or sequencing data. However, gene trees and species trees can differ as a result of gene duplication and loss (GDL), incomplete lineage sorting (ILS), and other biological processes. Thus gene tree correction methods need to take estimation error as well as gene tree heterogeneity into account. Many prior gene tree correction methods have been developed for the case where GDL is present. Results Here, we study the problem of gene tree correction where gene tree heterogeneity is instead due to ILS and/or HGT. We introduce TRACTION, a simple polynomial time method that provably finds an optimal solution to the RF-optimal tree refinement and completion (RF-OTRC) Problem, which seeks a refinement and completion of a singly-labeled gene tree with respect to a given singly-labeled species tree so as to minimize the Robinson−Foulds (RF) distance. Our extensive simulation study on 68,000 estimated gene trees shows that TRACTION matches or improves on the accuracy of well-established methods from the GDL literature when HGT and ILS are both present, and ties for best under the ILS-only conditions. Furthermore, TRACTION ties for fastest on these datasets. We also show that a naive generalization of the RF-OTRC problem to multi-labeled trees is possible, but can produce misleading results where gene tree heterogeneity is due to GDL.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3001017271",
    "type": "article"
  },
  {
    "title": "Superbubbles revisited",
    "doi": "https://doi.org/10.1186/s13015-018-0134-3",
    "publication_date": "2018-12-01",
    "publication_year": 2018,
    "authors": "Fabian Gärtner; Lydia Müller; Peter F. Stadler",
    "corresponding_authors": "Fabian Gärtner",
    "abstract": "Superbubbles are distinctive subgraphs in direct graphs that play an important role in assembly algorithms for high-throughput sequencing (HTS) data. Their practical importance derives from the fact they are connected to their host graph by a single entrance and a single exit vertex, thus allowing them to be handled independently. Efficient algorithms for the enumeration of superbubbles are therefore of important for the processing of HTS data. Superbubbles can be identified within the strongly connected components of the input digraph after transforming them into directed acyclic graphs. The algorithm by Sung et al. (IEEE ACM Trans Comput Biol Bioinform 12:770-777, 2015) achieves this task in O(mlog(m)) -time. The extraction of superbubbles from the transformed components was later improved to by Brankovic et al. (Theor Comput Sci 609:374-383, 2016) resulting in an overall O(m+n) -time algorithm.A re-analysis of the mathematical structure of superbubbles showed that the construction of auxiliary DAGs from the strongly connected components in the work of Sung et al. missed some details that can lead to the reporting of false positive superbubbles. We propose an alternative, even simpler auxiliary graph that solved the problem and retains the linear running time for general digraph. Furthermore, we describe a simpler, space-efficient O(m+n) -time algorithm for detecting superbubbles in DAGs that uses only simple data structures.We present a reference implementation of the algorithm that accepts many commonly used formats for the input graph and provides convenient access to the improved algorithm. https://github.com/Fabianexe/Superbubble.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4231484279",
    "type": "article"
  },
  {
    "title": "Bitpacking techniques for indexing genomes: I. Hash tables",
    "doi": "https://doi.org/10.1186/s13015-016-0069-5",
    "publication_date": "2016-04-18",
    "publication_year": 2016,
    "authors": "Thomas D. Wu",
    "corresponding_authors": "Thomas D. Wu",
    "abstract": "Hash tables constitute a widely used data structure for indexing genomes that provides a list of genomic positions for each possible oligomer of a given size. The offset array in a hash table grows exponentially with the oligomer size and precludes the use of larger oligomers that could facilitate rapid alignment of sequences to a genome. We propose to compress the offset array using vectorized bitpacking. We introduce an algorithm and data structure called BP64-columnar that achieves fast random access in arrays of monotonically nondecreasing integers. Experimental results based on hash tables for the fly, chicken, and human genomes show that BP64-columnar is 3 to 4 times faster than publicly available implementations of universal coding schemes, such as Elias gamma, Elias delta, and Fibonacci compression. Furthermore, among vectorized bitpacking schemes, our BP64-columnar format yields retrieval times that are faster than the fastest known bitpacking format by a factor of 3 for retrieving a single value, and a factor of 2 for retrieving two adjacent values. Our BP64-columnar scheme enables compression of genomic hash tables with fast retrieval. It also has potential applications to other domains requiring differential coding with random access.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2339497013",
    "type": "article"
  },
  {
    "title": "Analysis of gene copy number changes in tumor phylogenetics",
    "doi": "https://doi.org/10.1186/s13015-016-0088-2",
    "publication_date": "2016-09-22",
    "publication_year": 2016,
    "authors": "Jun Zhou; Yu Lin; Vaibhav Rajan; William Hoskins; Bing Feng; Jijun Tang",
    "corresponding_authors": "",
    "abstract": "Evolution of cancer cells is characterized by large scale and rapid changes in the chromosomal landscape. The fluorescence in situ hybridization (FISH) technique provides a way to measure the copy numbers of preselected genes in a group of cells and has been found to be a reliable source of data to model the evolution of tumor cells. Chowdhury et al. (Bioinformatics 29(13):189–98, 23; PLoS Comput Biol 10(7):1003740, 24) recently develop a computational model for tumor progression driven by gains and losses in cell count patterns obtained by FISH probes. Their model aims to find the rectilinear Steiner minimum tree (RSMT) (Chowdhury et al. in Bioinformatics 29(13):189–98, 23) and the duplication Steiner minimum tree (DSMT) (Chowdhury et al. in PLoS Comput Biol 10(7):1003740, 24) that describe the progression of FISH cell count patterns over its branches in a parsimonious manner. Both the RSMT and DSMT problems are NP-hard and heuristics are required to solve the problems efficiently. In this paper we propose two approaches to solve the RSMT problem, one inspired by iterative methods to address the \"small phylogeny\" problem (Sankoff et al. in J Mol Evol 7(2):133–49, 27; Blanchette et al. in Genome Inform 8:25–34, 28), and the other based on maximum parsimony phylogeny inference. We further show how to extend these heuristics to obtain solutions to the DSMT problem, that models large scale duplication events. Experimental results from both simulated and real tumor data show that our methods outperform previous heuristics (Chowdhury et al. in Bioinformatics 29(13):189–98, 23; Chowdhury et al. in PLoS Comput Biol 10(7):1003740, 24) in obtaining solutions to both RSMT and DSMT problems. The methods introduced here are able to provide more parsimony phylogenies compared to earlier ones which are consider better choices.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2522536125",
    "type": "article"
  },
  {
    "title": "Probability-based model of protein-protein interactions on biological timescales",
    "doi": "https://doi.org/10.1186/1748-7188-1-25",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Alexander Tournier; Paul W. Fitzjohn; Paul A. Bates",
    "corresponding_authors": "",
    "abstract": "Simulation methods can assist in describing and understanding complex networks of interacting proteins, providing fresh insights into the function and regulation of biological systems. Recent studies have investigated such processes by explicitly modelling the diffusion and interactions of individual molecules. In these approaches, two entities are considered to have interacted if they come within a set cutoff distance of each other. In this study, a new model of bimolecular interactions is presented that uses a simple, probability-based description of the reaction process. This description is well-suited to simulations on timescales relevant to biological systems (from seconds to hours), and provides an alternative to the previous description given by Smoluchowski. In the present approach (TFB) the diffusion process is explicitly taken into account in generating the probability that two freely diffusing chemical entities will interact within a given time interval. It is compared to the Smoluchowski method, as modified by Andrews and Bray (AB). When implemented, the AB & TFB methods give equivalent results in a variety of situations relevant to biology. Overall, the Smoluchowski method as modified by Andrews and Bray emerges as the most simple, robust and efficient method for simulating biological diffusion-reaction processes currently available.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2124685633",
    "type": "article"
  },
  {
    "title": "Refining motifs by improving information content scores using neighborhood profile search",
    "doi": "https://doi.org/10.1186/1748-7188-1-23",
    "publication_date": "2006-11-27",
    "publication_year": 2006,
    "authors": "Chandan K. Reddy; Yao‐Chung Weng; Hsiao-Dong Chiang",
    "corresponding_authors": "",
    "abstract": "The main goal of the motif finding problem is to detect novel, over-represented unknown signals in a set of sequences (e.g. transcription factor binding sites in a genome). The most widely used algorithms for finding motifs obtain a generative probabilistic representation of these over-represented signals and try to discover profiles that maximize the information content score. Although these profiles form a very powerful representation of the signals, the major difficulty arises from the fact that the best motif corresponds to the global maximum of a non-convex continuous function. Popular algorithms like Expectation Maximization (EM) and Gibbs sampling tend to be very sensitive to the initial guesses and are known to converge to the nearest local maximum very quickly. In order to improve the quality of the results, EM is used with multiple random starts or any other powerful stochastic global methods that might yield promising initial guesses (like projection algorithms). Global methods do not necessarily give initial guesses in the convergence region of the best local maximum but rather suggest that a promising solution is in the neighborhood region. In this paper, we introduce a novel optimization framework that searches the neighborhood regions of the initial alignment in a systematic manner to explore the multiple local optimal solutions. This effective search is achieved by transforming the original optimization problem into its corresponding dynamical system and estimating the practical stability boundary of the local maximum. Our results show that the popularly used EM algorithm often converges to sub-optimal solutions which can be significantly improved by the proposed neighborhood profile search. Based on experiments using both synthetic and real datasets, our method demonstrates significant improvements in the information content scores of the probabilistic models. The proposed method also gives the flexibility in using different local solvers and global methods depending on their suitability for some specific datasets.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2142657835",
    "type": "article"
  },
  {
    "title": "A basic analysis toolkit for biological sequences",
    "doi": "https://doi.org/10.1186/1748-7188-2-10",
    "publication_date": "2007-09-18",
    "publication_year": 2007,
    "authors": "Raffaele Giancarlo; Alessandro Siragusa; Enrico Siragusa; Filippo Utro",
    "corresponding_authors": "Raffaele Giancarlo",
    "abstract": "This paper presents a software library, nicknamed BATS, for some basic sequence analysis tasks. Namely, local alignments, via approximate string matching, and global alignments, via longest common subsequence and alignments with affine and concave gap cost functions. Moreover, it also supports filtering operations to select strings from a set and establish their statistical significance, via z-score computation. None of the algorithms is new, but although they are generally regarded as fundamental for sequence analysis, they have not been implemented in a single and consistent software package, as we do here. Therefore, our main contribution is to fill this gap between algorithmic theory and practice by providing an extensible and easy to use software library that includes algorithms for the mentioned string matching and alignment problems. The library consists of C/C++ library functions as well as Perl library functions. It can be interfaced with Bioperl and can also be used as a stand-alone system with a GUI. The software is available at http://www.math.unipa.it/~raffaele/BATS/ under the GNU GPL.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2155303474",
    "type": "article"
  },
  {
    "title": "A linear programming approach for estimating the structure of a sparse linear genetic network from transcript profiling data",
    "doi": "https://doi.org/10.1186/1748-7188-4-5",
    "publication_date": "2009-02-24",
    "publication_year": 2009,
    "authors": "Sahely Bhadra; Chiranjib Bhattacharyya; Nagasuma Chandra; Shahzad I. Mian",
    "corresponding_authors": "",
    "abstract": "Abstract Background A genetic network can be represented as a directed graph in which a node corresponds to a gene and a directed edge specifies the direction of influence of one gene on another. The reconstruction of such networks from transcript profiling data remains an important yet challenging endeavor. A transcript profile specifies the abundances of many genes in a biological sample of interest. Prevailing strategies for learning the structure of a genetic network from high-dimensional transcript profiling data assume sparsity and linearity. Many methods consider relatively small directed graphs, inferring graphs with up to a few hundred nodes. This work examines large undirected graphs representations of genetic networks, graphs with many thousands of nodes where an undirected edge between two nodes does not indicate the direction of influence, and the problem of estimating the structure of such a sparse linear genetic network (SLGN) from transcript profiling data. Results The structure learning task is cast as a sparse linear regression problem which is then posed as a LASSO ( l 1 -constrained fitting) problem and solved finally by formulating a Linear Program (LP). A bound on the Generalization Error of this approach is given in terms of the Leave-One-Out Error. The accuracy and utility of LP-SLGNs is assessed quantitatively and qualitatively using simulated and real data. The Dialogue for Reverse Engineering Assessments and Methods (DREAM) initiative provides gold standard data sets and evaluation metrics that enable and facilitate the comparison of algorithms for deducing the structure of networks. The structures of LP-SLGNs estimated from the I N S ILICO 1, I N S ILICO 2 and I N S ILICO 3 simulated DREAM2 data sets are comparable to those proposed by the first and/or second ranked teams in the DREAM2 competition. The structures of LP-SLGNs estimated from two published Saccharomyces cerevisae cell cycle transcript profiling data sets capture known regulatory associations. In each S. cerevisiae LP-SLGN, the number of nodes with a particular degree follows an approximate power law suggesting that its degree distributions is similar to that observed in real-world networks. Inspection of these LP-SLGNs suggests biological hypotheses amenable to experimental verification. Conclusion A statistically robust and computationally efficient LP-based method for estimating the topology of a large sparse undirected graph from high-dimensional data yields representations of genetic networks that are biologically plausible and useful abstractions of the structures of real genetic networks. Analysis of the statistical and topological properties of learned LP-SLGNs may have practical value; for example, genes with high random walk betweenness, a measure of the centrality of a node in a graph, are good candidates for intervention studies and hence integrated computational – experimental investigations designed to infer more realistic and sophisticated probabilistic directed graphical model representations of genetic networks. The LP-based solutions of the sparse linear regression problem described here may provide a method for learning the structure of transcription factor networks from transcript profiling and transcription factor binding motif data.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2010165912",
    "type": "article"
  },
  {
    "title": "Stability of multiple alignments and phylogenetic trees: an analysis of ABC-transporter proteins family",
    "doi": "https://doi.org/10.1186/1748-7188-3-15",
    "publication_date": "2008-11-06",
    "publication_year": 2008,
    "authors": "Holger Wagner; Burkhard Morgenstern; Andreas Dress",
    "corresponding_authors": "",
    "abstract": "Abstract Background Sequence-based phylogeny reconstruction is a fundamental task in Bioinformatics. Practically all methods for phylogeny reconstruction are based on multiple alignments. The quality and stability of the underlying alignments is therefore crucial for phylogenetic analysis. Results In this short report, we investigate alignments and alignment-based phylogenies constructed for a set of 22 ABC transporters using CLUSTAL W and DIALIGN. Comparing the 22 \"one-out phylogenies\" one can obtain for this sequence set, some intrinsic phylogenetic instability is observed — even if attention is restricted to branches with high bootstrapping frequencies, the so-called safe branches. We show that this instability is caused by the fact that both, CLUSTAL W as well as DIALIGN, apparently get \"confused\" by sequence repeats in some of the ABC-transporter. To deal with such problems, two new DIALIGN options are introduced that prove helpful in our context, the \"exclude-fragment\" (or \"xfr\") and the \"self-comparison\" (or \"sc\") option. Conclusion \"One-out strategies\", known to be a useful tool for testing the stability of all sorts of data-analysis procedures, can successfully be used also in testing alignment stability. In case instabilities are observed, the sequences under consideration should be carefully checked for putative causes. In case one suspects sequence repeats to be the cause, the new \"sc\" option can be used to detect such repeats, and the \"xfr\" option can help to resolve the resulting problems.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2110311614",
    "type": "article"
  },
  {
    "title": "Polynomial algorithms for the Maximal Pairing Problem: efficient phylogenetic targeting on arbitrary trees",
    "doi": "https://doi.org/10.1186/1748-7188-5-25",
    "publication_date": "2010-06-02",
    "publication_year": 2010,
    "authors": "Christian Arnold; Peter F. Stadler",
    "corresponding_authors": "",
    "abstract": "The Maximal Pairing Problem (MPP) is the prototype of a class of combinatorial optimization problems that are of considerable interest in bioinformatics: Given an arbitrary phylogenetic tree T and weights omegaxy for the paths between any two pairs of leaves (x, y), what is the collection of edge-disjoint paths between pairs of leaves that maximizes the total weight? Special cases of the MPP for binary trees and equal weights have been described previously; algorithms to solve the general MPP are still missing, however.We describe a relatively simple dynamic programming algorithm for the special case of binary trees. We then show that the general case of multifurcating trees can be treated by interleaving solutions to certain auxiliary Maximum Weighted Matching problems with an extension of this dynamic programming approach, resulting in an overall polynomial-time solution of complexity (n4 log n) w.r.t. the number n of leaves. The source code of a C implementation can be obtained under the GNU Public License from http://www.bioinf.uni-leipzig.de/Software/Targeting. For binary trees, we furthermore discuss several constrained variants of the MPP as well as a partition function approach to the probabilistic version of the MPP.The algorithms introduced here make it possible to solve the MPP also for large trees with high-degree vertices. This has practical relevance in the field of comparative phylogenetics and, for example, in the context of phylogenetic targeting, i.e., data collection with resource limitations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1986929358",
    "type": "article"
  },
  {
    "title": "Constructing perfect phylogenies and proper triangulations for three-state characters",
    "doi": "https://doi.org/10.1186/1748-7188-7-26",
    "publication_date": "2012-09-24",
    "publication_year": 2012,
    "authors": "Rob Gysel; Fumei Lam; Dan Gusfield",
    "corresponding_authors": "Rob Gysel",
    "abstract": "In this paper, we study the problem of constructing perfect phylogenies for three-state characters. Our work builds on two recent results. The first result states that for three-state characters, the local condition of examining all subsets of three characters is sufficient to determine the global property of admitting a perfect phylogeny. The second result applies tools from minimal triangulation theory to the partition intersection graph to determine if a perfect phylogeny exists. Despite the wealth of combinatorial tools and algorithms stemming from the chordal graph and minimal triangulation literature, it is unclear how to use such approaches to efficiently construct a perfect phylogeny for three-state characters when the data admits one. We utilize structural properties of both the partition intersection graph and the original data in order to achieve a competitive time bound.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2167067890",
    "type": "article"
  },
  {
    "title": "Efficient algorithms for analyzing segmental duplications with deletions and inversions in genomes",
    "doi": "https://doi.org/10.1186/1748-7188-5-11",
    "publication_date": "2010-01-04",
    "publication_year": 2010,
    "authors": "Crystal L. Kahn; Shay Mozes; Benjamin J. Raphael",
    "corresponding_authors": "Crystal L. Kahn; Benjamin J. Raphael",
    "abstract": "Segmental duplications, or low-copy repeats, are common in mammalian genomes. In the human genome, most segmental duplications are mosaics comprised of multiple duplicated fragments. This complex genomic organization complicates analysis of the evolutionary history of these sequences. One model proposed to explain this mosaic patterns is a model of repeated aggregation and subsequent duplication of genomic sequences. We describe a polynomial-time exact algorithm to compute duplication distance, a genomic distance defined as the most parsimonious way to build a target string by repeatedly copying substrings of a fixed source string. This distance models the process of repeated aggregation and duplication. We also describe extensions of this distance to include certain types of substring deletions and inversions. Finally, we provide a description of a sequence of duplication events as a context-free grammar (CFG). These new genomic distances will permit more biologically realistic analyses of segmental duplications in genomes.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2755374003",
    "type": "article"
  },
  {
    "title": "An effective sequence-alignment-free superpositioning of pairwise or multiple structures with missing data",
    "doi": "https://doi.org/10.1186/s13015-016-0079-3",
    "publication_date": "2016-06-21",
    "publication_year": 2016,
    "authors": "Jianbo Lu; Guoliang Xu; Shihua Zhang; Benzhuo Lu",
    "corresponding_authors": "",
    "abstract": "Superpositioning is an important problem in structural biology. Determining an optimal superposition requires a one-to-one correspondence between the atoms of two proteins structures. However, in practice, some atoms are missing from their original structures. Current superposition implementations address the missing data crudely by ignoring such atoms from their structures. In this paper, we propose an effective method for superpositioning pairwise and multiple structures without sequence alignment. It is a two-stage procedure including data reduction and data registration. Numerical experiments demonstrated that our method is effective and efficient. The code package of protein structure superposition method for addressing the cases with missing data is implemented by MATLAB, and it is freely available from: http://sourceforge.net/projects/pssm123/files/?source=navbar",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2466579736",
    "type": "article"
  },
  {
    "title": "Estimation of genetic diversity in viral populations from next generation sequencing data with extremely deep coverage",
    "doi": "https://doi.org/10.1186/s13015-016-0064-x",
    "publication_date": "2016-03-11",
    "publication_year": 2016,
    "authors": "Jean Paulo Lopes Zukurov; Sieberth do Nascimento-Brito; Ângela C. Volpini; Guilherme Oliveira; Luiz Mário Janini; Fernando Antoneli",
    "corresponding_authors": "",
    "abstract": "In this paper we propose a method and discuss its computational implementation as an integrated tool for the analysis of viral genetic diversity on data generated by high-throughput sequencing. Most methods for viral diversity estimation proposed so far are intended to take benefit of the longer reads produced by some NGS platforms in order to estimate a population of haplotypes. Our goal here is to take advantage of distinct virtues of a certain kind of NGS platform - the platform SOLiD (Life Technologies) is an example - that has not received much attention due to the short length of its reads, which renders haplotype estimation very difficult. However, this kind of platform has a very low error rate and extremely deep coverage per site and our method is designed to take advantage of these characteristics. We propose to measure the populational genetic diversity through a family of multinomial probability distributions indexed by the sites of the virus genome, each one representing the populational distribution of the diversity per site. The implementation of the method focuses on two main optimization strategies: a read mapping/alignment procedure that aims at the recovery of the maximum possible number of short-reads; the estimation of the multinomial parameters through a Bayesian approach, which, unlike simple frequency counting, allows one to take into account the prior information of the control population within the inference of a posterior experimental condition and provides a natural way to separate signal from noise, since it automatically furnishes Bayesian confidence intervals. The methods described in this paper have been implemented as an integrated tool called Tanden (Tool for Analysis of Diversity in Viral Populations).",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3103580715",
    "type": "article"
  },
  {
    "title": "Locality-sensitive bucketing functions for the edit distance",
    "doi": "https://doi.org/10.1186/s13015-023-00234-2",
    "publication_date": "2023-07-24",
    "publication_year": 2023,
    "authors": "Ke Chen; Mingfu Shao",
    "corresponding_authors": "",
    "abstract": "Many bioinformatics applications involve bucketing a set of sequences where each sequence is allowed to be assigned into multiple buckets. To achieve both high sensitivity and precision, bucketing methods are desired to assign similar sequences into the same bucket while assigning dissimilar sequences into distinct buckets. Existing k-mer-based bucketing methods have been efficient in processing sequencing data with low error rates, but encounter much reduced sensitivity on data with high error rates. Locality-sensitive hashing (LSH) schemes are able to mitigate this issue through tolerating the edits in similar sequences, but state-of-the-art methods still have large gaps.In this paper, we generalize the LSH function by allowing it to hash one sequence into multiple buckets. Formally, a bucketing function, which maps a sequence (of fixed length) into a subset of buckets, is defined to be [Formula: see text]-sensitive if any two sequences within an edit distance of [Formula: see text] are mapped into at least one shared bucket, and any two sequences with distance at least [Formula: see text] are mapped into disjoint subsets of buckets. We construct locality-sensitive bucketing (LSB) functions with a variety of values of [Formula: see text] and analyze their efficiency with respect to the total number of buckets needed as well as the number of buckets that a specific sequence is mapped to. We also prove lower bounds of these two parameters in different settings and show that some of our constructed LSB functions are optimal.These results lay the theoretical foundations for their practical use in analyzing sequences with high error rates while also providing insights for the hardness of designing ungapped LSH functions.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385214503",
    "type": "article"
  },
  {
    "title": "Efficient gene orthology inference via large-scale rearrangements",
    "doi": "https://doi.org/10.1186/s13015-023-00238-y",
    "publication_date": "2023-09-28",
    "publication_year": 2023,
    "authors": "Diego P. Rubert; Marília D. V. Braga",
    "corresponding_authors": "",
    "abstract": "Recently we developed a gene orthology inference tool based on genome rearrangements (Journal of Bioinformatics and Computational Biology 19:6, 2021). Given a set of genomes our method first computes all pairwise gene similarities. Then it runs pairwise ILP comparisons to compute optimal gene matchings, which minimize, by taking the similarities into account, the weighted rearrangement distance between the analyzed genomes (a problem that is NP-hard). The gene matchings are then integrated into gene families in the final step. The mentioned ILP includes an optimal capping that connects each end of a linear segment of one genome to an end of a linear segment in the other genome, producing an exponential increase of the search space.In this work, we design and implement a heuristic capping algorithm that replaces the optimal capping by clustering (based on their gene content intersections) the linear segments into [Formula: see text] subsets, whose ends are capped independently. Furthermore, in each subset, instead of allowing all possible connections, we let only the ends of content-related segments be connected. Although there is no guarantee that m is much bigger than one, and with the possible side effect of resulting in sub-optimal instead of optimal gene matchings, the heuristic works very well in practice, from both the speed performance and the quality of computed solutions. Our experiments on primate and fruit fly genomes show two positive results. First, for complete assemblies of five primates the version with heuristic capping reports orthologies that are very similar to the orthologies computed by the version of our tool with optimal capping. Second, we were able to efficiently analyze fruit fly genomes with incomplete assemblies distributed in hundreds or even thousands of contigs, obtaining gene families that are very similar to [Formula: see text] families. Indeed, our tool inferred a higher number of complete cliques, with a higher intersection with [Formula: see text], when compared to gene families computed by other inference tools. We added a post-processing for refining, with the aid of the [Formula: see text] algorithm, our ambiguous families (those with more than one gene per genome), improving even more the accuracy of our results. Our approach is implemented into a pipeline incorporating the pre-computation of gene similarities and the post-processing refinement of ambiguous families with [Formula: see text]. Both the original version with optimal capping and the new modified version with heuristic capping can be downloaded, together with their detailed documentations, at https://gitlab.ub.uni-bielefeld.de/gi/FFGC or as a Conda package at https://anaconda.org/bioconda/ffgc .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387141270",
    "type": "article"
  },
  {
    "title": "An asymmetric approach to preserve common intervals while sorting by reversals",
    "doi": "https://doi.org/10.1186/1748-7188-4-16",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Marília D. V. Braga; Christian Gautier; Marie‐France Sagot",
    "corresponding_authors": "",
    "abstract": "The reversal distance and optimal sequences of reversals to transform a genome into another are useful tools to analyse evolutionary scenarios. However, the number of sequences is huge and some additional criteria should be used to obtain a more accurate analysis. One strategy is searching for sequences that respect constraints, such as the common intervals (clusters of co-localised genes). Another approach is to explore the whole space of sorting sequences, eventually grouping them into classes of equivalence. Recently both strategies started to be put together, to restrain the space to the sequences that respect constraints. In particular an algorithm has been proposed to list classes whose sorting sequences do not break the common intervals detected between the two initial genomes A and B. This approach may reduce the space of sequences and is symmetric (the result of the analysis sorting A into B can be obtained from the analysis sorting B into A).We propose an alternative approach to restrain the space of sorting sequences, using progressive instead of initial detection of common intervals (the list of common intervals is updated after applying each reversal). This may reduce the space of sequences even more, but is shown to be asymmetric.We suggest that our method may be more realistic when the relation ancestor-descendant between the analysed genomes is clear and we apply it to do a better characterisation of the evolutionary scenario of the bacterium Rickettsia felis with respect to one of its ancestors.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2133458864",
    "type": "article"
  },
  {
    "title": "Sorting signed circular permutations by super short operations",
    "doi": "https://doi.org/10.1186/s13015-018-0131-6",
    "publication_date": "2018-07-25",
    "publication_year": 2018,
    "authors": "Andre Rodrigues Oliveira; Guillaume Fertin; Ulisses Dias; Zanoni Dias",
    "corresponding_authors": "",
    "abstract": "One way to estimate the evolutionary distance between two given genomes is to determine the minimum number of large-scale mutations, or genome rearrangements, that are necessary to transform one into the other. In this context, genomes can be represented as ordered sequences of genes, each gene being represented by a signed integer. If no gene is repeated, genomes are thus modeled as signed permutations of the form π=(π1π2…πn) , and in that case we can consider without loss of generality that one of them is the identity permutation ιn=(12…n) , and that we just need to sort the other (i.e., transform it into ιn ). The most studied genome rearrangement events are reversals, where a segment of the genome is reversed and reincorporated at the same location; and transpositions, where two consecutive segments are exchanged. Many variants, e.g., combining different types of (possibly constrained) rearrangements, have been proposed in the literature. One of them considers that the number of genes involved, in a reversal or a transposition, is never greater than two, which is known as the problem of sorting by super short operations (or SSOs).All problems considering SSOs in permutations have been shown to be in P , except for one, namely sorting signed circular permutations by super short reversals and super short transpositions. Here we fill this gap by introducing a new graph structure called cyclic permutation graph and providing a series of intermediate results, which allows us to design a polynomial algorithm for sorting signed circular permutations by super short reversals and super short transpositions.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2884488099",
    "type": "article"
  },
  {
    "title": "Repairing Boolean logical models from time-series data using Answer Set Programming",
    "doi": "https://doi.org/10.1186/s13015-019-0145-8",
    "publication_date": "2019-03-25",
    "publication_year": 2019,
    "authors": "Alexandre Lemos; Inês Lynce; Pedro T. Monteiro",
    "corresponding_authors": "Alexandre Lemos",
    "abstract": "Boolean models of biological signalling-regulatory networks are increasingly used to formally describe and understand complex biological processes. These models may become inconsistent as new data become available and need to be repaired. In the past, the focus has been shed on the inference of (classes of) models given an interaction network and time-series data sets. However, repair of existing models against new data is still in its infancy, where the process is still manually performed and therefore slow and prone to errors.In this work, we propose a method with an associated tool to suggest repairs over inconsistent Boolean models, based on a set of atomic repair operations. Answer Set Programming is used to encode the minimal repair problem as a combinatorial optimization problem. In particular, given an inconsistent model, the tool provides the minimal repairs that render the model capable of generating dynamics coherent with a (set of) time-series data set(s), considering either a synchronous or an asynchronous updating scheme.The method was validated using known biological models from different species, as well as synthetic models obtained from randomly generated networks. We discuss the method's limitations regarding each of the updating schemes and the considered minimization algorithm.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2940498855",
    "type": "article"
  },
  {
    "title": "LazyB: fast and cheap genome assembly",
    "doi": "https://doi.org/10.1186/s13015-021-00186-5",
    "publication_date": "2021-06-01",
    "publication_year": 2021,
    "authors": "Thomas Gatter; Sarah von Löhneysen; Jörg Fallmann; Polina Drozdova; Tom Hartmann; Peter F. Stadler",
    "corresponding_authors": "Thomas Gatter",
    "abstract": "Advances in genome sequencing over the last years have lead to a fundamental paradigm shift in the field. With steadily decreasing sequencing costs, genome projects are no longer limited by the cost of raw sequencing data, but rather by computational problems associated with genome assembly. There is an urgent demand for more efficient and and more accurate methods is particular with regard to the highly complex and often very large genomes of animals and plants. Most recently, \"hybrid\" methods that integrate short and long read data have been devised to address this need.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3165826709",
    "type": "article"
  },
  {
    "title": "Distinguishing linear and branched evolution given single-cell DNA sequencing data of tumors",
    "doi": "https://doi.org/10.1186/s13015-021-00194-5",
    "publication_date": "2021-07-06",
    "publication_year": 2021,
    "authors": "Leah L. Weber; Mohammed El-Kebir",
    "corresponding_authors": "",
    "abstract": "Abstract Background Cancer arises from an evolutionary process where somatic mutations give rise to clonal expansions. Reconstructing this evolutionary process is useful for treatment decision-making as well as understanding evolutionary patterns across patients and cancer types. In particular, classifying a tumor’s evolutionary process as either linear or branched and understanding what cancer types and which patients have each of these trajectories could provide useful insights for both clinicians and researchers. While comprehensive cancer phylogeny inference from single-cell DNA sequencing data is challenging due to limitations with current sequencing technology and the complexity of the resulting problem, current data might provide sufficient signal to accurately classify a tumor’s evolutionary history as either linear or branched. Results We introduce the Linear Perfect Phylogeny Flipping (LPPF) problem as a means of testing two alternative hypotheses for the pattern of evolution, which we prove to be NP-hard. We develop Phyolin, which uses constraint programming to solve the LPPF problem. Through both in silico experiments and real data application, we demonstrate the performance of our method, outperforming a competing machine learning approach. Conclusion Phyolin is an accurate, easy to use and fast method for classifying an evolutionary trajectory as linear or branched given a tumor’s single-cell DNA sequencing data.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3183032853",
    "type": "article"
  },
  {
    "title": "Haplotypes versus genotypes on pedigrees",
    "doi": "https://doi.org/10.1186/1748-7188-6-10",
    "publication_date": "2011-04-19",
    "publication_year": 2011,
    "authors": "Bonnie Kirkpatrick",
    "corresponding_authors": "Bonnie Kirkpatrick",
    "abstract": "Genome sequencing will soon produce haplotype data for individuals. For pedigrees of related individuals, sequencing appears to be an attractive alternative to genotyping. However, methods for pedigree analysis with haplotype data have not yet been developed, and the computational complexity of such problems has been an open question. Furthermore, it is not clear in which scenarios haplotype data would provide better estimates than genotype data for quantities such as recombination rates. To answer these questions, a reduction is given from genotype problem instances to haplotype problem instances, and it is shown that solving the haplotype problem yields the solution to the genotype problem, up to constant factors or coefficients. The pedigree analysis problems we will consider are the likelihood, maximum probability haplotype, and minimum recombination haplotype problems. Two algorithms are introduced: an exponential-time hidden Markov model (HMM) for haplotype data where some individuals are untyped, and a linear-time algorithm for pedigrees having haplotype data for all individuals. Recombination estimates from the general haplotype HMM algorithm are compared to recombination estimates produced by a genotype HMM. Having haplotype data on all individuals produces better estimates. However, having several untyped individuals can drastically reduce the utility of haplotype data.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1982581704",
    "type": "article"
  },
  {
    "title": "On the group theoretical background of assigning stepwise mutations onto phylogenies",
    "doi": "https://doi.org/10.1186/1748-7188-7-36",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Mareike Fischer; Steffen Klaere; Minh Anh Nguyen; Arndt von Haeseler",
    "corresponding_authors": "Steffen Klaere",
    "abstract": "Abstract Background Recently one step mutation matrices were introduced to model the impact of substitutions on arbitrary branches of a phylogenetic tree on an alignment site. This concept works nicely for the four-state nucleotide alphabet and provides an efficient procedure conjectured to compute the minimal number of substitutions needed to transform one alignment site into another. The present paper delivers a proof of the validity of this algorithm. Moreover, we provide several mathematical insights into the generalization of the OSM matrix to multi-state alphabets. The construction of the OSM matrix is only possible if the matrices representing the substitution types acting on the character states and the identity matrix form a commutative group with respect to matrix multiplication. We illustrate this approach by looking at Abelian groups over twenty states and critically discuss their biological usefulness when investigating amino acids.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1992783412",
    "type": "article"
  },
  {
    "title": "On the combinatorics of sparsification",
    "doi": "https://doi.org/10.1186/1748-7188-7-28",
    "publication_date": "2012-10-22",
    "publication_year": 2012,
    "authors": "Fenix W. D. Huang; Christian M. Reidys",
    "corresponding_authors": "",
    "abstract": "We study the sparsification of dynamic programming based on folding algorithms of RNA structures. Sparsification is a method that improves significantly the computation of minimum free energy (mfe) RNA structures. We provide a quantitative analysis of the sparsification of a particular decomposition rule, Λ∗. This rule splits an interval of RNA secondary and pseudoknot structures of fixed topological genus. Key for quantifying sparsifications is the size of the so called candidate sets. Here we assume mfe-structures to be specifically distributed (see Assumption 1) within arbitrary and irreducible RNA secondary and pseudoknot structures of fixed topological genus. We then present a combinatorial framework which allows by means of probabilities of irreducible sub-structures to obtain the expectation of the Λ∗-candidate set w.r.t. a uniformly random input sequence. We compute these expectations for arc-based energy models via energy-filtered generating functions (GF) in case of RNA secondary structures as well as RNA pseudoknot structures. Furthermore, for RNA secondary structures we also analyze a simplified loop-based energy model. Our combinatorial analysis is then compared to the expected number of Λ∗-candidates obtained from the folding mfe-structures. In case of the mfe-folding of RNA secondary structures with a simplified loop-based energy model our results imply that sparsification provides a significant, constant improvement of 91% (theory) to be compared to an 96% (experimental, simplified arc-based model) reduction. However, we do not observe a linear factor improvement. Finally, in case of the “full” loop-energy model we can report a reduction of 98% (experiment). Sparsification was initially attributed a linear factor improvement. This conclusion was based on the so called polymer-zeta property, which stems from interpreting polymer chains as self-avoiding walks. Subsequent findings however reveal that the O(n) improvement is not correct. The combinatorial analysis presented here shows that, assuming a specific distribution (see Assumption 1), of mfe-structures within irreducible and arbitrary structures, the expected number of Λ∗-candidates is Θ(n2). However, the constant reduction is quite significant, being in the range of 96%. We furthermore show an analogous result for the sparsification of the Λ∗-decomposition rule for RNA pseudoknotted structures of genus one. Finally we observe that the effect of sparsification is sensitive to the employed energy model.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2102833079",
    "type": "article"
  },
  {
    "title": "On the protein folding problem in 2D-triangular lattices",
    "doi": "https://doi.org/10.1186/1748-7188-8-30",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "A. S. M. Sohidull Islam; M. Sohel Rahman",
    "corresponding_authors": "",
    "abstract": "In this paper, we present a novel approximation algorithm to solve the protein folding problem in HP model. Our algorithm is polynomial in terms of the length of the given HP string. The expected approximation ratio of our algorithm is for n ≥ 6, where n2 is the total number of H's in a given HP string. The expected approximation ratio tends to reach 1 for large values of n. Hence our algorithm is expected to perform very well for larger HP strings.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2109822494",
    "type": "article"
  },
  {
    "title": "GRISOTTO: A greedy approach to improve combinatorial algorithms for motif discovery with prior knowledge",
    "doi": "https://doi.org/10.1186/1748-7188-6-13",
    "publication_date": "2011-04-22",
    "publication_year": 2011,
    "authors": "Alexandra M. Carvalho; Arlindo L. Oliveira",
    "corresponding_authors": "",
    "abstract": "Position-specific priors (PSP) have been used with success to boost EM and Gibbs sampler-based motif discovery algorithms. PSP information has been computed from different sources, including orthologous conservation, DNA duplex stability, and nucleosome positioning. The use of prior information has not yet been used in the context of combinatorial algorithms. Moreover, priors have been used only independently, and the gain of combining priors from different sources has not yet been studied. We extend RISOTTO, a combinatorial algorithm for motif discovery, by post-processing its output with a greedy procedure that uses prior information. PSP's from different sources are combined into a scoring criterion that guides the greedy search procedure. The resulting method, called GRISOTTO, was evaluated over 156 yeast TF ChIP-chip sequence-sets commonly used to benchmark prior-based motif discovery algorithms. Results show that GRISOTTO is at least as accurate as other twelve state-of-the-art approaches for the same task, even without combining priors. Furthermore, by considering combined priors, GRISOTTO is considerably more accurate than the state-of-the-art approaches for the same task. We also show that PSP's improve GRISOTTO ability to retrieve motifs from mouse ChiP-seq data, indicating that the proposed algorithm can be applied to data from a different technology and for a higher eukaryote. The conclusions of this work are twofold. First, post-processing the output of combinatorial algorithms by incorporating prior information leads to a very efficient and effective motif discovery method. Second, combining priors from different sources is even more beneficial than considering them separately.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2119678160",
    "type": "article"
  },
  {
    "title": "Protein Structure Idealization: How accurately is it possible to model protein structures with dihedral angles?",
    "doi": "https://doi.org/10.1186/1748-7188-8-5",
    "publication_date": "2013-02-25",
    "publication_year": 2013,
    "authors": "Xuefeng Cui; Shuai Cheng Li; Dongbo Bu; Babak Alipanahi; Ming Li",
    "corresponding_authors": "",
    "abstract": ": Previous studies show that the same type of bond lengths and angles fit Gaussian distributions well with small standard deviations on high resolution protein structure data. The mean values of these Gaussian distributions have been widely used as ideal bond lengths and angles in bioinformatics. However, we are not aware of any research done to evaluate how accurately we can model protein structures with dihedral angles and ideal bond lengths and angles.Here, we introduce the protein structure idealization problem. We focus on the protein backbone structure idealization. We describe a fast O(nm/ε) dynamic programming algorithm to find an idealized protein backbone structure that is approximately optimal according to our scoring function. The scoring function evaluates not only the free energy, but also the similarity with the target structure. Thus, the idealized protein structures found by our algorithm are guaranteed to be protein-like and close to the target protein structure.We have implemented our protein structure idealization algorithm and idealized the high resolution protein structures with low sequence identities of the CULLPDB_PC30_RES1.6_R0.25 data set. We demonstrate that idealized backbone structures always exist with small changes and significantly better free energy. We also applied our algorithm to refine protein pseudo-structures determined in NMR experiments.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2147043108",
    "type": "article"
  },
  {
    "title": "The feasibility of genome-scale biological network inference using Graphics Processing Units",
    "doi": "https://doi.org/10.1186/s13015-017-0100-5",
    "publication_date": "2017-03-20",
    "publication_year": 2017,
    "authors": "Thiagarajan Raghuram; Amir Alavi; Jagdeep T. Podichetty; Jason N. Bazil; Daniel Beard",
    "corresponding_authors": "",
    "abstract": "Systems research spanning fields from biology to finance involves the identification of models to represent the underpinnings of complex systems. Formal approaches for data-driven identification of network interactions include statistical inference-based approaches and methods to identify dynamical systems models that are capable of fitting multivariate data. Availability of large data sets and so-called 'big data' applications in biology present great opportunities as well as major challenges for systems identification/reverse engineering applications. For example, both inverse identification and forward simulations of genome-scale gene regulatory network models pose compute-intensive problems. This issue is addressed here by combining the processing power of Graphics Processing Units (GPUs) and a parallel reverse engineering algorithm for inference of regulatory networks. It is shown that, given an appropriate data set, information on genome-scale networks (systems of 1000 or more state variables) can be inferred using a reverse-engineering algorithm in a matter of days on a small-scale modern GPU cluster.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2602605148",
    "type": "article"
  },
  {
    "title": "Bitpacking techniques for indexing genomes: II. Enhanced suffix arrays",
    "doi": "https://doi.org/10.1186/s13015-016-0068-6",
    "publication_date": "2016-04-23",
    "publication_year": 2016,
    "authors": "Thomas D. Wu",
    "corresponding_authors": "Thomas D. Wu",
    "abstract": "Suffix arrays and their variants are used widely for representing genomes in search applications. Enhanced suffix arrays (ESAs) provide fast search speed, but require large auxiliary data structures for storing longest common prefix and child interval information. We explore techniques for compressing ESAs to accelerate genomic search and reduce memory requirements. We evaluate various bitpacking techniques that store integers in fewer than 32 bits each, as well as bytecoding methods that reserve a single byte per integer whenever possible. Our results on the fly, chicken, and human genomes show that bytecoding with an exception guide array is the fastest method for retrieving auxiliary information. Genomic searching can be further accelerated using a data structure called a discriminating character array, which reduces memory accesses to the suffix array and the genome string. Finally, integrating storage of the auxiliary and discriminating character arrays further speeds up genomic search. The combination of exception guide arrays, a discriminating character array, and integrated data storage provide a 2- to 3-fold increase in speed for genomic searching compared with using bytecoding alone, and is 20 % faster and 40 % more space-efficient than an uncompressed ESA.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2338810257",
    "type": "article"
  },
  {
    "title": "Towards sub-quadratic time and space complexity solutions for the dated tree reconciliation problem",
    "doi": "https://doi.org/10.1186/s13015-016-0077-5",
    "publication_date": "2016-05-21",
    "publication_year": 2016,
    "authors": "Benjamin Drinkwater; Michael Charleston",
    "corresponding_authors": "",
    "abstract": "Recent coevolutionary analysis has considered tree topology as a means to reduce the asymptotic complexity associated with inferring the complex coevolutionary interrelationships that arise between phylogenetic trees. Targeted algorithmic design for specific tree topologies has to date been highly successful, with one recent formulation providing a logarithmic space complexity reduction for the dated tree reconciliation problem. In this work we build on this prior analysis providing a further asymptotic space reduction, by providing a new formulation for the dynamic programming table used by a number of popular coevolutionary analysis techniques. This model gives rise to a sub quadratic running time solution for the dated tree reconciliation problem for selected tree topologies, and is shown to be, in practice, the fastest method for solving the dated tree reconciliation problem for expected evolutionary trees. This result is achieved through the analysis of not only the topology of the trees considered for coevolutionary analysis, but also the underlying structure of the dynamic programming algorithms that are traditionally applied to such analysis. The newly inferred theoretical complexity bounds introduced herein are then validated using a combination of synthetic and biological data sets, where the proposed model is shown to provide an $$O(\\sqrt{n})$$ space saving, while it is observed to run in half the time compared to the fastest known algorithm for solving the dated tree reconciliation problem. What is even more significant is that the algorithm derived herein is able to guarantee the optimality of its inferred solution, something that algorithms of comparable speed have to date been unable to achieve.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2397754459",
    "type": "article"
  },
  {
    "title": "On the use of cartographic projections in visualizing phylo-genetic tree space",
    "doi": "https://doi.org/10.1186/1748-7188-5-26",
    "publication_date": "2010-06-08",
    "publication_year": 2010,
    "authors": "Kenneth Sundberg; Mark Clement; Quinn Snell",
    "corresponding_authors": "",
    "abstract": "Phylogenetic analysis is becoming an increasingly important tool for biological research. Applications include epidemiological studies, drug development, and evolutionary analysis. Phylogenetic search is a known NP-Hard problem. The size of the data sets which can be analyzed is limited by the exponential growth in the number of trees that must be considered as the problem size increases. A better understanding of the problem space could lead to better methods, which in turn could lead to the feasible analysis of more data sets. We present a definition of phylogenetic tree space and a visualization of this space that shows significant exploitable structure. This structure can be used to develop search methods capable of handling much larger data sets.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2129838374",
    "type": "article"
  },
  {
    "title": "Connectivity problems on heterogeneous graphs",
    "doi": "https://doi.org/10.1186/s13015-019-0141-z",
    "publication_date": "2019-03-08",
    "publication_year": 2019,
    "authors": "Jimmy Ming‐Tai Wu; Alex Khodaverdian; Benjamin Weitz; Nir Yosef",
    "corresponding_authors": "",
    "abstract": "Network connectivity problems are abundant in computational biology research, where graphs are used to represent a range of phenomena: from physical interactions between molecules to more abstract relationships such as gene co-expression. One common challenge in studying biological networks is the need to extract meaningful, small subgraphs out of large databases of potential interactions. A useful abstraction for this task turned out to be the Steiner Network problems: given a reference \"database\" graph, find a parsimonious subgraph that satisfies a given set of connectivity demands. While this formulation proved useful in a number of instances, the next challenge is to account for the fact that the reference graph may not be static. This can happen for instance, when studying protein measurements in single cells or at different time points, whereby different subsets of conditions can have different protein milieu.We introduce the condition Steiner Network problem in which we concomitantly consider a set of distinct biological conditions. Each condition is associated with a set of connectivity demands, as well as a set of edges that are assumed to be present in that condition. The goal of this problem is to find a minimal subgraph that satisfies all the demands through paths that are present in the respective condition. We show that introducing multiple conditions as an additional factor makes this problem much harder to approximate. Specifically, we prove that for C conditions, this new problem is NP-hard to approximate to a factor of C-ϵ , for every C≥2 and ϵ>0 , and that this bound is tight. Moving beyond the worst case, we explore a special set of instances where the reference graph grows monotonically between conditions, and show that this problem admits substantially improved approximation algorithms. We also developed an integer linear programming solver for the general problem and demonstrate its ability to reach optimality with instances from the human protein interaction network.Our results demonstrate that in contrast to most connectivity problems studied in computational biology, accounting for multiplicity of biological conditions adds considerable complexity, which we propose to address with a new solver. Importantly, our results extend to several network connectivity problems that are commonly used in computational biology, such as Prize-Collecting Steiner Tree, and provide insight into the theoretical guarantees for their applications in a multiple condition setting.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2949811105",
    "type": "article"
  },
  {
    "title": "The distance and median problems in the single-cut-or-join model with single-gene duplications",
    "doi": "https://doi.org/10.1186/s13015-020-00169-y",
    "publication_date": "2020-05-04",
    "publication_year": 2020,
    "authors": "Aniket Mane; Manuel Lafond; Pedro Feijão; Cédric Chauve",
    "corresponding_authors": "",
    "abstract": "In the field of genome rearrangement algorithms, models accounting for gene duplication lead often to hard problems. For example, while computing the pairwise distance is tractable in most duplication-free models, the problem is NP-complete for most extensions of these models accounting for duplicated genes. Moreover, problems involving more than two genomes, such as the genome median and the Small Parsimony problem, are intractable for most duplication-free models, with some exceptions, for example the Single-Cut-or-Join (SCJ) model.We introduce a variant of the SCJ distance that accounts for duplicated genes, in the context of directed evolution from an ancestral genome to a descendant genome where orthology relations between ancestral genes and their descendant are known. Our model includes two duplication mechanisms: single-gene tandem duplication and the creation of single-gene circular chromosomes. We prove that in this model, computing the directed distance and a parsimonious evolutionary scenario in terms of SCJ and single-gene duplication events can be done in linear time. We also show that the directed median problem is tractable for this distance, while the rooted median problem, where we assume that one of the given genomes is ancestral to the median, is NP-complete. We also describe an Integer Linear Program for solving this problem. We evaluate the directed distance and rooted median algorithms on simulated data.Our results provide a simple genome rearrangement model, extending the SCJ model to account for single-gene duplications, for which we prove a mix of tractability and hardness results. For the NP-complete rooted median problem, we design a simple Integer Linear Program. Our publicly available implementation of these algorithms for the directed distance and median problems allow to solve efficiently these problems on large instances.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3021210550",
    "type": "article"
  },
  {
    "title": "Context-aware seeds for read mapping",
    "doi": "https://doi.org/10.1186/s13015-020-00172-3",
    "publication_date": "2020-05-23",
    "publication_year": 2020,
    "authors": "Hongyi Xin; Mingfu Shao; Carl Kingsford",
    "corresponding_authors": "",
    "abstract": "Abstract Motivation Most modern seed-and-extend NGS read mappers employ a seeding scheme that requires extracting t non-overlapping seeds in each read in order to find all valid mappings under an edit distance threshold of t . As t grows, this seeding scheme forces mappers to use more and shorter seeds, which increases the seed hits (seed frequencies) and therefore reduces the efficiency of mappers. Results We propose a novel seeding framework, context-aware seeds (CAS). CAS guarantees finding all valid mappings but uses fewer (and longer) seeds, which reduces seed frequencies and increases efficiency of mappers. CAS achieves this improvement by attaching a confidence radius to each seed in the reference. We prove that all valid mappings can be found if the sum of confidence radii of seeds are greater than t . CAS generalizes the existing pigeonhole-principle-based seeding scheme in which this confidence radius is implicitly always 1. Moreover, we design an efficient algorithm that constructs the confidence radius database in linear time. We experiment CAS with E. coli genome and show that CAS significantly reduces seed frequencies when compared with the state-of-the-art pigeonhole-principle-based seeding algorithm, the Optimal Seed Solver. Availability https://github.com/Kingsford-Group/CAS_code",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3030758078",
    "type": "article"
  },
  {
    "title": "On an enhancement of RNA probing data using information theory",
    "doi": "https://doi.org/10.1186/s13015-020-00176-z",
    "publication_date": "2020-08-07",
    "publication_year": 2020,
    "authors": "Thomas J. X. Li; Christian M. Reidys",
    "corresponding_authors": "",
    "abstract": "Abstract Identifying the secondary structure of an RNA is crucial for understanding its diverse regulatory functions. This paper focuses on how to enhance target identification in a Boltzmann ensemble of structures via chemical probing data. We employ an information-theoretic approach to solve the problem, via considering a variant of the Rényi-Ulam game. Our framework is centered around the ensemble tree, a hierarchical bi-partition of the input ensemble, that is constructed by recursively querying about whether or not a base pair of maximum information entropy is contained in the target. These queries are answered via relating local with global probing data, employing the modularity in RNA secondary structures. We present that leaves of the tree are comprised of sub-samples exhibiting a distinguished structure with high probability. In particular, for a Boltzmann ensemble incorporating probing data, which is well established in the literature, the probability of our framework correctly identifying the target in the leaf is greater than $$90\\%$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mn>90</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3047696866",
    "type": "article"
  },
  {
    "title": "A virtual pebble game to ensemble average graph rigidity",
    "doi": "https://doi.org/10.1186/s13015-015-0039-3",
    "publication_date": "2015-03-17",
    "publication_year": 2015,
    "authors": "Luis C. González; Hui Wang; Dennis R. Livesay; Donald J. Jacobs",
    "corresponding_authors": "",
    "abstract": "The body-bar Pebble Game (PG) algorithm is commonly used to calculate network rigidity properties in proteins and polymeric materials. To account for fluctuating interactions such as hydrogen bonds, an ensemble of constraint topologies are sampled, and average network properties are obtained by averaging PG characterizations. At a simpler level of sophistication, Maxwell constraint counting (MCC) provides a rigorous lower bound for the number of internal degrees of freedom (DOF) within a body-bar network, and it is commonly employed to test if a molecular structure is globally under-constrained or over-constrained. MCC is a mean field approximation (MFA) that ignores spatial fluctuations of distance constraints by replacing the actual molecular structure by an effective medium that has distance constraints globally distributed with perfect uniform density. The Virtual Pebble Game (VPG) algorithm is a MFA that retains spatial inhomogeneity in the density of constraints on all length scales. Network fluctuations due to distance constraints that may be present or absent based on binary random dynamic variables are suppressed by replacing all possible constraint topology realizations with the probabilities that distance constraints are present. The VPG algorithm is isomorphic to the PG algorithm, where integers for counting “pebbles” placed on vertices or edges in the PG map to real numbers representing the probability to find a pebble. In the VPG, edges are assigned pebble capacities, and pebble movements become a continuous flow of probability within the network. Comparisons between the VPG and average PG results over a test set of proteins and disordered lattices demonstrate the VPG quantitatively estimates the ensemble average PG results well. The VPG performs about 20% faster than one PG, and it provides a pragmatic alternative to averaging PG rigidity characteristics over an ensemble of constraint topologies. The utility of the VPG falls in between the most accurate but slowest method of ensemble averaging over hundreds to thousands of independent PG runs, and the fastest but least accurate MCC.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2102169545",
    "type": "article"
  },
  {
    "title": "A strand specific high resolution normalization method for chip-sequencing data employing multiple experimental control measurements",
    "doi": "https://doi.org/10.1186/1748-7188-7-2",
    "publication_date": "2012-01-16",
    "publication_year": 2012,
    "authors": "Stefan Enroth; Claes Andersson; Robin Andersson; Claes Wadelius; Mats G. Gustafsson; Jan Komorowski",
    "corresponding_authors": "Jan Komorowski",
    "abstract": "Abstract Background High-throughput sequencing is becoming the standard tool for investigating protein-DNA interactions or epigenetic modifications. However, the data generated will always contain noise due to e.g. repetitive regions or non-specific antibody interactions. The noise will appear in the form of a background distribution of reads that must be taken into account in the downstream analysis, for example when detecting enriched regions (peak-calling). Several reported peak-callers can take experimental measurements of background tag distribution into account when analysing a data set. Unfortunately, the background is only used to adjust peak calling and not as a pre-processing step that aims at discerning the signal from the background noise. A normalization procedure that extracts the signal of interest would be of universal use when investigating genomic patterns. Results We formulated such a normalization method based on linear regression and made a proof-of-concept implementation in R and C++. It was tested on simulated as well as on publicly available ChIP-seq data on binding sites for two transcription factors, MAX and FOXA1 and two control samples, Input and IgG. We applied three different peak-callers to (i) raw (un-normalized) data using statistical background models and (ii) raw data with control samples as background and (iii) normalized data without additional control samples as background. The fraction of called regions containing the expected transcription factor binding motif was largest for the normalized data and evaluation with qPCR data for FOXA1 suggested higher sensitivity and specificity using normalized data over raw data with experimental background. Conclusions The proposed method can handle several control samples allowing for correction of multiple sources of bias simultaneously. Our evaluation on both synthetic and experimental data suggests that the method is successful in removing background noise.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2125362005",
    "type": "article"
  },
  {
    "title": "A simple data-adaptive probabilistic variant calling model",
    "doi": "https://doi.org/10.1186/s13015-015-0037-5",
    "publication_date": "2015-03-03",
    "publication_year": 2015,
    "authors": "Steve Hoffmann; Peter F. Stadler; Korbinian Strimmer",
    "corresponding_authors": "",
    "abstract": "Several sources of noise obfuscate the identification of single nucleotide variation (SNV) in next generation sequencing data. For instance, errors may be introduced during library construction and sequencing steps. In addition, the reference genome and the algorithms used for the alignment of the reads are further critical factors determining the efficacy of variant calling methods. It is crucial to account for these factors in individual sequencing experiments. We introduce a simple data-adaptive model for variant calling. This model automatically adjusts to specific factors such as alignment errors. To achieve this, several characteristics are sampled from sites with low mismatch rates, and these are used to estimate empirical log-likelihoods. The likelihoods are then combined to a score that typically gives rise to a mixture distribution. From this we determine a decision threshold to separate potentially variant sites from the noisy background. In simulations we show that our simple model is competitive with frequently used much more complex SNV calling algorithms in terms of sensitivity and specificity. It performs specifically well in cases with low allele frequencies. The application to next-generation sequencing data reveals stark differences of the score distributions indicating a strong influence of data specific sources of noise. The proposed model is specifically designed to adjust to these differences.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2126389203",
    "type": "article"
  },
  {
    "title": "Predicting horizontal gene transfers with perfect transfer networks",
    "doi": "https://doi.org/10.1186/s13015-023-00242-2",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Alitzel López Sánchez; Manuel Lafond",
    "corresponding_authors": "",
    "abstract": "Horizontal gene transfer inference approaches are usually based on gene sequences: parametric methods search for patterns that deviate from a particular genomic signature, while phylogenetic methods use sequences to reconstruct the gene and species trees. However, it is well-known that sequences have difficulty identifying ancient transfers since mutations have enough time to erase all evidence of such events. In this work, we ask whether character-based methods can predict gene transfers. Their advantage over sequences is that homologous genes can have low DNA similarity, but still have retained enough important common motifs that allow them to have common character traits, for instance the same functional or expression profile. A phylogeny that has two separate clades that acquired the same character independently might indicate the presence of a transfer even in the absence of sequence similarity.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391565229",
    "type": "article"
  },
  {
    "title": "Finding maximal exact matches in graphs",
    "doi": "https://doi.org/10.1186/s13015-024-00255-5",
    "publication_date": "2024-03-11",
    "publication_year": 2024,
    "authors": "Nicola Rizzo; Manuel Cáceres; Veli Mäkinen",
    "corresponding_authors": "Nicola Rizzo",
    "abstract": "Abstract Background We study the problem of finding maximal exact matches (MEMs) between a query string Q and a labeled graph G . MEMs are an important class of seeds, often used in seed-chain-extend type of practical alignment methods because of their strong connections to classical metrics. A principled way to speed up chaining is to limit the number of MEMs by considering only MEMs of length at least $$\\kappa$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>κ</mml:mi></mml:math> ( $$\\kappa$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>κ</mml:mi></mml:math> -MEMs). However, on arbitrary input graphs, the problem of finding MEMs cannot be solved in truly sub-quadratic time under SETH (Equi et al., TALG 2023) even on acyclic graphs. Results In this paper we show an $$O(n\\cdot L \\cdot d^{L-1} + m + M_{\\kappa ,L})$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>·</mml:mo><mml:mi>L</mml:mi><mml:mo>·</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>κ</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math> -time algorithm finding all $$\\kappa$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>κ</mml:mi></mml:math> -MEMs between Q and G spanning exactly L nodes in G , where n is the total length of node labels, d is the maximum degree of a node in G , $$m = |Q|$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mi>Q</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:math> , and $$M_{\\kappa ,L}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>κ</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math> is the number of output MEMs. We use this algorithm to develop a $$\\kappa$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>κ</mml:mi></mml:math> -MEM finding solution on indexable Elastic Founder Graphs (Equi et al., Algorithmica 2022) running in time $$O(nH^2 + m + M_\\kappa )$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>κ</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math> , where H is the maximum number of nodes in a block, and $$M_\\kappa$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>M</mml:mi><mml:mi>κ</mml:mi></mml:msub></mml:math> is the total number of $$\\kappa$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>κ</mml:mi></mml:math> -MEMs. Our results generalize to the analysis of multiple query strings (MEMs between G and any of the strings). Additionally, we provide some experimental results showing that the number of graph MEMs is an order of magnitude smaller than the number of string MEMs of the corresponding concatenated collection. Conclusions We show that seed-chain-extend type of alignment methods can be implemented on top of indexable Elastic Founder Graphs by providing an efficient way to produce the seeds between a set of queries and the graph. The code is available in https://github.com/algbio/efg-mems .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392644939",
    "type": "article"
  },
  {
    "title": "Compression algorithm for colored de Bruijn graphs",
    "doi": "https://doi.org/10.1186/s13015-024-00254-6",
    "publication_date": "2024-05-26",
    "publication_year": 2024,
    "authors": "Amatur Rahman; Yoann Dufresne; Paul Medvedev",
    "corresponding_authors": "",
    "abstract": "A colored de Bruijn graph (also called a set of k-mer sets), is a set of k-mers with every k-mer assigned a set of colors. Colored de Bruijn graphs are used in a variety of applications, including variant calling, genome assembly, and database search. However, their size has posed a scalability challenge to algorithm developers and users. There have been numerous indexing data structures proposed that allow to store the graph compactly while supporting fast query operations. However, disk compression algorithms, which do not need to support queries on the compressed data and can thus be more space-efficient, have received little attention. The dearth of specialized compression tools has been a detriment to tool developers, tool users, and reproducibility efforts. In this paper, we develop a new tool that compresses colored de Bruijn graphs to disk, building on previous ideas for compression of k-mer sets and indexing colored de Bruijn graphs. We test our tool, called ESS-color, on various datasets, including both sequencing data and whole genomes. ESS-color achieves better compression than all evaluated tools and all datasets, with no other tool able to consistently achieve less than 44% space overhead. The software is available at http://github.com/medvedevgroup/ESSColor .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399027736",
    "type": "article"
  },
  {
    "title": "Metric multidimensional scaling for large single-cell datasets using neural networks",
    "doi": "https://doi.org/10.1186/s13015-024-00265-3",
    "publication_date": "2024-06-11",
    "publication_year": 2024,
    "authors": "Stefan Canzar; Van Hoan; Slobodan Jelić; Sören Laue; Domagoj Matijević; Tomislav Prusina",
    "corresponding_authors": "Stefan Canzar",
    "abstract": "Abstract Metric multidimensional scaling is one of the classical methods for embedding data into low-dimensional Euclidean space. It creates the low-dimensional embedding by approximately preserving the pairwise distances between the input points. However, current state-of-the-art approaches only scale to a few thousand data points. For larger data sets such as those occurring in single-cell RNA sequencing experiments, the running time becomes prohibitively large and thus alternative methods such as PCA are widely used instead. Here, we propose a simple neural network-based approach for solving the metric multidimensional scaling problem that is orders of magnitude faster than previous state-of-the-art approaches, and hence scales to data sets with up to a few million cells. At the same time, it provides a non-linear mapping between high- and low-dimensional space that can place previously unseen cells in the same embedding.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399542346",
    "type": "article"
  },
  {
    "title": "P-value based visualization of codon usage data",
    "doi": "https://doi.org/10.1186/1748-7188-1-10",
    "publication_date": "2006-06-29",
    "publication_year": 2006,
    "authors": "Peter Meinicke; Thomas Brodag; W. Florian Fricke; Stephan Waack",
    "corresponding_authors": "Peter Meinicke",
    "abstract": "Two important and not yet solved problems in bacterial genome research are the identification of horizontally transferred genes and the prediction of gene expression levels. Both problems can be addressed by multivariate analysis of codon usage data. In particular dimensionality reduction methods for visualization of multivariate data have shown to be effective tools for codon usage analysis. We here propose a multidimensional scaling approach using a novel similarity measure for codon usage tables. Our probabilistic similarity measure is based on P-values derived from the well-known chi-square test for comparison of two distributions. Experimental results on four microbial genomes indicate that the new method is well-suited for the analysis of horizontal gene transfer and translational selection. As compared with the widely-used correspondence analysis, our method did not suffer from outlier sensitivity and showed a better clustering of putative alien genes in most cases.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2157075613",
    "type": "article"
  },
  {
    "title": "A stitch in time: Efficient computation of genomic DNA melting bubbles",
    "doi": "https://doi.org/10.1186/1748-7188-3-10",
    "publication_date": "2008-07-17",
    "publication_year": 2008,
    "authors": "Eivind Tøstesen",
    "corresponding_authors": "Eivind Tøstesen",
    "abstract": "It is of biological interest to make genome-wide predictions of the locations of DNA melting bubbles using statistical mechanics models. Computationally, this poses the challenge that a generic search through all combinations of bubble starts and ends is quadratic.An efficient algorithm is described, which shows that the time complexity of the task is O(NlogN) rather than quadratic. The algorithm exploits that bubble lengths may be limited, but without a prior assumption of a maximal bubble length. No approximations, such as windowing, have been introduced to reduce the time complexity. More than just finding the bubbles, the algorithm produces a stitch profile, which is a probabilistic graphical model of bubbles and helical regions. The algorithm applies a probability peak finding method based on a hierarchical analysis of the energy barriers in the Poland-Scheraga model.Exact and fast computation of genomic stitch profiles is thus feasible. Sequences of several megabases have been computed, only limited by computer memory. Possible applications are the genome-wide comparisons of bubbles with promotors, TSS, viral integration sites, and other melting-related regions.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3100320679",
    "type": "article"
  },
  {
    "title": "Exact transcript quantification over splice graphs",
    "doi": "https://doi.org/10.1186/s13015-021-00184-7",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Cong Ma; Hongyu Zheng; Carl Kingsford",
    "corresponding_authors": "",
    "abstract": "Abstract Background The probability of sequencing a set of RNA-seq reads can be directly modeled using the abundances of splice junctions in splice graphs instead of the abundances of a list of transcripts. We call this model graph quantification, which was first proposed by Bernard et al. (Bioinformatics 30:2447–55, 2014). The model can be viewed as a generalization of transcript expression quantification where every full path in the splice graph is a possible transcript. However, the previous graph quantification model assumes the length of single-end reads or paired-end fragments is fixed. Results We provide an improvement of this model to handle variable-length reads or fragments and incorporate bias correction. We prove that our model is equivalent to running a transcript quantifier with exactly the set of all compatible transcripts. The key to our method is constructing an extension of the splice graph based on Aho-Corasick automata. The proof of equivalence is based on a novel reparameterization of the read generation model of a state-of-art transcript quantification method. Conclusion We propose a new approach for graph quantification, which is useful for modeling scenarios where reference transcriptome is incomplete or not available and can be further used in transcriptome assembly or alternative splicing analysis.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3160325511",
    "type": "article"
  },
  {
    "title": "Using the longest run subsequence problem within homology-based scaffolding",
    "doi": "https://doi.org/10.1186/s13015-021-00191-8",
    "publication_date": "2021-06-28",
    "publication_year": 2021,
    "authors": "Sven Schrinner; Manish Goel; Michael Wulfert; Philipp Spohr; Korbinian Schneeberger; Gunnar W. Klau",
    "corresponding_authors": "",
    "abstract": "Abstract Genome assembly is one of the most important problems in computational genomics. Here, we suggest addressing an issue that arises in homology-based scaffolding, that is, when linking and ordering contigs to obtain larger pseudo-chromosomes by means of a second incomplete assembly of a related species. The idea is to use alignments of binned regions in one contig to find the most homologous contig in the other assembly. We show that ordering the contigs of the other assembly can be expressed by a new string problem, the longest run subsequence problem (LRS). We show that LRS is NP-hard and present reduction rules and two algorithmic approaches that, together, are able to solve large instances of LRS to provable optimality. All data used in the experiments as well as our source code are freely available. We demonstrate its usefulness within an existing larger scaffolding approach by solving realistic instances resulting from partial Arabidopsis thaliana assemblies in short computation time.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3177008556",
    "type": "article"
  },
  {
    "title": "Heuristic algorithms for best match graph editing",
    "doi": "https://doi.org/10.1186/s13015-021-00196-3",
    "publication_date": "2021-08-17",
    "publication_year": 2021,
    "authors": "David Schaller; Manuela Geiß; Marc Hellmuth; Peter F. Stadler",
    "corresponding_authors": "",
    "abstract": "Abstract Background Best match graphs (BMGs) are a class of colored digraphs that naturally appear in mathematical phylogenetics as a representation of the pairwise most closely related genes among multiple species. An arc connects a gene x with a gene y from another species (vertex color) Y whenever it is one of the phylogenetically closest relatives of x . BMGs can be approximated with the help of similarity measures between gene sequences, albeit not without errors. Empirical estimates thus will usually violate the theoretical properties of BMGs. The corresponding graph editing problem can be used to guide error correction for best match data. Since the arc set modification problems for BMGs are NP-complete, efficient heuristics are needed if BMGs are to be used for the practical analysis of biological sequence data. Results Since BMGs have a characterization in terms of consistency of a certain set of rooted triples (binary trees on three vertices) defined on the set of genes, we consider heuristics that operate on triple sets. As an alternative, we show that there is a close connection to a set partitioning problem that leads to a class of top-down recursive algorithms that are similar to Aho’s supertree algorithm and give rise to BMG editing algorithms that are consistent in the sense that they leave BMGs invariant. Extensive benchmarking shows that community detection algorithms for the partitioning steps perform best for BMG editing. Conclusion Noisy BMG data can be corrected with sufficient accuracy and efficiency to make BMGs an attractive alternative to classical phylogenetic methods.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3195461127",
    "type": "article"
  },
  {
    "title": "A simpler linear-time algorithm for the common refinement of rooted phylogenetic trees on a common leaf set",
    "doi": "https://doi.org/10.1186/s13015-021-00202-8",
    "publication_date": "2021-12-01",
    "publication_year": 2021,
    "authors": "David Schaller; Marc Hellmuth; Peter F. Stadler",
    "corresponding_authors": "Peter F. Stadler",
    "abstract": "Abstract Background The supertree problem, i.e., the task of finding a common refinement of a set of rooted trees is an important topic in mathematical phylogenetics. The special case of a common leaf set L is known to be solvable in linear time. Existing approaches refine one input tree using information of the others and then test whether the results are isomorphic. Results An O ( k | L |) algorithm, , for constructing the common refinement T of k input trees with a common leaf set L is proposed that explicitly computes the parent function of T in a bottom-up approach. Conclusion is simpler to implement than other asymptotically optimal algorithms for the problem and outperforms the alternatives in empirical comparisons. Availability An implementation of in Python is freely available at https://github.com/david-schaller/tralda .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200303939",
    "type": "article"
  },
  {
    "title": "The gene family-free median of three",
    "doi": "https://doi.org/10.1186/s13015-017-0106-z",
    "publication_date": "2017-05-26",
    "publication_year": 2017,
    "authors": "Daniel Doerr; Metin Balaban; Pedro Feijão; Cédric Chauve",
    "corresponding_authors": "",
    "abstract": "The gene family-free framework for comparative genomics aims at providing methods for gene order analysis that do not require prior gene family assignment, but work directly on a sequence similarity graph. We study two problems related to the breakpoint median of three genomes, which asks for the construction of a fourth genome that minimizes the sum of breakpoint distances to the input genomes.We present a model for constructing a median of three genomes in this family-free setting, based on maximizing an objective function that generalizes the classical breakpoint distance by integrating sequence similarity in the score of a gene adjacency. We study its computational complexity and we describe an integer linear program (ILP) for its exact solution. We further discuss a related problem called family-free adjacencies for k genomes for the special case of [Formula: see text] and present an ILP for its solution. However, for this problem, the computation of exact solutions remains intractable for sufficiently large instances. We then proceed to describe a heuristic method, FFAdj-AM, which performs well in practice.The developed methods compute accurate positional orthologs for genomes comparable in size of bacterial genomes on simulated data and genomic data acquired from the OMA orthology database. In particular, FFAdj-AM performs equally or better when compared to the well-established gene family prediction tool MultiMSOAR.We study the computational complexity of a new family-free model and present algorithms for its solution. With FFAdj-AM, we propose an appealing alternative to established tools for identifying higher confidence positional orthologs.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2617573272",
    "type": "article"
  },
  {
    "title": "A polynomial delay algorithm for the enumeration of bubbles with length constraints in directed graphs",
    "doi": "https://doi.org/10.1186/s13015-015-0046-4",
    "publication_date": "2015-06-26",
    "publication_year": 2015,
    "authors": "Gustavo Sacomoto; Vincent Lacroix; Marie‐France Sagot",
    "corresponding_authors": "",
    "abstract": "The problem of enumerating bubbles with length constraints in directed graphs arises in transcriptomics where the question is to identify all alternative splicing events present in a sample of mRNAs sequenced by RNA-seq.We present a new algorithm for enumerating bubbles with length constraints in weighted directed graphs. This is the first polynomial delay algorithm for this problem and we show that in practice, it is faster than previous approaches.This settles one of the main open questions from Sacomoto et al. (BMC Bioinform 13:5, 2012). Moreover, the new algorithm allows us to deal with larger instances and possibly detect longer alternative splicing events.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1815037699",
    "type": "article"
  },
  {
    "title": "Predicting gene ontology annotations of orphan GWAS genes using protein-protein interactions",
    "doi": "https://doi.org/10.1186/1748-7188-9-10",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Usha Kuppuswamy; Seshan Ananthasubramanian; Yanli Wang; N. Balakrishnan; Madhavi K. Ganapathiraju",
    "corresponding_authors": "",
    "abstract": "Abstract Background The number of genome-wide association studies (GWAS) has increased rapidly in the past couple of years, resulting in the identification of genes associated with different diseases. The next step in translating these findings into biomedically useful information is to find out the mechanism of the action of these genes. However, GWAS studies often implicate genes whose functions are currently unknown; for example, MYEOV, ANKLE1, TMEM45B and ORAOV1 are found to be associated with breast cancer, but their molecular function is unknown. Results We carried out Bayesian inference of Gene Ontology (GO) term annotations of genes by employing the directed acyclic graph structure of GO and the network of protein-protein interactions (PPIs). The approach is designed based on the fact that two proteins that interact biophysically would be in physical proximity of each other, would possess complementary molecular function, and play role in related biological processes. Predicted GO terms were ranked according to their relative association scores and the approach was evaluated quantitatively by plotting the precision versus recall values and F-scores (the harmonic mean of precision and recall) versus varying thresholds. Precisions of ~58% and ~ 40% for localization and functions respectively of proteins were determined at a threshold of ~30 (top 30 GO terms in the ranked list). Comparison with function prediction based on semantic similarity among nodes in an ontology and incorporation of those similarities in a k-nearest neighbor classifier confirmed that our results compared favorably. Conclusions This approach was applied to predict the cellular component and molecular function GO terms of all human proteins that have interacting partners possessing at least one known GO annotation. The list of predictions is available at http://severus.dbmi.pitt.edu/engo/GOPRED.html . We present the algorithm, evaluations and the results of the computational predictions, especially for genes identified in GWAS studies to be associated with diseases, which are of translational interest.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2100350647",
    "type": "article"
  },
  {
    "title": "HIA: a genome mapper using hybrid index-based sequence alignment",
    "doi": "https://doi.org/10.1186/s13015-015-0062-4",
    "publication_date": "2015-12-01",
    "publication_year": 2015,
    "authors": "Jongpill Choi; Kiejung Park; Seong Beom Cho; Myungguen Chung",
    "corresponding_authors": "Jongpill Choi",
    "abstract": "A number of alignment tools have been developed to align sequencing reads to the human reference genome. The scale of information from next-generation sequencing (NGS) experiments, however, is increasing rapidly. Recent studies based on NGS technology have routinely produced exome or whole-genome sequences from several hundreds or thousands of samples. To accommodate the increasing need of analyzing very large NGS data sets, it is necessary to develop faster, more sensitive and accurate mapping tools. HIA uses two indices, a hash table index and a suffix array index. The hash table performs direct lookup of a q-gram, and the suffix array performs very fast lookup of variable-length strings by exploiting binary search. We observed that combining hash table and suffix array (hybrid index) is much faster than the suffix array method for finding a substring in the reference sequence. Here, we defined the matching region (MR) is a longest common substring between a reference and a read. And, we also defined the candidate alignment regions (CARs) as a list of MRs that is close to each other. The hybrid index is used to find candidate alignment regions (CARs) between a reference and a read. We found that aligning only the unmatched regions in the CAR is much faster than aligning the whole CAR. In benchmark analysis, HIA outperformed in mapping speed compared with the other aligners, without significant loss of mapping accuracy. Our experiments show that the hybrid of hash table and suffix array is useful in terms of speed for mapping NGS sequencing reads to the human reference genome sequence. In conclusion, our tool is appropriate for aligning massive data sets generated by NGS sequencing.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2219104018",
    "type": "article"
  },
  {
    "title": "All galls are divided into three or more parts: recursive enumeration of labeled histories for galled trees",
    "doi": "https://doi.org/10.1186/s13015-023-00224-4",
    "publication_date": "2023-02-13",
    "publication_year": 2023,
    "authors": "Shaili Mathur; Noah A. Rosenberg",
    "corresponding_authors": "",
    "abstract": "In mathematical phylogenetics, a labeled rooted binary tree topology can possess any of a number of labeled histories, each of which represents a possible temporal ordering of its coalescences. Labeled histories appear frequently in calculations that describe the combinatorics of phylogenetic trees. Here, we generalize the concept of labeled histories from rooted phylogenetic trees to rooted phylogenetic networks, specifically for the class of rooted phylogenetic networks known as rooted galled trees.Extending a recursive algorithm for enumerating the labeled histories of a labeled tree topology, we present a method to enumerate the labeled histories associated with a labeled rooted galled tree. The method relies on a recursive decomposition by which each gall in a galled tree possesses three or more descendant subtrees. We exhaustively provide the numbers of labeled histories for all small galled trees, finding that each gall reduces the number of labeled histories relative to a specified galled tree that does not contain it.The results expand the set of structures for which labeled histories can be enumerated, extending a well-known calculation for phylogenetic trees to a class of phylogenetic networks.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4320480678",
    "type": "article"
  },
  {
    "title": "A classification algorithm based on dynamic ensemble selection to predict mutational patterns of the envelope protein in HIV-infected patients",
    "doi": "https://doi.org/10.1186/s13015-023-00228-0",
    "publication_date": "2023-06-19",
    "publication_year": 2023,
    "authors": "Mohammad Fili; Guiping Hu; Changze Han; Alexa Kort; John Trettin; Hillel Haim",
    "corresponding_authors": "",
    "abstract": "Abstract Background Therapeutics against the envelope (Env) proteins of human immunodeficiency virus type 1 (HIV-1) effectively reduce viral loads in patients. However, due to mutations, new therapy-resistant Env variants frequently emerge. The sites of mutations on Env that appear in each patient are considered random and unpredictable. Here we developed an algorithm to estimate for each patient the mutational state of each position based on the mutational state of adjacent positions on the three-dimensional structure of the protein. Methods We developed a dynamic ensemble selection algorithm designated k-best classifiers. It identifies the best classifiers within the neighborhood of a new observation and applies them to predict the variability state of each observation. To evaluate the algorithm, we applied amino acid sequences of Envs from 300 HIV-1-infected individuals (at least six sequences per patient). For each patient, amino acid variability values at all Env positions were mapped onto the three-dimensional structure of the protein. Then, the variability state of each position was estimated by the variability at adjacent positions of the protein. Results The proposed algorithm showed higher performance than the base learner and a panel of classification algorithms. The mutational state of positions in the high-mannose patch and CD4-binding site of Env, which are targeted by multiple therapeutics, was predicted well. Importantly, the algorithm outperformed other classification techniques for predicting the variability state at multi-position footprints of therapeutics on Env. Conclusions The proposed algorithm applies a dynamic classifier-scoring approach that increases its performance relative to other classification methods. Better understanding of the spatiotemporal patterns of variability across Env may lead to new treatment strategies that are tailored to the unique mutational patterns of each patient. More generally, we propose the algorithm as a new high-performance dynamic ensemble selection technique.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4381162633",
    "type": "article"
  },
  {
    "title": "Constructing founder sets under allelic and non-allelic homologous recombination",
    "doi": "https://doi.org/10.1186/s13015-023-00241-3",
    "publication_date": "2023-09-29",
    "publication_year": 2023,
    "authors": "Konstantinn Bonnet; Tobias Marschall; Daniel Doerr",
    "corresponding_authors": "",
    "abstract": "Homologous recombination between the maternal and paternal copies of a chromosome is a key mechanism for human inheritance and shapes population genetic properties of our species. However, a similar mechanism can also act between different copies of the same sequence, then called non-allelic homologous recombination (NAHR). This process can result in genomic rearrangements-including deletion, duplication, and inversion-and is underlying many genomic disorders. Despite its importance for genome evolution and disease, there is a lack of computational models to study genomic loci prone to NAHR. In this work, we propose such a computational model, providing a unified framework for both (allelic) homologous recombination and NAHR. Our model represents a set of genomes as a graph, where haplotypes correspond to walks through this graph. We formulate two founder set problems under our recombination model, provide flow-based algorithms for their solution, describe exact methods to characterize the number of recombinations, and demonstrate scalability to problem instances arising in practice.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387163927",
    "type": "article"
  },
  {
    "title": "EMMA: a new method for computing multiple sequence alignments given a constraint subset alignment",
    "doi": "https://doi.org/10.1186/s13015-023-00247-x",
    "publication_date": "2023-12-07",
    "publication_year": 2023,
    "authors": "Chengze Shen; Baqiao Liu; Kelly P. Williams; Tandy Warnow",
    "corresponding_authors": "",
    "abstract": "Adding sequences into an existing (possibly user-provided) alignment has multiple applications, including updating a large alignment with new data, adding sequences into a constraint alignment constructed using biological knowledge, or computing alignments in the presence of sequence length heterogeneity. Although this is a natural problem, only a few tools have been developed to use this information with high fidelity.We present EMMA (Extending Multiple alignments using MAFFT--add) for the problem of adding a set of unaligned sequences into a multiple sequence alignment (i.e., a constraint alignment). EMMA builds on MAFFT--add, which is also designed to add sequences into a given constraint alignment. EMMA improves on MAFFT--add methods by using a divide-and-conquer framework to scale its most accurate version, MAFFT-linsi--add, to constraint alignments with many sequences. We show that EMMA has an accuracy advantage over other techniques for adding sequences into alignments under many realistic conditions and can scale to large datasets with high accuracy (hundreds of thousands of sequences). EMMA is available at https://github.com/c5shen/EMMA .EMMA is a new tool that provides high accuracy and scalability for adding sequences into an existing alignment.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389430430",
    "type": "article"
  },
  {
    "title": "Characteristics of predictor sets found using differential prioritization",
    "doi": "https://doi.org/10.1186/1748-7188-2-7",
    "publication_date": "2007-06-04",
    "publication_year": 2007,
    "authors": "Chia-Huey Ooi; Madhu Chetty; Shyh Wei Teng",
    "corresponding_authors": "",
    "abstract": "Feature selection plays an undeniably important role in classification problems involving high dimensional datasets such as microarray datasets. For filter-based feature selection, two well-known criteria used in forming predictor sets are relevance and redundancy. However, there is a third criterion which is at least as important as the other two in affecting the efficacy of the resulting predictor sets. This criterion is the degree of differential prioritization (DDP), which varies the emphases on relevance and redundancy depending on the value of the DDP. Previous empirical works on publicly available microarray datasets have confirmed the effectiveness of the DDP in molecular classification. We now propose to establish the fundamental strengths and merits of the DDP-based feature selection technique. This is to be done through a simulation study which involves vigorous analyses of the characteristics of predictor sets found using different values of the DDP from toy datasets designed to mimic real-life microarray datasets.A simulation study employing analytical measures such as the distance between classes before and after transformation using principal component analysis is implemented on toy datasets. From these analyses, the necessity of adjusting the differential prioritization based on the dataset of interest is established. This conclusion is supported by comparisons against both simplistic rank-based selection and state-of-the-art equal-priorities scoring methods, which demonstrates the superiority of the DDP-based feature selection technique. Reapplying similar analyses to real-life multiclass microarray datasets provides further confirmation of our findings and of the significance of the DDP for practical applications.The findings have been achieved based on analytical evaluations, not empirical evaluation involving classifiers, thus providing further basis for the usefulness of the DDP and validating the need for unequal priorities on relevance and redundancy during feature selection for microarray datasets, especially highly multiclass datasets.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1995017245",
    "type": "article"
  },
  {
    "title": "An average-case sublinear forward algorithm for the haploid Li and Stephens model",
    "doi": "https://doi.org/10.1186/s13015-019-0144-9",
    "publication_date": "2019-04-02",
    "publication_year": 2019,
    "authors": "Yohei Rosen; Benedict Paten",
    "corresponding_authors": "Yohei Rosen",
    "abstract": "Hidden Markov models of haplotype inheritance such as the Li and Stephens model allow for computationally tractable probability calculations using the forward algorithm as long as the representative reference panel used in the model is sufficiently small. Specifically, the monoploid Li and Stephens model and its variants are linear in reference panel size unless heuristic approximations are used. However, sequencing projects numbering in the thousands to hundreds of thousands of individuals are underway, and others numbering in the millions are anticipated. To make the forward algorithm for the haploid Li and Stephens model computationally tractable for these datasets, we have created a numerically exact version of the algorithm with observed average case sublinear runtime with respect to reference panel size k when tested against the 1000 Genomes dataset. We show a forward algorithm which avoids any tradeoff between runtime and model complexity. Our algorithm makes use of two general strategies which might be applicable to improving the time complexity of other future sequence analysis algorithms: sparse dynamic programming matrices and lazy evaluation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2941113916",
    "type": "article"
  },
  {
    "title": "A branching process for homology distribution-based inference of polyploidy, speciation and loss",
    "doi": "https://doi.org/10.1186/s13015-019-0153-8",
    "publication_date": "2019-08-01",
    "publication_year": 2019,
    "authors": "Yue Zhang; Chunfang Zheng; David Sankoff",
    "corresponding_authors": "",
    "abstract": "The statistical distribution of the similarity or difference between pairs of paralogous genes, created by whole genome doubling, or between pairs of orthologous genes in two related species is an important source of information about genomic evolution, especially in plants.We derive the mixture of distributions of sequence similarity for duplicate gene pairs generated by repeated episodes of whole gene doubling. This involves integrating sequence divergence and gene pair loss through fractionation, using a branching process and a mutational model. We account not only for the timing of these events in terms of local modes, but also the amplitude and variance of the component distributions. This model is then extended to orthologous gene pairs.We apply the model and inference procedures to the evolution of the Solanaceae, focusing on the genomes of economically important crops. We assess how consistent or variable fractionation rates are from species to species and over time.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2964468345",
    "type": "article"
  },
  {
    "title": "Linear-time algorithms for phylogenetic tree completion under Robinson–Foulds distance",
    "doi": "https://doi.org/10.1186/s13015-020-00166-1",
    "publication_date": "2020-04-13",
    "publication_year": 2020,
    "authors": "Mukul S. Bansal",
    "corresponding_authors": "Mukul S. Bansal",
    "abstract": "We consider two fundamental computational problems that arise when comparing phylogenetic trees, rooted or unrooted, with non-identical leaf sets. The first problem arises when comparing two trees where the leaf set of one tree is a proper subset of the other. The second problem arises when the two trees to be compared have only partially overlapping leaf sets. The traditional approach to handling these problems is to first restrict the two trees to their common leaf set. An alternative approach that has shown promise is to first complete the trees by adding missing leaves, so that the resulting trees have identical leaf sets. This requires the computation of an optimal completion that minimizes the distance between the two resulting trees over all possible completions.We provide optimal linear-time algorithms for both completion problems under the widely-used Robinson-Foulds (RF) distance measure. Our algorithm for the first problem improves the time complexity of the current fastest algorithm from quadratic (in the size of the two trees) to linear. No algorithms have yet been proposed for the more general second problem where both trees have missing leaves. We advance the study of this general problem by proposing a useful restricted version of the general problem and providing optimal linear-time algorithms for the restricted version. Our experimental results on biological data sets suggest that completion-based RF distances can be very different compared to traditional RF distances.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3016377605",
    "type": "article"
  },
  {
    "title": "Fast computation of genome-metagenome interaction effects",
    "doi": "https://doi.org/10.1186/s13015-020-00173-2",
    "publication_date": "2020-07-01",
    "publication_year": 2020,
    "authors": "Florent Guinot; Marie Szafranski; Julien Chiquet; Anouk Zancarini; Christine Le Signor; Christophe Mougel; Christophe Ambroise",
    "corresponding_authors": "",
    "abstract": "Association studies have been widely used to search for associations between common genetic variants observations and a given phenotype. However, it is now generally accepted that genes and environment must be examined jointly when estimating phenotypic variance. In this work we consider two types of biological markers: genotypic markers, which characterize an observation in terms of inherited genetic information, and metagenomic marker which are related to the environment. Both types of markers are available in their millions and can be used to characterize any observation uniquely.Our focus is on detecting interactions between groups of genetic and metagenomic markers in order to gain a better understanding of the complex relationship between environment and genome in the expression of a given phenotype.We propose a novel approach for efficiently detecting interactions between complementary datasets in a high-dimensional setting with a reduced computational cost. The method, named SICOMORE, reduces the dimension of the search space by selecting a subset of supervariables in the two complementary datasets. These supervariables are given by a weighted group structure defined on sets of variables at different scales. A Lasso selection is then applied on each type of supervariable to obtain a subset of potential interactions that will be explored via linear model testing.We compare SICOMORE with other approaches in simulations, with varying sample sizes, noise, and numbers of true interactions. SICOMORE exhibits convincing results in terms of recall, as well as competitive performances with respect to running time. The method is also used to detect interaction between genomic markers in Medicago truncatula and metagenomic markers in its rhizosphere bacterial community.An R package is available [4], along with its documentation and associated scripts, allowing the reader to reproduce the results presented in the paper.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3039505655",
    "type": "article"
  },
  {
    "title": "An FPT haplotyping algorithm on pedigrees with a small number of sites",
    "doi": "https://doi.org/10.1186/1748-7188-6-8",
    "publication_date": "2011-04-19",
    "publication_year": 2011,
    "authors": "Duong D. Doan; Patricia Evans",
    "corresponding_authors": "",
    "abstract": "Genetic disease studies investigate relationships between changes in chromosomes and genetic diseases. Single haplotypes provide useful information for these studies but extracting single haplotypes directly by biochemical methods is expensive. A computational method to infer haplotypes from genotype data is therefore important. We investigate the problem of computing the minimum number of recombination events for general pedigrees with a small number of sites for all members.We show that this NP-hard problem can be parametrically reduced to the Bipartization by Edge Removal problem with additional parity constraints. We solve this problem with an exact algorithm that runs in time, where n is the number of members, m is the number of sites, and k is the number of recombination events.This algorithm infers haplotypes for a small number of sites, which can be useful for genetic disease studies to track down how changes in haplotypes such as recombinations relate to genetic disease.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2115843315",
    "type": "article"
  },
  {
    "title": "MCOIN: a novel heuristic for determining transcription factor binding site motif width",
    "doi": "https://doi.org/10.1186/1748-7188-8-16",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Alastair M. Kilpatrick; B. Q. Ward; Stuart Aitken",
    "corresponding_authors": "",
    "abstract": "In transcription factor binding site discovery, the true width of the motif to be discovered is generally not known a priori. The ability to compute the most likely width of a motif is therefore a highly desirable property for motif discovery algorithms. However, this is a challenging computational problem as a result of changing model dimensionality at changing motif widths. The complexity of the problem is increased as the discovered model at the true motif width need not be the most statistically significant in a set of candidate motif models. Further, the core motif discovery algorithm used cannot guarantee to return the best possible result at each candidate width. We present MCOIN, a novel heuristic for automatically determining transcription factor binding site motif width, based on motif containment and information content. Using realistic synthetic data and previously characterised prokaryotic data, we show that MCOIN outperforms the current most popular method (E-value of the resulting multiple alignment) as a predictor of motif width, based on mean absolute error. MCOIN is also shown to choose models which better match known sites at higher levels of motif conservation, based on ROC analysis. We demonstrate the performance of MCOIN as part of a deterministic motif discovery algorithm and conclude that MCOIN outperforms current methods for determining motif width.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2130272510",
    "type": "article"
  },
  {
    "title": "Fast half-sibling population reconstruction: theory and algorithms",
    "doi": "https://doi.org/10.1186/1748-7188-8-20",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Daniel L. Dexter; Daniel G. Brown",
    "corresponding_authors": "",
    "abstract": "Kinship inference is the task of identifying genealogically related individuals. Kinship information is important for determining mating structures, notably in endangered populations. Although many solutions exist for reconstructing full sibling relationships, few exist for half-siblings.We consider the problem of determining whether a proposed half-sibling population reconstruction is valid under Mendelian inheritance assumptions. We show that this problem is NP-complete and provide a 0/1 integer program that identifies the minimum number of individuals that must be removed from a population in order for the reconstruction to become valid. We also present SibJoin, a heuristic-based clustering approach based on Mendelian genetics, which is strikingly fast. The software is available at http://github.com/ddexter/SibJoin.git+.Our SibJoin algorithm is reasonably accurate and thousands of times faster than existing algorithms. The heuristic is used to infer a half-sibling structure for a population which was, until recently, too large to evaluate.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2130654549",
    "type": "article"
  },
  {
    "title": "Using graph models to find transcription factor modules: the hitting set problem and an exact algorithm",
    "doi": "https://doi.org/10.1186/1748-7188-8-2",
    "publication_date": "2013-01-16",
    "publication_year": 2013,
    "authors": "Songjian Lu; Xinghua Lu",
    "corresponding_authors": "",
    "abstract": ": Systematically perturbing a cellular system and monitoring the effects of the perturbations on gene expression provide a powerful approach to study signal transduction in gene expression systems. A critical step of revealing a signal transduction pathway regulating gene expression is to identify transcription factors transmitting signals in the system. In this paper, we address the task of identifying modules of cooperative transcription factors based on results derived from systems-biology experiments at two levels: First, a graph algorithm is developed to identify a minimum set of co-operative TFs that covers the differentially expressed genes under each systematic perturbation. Second, using a clique-finding approach, modules of TFs that tend to consistently cooperate together under various perturbations are further identified. Our results indicate that this approach is capable of identifying many known TF modules based on the individual experiment; thus we provide a novel graph-based method of identifying context-specific and highly reused TF-modules.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2141027956",
    "type": "article"
  },
  {
    "title": "Stochastic errors vs. modeling errors in distance based phylogenetic reconstructions",
    "doi": "https://doi.org/10.1186/1748-7188-7-22",
    "publication_date": "2012-08-31",
    "publication_year": 2012,
    "authors": "Daniel Doerr; Ilan Gronau; Shlomo Moran; Irad Yavneh",
    "corresponding_authors": "",
    "abstract": "Abstract Background Distance-based phylogenetic reconstruction methods use evolutionary distances between species in order to reconstruct the phylogenetic tree spanning them. There are many different methods for estimating distances from sequence data. These methods assume different substitution models and have different statistical properties. Since the true substitution model is typically unknown, it is important to consider the effect of model misspecification on the performance of a distance estimation method. Results This paper continues the line of research which attempts to adjust to each given set of input sequences a distance function which maximizes the expected topological accuracy of the reconstructed tree. We focus here on the effect of systematic error caused by assuming an inadequate model, but consider also the stochastic error caused by using short sequences. We introduce a theoretical framework for analyzing both sources of error based on the notion of deviation from additivity , which quantifies the contribution of model misspecification to the estimation error. We demonstrate this framework by studying the behavior of the Jukes-Cantor distance function when applied to data generated according to Kimura’s two-parameter model with a transition-transversion bias. We provide both a theoretical derivation for this case, and a detailed simulation study on quartet trees. Conclusions We demonstrate both analytically and experimentally that by deliberately assuming an oversimplified evolutionary model, it is possible to increase the topological accuracy of reconstruction. Our theoretical framework provides new insights into the mechanisms that enables statistically inconsistent reconstruction methods to outperform consistent methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2173319564",
    "type": "article"
  },
  {
    "title": "Extracting conflict-free information from multi-labeled trees",
    "doi": "https://doi.org/10.1186/1748-7188-8-18",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Akshay Deepak; David Fernández‐Baca; Michelle M. McMahon",
    "corresponding_authors": "",
    "abstract": "A multi-labeled tree, or MUL-tree, is a phylogenetic tree where two or more leaves share a label, e.g., a species name. A MUL-tree can imply multiple conflicting phylogenetic relationships for the same set of taxa, but can also contain conflict-free information that is of interest and yet is not obvious. We define the information content of a MUL-tree T as the set of all conflict-free quartet topologies implied by T, and define the maximal reduced form of T as the smallest tree that can be obtained from T by pruning leaves and contracting edges while retaining the same information content. We show that any two MUL-trees with the same information content exhibit the same reduced form. This introduces an equivalence relation among MUL-trees with potential applications to comparing MUL-trees. We present an efficient algorithm to reduce a MUL-tree to its maximally reduced form and evaluate its performance on empirical datasets in terms of both quality of the reduced tree and the degree of data reduction achieved. Our measure of conflict-free information content based on quartets is simple and topologically appealing. In the experiments, the maximally reduced form is often much smaller than the original tree, yet retains most of the taxa. The reduction algorithm is quadratic in the number of leaves and its complexity is unaffected by the multiplicity of leaf labels or the degree of the nodes.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2569585400",
    "type": "article"
  },
  {
    "title": "Shape decomposition algorithms for laser capture microdissection",
    "doi": "https://doi.org/10.1186/s13015-021-00193-6",
    "publication_date": "2021-07-08",
    "publication_year": 2021,
    "authors": "Leonie Selbach; Tobias Kowalski; Klaus Gerwert; Maike Buchin; Axel Mosig",
    "corresponding_authors": "",
    "abstract": "Abstract Background In the context of biomarker discovery and molecular characterization of diseases, laser capture microdissection is a highly effective approach to extract disease-specific regions from complex, heterogeneous tissue samples. For the extraction to be successful, these regions have to satisfy certain constraints in size and shape and thus have to be decomposed into feasible fragments. Results We model this problem of constrained shape decomposition as the computation of optimal feasible decompositions of simple polygons. We use a skeleton-based approach and present an algorithmic framework that allows the implementation of various feasibility criteria as well as optimization goals. Motivated by our application, we consider different constraints and examine the resulting fragmentations. We evaluate our algorithm on lung tissue samples in comparison to a heuristic decomposition approach. Our method achieved a success rate of over 95% in the microdissection and tissue yield was increased by 10–30%. Conclusion We present a novel approach for constrained shape decomposition by demonstrating its advantages for the application in the microdissection of tissue samples. In comparison to the previous decomposition approach, the proposed method considerably increases the amount of successfully dissected tissue.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3180429672",
    "type": "article"
  },
  {
    "title": "Linear model for fast background subtraction in oligonucleotide microarrays",
    "doi": "https://doi.org/10.1186/1748-7188-4-15",
    "publication_date": "2009-11-16",
    "publication_year": 2009,
    "authors": "Kyung Myriam Kroll; G. T. Barkema; Enrico Carlon",
    "corresponding_authors": "Enrico Carlon",
    "abstract": "One important preprocessing step in the analysis of microarray data is background subtraction. In high-density oligonucleotide arrays this is recognized as a crucial step for the global performance of the data analysis from raw intensities to expression values.We propose here an algorithm for background estimation based on a model in which the cost function is quadratic in a set of fitting parameters such that minimization can be performed through linear algebra. The model incorporates two effects: 1) Correlated intensities between neighboring features in the chip and 2) sequence-dependent affinities for non-specific hybridization fitted by an extended nearest-neighbor model.The algorithm has been tested on 360 GeneChips from publicly available data of recent expression experiments. The algorithm is fast and accurate. Strong correlations between the fitted values for different experiments as well as between the free-energy parameters and their counterparts in aqueous solution indicate that the model captures a significant part of the underlying physical chemistry.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2153729031",
    "type": "article"
  },
  {
    "title": "Survival associated pathway identification with group L p penalized global AUC maximization",
    "doi": "https://doi.org/10.1186/1748-7188-5-30",
    "publication_date": "2010-08-16",
    "publication_year": 2010,
    "authors": "Zhenqiu Liu; Laurence S. Magder; Terry Hyslop; Li Mao",
    "corresponding_authors": "Zhenqiu Liu",
    "abstract": "It has been demonstrated that genes in a cell do not act independently. They interact with one another to complete certain biological processes or to implement certain molecular functions. How to incorporate biological pathways or functional groups into the model and identify survival associated gene pathways is still a challenging problem. In this paper, we propose a novel iterative gradient based method for survival analysis with group Lp penalized global AUC summary maximization. Unlike LASSO, Lp (p < 1) (with its special implementation entitled adaptive LASSO) is asymptotic unbiased and has oracle properties 1. We first extend Lp for individual gene identification to group Lp penalty for pathway selection, and then develop a novel iterative gradient algorithm for penalized global AUC summary maximization (IGGAUCS). This method incorporates the genetic pathways into global AUC summary maximization and identifies survival associated pathways instead of individual genes. The tuning parameters are determined using 10-fold cross validation with training data only. The prediction performance is evaluated using test data. We apply the proposed method to survival outcome analysis with gene expression profile and identify multiple pathways simultaneously. Experimental results with simulation and gene expression data demonstrate that the proposed procedures can be used for identifying important biological pathways that are related to survival phenotype and for building a parsimonious model for predicting the survival times.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2429955629",
    "type": "article"
  },
  {
    "title": "A better scoring model for de novo peptide sequencing: the symmetric difference between explained and measured masses",
    "doi": "https://doi.org/10.1186/s13015-017-0104-1",
    "publication_date": "2017-05-11",
    "publication_year": 2017,
    "authors": "Thomas Tschager; Simon Rösch; Ludovic Gillet; Peter Widmayer",
    "corresponding_authors": "Thomas Tschager",
    "abstract": "Given a peptide as a string of amino acids, the masses of all its prefixes and suffixes can be found by a trivial linear scan through the amino acid masses. The inverse problem is the ideal de novo peptide sequencing problem: Given all prefix and suffix masses, determine the string of amino acids. In biological reality, the given masses are measured in a lab experiment, and measurements by necessity are noisy. The (real, noisy) de novo peptide sequencing problem therefore has a noisy input: a few of the prefix and suffix masses of the peptide are missing and a few other masses are given in addition. For this setting, we ask for an amino acid string that explains the given masses as accurately as possible. Past approaches interpreted accuracy by searching for a string that explains as many masses as possible. We feel, however, that it is not only bad to not explain a mass that appears, but also to explain a mass that does not appear. We propose to minimize the symmetric difference between the set of given masses and the set of masses that the string explains. For this new optimization problem, we propose an efficient algorithm that computes both the best and the k best solutions. Proof-of-concept experiments on measurements of synthesized peptides show that our approach leads to better results compared to finding a string that explains as many given masses as possible. We conclude that considering the symmetric difference as optimization goal can improve the identification rates for de novo peptide sequencing. A preliminary version of this work has been presented at WABI 2016.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2613852274",
    "type": "article"
  },
  {
    "title": "Two metrics on rooted unordered trees with labels",
    "doi": "https://doi.org/10.1186/s13015-022-00220-0",
    "publication_date": "2022-06-06",
    "publication_year": 2022,
    "authors": "Yue Wang",
    "corresponding_authors": "Yue Wang",
    "abstract": "The early development of a zygote can be mathematically described by a developmental tree. To compare developmental trees of different species, we need to define distances on trees. If children cells after a division are not distinguishable, developmental trees are represented by the space $\\mathcal{T}$ of rooted trees with possibly repeated labels, where all vertices are unordered. If children cells after a division are partially distinguishable, developmental trees are represented by the space $\\mathcal{P}$ of rooted trees with possibly repeated labels, where vertices can be ordered or unordered. On $\\mathcal{T}$, the space of rooted unordered trees with possibly repeated labels, we define two metrics: the best-match metric and the left-regular metric, which show some advantages over existing methods. On $\\mathcal{P}$, the space of rooted labeled trees with ordered or unordered vertices, there is no metric, and we define a semimetric, which is a variant of the best-match metric. To compute the best-match distance between two trees, the expected time complexity and worst-case time complexity are both $\\mathcal{O}(n^2)$, where $n$ is the tree size. To compute the left-regular distance between two trees, the expected time complexity is $\\mathcal{O}(n)$, and the worst-case time complexity is $\\mathcal{O}(n\\log n)$. For rooted labeled trees with (fully/partially) unordered vertices, we define metrics (semimetric) that have fast algorithms to compute and have advantages over existing methods. Such trees also appear outside of developmental biology, and such metrics can be applied to other types of trees which have more extensive applications, especially in molecular biology.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3138342027",
    "type": "article"
  },
  {
    "title": "Bi-alignments with affine gaps costs",
    "doi": "https://doi.org/10.1186/s13015-022-00219-7",
    "publication_date": "2022-05-16",
    "publication_year": 2022,
    "authors": "Peter F. Stadler; Sebastian Will",
    "corresponding_authors": "Peter F. Stadler",
    "abstract": "Abstract Background Commonly, sequence and structure elements are assumed to evolve congruently, such that homologous sequence positions correspond to homologous structural features. Assuming congruent evolution, alignments based on sequence and structure similarity can therefore optimize both similarities at the same time in a single alignment. To model incongruent evolution, where sequence and structural features diverge positionally, we recently introduced bi-alignments . This generalization of sequence and structure-based alignments is best understood as alignments of two distinct pairwise alignments of the same entities: one modeling sequence similarity, the other structural similarity. Results Optimal bi-alignments with affine gap costs (or affine shift cost) for two constituent alignments can be computed exactly in quartic space and time. Even bi-alignments with affine shift and gap cost, as well as bi-alignment with sub-additive gap cost are optimized efficiently. Affine gap-cost bi-alignment of large proteins ( $$\\sim 930$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mo>∼</mml:mo> <mml:mn>930</mml:mn> </mml:mrow> </mml:math> aa) can be computed. Conclusion Affine cost bi-alignments are of practical interest to study shifts of protein sequences and protein structures relative to each other. Availability The affine cost bi-alignment algorithm has been implemented in Python 3 and Cython. It is available as free software from https://github.com/s-will/BiAlign/releases/tag/v0.3 and as bioconda package .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4280581584",
    "type": "article"
  },
  {
    "title": "Derivative-free neural network for optimizing the scoring functions associated with dynamic programming of pairwise-profile alignment",
    "doi": "https://doi.org/10.1186/s13015-018-0123-6",
    "publication_date": "2018-02-15",
    "publication_year": 2018,
    "authors": "Kazunori Yamada",
    "corresponding_authors": "Kazunori Yamada",
    "abstract": "A profile-comparison method with position-specific scoring matrix (PSSM) is among the most accurate alignment methods. Currently, cosine similarity and correlation coefficients are used as scoring functions of dynamic programming to calculate similarity between PSSMs. However, it is unclear whether these functions are optimal for profile alignment methods. By definition, these functions cannot capture nonlinear relationships between profiles. Therefore, we attempted to discover a novel scoring function, which was more suitable for the profile-comparison method than existing functions, using neural networks.Although neural networks required derivative-of-cost functions, the problem being addressed in this study lacked them. Therefore, we implemented a novel derivative-free neural network by combining a conventional neural network with an evolutionary strategy optimization method used as a solver. Using this novel neural network system, we optimized the scoring function to align remote sequence pairs. Our results showed that the pairwise-profile aligner using the novel scoring function significantly improved both alignment sensitivity and precision relative to aligners using existing functions.We developed and implemented a novel derivative-free neural network and aligner (Nepal) for optimizing sequence alignments. Nepal improved alignment quality by adapting to remote sequence alignments and increasing the expressiveness of similarity scores. Additionally, this novel scoring function can be realized using a simple matrix operation and easily incorporated into other aligners. Moreover our scoring function could potentially improve the performance of homology detection and/or multiple-sequence alignment of remote homologous sequences. The goal of the study was to provide a novel scoring function for profile alignment method and develop a novel learning system capable of addressing derivative-free problems. Our system is capable of optimizing the performance of other sophisticated methods and solving problems without derivative-of-cost functions, which do not always exist in practical problems. Our results demonstrated the usefulness of this optimization method for derivative-free problems.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2789974803",
    "type": "article"
  },
  {
    "title": "Precise parallel volumetric comparison of molecular surfaces and electrostatic isopotentials",
    "doi": "https://doi.org/10.1186/s13015-020-00168-z",
    "publication_date": "2020-05-25",
    "publication_year": 2020,
    "authors": "Georgi Georgiev; Kevin F. Dodd; Brian Y. Chen",
    "corresponding_authors": "",
    "abstract": "Abstract Geometric comparisons of binding sites and their electrostatic properties can identify subtle variations that select different binding partners and subtle similarities that accommodate similar partners. Because subtle features are central for explaining how proteins achieve specificity, algorithmic efficiency and geometric precision are central to algorithmic design. To address these concerns, this paper presents pClay, the first algorithm to perform parallel and arbitrarily precise comparisons of molecular surfaces and electrostatic isopotentials as geometric solids. pClay was presented at the 2019 Workshop on Algorithms in Bioinformatics (WABI 2019) and is described in expanded detail here, especially with regard to the comparison of electrostatic isopotentials. Earlier methods have generally used parallelism to enhance computational throughput, pClay is the first algorithm to use parallelism to make arbitrarily high precision comparisons practical. It is also the first method to demonstrate that high precision comparisons of geometric solids can yield more precise structural inferences than algorithms that use existing standards of precision. One advantage of added precision is that statistical models can be trained with more accurate data. Using structural data from an existing method, a model of steric variations between binding cavities can overlook 53% of authentic steric influences on specificity, whereas a model trained with data from pClay overlooks none. Our results also demonstrate the parallel performance of pClay on both workstation CPUs and a 61-core Xeon Phi. While slower on one core, additional processor cores rapidly outpaced single core performance and existing methods. Based on these results, it is clear that pClay has applications in the automatic explanation of binding mechanisms and in the rational design of protein binding preferences.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3030782321",
    "type": "article"
  },
  {
    "title": "Detecting transcriptomic structural variants in heterogeneous contexts via the Multiple Compatible Arrangements Problem",
    "doi": "https://doi.org/10.1186/s13015-020-00170-5",
    "publication_date": "2020-05-15",
    "publication_year": 2020,
    "authors": "Yutong Qiu; Cong Ma; Han Xie; Carl Kingsford",
    "corresponding_authors": "",
    "abstract": "Abstract Background Transcriptomic structural variants (TSVs)—large-scale transcriptome sequence change due to structural variation - are common in cancer. TSV detection from high-throughput sequencing data is a computationally challenging problem. Among all the confounding factors, sample heterogeneity, where each sample contains multiple distinct alleles, poses a critical obstacle to accurate TSV prediction. Results To improve TSV detection in heterogeneous RNA-seq samples, we introduce the Multiple Compatible Arrangements Problem (MCAP), which seeks k genome arrangements that maximize the number of reads that are concordant with at least one arrangement. This models a heterogeneous or diploid sample. We prove that MCAP is NP-complete and provide a $$\\frac{1}{4}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:math> -approximation algorithm for $$k=1$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math> and a $$\\frac{3}{4}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mfrac><mml:mn>3</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:math> -approximation algorithm for the diploid case ( $$k=2$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math> ) assuming an oracle for $$k=1$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math> . Combining these, we obtain a $$\\frac{3}{16}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mfrac><mml:mn>3</mml:mn><mml:mn>16</mml:mn></mml:mfrac></mml:math> -approximation algorithm for MCAP when $$k=2$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math> (without an oracle). We also present an integer linear programming formulation for general k . We characterize the conflict structures in the graph that require $$k&gt;1$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math> alleles to satisfy read concordancy and show that such structures are prevalent. Conclusions We show that the solution to MCAP accurately addresses sample heterogeneity during TSV detection. Our algorithms have improved performance on TCGA cancer samples and cancer cell line samples compared to a TSV calling tool, SQUID. The software is available at https://github.com/Kingsford-Group/diploidsquid .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3030981410",
    "type": "article"
  },
  {
    "title": "Fast and accurate structure probability estimation for simultaneous alignment and folding of RNAs with Markov chains",
    "doi": "https://doi.org/10.1186/s13015-020-00179-w",
    "publication_date": "2020-11-13",
    "publication_year": 2020,
    "authors": "Milad Miladi; Martin Raden; Sebastian Will; Rolf Backofen",
    "corresponding_authors": "",
    "abstract": "Simultaneous alignment and folding (SA&F) of RNAs is the indispensable gold standard for inferring the structure of non-coding RNAs and their general analysis. The original algorithm, proposed by Sankoff, solves the theoretical problem exactly with a complexity of [Formula: see text] in the full energy model. Over the last two decades, several variants and improvements of the Sankoff algorithm have been proposed to reduce its extreme complexity by proposing simplified energy models or imposing restrictions on the predicted alignments.Here, we introduce a novel variant of Sankoff's algorithm that reconciles the simplifications of PMcomp, namely moving from the full energy model to a simpler base pair-based model, with the accuracy of the loop-based full energy model. Instead of estimating pseudo-energies from unconditional base pair probabilities, our model calculates energies from conditional base pair probabilities that allow to accurately capture structure probabilities, which obey a conditional dependency. This model gives rise to the fast and highly accurate novel algorithm Pankov (Probabilistic Sankoff-like simultaneous alignment and folding of RNAs inspired by Markov chains).Pankov benefits from the speed-up of excluding unreliable base-pairing without compromising the loop-based free energy model of the Sankoff's algorithm. We show that Pankov outperforms its predecessors LocARNA and SPARSE in folding quality and is faster than LocARNA.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3100972074",
    "type": "article"
  },
  {
    "title": "Sparse estimation for structural variability",
    "doi": "https://doi.org/10.1186/1748-7188-6-12",
    "publication_date": "2011-04-19",
    "publication_year": 2011,
    "authors": "Raghavendra Hosur; Rohit Singh; Bonnie Berger",
    "corresponding_authors": "",
    "abstract": "Proteins are dynamic molecules that exhibit a wide range of motions; often these conformational changes are important for protein function. Determining biologically relevant conformational changes, or true variability, efficiently is challenging due to the noise present in structure data.In this paper we present a novel approach to elucidate conformational variability in structures solved using X-ray crystallography. We first infer an ensemble to represent the experimental data and then formulate the identification of truly variable members of the ensemble (as opposed to those that vary only due to noise) as a sparse estimation problem. Our results indicate that the algorithm is able to accurately distinguish genuine conformational changes from variability due to noise. We validate our predictions for structures in the Protein Data Bank by comparing with NMR experiments, as well as on synthetic data. In addition to improved performance over existing methods, the algorithm is robust to the levels of noise present in real data. In the case of Human Ubiquitin-conjugating enzyme Ubc9, variability identified by the algorithm corresponds to functionally important residues implicated by mutagenesis experiments. Our algorithm is also general enough to be integrated into state-of-the-art software tools for structure-inference.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1998513318",
    "type": "article"
  },
  {
    "title": "Enumerating all maximal frequent subtrees in collections of phylogenetic trees",
    "doi": "https://doi.org/10.1186/1748-7188-9-16",
    "publication_date": "2014-06-18",
    "publication_year": 2014,
    "authors": "Akshay Deepak; David Fernández‐Baca",
    "corresponding_authors": "",
    "abstract": "A common problem in phylogenetic analysis is to identify frequent patterns in a collection of phylogenetic trees. The goal is, roughly, to find a subset of the species (taxa) on which all or some significant subset of the trees agree. One popular method to do so is through maximum agreement subtrees (MASTs). MASTs are also used, among other things, as a metric for comparing phylogenetic trees, computing congruence indices and to identify horizontal gene transfer events.We give algorithms and experimental results for two approaches to identify common patterns in a collection of phylogenetic trees, one based on agreement subtrees, called maximal agreement subtrees, the other on frequent subtrees, called maximal frequent subtrees. These approaches can return subtrees on larger sets of taxa than MASTs, and can reveal new common phylogenetic relationships not present in either MASTs or the majority rule tree (a popular consensus method). Our current implementation is available on the web at https://code.google.com/p/mfst-miner/.Our computational results confirm that maximal agreement subtrees and all maximal frequent subtrees can reveal a more complete phylogenetic picture of the common patterns in collections of phylogenetic trees than maximum agreement subtrees; they are also often more resolved than the majority rule tree. Further, our experiments show that enumerating maximal frequent subtrees is considerably more practical than enumerating ordinary (not necessarily maximal) frequent subtrees.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2004446301",
    "type": "article"
  },
  {
    "title": "Listing all sorting reversals in quadratic time",
    "doi": "https://doi.org/10.1186/1748-7188-6-11",
    "publication_date": "2011-04-19",
    "publication_year": 2011,
    "authors": "Krister M. Swenson; Ghada Badr; David Sankoff",
    "corresponding_authors": "",
    "abstract": "We describe an average-case O(n2) algorithm to list all reversals on a signed permutation π that, when applied to π, produce a permutation that is closer to the identity. This algorithm is optimal in the sense that, the time it takes to write the list is Ω(n2) in the worst case.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2039803514",
    "type": "article"
  },
  {
    "title": "Characterizing compatibility and agreement of unrooted trees via cuts in graphs",
    "doi": "https://doi.org/10.1186/1748-7188-9-13",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Sudheer Vakati; David Fernández‐Baca",
    "corresponding_authors": "",
    "abstract": "Deciding whether there is a single tree -a supertree- that summarizes the evolutionary information in a collection of unrooted trees is a fundamental problem in phylogenetics. We consider two versions of this question: agreement and compatibility. In the first, the supertree is required to reflect precisely the relationships among the species exhibited by the input trees. In the second, the supertree can be more refined than the input trees. Testing for compatibility is an NP-complete problem; however, the problem is solvable in polynomial time when the number of input trees is fixed. Testing for agreement is also NP-complete, but it is not known whether it is fixed-parameter tractable. Compatibility can be characterized in terms of the existence of a specific kind of triangulation in a structure known as the display graph. Alternatively, it can be characterized as a chordal graph sandwich problem in a structure known as the edge label intersection graph. No characterization of agreement was known.We present a simple and natural characterization of compatibility in terms of minimal cuts in the display graph, which is closely related to compatibility of splits. We then derive a characterization for agreement.Explicit characterizations of tree compatibility and agreement are essential to finding practical algorithms for these problems. The simplicity of the characterizations presented here could help to achieve this goal.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2738316389",
    "type": "article"
  },
  {
    "title": "A constraint solving approach to model reduction by tropical equilibration",
    "doi": "https://doi.org/10.1186/preaccept-1468617733126024",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Sylvain Soliman; François Fages; Ovidiu Radulescu",
    "corresponding_authors": "",
    "abstract": "Model reduction is a central topic in systems biology and dynamical systems theory, for reducing the complexity of detailed models, finding important parameters, and developing multi-scale models for instance. While singular perturbation theory is a standard mathematical tool to analyze the different time scales of a dynamical system and decompose the system accordingly, tropical methods provide a simple algebraic framework to perform these analyses systematically in polynomial systems. The crux of these methods is in the computation of tropical equilibrations. In this paper we show that constraint-based methods, using reified constraints for expressing the equilibration conditions, make it possible to numerically solve non-linear tropical equilibration problems, out of reach of standard computation methods. We illustrate this approach first with the detailed reduction of a simple biochemical mechanism, the Michaelis-Menten enzymatic reaction model, and second, with large-scale performance figures obtained on the http://biomodels.net repository.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4240714589",
    "type": "article"
  },
  {
    "title": "Association of repeatedly measured intermediate risk factors for complex diseases with high dimensional SNP data",
    "doi": "https://doi.org/10.1186/1748-7188-5-17",
    "publication_date": "2010-02-11",
    "publication_year": 2010,
    "authors": "Sandra Waaijenborg; Aeilko H. Zwinderman",
    "corresponding_authors": "",
    "abstract": "The causes of complex diseases are difficult to grasp since many different factors play a role in their onset. To find a common genetic background, many of the existing studies divide their population into controls and cases; a classification that is likely to cause heterogeneity within the two groups. Rather than dividing the study population into cases and controls, it is better to identify the phenotype of a complex disease by a set of intermediate risk factors. But these risk factors often vary over time and are therefore repeatedly measured. We introduce a method to associate multiple repeatedly measured intermediate risk factors with a high dimensional set of single nucleotide polymorphisms (SNPs). Via a two-step approach, we summarized the time courses of each individual and, secondly apply these to penalized nonlinear canonical correlation analysis to obtain sparse results. Application of this method to two datasets which study the genetic background of cardiovascular diseases, show that compared to progression over time, mainly the constant levels in time are associated with sets of SNPs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1975814054",
    "type": "article"
  },
  {
    "title": "Challenges in experimental data integration within genome-scale metabolic models",
    "doi": "https://doi.org/10.1186/1748-7188-5-20",
    "publication_date": "2010-04-22",
    "publication_year": 2010,
    "authors": "Pierre-Yves Bourguignon; Areejit Samal; François Képès; Jürgen Jost; Olivier Martin",
    "corresponding_authors": "",
    "abstract": "Abstract A report of the meeting \"Challenges in experimental data integration within genome-scale metabolic models\", Institut Henri Poincaré, Paris, October 10-11 2009, organized by the CNRS-MPG joint program in Systems Biology.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2098528368",
    "type": "article"
  },
  {
    "title": "Distinguishing between hot-spots and melting-pots of genetic diversity using haplotype connectivity",
    "doi": "https://doi.org/10.1186/1748-7188-5-19",
    "publication_date": "2010-03-20",
    "publication_year": 2010,
    "authors": "Thanh Binh Nguyen; Andreas Spillner; Brent C. Emerson; Vincent Moulton",
    "corresponding_authors": "",
    "abstract": "Abstract We introduce a method to help identify how the genetic diversity of a species within a geographic region might have arisen. This problem appears, for example, in the context of identifying refugia in phylogeography, and in the conservation of biodiversity where it is a factor in nature reserve selection. Complementing current methods for measuring genetic diversity, we analyze pairwise distances between the haplotypes of a species found in a geographic region and derive a quantity, called haplotype connectivity, that aims to capture how divergent the haplotypes are relative to one another. We propose using haplotype connectivity to indicate whether, for geographic regions that harbor a highly diverse collection of haplotypes, diversity evolved inside a region over a long period of time (a \"hot-spot\") or is the result of a more recent mixture (a \"melting-pot\"). We describe how the haplotype connectivity for a collection of haplotypes can be computed efficiently and briefly discuss some related optimization problems that arise in this context. We illustrate the applicability of our method using two previously published data sets of a species of beetle from the genus Brachyderes and a species of tree from the genus Pinus .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2107441413",
    "type": "article"
  },
  {
    "title": "The approximability of the String Barcoding problem",
    "doi": "https://doi.org/10.1186/1748-7188-1-12",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Giuseppe Lancia; Roméo Rizzi",
    "corresponding_authors": "",
    "abstract": "The String Barcoding (SBC) problem, introduced by Rash and Gusfield (RECOMB, 2002), consists in finding a minimum set of substrings that can be used to distinguish between all members of a set of given strings. In a computational biology context, the given strings represent a set of known viruses, while the substrings can be used as probes for an hybridization experiment via microarray. Eventually, one aims at the classification of new strings (unknown viruses) through the result of the hybridization experiment. In this paper we show that SBC is as hard to approximate as Set Cover. Furthermore, we show that the constrained version of SBC (with probes of bounded length) is also hard to approximate. These negative results are tight.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1593495041",
    "type": "article"
  },
  {
    "title": "Pattern statistics on Markov chains and sensitivity to parameter estimation",
    "doi": "https://doi.org/10.1186/1748-7188-1-17",
    "publication_date": "2006-10-17",
    "publication_year": 2006,
    "authors": "Grégory Nuel",
    "corresponding_authors": "Grégory Nuel",
    "abstract": "In order to compute pattern statistics in computational biology a Markov model is commonly used to take into account the sequence composition. Usually its parameter must be estimated. The aim of this paper is to determine how sensitive these statistics are to parameter estimation, and what are the consequences of this variability on pattern studies (finding the most over-represented words in a genome, the most significant common words to a set of sequences,...). In the particular case where pattern statistics (overlap counting only) computed through binomial approximations we use the delta-method to give an explicit expression of σ, the standard deviation of a pattern statistic. This result is validated using simulations and a simple pattern study is also considered. We establish that the use of high order Markov model could easily lead to major mistakes due to the high sensitivity of pattern statistics to parameter estimation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2161044972",
    "type": "article"
  },
  {
    "title": "Locus-aware decomposition of gene trees with respect to polytomous species trees",
    "doi": "https://doi.org/10.1186/s13015-018-0128-1",
    "publication_date": "2018-06-04",
    "publication_year": 2018,
    "authors": "Michał Aleksander Ciach; Anna Muszewska; Paweł Górecki",
    "corresponding_authors": "Michał Aleksander Ciach",
    "abstract": "Horizontal gene transfer (HGT), a process of acquisition and fixation of foreign genetic material, is an important biological phenomenon. Several approaches to HGT inference have been proposed. However, most of them either rely on approximate, non-phylogenetic methods or on the tree reconciliation, which is computationally intensive and sensitive to parameter values.We investigate the locus tree inference problem as a possible alternative that combines the advantages of both approaches. We present several algorithms to solve the problem in the parsimony framework. We introduce a novel tree mapping, which allows us to obtain a heuristic solution to the problems of locus tree inference and duplication classification.Our approach allows for faster comparisons of gene and species trees and improves known algorithms for duplication inference in the presence of polytomies in the species trees. We have implemented our algorithms in a software tool available at https://github.com/mciach/LocusTreeInference.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2806175468",
    "type": "article"
  },
  {
    "title": "Aligning coding sequences with frameshift extension penalties",
    "doi": "https://doi.org/10.1186/s13015-017-0101-4",
    "publication_date": "2017-03-31",
    "publication_year": 2017,
    "authors": "Safa Jammali; Esaie Kuitche; Ayoub Rachati; François Bélanger; Michelle S. Scott; Aïda Ouangraoua",
    "corresponding_authors": "Safa Jammali",
    "abstract": "Frameshift translation is an important phenomenon that contributes to the appearance of novel coding DNA sequences (CDS) and functions in gene evolution, by allowing alternative amino acid translations of gene coding regions. Frameshift translations can be identified by aligning two CDS, from a same gene or from homologous genes, while accounting for their codon structure. Two main classes of algorithms have been proposed to solve the problem of aligning CDS, either by amino acid sequence alignment back-translation, or by simultaneously accounting for the nucleotide and amino acid levels. The former does not allow to account for frameshift translations and up to now, the latter exclusively accounts for frameshift translation initiation, not considering the length of the translation disruption caused by a frameshift. We introduce a new scoring scheme with an algorithm for the pairwise alignment of CDS accounting for frameshift translation initiation and length, while simultaneously considering nucleotide and amino acid sequences. The main specificity of the scoring scheme is the introduction of a penalty cost accounting for frameshift extension length to compute an adequate similarity score for a CDS alignment. The second specificity of the model is that the search space of the problem solved is the set of all feasible alignments between two CDS. Previous approaches have considered restricted search space or additional constraints on the decomposition of an alignment into length-3 sub-alignments. The algorithm described in this paper has the same asymptotic time complexity as the classical Needleman–Wunsch algorithm. We compare the method to other CDS alignment methods based on an application to the comparison of pairs of CDS from homologous human, mouse and cow genes of ten mammalian gene families from the Ensembl-Compara database. The results show that our method is particularly robust to parameter changes as compared to existing methods. It also appears to be a good compromise, performing well both in the presence and absence of frameshift translations. An implementation of the method is available at https://github.com/UdeS-CoBIUS/FsePSA .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3104515326",
    "type": "article"
  },
  {
    "title": "Mapping sequences by parts",
    "doi": "https://doi.org/10.1186/1748-7188-2-11",
    "publication_date": "2007-09-19",
    "publication_year": 2007,
    "authors": "Gilles Didier; Carito Guziołowski",
    "corresponding_authors": "",
    "abstract": "We present the N-map method, a pairwise and asymmetrical approach which allows us to compare sequences by taking into account evolutionary events that produce shuffled, reversed or repeated elements. Basically, the optimal N-map of a sequence s over a sequence t is the best way of partitioning the first sequence into N parts and placing them, possibly complementary reversed, over the second sequence in order to maximize the sum of their gapless alignment scores. We introduce an algorithm computing an optimal N-map with time complexity O (|s| × |t| × N) using O (|s| × |t| × N) memory space. Among all the numbers of parts taken in a reasonable range, we select the value N for which the optimal N-map has the most significant score. To evaluate this significance, we study the empirical distributions of the scores of optimal N-maps and show that they can be approximated by normal distributions with a reasonable accuracy. We test the functionality of the approach over random sequences on which we apply artificial evolutionary events. The method is illustrated with four case studies of pairs of sequences involving non-standard evolutionary events.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2011572868",
    "type": "article"
  },
  {
    "title": "A general framework for genome rearrangement with biological constraints",
    "doi": "https://doi.org/10.1186/s13015-019-0149-4",
    "publication_date": "2019-07-19",
    "publication_year": 2019,
    "authors": "Pijus Simonaitis; Annie Château; Krister M. Swenson",
    "corresponding_authors": "",
    "abstract": "This paper generalizes previous studies on genome rearrangement under biological constraints, using double cut and join (DCJ). We propose a model for weighted DCJ, along with a family of optimization problems called φ -MCPS (Minimum Cost Parsimonious Scenario), that are based on labeled graphs. We show how to compute solutions to general instances of φ -MCPS, given an algorithm to compute φ -MCPS on a circular genome with exactly one occurrence of each gene. These general instances can have an arbitrary number of circular and linear chromosomes, and arbitrary gene content. The practicality of the framework is displayed by presenting polynomial-time algorithms that generalize the results of Bulteau, Fertin, and Tannier on the Sorting by wDCJs and indels in intergenes problem, and that generalize previous results on the Minimum Local Parsimonious Scenario problem.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2963013324",
    "type": "article"
  },
  {
    "title": "TMRS: an algorithm for computing the time to the most recent substitution event from a multiple alignment column",
    "doi": "https://doi.org/10.1186/s13015-019-0158-3",
    "publication_date": "2019-11-18",
    "publication_year": 2019,
    "authors": "Hisanori Kiryu; Yuto Ichikawa; Yasuhiro Kojima",
    "corresponding_authors": "Hisanori Kiryu",
    "abstract": "As the number of sequenced genomes grows, researchers have access to an increasingly rich source for discovering detailed evolutionary information. However, the computational technologies for inferring biologically important evolutionary events are not sufficiently developed.We present algorithms to estimate the evolutionary time ( tMRS ) to the most recent substitution event from a multiple alignment column by using a probabilistic model of sequence evolution. As the confidence in estimated tMRS values varies depending on gap fractions and nucleotide patterns of alignment columns, we also compute the standard deviation σ of tMRS by using a dynamic programming algorithm. We identified a number of human genomic sites at which the last substitutions occurred between two speciation events in the human lineage with confidence. A large fraction of such sites have substitutions that occurred between the concestor nodes of Hominoidea and Euarchontoglires. We investigated the correlation between tissue-specific transcribed enhancers and the distribution of the sites with specific substitution time intervals, and found that brain-specific transcribed enhancers are threefold enriched in the density of substitutions in the human lineage relative to expectations.We have presented algorithms to estimate the evolutionary time ( tMRS ) to the most recent substitution event from a multiple alignment column by using a probabilistic model of sequence evolution. Our algorithms will be useful for Evo-Devo studies, as they facilitate screening potential genomic sites that have played an important role in the acquisition of unique biological features by target species.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2983899853",
    "type": "article"
  },
  {
    "title": "GrpClassifierEC: a novel classification approach based on the ensemble clustering space",
    "doi": "https://doi.org/10.1186/s13015-020-0162-7",
    "publication_date": "2020-02-13",
    "publication_year": 2020,
    "authors": "Loai AbdAllah; Malik Yousef",
    "corresponding_authors": "Malik Yousef",
    "abstract": "Abstract Background Advances in molecular biology have resulted in big and complicated data sets, therefore a clustering approach that able to capture the actual structure and the hidden patterns of the data is required. Moreover, the geometric space may not reflects the actual similarity between the different objects. As a result, in this research we use clustering-based space that convert the geometric space of the molecular to a categorical space based on clustering results. Then we use this space for developing a new classification algorithm. Results In this study, we propose a new classification method named GrpClassifierEC that replaces the given data space with categorical space based on ensemble clustering (EC). The EC space is defined by tracking the membership of the points over multiple runs of clustering algorithms. Different points that were included in the same clusters will be represented as a single point. Our algorithm classifies all these points as a single class. The similarity between two objects is defined as the number of times that these objects were not belong to the same cluster. In order to evaluate our suggested method, we compare its results to the k nearest neighbors, Decision tree and Random forest classification algorithms on several benchmark datasets. The results confirm that the suggested new algorithm GrpClassifierEC outperforms the other algorithms. Conclusions Our algorithm can be integrated with many other algorithms. In this research, we use only the k-means clustering algorithm with different k values. In future research, we propose several directions: (1) checking the effect of the clustering algorithm to build an ensemble clustering space. (2) Finding poor clustering results based on the training data, (3) reducing the volume of the data by combining similar points based on the EC. Availability and implementation The KNIME workflow, implementing GrpClassifierEC , is available at https://malikyousef.com",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3012907425",
    "type": "article"
  },
  {
    "title": "A polynomial time algorithm for computing the area under a GDT curve",
    "doi": "https://doi.org/10.1186/s13015-015-0058-0",
    "publication_date": "2015-10-26",
    "publication_year": 2015,
    "authors": "Aleksandar Poleksić",
    "corresponding_authors": "Aleksandar Poleksić",
    "abstract": "Progress in the field of protein three-dimensional structure prediction depends on the development of new and improved algorithms for measuring the quality of protein models. Perhaps the best descriptor of the quality of a protein model is the GDT function that maps each distance cutoff θ to the number of atoms in the protein model that can be fit under the distance θ from the corresponding atoms in the experimentally determined structure. It has long been known that the area under the graph of this function (GDT_A) can serve as a reliable, single numerical measure of the model quality. Unfortunately, while the well-known GDT_TS metric provides a crude approximation of GDT_A, no algorithm currently exists that is capable of computing accurate estimates of GDT_A.We prove that GDT_A is well defined and that it can be approximated by the Riemann sums, using available methods for computing accurate (near-optimal) GDT function values.In contrast to the GDT_TS metric, GDT_A is neither insensitive to large nor oversensitive to small changes in model's coordinates. Moreover, the problem of computing GDT_A is tractable. More specifically, GDT_A can be computed in cubic asymptotic time in the size of the protein model.This paper presents the first algorithm capable of computing the near-optimal estimates of the area under the GDT function for a protein model. We believe that the techniques implemented in our algorithm will pave ways for the development of more practical and reliable procedures for estimating 3D model quality.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1861969738",
    "type": "article"
  },
  {
    "title": "Erratum to: Inferring interaction type in gene regulatory networks using co-expression data",
    "doi": "https://doi.org/10.1186/s13015-015-0055-3",
    "publication_date": "2015-08-10",
    "publication_year": 2015,
    "authors": "Pegah Khosravi; Vahid Gazestani; Leila Pirhaji; Brian K. Law; Mehdi Sadeghi; Gary D. Bader; Bahram Goliaei",
    "corresponding_authors": "",
    "abstract": "[This corrects the article DOI: 10.1186/s13015-015-0054-4.].",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1909028472",
    "type": "erratum"
  },
  {
    "title": "Using the message passing algorithm on discrete data to detect faults in boolean regulatory networks",
    "doi": "https://doi.org/10.1186/s13015-014-0020-6",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Anwoy Kumar Mohanty; Aniruddha Datta; V.S. Venkatraj",
    "corresponding_authors": "",
    "abstract": "An important problem in systems biology is to model gene regulatory networks which can then be utilized to develop novel therapeutic methods for cancer treatment. Knowledge about which proteins/genes are dysregulated in a regulatory network, such as in the Mitogen Activated Protein Kinase (MAPK) Network, can be used not only to decide upon which therapy to use for a particular case of cancer, but also help in discovering effective targets for new drugs. In this work we demonstrate how one can start from a model signal transduction network derived from prior knowledge, and infer from gene expression data the probable locations of dysregulations in the network. Our model is based on Boolean networks, and the inference problem is solved using a version of the message passing algorithm. We have done simulation experiments on synthetic data to verify the efficacy of the algorithm as compared to the results from the much more computationally intensive Markov Chain Monte-Carlo methods. We also applied the model to analyze data collected from fibroblasts, thereby demonstrating how this model can be used on real world data.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2146439953",
    "type": "article"
  },
  {
    "title": "Using a constraint-based regression method for relative quantification of somatic mutations in pyrosequencing signals: a case for NRAS analysis",
    "doi": "https://doi.org/10.1186/s13015-016-0086-4",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Jérôme Ambroise; Jamal Badir; Louise Nienhaus; Annie Robert; Anne‐France Dekairelle; Jean‐Luc Gala",
    "corresponding_authors": "Jérôme Ambroise",
    "abstract": "Pyrosequencing Allele Quantification (AQ) is a cost-effective DNA sequencing method that can be used for detecting somatic mutations in formalin-fixed paraffin-embedded (FFPE) samples. The method displays a low turnaround time and a high sensitivity. Pyrosequencing suffers however from two main drawbacks including (i) low specificity and (ii) difficult signal interpretation when multiple mutations are reported in a hotspot genomic region.Using a constraint-based regression method, the new AdvISER-PYRO-SMQ algorithm was developed in the current study and implemented into an R package. As a proof-of-concept, AdvISER-PYRO-SMQ was used to identify a set of 9 distinct point mutations affecting codon 61 of the NRAS oncogene. In parallel, a pyrosequencing assay using the Qiagen software and its AQ module was used to assess selectively the presence of a single point mutation (NRAS[Formula: see text] - Q61R-1) among the set of codon 61 mutations, and to analyze related pyrosequencing signals. AdvISER-PYRO-SMQ produced a lower limit of blank (0 %) than the AQ module of Qiagen software (5.1 %) and similar limit of detection were obtained for both software (5.6 vs 4.8 %). AdvISER-PYRO-SMQ was able to screen for the presence of 9 distinct mutations with a single pyrosequencing reaction whereas the AQ module was limited to screen a single mutation per reaction.Using a constraint-based regression method enables to analyze pyrosequencing signal and to detect multiple mutations within a hotspot genomic region with an optimal compromise between sensitivity and specificity. The AdvISER-PYRO-SMQ R package provides a generic tool which can be applied on a wide range of somatic mutations. Its implementation in a Shiny web interactive application (available at https://ucl-irec-ctma.shinyapps.io/Pyrosequencing-NRAS-61/) enables its use in research or clinical routine applications.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2520929647",
    "type": "article"
  },
  {
    "title": "Erratum to: A representation of a compressed de Bruijn graph for pan-genome analysis that enables search",
    "doi": "https://doi.org/10.1186/s13015-016-0090-8",
    "publication_date": "2016-11-28",
    "publication_year": 2016,
    "authors": "Timo Beller; Enno Ohlebusch",
    "corresponding_authors": "",
    "abstract": "[This corrects the article DOI: 10.1186/s13015-016-0083-7.].",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2558824011",
    "type": "erratum"
  },
  {
    "title": "Not assessing the efficiency of multiple sequence alignment programs",
    "doi": "https://doi.org/10.1186/1748-7188-9-18",
    "publication_date": "2014-07-05",
    "publication_year": 2014,
    "authors": "Andrew E. Torda",
    "corresponding_authors": "Andrew E. Torda",
    "abstract": "One can search for messages in the digits of π or a Kazakhstan telephone book, but there may be hidden messages closer to home. A recent publication in this journal purportedly compared a set of multiple sequence alignment programs. The real purpose of the article may have been to remind readers how to present scientific data.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3152311419",
    "type": "article"
  },
  {
    "title": "The solution surface of the Li-Stephens haplotype copying model",
    "doi": "https://doi.org/10.1186/s13015-023-00237-z",
    "publication_date": "2023-08-09",
    "publication_year": 2023,
    "authors": "Y. Jin; Jonathan Terhorst",
    "corresponding_authors": "",
    "abstract": "Abstract The Li-Stephens (LS) haplotype copying model forms the basis of a number of important statistical inference procedures in genetics. LS is a probabilistic generative model which supposes that a sampled chromosome is an imperfect mosaic of other chromosomes found in a population. In the frequentist setting which is the focus of this paper, the output of LS is a “copying path” through chromosome space. The behavior of LS depends crucially on two user-specified parameters, $$\\theta $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>θ</mml:mi> </mml:math> and $$\\rho $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>ρ</mml:mi> </mml:math> , which are respectively interpreted as the rates of mutation and recombination. However, because LS is not based on a realistic model of ancestry, the precise connection between these parameters and the biological phenomena they represent is unclear. Here, we offer an alternative perspective, which considers $$\\theta $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>θ</mml:mi> </mml:math> and $$\\rho $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>ρ</mml:mi> </mml:math> as tuning parameters, and seeks to understand their impact on the LS output. We derive an algorithm which, for a given dataset, efficiently partitions the $$(\\theta ,\\rho )$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>)</mml:mo> </mml:mrow> </mml:math> plane into regions where the output of the algorithm is constant, thereby enumerating all possible solutions to the LS model in one go. We extend this approach to the “diploid LS” model commonly used for phasing. We demonstrate the usefulness of our method by studying the effects of changing $$\\theta $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>θ</mml:mi> </mml:math> and $$\\rho $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>ρ</mml:mi> </mml:math> when using LS for common bioinformatic tasks. Our findings indicate that using the conventional (i.e., population-scaled) values for $$\\theta $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>θ</mml:mi> </mml:math> and $$\\rho $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>ρ</mml:mi> </mml:math> produces near optimal results for imputation, but may systematically inflate switch error in the case of phasing diploid genotypes.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385703311",
    "type": "article"
  },
  {
    "title": "Getting DNA copy numbers without control samples",
    "doi": "https://doi.org/10.1186/1748-7188-7-19",
    "publication_date": "2012-08-16",
    "publication_year": 2012,
    "authors": "María Ortiz-Estévez; Ander Aramburu; Ángel Rubio",
    "corresponding_authors": "",
    "abstract": "The selection of the reference to scale the data in a copy number analysis has paramount importance to achieve accurate estimates. Usually this reference is generated using control samples included in the study. However, these control samples are not always available and in these cases, an artificial reference must be created. A proper generation of this signal is crucial in terms of both noise and bias.We propose NSA (Normality Search Algorithm), a scaling method that works with and without control samples. It is based on the assumption that genomic regions enriched in SNPs with identical copy numbers in both alleles are likely to be normal. These normal regions are predicted for each sample individually and used to calculate the final reference signal. NSA can be applied to any CN data regardless the microarray technology and preprocessing method. It also finds an optimal weighting of the samples minimizing possible batch effects.Five human datasets (a subset of HapMap samples, Glioblastoma Multiforme (GBM), Ovarian, Prostate and Lung Cancer experiments) have been analyzed. It is shown that using only tumoral samples, NSA is able to remove the bias in the copy number estimation, to reduce the noise and therefore, to increase the ability to detect copy number aberrations (CNAs). These improvements allow NSA to also detect recurrent aberrations more accurately than other state of the art methods.NSA provides a robust and accurate reference for scaling probe signals data to CN values without the need of control samples. It minimizes the problems of bias, noise and batch effects in the estimation of CNs. Therefore, NSA scaling approach helps to better detect recurrent CNAs than current methods. The automatic selection of references makes it useful to perform bulk analysis of many GEO or ArrayExpress experiments without the need of developing a parser to find the normal samples or possible batches within the data. The method is available in the open-source R package NSA, which is an add-on to the aroma.cn framework. http://www.aroma-project.org/addons.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2002754342",
    "type": "article"
  },
  {
    "title": "Asymptotic structural properties of quasi-random saturated structures of RNA",
    "doi": "https://doi.org/10.1186/1748-7188-8-24",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Peter Clote; Evangelos Kranakis; Danny Kriz̧anc",
    "corresponding_authors": "",
    "abstract": "RNA folding depends on the distribution of kinetic traps in the landscape of all secondary structures. Kinetic traps in the Nussinov energy model are precisely those secondary structures that are saturated, meaning that no base pair can be added without introducing either a pseudoknot or base triple. In previous work, we investigated asymptotic combinatorics of both random saturated structures and of quasi-random saturated structures, where the latter are constructed by a natural stochastic process.We prove that for quasi-random saturated structures with the uniform distribution, the asymptotic expected number of external loops is O(logn) and the asymptotic expected maximum stem length is O(logn), while under the Zipf distribution, the asymptotic expected number of external loops is O(log2n) and the asymptotic expected maximum stem length is O(logn/log logn).Quasi-random saturated structures are generated by a stochastic greedy method, which is simple to implement. Structural features of random saturated structures appear to resemble those of quasi-random saturated structures, and the latter appear to constitute a class for which both the generation of sampled structures as well as a combinatorial investigation of structural features may be simpler to undertake.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2116920786",
    "type": "article"
  },
  {
    "title": "Sampling solution traces for the problem of sorting permutations by signed reversals",
    "doi": "https://doi.org/10.1186/1748-7188-7-18",
    "publication_date": "2012-06-15",
    "publication_year": 2012,
    "authors": "Christian Baudet; Zanoni Dias; Marie‐France Sagot",
    "corresponding_authors": "",
    "abstract": "Traditional algorithms to solve the problem of sorting by signed reversals output just one optimal solution while the space of all optimal solutions can be huge. A so-called trace represents a group of solutions which share the same set of reversals that must be applied to sort the original permutation following a partial ordering. By using traces, we therefore can represent the set of optimal solutions in a more compact way. Algorithms for enumerating the complete set of traces of solutions were developed. However, due to their exponential complexity, their practical use is limited to small permutations. A partial enumeration of traces is a sampling of the complete set of traces and can be an alternative for the study of distinct evolutionary scenarios of big permutations. Ideally, the sampling should be done uniformly from the space of all optimal solutions. This is however conjectured to be ♯ P-complete. We propose and evaluate three algorithms for producing a sampling of the complete set of traces that instead can be shown in practice to preserve some of the characteristics of the space of all solutions. The first algorithm (RA) performs the construction of traces through a random selection of reversals on the list of optimal 1-sequences. The second algorithm (DFALT) consists in a slight modification of an algorithm that performs the complete enumeration of traces. Finally, the third algorithm (SWA) is based on a sliding window strategy to improve the enumeration of traces. All proposed algorithms were able to enumerate traces for permutations with up to 200 elements. We analysed the distribution of the enumerated traces with respect to their height and average reversal length. Various works indicate that the reversal length can be an important aspect in genome rearrangements. The algorithms RA and SWA show a tendency to lose traces with high average reversal length. Such traces are however rare, and qualitatively our results show that, for testable-sized permutations, the algorithms DFALT and SWA produce distributions which approximate the reversal length distributions observed with a complete enumeration of the set of traces.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2118343613",
    "type": "article"
  },
  {
    "title": "Estimating population size via line graph reconstruction",
    "doi": "https://doi.org/10.1186/1748-7188-8-17",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Bjarni V. Halldórsson; Dima Blokh; Roded Sharan",
    "corresponding_authors": "",
    "abstract": "We propose a novel graph theoretic method to estimate haplotype population size from genotype data. The method considers only the potential sharing of haplotypes between individuals and is based on transforming the graph of potential haplotype sharing into a line graph using a minimum number of edge and vertex deletions.We show that the resulting line graph deletion problems are NP complete and provide exact integer programming solutions for them. We test our approach using extensive simulations of multiple population evolution and genotypes sampling scenarios. Our results also indicate that the method may be useful in comparing populations and it may be used as a first step in a method for haplotype phasing.Our computational experiments show that when most of the sharings are true sharings the problem can be solved very fast and the estimated size is very close to the true size; when many of the potential sharings do not stem from true haplotype sharing, our method gives reasonable lower bounds on the underlying number of haplotypes. In comparison, a naive approach of phasing the input genotypes provides trivial upper bounds of twice the number of genotypes.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2131805297",
    "type": "article"
  },
  {
    "title": "Estimating the evidence of selection and the reliability of inference in unigenic evolution",
    "doi": "https://doi.org/10.1186/1748-7188-5-35",
    "publication_date": "2010-11-08",
    "publication_year": 2010,
    "authors": "Andrew D. Fernandes; Benjamin P. Kleinstiver; David R. Edgell; Lindi M. Wahl; Gregory B. Gloor",
    "corresponding_authors": "Andrew D. Fernandes",
    "abstract": "Unigenic evolution is a large-scale mutagenesis experiment used to identify residues that are potentially important for protein function. Both currently-used methods for the analysis of unigenic evolution data analyze 'windows' of contiguous sites, a strategy that increases statistical power but incorrectly assumes that functionally-critical sites are contiguous. In addition, both methods require the questionable assumption of asymptotically-large sample size due to the presumption of approximate normality.We develop a novel approach, termed the Evidence of Selection (EoS), removing the assumption that functionally important sites are adjacent in sequence and and explicitly modelling the effects of limited sample-size. Precise statistical derivations show that the EoS score can be easily interpreted as an expected log-odds-ratio between two competing hypotheses, namely, the hypothetical presence or absence of functional selection for a given site. Using the EoS score, we then develop selection criteria by which functionally-important yet non-adjacent sites can be identified. An approximate power analysis is also developed to estimate the reliability of inference given the data. We validate and demonstrate the the practical utility of our method by analysis of the homing endonuclease I-Bmol, comparing our predictions with the results of existing methods.Our method is able to assess both the evidence of selection at individual amino acid sites and estimate the reliability of those inferences. Experimental validation with I-Bmol proves its utility to identify functionally-important residues of poorly characterized proteins, demonstrating increased sensitivity over previous methods without loss of specificity. With the ability to guide the selection of precise experimental mutagenesis conditions, our method helps make unigenic analysis a more broadly applicable technique with which to probe protein function.Software to compute, plot, and summarize EoS data is available as an open-source package called 'unigenic' for the 'R' programming language at http://www.fernandes.org/txp/article/13/an-analytical-framework-for-unigenic-evolution.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2017156855",
    "type": "article"
  },
  {
    "title": "Core column prediction for protein multiple sequence alignments",
    "doi": "https://doi.org/10.1186/s13015-017-0102-3",
    "publication_date": "2017-04-19",
    "publication_year": 2017,
    "authors": "Dan DeBlasio; John Kececioglu",
    "corresponding_authors": "",
    "abstract": "In a computed protein multiple sequence alignment, the coreness of a column is the fraction of its substitutions that are in so-called core columns of the gold-standard reference alignment of its proteins. In benchmark suites of protein reference alignments, the core columns of the reference alignment are those that can be confidently labeled as correct, usually due to all residues in the column being sufficiently close in the spatial superposition of the known three-dimensional structures of the proteins. Typically the accuracy of a protein multiple sequence alignment that has been computed for a benchmark is only measured with respect to the core columns of the reference alignment. When computing an alignment in practice, however, a reference alignment is not known, so the coreness of its columns can only be predicted.We develop for the first time a predictor of column coreness for protein multiple sequence alignments. This allows us to predict which columns of a computed alignment are core, and hence better estimate the alignment's accuracy. Our approach to predicting coreness is similar to nearest-neighbor classification from machine learning, except we transform nearest-neighbor distances into a coreness prediction via a regression function, and we learn an appropriate distance function through a new optimization formulation that solves a large-scale linear programming problem. We apply our coreness predictor to parameter advising, the task of choosing parameter values for an aligner's scoring function to obtain a more accurate alignment of a specific set of sequences. We show that for this task, our predictor strongly outperforms other column-confidence estimators from the literature, and affords a substantial boost in alignment accuracy.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2605412425",
    "type": "article"
  },
  {
    "title": "Isometric gene tree reconciliation revisited",
    "doi": "https://doi.org/10.1186/s13015-017-0108-x",
    "publication_date": "2017-06-13",
    "publication_year": 2017,
    "authors": "Broňa Brejová; Askar Gafurov; Dana Pardubská; Michal Šabo; Tomáš Vinař",
    "corresponding_authors": "Broňa Brejová",
    "abstract": "Isometric gene tree reconciliation is a gene tree/species tree reconciliation problem where both the gene tree and the species tree include branch lengths, and these branch lengths must be respected by the reconciliation. The problem was introduced by Ma et al. in 2008 in the context of reconstructing evolutionary histories of genomes in the infinite sites model. In this paper, we show that the original algorithm by Ma et al. is incorrect, and we propose a modified algorithm that addresses the problems that we discovered. We have also improved the running time from $$O(N^2)$$ to $$O(N\\log N)$$ , where N is the total number of nodes in the two input trees. Finally, we examine two new variants of the problem: reconciliation of two unrooted trees and scaling of branch lengths of the gene tree during reconciliation of two rooted trees. We provide several new algorithms for isometric reconciliation of trees. Some questions in this area remain open; most importantly extensions of the problem allowing for imprecise estimates of branch lengths.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2626652475",
    "type": "article"
  },
  {
    "title": "Algorithms for matching partially labelled sequence graphs",
    "doi": "https://doi.org/10.1186/s13015-017-0115-y",
    "publication_date": "2017-09-25",
    "publication_year": 2017,
    "authors": "William R. Taylor",
    "corresponding_authors": "William R. Taylor",
    "abstract": "In order to find correlated pairs of positions between proteins, which are useful in predicting interactions, it is necessary to concatenate two large multiple sequence alignments such that the sequences that are joined together belong to those that interact in their species of origin. When each protein is unique then the species name is sufficient to guide this match, however, when there are multiple related sequences (paralogs) in each species then the pairing is more difficult. In bacteria a good guide can be gained from genome co-location as interacting proteins tend to be in a common operon but in eukaryotes this simple principle is not sufficient. The methods developed in this paper take sets of paralogs for different proteins found in the same species and make a pairing based on their evolutionary distance relative to a set of other proteins that are unique and so have a known relationship (singletons). The former constitute a set of unlabelled nodes in a graph while the latter are labelled. Two variants were tested, one based on a phylogenetic tree of the sequences (the topology-based method) and a simpler, faster variant based only on the inter-sequence distances (the distance-based method). Over a set of test proteins, both gave good results, with the topology method performing slightly better. The methods develop here still need refinement and augmentation from constraints other than the sequence data alone, such as known interactions from annotation and databases, or non-trivial relationships in genome location. With the ever growing numbers of eukaryotic genomes, it is hoped that the methods described here will open a route to the use of these data equal to the current success attained with bacterial sequences.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2757029804",
    "type": "article"
  },
  {
    "title": "Approximate search for known gene clusters in new genomes using PQ-trees",
    "doi": "https://doi.org/10.1186/s13015-021-00190-9",
    "publication_date": "2021-07-09",
    "publication_year": 2021,
    "authors": "Galia R. Zimerman; Dina Svetlitsky; Meirav Zehavi; Michal Ziv-Ukelson",
    "corresponding_authors": "",
    "abstract": "Abstract Gene clusters are groups of genes that are co-locally conserved across various genomes, not necessarily in the same order. Their discovery and analysis is valuable in tasks such as gene annotation and prediction of gene interactions, and in the study of genome organization and evolution. The discovery of conserved gene clusters in a given set of genomes is a well studied problem, but with the rapid sequencing of prokaryotic genomes a new problem is inspired. Namely, given an already known gene cluster that was discovered and studied in one genomic dataset, to identify all the instances of the gene cluster in a given new genomic sequence. Thus, we define a new problem in comparative genomics, denoted PQ-Tree Search that takes as input a PQ-tree T representing the known gene orders of a gene cluster of interest, a gene-to-gene substitution scoring function h , integer arguments $$d_T$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math> and $$d_S$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>d</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math> , and a new sequence of genes S . The objective is to identify in S approximate new instances of the gene cluster; These instances could vary from the known gene orders by genome rearrangements that are constrained by T , by gene substitutions that are governed by h , and by gene deletions and insertions that are bounded from above by $$d_T$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math> and $$d_S$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>d</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math> , respectively. We prove that PQ-Tree Search is -hard and propose a parameterized algorithm that solves the optimization variant of PQ-Tree Search in $$O^*(2^{\\gamma })$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:msup><mml:mi>O</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>γ</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> time, where $$\\gamma$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>γ</mml:mi></mml:math> is the maximum degree of a node in T and $$O^*$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msup><mml:mi>O</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:math> is used to hide factors polynomial in the input size. The algorithm is implemented as a search tool, denoted PQFinder, and applied to search for instances of chromosomal gene clusters in plasmids, within a dataset of 1,487 prokaryotic genomes. We report on 29 chromosomal gene clusters that are rearranged in plasmids, where the rearrangements are guided by the corresponding PQ-trees. One of these results, coding for a heavy metal efflux pump, is further analysed to exemplify how PQFinder can be harnessed to reveal interesting new structural variants of known gene clusters.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3081786092",
    "type": "article"
  },
  {
    "title": "The energy-spectrum of bicompatible sequences",
    "doi": "https://doi.org/10.1186/s13015-021-00187-4",
    "publication_date": "2021-06-01",
    "publication_year": 2021,
    "authors": "Fenix W. D. Huang; Christopher L. Barrett; Christian M. Reidys",
    "corresponding_authors": "",
    "abstract": "Genotype-phenotype maps provide a meaningful filtration of sequence space and RNA secondary structures are particular such phenotypes. Compatible sequences, which satisfy the base-pairing constraints of a given RNA structure, play an important role in the context of neutral evolution. Sequences that are simultaneously compatible with two given structures (bicompatible sequences), are beacons in phenotypic transitions, induced by erroneously replicating populations of RNA sequences. RNA riboswitches, which are capable of expressing two distinct secondary structures without changing the underlying sequence, are one example of bicompatible sequences in living organisms.We present a full loop energy model Boltzmann sampler of bicompatible sequences for pairs of structures. The sequence sampler employs a dynamic programming routine whose time complexity is polynomial when assuming the maximum number of exposed vertices, [Formula: see text], is a constant. The parameter [Formula: see text] depends on the two structures and can be very large. We introduce a novel topological framework encapsulating the relations between loops that sheds light on the understanding of [Formula: see text]. Based on this framework, we give an algorithm to sample sequences with minimum [Formula: see text] on a particular topologically classified case as well as giving hints to the solution in the other cases. As a result, we utilize our sequence sampler to study some established riboswitches.Our analysis of riboswitch sequences shows that a pair of structures needs to satisfy key properties in order to facilitate phenotypic transitions and that pairs of random structures are unlikely to do so. Our analysis observes a distinct signature of riboswitch sequences, suggesting a new criterion for identifying native sequences and sequences subjected to evolutionary pressure. Our free software is available at: https://github.com/FenixHuang667/Bifold .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3164922525",
    "type": "article"
  },
  {
    "title": "Fast and efficient Rmap assembly using the Bi-labelled de Bruijn graph",
    "doi": "https://doi.org/10.1186/s13015-021-00182-9",
    "publication_date": "2021-05-25",
    "publication_year": 2021,
    "authors": "Kingshuk Mukherjee; Massimiliano Rossi; Leena Salmela; Christina Boucher",
    "corresponding_authors": "Kingshuk Mukherjee",
    "abstract": "Abstract Genome wide optical maps are high resolution restriction maps that give a unique numeric representation to a genome. They are produced by assembling hundreds of thousands of single molecule optical maps, which are called Rmaps. Unfortunately, there are very few choices for assembling Rmap data. There exists only one publicly-available non-proprietary method for assembly and one proprietary software that is available via an executable. Furthermore, the publicly-available method, by Valouev et al. (Proc Natl Acad Sci USA 103(43):15770–15775, 2006), follows the overlap-layout-consensus (OLC) paradigm, and therefore, is unable to scale for relatively large genomes. The algorithm behind the proprietary method, Bionano Genomics’ Solve, is largely unknown. In this paper, we extend the definition of bi-labels in the paired de Bruijn graph to the context of optical mapping data, and present the first de Bruijn graph based method for Rmap assembly. We implement our approach, which we refer to as rmapper , and compare its performance against the assembler of Valouev et al. (Proc Natl Acad Sci USA 103(43):15770–15775, 2006) and Solve by Bionano Genomics on data from three genomes: E. coli , human, and climbing perch fish ( Anabas Testudineus ). Our method was able to successfully run on all three genomes. The method of Valouev et al. (Proc Natl Acad Sci USA 103(43):15770–15775, 2006) only successfully ran on E. coli . Moreover, on the human genome rmapper was at least 130 times faster than Bionano Solve, used five times less memory and produced the highest genome fraction with zero mis-assemblies. Our software, rmapper is written in C++ and is publicly available under GNU General Public License at https://github.com/kingufl/Rmapper .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3165081484",
    "type": "article"
  },
  {
    "title": "A fast and accurate enumeration-based algorithm for haplotyping a triploid individual",
    "doi": "https://doi.org/10.1186/s13015-018-0129-0",
    "publication_date": "2018-06-01",
    "publication_year": 2018,
    "authors": "Jingli Wu; Qian Zhang",
    "corresponding_authors": "Jingli Wu",
    "abstract": "Haplotype assembly, reconstructing haplotypes from sequence data, is one of the major computational problems in bioinformatics. Most of the current methodologies for haplotype assembly are designed for diploid individuals. In recent years, genomes having more than two sets of homologous chromosomes have attracted many research groups that are interested in the genomics of disease, phylogenetics, botany and evolution. However, there is still a lack of methods for reconstructing polyploid haplotypes. In this work, the minimum error correction with genotype information (MEC/GI) model, an important combinatorial model for haplotyping a single individual, is used to study the triploid individual haplotype reconstruction problem. A fast and accurate enumeration-based algorithm enumeration haplotyping triploid with least difference (EHTLD) is proposed for solving the MEC/GI model. The EHTLD algorithm tries to reconstruct the three haplotypes according to the order of single nucleotide polymorphism (SNP) loci along them. When reconstructing a given SNP site, the EHTLD algorithm enumerates three kinds of SNP values in terms of the corresponding site’s genotype value, and chooses the one, which leads to the minimum difference between the reconstructed haplotypes and the sequenced fragments covering that SNP site, to fill the SNP loci being reconstructed. Extensive experimental comparisons were performed between the EHTLD algorithm and the well known HapCompass and HapTree. Compared with algorithms HapCompass and HapTree, the EHTLD algorithm can reconstruct more accurate haplotypes, which were proven by a number of experiments.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2807025746",
    "type": "article"
  },
  {
    "title": "Improved de novo peptide sequencing using LC retention time information",
    "doi": "https://doi.org/10.1186/s13015-018-0132-5",
    "publication_date": "2018-08-29",
    "publication_year": 2018,
    "authors": "Yves Frank; Tomáš Hrúz; Thomas Tschager; Valentin Venzin",
    "corresponding_authors": "",
    "abstract": "Liquid chromatography combined with tandem mass spectrometry is an important tool in proteomics for peptide identification. Liquid chromatography temporally separates the peptides in a sample. The peptides that elute one after another are analyzed via tandem mass spectrometry by measuring the mass-to-charge ratio of a peptide and its fragments. De novo peptide sequencing is the problem of reconstructing the amino acid sequences of a peptide from this measurement data. Past de novo sequencing algorithms solely consider the mass spectrum of the fragments for reconstructing a sequence.We propose to additionally exploit the information obtained from liquid chromatography. We study the problem of computing a sequence that is not only in accordance with the experimental mass spectrum, but also with the chromatographic retention time. We consider three models for predicting the retention time and develop algorithms for de novo sequencing for each model.Based on an evaluation for two prediction models on experimental data from synthesized peptides we conclude that the identification rates are improved by exploiting the chromatographic information. In our evaluation, we compare our algorithms using the retention time information with algorithms using the same scoring model, but not the retention time.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2888787159",
    "type": "article"
  },
  {
    "title": "A cubic algorithm for the generalized rank median of three genomes",
    "doi": "https://doi.org/10.1186/s13015-019-0150-y",
    "publication_date": "2019-07-26",
    "publication_year": 2019,
    "authors": "Leonid Chindelevitch; Sean La; João Meidânis",
    "corresponding_authors": "Leonid Chindelevitch",
    "abstract": "The area of genome rearrangements has given rise to a number of interesting biological, mathematical and algorithmic problems. Among these, one of the most intractable ones has been that of finding the median of three genomes, a special case of the ancestral reconstruction problem. In this work we re-examine our recently proposed way of measuring genome rearrangement distance, namely, the rank distance between the matrix representations of the corresponding genomes, and show that the median of three genomes can be computed exactly in polynomial time O(nω) , where ω≤3 , with respect to this distance, when the median is allowed to be an arbitrary orthogonal matrix.We define the five fundamental subspaces depending on three input genomes, and use their properties to show that a particular action on each of these subspaces produces a median. In the process we introduce the notion of M-stable subspaces. We also show that the median found by our algorithm is always orthogonal, symmetric, and conserves any adjacencies or telomeres present in at least 2 out of 3 input genomes.We test our method on both simulated and real data. We find that the majority of the realistic inputs result in genomic outputs, and for those that do not, our two heuristics perform well in terms of reconstructing a genomic matrix attaining a score close to the lower bound, while running in a reasonable amount of time. We conclude that the rank distance is not only theoretically intriguing, but also practically useful for median-finding, and potentially ancestral genome reconstruction.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2965995818",
    "type": "article"
  },
  {
    "title": "Bayesian localization of CNV candidates in WGS data within minutes",
    "doi": "https://doi.org/10.1186/s13015-019-0154-7",
    "publication_date": "2019-09-23",
    "publication_year": 2019,
    "authors": "John Wiedenhoeft; Alex Cagan; R. V. Kozhemyakina; Р. Г. Гулевич; Alexander Schliep",
    "corresponding_authors": "",
    "abstract": "Full Bayesian inference for detecting copy number variants (CNV) from whole-genome sequencing (WGS) data is still largely infeasible due to computational demands. A recently introduced approach to perform Forward-Backward Gibbs sampling using dynamic Haar wavelet compression has alleviated issues of convergence and, to some extent, speed. Yet, the problem remains challenging in practice.In this paper, we propose an improved algorithmic framework for this approach. We provide new space-efficient data structures to query sufficient statistics in logarithmic time, based on a linear-time, in-place transform of the data, which also improves on the compression ratio. We also propose a new approach to efficiently store and update marginal state counts obtained from the Gibbs sampler.Using this approach, we discover several CNV candidates in two rat populations divergently selected for tame and aggressive behavior, consistent with earlier results concerning the domestication syndrome as well as experimental observations. Computationally, we observe a 29.5-fold decrease in memory, an average 5.8-fold speedup, as well as a 191-fold decrease in minor page faults. We also observe that metrics varied greatly in the old implementation, but not the new one. We conjecture that this is due to the better compression scheme. The fully Bayesian segmentation of the entire WGS data set required 3.5 min and 1.24 GB of memory, and can hence be performed on a commodity laptop.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2975847197",
    "type": "article"
  },
  {
    "title": "New generalized metric based on branch length distance to compare B cell lineage trees",
    "doi": "https://doi.org/10.1186/s13015-024-00267-1",
    "publication_date": "2024-10-05",
    "publication_year": 2024,
    "authors": "Mahsa Farnia; Nadia Tahiri",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403127906",
    "type": "article"
  },
  {
    "title": "On the parameterized complexity of the median and closest problems under some permutation metrics",
    "doi": "https://doi.org/10.1186/s13015-024-00269-z",
    "publication_date": "2024-12-24",
    "publication_year": 2024,
    "authors": "Luís Cunha; Ignasi Sau; Uéverton S. Souza",
    "corresponding_authors": "",
    "abstract": "Genome rearrangements are events where large blocks of DNA exchange places during evolution. The analysis of these events is a promising tool for understanding evolutionary genomics, providing data for phylogenetic reconstruction based on genome rearrangement measures. Many pairwise rearrangement distances have been proposed, based on finding the minimum number of rearrangement events to transform one genome into the other, using some predefined operation. When more than two genomes are considered, we have the more challenging problem of rearrangement-based phylogeny reconstruction. Given a set of genomes and a distance notion, there are at least two natural ways to define the \"target\" genome. On the one hand, finding a genome that minimizes the sum of the distances from this to any other, called the median genome. On the other hand, finding a genome that minimizes the maximum distance to any other, called the closest genome. Considering genomes as permutations of distinct integers, some distance metrics have been extensively studied. We investigate the median and closest problems on permutations over the following metrics: breakpoint distance, swap distance, block-interchange distance, short-block-move distance, and transposition distance. In biological applications some values are usually very small, such as the solution value d or the number k of input permutations. For each of these metrics and parameters d or k, we analyze the closest and the median problems from the viewpoint of parameterized complexity. We obtain the following results: NP-hardness for finding the median/closest permutation regarding some metrics of distance, even for only $$k = 3$$ permutations; Polynomial kernels for the problems of finding the median permutation of all studied metrics, considering the target distance d as parameter; NP-hardness result for finding the closest permutation by short-block-moves; FPT algorithms and infeasibility of polynomial kernels for finding the closest permutation for some metrics when parameterized by the target distance d.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405734317",
    "type": "article"
  },
  {
    "title": "Non-parametric and semi-parametric support estimation using SEquential RESampling random walks on biomolecular sequences",
    "doi": "https://doi.org/10.1186/s13015-020-00167-0",
    "publication_date": "2020-04-16",
    "publication_year": 2020,
    "authors": "Wei Wang; J. Torquil Smith; Hussein A. Hejase; Kevin J. Liu",
    "corresponding_authors": "",
    "abstract": "Non-parametric and semi-parametric resampling procedures are widely used to perform support estimation in computational biology and bioinformatics. Among the most widely used methods in this class is the standard bootstrap method, which consists of random sampling with replacement. While not requiring assumptions about any particular parametric model for resampling purposes, the bootstrap and related techniques assume that sites are independent and identically distributed (i.i.d.). The i.i.d. assumption can be an over-simplification for many problems in computational biology and bioinformatics. In particular, sequential dependence within biomolecular sequences is often an essential biological feature due to biochemical function, evolutionary processes such as recombination, and other factors. To relax the simplifying i.i.d. assumption, we propose a new non-parametric/semi-parametric sequential resampling technique that generalizes \"Heads-or-Tails\" mirrored inputs, a simple but clever technique due to Landan and Graur. The generalized procedure takes the form of random walks along either aligned or unaligned biomolecular sequences. We refer to our new method as the SERES (or \"SEquential RESampling\") method. To demonstrate the performance of the new technique, we apply SERES to estimate support for the multiple sequence alignment problem. Using simulated and empirical data, we show that SERES-based support estimation yields comparable or typically better performance compared to state-of-the-art methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3030881994",
    "type": "article"
  },
  {
    "title": "Algorithms for the quantitative Lock/Key model of cytoplasmic incompatibility",
    "doi": "https://doi.org/10.1186/s13015-020-00174-1",
    "publication_date": "2020-07-22",
    "publication_year": 2020,
    "authors": "Tiziana Calamoneri; Mattia Gastaldello; Arnaud Mary; Marie-France Sagot; Blerina Sinaimeri",
    "corresponding_authors": "",
    "abstract": "Abstract Cytoplasmic incompatibility (CI) relates to the manipulation by the parasite Wolbachia of its host reproduction. Despite its widespread occurrence, the molecular basis of CI remains unclear and theoretical models have been proposed to understand the phenomenon. We consider in this paper the quantitative Lock-Key model which currently represents a good hypothesis that is consistent with the data available. CI is in this case modelled as the problem of covering the edges of a bipartite graph with the minimum number of chain subgraphs. This problem is already known to be NP-hard, and we provide an exponential algorithm with a non trivial complexity. It is frequent that depending on the dataset, there may be many optimal solutions which can be biologically quite different among them. To rely on a single optimal solution may therefore be problematic. To this purpose, we address the problem of enumerating (listing) all minimal chain subgraph covers of a bipartite graph and show that it can be solved in quasi-polynomial time. Interestingly, in order to solve the above problems, we considered also the problem of enumerating all the maximal chain subgraphs of a bipartite graph and improved on the current results in the literature for the latter. Finally, to demonstrate the usefulness of our methods we show an application on a real dataset.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3044583781",
    "type": "article"
  },
  {
    "title": "Correction: Heuristic shortest hyperpaths in cell signaling hypergraphs",
    "doi": "https://doi.org/10.1186/s13015-022-00222-y",
    "publication_date": "2022-12-29",
    "publication_year": 2022,
    "authors": "Spencer Krieger; John Kececioglu",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4313272315",
    "type": "erratum"
  },
  {
    "title": "Global exact optimisations for chloroplast structural haplotype scaffolding",
    "doi": "https://doi.org/10.1186/s13015-023-00243-1",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Victor Epain; Rumen Andonov",
    "corresponding_authors": "",
    "abstract": "Abstract Background Scaffolding is an intermediate stage of fragment assembly. It consists in orienting and ordering the contigs obtained by the assembly of the sequencing reads. In the general case, the problem has been largely studied with the use of distances data between the contigs. Here we focus on a dedicated scaffolding for the chloroplast genomes. As these genomes are small, circular and with few specific repeats, numerous approaches have been proposed to assemble them. However, their specificities have not been sufficiently exploited. Results We give a new formulation for the scaffolding in the case of chloroplast genomes as a discrete optimisation problem, that we prove the decision version to be $$\\mathcal{NP}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>NP</mml:mi> </mml:math> -Complete. We take advantage of the knowledge of chloroplast genomes and succeed in expressing the relationships between a few specific genomic repeats in mathematical constraints. Our approach is independent of the distances and adopts a genomic regions view, with the priority on scaffolding the repeats first. In this way, we encode the structural haplotype issue in order to retrieve several genome forms that coexist in the same chloroplast cell. To solve exactly the optimisation problem, we develop an integer linear program that we implement in Python3 package khloraascaf. We test it on synthetic data to investigate its performance behaviour and its robustness against several chosen difficulties. Conclusions We succeed to model biological knowledge on genomic structures to scaffold chloroplast genomes. Our results suggest that modelling genomic regions is sufficient for scaffolding repeats and is suitable for finding several solutions corresponding to several genome forms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391575695",
    "type": "article"
  },
  {
    "title": "Median quartet tree search algorithms using optimal subtree prune and regraft",
    "doi": "https://doi.org/10.1186/s13015-024-00257-3",
    "publication_date": "2024-03-13",
    "publication_year": 2024,
    "authors": "Shayesteh Arasti; Siavash Mirarab",
    "corresponding_authors": "Siavash Mirarab",
    "abstract": "Abstract Gene trees can be different from the species tree due to biological processes and inference errors. One way to obtain a species tree is to find one that maximizes some measure of similarity to a set of gene trees. The number of shared quartets between a potential species tree and gene trees provides a statistically justifiable score; if maximized properly, it could result in a statistically consistent estimator of the species tree under several statistical models of discordance. However, finding the median quartet score tree, one that maximizes this score, is NP-Hard, motivating several existing heuristic algorithms. These heuristics do not follow the hill-climbing paradigm used extensively in phylogenetics. In this paper, we make theoretical contributions that enable an efficient hill-climbing approach. Specifically, we show that a subtree of size m can be placed optimally on a tree of size n in quasi-linear time with respect to n and (almost) independently of m . This result enables us to perform subtree prune and regraft (SPR) rearrangements as part of a hill-climbing search. We show that this approach can slightly improve upon the results of widely-used methods such as ASTRAL in terms of the optimization score but not necessarily accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392747541",
    "type": "article"
  },
  {
    "title": "Fast, parallel, and cache-friendly suffix array construction",
    "doi": "https://doi.org/10.1186/s13015-024-00263-5",
    "publication_date": "2024-04-28",
    "publication_year": 2024,
    "authors": "Jamshed Khan; Tobias Rubel; Erin K. Molloy; Laxman Dhulipala; Rob Patro",
    "corresponding_authors": "",
    "abstract": "Abstract Purpose String indexes such as the suffix array ( sa ) and the closely related longest common prefix ( lcp ) array are fundamental objects in bioinformatics and have a wide variety of applications. Despite their importance in practice, few scalable parallel algorithms for constructing these are known, and the existing algorithms can be highly non-trivial to implement and parallelize. Methods In this paper we present caps-sa , a simple and scalable parallel algorithm for constructing these string indexes inspired by samplesort and utilizing an LCP-informed mergesort. Due to its design, caps-sa has excellent memory-locality and thus incurs fewer cache misses and achieves strong performance on modern multicore systems with deep cache hierarchies. Results We show that despite its simple design, caps-sa outperforms existing state-of-the-art parallel sa and lcp -array construction algorithms on modern hardware. Finally, motivated by applications in modern aligners where the query strings have bounded lengths, we introduce the notion of a bounded-context sa and show that caps-sa can easily be extended to exploit this structure to obtain further speedups. We make our code publicly available at https://github.com/jamshed/CaPS-SA .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4395954957",
    "type": "article"
  },
  {
    "title": "Revisiting the complexity of and algorithms for the graph traversal edit distance and its variants",
    "doi": "https://doi.org/10.1186/s13015-024-00262-6",
    "publication_date": "2024-04-29",
    "publication_year": 2024,
    "authors": "Yutong Qiu; Yihang Shen; Carl Kingsford",
    "corresponding_authors": "",
    "abstract": "Abstract The graph traversal edit distance (GTED), introduced by Ebrahimpour Boroojeny et al. (2018), is an elegant distance measure defined as the minimum edit distance between strings reconstructed from Eulerian trails in two edge-labeled graphs. GTED can be used to infer evolutionary relationships between species by comparing de Bruijn graphs directly without the computationally costly and error-prone process of genome assembly. Ebrahimpour Boroojeny et al. (2018) propose two ILP formulations for GTED and claim that GTED is polynomially solvable because the linear programming relaxation of one of the ILPs always yields optimal integer solutions. The claim that GTED is polynomially solvable is contradictory to the complexity results of existing string-to-graph matching problems. We resolve this conflict in complexity results by proving that GTED is NP-complete and showing that the ILPs proposed by Ebrahimpour Boroojeny et al. do not solve GTED but instead solve for a lower bound of GTED and are not solvable in polynomial time. In addition, we provide the first two, correct ILP formulations of GTED and evaluate their empirical efficiency. These results provide solid algorithmic foundations for comparing genome graphs and point to the direction of heuristics. The source code to reproduce experimental results is available at https://github.com/Kingsford-Group/gtednewilp/ .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396227784",
    "type": "article"
  },
  {
    "title": "Graph-distance distribution of the Boltzmann ensemble of RNA secondary structures",
    "doi": "https://doi.org/10.1186/1748-7188-9-19",
    "publication_date": "2014-09-11",
    "publication_year": 2014,
    "authors": "Jing Qin; Markus Fricke; Manja Marz; Peter F. Stadler; Rolf Backofen",
    "corresponding_authors": "Rolf Backofen",
    "abstract": "Background: Large RNA molecules are often composed of multiple functional domains whose spatial arrangement strongly influences their function. Pre-mRNA splicing, for instance, relies on the spatial proximity of the splice junctions that can be separated by very long introns. Similar effects appear in the processing of RNA virus genomes. Albeit a crude measure, the distribution of spatial distances in thermodynamic equilibrium harbors useful information on the shape of the molecule that in turn can give insights into the interplay of its functional domains.Result: Spatial distance can be approximated by the graph-distance in RNA secondary structure. We show here that the equilibrium distribution of graph-distances between a fixed pair of nucleotides can be computed in polynomial time by means of dynamic programming. While a naïve implementation would yield recursions with a very high time complexity of O(n 6D 5) for sequence length n and D distinct distance values, it is possible to reduce this to O(n 4) for practical applications in which predominantly small distances are of of interest. Further reductions, however, seem to be difficult. Therefore, we introduced sampling approaches that are much easier to implement. They are also theoretically favorable for several real-life applications, in particular since these primarily concern long-range interactions in very large RNA molecules.Conclusions: The graph-distance distribution can be computed using a dynamic programming approach. Although a crude approximation of reality, our initial results indicate that the graph-distance can be related to the smFRET data. The additional file and the software of our paper are available from http://www.rna.uni-jena.de/RNAgraphdist.html.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2028460948",
    "type": "article"
  },
  {
    "title": "Erratum to: Circular sequence comparison: algorithms and applications",
    "doi": "https://doi.org/10.1186/s13015-016-0084-6",
    "publication_date": "2016-07-27",
    "publication_year": 2016,
    "authors": "Roberto Grossi; Costas S. Iliopoulos; Robert Mercaş; Nadia Pisanti; Solon P. Pissis; Ahmad Retha; Fatima Vayani",
    "corresponding_authors": "",
    "abstract": "[This corrects the article DOI: 10.1186/s13015-016-0076-6.].",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2489185839",
    "type": "erratum"
  },
  {
    "title": "Using Robinson-Foulds supertrees in divide-and-conquer phylogeny estimation",
    "doi": "https://doi.org/10.1186/s13015-021-00189-2",
    "publication_date": "2021-06-28",
    "publication_year": 2021,
    "authors": "Xilin Yu; Thien Le; Sarah Christensen; Erin K. Molloy; Tandy Warnow",
    "corresponding_authors": "Xilin Yu",
    "abstract": "One of the Grand Challenges in Science is the construction of the Tree of Life, an evolutionary tree containing several million species, spanning all life on earth. However, the construction of the Tree of Life is enormously computationally challenging, as all the current most accurate methods are either heuristics for NP-hard optimization problems or Bayesian MCMC methods that sample from tree space. One of the most promising approaches for improving scalability and accuracy for phylogeny estimation uses divide-and-conquer: a set of species is divided into overlapping subsets, trees are constructed on the subsets, and then merged together using a \"supertree method\". Here, we present Exact-RFS-2, the first polynomial-time algorithm to find an optimal supertree of two trees, using the Robinson-Foulds Supertree (RFS) criterion (a major approach in supertree estimation that is related to maximum likelihood supertrees), and we prove that finding the RFS of three input trees is NP-hard. Exact-RFS-2 is available in open source form on Github at https://github.com/yuxilin51/GreedyRFS .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3174683222",
    "type": "article"
  },
  {
    "title": "DeepGRP: engineering a software tool for predicting genomic repetitive elements using Recurrent Neural Networks with attention",
    "doi": "https://doi.org/10.1186/s13015-021-00199-0",
    "publication_date": "2021-08-23",
    "publication_year": 2021,
    "authors": "Fabian Hausmann; Stefan Kurtz",
    "corresponding_authors": "",
    "abstract": "Abstract Background Repetitive elements contribute a large part of eukaryotic genomes. For example, about 40 to 50% of human, mouse and rat genomes are repetitive. So identifying and classifying repeats is an important step in genome annotation. This annotation step is traditionally performed using alignment based methods, either in a de novo approach or by aligning the genome sequence to a species specific set of repetitive sequences. Recently, Li (Bioinformatics 35:4408–4410, 2019) developed a novel software tool to annotate repetitive sequences using a recurrent neural network trained on sample annotations of repetitive elements. Results We have developed the methods of further and engineered a new software tool . This combines the basic concepts of Li (Bioinformatics 35:4408–4410, 2019) with current techniques developed for neural machine translation, the attention mechanism, for the task of nucleotide-level annotation of repetitive elements. An evaluation on the human genome shows a 20% improvement of the Matthews correlation coefficient for the predictions delivered by , when compared to . predicts two additional classes of repeats (compared to ) and is able to transfer repeat annotations, using RepeatMasker-based training data to a different species (mouse). Additionally, we could show that predicts repeats annotated in the Dfam database, but not annotated by RepeatMasker. is highly scalable due to its implementation in the TensorFlow framework. For example, the GPU-accelerated version of is approx. 1.8 times faster than , approx. 8.6 times faster than RepeatMasker and over 100 times faster than HMMER searching for models of the Dfam database. Conclusions By incorporating methods from neural machine translation, achieves a consistent improvement of the quality of the predictions compared to . Improved running times are obtained by employing TensorFlow as implementation framework and the use of GPUs. By incorporating two additional classes of repeats, provides more complete annotations, which were evaluated against three state-of-the-art tools for repeat annotation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3193626225",
    "type": "article"
  },
  {
    "title": "Testing the agreement of trees with internal labels",
    "doi": "https://doi.org/10.1186/s13015-021-00201-9",
    "publication_date": "2021-12-01",
    "publication_year": 2021,
    "authors": "David Fernández‐Baca; Lei Liu",
    "corresponding_authors": "",
    "abstract": "Abstract Background A semi-labeled tree is a tree where all leaves as well as, possibly, some internal nodes are labeled with taxa. Semi-labeled trees encompass ordinary phylogenetic trees and taxonomies. Suppose we are given a collection $${\\mathcal {P}}= \\{{\\mathcal {T}}_1, {\\mathcal {T}}_2, \\ldots , {\\mathcal {T}}_k\\}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math> of semi-labeled trees, called input trees, over partially overlapping sets of taxa. The agreement problem asks whether there exists a tree $${\\mathcal {T}}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>T</mml:mi></mml:math> , called an agreement tree, whose taxon set is the union of the taxon sets of the input trees such that the restriction of $${\\mathcal {T}}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>T</mml:mi></mml:math> to the taxon set of $${\\mathcal {T}}_i$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math> is isomorphic to $${\\mathcal {T}}_i$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math> , for each $$i \\in \\{1, 2, \\ldots , k\\}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math> . The agreement problems is a special case of the supertree problem, the problem of synthesizing a collection of phylogenetic trees with partially overlapping taxon sets into a single supertree that represents the information in the input trees. An obstacle to building large phylogenetic supertrees is the limited amount of taxonomic overlap among the phylogenetic studies from which the input trees are obtained. Incorporating taxonomies into supertree analyses can alleviate this issue. Results We give a $${\\mathcal {O}}(n k (\\sum _{i \\in [k]} d_i + \\log ^2(nk)))$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mo>log</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math> algorithm for the agreement problem, where n is the total number of distinct taxa in $${\\mathcal {P}}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>P</mml:mi></mml:math> , k is the number of trees in $${\\mathcal {P}}$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>P</mml:mi></mml:math> , and $$d_i$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math> is the maximum number of children of a node in $${\\mathcal {T}}_i$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math> . Conclusion Our algorithm can aid in integrating taxonomies into supertree analyses. Our computational experience with the algorithm suggests that its performance in practice is much better than its worst-case bound indicates.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4205688765",
    "type": "article"
  },
  {
    "title": "Review of “Bioinformatics: A Computing Perspective” edited by Shuba Gopal, Anne Haake, Rhys Price Jones and Paul Tymann",
    "doi": "https://doi.org/10.1186/1748-7188-4-9",
    "publication_date": "2009-06-24",
    "publication_year": 2009,
    "authors": "Dae-Won Kim; Hong-Seog Park",
    "corresponding_authors": "",
    "abstract": "Bioinformatics is a vast multidisciplinary field. Bioinformatics: A Computing Perspective does a good job of compiling the relevant topics about the state of the art and challenges facing bioinformatics in the field of biology. This book has several strong points. Although it is consisting of many chapters and subtitles dealing with detailed computing techniques and experimental methodologies, a diversity of attempts have been made to bring together descriptions of bioinformatics, biological basics, detailed experimental methodologies and computing algorithm for understanding biological complexes. The style is very readable, and discusses both the biology and the computation of every topic presented. In particular, this text places emphasis on practical skills involved in analyzing genes and proteins, extracting sequence data, and identifying genes and proteins implicated in disease by providing ample illustrative examples and extensive java codes. Many algorithms are built up in steps, showing how successive insights from both computation and biology can make existing techniques work better. Consequently, this book is inspiring readers to design the breakthrough computational approaches and algorithms that will provide novel and valuable insights into biological phenomena.\r\n\r\nIn summary, this book provides an excellent resource for those initiating a study of biology based on computer science and provides interesting facts to give life to lectures on this topic. Most of the topics discussed show a fascinating snapshot of current progress in bioinformatics using advanced technologies. Moreover, the examples in this book help provide an understanding of the complicated algorithms described.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1520008714",
    "type": "article"
  },
  {
    "title": "アフィメトリクス遺伝子発現データからの特異的発現遺伝子のランキング:再現性、感度と特異性による方法",
    "doi": null,
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "K. Shimizu; Yasuhiro Nakai; Koji Kadota",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3118257967",
    "type": "article"
  },
  {
    "title": "Review of \"Reconstructing Evolution: New mathematical and computational advances\" edited by Olivier Gascuel and Mike Steel",
    "doi": "https://doi.org/10.1186/1748-7188-2-14",
    "publication_date": "2007-11-06",
    "publication_year": 2007,
    "authors": "Andreas Spillner",
    "corresponding_authors": "Andreas Spillner",
    "abstract": "The main theme of the book is mathematical models of evolutionary processes in biology.These models have become increasingly complex in order to cope with the various aspects involved in the study of these processes and involve a wide range of concepts from different areas of mathematics and computer science.This book gives an overview of some of the key contemporary topics in the field.The book is organized in 10 chapters, each written by an expert or a group of experts on the topic addressed in that chapter.At the end of many chapters directions for future research are discussed.Each chapter comes with its own list of references.At the end of the book the reader can find an index that helps to locate quickly the context within which a particular concept appears.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2066400776",
    "type": "article"
  },
  {
    "title": "On the complexity of non-binary tree reconciliation with endosymbiotic gene transfer",
    "doi": "https://doi.org/10.1186/s13015-023-00231-5",
    "publication_date": "2023-07-30",
    "publication_year": 2023,
    "authors": "Mathieu Gascon; Nadia El-Mabrouk",
    "corresponding_authors": "",
    "abstract": "Abstract Reconciling a non-binary gene tree with a binary species tree can be done efficiently in the absence of horizontal gene transfers, but becomes NP-hard in the presence of gene transfers. Here, we focus on the special case of endosymbiotic gene transfers (EGT), i.e. transfers between the mitochondrial and nuclear genome of the same species. More precisely, given a multifurcated (non-binary) gene tree with leaves labeled 0 or 1 depending on whether the corresponding genes belong to the mitochondrial or nuclear genome of the corresponding species, we investigate the problem of inferring a most parsimonious Duplication, Loss and EGT (DLE) Reconciliation of any binary refinement of the tree. We present a general two-steps method: ignoring the 0–1 labeling of leaves, output a binary resolution minimizing the Duplication and Loss (DL) Reconciliation and then, for such resolution, assign a known number of 0s and 1s to the leaves in a way minimizing EGT events. While the first step corresponds to the well studied non-binary DL-Reconciliation problem, the complexity of the label assignment problem corresponding to the second step is unknown. We show that this problem is NP-complete, even when the tree is restricted to a single polytomy, and even if transfers can occur in only one direction. We present a general algorithm solving each polytomy separately, which is shown optimal for a unitary cost of operation, and a polynomial-time algorithm for solving a polytomy in the special case where genes are specific to a single genome (mitochondrial or nuclear) in all but one species. This work represents the first algorithmic study for reconciliation with endosymbiotic gene transfers in the case of a multifurcated gene tree.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4385397331",
    "type": "article"
  },
  {
    "title": "phyBWT2: phylogeny reconstruction via eBWT positional clustering",
    "doi": "https://doi.org/10.1186/s13015-023-00232-4",
    "publication_date": "2023-08-03",
    "publication_year": 2023,
    "authors": "Veronica Guerrini; Alessio Conte; Roberto Grossi; Gianni Liti; Giovanna Rosone; Lorenzo Tattini",
    "corresponding_authors": "Veronica Guerrini; Alessio Conte; Roberto Grossi; Giovanna Rosone",
    "abstract": "Abstract Background Molecular phylogenetics studies the evolutionary relationships among the individuals of a population through their biological sequences. It may provide insights about the origin and the evolution of viral diseases, or highlight complex evolutionary trajectories. A key task is inferring phylogenetic trees from any type of sequencing data, including raw short reads. Yet, several tools require pre-processed input data e.g. from complex computational pipelines based on de novo assembly or from mappings against a reference genome. As sequencing technologies keep becoming cheaper, this puts increasing pressure on designing methods that perform analysis directly on their outputs. From this viewpoint, there is a growing interest in alignment-, assembly-, and reference-free methods that could work on several data including raw reads data. Results We present phyBWT2, a newly improved version of phyBWT (Guerrini et al. in 22nd International Workshop on Algorithms in Bioinformatics (WABI) 242:23–12319, 2022). Both of them directly reconstruct phylogenetic trees bypassing both the alignment against a reference genome and de novo assembly. They exploit the combinatorial properties of the extended Burrows-Wheeler Transform (eBWT) and the corresponding eBWT positional clustering framework to detect relevant blocks of the longest shared substrings of varying length (unlike the k -mer-based approaches that need to fix the length k a priori). As a result, they provide novel alignment-, assembly-, and reference-free methods that build partition trees without relying on the pairwise comparison of sequences, thus avoiding to use a distance matrix to infer phylogeny. In addition, phyBWT2 outperforms phyBWT in terms of running time, as the former reconstructs phylogenetic trees step-by-step by considering multiple partitions, instead of just one partition at a time, as previously done by the latter. Conclusions Based on the results of the experiments on sequencing data, we conclude that our method can produce trees of quality comparable to the benchmark phylogeny by handling datasets of different types (short reads, contigs, or entire genomes). Overall, the experiments confirm the effectiveness of phyBWT2 that improves the performance of its previous version phyBWT, while preserving the accuracy of the results.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4385549248",
    "type": "article"
  },
  {
    "title": "Relative timing information and orthology in evolutionary scenarios",
    "doi": "https://doi.org/10.1186/s13015-023-00240-4",
    "publication_date": "2023-11-08",
    "publication_year": 2023,
    "authors": "David Schaller; Tom Hartmann; Manuel Lafond; Peter F. Stadler; Nicolas Wieseke; Marc Hellmuth",
    "corresponding_authors": "",
    "abstract": "Abstract Background Evolutionary scenarios describing the evolution of a family of genes within a collection of species comprise the mapping of the vertices of a gene tree T to vertices and edges of a species tree S . The relative timing of the last common ancestors of two extant genes (leaves of T ) and the last common ancestors of the two species (leaves of S ) in which they reside is indicative of horizontal gene transfers (HGT) and ancient duplications. Orthologous gene pairs, on the other hand, require that their last common ancestors coincides with a corresponding speciation event. The relative timing information of gene and species divergences is captured by three colored graphs that have the extant genes as vertices and the species in which the genes are found as vertex colors: the equal-divergence-time (EDT) graph, the later-divergence-time (LDT) graph and the prior-divergence-time (PDT) graph, which together form an edge partition of the complete graph. Results Here we give a complete characterization in terms of informative and forbidden triples that can be read off the three graphs and provide a polynomial time algorithm for constructing an evolutionary scenario that explains the graphs, provided such a scenario exists. While both LDT and PDT graphs are cographs, this is not true for the EDT graph in general. We show that every EDT graph is perfect. While the information about LDT and PDT graphs is necessary to recognize EDT graphs in polynomial-time for general scenarios, this extra information can be dropped in the HGT-free case. However, recognition of EDT graphs without knowledge of putative LDT and PDT graphs is NP-complete for general scenarios. In contrast, PDT graphs can be recognized in polynomial-time. We finally connect the EDT graph to the alternative definitions of orthology that have been proposed for scenarios with horizontal gene transfer. With one exception, the corresponding graphs are shown to be colored cographs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388490419",
    "type": "article"
  },
  {
    "title": "New algorithms for structure informed genome rearrangement",
    "doi": "https://doi.org/10.1186/s13015-023-00239-x",
    "publication_date": "2023-12-01",
    "publication_year": 2023,
    "authors": "Eden Ozeri; Meirav Zehavi; Michal Ziv-Ukelson",
    "corresponding_authors": "Eden Ozeri",
    "abstract": "We define two new computational problems in the domain of perfect genome rearrangements, and propose three algorithms to solve them. The rearrangement scenarios modeled by the problems consider Reversal and Block Interchange operations, and a PQ-tree is utilized to guide the allowed operations and to compute their weights. In the first problem, [Formula: see text] ([Formula: see text]), we define the basic structure-informed rearrangement measure. Here, we assume that the gene order members of the gene cluster from which the PQ-tree is constructed are permutations. The PQ-tree representing the gene cluster is ordered such that the series of gene IDs spelled by its leaves is equivalent to that of the reference gene order. Then, a structure-informed genome rearrangement distance is computed between the ordered PQ-tree and the target gene order. The second problem, [Formula: see text] ([Formula: see text]), generalizes [Formula: see text], where the gene order members are not necessarily permutations and the structure informed rearrangement measure is extended to also consider up to [Formula: see text] and [Formula: see text] gene insertion and deletion operations, respectively, when modelling the PQ-tree informed divergence process from the reference gene order to the target gene order. The first algorithm solves [Formula: see text] in [Formula: see text] time and [Formula: see text] space, where [Formula: see text] is the maximum number of children of a node, n is the length of the string and the number of leaves in the tree, and [Formula: see text] and [Formula: see text] are the number of P-nodes and Q-nodes in the tree, respectively. If one of the penalties of [Formula: see text] is 0, then the algorithm runs in [Formula: see text] time and [Formula: see text] space. The second algorithm solves [Formula: see text] in [Formula: see text] time and [Formula: see text] space, where [Formula: see text] is the maximum number of children of a node, n is the length of the string, m is the number of leaves in the tree, [Formula: see text] and [Formula: see text] are the number of P-nodes and Q-nodes in the tree, respectively, and allowing up to [Formula: see text] deletions from the tree and up to [Formula: see text] deletions from the string. The third algorithm is intended to reduce the space complexity of the second algorithm. It solves a variant of the problem (where one of the penalties of [Formula: see text] is 0) in [Formula: see text] time and [Formula: see text] space. The algorithm is implemented as a software tool, denoted MEM-Rearrange, and applied to the comparative and evolutionary analysis of 59 chromosomal gene clusters extracted from a dataset of 1487 prokaryotic genomes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389207770",
    "type": "article"
  },
  {
    "title": "Quartets enable statistically consistent estimation of cell lineage trees under an unbiased error and missingness model",
    "doi": "https://doi.org/10.1186/s13015-023-00248-w",
    "publication_date": "2023-12-01",
    "publication_year": 2023,
    "authors": "Yunheng Han; Erin K. Molloy",
    "corresponding_authors": "",
    "abstract": "Cancer progression and treatment can be informed by reconstructing its evolutionary history from tumor cells. Although many methods exist to estimate evolutionary trees (called phylogenies) from molecular sequences, traditional approaches assume the input data are error-free and the output tree is fully resolved. These assumptions are challenged in tumor phylogenetics because single-cell sequencing produces sparse, error-ridden data and because tumors evolve clonally. Here, we study the theoretical utility of methods based on quartets (four-leaf, unrooted phylogenetic trees) in light of these barriers. We consider a popular tumor phylogenetics model, in which mutations arise on a (highly unresolved) tree and then (unbiased) errors and missing values are introduced. Quartets are then implied by mutations present in two cells and absent from two cells. Our main result is that the most probable quartet identifies the unrooted model tree on four cells. This motivates seeking a tree such that the number of quartets shared between it and the input mutations is maximized. We prove an optimal solution to this problem is a consistent estimator of the unrooted cell lineage tree; this guarantee includes the case where the model tree is highly unresolved, with error defined as the number of false negative branches. Lastly, we outline how quartet-based methods might be employed when there are copy number aberrations and other challenges specific to tumor phylogenetics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389225880",
    "type": "article"
  },
  {
    "title": "Correction: Constructing founder sets under allelic and non-allelic homologous recombination",
    "doi": "https://doi.org/10.1186/s13015-023-00244-0",
    "publication_date": "2023-12-06",
    "publication_year": 2023,
    "authors": "Konstantinn Bonnet; Tobias Marschall; Daniel Doerr",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389391379",
    "type": "erratum"
  },
  {
    "title": "A linear-time algorithm that avoids inverses and computes Jackknife (leave-one-out) products like convolutions or other operators in commutative semigroups",
    "doi": "https://doi.org/10.1186/s13015-020-00178-x",
    "publication_date": "2020-09-19",
    "publication_year": 2020,
    "authors": "John L. Spouge; Joseph M. Ziegelbauer; Mileidy W. Gonzalez",
    "corresponding_authors": "John L. Spouge",
    "abstract": "Data about herpesvirus microRNA motifs on human circular RNAs suggested the following statistical question. Consider independent random counts, not necessarily identically distributed. Conditioned on the sum, decide whether one of the counts is unusually large. Exact computation of the p-value leads to a specific algorithmic problem. Given n elements g0,g1,…,gn-1 in a set G with the closure and associative properties and a commutative product without inverses, compute the jackknife (leave-one-out) products g¯j=g0g1⋯gj-1gj+1⋯gn-1 ( 0≤j<n ).This article gives a linear-time Jackknife Product algorithm. Its upward phase constructs a standard segment tree for computing segment products like gi,j=gigi+1⋯gj-1 ; its novel downward phase mirrors the upward phase while exploiting the symmetry of gj and its complement g¯j . The algorithm requires storage for 2n elements of G and only about 3n products. In contrast, the standard segment tree algorithms require about n products for construction and log2n products for calculating each g¯j , i.e., about nlog2n products in total; and a naïve quadratic algorithm using n-2 element-by-element products to compute each g¯j requires nn-2 products.In the herpesvirus application, the Jackknife Product algorithm required 15 min; standard segment tree algorithms would have taken an estimated 3 h; and the quadratic algorithm, an estimated 1 month. The Jackknife Product algorithm has many possible uses in bioinformatics and statistics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3087688060",
    "type": "article"
  },
  {
    "title": "Perplexity: evaluating transcript abundance estimation in the absence of ground truth",
    "doi": "https://doi.org/10.1186/s13015-022-00214-y",
    "publication_date": "2022-03-25",
    "publication_year": 2022,
    "authors": "Jason Fan; Spencer Skylar Chan; Rob Patro",
    "corresponding_authors": "Jason Fan",
    "abstract": "There has been rapid development of probabilistic models and inference methods for transcript abundance estimation from RNA-seq data. These models aim to accurately estimate transcript-level abundances, to account for different biases in the measurement process, and even to assess uncertainty in resulting estimates that can be propagated to subsequent analyses. The assumed accuracy of the estimates inferred by such methods underpin gene expression based analysis routinely carried out in the lab. Although hyperparameter selection is known to affect the distributions of inferred abundances (e.g. producing smooth versus sparse estimates), strategies for performing model selection in experimental data have been addressed informally at best.We derive perplexity for evaluating abundance estimates on fragment sets directly. We adapt perplexity from the analogous metric used to evaluate language and topic models and extend the metric to carefully account for corner cases unique to RNA-seq. In experimental data, estimates with the best perplexity also best correlate with qPCR measurements. In simulated data, perplexity is well behaved and concordant with genome-wide measurements against ground truth and differential expression analysis. Furthermore, we demonstrate theoretically and experimentally that perplexity can be computed for arbitrary transcript abundance estimation models.Alongside the derivation and implementation of perplexity for transcript abundance estimation, our study is the first to make possible model selection for transcript abundance estimation on experimental data in the absence of ground truth.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4225986528",
    "type": "article"
  },
  {
    "title": "Efficiently sparse listing of classes of optimal cophylogeny reconciliations",
    "doi": "https://doi.org/10.1186/s13015-022-00206-y",
    "publication_date": "2022-02-15",
    "publication_year": 2022,
    "authors": "Yishu Wang; Arnaud Mary; Marie-France Sagot; Blerina Sinaimeri",
    "corresponding_authors": "",
    "abstract": "Cophylogeny reconciliation is a powerful method for analyzing host-parasite (or host-symbiont) co-evolution. It models co-evolution as an optimization problem where the set of all optimal solutions may represent different biological scenarios which thus need to be analyzed separately. Despite the significant research done in the area, few approaches have addressed the problem of helping the biologist deal with the often huge space of optimal solutions.In this paper, we propose a new approach to tackle this problem. We introduce three different criteria under which two solutions may be considered biologically equivalent, and then we propose polynomial-delay algorithms that enumerate only one representative per equivalence class (without listing all the solutions).Our results are of both theoretical and practical importance. Indeed, as shown by the experiments, we are able to significantly reduce the space of optimal solutions while still maintaining important biological information about the whole space.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4226156846",
    "type": "article"
  },
  {
    "title": "On a greedy approach for genome scaffolding",
    "doi": "https://doi.org/10.1186/s13015-022-00223-x",
    "publication_date": "2022-10-29",
    "publication_year": 2022,
    "authors": "Tom Davot; Annie Château; Rohan Fossé; Rodolphe Giroudeau; Mathias Weller",
    "corresponding_authors": "Tom Davot",
    "abstract": "Scaffolding is a bioinformatics problem aimed at completing the contig assembly process by determining the relative position and orientation of these contigs. It can be seen as a paths and cycles cover problem of a particular graph called the \"scaffold graph\".We provide some NP-hardness and inapproximability results on this problem. We also adapt a greedy approximation algorithm on complete graphs so that it works on a special class aiming to be close to real instances. The described algorithm is the first polynomial-time approximation algorithm designed for this problem on non-complete graphs.Tests on a set of simulated instances show that our algorithm provides better results than the version on complete graphs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4307812615",
    "type": "article"
  }
]