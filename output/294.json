[
  {
    "title": "Evaluating Effect Size in Psychological Research: Sense and Nonsense",
    "doi": "https://doi.org/10.1177/2515245919847202",
    "publication_date": "2019-05-08",
    "publication_year": 2019,
    "authors": "David C. Funder; Daniel J. Ozer",
    "corresponding_authors": "David C. Funder",
    "abstract": "Effect sizes are underappreciated and often misinterpreted—the most common mistakes being to describe them in ways that are uninformative (e.g., using arbitrary standards) or misleading (e.g., squaring effect-size rs). We propose that effect sizes can be usefully evaluated by comparing them with well-understood benchmarks or by considering them in terms of concrete consequences. In that light, we conclude that when reliably estimated (a critical consideration), an effect-size r of .05 indicates an effect that is very small for the explanation of single events but potentially consequential in the not-very-long run, an effect-size r of .10 indicates an effect that is still small at the level of single events but potentially more ultimately consequential, an effect-size r of .20 indicates a medium effect that is of some explanatory and practical use even in the short run and therefore even more important, and an effect-size r of .30 indicates a large effect that is potentially powerful in both the short and the long run. A very large effect size ( r = .40 or greater) in the context of psychological research is likely to be a gross overestimate that will rarely be found in a large sample or in a replication. Our goal is to help advance the treatment of effect sizes so that rather than being numbers that are ignored, reported without interpretation, or interpreted superficially or incorrectly, they become aspects of research reports that can better inform the application and theoretical development of psychological research.",
    "cited_by_count": 2697,
    "openalex_id": "https://openalex.org/W2944339144",
    "type": "article"
  },
  {
    "title": "Equivalence Testing for Psychological Research: A Tutorial",
    "doi": "https://doi.org/10.1177/2515245918770963",
    "publication_date": "2018-06-01",
    "publication_year": 2018,
    "authors": "Daniël Lakens; Anne M. Scheel; Peder Mortvedt Isager",
    "corresponding_authors": "Daniël Lakens",
    "abstract": "Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.",
    "cited_by_count": 1279,
    "openalex_id": "https://openalex.org/W2788930055",
    "type": "article"
  },
  {
    "title": "Many Labs 2: Investigating Variation in Replicability Across Samples and Settings",
    "doi": "https://doi.org/10.1177/2515245918810225",
    "publication_date": "2018-12-01",
    "publication_year": 2018,
    "authors": "Richard Klein; Michelangelo Vianello; Fred Hasselman; Byron G. Adams; Reginald B. Adams; Sinan Alper; Mark Aveyard; Jordan Axt; Mayowa T. Babalola; Štěpán Bahník; Rishtee Batra; Mihály Berkics; Michael J. Bernstein; Daniel R. Berry; Olga Białobrzeska; Evans Dami Binan; Konrad Bocian; Mark J. Brandt; Robert Busching; Anna Cabak Rédei; Huajian Cai; Fanny Cambier; Katarzyna Cantarero; Cheryl L. Carmichael; Francisco Cerić; Jesse Chandler; Jen‐Ho Chang; Armand Chatard; Eva E. Chen; Winnee Cheong; David C. Cicero; Sharon Coen; Jennifer A. Coleman; Brian Collisson; Morgan Conway; Katherine S. Corker; Paul Curran; Fiery Cushman; Zubairu Kwambo Dagona; Ilker Dalgar; Anna Dalla Rosa; William E. Davis; Maaike de Bruijn; Leander De Schutter; Thierry Devos; Marieke de Vries; Canay Doğulu; Nerisa Dozo; Kristin Nicole Dukes; Yarrow Dunham; Kevin Durrheim; Charles R. Ebersole; John E. Edlund; Anja Eller; Alexander Scott English; Carolyn Finck; Natalia Frankowska; Miguel-Ángel Freyre; Michael Friedman; Elisa Maria Galliani; Joshua C. Gandi; Tanuka Ghoshal; Steffen R. Giessner; Tripat Gill; Timo Gnambs; Ángel Gómez; Roberto González; Jesse Graham; Jon Grahe; Ivan Grahek; Eva G. T. Green; Kakul Hai; Matthew Haigh; Elizabeth L. Haines; Michael P. Hall; Marie E. Heffernan; Joshua A. Hicks; Petr Houdek; Jeffrey R. Huntsinger; Ho Phi Huynh; Hans IJzerman; Yoel Inbar; Åse Innes-Ker; William Jiménez‐Leal; Melissa-Sue John; Jennifer A. Joy-Gaba; Roza Gizem Kamiloglu; Heather Barry Kappes; Serdar Karabatı; Haruna Karick; Victor N. Keller; Anna Kende; Nicolas Kervyn; Goran Knežević; Carrie Kovacs; Lacy E. Krueger; German Kurapov; Jamie Kurtz; Daniël Lakens; Ljiljana B. Lazarević",
    "corresponding_authors": "Richard Klein",
    "abstract": "We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance ( p &lt; .05), we found that 15 (54%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion ( p &lt; .0001), 14 (50%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25%) of the replications yielded effect sizes larger than the original ones, and 21 (75%) yielded effect sizes smaller than the original ones. The median comparable Cohen’s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (&lt; 0.20) in 16 of the replications (57%), and 9 effects (32%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.",
    "cited_by_count": 954,
    "openalex_id": "https://openalex.org/W2776961836",
    "type": "article"
  },
  {
    "title": "Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data",
    "doi": "https://doi.org/10.1177/2515245917745629",
    "publication_date": "2018-01-29",
    "publication_year": 2018,
    "authors": "Julia M. Rohrer",
    "corresponding_authors": "Julia M. Rohrer",
    "abstract": "Correlation does not imply causation; but often, observational data are the only option, even though the research question at hand involves causality. This article discusses causal inference based on observational data, introducing readers to graphical causal models that can provide a powerful tool for thinking more clearly about the interrelations between variables. Topics covered include the rationale behind the statistical control of third variables, common procedures for statistical control, and what can go wrong during their implementation. Certain types of third variables—colliders and mediators—should not be controlled for because that can actually move the estimate of an association away from the value of the causal effect of interest. More subtle variations of such harmful control include using unrepresentative samples, which can undermine the validity of causal conclusions, and statistically controlling for mediators. Drawing valid causal inferences on the basis of observational data is not a mechanistic procedure but rather always depends on assumptions that require domain knowledge and that can be more or less plausible. However, this caveat holds not only for research based on observational data, but for all empirical research endeavors.",
    "cited_by_count": 873,
    "openalex_id": "https://openalex.org/W2776946813",
    "type": "article"
  },
  {
    "title": "Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results",
    "doi": "https://doi.org/10.1177/2515245917747646",
    "publication_date": "2018-08-23",
    "publication_year": 2018,
    "authors": "Raphael Silberzahn; Eric Luis Uhlmann; Daniel P. Martin; Pasquale Anselmi; Frederik Aust; Eli Awtrey; Štěpán Bahník; Feng Bai; Colin Bannard; Evelina Bonnier; Rickard Carlsson; Felix Cheung; Garret Christensen; Russ Clay; Maureen A. Craig; Anna Dalla Rosa; Lammertjan Dam; Mathew H. Evans; Ismael Flores Cervantes; Nathan M. Fong; Monica Gamez-Djokic; Andreas Glenz; Shauna Gordon-McKeon; Timothy Heaton; Karin Hederos; Moritz Heene; Alicia Hofelich Mohr; Fabia Högden; Kent Ngan‐Cheung Hui; Magnus Johannesson; Jonathan Kalodimos; Erikson Kaszubowski; Deanna M. Kennedy; Ryan F. Lei; Thomas Lindsay; Silvia Liverani; Christopher R. Madan; Daniel C. Molden; Eric Molleman; Richard D. Morey; Laetitia B. Mulder; B. R. Nijstad; Nolan Pope; Bryson Pope; Jason M. Prenoveau; Floor Rink; Egidio Robusto; Hadiya Roderique; Anna Sandberg; Elmar Schlüter; Felix D. Schönbrodt; Martin F. Sherman; S. Amy Sommer; Kristin Lee Sotak; Seth M. Spain; Christoph Spörlein; Tom Stafford; Luca Stefanutti; Susanne Täuber; Johannes Ullrich; Michelangelo Vianello; Eric‐Jan Wagenmakers; Maciej Witkowiak; Sangsuk Yoon; Brian A. Nosek",
    "corresponding_authors": "Raphael Silberzahn",
    "abstract": "Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 ( Mdn = 1.31) in odds-ratio units. Twenty teams (69%) found a statistically significant positive effect, and 9 teams (31%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.",
    "cited_by_count": 787,
    "openalex_id": "https://openalex.org/W2762364541",
    "type": "article"
  },
  {
    "title": "Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them",
    "doi": "https://doi.org/10.1177/2515245920952393",
    "publication_date": "2020-12-01",
    "publication_year": 2020,
    "authors": "Jessica Kay Flake; Eiko I. Fried",
    "corresponding_authors": "Jessica Kay Flake",
    "abstract": "In this article, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study’s inferences, and are necessary for meaningful replication studies.",
    "cited_by_count": 650,
    "openalex_id": "https://openalex.org/W3108041100",
    "type": "article"
  },
  {
    "title": "Rejecting or Accepting Parameter Values in Bayesian Estimation",
    "doi": "https://doi.org/10.1177/2515245918771304",
    "publication_date": "2018-05-08",
    "publication_year": 2018,
    "authors": "John K. Kruschke",
    "corresponding_authors": "John K. Kruschke",
    "abstract": "This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors.",
    "cited_by_count": 613,
    "openalex_id": "https://openalex.org/W2801776818",
    "type": "article"
  },
  {
    "title": "Ordinal Regression Models in Psychology: A Tutorial",
    "doi": "https://doi.org/10.1177/2515245918823199",
    "publication_date": "2019-02-25",
    "publication_year": 2019,
    "authors": "Paul‐Christian Bürkner; Matti Vuorre",
    "corresponding_authors": "Paul‐Christian Bürkner",
    "abstract": "Ordinal variables, although extremely common in psychology, are almost exclusively analyzed with statistical models that falsely assume them to be metric. This practice can lead to distorted effect-size estimates, inflated error rates, and other problems. We argue for the application of ordinal models that make appropriate assumptions about the variables under study. In this Tutorial, we first explain the three major classes of ordinal models: the cumulative, sequential, and adjacent-category models. We then show how to fit ordinal models in a fully Bayesian framework with the R package brms, using data sets on opinions about stem-cell research and time courses of marriage. The appendices provide detailed mathematical derivations of the models and a discussion of censored ordinal models. Compared with metric models, ordinal models provide better theoretical interpretation and numerical inference from ordinal data, and we recommend their widespread adoption in psychology.",
    "cited_by_count": 582,
    "openalex_id": "https://openalex.org/W2888034815",
    "type": "article"
  },
  {
    "title": "Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods",
    "doi": "https://doi.org/10.1177/2515245919847196",
    "publication_date": "2019-06-01",
    "publication_year": 2019,
    "authors": "Evan C. Carter; Felix D. Schönbrodt; Will M. Gervais; Joseph Hilgard",
    "corresponding_authors": "Evan C. Carter",
    "abstract": "Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/ .",
    "cited_by_count": 495,
    "openalex_id": "https://openalex.org/W2781957993",
    "type": "article"
  },
  {
    "title": "Your Coefficient Alpha Is Probably Wrong, but Which Coefficient Omega Is Right? A Tutorial on Using R to Obtain Better Reliability Estimates",
    "doi": "https://doi.org/10.1177/2515245920951747",
    "publication_date": "2020-11-06",
    "publication_year": 2020,
    "authors": "David B. Flora",
    "corresponding_authors": "David B. Flora",
    "abstract": "Measurement quality has recently been highlighted as an important concern for advancing a cumulative psychological science. An implication is that researchers should move beyond mechanistically reporting coefficient alpha toward more carefully assessing the internal structure and reliability of multi-item scales. Yet a researcher may be discouraged upon discovering that a prominent alternative to alpha, namely, coefficient omega, can be calculated in a variety of ways. In this Tutorial, I alleviate this potential confusion by describing alternative forms of omega and providing guidelines for choosing an appropriate omega estimate pertaining to the measurement of a target construct represented with a confirmatory factor analysis model. Several applied examples demonstrate how to compute different forms of omega in R.",
    "cited_by_count": 437,
    "openalex_id": "https://openalex.org/W3095583164",
    "type": "article"
  },
  {
    "title": "An Introduction to Linear Mixed-Effects Modeling in R",
    "doi": "https://doi.org/10.1177/2515245920960351",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Violet A. Brown",
    "corresponding_authors": "Violet A. Brown",
    "abstract": "This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers. This Tutorial includes snippets of R code throughout; the data and R script used to build the models described in the text are available via OSF at https://osf.io/v6qag/ , so readers can follow along if they wish. The goal of this practical introduction is to provide researchers with the tools they need to begin implementing mixed-effects models in their own research.",
    "cited_by_count": 421,
    "openalex_id": "https://openalex.org/W3044002230",
    "type": "article"
  },
  {
    "title": "Power Analysis for Parameter Estimation in Structural Equation Modeling: A Discussion and Tutorial",
    "doi": "https://doi.org/10.1177/2515245920918253",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Yilin Andre Wang; Mijke Rhemtulla",
    "corresponding_authors": "Yilin Andre Wang",
    "abstract": "Despite the widespread and rising popularity of structural equation modeling (SEM) in psychology, there is still much confusion surrounding how to choose an appropriate sample size for SEM. Currently available guidance primarily consists of sample-size rules of thumb that are not backed up by research and power analyses for detecting model misspecification. Missing from most current practices is power analysis for detecting a target effect (e.g., a regression coefficient between latent variables). In this article, we (a) distinguish power to detect model misspecification from power to detect a target effect, (b) report the results of a simulation study on power to detect a target regression coefficient in a three-predictor latent regression model, and (c) introduce a user-friendly Shiny app, pwrSEM, for conducting power analysis for detecting target effects in structural equation models.",
    "cited_by_count": 420,
    "openalex_id": "https://openalex.org/W3027373163",
    "type": "article"
  },
  {
    "title": "Psychological Science Needs a Standard Practice of Reporting the Reliability of Cognitive-Behavioral Measurements",
    "doi": "https://doi.org/10.1177/2515245919879695",
    "publication_date": "2019-11-06",
    "publication_year": 2019,
    "authors": "Sam Parsons; Anne‐Wil Kruijt; Elaine Fox",
    "corresponding_authors": "Sam Parsons",
    "abstract": "Psychological science relies on behavioral measures to assess cognitive processing; however, the field has not yet developed a tradition of routinely examining the reliability of these behavioral measures. Reliable measures are essential to draw robust inferences from statistical analyses, and subpar reliability has severe implications for measures’ validity and interpretation. Without examining and reporting the reliability of measurements used in an analysis, it is nearly impossible to ascertain whether results are robust or have arisen largely from measurement error. In this article, we propose that researchers adopt a standard practice of estimating and reporting the reliability of behavioral assessments of cognitive processing. We illustrate the need for this practice using an example from experimental psychopathology, the dot-probe task, although we argue that reporting reliability is relevant across fields (e.g., social cognition and cognitive psychology). We explore several implications of low measurement reliability and the detrimental impact that failure to assess measurement reliability has on interpretability and comparison of results and therefore research quality. We argue that researchers in the field of cognition need to report measurement reliability as routine practice so that more reliable assessment tools can be developed. To provide some guidance on estimating and reporting reliability, we describe the use of bootstrapped split-half estimation and intraclass correlation coefficients to estimate internal consistency and test-retest reliability, respectively. For future researchers to build upon current results, it is imperative that all researchers provide psychometric information sufficient for estimating the accuracy of inferences and informing further development of cognitive-behavioral assessments.",
    "cited_by_count": 417,
    "openalex_id": "https://openalex.org/W2955312101",
    "type": "article"
  },
  {
    "title": "Simulation-Based Power Analysis for Factorial Analysis of Variance Designs",
    "doi": "https://doi.org/10.1177/2515245920951503",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Daniël Lakens; Aaron R. Caldwell",
    "corresponding_authors": "Daniël Lakens",
    "abstract": "Researchers often rely on analysis of variance (ANOVA) when they report results of experiments. To ensure that a study is adequately powered to yield informative results with an ANOVA, researchers can perform an a priori power analysis. However, power analysis for factorial ANOVA designs is often a challenge. Current software solutions do not allow power analyses for complex designs with several within-participants factors. Moreover, power analyses often need [Formula: see text] or Cohen’s f as input, but these effect sizes are not intuitive and do not generalize to different experimental designs. We have created the R package Superpower and online Shiny apps to enable researchers without extensive programming experience to perform simulation-based power analysis for ANOVA designs of up to three within- or between-participants factors. Predicted effects are entered by specifying means, standard deviations, and, for within-participants factors, the correlations. The simulation provides the statistical power for all ANOVA main effects, interactions, and individual comparisons. The software can plot power across a range of sample sizes, can control for multiple comparisons, and can compute power when the homogeneity or sphericity assumption is violated. This Tutorial demonstrates how to perform a priori power analysis to design informative studies for main effects, interactions, and individual comparisons and highlights important factors that determine the statistical power for factorial ANOVA designs.",
    "cited_by_count": 416,
    "openalex_id": "https://openalex.org/W3138585281",
    "type": "article"
  },
  {
    "title": "The Psychological Science Accelerator: Advancing Psychology Through a Distributed Collaborative Network",
    "doi": "https://doi.org/10.1177/2515245918797607",
    "publication_date": "2018-10-01",
    "publication_year": 2018,
    "authors": "Hannah Moshontz; Lorne Campbell; Charles R. Ebersole; Hans IJzerman; Heather L. Urry; Patrick S. Forscher; Jon Grahe; Randy J. McCarthy; Erica D. Musser; Jan Antfolk; Christopher M. Castille; Thomas Rhys Evans; Susann Fiedler; Jessica Kay Flake; Diego A. Forero; Steve M. J. Janssen; Justin Robert Keene; John Protzko; Balázs Aczél; Sara Álvarez Solas; Daniel Ansari; Dana Awlia; Ernest Baskin; Carlota Batres; Martha Lucia Borras-Guevara; Cameron Brick; Priyanka Chandel; Armand Chatard; William J. Chopik; David Clarance; Nicholas A. Coles; Katherine S. Corker; Barnaby Dixson; Vilius Dranseika; Yarrow Dunham; Nicholas W. Fox; Gwendolyn Gardiner; S. Mason Garrison; Tripat Gill; Amanda Hahn; Bastian Jaeger; Pavol Kačmár; Gwenaël Kaminski; Philipp Kanske; Zoltán Kekecs; Melissa Kline; Monica A. Koehn; Pratibha Kujur; Carmel Levitan; Jeremy K. Miller; Ceylan Okan; Jerome Olsen; Óscar Oviedo-Trespalacios; Asil Ali Özdoğru; Babita Pande; Arti Parganiha; Noorshama Parveen; Gerit Pfuhl; Sraddha Pradhan; Ivan Ropovik; Nicholas O. Rule; Blair Saunders; Vidar Schei; Kathleen Schmidt; Margaret Messiah Singh; Miroslav Sirota; Crystal N. Steltenpohl; Stefan Stieger; Daniel Storage; Gavin Brent Sullivan; Anna Szabelska; Christian K. Tamnes; Miguel A. Vadillo; Jaroslava Varella Valentová; Wolf Vanpaemel; Marco Antônio Corrêa Varella; Evie Vergauwe; Mark Verschoor; Michelangelo Vianello; Martin Voracek; Glenn Patrick Williams; John Paul Wilson; Janis Zickfeld; Jack Arnal; Burak Aydın; Sau-Chin Chen; Lisa M. DeBruine; Ana María Fernández; Kai T. Horstmann; Peder Mortvedt Isager; Benedict C. Jones; Aycan Kapucu; Hause Lin; Michael C. Mensink; Gorka Navarrete; Miguel Alejandro A. Silan; Christopher R. Chartier",
    "corresponding_authors": "Christopher R. Chartier",
    "abstract": "Concerns about the veracity of psychological research have been growing. Many findings in psychological science are based on studies with insufficient statistical power and nonrepresentative samples, or may otherwise be limited to specific, ungeneralizable settings or populations. Crowdsourced research, a type of large-scale collaboration in which one or more research projects are conducted across multiple lab sites, offers a pragmatic solution to these and other current methodological challenges. The Psychological Science Accelerator (PSA) is a distributed network of laboratories designed to enable and support crowdsourced research projects. These projects can focus on novel research questions or replicate prior research in large, diverse samples. The PSA’s mission is to accelerate the accumulation of reliable and generalizable evidence in psychological science. Here, we describe the background, structure, principles, procedures, benefits, and challenges of the PSA. In contrast to other crowdsourced research networks, the PSA is ongoing (as opposed to time limited), efficient (in that structures and principles are reused for different projects), decentralized, diverse (in both subjects and researchers), and inclusive (of proposals, contributions, and other relevant input from anyone inside or outside the network). The PSA and other approaches to crowdsourced psychological science will advance understanding of mental processes and behaviors by enabling rigorous research and systematic examination of its generalizability.",
    "cited_by_count": 335,
    "openalex_id": "https://openalex.org/W2810320295",
    "type": "article"
  },
  {
    "title": "An Excess of Positive Results: Comparing the Standard Psychology Literature With Registered Reports",
    "doi": "https://doi.org/10.1177/25152459211007467",
    "publication_date": "2021-04-01",
    "publication_year": 2021,
    "authors": "Anne M. Scheel; Mitchell Schijen; Daniël Lakens",
    "corresponding_authors": "",
    "abstract": "Selectively publishing results that support the tested hypotheses (“positive” results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs ( N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature ( N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96% positive results in standard reports but only 44% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.",
    "cited_by_count": 313,
    "openalex_id": "https://openalex.org/W3082857103",
    "type": "article"
  },
  {
    "title": "Quantifying Sources of Variability in Infancy Research Using the Infant-Directed-Speech Preference",
    "doi": "https://doi.org/10.1177/2515245919900809",
    "publication_date": "2020-03-01",
    "publication_year": 2020,
    "authors": "Michael C. Frank; Katie Alcock; Natalia Arias‐Trejo; Gisa Aschersleben; Dare A. Baldwin; Stéphanie Barbu; Elika Bergelson; Christina Bergmann; Alexis K. Black; Ryan Blything; Maximilian P. Böhland; Petra Bolitho; Arielle Borovsky; Shannon M. Brady; Bettina Braun; Anna Brown; Krista Byers‐Heinlein; Linda Campbell; Cara H. Cashon; Mihye Choi; Joan Christodoulou; Laura K. Cirelli; Stefania Conte; Sara Cordes; Christopher Martin Mikkelsen Cox; Alejandrina Cristià; Rhodri Cusack; Catherine Davies; Maartje de Klerk; Claire Delle Luche; Laura de Ruiter; Dhanya Dinakar; Kate C. Dixon; Virginie Durier; Samantha Durrant; Christopher T. Fennell; Brock Ferguson; Alissa L. Ferry; Paula Fikkert; Teresa Flanagan; Caroline Floccia; Megan Foley; Tom Fritzsche; Rebecca Louise Ann Frost; Anja Gampe; Judit Gervain; Nayeli Gonzalez‐Gomez; Anna Gupta; Laura E. Hahn; J. Kiley Hamlin; Erin E. Hannon; Naomi Havron; Jessica Hay; Mikołaj Hernik; Barbara Höhle; Derek M. Houston; Lauren H. Howard; Mitsuhiko Ishikawa; Shoji Itakura; Iain Jackson; Krisztina V. Jakobsen; Marianna Jartó; Scott P. Johnson; Caroline Junge; Didar Karadağ; Natalia Kartushina; Danielle Kellier; Tamar Keren‐Portnoy; Kelsey Klassen; Melissa Kline; Eon-Suk Ko; Jonathan F. Kominsky; Jessica Elizabeth Kosie; Haley E. Kragness; Andrea A. R. Krieger; Florian Krieger; Jill Lany; Roberto J. Lazo; Michelle Lee; Chloé Leservoisier; Clara C. Levelt; Casey Lew‐Williams; Matthias Lippold; Ulf Liszkowski; Liquan Liu; Steven G. Luke; Rebecca A. Lundwall; Viola Macchi Cassia; Nivedita Mani; Caterina Marino; Alia Martin; Meghan Mastroberardino; Victoria Mateu; Julien Mayor; Katharina Menn; Christine Michel; Yusuke Moriguchi; Benjamin Morris; Karli M Nave; Thierry Nazzi",
    "corresponding_authors": "",
    "abstract": "Psychological scientists have become increasingly concerned with issues related to methodology and replicability, and infancy researchers in particular face specific challenges related to replicability: For example, high-powered studies are difficult to conduct, testing conditions vary across labs, and different labs have access to different infant populations. Addressing these concerns, we report on a large-scale, multisite study aimed at (a) assessing the overall replicability of a single theoretically important phenomenon and (b) examining methodological, cultural, and developmental moderators. We focus on infants’ preference for infant-directed speech (IDS) over adult-directed speech (ADS). Stimuli of mothers speaking to their infants and to an adult in North American English were created using seminaturalistic laboratory-based audio recordings. Infants’ relative preference for IDS and ADS was assessed across 67 laboratories in North America, Europe, Australia, and Asia using the three common methods for measuring infants’ discrimination (head-turn preference, central fixation, and eye tracking). The overall meta-analytic effect size (Cohen’s d) was 0.35, 95% confidence interval = [0.29, 0.42], which was reliably above zero but smaller than the meta-analytic mean computed from previous literature (0.67). The IDS preference was significantly stronger in older children, in those children for whom the stimuli matched their native language and dialect, and in data from labs using the head-turn preference procedure. Together, these findings replicate the IDS preference but suggest that its magnitude is modulated by development, native-language experience, and testing procedure.",
    "cited_by_count": 302,
    "openalex_id": "https://openalex.org/W2782905495",
    "type": "article"
  },
  {
    "title": "Two Lines: A Valid Alternative to the Invalid Testing of U-Shaped Relationships With Quadratic Regressions",
    "doi": "https://doi.org/10.1177/2515245918805755",
    "publication_date": "2018-11-28",
    "publication_year": 2018,
    "authors": "Uri Simonsohn",
    "corresponding_authors": "Uri Simonsohn",
    "abstract": "Many psychological theories predict U-shaped relationships: The effect of x is positive for low values of x, but negative for high values, or vice versa. Despite implying merely a change of sign, hypotheses about U-shaped functions are tested almost exclusively via quadratic regressions, an approach that imposes an arbitrary functional-form assumption that in some scenarios can lead to a 100% rate of false positives (e.g., the incorrect conclusion that y = log( x) is U shaped). Estimating two regression lines, one for low and one for high values of x, allows testing for a sign change without a functional-form assumption. I introduce the Robin Hood algorithm as a way to set the break point between the lines. This algorithm delivers higher power to detect U shapes than all the other break-point-setting alternatives I compared with it. The article includes simulations demonstrating the performance of the two-lines test and reanalyses of published results using this test. An app for running the two-lines test is available at http://webstimate.org/twolines .",
    "cited_by_count": 299,
    "openalex_id": "https://openalex.org/W4256270647",
    "type": "article"
  },
  {
    "title": "A Conceptual Introduction to Bayesian Model Averaging",
    "doi": "https://doi.org/10.1177/2515245919898657",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Max Hinne; Quentin F. Gronau; Don van den Bergh; Eric‐Jan Wagenmakers",
    "corresponding_authors": "Max Hinne",
    "abstract": "Many statistical scenarios initially involve several candidate models that describe the data-generating process. Analysis often proceeds by first selecting the best model according to some criterion and then learning about the parameters of this selected model. Crucially, however, in this approach the parameter estimates are conditioned on the selected model, and any uncertainty about the model-selection process is ignored. An alternative is to learn the parameters for all candidate models and then combine the estimates according to the posterior probabilities of the associated models. This approach is known as Bayesian model averaging (BMA). BMA has several important advantages over all-or-none selection methods, but has been used only sparingly in the social sciences. In this conceptual introduction, we explain the principles of BMA, describe its advantages over all-or-none model selection, and showcase its utility in three examples: analysis of covariance, meta-analysis, and network analysis.",
    "cited_by_count": 289,
    "openalex_id": "https://openalex.org/W3001129757",
    "type": "article"
  },
  {
    "title": "Data Sharing in Psychology: A Survey on Barriers and Preconditions",
    "doi": "https://doi.org/10.1177/2515245917751886",
    "publication_date": "2018-02-15",
    "publication_year": 2018,
    "authors": "Bobby Lee Houtkoop; Chris Chambers; Malcolm Macleod; Dorothy Bishop; Thomas E. Nichols; Eric‐Jan Wagenmakers",
    "corresponding_authors": "Bobby Lee Houtkoop",
    "abstract": "Despite its potential to accelerate academic progress in psychological science, public data sharing remains relatively uncommon. In order to discover the perceived barriers to public data sharing and possible means for lowering them, we conducted a survey, which elicited responses from 600 authors of articles in psychology. The results confirmed that data are shared only infrequently. Perceived barriers included respondents’ belief that sharing is not a common practice in their fields, their preference to share data only upon request, their perception that sharing requires extra work, and their lack of training in sharing data. Our survey suggests that strong encouragement from institutions, journals, and funders will be particularly effective in overcoming these barriers, in combination with educational materials that demonstrate where and how data can be shared effectively.",
    "cited_by_count": 275,
    "openalex_id": "https://openalex.org/W2784830894",
    "type": "article"
  },
  {
    "title": "Statistical Rituals: The Replication Delusion and How We Got There",
    "doi": "https://doi.org/10.1177/2515245918771329",
    "publication_date": "2018-06-01",
    "publication_year": 2018,
    "authors": "Gerd Gigerenzer",
    "corresponding_authors": "Gerd Gigerenzer",
    "abstract": "The “replication crisis” has been attributed to misguided external incentives gamed by researchers (the strategic-game hypothesis). Here, I want to draw attention to a complementary internal factor, namely, researchers’ widespread faith in a statistical ritual and associated delusions (the statistical-ritual hypothesis). The “null ritual,” unknown in statistics proper, eliminates judgment precisely at points where statistical theories demand it. The crucial delusion is that the p value specifies the probability of a successful replication (i.e., 1 – p), which makes replication studies appear to be superfluous. A review of studies with 839 academic psychologists and 991 students shows that the replication delusion existed among 20% of the faculty teaching statistics in psychology, 39% of the professors and lecturers, and 66% of the students. Two further beliefs, the illusion of certainty (e.g., that statistical significance proves that an effect exists) and Bayesian wishful thinking (e.g., that the probability of the alternative hypothesis being true is 1 – p), also make successful replication appear to be certain or almost certain, respectively. In every study reviewed, the majority of researchers (56%–97%) exhibited one or more of these delusions. Psychology departments need to begin teaching statistical thinking, not rituals, and journal editors should no longer accept manuscripts that report results as “significant” or “not significant.”",
    "cited_by_count": 252,
    "openalex_id": "https://openalex.org/W2807758254",
    "type": "article"
  },
  {
    "title": "Visualization of Brain Statistics With R Packages <i>ggseg</i> and <i>ggseg3d</i>",
    "doi": "https://doi.org/10.1177/2515245920928009",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Athanasia M. Mowinckel; Dídac Vidal-Piñeiro",
    "corresponding_authors": "",
    "abstract": "There is an increased emphasis on visualizing neuroimaging results in more intuitive ways. Common statistical tools for dissemination of these results, such as bar charts, lack the spatial dimension that is inherent in neuroimaging data. Here we present two packages for the statistical software R that integrate this spatial component. The ggseg and ggseg3d packages visualize predefined brain segmentations as 2D polygons and 3D meshes, respectively. Both packages are integrated with other well-established R packages, which allows great flexibility. In this Tutorial, we describe the main data and functions in the ggseg and ggseg3d packages for visualization of brain atlases. The highlighted functions are able to display brain-segmentation plots in R. Further, the accompanying ggsegExtra package includes a wider collection of atlases and is intended for community-based efforts to develop additional compatible atlases for ggseg and ggseg3d. Overall, the ggseg packages facilitate parcellation-based visualizations in R, improve and facilitate the dissemination of results, and increase the efficiency of workflows.",
    "cited_by_count": 246,
    "openalex_id": "https://openalex.org/W3108978982",
    "type": "article"
  },
  {
    "title": "Practical Tips for Ethical Data Sharing",
    "doi": "https://doi.org/10.1177/2515245917747656",
    "publication_date": "2018-02-23",
    "publication_year": 2018,
    "authors": "Michelle N. Meyer",
    "corresponding_authors": "Michelle N. Meyer",
    "abstract": "This Tutorial provides practical dos and don’ts for sharing research data in ways that are effective, ethical, and compliant with the federal Common Rule. I first consider best practices for prospectively incorporating data-sharing plans into research, discussing what to say—and what not to say—in consent forms and institutional review board applications, tools for data de-identification and how to think about the risks of re-identification, and what to consider when selecting a data repository. Turning to data that have already been collected, I discuss the ethical and regulatory issues raised by sharing data when the consent form either was silent about data sharing or explicitly promised participants that the data would not be shared. Finally, I discuss ethical issues in sharing “public” data.",
    "cited_by_count": 245,
    "openalex_id": "https://openalex.org/W2793219767",
    "type": "article"
  },
  {
    "title": "Improving Present Practices in the Visual Display of Interactions",
    "doi": "https://doi.org/10.1177/2515245917746792",
    "publication_date": "2018-03-28",
    "publication_year": 2018,
    "authors": "Connor McCabe; Dale S. Kim; Kevin M. King",
    "corresponding_authors": "Connor McCabe",
    "abstract": "Interaction plots are used frequently in psychology research to make inferences about moderation hypotheses. A common method of analyzing and displaying interactions is to create simple-slopes or marginal-effects plots using standard software programs. However, these plots omit features that are essential to both graphic integrity and statistical inference. For example, they often do not display all quantities of interest, omit information about uncertainty, or do not show the observed data underlying an interaction, and failure to include these features undermines the strength of the inferences that may be drawn from such displays. Here, we review the strengths and limitations of present practices in analyzing and visualizing interaction effects in psychology. We provide simulated examples of the conditions under which visual displays may lead to inappropriate inferences and introduce open-source software that provides optimized utilities for analyzing and visualizing interactions.",
    "cited_by_count": 244,
    "openalex_id": "https://openalex.org/W2794729814",
    "type": "article"
  },
  {
    "title": "A Unified Framework to Quantify the Credibility of Scientific Findings",
    "doi": "https://doi.org/10.1177/2515245918787489",
    "publication_date": "2018-08-10",
    "publication_year": 2018,
    "authors": "Etienne P. LeBel; Randy J. McCarthy; Brian D. Earp; Malte Elson; Wolf Vanpaemel",
    "corresponding_authors": "Etienne P. LeBel",
    "abstract": "Societies invest in scientific studies to better understand the world and attempt to harness such improved understanding to address pressing societal problems. Published research, however, can be useful for theory or application only if it is credible. In science, a credible finding is one that has repeatedly survived risky falsification attempts. However, state-of-the-art meta-analytic approaches cannot determine the credibility of an effect because they do not account for the extent to which each included study has survived such attempted falsification. To overcome this problem, we outline a unified framework for estimating the credibility of published research by examining four fundamental falsifiability-related dimensions: (a) transparency of the methods and data, (b) reproducibility of the results when the same data-processing and analytic decisions are reapplied, (c) robustness of the results to different data-processing and analytic decisions, and (d) replicability of the effect. This framework includes a standardized workflow in which the degree to which a finding has survived scrutiny is quantified along these four facets of credibility. The framework is demonstrated by applying it to published replications in the psychology literature. Finally, we outline a Web implementation of the framework and conclude by encouraging the community of researchers to contribute to the development and crowdsourcing of this platform.",
    "cited_by_count": 210,
    "openalex_id": "https://openalex.org/W2777631817",
    "type": "article"
  },
  {
    "title": "Why the Cross-Lagged Panel Model Is Almost Never the Right Choice",
    "doi": "https://doi.org/10.1177/25152459231158378",
    "publication_date": "2023-01-01",
    "publication_year": 2023,
    "authors": "Richard E. Lucas",
    "corresponding_authors": "Richard E. Lucas",
    "abstract": "The cross-lagged panel model (CLPM) is a widely used technique for examining reciprocal causal effects using longitudinal data. Critics of the CLPM have noted that by failing to account for certain person-level associations, estimates of these causal effects can be biased. Because of this, models that incorporate stable-trait components (e.g., the random-intercept CLPM) have become popular alternatives. Debates about the merits of the CLPM have continued, however, with some researchers arguing that the CLPM is more appropriate than modern alternatives for examining common psychological questions. In this article, I discuss the ways that these defenses of the CLPM fail to acknowledge well-known limitations of the model. I propose some possible sources of confusion regarding these models and provide alternative ways of thinking about the problems with the CLPM. I then show in simulated data that with realistic assumptions, the CLPM is very likely to find spurious cross-lagged effects when they do not exist and can sometimes underestimate these effects when they do exist.",
    "cited_by_count": 201,
    "openalex_id": "https://openalex.org/W4361799298",
    "type": "article"
  },
  {
    "title": "Hidden Invalidity Among 15 Commonly Used Measures in Social and Personality Psychology",
    "doi": "https://doi.org/10.1177/2515245919882903",
    "publication_date": "2020-04-09",
    "publication_year": 2020,
    "authors": "Ian Hussey; Sean Hughes",
    "corresponding_authors": "Ian Hussey",
    "abstract": "It has recently been demonstrated that metrics of structural validity are severely underreported in social and personality psychology. We comprehensively assessed structural validity in a uniquely large and varied data set ( N = 144,496 experimental sessions) to investigate the psychometric properties of some of the most widely used self-report measures ( k = 15 questionnaires, 26 scales) in social and personality psychology. When the scales were assessed using the modal practice of considering only internal consistency, 88% of them appeared to possess good validity. Yet when validity was assessed comprehensively (via internal consistency, immediate and delayed test-retest reliability, factor structure, and measurement invariance for age and gender groups), only 4% demonstrated good validity. Furthermore, the less commonly a test was reported in the literature, the more likely the scales were to fail that test (e.g., scales failed measurement invariance much more often than internal consistency). This suggests that the pattern of underreporting in the field may represent widespread hidden invalidity of the measures used and may therefore pose a threat to many research findings. We highlight the degrees of freedom afforded to researchers in the assessment and reporting of structural validity and introduce the concept of validity hacking ( v-hacking), similar to the better-known concept of p-hacking. We argue that the practice of v-hacking should be acknowledged and addressed.",
    "cited_by_count": 191,
    "openalex_id": "https://openalex.org/W4241206412",
    "type": "article"
  },
  {
    "title": "That’s a Lot to Process! Pitfalls of Popular Path Models",
    "doi": "https://doi.org/10.1177/25152459221095827",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Julia M. Rohrer; Paul Hünermund; Ruben C. Arslan; Malte Elson",
    "corresponding_authors": "Julia M. Rohrer",
    "abstract": "Path models to test claims about mediation and moderation are a staple of psychology. But applied researchers may sometimes not understand the underlying causal inference problems and thus endorse conclusions that rest on unrealistic assumptions. In this article, we aim to provide a clear explanation for the limited conditions under which standard procedures for mediation and moderation analysis can succeed. We discuss why reversing arrows or comparing model fit indices cannot tell us which model is the right one and how tests of conditional independence can at least tell us where our model goes wrong. Causal modeling practices in psychology are far from optimal but may be kept alive by domain norms that demand every article makes some novel claim about processes and boundary conditions. We end with a vision for a different research culture in which causal inference is pursued in a much slower, more deliberate, and collaborative manner.",
    "cited_by_count": 190,
    "openalex_id": "https://openalex.org/W4288766776",
    "type": "article"
  },
  {
    "title": "Recommendations for Increasing the Transparency of Analysis of Preexisting Data Sets",
    "doi": "https://doi.org/10.1177/2515245919848684",
    "publication_date": "2019-06-11",
    "publication_year": 2019,
    "authors": "Sara J. Weston; Stuart J. Ritchie; Julia M. Rohrer; Andrew K Przybylski",
    "corresponding_authors": "Sara J. Weston",
    "abstract": "Secondary data analysis, or the analysis of preexisting data, provides a powerful tool for the resourceful psychological scientist. Never has this been more true than now, when technological advances enable both sharing data across labs and continents and mining large sources of preexisting data. However, secondary data analysis is easily overlooked as a key domain for developing new open-science practices or improving analytic methods for robust data analysis. In this article, we provide researchers with the knowledge necessary to incorporate secondary data analysis into their methodological toolbox. We explain that secondary data analysis can be used for either exploratory or confirmatory work, and can be either correlational or experimental, and we highlight the advantages and disadvantages of this type of research. We describe how transparency-enhancing practices can improve and alter interpretations of results from secondary data analysis and discuss approaches that can be used to improve the robustness of reported results. We close by suggesting ways in which scientific subfields and institutions could address and improve the use of secondary data analysis.",
    "cited_by_count": 187,
    "openalex_id": "https://openalex.org/W2926841456",
    "type": "article"
  },
  {
    "title": "Statistical Control Requires Causal Justification",
    "doi": "https://doi.org/10.1177/25152459221095823",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Anna Wysocki; Katherine M. Lawson; Mijke Rhemtulla",
    "corresponding_authors": "Anna Wysocki",
    "abstract": "It is common practice in correlational or quasiexperimental studies to use statistical control to remove confounding effects from a regression coefficient. Controlling for relevant confounders can debias the estimated causal effect of a predictor on an outcome; that is, it can bring the estimated regression coefficient closer to the value of the true causal effect. But statistical control works only under ideal circumstances. When the selected control variables are inappropriate, controlling can result in estimates that are more biased than uncontrolled estimates. Despite the ubiquity of statistical control in published regression analyses and the consequences of controlling for inappropriate third variables, the selection of control variables is rarely explicitly justified in print. We argue that to carefully select appropriate control variables, researchers must propose and defend a causal structure that includes the outcome, predictors, and plausible confounders. We underscore the importance of causality when selecting control variables by demonstrating how regression coefficients are affected by controlling for appropriate and inappropriate variables. Finally, we provide practical recommendations for applied researchers who wish to use statistical control.",
    "cited_by_count": 187,
    "openalex_id": "https://openalex.org/W4283693641",
    "type": "article"
  },
  {
    "title": "Research in Social Psychology Changed Between 2011 and 2016: Larger Sample Sizes, More Self-Report Measures, and More Online Studies",
    "doi": "https://doi.org/10.1177/2515245919838781",
    "publication_date": "2019-04-12",
    "publication_year": 2019,
    "authors": "Kai Sassenberg; Lara Ditrich",
    "corresponding_authors": "",
    "abstract": "The debate about false positives in psychological research has led to a demand for higher statistical power. To meet this demand, researchers need to collect data from larger samples—which is important to increase replicability, but can be costly in both time and money (i.e., remuneration of participants). Given that researchers might need to compensate for these higher costs, we hypothesized that larger sample sizes might have been accompanied by more frequent use of less costly research methods (i.e., online data collection and self-report measures). To test this idea, we analyzed social psychology studies published in 2009, 2011, 2016, and 2018. Indeed, research reported in 2016 and 2018 (vs. 2009 and 2011) had larger sample sizes and relied more on online data collection and self-report measures. Thus, over these years, research improved in its statistical power, but also changed with regard to the methods applied. Implications for social psychology as a discipline are discussed.",
    "cited_by_count": 166,
    "openalex_id": "https://openalex.org/W2937214313",
    "type": "article"
  },
  {
    "title": "Cross-Validation: A Method Every Psychologist Should Know",
    "doi": "https://doi.org/10.1177/2515245919898466",
    "publication_date": "2020-05-27",
    "publication_year": 2020,
    "authors": "Mark de Rooij; Wouter D. Weeda",
    "corresponding_authors": "",
    "abstract": "Cross-validation is a statistical procedure that every psychologist should know. Most are possibly familiar with the procedure in a global way but have not used it for the analysis of their own data. We introduce cross-validation for the purpose of model selection in a general sense, as well as an R package we have developed for this kind of analysis, and we present examples illustrating the use of this package for types of research problems that are often encountered in the social sciences. Cross-validation can be an easy-to-use alternative to null-hypothesis testing, and it has the benefit that it does not make as many assumptions.",
    "cited_by_count": 161,
    "openalex_id": "https://openalex.org/W3031690984",
    "type": "article"
  },
  {
    "title": "A Traveler’s Guide to the Multiverse: Promises, Pitfalls, and a Framework for the Evaluation of Analytic Decisions",
    "doi": "https://doi.org/10.1177/2515245920954925",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Marco Del Giudice; Steven W. Gangestad",
    "corresponding_authors": "",
    "abstract": "Decisions made by researchers while analyzing data (e.g., how to measure variables, how to handle outliers) are sometimes arbitrary, without an objective justification for choosing one alternative over another. Multiverse-style methods (e.g., specification curve, vibration of effects) estimate an effect across an entire set of possible specifications to expose the impact of hidden degrees of freedom and/or obtain robust, less biased estimates of the effect of interest. However, if specifications are not truly arbitrary, multiverse-style analyses can produce misleading results, potentially hiding meaningful effects within a mass of poorly justified alternatives. So far, a key question has received scant attention: How does one decide whether alternatives are arbitrary? We offer a framework and conceptual tools for doing so. We discuss three kinds of a priori nonequivalence among alternatives—measurement nonequivalence, effect nonequivalence, and power/precision nonequivalence. The criteria we review lead to three decision scenarios: Type E decisions (principled equivalence), Type N decisions (principled nonequivalence), and Type U decisions (uncertainty). In uncertain scenarios, multiverse-style analysis should be conducted in a deliberately exploratory fashion. The framework is discussed with reference to published examples and illustrated with the help of a simulated data set. Our framework will help researchers reap the benefits of multiverse-style methods while avoiding their pitfalls.",
    "cited_by_count": 140,
    "openalex_id": "https://openalex.org/W3127662205",
    "type": "article"
  },
  {
    "title": "Understanding Mixed-Effects Models Through Data Simulation",
    "doi": "https://doi.org/10.1177/2515245920965119",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Lisa M. DeBruine; Dale J. Barr",
    "corresponding_authors": "Lisa M. DeBruine",
    "abstract": "Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed-effects models. However, much of this research is analyzed using analysis of variance on aggregated responses because researchers are not confident specifying and interpreting mixed-effects models. This Tutorial explains how to simulate data with random-effects structure and analyze the data using linear mixed-effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation not only can enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs. All materials associated with this article can be accessed at https://osf.io/3cz2e/ .",
    "cited_by_count": 131,
    "openalex_id": "https://openalex.org/W2951982695",
    "type": "article"
  },
  {
    "title": "How Many Participants Do I Need to Test an Interaction? Conducting an Appropriate Power Analysis and Achieving Sufficient Power to Detect an Interaction",
    "doi": "https://doi.org/10.1177/25152459231178728",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Nicolas Sommet; David L. Weissman; Nicolas Cheutin; Andrew J. Elliot",
    "corresponding_authors": "Nicolas Sommet",
    "abstract": "Power analysis for first-order interactions poses two challenges: (a) Conducting an appropriate power analysis is difficult because the typical expected effect size of an interaction depends on its shape, and (b) achieving sufficient power is difficult because interactions are often modest in size. This article consists of three parts. In the first part, we address the first challenge. We first use a fictional study to explain the difference between power analyses for interactions and main effects. Then, we introduce an intuitive taxonomy of 12 types of interactions based on the shape of the interaction (reversed, fully attenuated, partially attenuated) and the size of the simple slopes (median, smaller, larger), and we offer mathematically derived sample-size recommendations to detect each interaction with a power of .80/.90/.95 (for two-tailed tests in between-participants designs). In the second part, we address the second challenge. We first describe a preregistered metastudy (159 studies from recent articles in influential psychology journals) showing that the median power to detect interactions of a typical size is .18. Then, we use simulations (≈900,000,000 data sets) to generate power curves for the 12 types of interactions and test three approaches to increase power without increasing sample size: (a) preregistering one-tailed tests (+21% gain), (b) using a mixed design (+75% gain), and (c) preregistering contrast analysis for a fully attenuated interaction (+62% gain). In the third part, we introduce INT×Power ( www.intxpower.com ), a web application that enables users to draw their interaction and determine the sample size needed to reach the power of their choice with the option of using/combining these approaches.",
    "cited_by_count": 124,
    "openalex_id": "https://openalex.org/W4386851549",
    "type": "article"
  },
  {
    "title": "Selection of the Number of Participants in Intensive Longitudinal Studies: A User-Friendly Shiny App and Tutorial for Performing Power Analysis in Multilevel Regression Models That Account for Temporal Dependencies",
    "doi": "https://doi.org/10.1177/2515245920978738",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Ginette Lafit; Janne Adolf; Egon Dejonckheere; Inez Myin‐Germeys; Wolfgang Viechtbauer; Eva Ceulemans",
    "corresponding_authors": "Ginette Lafit",
    "abstract": "In recent years, the popularity of procedures for collecting intensive longitudinal data, such as the experience-sampling method, has increased greatly. The data collected using such designs allow researchers to study the dynamics of psychological functioning and how these dynamics differ across individuals. To this end, the data are often modeled with multilevel regression models. An important question that arises when researchers design intensive longitudinal studies is how to determine the number of participants needed to test specific hypotheses regarding the parameters of these models with sufficient power. Power calculations for intensive longitudinal studies are challenging because of the hierarchical data structure in which repeated observations are nested within the individuals and because of the serial dependence that is typically present in these data. We therefore present a user-friendly application and step-by-step tutorial for performing simulation-based power analyses for a set of models that are popular in intensive longitudinal research. Because many studies use the same sampling protocol (i.e., a fixed number of at least approximately equidistant observations) within individuals, we assume that this protocol is fixed and focus on the number of participants. All included models explicitly account for the temporal dependencies in the data by assuming serially correlated errors or including autoregressive effects.",
    "cited_by_count": 122,
    "openalex_id": "https://openalex.org/W3120504298",
    "type": "article"
  },
  {
    "title": "These Are Not the Effects You Are Looking for: Causality and the Within-/Between-Persons Distinction in Longitudinal Data Analysis",
    "doi": "https://doi.org/10.1177/25152459221140842",
    "publication_date": "2023-01-01",
    "publication_year": 2023,
    "authors": "Julia M. Rohrer; Kou Murayama",
    "corresponding_authors": "Julia M. Rohrer",
    "abstract": "In psychological science, researchers often pay particular attention to the distinction between within- and between-persons relationships in longitudinal data analysis. Here, we aim to clarify the relationship between the within- and between-persons distinction and causal inference and show that the distinction is informative but does not play a decisive role in causal inference. Our main points are threefold. First, within-persons data are not necessary for causal inference; for example, between-persons experiments can inform about (average) causal effects. Second, within-persons data are not sufficient for causal inference; for example, time-varying confounders can lead to spurious within-persons associations. Finally, despite not being sufficient, within-persons data can be tremendously helpful for causal inference. We provide pointers to help readers navigate the more technical literature on longitudinal models and conclude with a call for more conceptual clarity: Instead of letting statistical models dictate which substantive questions researchers ask, researchers should start with well-defined theoretical estimands, which in turn determine both study design and data analysis.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W4321440760",
    "type": "article"
  },
  {
    "title": "A Causal Framework for Cross-Cultural Generalizability",
    "doi": "https://doi.org/10.1177/25152459221106366",
    "publication_date": "2022-07-01",
    "publication_year": 2022,
    "authors": "Dominik Deffner; Julia M. Rohrer; Richard McElreath",
    "corresponding_authors": "Dominik Deffner",
    "abstract": "Behavioral researchers increasingly recognize the need for more diverse samples that capture the breadth of human experience. Current attempts to establish generalizability across populations focus on threats to validity, constraints on generalization, and the accumulation of large, cross-cultural data sets. But for continued progress, we also require a framework that lets us determine which inferences can be drawn and how to make informative cross-cultural comparisons. We describe a generative causal-modeling framework and outline simple graphical criteria to derive analytic strategies and implied generalizations. Using both simulated and real data, we demonstrate how to project and compare estimates across populations and further show how to formally represent measurement equivalence or inequivalence across societies. We conclude with a discussion of how a formal framework for generalizability can assist researchers in designing more informative cross-cultural studies and thus provides a more solid foundation for cumulative and generalizable behavioral research.",
    "cited_by_count": 113,
    "openalex_id": "https://openalex.org/W4296558598",
    "type": "article"
  },
  {
    "title": "Best Practices in Supervised Machine Learning: A Tutorial for Psychologists",
    "doi": "https://doi.org/10.1177/25152459231162559",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Florian Pargent; Ramona Schoedel; Clemens Stachl",
    "corresponding_authors": "Florian Pargent",
    "abstract": "Supervised machine learning (ML) is becoming an influential analytical method in psychology and other social sciences. However, theoretical ML concepts and predictive-modeling techniques are not yet widely taught in psychology programs. This tutorial is intended to provide an intuitive but thorough primer and introduction to supervised ML for psychologists in four consecutive modules. After introducing the basic terminology and mindset of supervised ML, in Module 1, we cover how to use resampling methods to evaluate the performance of ML models (bias-variance trade-off, performance measures, k-fold cross-validation). In Module 2, we introduce the nonlinear random forest, a type of ML model that is particularly user-friendly and well suited to predicting psychological outcomes. Module 3 is about performing empirical benchmark experiments (comparing the performance of several ML models on multiple data sets). Finally, in Module 4, we discuss the interpretation of ML models, including permutation variable importance measures, effect plots (partial-dependence plots, individual conditional-expectation profiles), and the concept of model fairness. Throughout the tutorial, intuitive descriptions of theoretical concepts are provided, with as few mathematical formulas as possible, and followed by code examples using the mlr3 and companion packages in R. Key practical-analysis steps are demonstrated on the publicly available PhoneStudy data set ( N = 624), which includes more than 1,800 variables from smartphone sensing to predict Big Five personality trait scores. The article contains a checklist to be used as a reminder of important elements when performing, reporting, or reviewing ML analyses in psychology. Additional examples and more advanced concepts are demonstrated in online materials ( https://osf.io/9273g/ ).",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W4385521225",
    "type": "article"
  },
  {
    "title": "Tutorial: Power Analyses for Interaction Effects in Cross-Sectional Regressions",
    "doi": "https://doi.org/10.1177/25152459231187531",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "David A. A. Baranger; Megan C. Finsaas; Brandon L. Goldstein; Colin Vize; Donald R. Lynam; Thomas M. Olino",
    "corresponding_authors": "David A. A. Baranger",
    "abstract": "Interaction analyses (also termed “moderation” analyses or “moderated multiple regression”) are a form of linear regression analysis designed to test whether the association between two variables changes when conditioned on a third variable. It can be challenging to perform a power analysis for interactions with existing software, particularly when variables are correlated and continuous. Moreover, although power is affected by main effects, their correlation, and variable reliability, it can be unclear how to incorporate these effects into a power analysis. The R package InteractionPoweR and associated Shiny apps allow researchers with minimal or no programming experience to perform analytic and simulation-based power analyses for interactions. At minimum, these analyses require the Pearson’s correlation between variables and sample size, and additional parameters, including reliability and the number of discrete levels that a variable takes (e.g., binary or Likert scale), can optionally be specified. In this tutorial, we demonstrate how to perform power analyses using our package and give examples of how power can be affected by main effects, correlations between main effects, reliability, and variable distributions. We also include a brief discussion of how researchers may select an appropriate interaction effect size when performing a power analysis.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W4387065116",
    "type": "article"
  },
  {
    "title": "Selecting the Number and Labels of Topics in Topic Modeling: A Tutorial",
    "doi": "https://doi.org/10.1177/25152459231160105",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Sara J. Weston; Ian Shryock; Ryan Light; Phillip A. Fisher",
    "corresponding_authors": "Sara J. Weston",
    "abstract": "Topic modeling is a type of text analysis that identifies clusters of co-occurring words, or latent topics. A challenging step of topic modeling is determining the number of topics to extract. This tutorial describes tools researchers can use to identify the number and labels of topics in topic modeling. First, we outline the procedure for narrowing down a large range of models to a select number of candidate models. This procedure involves comparing the large set on fit metrics, including exclusivity, residuals, variational lower bound, and semantic coherence. Next, we describe the comparison of a small number of models using project goals as a guide and information about topic representative and solution congruence. Finally, we describe tools for labeling topics, including frequent and exclusive words, key examples, and correlations among topics.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W4378194344",
    "type": "article"
  },
  {
    "title": "Best Laid Plans: A Guide to Reporting Preregistration Deviations",
    "doi": "https://doi.org/10.1177/25152459231213802",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Emily C Willroth; Olivia E. Atherton",
    "corresponding_authors": "Olivia E. Atherton",
    "abstract": "Psychological scientists are increasingly using preregistration as a tool to increase the credibility of research findings. Many of the benefits of preregistration rest on the assumption that preregistered plans are followed perfectly. However, research suggests that this is the exception rather than the norm, and there are many reasons why researchers may deviate from their preregistered plans. Preregistration can still be a valuable tool, even in the presence of deviations, as long as those deviations are well documented and transparently reported. Unfortunately, most preregistration deviations in psychology go unreported or are reported in unsystematic ways. In the current article, we offer a solution to this problem by providing a framework for transparent and standardized reporting of preregistration deviations, which was developed by drawing on our own experiences with preregistration, existing unpublished templates, feedback from colleagues and reviewers, and the results of a survey of 34 psychology-journal editors. This framework provides a clear template for what to do when things do not go as planned. We conclude by encouraging researchers to adopt this framework in their own preregistered research and by suggesting that journals implement structural policies around the transparent reporting of preregistration deviations.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W4391548187",
    "type": "article"
  },
  {
    "title": "An Aberrant Abundance of Cronbach’s Alpha Values at .70",
    "doi": "https://doi.org/10.1177/25152459241287123",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Ian Hussey; Taym Alsalti; Frank A. Bosco; Malte Elson; Ruben C. Arslan",
    "corresponding_authors": "",
    "abstract": "Cronbach’s α is the most widely reported metric of the reliability of psychological measures. Decisions about an observed α’s adequacy are often made using rule-of-thumb thresholds, such as α of at least .70. Such thresholds can put pressure on researchers to make their measures meet these criteria, similar to the pressure to meet the significance threshold with p values. We examined whether α values reported in the psychology literature are inflated at the rule-of-thumb thresholds (αs = .70, .80, .90) because of, for example, overfitting to in-sample data (α-hacking) or publication bias. We extracted reported α values from three very large data sets covering the general psychology literature (&gt; 30,000 α values taken from &gt; 74,000 published articles in American Psychological Association [APA] journals), the industrial and organizational (I/O) psychology literature (&gt; 89,000 α values taken from &gt; 14,000 published articles in I/O journals), and the APA’s PsycTests database, which aims to cover all psychological measures published since 1894 (&gt; 67,000 α values taken from &gt; 60,000 measures). The distributions of these values show robust evidence of excesses at the α = .70 rule-of-thumb threshold that cannot be explained by justifiable measurement practices. We discuss the scope, causes, and consequences of α-hacking and how increased transparency, preregistration of measurement strategy, and standardized protocols could mitigate this problem.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4406518090",
    "type": "article"
  },
  {
    "title": "An Updated Recommendation for Multiple Comparisons",
    "doi": "https://doi.org/10.1177/2515245918808784",
    "publication_date": "2019-01-28",
    "publication_year": 2019,
    "authors": "Derek Sauder; Christine E. DeMars",
    "corresponding_authors": "Derek Sauder",
    "abstract": "Instructors of introductory and intermediate statistics courses often teach the use of analysis of variance (ANOVA) for the purpose of comparing more than two group means and pairwise comparison procedures (PCPs) to determine which group means differ from one another following a statistically significant ANOVA test. SPSS provides 18 PCPs. The purpose of this study was to determine which PCP has the best power and maintains Type I error control both when assumptions of equal sample size and equal variance are met and when they are violated, so as to provide a single resource for use in introductory and intermediate applied statistics courses. Testing the PCPs with simulated data revealed that only the four tests developed to be used under assumption violations adequately controlled Type I error, so we recommend using one of these procedures. Power results were similar for all four of these tests, but were slightly higher for the Games-Howell test than for the others.",
    "cited_by_count": 142,
    "openalex_id": "https://openalex.org/W2914370659",
    "type": "article"
  },
  {
    "title": "How Do I Know What My Theory Predicts?",
    "doi": "https://doi.org/10.1177/2515245919876960",
    "publication_date": "2019-11-14",
    "publication_year": 2019,
    "authors": "Zoltán Dienes",
    "corresponding_authors": "Zoltán Dienes",
    "abstract": "To get evidence for or against a theory relative to the null hypothesis, one needs to know what the theory predicts. The amount of evidence can then be quantified by a Bayes factor. Specifying the sizes of the effect one’s theory predicts may not come naturally, but I show some ways of thinking about the problem, some simple heuristics that are often useful when one has little relevant prior information. These heuristics include the room-to-move heuristic (for comparing mean differences), the ratio-of-scales heuristic (for regression slopes), the ratio-of-means heuristic (for regression slopes), the basic-effect heuristic (for analysis of variance effects), and the total-effect heuristic (for mediation analysis).",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W2972962186",
    "type": "article"
  },
  {
    "title": "A Simple Method for Removing Bias From a Popular Measure of Standardized Effect Size: Adjusted Partial Eta Squared",
    "doi": "https://doi.org/10.1177/2515245919855053",
    "publication_date": "2019-07-30",
    "publication_year": 2019,
    "authors": "J. Toby Mordkoff",
    "corresponding_authors": "J. Toby Mordkoff",
    "abstract": "Accurate estimates of population effect size are critical to empirical science, for both reporting experimental results and conducting a priori power analyses. Unfortunately, the current most-popular measure of standardized effect size, partial eta squared ([Formula: see text]), is known to have positive bias. Two less-biased alternatives, partial epsilon squared ([Formula: see text]) and partial omega squared ([Formula: see text]), have both existed for decades, but neither is often employed. Given that researchers appear reluctant to abandon [Formula: see text], this article provides a simple method for removing bias from this measure, to produce a value referred to as adjusted partial eta squared (adj [Formula: see text]). Some of the many benefits of adopting this measure are briefly discussed.",
    "cited_by_count": 118,
    "openalex_id": "https://openalex.org/W2965249208",
    "type": "article"
  },
  {
    "title": "Analysis of Open Data and Computational Reproducibility in Registered Reports in Psychology",
    "doi": "https://doi.org/10.1177/2515245920918872",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Pepijn Obels; Daniël Lakens; Nicholas A. Coles; Jaroslav Gottfried; Seth A. Green",
    "corresponding_authors": "Daniël Lakens",
    "abstract": "Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to reuse or check published research. However, these benefits will emerge only if researchers can reproduce the analyses reported in published articles and if data are annotated well enough so that it is clear what all variable and value labels mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify those that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature from 2014 to 2018 and attempted to independently computationally reproduce the main results in each article. Of the 62 articles that met our inclusion criteria, 41 had data available, and 37 had analysis scripts available. Both data and code for 36 of the articles were shared. We could run the scripts for 31 analyses, and we reproduced the main results for 21 articles. Although the percentage of articles for which both data and code were shared (36 out of 62, or 58%) and the percentage of articles for which main results could be computationally reproduced (21 out of 36, or 58%) were relatively high compared with the percentages found in other studies, there is clear room for improvement. We provide practical recommendations based on our observations and cite examples of good research practices in the studies whose main results we reproduced.",
    "cited_by_count": 117,
    "openalex_id": "https://openalex.org/W3008000555",
    "type": "article"
  },
  {
    "title": "Registered Replication Report on Mazar, Amir, and Ariely (2008)",
    "doi": "https://doi.org/10.1177/2515245918781032",
    "publication_date": "2018-09-01",
    "publication_year": 2018,
    "authors": "Bruno Verschuère; Ewout H. Meijer; Ariane Jim; Katherine Hoogesteyn; Robin Orthey; Randy J. McCarthy; John J. Skowronski; Oguz A. Acar; Balázs Aczél; Bence E. Bakos; Fernando Barbosa; Ernest Baskin; Laurent Bègue; Gershon Ben‐Shakhar; Angie R. Birt; Lisa Blatz; Steve D. Charman; Aline Claesen; Samuel L. Clay; Sean P. Coary; Jan Crusius; Jacqueline R. Evans; Noa Feldman; Fernando Ferreira‐Santos; Matthias Gamer; Sara Gomes; Marta González‐Iraizoz; Felix Holzmeister; Jürgen Huber; Andrea Isoni; Ryan K. Jessup; Michael Kirchler; Nathalie klein Selle; Lina Koppel; Márton Kovács; Tei Laine; Frank Lentz; David D. Loschelder; Elliot A. Ludvig; Monty L. Lynn; Scott D. Martin; Neil McLatchie; Mario Mechtel; Galit Nahari; Asil Ali Özdoğru; Rita Pasion; Charlotte R. Pennington; Arne Roets; Nir Rozmann; Irene Scopelliti; Eli Spiegelman; Kristina Suchotzki; Angela Sutan; Péter Szécsi; Gustav Tinghög; Jean-Christian Tisserand; Ulrich S. Tran; Alain Van Hiel; Wolf Vanpaemel; Daniel Västfjäll; Thomas Verliefde; Kévin Vezirian; Martin Voracek; Lara Warmelink; Katherine Wick; Bradford J. Wiggins; Keith Wylie; Ezgi Yıldız",
    "corresponding_authors": "",
    "abstract": "The self-concept maintenance theory holds that many people will cheat in order to maximize self-profit, but only to the extent that they can do so while maintaining a positive self-concept. Mazar, Amir, and Ariely (2008, Experiment 1) gave participants an opportunity and incentive to cheat on a problem-solving task. Prior to that task, participants either recalled the Ten Commandments (a moral reminder) or recalled 10 books they had read in high school (a neutral task). Results were consistent with the self-concept maintenance theory. When given the opportunity to cheat, participants given the moral-reminder priming task reported solving 1.45 fewer matrices than did those given a neutral prime (Cohen’s d = 0.48); moral reminders reduced cheating. Mazar et al.’s article is among the most cited in deception research, but their Experiment 1 has not been replicated directly. This Registered Replication Report describes the aggregated result of 25 direct replications (total N = 5,786), all of which followed the same preregistered protocol. In the primary meta-analysis (19 replications, total n = 4,674), participants who were given an opportunity to cheat reported solving 0.11 more matrices if they were given a moral reminder than if they were given a neutral reminder (95% confidence interval = [−0.09, 0.31]). This small effect was numerically in the opposite direction of the effect observed in the original study (Cohen’s d = −0.04).",
    "cited_by_count": 113,
    "openalex_id": "https://openalex.org/W2891039941",
    "type": "article"
  },
  {
    "title": "Making the Black Box Transparent: A Template and Tutorial for Registration of Studies Using Experience-Sampling Methods",
    "doi": "https://doi.org/10.1177/2515245920924686",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Olivia J Kirtley; Ginette Lafit; Robin Achterhof; Anu Pauliina Hiekkaranta; Inez Myin‐Germeys",
    "corresponding_authors": "Olivia J Kirtley",
    "abstract": "A growing interest in understanding complex and dynamic psychological processes as they occur in everyday life has led to an increase in studies using ambulatory assessment techniques, including the experience-sampling method (ESM) and ecological momentary assessment. These methods, however, tend to involve numerous forking paths and researcher degrees of freedom, even beyond those typically encountered with other research methodologies. Although a number of researchers working with ESM techniques are actively engaged in efforts to increase the methodological rigor and transparency of research that uses them, currently there is little routine implementation of open-science practices in ESM research. In this article, we discuss the ways in which ESM research is especially vulnerable to threats to transparency, reproducibility, and replicability. We propose that greater use of study registration, a cornerstone of open science, may address some of these threats to the transparency of ESM research. Registration of ESM research is not without challenges, including model selection, accounting for potential model-convergence issues, and the use of preexisting data sets. As these may prove to be significant barriers for ESM researchers, we also discuss ways of overcoming these challenges and of documenting them in a registration. A further challenge is that current general preregistration templates do not adequately capture the unique features of ESM. We present a registration template for ESM research and also discuss registration of studies using preexisting data.",
    "cited_by_count": 113,
    "openalex_id": "https://openalex.org/W4242853382",
    "type": "article"
  },
  {
    "title": "Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts",
    "doi": "https://doi.org/10.1177/2515245919885611",
    "publication_date": "2020-01-22",
    "publication_year": 2020,
    "authors": "Brenton M. Wiernik; Jeffrey A. Dahlke",
    "corresponding_authors": "Brenton M. Wiernik",
    "abstract": "Most published meta-analyses address only artifactual variance due to sampling error and ignore the role of other statistical and psychometric artifacts, such as measurement error variance (due to factors including unreliability of measurements, group misclassification, and variable treatment strength) and selection effects (including range restriction or enhancement and collider biases). These artifacts can have severe biasing effects on the results of individual studies and meta-analyses. Failing to account for these artifacts can lead to inaccurate conclusions about the mean effect size and between-studies effect-size heterogeneity, and can influence the results of meta-regression, publication-bias, and sensitivity analyses. In this article, we provide a brief introduction to the biasing effects of measurement error variance and selection effects and their relevance to a variety of research designs. We describe how to estimate the effects of these artifacts in different research designs and correct for their impacts in primary studies and meta-analyses. We consider meta-analyses of correlations, observational group differences, and experimental effects. We provide R code to implement the corrections described.",
    "cited_by_count": 111,
    "openalex_id": "https://openalex.org/W3002826573",
    "type": "article"
  },
  {
    "title": "Quantifying Support for the Null Hypothesis in Psychology: An Empirical Investigation",
    "doi": "https://doi.org/10.1177/2515245918773742",
    "publication_date": "2018-07-17",
    "publication_year": 2018,
    "authors": "Balázs Aczél; Bence Pálfi; Aba Szollosi; Márton Kovács; Barnabás Szászi; Péter Szécsi; Márk Zrubka; Quentin F. Gronau; Don van den Bergh; Eric‐Jan Wagenmakers",
    "corresponding_authors": "Eric‐Jan Wagenmakers",
    "abstract": "In the traditional statistical framework, nonsignificant results leave researchers in a state of suspended disbelief. In this study, we examined, empirically, the treatment and evidential impact of nonsignificant results. Our specific goals were twofold: to explore how psychologists interpret and communicate nonsignificant results and to assess how much these results constitute evidence in favor of the null hypothesis. First, we examined all nonsignificant findings mentioned in the abstracts of the 2015 volumes of Psychonomic Bulletin &amp; Review, Journal of Experimental Psychology: General, and Psychological Science ( N = 137). In 72% of these cases, nonsignificant results were misinterpreted, in that the authors inferred that the effect was absent. Second, a Bayes factor reanalysis revealed that fewer than 5% of the nonsignificant findings provided strong evidence (i.e., BF 01 &gt; 10) in favor of the null hypothesis over the alternative hypothesis. We recommend that researchers expand their statistical tool kit in order to correctly interpret nonsignificant results and to be able to evaluate the evidence for and against the null hypothesis.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W2883046077",
    "type": "article"
  },
  {
    "title": "The Failings of Conventional Mediation Analysis and a Design-Based Alternative",
    "doi": "https://doi.org/10.1177/25152459211047227",
    "publication_date": "2021-10-01",
    "publication_year": 2021,
    "authors": "John G. Bullock; Donald P. Green",
    "corresponding_authors": "Donald P. Green",
    "abstract": "Scholars routinely test mediation claims by using some form of measurement-of-mediation analysis whereby outcomes are regressed on treatments and mediators to assess direct and indirect effects. Indeed, it is rare for an issue of any leading journal of social or personality psychology not to include such an analysis. Statisticians have for decades criticized this method on the grounds that it relies on implausible assumptions, but these criticisms have been largely ignored. After presenting examples and simulations that dramatize the weaknesses of the measurement-of-mediation approach, we suggest that scholars instead use an approach that is rooted in experimental design. We propose implicit-mediation analysis, which adds and subtracts features of the treatment in ways that implicate some mediators and not others. We illustrate the approach with examples from recently published articles, explain the differences between the approach and other experimental approaches to mediation, and formalize the assumptions and statistical procedures that allow researchers to learn from experiments that encourage changes in mediators.",
    "cited_by_count": 102,
    "openalex_id": "https://openalex.org/W3206251524",
    "type": "article"
  },
  {
    "title": "Practical Solutions for Sharing Data and Materials From Psychological Research",
    "doi": "https://doi.org/10.1177/2515245917746500",
    "publication_date": "2018-01-10",
    "publication_year": 2018,
    "authors": "Rick O. Gilmore; Joy Kennedy; Karen E. Adolph",
    "corresponding_authors": "Rick O. Gilmore",
    "abstract": "Widespread sharing of data and materials (including displays and text- and video-based descriptions of experimental procedures) will improve the reproducibility of psychological science and accelerate the pace of discovery. In this article, we discuss some of the challenges to open sharing and offer practical solutions for researchers who wish to share more of the products-and process-of their research. Many of these solutions were devised by the Databrary.org data library for storing and sharing video, audio, and other forms of sensitive or personally identifiable data. We also discuss ways in which researchers can make shared data and materials easier for others to find and reuse. Widely adopted, these solutions and practices will increase transparency and speed progress in psychological science.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W2783892247",
    "type": "article"
  },
  {
    "title": "Why Do Some Psychology Researchers Resist Adopting Proposed Reforms to Research Practices? A Description of Researchers’ Rationales",
    "doi": "https://doi.org/10.1177/2515245918757427",
    "publication_date": "2018-03-07",
    "publication_year": 2018,
    "authors": "Anthony N. Washburn; Brittany E. Hanson; Matt Motyl; Linda J. Skitka; Caitlyn Yantis; Kendal Wong; Jiaqing Sun; JP Prims; Allison B. Mueller; Zachary J. Melton; Timothy S Carsel",
    "corresponding_authors": "Anthony N. Washburn",
    "abstract": "In response to the replication crisis, many psychologists recommended that the field adopt several proposed reforms to research practices, such as preregistration, to make research more replicable. However, how researchers have received these proposals is not well known because, to our knowledge, no systematic investigation into use of these reforms has been conducted. We wanted to learn about the rationales researchers have for not adopting the proposed reforms. We analyzed survey data of 1,035 researchers in social and personality psychology who were asked to indicate whether they thought it was acceptable to not follow four specific proposed reforms and to explain their reasoning when they thought it was acceptable to not adopt these reforms. The four reforms were preregistering hypotheses and methods, making data publicly available online, conducting formal power analyses, and reporting effect sizes. Our results suggest that (a) researchers have adopted some of the proposed reforms (e.g., reporting effect sizes) more than others (e.g., preregistering studies) and (b) rationales for not adopting them reflect a need for more discussion and education about their utility and feasibility.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W2793704342",
    "type": "article"
  },
  {
    "title": "Persons as Effect Sizes",
    "doi": "https://doi.org/10.1177/2515245920922982",
    "publication_date": "2020-10-09",
    "publication_year": 2020,
    "authors": "James W. Grice; Eliwid Medellin; Ian T. Jones; Samantha Horvath; Hailey McDaniel; Chance O’lansen; Meggie Baker",
    "corresponding_authors": "James W. Grice",
    "abstract": "Traditional indices of effect size are designed to answer questions about average group differences, associations between variables, and relative risk. For many researchers, an additional, important question is, “How many people in my study behaved or responded in a manner consistent with theoretical expectation?” We show how the answer to this question can be computed and reported as a straightforward percentage for a wide variety of study designs. This percentage essentially treats persons as an effect size, and it can easily be understood by scientists, professionals, and laypersons alike. For instance, imagine that in addition to d or η 2 , a researcher reports that 80% of participants matched theoretical expectation. No statistical training is required to understand the basic meaning of this percentage. By analyzing recently published studies, we show how computing this percentage can reveal novel patterns within data that provide insights for extending and developing the theory under investigation.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W3092133607",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Testing Pre-Data-Collection Peer Review as an Intervention to Increase Replicability",
    "doi": "https://doi.org/10.1177/2515245920958687",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Charles R. Ebersole; Maya B. Mathur; Erica Baranski; Diane-Jo Bart-Plange; Nicholas R. Buttrick; Christopher R. Chartier; Katherine S. Corker; Martin Corley; Joshua K. Hartshorne; Hans IJzerman; Ljiljana B. Lazarević; Hugh Rabagliati; Ivan Ropovik; Balázs Aczél; Lena Fanya Aeschbach; Luca Andrighetto; Jack Arnal; Holly Arrow; Peter Babinčák; Bence E. Bakos; Gabriel Baník; Ernest Baskin; Radomir Belopavlović; Michael H. Bernstein; Michał Białek; Nicholas Bloxsom; Bojana Bodroža; Diane B. V. Bonfiglio; Leanne Boucher; Florian Brühlmann; Claudia Chloe Brumbaugh; Erica Casini; Yiling Chen; Carlo Chiorri; William J. Chopik; Oliver Christ; Antonia M. Ciunci; Heather M. Claypool; Sean P. Coary; Marija V. Čolić; W. Matthew Collins; Paul Curran; Chris Day; Benjamin Dering; Anna Dreber; John E. Edlund; Filipe Falcão; Anna Fedor; Lily Feinberg; Ian Ferguson; Máire B. Ford; Michael C. Frank; Emily Fryberger; Alexander Garinther; Katarzyna Gawryluk; Kayla Ashbaugh; Mauro Giacomantonio; Steffen R. Giessner; Jon Grahe; Rosanna E. Guadagno; Ewa Hałasa; Peter Hancock; Rias A. Hilliard; Joachim Hüffmeier; Sean Hughes; Katarzyna Idzikowska; Michael Inzlicht; Alan Jern; William Jiménez‐Leal; Magnus Johannesson; Jennifer A. Joy-Gaba; Mathias Kauff; Danielle Kellier; Grecia Kessinger; Mallory C. Kidwell; Amanda M. Kimbrough; Josiah King; Vanessa S. Kolb; Sabina Kołodziej; Márton Kovács; Karolina Krasuska; Sue Kraus; Lacy E. Krueger; Katarzyna Kuchno; Caio Ambrosio Lage; Eleanor V. Langford; Carmel Levitan; Tiago Jessé Souza de Lima; Hause Lin; Samuel Lins; Jia E. Loy; Dylan Manfredi; Łukasz Markiewicz; Madhavi Menon; Brett Mercier; Mitchell M. Metzger; Venus Meyet; Ailsa E. Millen; Jeremy K. Miller; Andres Montealegre",
    "corresponding_authors": "Charles R. Ebersole",
    "abstract": "Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect ( p &lt; .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3–9; median total sample = 1,279.5, range = 276–3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols (Δ r = .002 or .014, depending on analytic approach). The median effect size for the revised protocols ( r = .05) was similar to that of the RP:P protocols ( r = .04) and the original RP:P replications ( r = .11), and smaller than that of the original studies ( r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00–.15) were 78% smaller, on average, than the original effect sizes (median r = .37, range = .19–.50).",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2936257342",
    "type": "article"
  },
  {
    "title": "The Percentile Bootstrap: A Primer With Step-by-Step Instructions in R",
    "doi": "https://doi.org/10.1177/2515245920911881",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Guillaume A. Rousselet; Cyril Pernet; Rand R. Wilcox",
    "corresponding_authors": "Guillaume A. Rousselet",
    "abstract": "The percentile bootstrap is the Swiss Army knife of statistics: It is a nonparametric method based on data-driven simulations. It can be applied to many statistical problems, as a substitute to standard parametric approaches, or in situations for which parametric methods do not exist. In this Tutorial, we cover R code to implement the percentile bootstrap to make inferences about central tendency (e.g., means and trimmed means) and spread in a one-sample example and in an example comparing two independent groups. For each example, we explain how to derive a bootstrap distribution and how to get a confidence interval and a p value from that distribution. We also demonstrate how to run a simulation to assess the behavior of the bootstrap. For some purposes, such as making inferences about the mean, the bootstrap performs poorly. But for other purposes, it is the only known method that works well over a broad range of situations. More broadly, combining the percentile bootstrap with robust estimators (i.e., estimators that are not overly sensitive to outliers) can help users gain a deeper understanding of their data than they would using conventional methods.",
    "cited_by_count": 82,
    "openalex_id": "https://openalex.org/W2999655107",
    "type": "article"
  },
  {
    "title": "A Primer on Bayesian Model-Averaged Meta-Analysis",
    "doi": "https://doi.org/10.1177/25152459211031256",
    "publication_date": "2021-07-01",
    "publication_year": 2021,
    "authors": "Quentin F. Gronau; Daniel W. Heck; Sophie Wilhelmina Berkhout; Julia M. Haaf; Eric‐Jan Wagenmakers",
    "corresponding_authors": "Quentin F. Gronau",
    "abstract": "Meta-analysis is the predominant approach for quantitatively synthesizing a set of studies. If the studies themselves are of high quality, meta-analysis can provide valuable insights into the current scientific state of knowledge about a particular phenomenon. In psychological science, the most common approach is to conduct frequentist meta-analysis. In this primer, we discuss an alternative method, Bayesian model-averaged meta-analysis. This procedure combines the results of four Bayesian meta-analysis models: (a) fixed-effect null hypothesis, (b) fixed-effect alternative hypothesis, (c) random-effects null hypothesis, and (d) random-effects alternative hypothesis. These models are combined according to their plausibilities given the observed data to address the two key questions “Is the overall effect nonzero?” and “Is there between-study variability in effect size?” Bayesian model-averaged meta-analysis therefore avoids the need to select either a fixed-effect or random-effects model and instead takes into account model uncertainty in a principled manner.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W3162770616",
    "type": "article"
  },
  {
    "title": "Justify Your Alpha: A Primer on Two Practical Approaches",
    "doi": "https://doi.org/10.1177/25152459221080396",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Maximilian Maier; Daniël Lakens",
    "corresponding_authors": "Daniël Lakens",
    "abstract": "The default use of an alpha level of .05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power, p values lower than the alpha level can be more likely when the null hypothesis is true than when the alternative hypothesis is true (i.e., Lindley’s paradox). In this article, we explain two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of .05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley’s paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors) but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that are better justified should improve statistical inferences and can increase the efficiency and informativeness of scientific research.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W4281613210",
    "type": "article"
  },
  {
    "title": "Adjusting for Publication Bias in JASP and R: Selection Models, PET-PEESE, and Robust Bayesian Meta-Analysis",
    "doi": "https://doi.org/10.1177/25152459221109259",
    "publication_date": "2022-07-01",
    "publication_year": 2022,
    "authors": "František Bartoš; Maximilian Maier; Daniel Quintana; Eric‐Jan Wagenmakers",
    "corresponding_authors": "František Bartoš",
    "abstract": "Meta-analyses are essential for cumulative science, but their validity can be compromised by publication bias. To mitigate the impact of publication bias, one may apply publication-bias-adjustment techniques such as precision-effect test and precision-effect estimate with standard errors (PET-PEESE) and selection models. These methods, implemented in JASP and R, allow researchers without programming experience to conduct state-of-the-art publication-bias-adjusted meta-analysis. In this tutorial, we demonstrate how to conduct a publication-bias-adjusted meta-analysis in JASP and R and interpret the results. First, we explain two frequentist bias-correction methods: PET-PEESE and selection models. Second, we introduce robust Bayesian meta-analysis, a Bayesian approach that simultaneously considers both PET-PEESE and selection models. We illustrate the methodology on an example data set, provide an instructional video ( https://bit.ly/pubbias ) and an R-markdown script ( https://osf.io/uhaew/ ), and discuss the interpretation of the results. Finally, we include concrete guidance on reporting the meta-analytic results in an academic article.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W4294203167",
    "type": "article"
  },
  {
    "title": "A Multilab Study of Bilingual Infants: Exploring the Preference for Infant-Directed Speech",
    "doi": "https://doi.org/10.1177/2515245920974622",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Krista Byers‐Heinlein; Angeline Tsui; Christina Bergmann; Alexis K. Black; Anna Brown; M. Julia Carbajal; Samantha Durrant; Christopher T. Fennell; Anne-Caroline Fiévet; Michael C. Frank; Anja Gampe; Judit Gervain; Nayeli Gonzalez‐Gomez; J. Kiley Hamlin; Naomi Havron; Mikołaj Hernik; Shila Kerr; Hilary Killam; Kelsey Klassen; Jessica Elizabeth Kosie; Ágnes Melinda Kovács; Casey Lew‐Williams; Liquan Liu; Nivedita Mani; Caterina Marino; Meghan Mastroberardino; Victoria Mateu; Claire Noble; Adriel John Orena; Linda Polka; Christine Potter; Melanie S. Schreiner; Leher Singh; Mélanie Söderström; Megha Sundara; Connor Waddell; Janet F. Werker; Stephanie Wermelinger",
    "corresponding_authors": "Krista Byers‐Heinlein",
    "abstract": "From the earliest months of life, infants prefer listening to and learn better from infant-directed speech (IDS) than adult-directed speech (ADS). Yet, IDS differs within communities, across languages, and across cultures, both in form and in prevalence. This large-scale, multi-site study used the diversity of bilingual infant experiences to explore the impact of different types of linguistic experience on infants' IDS preference. As part of the multi-lab ManyBabies 1 project, we compared lab-matched samples of 333 bilingual and 385 monolingual infants' preference for North-American English IDS (cf. ManyBabies Consortium, 2020: ManyBabies 1), tested in 17 labs in 7 countries. Those infants were tested in two age groups: 6-9 months (the younger sample) and 12-15 months (the older sample). We found that bilingual and monolingual infants both preferred IDS to ADS, and did not differ in terms of the overall magnitude of this preference. However, amongst bilingual infants who were acquiring North-American English (NAE) as a native language, greater exposure to NAE was associated with a stronger IDS preference, extending the previous finding from ManyBabies 1 that monolinguals learning NAE as a native language showed a stronger preference than infants unexposed to NAE. Together, our findings indicate that IDS preference likely makes a similar contribution to monolingual and bilingual development, and that infants are exquisitely sensitive to the nature and frequency of different types of language input in their early environments.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W3046471626",
    "type": "article"
  },
  {
    "title": "Precise Answers to Vague Questions: Issues With Interactions",
    "doi": "https://doi.org/10.1177/25152459211007368",
    "publication_date": "2021-04-01",
    "publication_year": 2021,
    "authors": "Julia M. Rohrer; Ruben C. Arslan",
    "corresponding_authors": "Ruben C. Arslan",
    "abstract": "Psychological theories often invoke interactions but remain vague regarding the details. As a consequence, researchers may not know how to properly test them and may potentially run analyses that reliably return the wrong answer to their research question. We discuss three major issues regarding the prediction and interpretation of interactions. First, interactions can be removable in the sense that they appear or disappear depending on scaling decisions, with consequences for a variety of situations (e.g., binary or categorical outcomes, bounded scales with floor and ceiling effects). Second, interactions may be conceptualized as changes in slope or changes in correlations, and because these two phenomena do not necessarily coincide, researchers might draw wrong conclusions. Third, interactions may or may not be causally identified, and this determines which interpretations are valid. Researchers who remain unaware of these distinctions might accidentally analyze their data in a manner that returns the technically correct answer to the wrong question. We illustrate all issues with examples from psychology and issue recommendations for how to best address them in a productive manner.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W3118373570",
    "type": "article"
  },
  {
    "title": "A Conceptual Framework for Investigating and Mitigating Machine-Learning Measurement Bias (MLMB) in Psychological Assessment",
    "doi": "https://doi.org/10.1177/25152459211061337",
    "publication_date": "2022-01-01",
    "publication_year": 2022,
    "authors": "Louis Tay; Sang Eun Woo; Louis Hickman; Brandon M. Booth; Sidney K. D’Mello",
    "corresponding_authors": "Louis Tay",
    "abstract": "Given significant concerns about fairness and bias in the use of artificial intelligence (AI) and machine learning (ML) for psychological assessment, we provide a conceptual framework for investigating and mitigating machine-learning measurement bias (MLMB) from a psychometric perspective. MLMB is defined as differential functioning of the trained ML model between subgroups. MLMB manifests empirically when a trained ML model produces different predicted score levels for different subgroups (e.g., race, gender) despite them having the same ground-truth levels for the underlying construct of interest (e.g., personality) and/or when the model yields differential predictive accuracies across the subgroups. Because the development of ML models involves both data and algorithms, both biased data and algorithm-training bias are potential sources of MLMB. Data bias can occur in the form of nonequivalence between subgroups in the ground truth, platform-based construct, behavioral expression, and/or feature computing. Algorithm-training bias can occur when algorithms are developed with nonequivalence in the relation between extracted features and ground truth (i.e., algorithm features are differentially used, weighted, or transformed between subgroups). We explain how these potential sources of bias may manifest during ML model development and share initial ideas for mitigating them, including recognizing that new statistical and algorithmic procedures need to be developed. We also discuss how this framework clarifies MLMB but does not reduce the complexity of the issue.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W4210341086",
    "type": "article"
  },
  {
    "title": "A Guide for Calculating Study-Level Statistical Power for Meta-Analyses",
    "doi": "https://doi.org/10.1177/25152459221147260",
    "publication_date": "2023-01-01",
    "publication_year": 2023,
    "authors": "Daniel Quintana",
    "corresponding_authors": "Daniel Quintana",
    "abstract": "Meta-analysis is a popular approach in the psychological sciences for synthesizing data across studies. However, the credibility of meta-analysis outcomes depends on the evidential value of studies included in the body of evidence used for data synthesis. One important consideration for determining a study’s evidential value is the statistical power of the study’s design/statistical test combination for detecting hypothetical effect sizes of interest. Studies with a design/test combination that cannot reliably detect a wide range of effect sizes are more susceptible to questionable research practices and exaggerated effect sizes. Therefore, determining the statistical power for design/test combinations for studies included in meta-analyses can help researchers make decisions regarding confidence in the body of evidence. Because the one true population effect size is unknown when hypothesis testing, an alternative approach is to determine statistical power for a range of hypothetical effect sizes. This tutorial introduces the metameta R package and web app, which facilitates the straightforward calculation and visualization of study-level statistical power in meta-analyses for a range of hypothetical effect sizes. Readers will be shown how to reanalyze data using information typically presented in meta-analysis forest plots or tables and how to integrate the metameta package when reporting novel meta-analyses. A step-by-step companion screencast video tutorial is also provided to assist readers using the R package.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W4361802251",
    "type": "article"
  },
  {
    "title": "Psychology Is a Property of Persons, Not Averages or Distributions: Confronting the Group-to-Person Generalizability Problem in Experimental Psychology",
    "doi": "https://doi.org/10.1177/25152459231186615",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Ryan M. McManus; Liane Young; Joseph Sweetman",
    "corresponding_authors": "Ryan M. McManus",
    "abstract": "When experimental psychologists make a claim (e.g., “Participants judged X as morally worse than Y”), how many participants are represented? Such claims are often based exclusively on group-level analyses; here, psychologists often fail to report or perhaps even investigate how many participants judged X as morally worse than Y. More troubling, group-level analyses do not necessarily generalize to the person level: “the group-to-person generalizability problem.” We first argue for the necessity of designing experiments that allow investigation of whether claims represent most participants. Second, we report findings that in a survey of researchers (and laypeople), most interpret claims based on group-level effects as being intended to represent most participants in a study. Most believe this ought to be the case if a claim is used to support a general, person-level psychological theory. Third, building on prior approaches, we document claims in the experimental-psychology literature, derived from sets of typical group-level analyses, that describe only a (sometimes tiny) minority of participants. Fourth, we reason through an example from our own research to illustrate this group-to-person generalizability problem. In addition, we demonstrate how claims from sets of simulated group-level effects can emerge without a single participant’s responses matching these patterns. Fifth, we conduct four experiments that rule out several methodology-based noise explanations of the problem. Finally, we propose a set of simple and flexible options to help researchers confront the group-to-person generalizability problem in their own work.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W4386494545",
    "type": "article"
  },
  {
    "title": "Bayesian Repeated-Measures Analysis of Variance: An Updated Methodology Implemented in JASP",
    "doi": "https://doi.org/10.1177/25152459231168024",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Don van den Bergh; Eric‐Jan Wagenmakers; Frederik Aust",
    "corresponding_authors": "Don van den Bergh",
    "abstract": "Analysis of variance (ANOVA) is widely used to assess the influence of one or more experimental (or quasi-experimental) manipulations on a continuous outcome. Traditionally, ANOVA is carried out in a frequentist manner using p values, but a Bayesian alternative has been proposed. Assuming that the proposed Bayesian ANOVA is closely modeled after its frequentist counterpart, one may be surprised to find that the two can yield very different conclusions when the design involves multiple repeated-measures factors. We illustrate such a discrepancy with a real data set from a two-factorial within-subject experiment. For this data set, the results of a frequentist and Bayesian ANOVA are in a disagreement about which main effect accounts for the variance in the data. The reason for this disagreement is that frequentist and the proposed Bayesian ANOVA use different model specifications. As currently implemented, the proposed Bayesian ANOVA assumes that there are no individual differences in the magnitude of effects. We suspect that this assumption is neither obvious to nor desired by most analysts because it is untenable in most applications. We argue here that the Bayesian ANOVA should be revised to allow for individual differences. As a default, we suggest the standard frequentist model specification but discuss a recently proposed alternative and provide guidance on how to choose the appropriate model specification. We end by discussing the implications of the revised model specification for previously published results of Bayesian ANOVAs.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W4381892551",
    "type": "article"
  },
  {
    "title": "Bayesian Analysis of Cross-Sectional Networks: A Tutorial in R and JASP",
    "doi": "https://doi.org/10.1177/25152459231193334",
    "publication_date": "2023-10-01",
    "publication_year": 2023,
    "authors": "Karoline Huth; Jill de Ron; Anna E. Goudriaan; Judy Luigjes; Reza Mohammadi; Ruth J. van Holst; Eric‐Jan Wagenmakers; Maarten Marsman",
    "corresponding_authors": "Karoline Huth",
    "abstract": "Network psychometrics is a new direction in psychological research that conceptualizes psychological constructs as systems of interacting variables. In network analysis, variables are represented as nodes, and their interactions yield (partial) associations. Current estimation methods mostly use a frequentist approach, which does not allow for proper uncertainty quantification of the model and its parameters. Here, we outline a Bayesian approach to network analysis that offers three main benefits. In particular, applied researchers can use Bayesian methods to (1) determine structure uncertainty, (2) obtain evidence for edge inclusion and exclusion (i.e., distinguish conditional dependence or independence between variables), and (3) quantify parameter precision. In this article, we provide a conceptual introduction to Bayesian inference, describe how researchers can facilitate the three benefits for networks, and review the available R packages. In addition, we present two user-friendly software solutions: a new R package, easybgm, for fitting, extracting, and visualizing the Bayesian analysis of networks and a graphical user interface implementation in JASP. The methodology is illustrated with a worked-out example of a network of personality traits and mental health.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W4387935915",
    "type": "article"
  },
  {
    "title": "A Primer on Structural Equation Model Diagrams and Directed Acyclic Graphs: When and How to Use Each in Psychological and Epidemiological Research",
    "doi": "https://doi.org/10.1177/25152459231156085",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Zachary J. Kunicki; Meghan L. Smith; Eleanor J. Murray",
    "corresponding_authors": "Zachary J. Kunicki",
    "abstract": "Many psychological researchers use some form of a visual diagram in their research processes. Model diagrams used with structural equation models (SEMs) and causal directed acyclic graphs (DAGs) can guide causal-inference research. SEM diagrams and DAGs share visual similarities, often leading researchers familiar with one to wonder how the other differs. This article is intended to serve as a guide for researchers in the psychological sciences and psychiatric epidemiology on the distinctions between these methods. We offer high-level overviews of SEMs and causal DAGs using a guiding example. We then compare and contrast the two methodologies and describe when each would be used. In brief, SEM diagrams are both a conceptual and statistical tool in which a model is drawn and then tested, whereas causal DAGs are exclusively conceptual tools used to help guide researchers in developing an analytic strategy and interpreting results. Causal DAGs are explicitly tools for causal inference, whereas the results of a SEM are only sometimes interpreted causally. A DAG may be thought of as a “qualitative schematic” for some SEMs, whereas SEMs may be thought of as an “algebraic system” for a causal DAG. As psychology begins to adopt more causal-modeling concepts and psychiatric epidemiology begins to adopt more latent-variable concepts, the ability of researchers to understand and possibly combine both of these tools is valuable. Using an applied example, we provide sample analyses, code, and write-ups for both SEM and causal DAG approaches.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4365460377",
    "type": "article"
  },
  {
    "title": "How to Safely Reassess Variability and Adapt Sample Size? A Primer for the Independent Samples <i>t</i> Test",
    "doi": "https://doi.org/10.1177/25152459231212128",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Lara Vankelecom; Tom Loeys; Beatrijs Moerkerke",
    "corresponding_authors": "Lara Vankelecom",
    "abstract": "When researchers aim to test hypotheses, setting up adequately powered studies is crucial to avoid missing important effects and to increase the probability that published significant effects reflect true effects. Without a priori good knowledge about the population effect size and variability, power analyses may underestimate the true required sample size. However, a specific type of a two-stage adaptive design in which the sample size can be reestimated during the data collection might partially mitigate the problem. In the design proposed in this article, the variability of the data collected at the first stage is estimated and then used to reassess the originally planned sample size of the study while the unstandardized effect size is fixed at a smallest effect size of interest. In this article, we explain how to implement such a two-stage sample-size reestimation design in the setting in which interest lies in comparing means of two independent groups. We investigate through simulation the implications on the Type I error rate (T1ER) of the final independent samples t test. Inflation can be substantial when the interim variance estimate is based on a small sample. However, the T1ER approaches the nominal level when more first-stage data are collected. An R-function is provided that enables researchers to calculate for their specific study (a) the maximum T1ER inflation and (b) the adjusted [Formula: see text] level to be used in the final t test to correct for the inflation. Finally, the desired property of this design to better ensure the power of the study is verified.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4391385077",
    "type": "article"
  },
  {
    "title": "A Multilab Replication of the Induced-Compliance Paradigm of Cognitive Dissonance",
    "doi": "https://doi.org/10.1177/25152459231213375",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "David C. Vaidis; Willem W. A. Sleegers; Florian van Leeuwen; Kenneth G. DeMarree; Bjørn Sætrevik; Robert M. Ross; Kathleen Schmidt; John Protzko; Coby Morvinski; Omid Ghasemi; Andrew Roberts; Jeff Stone; Alexandre Bran; Amélie Gourdon-Kanhukamwe; Ceren Günsoy; Lisa S. Moussaoui; Andrew R. Smith; Armelle Nugier; Marie‐Pierre Fayant; Ali H. Al‐Hoorie; Obed Kwame Appiah; Spencer Arbige; Benjamin Aubert‐Teillaud; Olga Białobrzeska; Stéphanie Bordel; Valérian Boudjemadi; Hilmar Brohmer; Quinn Cabooter; Mehdi Chahir; Ianis Chassang; Armand Chatard; Y.Y. Chou; Sungeun Chung; Mioara Cristea; Joséphine Daga; Gregory John Depow; Olivier Desrichard; Dmitrii Dubrov; Thomas Rhys Evans; Séverine Falkowicz; Sylvain Ferreira; Tim Figureau; Valérie Fointiat; Théo Friedrich; Anastasia S. Gashkova; Fabien Girandola; Marine Granjon; Dmitry Grigoryev; Gül Günaydın; Şevval Güzel; Mahsa Hazrati; Mai Helmy; Ayumi Ikeda; Michael Inzlicht; Sara Jaubert; Dauren Kasanov; Mohammad Mohsen Khoddami; Taenyun Kim; Kiyoshi Kiyokawa; Rabia I. Kodapanakkal; Alexandra I. Kosachenko; Kortney Maedge; John H. Mahaney; Marie‐Amélie Martinie; Vitor N. Mascheretti; Yoriko Matsuda; Maxime Mauduy; Nicolas Mauny; Armand Metzen; Eva Moreno‐Bella; Miguel Moya; Kévin Nadarajah; Pegah Nejat; Elisabeth Norman; Irmak Olcaysoy Ökten; Asil Ali Özdoğru; Cansu Özer; Elena Padial‐Rojas; Yuri G. Pavlov; Monica Perusquía-Hernández; Dora Proost; Aleksandra Niemyjska; Odile Rohmer; Emre Selçuk; Cécile Sénémeaud; Yaniv Shani; Elena A. Shmeleva; Emmelie Simoens; Kaitlin A. Smith; Alain Somat; Hayeon Song; Fatih Sönmez; Lionel Souchet; J.J. Taylor; Ilja van Beest; Nicolas Van der Linden; Steven Verheyen; Bruno Verschuère; Kévin Vezirian; Luc Vieira",
    "corresponding_authors": "David C. Vaidis",
    "abstract": "According to cognitive-dissonance theory, performing counterattitudinal behavior produces a state of dissonance that people are motivated to resolve, usually by changing their attitude to be in line with their behavior. One of the most popular experimental paradigms used to produce such attitude change is the induced-compliance paradigm. Despite its popularity, the replication crisis in social psychology and other fields, as well as methodological limitations associated with the paradigm, raise concerns about the robustness of classic studies in this literature. We therefore conducted a multilab constructive replication of the induced-compliance paradigm based on Croyle and Cooper (Experiment 1). In a total of 39 labs from 19 countries and 14 languages, participants ( N = 4,898) were assigned to one of three conditions: writing a counterattitudinal essay under high choice, writing a counterattitudinal essay under low choice, or writing a neutral essay under high choice. The primary analyses failed to support the core hypothesis: No significant difference in attitude was observed after writing a counterattitudinal essay under high choice compared with low choice. However, we did observe a significant difference in attitude after writing a counterattitudinal essay compared with writing a neutral essay. Secondary analyses revealed the pattern of results to be robust to data exclusions, lab variability, and attitude assessment. Additional exploratory analyses were conducted to test predictions from cognitive-dissonance theory. Overall, the results call into question whether the induced-compliance paradigm provides robust evidence for cognitive dissonance.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4391548211",
    "type": "article"
  },
  {
    "title": "So You Want to Do ESM? 10 Essential Topics for Implementing the Experience-Sampling Method",
    "doi": "https://doi.org/10.1177/25152459241267912",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Jessica Fritz; Marilyn L. Piccirillo; Zachary D. Cohen; Madelyn Frumkin; Olivia J Kirtley; Julia Moeller; Andreas B. Neubauer; Lesley A. Norris; Noémi Katalin Schuurman; Evelien Snippe; Laura F. Bringmann",
    "corresponding_authors": "",
    "abstract": "The experience-sampling method (ESM) captures psychological experiences over time and in everyday contexts, thereby offering exciting potential for collecting more temporally fine-grained and ecologically valid data for psychological research. Given that rapid methodological developments make it increasingly difficult for novice ESM researchers to be well informed about standards of ESM research and to identify resources that can serve as useful starting points, we here provide a primer on 10 essential design and implementation considerations for ESM studies. Specifically, we (a) compare ESM with cross-sectional, panel, and cohort approaches and discuss considerations regarding (b) item content and phrasing; (c) choosing and formulating response options; (d) timescale (sampling scheme, sampling frequency, survey length, and study duration); (e) change properties and stationarity; (f) power and effect sizes; (g) missingness, attrition, and compliance; (h) data assessment and administration; (i) reliability; and (j) replicability and generalizability. For all 10 topics, we discuss challenges and—if available—potential solutions and provide literature that can serve as starting points for more in-depth readings. We also share access to a living, web-based resources library with a more extensive catalogue of literature to facilitate further learning about the design and implementation of ESM. Finally, we list topics that although beyond the scope of our article, can be relevant for the success of ESM studies. Taken together, our article highlights the most essential design and implementation considerations for ESM studies, aids the identification of relevant in-depth readings, and can thereby support the quality of future ESM studies.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4402906965",
    "type": "article"
  },
  {
    "title": "Natural Experiments: Missed Opportunities for Causal Inference in Psychology",
    "doi": "https://doi.org/10.1177/25152459231218610",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Michael P. Grosz; Adam Ayaita; Ruben C. Arslan; Susanne Buecker; Tobias Ebert; Paul Hünermund; Sandrine R. Müller; Sven Rieger; Alexandra Zapko–Willmes; Julia M. Rohrer",
    "corresponding_authors": "Michael P. Grosz",
    "abstract": "Knowledge about causal effects is essential for building useful theories and designing effective interventions. The preferred design for learning about causal effects is randomized experiments (i.e., studies in which the researchers randomly assign units to treatment and control conditions). However, randomized experiments are often unethical or unfeasible. On the other hand, observational studies are usually feasible but lack the random assignment that renders randomized experiments causally informative. Natural experiments can sometimes offer unique opportunities for dealing with this dilemma, allowing causal inference on the basis of events that are not controlled by researchers but that nevertheless establish random or as-if random assignment to treatment and control conditions. Yet psychological researchers have rarely exploited natural experiments. To remedy this shortage, we describe three main types of studies exploiting natural experiments (standard natural experiments, instrumental-variable designs, and regression-discontinuity designs) and provide examples from psychology and economics to illustrate how natural experiments can be harnessed. Natural experiments are challenging to find, provide information about only specific causal effects, and involve assumptions that are difficult to validate empirically. Nevertheless, we argue that natural experiments provide valuable causal-inference opportunities that have not yet been sufficiently exploited by psychologists.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4391854050",
    "type": "article"
  },
  {
    "title": "Simulation-Based Power Analyses for the Smallest Effect Size of Interest: A Confidence-Interval Approach for Minimum-Effect and Equivalence Testing",
    "doi": "https://doi.org/10.1177/25152459241240722",
    "publication_date": "2024-04-01",
    "publication_year": 2024,
    "authors": "Paul Riesthuis",
    "corresponding_authors": "Paul Riesthuis",
    "abstract": "Effect sizes are often used in psychology because they are crucial when determining the required sample size of a study and when interpreting the implications of a result. Recently, researchers have been encouraged to contextualize their effect sizes and determine what the smallest effect size is that yields theoretical or practical implications, also known as the “smallest effect size of interest” (SESOI). Having a SESOI will allow researchers to have more specific hypotheses, such as whether their findings are truly meaningful (i.e., minimum-effect testing) or whether no meaningful effect exists (i.e., equivalence testing). These types of hypotheses should be reflected in power analyses to accurately determine the required sample size. Through a confidence-interval-focused approach and simulations, I show how to conduct power analyses for minimum-effect and equivalence testing. Moreover, I show that conducting a power analysis for the SESOI might result in inconclusive results. This confidence-interval-focused simulation-based power analysis can be easily adopted to different types of research areas and designs. Last, I provide recommendations on how to conduct such simulation-based power analyses.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4394824516",
    "type": "article"
  },
  {
    "title": "Careless Responding: Why Many Findings Are Spurious or Spuriously Inflated",
    "doi": "https://doi.org/10.1177/25152459241231581",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Morgan D. Stosic; Brett A. Murphy; Fred Duong; Amber A. Fultz; Summer E. Harvey; Frank J. Bernieri",
    "corresponding_authors": "Brett A. Murphy",
    "abstract": "Contrary to long-standing conventional wisdom, failing to exclude data from carelessly responding participants on questionnaires or behavioral tasks will frequently result in false-positive or spuriously inflated findings. Despite prior publications demonstrating this disturbing statistical confound, it continues to be widely underappreciated by most psychologists, including highly experienced journal editors. In this article, we aim to comprehensively explain and demonstrate the severity and widespread prevalence of careless responding’s (CR) inflationary effects in psychological research. We first describe when and why one can expect to observe the inflationary effect of unremoved CR data in a manner accessible to early graduate or advanced undergraduate students. To this end, we provide an online simulator tool and instructional videos for use in classrooms. We then illustrate realistic magnitudes of the severity of unremoved CR data by presenting novel reanalyses of data sets from three high-profile articles: We found that many of their published effects would have been meaningfully, sometimes dramatically, inflated if they had not rigorously screened out CR data. To demonstrate the frequency with which researchers fail to adequately screen for CR, we then conduct a systematic review of CR screening procedures in studies using paid online samples (e.g., MTurk) published across two prominent psychological-science journals. These findings suggest that most researchers either did not conduct any kind of CR screening or conducted only bare minimal screening. To help researchers avoid publishing spuriously inflated findings, we summarize best practices to help mitigate the threats of CR data.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4392854292",
    "type": "article"
  },
  {
    "title": "It’s All About Timing: Exploring Different Temporal Resolutions for Analyzing Digital-Phenotyping Data",
    "doi": "https://doi.org/10.1177/25152459231202677",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Anna M Langener; Gert Stulp; Nicholas C. Jacobson; Andrea Costanzo; Raj Jagesar; Martien J. Kas; Laura F. Bringmann",
    "corresponding_authors": "Anna M Langener",
    "abstract": "The use of smartphones and wearable sensors to passively collect data on behavior has great potential for better understanding psychological well-being and mental disorders with minimal burden. However, there are important methodological challenges that may hinder the widespread adoption of these passive measures. A crucial one is the issue of timescale: The chosen temporal resolution for summarizing and analyzing the data may affect how results are interpreted. Despite its importance, the choice of temporal resolution is rarely justified. In this study, we aim to improve current standards for analyzing digital-phenotyping data by addressing the time-related decisions faced by researchers. For illustrative purposes, we use data from 10 students whose behavior (e.g., GPS, app usage) was recorded for 28 days through the Behapp application on their mobile phones. In parallel, the participants actively answered questionnaires on their phones about their mood several times a day. We provide a walk-through on how to study different timescales by doing individualized correlation analyses and random-forest prediction models. By doing so, we demonstrate how choosing different resolutions can lead to different conclusions. Therefore, we propose conducting a multiverse analysis to investigate the consequences of choosing different temporal resolutions. This will improve current standards for analyzing digital-phenotyping data and may help combat the replications crisis caused in part by researchers making implicit decisions.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4391028286",
    "type": "article"
  },
  {
    "title": "Reliability and Feasibility of Linear Mixed Models in Fully Crossed Experimental Designs",
    "doi": "https://doi.org/10.1177/25152459231214454",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Michele Scandola; Emmanuele Tidoni",
    "corresponding_authors": "Emmanuele Tidoni",
    "abstract": "The use of linear mixed models (LMMs) is increasing in psychology and neuroscience research In this article, we focus on the implementation of LMMs in fully crossed experimental designs. A key aspect of LMMs is choosing a random-effects structure according to the experimental needs. To date, opposite suggestions are present in the literature, spanning from keeping all random effects (maximal models), which produces several singularity and convergence issues, to removing random effects until the best fit is found, with the risk of inflating Type I error (reduced models). However, defining the random structure to fit a nonsingular and convergent model is not straightforward. Moreover, the lack of a standard approach may lead the researcher to make decisions that potentially inflate Type I errors. After reviewing LMMs, we introduce a step-by-step approach to avoid convergence and singularity issues and control for Type I error inflation during model reduction of fully crossed experimental designs. Specifically, we propose the use of complex random intercepts (CRIs) when maximal models are overparametrized. CRIs are multiple random intercepts that represent the residual variance of categorical fixed effects within a given grouping factor. We validated CRIs and the proposed procedure by extensive simulations and a real-case application. We demonstrate that CRIs can produce reliable results and require less computational resources. Moreover, we outline a few criteria and recommendations on how and when scholars should reduce overparametrized models. Overall, the proposed procedure provides clear solutions to avoid overinflated results using LMMs in psychology and neuroscience.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4391697246",
    "type": "article"
  },
  {
    "title": "dockerHDDM: A User-Friendly Environment for Bayesian Hierarchical Drift-Diffusion Modeling",
    "doi": "https://doi.org/10.1177/25152459241298700",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Wanke Pan; Haiyang Geng; Lei Zhang; Alexander Fengler; Michael J. Frank; Ru‐Yuan Zhang; Hu Chuan-Peng",
    "corresponding_authors": "",
    "abstract": "Drift-diffusion models (DDMs) are pivotal in understanding evidence-accumulation processes during decision-making across psychology, behavioral economics, neuroscience, and psychiatry. Hierarchical DDMs (HDDMs), a Python library for hierarchical Bayesian estimation of DDMs, has been widely used among researchers, including researchers with limited coding proficiency, in fitting DDMs and other sequential sampling models. However, issues of compatibility in installation and lack of support for more recent Bayesian-modeling functionalities pose serious challenges for new users, limiting broader adaptation and reproducibility of HDDMs. To address these issues, we created dockerHDDM, a user-friendly computational environment for HDDMs with new features. dockerHDDM brings three improvements: (a) easy to install once docker is installed, ensuring reproducibility and saving time for researchers; (b) compatible with machines with Apple chips; (c) seamless integration with ArviZ, a state-of-the-art Bayesian-modeling library. This tutorial serves as a practical, hands-on guide for researchers to leverage dockerHDDM’s capabilities in conducting efficient Bayesian hierarchical analysis of DDMs. The notebook presented here and in the docker image will enable researchers with various programming levels to model their data with HDDMs.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4407445744",
    "type": "article"
  },
  {
    "title": "Thinking Clearly About Age, Period, and Cohort Effects",
    "doi": "https://doi.org/10.1177/25152459251342750",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Julia M. Rohrer",
    "corresponding_authors": "Julia M. Rohrer",
    "abstract": "Psychological researchers are interested in how things change over time and routinely make claims about age effects (e.g., personality maturation), cohort effects (e.g., generational differences in narcissism), and sometimes, period effects (e.g., secular trends in mental health). The age-period-cohort identification problem means that these claims are not possible based on the data alone: Any possible temporal pattern can be explained by an infinite number of combinations of age, period, and cohort effects. This concern holds regardless of the study design (it also applies to longitudinal designs covering multiple cohorts) and the number of observations available (it also applies if researchers observe the whole population). Researchers usually rely on statistical models that impose constraints to pick one specific decomposition of effects. Unfortunately, these constraints often remain opaque, resulting in a lack of scrutiny of the underlying assumptions. How can researchers reason more transparently and systematically about age, period, and cohort? Here, I summarize advances in the understanding of the precise nature of the identification problem, provide an overview of ways to move forward, and highlight one approach that is particularly transparent about assumptions: bounding analysis, a framework developed by sociologists Ethan Fosse and Christopher Winship. To illustrate this approach, I analyze how age, period, and cohort affect attitudes toward working mothers in the German General Social Survey.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4411048715",
    "type": "article"
  },
  {
    "title": "Power, Dominance, and Constraint: A Note on the Appeal of Different Design Traditions",
    "doi": "https://doi.org/10.1177/2515245917745058",
    "publication_date": "2018-02-08",
    "publication_year": 2018,
    "authors": "Jeffrey N. Rouder; Julia M. Haaf",
    "corresponding_authors": "Jeffrey N. Rouder",
    "abstract": "The recent field-wide emphasis on power has brought the number of participants used in psychological experiments into focus. Social psychology typically follows a tradition in which many participants perform a small number of trials each; in psychophysics, the tradition is to include only a few participants, who perform many trials each; and the tradition in cognitive psychology falls in between, balancing the number of participants and trials. We ask whether it is better to add trials or to add participants if one wishes to increase power. The answer is straightforward—greatest power is achieved by using more people, and the gain from adding people is greater than the gain from adding trials. In light of these results, the design parameters in the social psychology tradition seem ideal. Yet there are conditions in which one may trade people for trials with only a minor decrement in power. Under these conditions, the limiting factor is the trial-to-trial variability rather than the variability across people in the population. These conditions are highly plausible, and we present a theoretical argument as to why. We think that most cognitive effects are characterized by stochastic dominance; that is, everyone’s true effect is in the same direction. For example, it is plausible that when performing the Stroop task, all people truly identify congruent colors faster than incongruent ones. When dominance holds, small mean effects imply a small degree of variability across the population. It is this degree of homogeneity, the consequence of dominance, that licenses the design parameters of the cognitive psychology and psychophysics traditions.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2787074765",
    "type": "article"
  },
  {
    "title": "Using OSF to Share Data: A Step-by-Step Guide",
    "doi": "https://doi.org/10.1177/2515245918757689",
    "publication_date": "2018-02-21",
    "publication_year": 2018,
    "authors": "Courtney K. Soderberg",
    "corresponding_authors": "Courtney K. Soderberg",
    "abstract": "Sharing data, materials, and analysis scripts with reviewers and readers is valued in psychological science. To facilitate this sharing, files should be stored in a stable location, referenced with unique identifiers, and cited in published work associated with them. This Tutorial provides a step-by-step guide to using OSF to meet the needs for sharing psychological data.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2788152258",
    "type": "article"
  },
  {
    "title": "A Practical Guide to Variable Selection in Structural Equation Modeling by Using Regularized Multiple-Indicators, Multiple-Causes Models",
    "doi": "https://doi.org/10.1177/2515245919826527",
    "publication_date": "2019-03-01",
    "publication_year": 2019,
    "authors": "Ross Jacobucci; Andreas M. Brandmaier; Rogier Kievit",
    "corresponding_authors": "Ross Jacobucci",
    "abstract": "Methodological innovations have allowed researchers to consider increasingly sophisticated statistical models that are better in line with the complexities of real-world behavioral data. However, despite these powerful new analytic approaches, sample sizes may not always be sufficiently large to deal with the increase in model complexity. This difficult modeling scenario entails large models with a limited number of observations given the number of parameters. Here, we describe a particular strategy to overcome this challenge: regularization, a method of penalizing model complexity during estimation. Regularization has proven to be a viable option for estimating parameters in this small-sample, many-predictors setting, but so far it has been used mostly in linear regression models. We show how to integrate regularization within structural equation models, a popular analytic approach in psychology. We first describe the rationale behind regularization in regression contexts and how it can be extended to regularized structural equation modeling. We then evaluate our approach using a simulation study, showing that regularized structural equation modeling outperforms traditional structural equation modeling in situations with a large number of predictors and a small sample size. Next, we illustrate the power of this approach in two empirical examples: modeling the neural determinants of visual short-term memory and identifying demographic correlates of stress, anxiety, and depression.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2998225886",
    "type": "article"
  },
  {
    "title": "Bayesian Reanalyses From Summary Statistics: A Guide for Academic Consumers",
    "doi": "https://doi.org/10.1177/2515245918779348",
    "publication_date": "2018-08-13",
    "publication_year": 2018,
    "authors": "Alexander Ly; Akash Raj; Alexander Etz; Maarten Marsman; Quentin F. Gronau; Eric‐Jan Wagenmakers",
    "corresponding_authors": "Alexander Ly",
    "abstract": "Across the social sciences, researchers have overwhelmingly used the classical statistical paradigm to draw conclusions from data, often focusing heavily on a single number: p. Recent years, however, have witnessed a surge of interest in an alternative statistical paradigm: Bayesian inference, in which probabilities are attached to parameters and models. We feel it is informative to provide statistical conclusions that go beyond a single number, and—regardless of one’s statistical preference—it can be prudent to report the results from both the classical and the Bayesian paradigms. In order to promote a more inclusive and insightful approach to statistical inference, we show how the Summary Stats module in the open-source software program JASP ( https://jasp-stats.org ) can provide comprehensive Bayesian reanalyses from just a few commonly reported summary statistics, such as t and N. These Bayesian reanalyses allow researchers—and also editors, reviewers, readers, and reporters—to (a) quantify evidence on a continuous scale using Bayes factors, (b) assess the robustness of that evidence to changes in the prior distribution, and (c) gauge which posterior parameter ranges are more credible than others by examining the posterior distribution of the effect size. The procedure is illustrated using Festinger and Carlsmith’s (1959) seminal study on cognitive dissonance.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2780361841",
    "type": "article"
  },
  {
    "title": "Crud (Re)Defined",
    "doi": "https://doi.org/10.1177/2515245920917961",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Amy Orben; Daniël Lakens",
    "corresponding_authors": "Amy Orben",
    "abstract": "The idea that in behavioral research everything correlates with everything else was a niche area of the scientific literature for more than half a century. With the increasing availability of large data sets in psychology, the “crud” factor has, however, become more relevant than ever before. When referenced in empirical work, it is often used by researchers to discount minute—but statistically significant—effects that are deemed too small to be considered meaningful. This review tracks the history of the crud factor and examines how its use in the psychological- and behavioral-science literature has developed to this day. We highlight a common and deep-seated lack of understanding about what the crud factor is and discuss whether it can be proven to exist or estimated and how it should be interpreted. This lack of understanding makes the crud factor a convenient tool for psychologists to use to disregard unwanted results, even though the presence of a crud factor should be a large inconvenience for the discipline. To inspire a concerted effort to take the crud factor more seriously, we clarify the definitions of important concepts, highlight current pitfalls, and pose questions that need to be addressed to ultimately improve understanding of the crud factor. Such work will be necessary to develop the crud factor into a useful concept encouraging improved psychological research.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W3035128275",
    "type": "article"
  },
  {
    "title": "Two Lines: A Valid Alternative to the Invalid Testing of U-Shaped Relationships With Quadratic Regressions:",
    "doi": "https://doi.org/10.25384/sage.c.4317416.v1",
    "publication_date": "2018-11-28",
    "publication_year": 2018,
    "authors": "Uri Simonsohn",
    "corresponding_authors": "Uri Simonsohn",
    "abstract": "Many psychological theories predict U-shaped relationships: The effect of x is positive for low values of x, but negative for high values, or vice versa. Despite implying merely a change of sign, h...",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2747432865",
    "type": "article"
  },
  {
    "title": "How to Automatically Document Data With the <i>codebook</i> Package to Facilitate Data Reuse",
    "doi": "https://doi.org/10.1177/2515245919838783",
    "publication_date": "2019-05-16",
    "publication_year": 2019,
    "authors": "Ruben C. Arslan",
    "corresponding_authors": "Ruben C. Arslan",
    "abstract": "Data documentation in psychology lags behind not only many other disciplines, but also basic standards of usefulness. Psychological scientists often prefer to invest the time and effort that would be necessary to document existing data well in other duties, such as writing and collecting more data. Codebooks therefore tend to be unstandardized and stored in proprietary formats, and they are rarely properly indexed in search engines. This means that rich data sets are sometimes used only once—by their creators—and left to disappear into oblivion. Even if they can find an existing data set, researchers are unlikely to publish analyses based on it if they cannot be confident that they understand it well enough. My codebook package makes it easier to generate rich metadata in human- and machine-readable codebooks. It uses metadata from existing sources and automates some tedious tasks, such as documenting psychological scales and reliabilities, summarizing descriptive statistics, and identifying patterns of missingness. The codebook R package and Web app make it possible to generate a rich codebook in a few minutes and just three clicks. Over time, its use could lead to psychological data becoming findable, accessible, interoperable, and reusable, thereby reducing research waste and benefiting both its users and the scientific community as a whole.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2946317740",
    "type": "article"
  },
  {
    "title": "Laypeople Can Predict Which Social-Science Studies Will Be Replicated Successfully",
    "doi": "https://doi.org/10.1177/2515245920919667",
    "publication_date": "2020-08-21",
    "publication_year": 2020,
    "authors": "Suzanne Hoogeveen; Alexandra Sarafoglou; Eric‐Jan Wagenmakers",
    "corresponding_authors": "",
    "abstract": "Large-scale collaborative projects recently demonstrated that several key findings from the social-science literature could not be replicated successfully. Here, we assess the extent to which a finding’s replication success relates to its intuitive plausibility. Each of 27 high-profile social-science findings was evaluated by 233 people without a Ph.D. in psychology. Results showed that these laypeople predicted replication success with above-chance accuracy (i.e., 59%). In addition, when participants were informed about the strength of evidence from the original studies, this boosted their prediction performance to 67%. We discuss the prediction patterns and apply signal detection theory to disentangle detection ability from response bias. Our study suggests that laypeople’s predictions contain useful information for assessing the probability that a given finding will be replicated successfully.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W3080884520",
    "type": "article"
  },
  {
    "title": "Establishing Construct Continua in Construct Validation: The Process of Continuum Specification",
    "doi": "https://doi.org/10.1177/2515245918775707",
    "publication_date": "2018-07-12",
    "publication_year": 2018,
    "authors": "Louis Tay; Andrew T. Jebb",
    "corresponding_authors": "Louis Tay",
    "abstract": "Many areas of psychological science rely heavily on theoretical constructs, such as personality traits, attitudes, and emotions, and many of these measured constructs are defined by a continuum that represents the different degrees of the attribute. However, these continua are not usually considered by psychologists during the process of scale development and validation. Unfortunately, this can lead to numerous scientific problems, such as incomplete measurement of the construct, difficulties in distinguishing between constructs, and compromised evidence for validity. The purpose of the current article is to propose an approach for carefully considering these issues in psychological measurement. This approach, which we term continuum specification, is a two-stage process in which the researcher defines and then properly operationalizes the target continuum. Defining the continuum involves specifying its polarity (i.e., the meaning of its poles, or ends) and the nature of its gradations (i.e., the quality that separates high from low scores). Operationalizing the continuum means using this definition to develop a measure that (a) sufficiently captures the entire continuum, (b) has appropriate response options, (c) uses correct procedures for assessing dimensionality, and (d) accounts for the underlying response process. These issues have significant implications for psychological measurement.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2872438581",
    "type": "article"
  },
  {
    "title": "Linear Discriminant Analysis for Prediction of Group Membership: A User-Friendly Primer",
    "doi": "https://doi.org/10.1177/2515245919849378",
    "publication_date": "2019-07-09",
    "publication_year": 2019,
    "authors": "Peter Boedeker; Nathan T. Kearns",
    "corresponding_authors": "Peter Boedeker",
    "abstract": "In psychology, researchers are often interested in the predictive classification of individuals. Various models exist for such a purpose, but which model is considered a best practice is conditional on attributes of the data. Under certain conditions, linear discriminant analysis (LDA) has been shown to perform better than other predictive methods, such as logistic regression, multinomial logistic regression, random forests, support-vector machines, and the K-nearest neighbor algorithm. The purpose of this Tutorial is to provide researchers who already have a basic level of statistical training with a general overview of LDA and an example of its implementation and interpretation. Decisions that must be made when conducting an LDA (e.g., prior specification, choice of cross-validation procedures) and methods of evaluating case classification (posterior probability, typicality probability) and overall classification (hit rate, Huberty’s I index) are discussed. LDA for prediction is described from a modern Bayesian perspective, as opposed to its original derivation. A step-by-step example of implementing and interpreting LDA results is provided. All analyses were conducted in R, and the script is provided; the data are available online.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W4242324542",
    "type": "article"
  },
  {
    "title": "Registered Replication Report on Fischer, Castel, Dodd, and Pratt (2003)",
    "doi": "https://doi.org/10.1177/2515245920903079",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Lincoln Colling; Dénes Szűcs; Damiano De Marco; Krzysztof Cipora; Rolf Ulrich; Hans‐Christoph Nuerk; Mojtaba Soltanlou; Donna Bryce; Sau-Chin Chen; Philipp A. Schroeder; Dion Henare; Christine K. Chrystall; Paul M. Corballis; Daniel Ansari; Celia Goffin; H. Moriah Sokolowski; Peter Hancock; Ailsa E. Millen; Steve Langton; Kevin J. Holmes; Mark S. Saviano; Tia A. Tummino; Oliver Lindemann; Rolf A. Zwaan; Jiří Lukavský; Adéla Becková; Marek Vranka; Simone Cutini; Irene C. Mammarella; Claudio Mulatti; Raoul Bell; Axel Buchner; Laura Mieth; Jan Philipp Röer; Elise Klein; Stefan Huber; Korbinian Moeller; Brenda Ocampo; Juan Lupiáñez; Javier Ortiz-Tudela; Juanma de la Fuente; Julio Santiago; Marc Ouellet; Edward M. Hubbard; Elizabeth Y. Toomarian; Remo Job; Barbara Treccani; Blakeley B. McShane",
    "corresponding_authors": "Lincoln Colling",
    "abstract": "The attentional spatial-numerical association of response codes (Att-SNARC) effect (Fischer, Castel, Dodd, &amp; Pratt, 2003)—the finding that participants are quicker to detect left-side targets when the targets are preceded by small numbers and quicker to detect right-side targets when they are preceded by large numbers—has been used as evidence for embodied number representations and to support strong claims about the link between number and space (e.g., a mental number line). We attempted to replicate Experiment 2 of Fischer et al. by collecting data from 1,105 participants at 17 labs. Across all 1,105 participants and four interstimulus-interval conditions, the proportion of times the effect we observed was positive (i.e., directionally consistent with the original effect) was .50. Further, the effects we observed both within and across labs were minuscule and incompatible with those observed by Fischer et al. Given this, we conclude that we failed to replicate the effect reported by Fischer et al. In addition, our analysis of several participant-level moderators (finger-counting habits, reading and writing direction, handedness, and mathematics fluency and mathematics anxiety) revealed no substantial moderating effects. Our results indicate that the Att-SNARC effect cannot be used as evidence to support strong claims about the link between number and space.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W3024441113",
    "type": "article"
  },
  {
    "title": "Putting Psychology to the Test: Rethinking Model Evaluation Through Benchmarking and Prediction",
    "doi": "https://doi.org/10.1177/25152459211026864",
    "publication_date": "2021-07-01",
    "publication_year": 2021,
    "authors": "Roberta Rocca; Tal Yarkoni",
    "corresponding_authors": "Roberta Rocca",
    "abstract": "Consensus on standards for evaluating models and theories is an integral part of every science. Nonetheless, in psychology, relatively little focus has been placed on defining reliable communal metrics to assess model performance. Evaluation practices are often idiosyncratic and are affected by a number of shortcomings (e.g., failure to assess models’ ability to generalize to unseen data) that make it difficult to discriminate between good and bad models. Drawing inspiration from fields such as machine learning and statistical genetics, we argue in favor of introducing common benchmarks as a means of overcoming the lack of reliable model evaluation criteria currently observed in psychology. We discuss a number of principles benchmarks should satisfy to achieve maximal utility, identify concrete steps the community could take to promote the development of such benchmarks, and address a number of potential pitfalls and concerns that may arise in the course of implementation. We argue that reaching consensus on common evaluation benchmarks will foster cumulative progress in psychology and encourage researchers to place heavier emphasis on the practical utility of scientific models.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W3170561227",
    "type": "article"
  },
  {
    "title": "ManyClasses 1: Assessing the Generalizable Effect of Immediate Feedback Versus Delayed Feedback Across Many College Classes",
    "doi": "https://doi.org/10.1177/25152459211027575",
    "publication_date": "2021-07-01",
    "publication_year": 2021,
    "authors": "Emily R. Fyfe; Joshua R. de Leeuw; Paulo F. Carvalho; Robert L. Goldstone; Janelle Sherman; David M. Admiraal; Laura Alford; Alison Bonner; Chad E. Brassil; Christopher Brooks; Tracey Carbonetto; Sau Hou Chang; Laura Cruz; Melina T. Czymoniewicz‐Klippel; Frances Daniel; M. D. Driessen; Noel Habashy; Carrie L. Hanson-Bradley; Edward R. Hirt; Virginia Hojas Carbonell; Daniel K. Jackson; Shay Jones; Jennifer L. Keagy; Brandi Keith; Sarah Malmquist; B. R. McQuarrie; Kelsey J. Metzger; Maung K. Min; Sameer Patil; Ryan S. Patrick; Etienne Pelaprat; Maureen L. Petrunich-Rutherford; Meghan R. Porter; Kristina Prescott; Cathrine Reck; Terri Renner; Eric Robbins; Adam R. Smith; Phil Stuczynski; Jaye Thompson; Nikolaos Tsotakos; Judith K. Turk; Kyle Unruh; Jennifer D. Webb; Stephanie N. Whitehead; Elaine C. Wisniewski; Ke Anne Zhang; Benjamin Motz",
    "corresponding_authors": "Emily R. Fyfe",
    "abstract": "Psychology researchers have long attempted to identify educational practices that improve student learning. However, experimental research on these practices is often conducted in laboratory contexts or in a single course, which threatens the external validity of the results. In this article, we establish an experimental paradigm for evaluating the benefits of recommended practices across a variety of authentic educational contexts—a model we call ManyClasses. The core feature is that researchers examine the same research question and measure the same experimental effect across many classes spanning a range of topics, institutions, teacher implementations, and student populations. We report the first ManyClasses study, in which we examined how the timing of feedback on class assignments, either immediate or delayed by a few days, affected subsequent performance on class assessments. Across 38 classes, the overall estimate for the effect of feedback timing was 0.002 (95% highest density interval = [−0.05, 0.05]), which indicates that there was no effect of immediate feedback compared with delayed feedback on student learning that generalizes across classes. Furthermore, there were no credibly nonzero effects for 40 preregistered moderators related to class-level and student-level characteristics. Yet our results provide hints that in certain kinds of classes, which were undersampled in the current study, there may be modest advantages for delayed feedback. More broadly, these findings provide insights regarding the feasibility of conducting within-class randomized experiments across a range of naturally occurring learning environments.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W3162001182",
    "type": "article"
  },
  {
    "title": "Making Sense of Model Generalizability: A Tutorial on Cross-Validation in R and Shiny",
    "doi": "https://doi.org/10.1177/2515245920947067",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Q. Chelsea Song; Chen Tang; Serena Wee",
    "corresponding_authors": "Q. Chelsea Song",
    "abstract": "Model generalizability describes how well the findings from a sample are applicable to other samples in the population. In this Tutorial, we explain model generalizability through the statistical concept of model overfitting and its outcome (i.e., validity shrinkage in new samples), and we use a Shiny app to simulate and visualize how model generalizability is influenced by three factors: model complexity, sample size, and effect size. We then discuss cross-validation as an approach for evaluating model generalizability and provide guidelines for implementing this approach. To help researchers understand how to apply cross-validation to their own research, we walk through an example, accompanied by step-by-step illustrations in R. This Tutorial is expected to help readers develop the basic knowledge and skills to use cross-validation to evaluate model generalizability in their research and practice.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W3136751137",
    "type": "article"
  },
  {
    "title": "Caution, Preprint! Brief Explanations Allow Nonscientists to Differentiate Between Preprints and Peer-Reviewed Journal Articles",
    "doi": "https://doi.org/10.1177/25152459211070559",
    "publication_date": "2022-01-01",
    "publication_year": 2022,
    "authors": "Tobias Wingen; Jana Berkessel; Simone Dohle",
    "corresponding_authors": "Tobias Wingen",
    "abstract": "A growing number of psychological research findings are initially published as preprints. Preprints are not peer reviewed and thus did not undergo the established scientific quality-control process. Many researchers hence worry that these preprints reach nonscientists, such as practitioners, journalists, and policymakers, who might be unable to differentiate them from the peer-reviewed literature. Across five studies in Germany and the United States, we investigated whether this concern is warranted and whether this problem can be solved by providing nonscientists with a brief explanation of preprints and the peer-review process. Studies 1 and 2 showed that without an explanation, nonscientists perceive research findings published as preprints as equally credible as findings published as peer-reviewed articles. However, an explanation of the peer-review process reduces the credibility of preprints (Studies 3 and 4). In Study 5, we developed and tested a shortened version of this explanation, which we recommend adding to preprints. This explanation again allowed nonscientists to differentiate between preprints and the peer-reviewed literature. In sum, our research demonstrates that even a short explanation of the concept of preprints and their lack of peer review allows nonscientists who evaluate scientific findings to adjust their credibility perception accordingly. This would allow harvesting the benefits of preprints, such as faster and more accessible science communication, while reducing concerns about public overconfidence in the presented findings.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W4226526865",
    "type": "article"
  },
  {
    "title": "Data Visualization Using R for Researchers Who Do Not Use R",
    "doi": "https://doi.org/10.1177/25152459221074654",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Emily Nordmann; Phil McAleer; Wilhelmiina Toivo; Helena Paterson; Lisa M. DeBruine",
    "corresponding_authors": "",
    "abstract": "In addition to benefiting reproducibility and transparency, one of the advantages of using R is that researchers have a much larger range of fully customizable data visualizations options than are typically available in point-and-click software because of the open-source nature of R. These visualization options not only look attractive but also can increase transparency about the distribution of the underlying data rather than relying on commonly used visualizations of aggregations, such as bar charts of means. In this tutorial, we provide a practical introduction to data visualization using R specifically aimed at researchers who have little to no prior experience of using R. First, we detail the rationale for using R for data visualization and introduce the “grammar of graphics” that underlies data visualization using the ggplot package. The tutorial then walks the reader through how to replicate plots that are commonly available in point-and-click software, such as histograms and box plots, and shows how the code for these “basic” plots can be easily extended to less commonly available options, such as violin box plots. The data set and code used in this tutorial and an interactive version with activity solutions, additional resources, and advanced plotting options are available at https://osf.io/bj83f/ .",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W4288766549",
    "type": "article"
  },
  {
    "title": "Using Market-Research Panels for Behavioral Science: An Overview and Tutorial",
    "doi": "https://doi.org/10.1177/25152459221140388",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Aaron J. Moss; David Hauser; Cheskie Rosenzweig; Shalom Noach Jaffe; Jonathan Robinson; Leib Litman",
    "corresponding_authors": "Leib Litman",
    "abstract": "Behavioral scientists looking to run online studies are confronted with a bevy of options. Where to recruit participants? Which tools to use for survey creation and study management? How to maintain data quality? In this tutorial, we highlight the unique capabilities of market-research panels and demonstrate how researchers can effectively sample from such panels. Unlike the microtask platforms most academics are familiar with (e.g., MTurk and Prolific), market-research panels have access to more than 100 million potential participants worldwide, provide more representative samples, and excel at demographic targeting. However, efficiently gathering data from online panels requires integration between the panel and a researcher’s survey in ways that are uncommon on microtask sites. For example, panels allow researchers to target participants according to preprofiled demographics (“Level 1” targeting, e.g., parents) and demographics that are not preprofiled but are screened for within the survey (“Level 2” targeting, e.g., parents of autistic children). In this article, we demonstrate how to sample hard-to-reach groups using market-research panels. We also describe several best practices for conducting research using online panels, including setting in-survey quotas to control sample composition and managing data quality. Our aim is to provide researchers with enough information to determine whether market-research panels are right for their research and to outline the necessary considerations for using such panels.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4380537158",
    "type": "article"
  },
  {
    "title": "Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses",
    "doi": "https://doi.org/10.1177/25152459231162567",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Stefano Coretta; Joseph V. Casillas; Simon Roessig; Michael Franke; Byron Ahn; Ali H. Al‐Hoorie; Jalal Al‐Tamimi; Najd E. Alotaibi; Mohammed K. AlShakhori; Ruth Altmiller; Pablo R. Arantes; Angeliki Athanasopoulou; Melissa M. Baese‐Berk; George Bailey; Cheman Baira A Sangma; Eleonora J. Beier; Gabriela M. Benavides; Nicole Benker; Emelia P. BensonMeyer; Nina R. Benway; Grant M. Berry; Liwen Bing; Christina Bjorndahl; Mariška Bolyanatz; Aaron Braver; Violet A. Brown; Alicia M. Brown; Alejna Brugos; Erin Michelle Buchanan; Tanna Butlin; Andrés Buxó‐Lugo; Coline Caillol; Francesco Cangemi; Christopher Carignan; Sita Carraturo; Tiphaine Caudrelier; Eleanor Chodroff; Michelle Cohn; Johanna Cronenberg; Olivier Crouzet; Erica L. Dagar; Charlotte Dawson; Carissa A. Diantoro; Marie Dokovova; Shiloh Drake; Fengting Du; Margaux Dubuis; Florent Duême; Matthew Durward; Ander Egurtzegi; Mahmoud Medhat Elsherif; Janina Esser; Emmanuel Ferragne; Fernanda Ferreira; Lauren Fink; Sara Finley; Kurtis Foster; Paul Foulkes; Rosa Franzke; Gabriel Frazer-McKee; Robert Fromont; Christina García; Jason Geller; Camille L. Grasso; Pia Greca; Martine Grice; Magdalena Grose-Hodge; Amelia Gully; Caitlin Halfacre; Ivy Hauser; Jen Hay; Robert Haywood; Sam Hellmuth; Allison Hilger; Nicole Holliday; Damar Hoogland; Yaqian Huang; Vincent Hughes; Ane Icardo Isasa; Zlatomira Ilchovska; Hae‐Sung Jeon; Jacq Jones; Mágat N. Junges; Stephanie Kaefer; Constantijn Kaland; Matthew C. Kelley; Niamh Kelly; Thomas Kettig; Ghada Khattab; Ruud Koolen; Emiel Krahmer; Dorota Krajewska; Andreas Krug; Abhilasha Ashok Kumar; Anna Lander; Tomas O. Lentz; Wanyin Li; Yanyu Li; Maria Lialiou; Ronaldo Mangueira Lima",
    "corresponding_authors": "",
    "abstract": "Recent empirical studies have highlighted the large degree of analytic flexibility in data analysis that can lead to substantially different conclusions based on the same data set. Thus, researchers have expressed their concerns that these researcher degrees of freedom might facilitate bias and can lead to claims that do not stand the test of time. Even greater flexibility is to be expected in fields in which the primary data lend themselves to a variety of possible operationalizations. The multidimensional, temporally extended nature of speech constitutes an ideal testing ground for assessing the variability in analytic approaches, which derives not only from aspects of statistical modeling but also from decisions regarding the quantification of the measured behavior. In this study, we gave the same speech-production data set to 46 teams of researchers and asked them to answer the same research question, resulting in substantial variability in reported effect sizes and their interpretation. Using Bayesian meta-analytic tools, we further found little to no evidence that the observed variability can be explained by analysts’ prior beliefs, expertise, or the perceived quality of their analyses. In light of this idiosyncratic variability, we recommend that researchers more transparently share details of their analysis, strengthen the link between theoretical construct and quantitative system, and calibrate their (un)certainty in their conclusions.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4384938832",
    "type": "article"
  },
  {
    "title": "Conducting Research With People in Lower-Socioeconomic-Status Contexts",
    "doi": "https://doi.org/10.1177/25152459231193044",
    "publication_date": "2023-10-01",
    "publication_year": 2023,
    "authors": "Lydia F. Emery; David M. Silverman; Rebecca M. Carey",
    "corresponding_authors": "Lydia F. Emery",
    "abstract": "In recent years, the field of psychology has increasingly recognized the importance of conducting research with lower-socioeconomic-status (SES) participants. Given that SES can powerfully shape people’s thoughts and actions, socioeconomically diverse samples are necessary for rigorous, generalizable research. However, even when researchers aim to collect data with these samples, they often encounter methodological and practical challenges to recruiting and retaining lower-SES participants in their studies. We propose that there are two key factors to consider when trying to recruit and retain lower-SES participants—trust and accessibility. Researchers can build trust by creating personal connections with participants and communities, paying participants fairly, and considering how participants will view their research. Researchers can enhance accessibility by recruiting in participants’ own communities, tailoring study administration to participants’ circumstances, and being flexible in payment methods. Our goal is to provide recommendations that can help to build a more inclusive science.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4388079347",
    "type": "article"
  },
  {
    "title": "Open-Science Guidance for Qualitative Research: An Empirically Validated Approach for De-Identifying Sensitive Narrative Data",
    "doi": "https://doi.org/10.1177/25152459231205832",
    "publication_date": "2023-10-01",
    "publication_year": 2023,
    "authors": "Rebecca Campbell; McKenzie Javorka; Jasmine Engleton; Kathryn Fishwick; Katie Gregory; Rachael Goodman‐Williams",
    "corresponding_authors": "Rebecca Campbell",
    "abstract": "The open-science movement seeks to make research more transparent and accessible. To that end, researchers are increasingly expected to share de-identified data with other scholars for review, reanalysis, and reuse. In psychology, open-science practices have been explored primarily within the context of quantitative data, but demands to share qualitative data are becoming more prevalent. Narrative data are far more challenging to de-identify fully, and because qualitative methods are often used in studies with marginalized, minoritized, and/or traumatized populations, data sharing may pose substantial risks for participants if their information can be later reidentified. To date, there has been little guidance in the literature on how to de-identify qualitative data. To address this gap, we developed a methodological framework for remediating sensitive narrative data. This multiphase process is modeled on common qualitative-coding strategies. The first phase includes consultations with diverse stakeholders and sources to understand reidentifiability risks and data-sharing concerns. The second phase outlines an iterative process for recognizing potentially identifiable information and constructing individualized remediation strategies through group review and consensus. The third phase includes multiple strategies for assessing the validity of the de-identification analyses (i.e., whether the remediated transcripts adequately protect participants’ privacy). We applied this framework to a set of 32 qualitative interviews with sexual-assault survivors. We provide case examples of how blurring and redaction techniques can be used to protect names, dates, locations, trauma histories, help-seeking experiences, and other information about dyadic interactions.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4389108828",
    "type": "article"
  },
  {
    "title": "Diagnosing the Misuse of the Bayes Factor in Applied Research",
    "doi": "https://doi.org/10.1177/25152459231213371",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Jorge N. Tendeiro; Henk A. L. Kiers; Rink Hoekstra; Tsz Keung Wong; Richard D. Morey",
    "corresponding_authors": "Jorge N. Tendeiro",
    "abstract": "Hypothesis testing is often used for inference in the social sciences. In particular, null hypothesis significance testing (NHST) and its p value have been ubiquitous in published research for decades. Much more recently, null hypothesis Bayesian testing (NHBT) and its Bayes factor have also started to become more commonplace in applied research. Following preliminary work by Wong and colleagues, we investigated how, and to what extent, researchers misapply the Bayes factor in applied psychological research by means of a literature study. Based on a final sample of 167 articles, our results indicate that, not unlike NHST and the [Formula: see text] value, the use of NHBT and the Bayes factor also shows signs of misconceptions. We consider the root causes of the identified problems and provide suggestions to improve the current state of affairs. This article is aimed to assist researchers in drawing the best inferences possible while using NHBT and the Bayes factor in applied research.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4391800280",
    "type": "article"
  },
  {
    "title": "The Moderating Role of Culture in the Generalizability of Psychological Phenomena",
    "doi": "https://doi.org/10.1177/25152459231225163",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Robin Schimmelpfennig; Rachel Spicer; Cindel White; Will M. Gervais; Ara Norenzayan; Steven J. Heine; Joseph Henrich; Michael Muthukrishna",
    "corresponding_authors": "Robin Schimmelpfennig",
    "abstract": "",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4391548201",
    "type": "article"
  },
  {
    "title": "The Causal Cookbook: Recipes for Propensity Scores, G-Computation, and Doubly Robust Standardization",
    "doi": "https://doi.org/10.1177/25152459241236149",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Arthur Chatton; Julia M. Rohrer",
    "corresponding_authors": "",
    "abstract": "Recent developments in the causal-inference literature have renewed psychologists’ interest in how to improve causal conclusions based on observational data. A lot of the recent writing has focused on concerns of causal identification (under which conditions is it, in principle, possible to recover causal effects?); in this primer, we turn to causal estimation (how do researchers actually turn the data into an effect estimate?) and modern approaches to it that are commonly used in epidemiology. First, we explain how causal estimands can be defined rigorously with the help of the potential-outcomes framework, and we highlight four crucial assumptions necessary for causal inference to succeed (exchangeability, positivity, consistency, and noninterference). Next, we present three types of approaches to causal estimation and compare their strengths and weaknesses: propensity-score methods (in which the independent variable is modeled as a function of controls), g-computation methods (in which the dependent variable is modeled as a function of both controls and the independent variable), and doubly robust estimators (which combine models for both independent and dependent variables). A companion R Notebook is available at github.com/ArthurChatton/CausalCookbook. We hope that this nontechnical introduction not only helps psychologists and other social scientists expand their causal toolbox but also facilitates communication across disciplinary boundaries when it comes to causal inference, a research goal common to all fields of research.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4393325638",
    "type": "article"
  },
  {
    "title": "Not Another Post Hoc Paper: A New Look at Contrast Analysis and Planned Comparisons",
    "doi": "https://doi.org/10.1177/25152459241293110",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Umberto Granziol; Maximilian M. Rabe; Marcello Gallucci; Andrea Spoto; Giulio Vidotto",
    "corresponding_authors": "",
    "abstract": "Before conducting statistical analyses, scholars and researchers often have specific hypotheses about differences between groups of means. Their hypotheses are frequently tested by applying post hoc comparisons with statistically significant simple or interaction effects. With the exception of exploratory studies, using post hoc comparisons can increase Type I error rates and decrease statistical power. A well-known solution involves planning comparisons before the study. However, the coding of such planned comparisons can be difficult to understand and implement, especially for customized comparisons and interaction effects. In this tutorial, we aim to reduce such difficulties by examining all the possible types of planned comparisons, even the customized ones, for both main and interaction effects. In this tutorial, a Shiny App coded in R and called “appRiori” is presented. appRiori is coded to help in understanding both the logic behind the planned comparisons and the way to interpret them when a model is tested. By using empirical examples on reproducible data, we explain how to code any default planned comparison executable in R. Moreover, through some features of appRiori, the customization of planned comparisons is shown, even on interaction effects, such as the possibility of creating customized contrast through click-and-drop menus. For each step, the R code related to the planned comparison is provided. Implications and fields of use of planned comparisons and appRiori are discussed.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406956292",
    "type": "article"
  },
  {
    "title": "Preregistration of Psychology Meta-Analyses: A Cross-Sectional Study of Prevalence and Practice",
    "doi": "https://doi.org/10.1177/25152459241300113",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Alejandro Sandoval‐Lentisco; Miriam Tortajada; Rubén López‐Nicolás; José A López-López; Eric‐Jan Wagenmakers; Julio Sánchez‐Meca; Tom E Hardwicke",
    "corresponding_authors": "",
    "abstract": "Meta-analyses play an influential role in synthesizing the existing evidence on a particular topic. Consequently, it is especially important that meta-analyses are conducted and reported to the highest standards and that the risk of bias is minimized. Preregistration can help detect and reduce bias arising from opportunistic use of “researcher degrees of freedom.” However, little is known about the prevalence and practice of the preregistration of meta-analyses in psychology. In this study, we first measured the prevalence of preregistration in all psychology meta-analyses published in 2021. Next, for 100 randomly selected preregistered meta-analyses, we evaluated the preregistration’s coverage of key meta-analytic decisions and the extent to which published meta-analyses deviated from their preregistered protocols. Of all 1,403 eligible psychology meta-analyses published in 2021, 382 (27%) were preregistered. In our random sample, we found that key PRISMA-P decision items were often omitted from preregistered protocols—out of the 23 decision items that were examined, the median number of items covered was 13 (interquartile range [IQR] = 11–14). We also found that all 100 preregistered meta-analyses contained at least one deviation from the preregistered protocol ( Mdn = 9, IQR = 6.75–11) and that most deviations were undisclosed ( Mdn = 8, IQR = 6–11). These findings suggest that the infrequent use and poor implementation of preregistration in psychology meta-analyses undermines its potential to reduce bias and increase transparency.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4407493597",
    "type": "article"
  },
  {
    "title": "Registered Replication Report on Srull and Wyer (1979)",
    "doi": "https://doi.org/10.1177/2515245918777487",
    "publication_date": "2018-09-01",
    "publication_year": 2018,
    "authors": "Randy J. McCarthy; John J. Skowronski; Bruno Verschuère; Ewout H. Meijer; Ariane Jim; Katherine Hoogesteyn; Robin Orthey; Oguz A. Acar; Balázs Aczél; Bence E. Bakos; Fernando Barbosa; Ernest Baskin; Laurent Bègue; Gershon Ben‐Shakhar; Angie R. Birt; Lisa Blatz; Steve D. Charman; Aline Claesen; Samuel L. Clay; Sean P. Coary; Jan Crusius; Jacqueline R. Evans; Noa Feldman; Fernando Ferreira‐Santos; Matthias Gamer; Coby Gerlsma; Sara Gomes; Marta González‐Iraizoz; Felix Holzmeister; Jürgen Huber; Rafaële J. C. Huntjens; Andrea Isoni; Ryan K. Jessup; Michael Kirchler; Nathalie klein Selle; Lina Koppel; Márton Kovács; Tei Laine; Frank Lentz; David D. Loschelder; Elliot A. Ludvig; Monty L. Lynn; Scott D. Martin; Neil McLatchie; Mario Mechtel; Galit Nahari; Asil Ali Özdoğru; Rita Pasion; Charlotte R. Pennington; Arne Roets; Nir Rozmann; Irene Scopelliti; Eli Spiegelman; Kristina Suchotzki; Angela Sutan; Péter Szécsi; Gustav Tinghög; Jean-Christian Tisserand; Ulrich S. Tran; Alain Van Hiel; Wolf Vanpaemel; Daniel Västfjäll; Thomas Verliefde; Kévin Vezirian; Martin Voracek; Lara Warmelink; Katherine Wick; Bradford J. Wiggins; Keith Wylie; Ezgi Yıldız",
    "corresponding_authors": "",
    "abstract": "Srull and Wyer (1979) demonstrated that exposing participants to more hostility-related stimuli caused them subsequently to interpret ambiguous behaviors as more hostile. In their Experiment 1, participants descrambled sets of words to form sentences. In one condition, 80% of the descrambled sentences described hostile behaviors, and in another condition, 20% described hostile behaviors. Following the descrambling task, all participants read a vignette about a man named Donald who behaved in an ambiguously hostile manner and then rated him on a set of personality traits. Next, participants rated the hostility of various ambiguously hostile behaviors (all ratings on scales from 0 to 10). Participants who descrambled mostly hostile sentences rated Donald and the ambiguous behaviors as approximately 3 scale points more hostile than did those who descrambled mostly neutral sentences. This Registered Replication Report describes the results of 26 independent replications ( N = 7,373 in the total sample; k = 22 labs and N = 5,610 in the primary analyses) of Srull and Wyer’s Experiment 1, each of which followed a preregistered and vetted protocol. A random-effects meta-analysis showed that the protagonist was seen as 0.08 scale points more hostile when participants were primed with 80% hostile sentences than when they were primed with 20% hostile sentences (95% confidence interval, CI = [0.004, 0.16]). The ambiguously hostile behaviors were seen as 0.08 points less hostile when participants were primed with 80% hostile sentences than when they were primed with 20% hostile sentences (95% CI = [−0.18, 0.01]). Although the confidence interval for one outcome excluded zero and the observed effect was in the predicted direction, these results suggest that the currently used methods do not produce an assimilative priming effect that is practically and routinely detectable.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2892027854",
    "type": "article"
  },
  {
    "title": "Reproducibility and Replicability in a Fast-Paced Methodological World",
    "doi": "https://doi.org/10.1177/2515245919847421",
    "publication_date": "2019-05-16",
    "publication_year": 2019,
    "authors": "Sacha Epskamp",
    "corresponding_authors": "Sacha Epskamp",
    "abstract": "Methodological developments and software implementations are progressing at an increasingly fast pace. The introduction and widespread acceptance of preprint archived reports and open-source software have made state-of-the-art statistical methods readily accessible to researchers. At the same time, researchers are increasingly concerned that their results should be reproducible (i.e., the same analysis should yield the same numeric results at a later time), which is a basic requirement for assessing the results’ replicability (i.e., whether results at a later time support the same conclusions). Although this age of fast-paced methodology greatly facilitates reproducibility and replicability, it also undermines them in ways not often realized by researchers. This article draws researchers’ attention to these threats and proposes guidelines to help minimize their impact. Reproducibility may be influenced by software development and change over time, a problem that is greatly compounded by the rising dependency between software packages. Replicability is affected by rapidly changing standards, researcher degrees of freedom, and possible bugs or errors in code, whether introduced by software developers or empirical researchers implementing an analysis. This article concludes with a list of recommendations to improve the reproducibility and replicability of results.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2944997256",
    "type": "article"
  },
  {
    "title": "Improving the Replicability of Psychological Science Through Pedagogy",
    "doi": "https://doi.org/10.1177/2515245917740427",
    "publication_date": "2018-01-24",
    "publication_year": 2018,
    "authors": "Robert D. Hawkins; Eric Smith; Carolyn Au; Juan Miguel Arias; Rhia Catapano; Eric Hermann; Martin Keil; Andrew K. Lampinen; Sarah Raposo; Jesse Reynolds; Shima Salehi; Justin Salloum; Jed Tan; Michael C. Frank",
    "corresponding_authors": "Michael C. Frank",
    "abstract": "Replications are important to science, but who will do them? One proposal is that students can conduct replications as part of their training. As a proof of concept for this idea, here we report a series of 11 preregistered replications of findings from the 2015 volume of Psychological Science, all conducted as part of a graduate-level course. As was expected given larger, more systematic prior efforts, the replications typically yielded effects that were smaller than the original ones: The modal outcome was partial support for the original claim. This work documents the challenges facing motivated students as they attempt to replicate previously published results on a first attempt. We describe the workflow and pedagogical methods that were used in the class and discuss implications both for the adoption of this pedagogical model and for replication research more broadly.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2610216373",
    "type": "article"
  },
  {
    "title": "Enabling Open-Science Initiatives in Clinical Psychology and Psychiatry Without Sacrificing Patients’ Privacy: Current Practices and Future Challenges",
    "doi": "https://doi.org/10.1177/2515245917749652",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Colin G. Walsh; Weiyi Xia; Muqun Li; Joshua C. Denny; Paul A. Harris; Bradley Malin",
    "corresponding_authors": "Bradley Malin",
    "abstract": "The psychological and psychiatric communities are generating data on an ever-increasing scale. To ensure that society reaps the greatest utility in research and clinical care from such rich resources, there is significant interest in wide-scale, open data sharing to foster scientific endeavors. However, it is imperative that such open-science initiatives ensure that data-privacy concerns are adequately addressed. In this article, we focus on these issues in clinical research. We review the privacy risks and then discuss how they can be mitigated through appropriate governance mechanisms that are both social (e.g., the application of data-use agreements) and technological (e.g., de-identification of structured data and unstructured narratives). We also discuss the benefits and drawbacks of these mechanisms, particularly as regards data fidelity. Our focus is on de-identification methods that meet regulatory requirements, such as the Privacy Rule of the Health Insurance Portability and Accountability Act of 1996. To illustrate their potential, we show how the principles we discuss have been applied in a large-scale clinical database and distributed research networks. We close this article with a discussion of challenges in supporting data privacy as open-science initiatives grow in their scale and complexity.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2792713421",
    "type": "article"
  },
  {
    "title": "Do Policy Statements on Media Effects Faithfully Represent the Science?",
    "doi": "https://doi.org/10.1177/2515245918811301",
    "publication_date": "2019-02-04",
    "publication_year": 2019,
    "authors": "Malte Elson; Christopher J. Ferguson; Mary Gregerson; Jerri Lynn Hogg; James Ivory; Dana Klisanin; Patrick M. Markey; Deborah L. Nichols; Shahbaz Siddiqui; June Wilson",
    "corresponding_authors": "",
    "abstract": "Professional advocacy associations such as the American Psychological Association (APA) and American Academy of Pediatrics commonly release policy statements regarding science and behavior. Policymakers and the general public may assume that such statements reflect objective conclusions, but their actual fidelity in representing science remains largely untested. For example, in recent decades, policy statements related to media effects have been released with increasing regularity. However, they have often provoked criticisms that they do not adequately reflect the state of the science on media effects. The News Media, Public Education and Public Policy Committee (a standing committee of APA’s Division 46, the Media Psychology and Technology division) reviewed all publicly available policy statements on media effects produced by professional organizations and evaluated them using a standardized rubric. It was found that current policy statements tend to be more definitive than is warranted by the underlying science, and often ignore conflicting research results. These findings have broad implications for policy statements more generally, outside the field of media effects. In general, the committee suggests that professional organizations run the risk of misinforming the public when they release policy statements that do not acknowledge debates and inconsistencies in a field, or limitations of methodology. In formulating policy statements, advocacy organizations may wish to focus less on claiming consensus and more on acknowledging areas of agreement, areas of disagreement, and limitations.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2912753128",
    "type": "article"
  },
  {
    "title": "Assessing Theoretical Conclusions With Blinded Inference to Investigate a Potential Inference Crisis",
    "doi": "https://doi.org/10.1177/2515245919869583",
    "publication_date": "2019-09-17",
    "publication_year": 2019,
    "authors": "Jeffrey J. Starns; Andrea M. Cataldo; Caren M. Rotello; Jeffrey Annis; Andrew J. Aschenbrenner; Arndt Bröder; Gregory E. Cox; Amy H. Criss; Ryan Curl; Ian G. Dobbins; John C. Dunn; Tasnuva Enam; Nathan J. Evans; Simon Farrell; Scott H. Fraundorf; Scott D. Gronlund; Andrew Heathcote; Daniel W. Heck; Jason L. Hicks; Mark J. Huff; David Kellen; Kylie N. Key; Aslı Kılıç; Karl Christoph Klauer; Kyle R. Kraemer; Fábio P. Leite; Marianne E. Lloyd; Simone Malejka; Alice Mason; Ryan M. McAdoo; Ian M. McDonough; Robert B. Michael; Laura Mickes; Eda Mızrak; D.P. Morgan; Shane T. Mueller; Adam F Osth; Angus Reynolds; Travis M. Seale‐Carlisle; Henrik Singmann; Jennifer Sloane; Andrew M. Smith; Gabriel Tillman; Don van Ravenzwaaij; Christoph T. Weidemann; Gary L. Wells; Corey N. White; Jack Harvey Wilson",
    "corresponding_authors": "Jeffrey J. Starns",
    "abstract": "Scientific advances across a range of disciplines hinge on the ability to make inferences about unobservable theoretical entities on the basis of empirical data patterns. Accurate inferences rely on both discovering valid, replicable data patterns and accurately interpreting those patterns in terms of their implications for theoretical constructs. The replication crisis in science has led to widespread efforts to improve the reliability of research findings, but comparatively little attention has been devoted to the validity of inferences based on those findings. Using an example from cognitive psychology, we demonstrate a blinded-inference paradigm for assessing the quality of theoretical inferences from data. Our results reveal substantial variability in experts’ judgments on the very same data, hinting at a possible inference crisis.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2968320970",
    "type": "article"
  },
  {
    "title": "Introduction to the Concept of Likelihood and Its Applications",
    "doi": "https://doi.org/10.1177/2515245917744314",
    "publication_date": "2018-03-01",
    "publication_year": 2018,
    "authors": "Alexander Etz",
    "corresponding_authors": "Alexander Etz",
    "abstract": "This Tutorial explains the statistical concept known as likelihood and discusses how it underlies common frequentist and Bayesian statistical methods. The article is suitable for researchers interested in understanding the basis of their statistical tools and is also intended as a resource for teachers to use in their classrooms to introduce the topic to students at a conceptual level.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2765646069",
    "type": "article"
  },
  {
    "title": "Improving Transparency, Falsifiability, and Rigor by Making Hypothesis Tests Machine-Readable",
    "doi": "https://doi.org/10.1177/2515245920970949",
    "publication_date": "2021-04-01",
    "publication_year": 2021,
    "authors": "Daniël Lakens; Lisa M. DeBruine",
    "corresponding_authors": "Daniël Lakens",
    "abstract": "Making scientific information machine-readable greatly facilitates its reuse. Many scientific articles have the goal to test a hypothesis, so making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine-readable. We believe there are two benefits to specifying a hypothesis test in such a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis tests become more transparent, falsifiable, and rigorous. Second, scientists benefit if information related to hypothesis tests in scientific articles is easily findable and reusable, for example, to perform meta-analyses, conduct peer review, and examine metascientific research questions. We examine what a machine-readable hypothesis test should look like and demonstrate the feasibility of machine-readable hypothesis tests in a real-life example using the fully operational prototype R package scienceverse.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W3159057964",
    "type": "article"
  },
  {
    "title": "Summary Plots With Adjusted Error Bars: The <i>superb</i> Framework With an Implementation in R",
    "doi": "https://doi.org/10.1177/25152459211035109",
    "publication_date": "2021-07-01",
    "publication_year": 2021,
    "authors": "Denis Cousineau; Marc-André Goulet; Bradley Harding",
    "corresponding_authors": "Denis Cousineau",
    "abstract": "Plotting the data of an experiment allows researchers to illustrate the main results of a study, show effect sizes, compare conditions, and guide interpretations. To achieve all this, it is necessary to show point estimates of the results and their precision using error bars. Often, and potentially unbeknownst to them, researchers use a type of error bars—the confidence intervals—that convey limited information. For instance, confidence intervals do not allow comparing results (a) between groups, (b) between repeated measures, (c) when participants are sampled in clusters, and (d) when the population size is finite. The use of such stand-alone error bars can lead to discrepancies between the plot’s display and the conclusions derived from statistical tests. To overcome this problem, we propose to generalize the precision of the results (the confidence intervals) by adjusting them so that they take into account the experimental design and the sampling methodology. Unfortunately, most software dedicated to statistical analyses do not offer options to adjust error bars. As a solution, we developed an open-access, open-source library for R— superb—that allows users to create summary plots with easily adjusted error bars.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W3193469532",
    "type": "article"
  },
  {
    "title": "Psychologists Should Use Brunner-Munzel’s Instead of Mann-Whitney’s <i>U</i> Test as the Default Nonparametric Procedure",
    "doi": "https://doi.org/10.1177/2515245921999602",
    "publication_date": "2021-04-01",
    "publication_year": 2021,
    "authors": "Julian D. Karch",
    "corresponding_authors": "Julian D. Karch",
    "abstract": "To investigate whether a variable tends to be larger in one population than in another, the t test is the standard procedure. In some situations, the parametric t test is inappropriate, and a nonparametric procedure should be used instead. The default nonparametric procedure is Mann-Whitney’s U test. Despite being a nonparametric test, Mann-Whitney’s test is associated with a strong assumption, known as exchangeability. I demonstrate that if exchangeability is violated, Mann-Whitney’s test can lead to wrong statistical inferences even for large samples. In addition, I argue that in psychology, exchangeability is typically not met. As a remedy, I introduce Brunner-Munzel’s test and demonstrate that it provides good Type I error rate control even if exchangeability is not met and that it has similar power as Mann-Whitney’s test. Consequently, I recommend using Brunner-Munzel’s test by default. To facilitate this, I provide advice on how to perform and report on Brunner-Munzel’s test.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W3159318434",
    "type": "article"
  },
  {
    "title": "iCatcher+: Robust and Automated Annotation of Infants’ and Young Children’s Gaze Behavior From Videos Collected in Laboratory, Field, and Online Studies",
    "doi": "https://doi.org/10.1177/25152459221147250",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Yotam Erel; Katherine Adams Shannon; Junyi Chu; Kim Scott; Melissa Kline Struhl; Peng Cao; Xincheng Tan; Peter Hart; Gal Raz; Sabrina Piccolo; Catherine Mei; Christine Potter; Sagi Jaffe‐Dax; Casey Lew‐Williams; Joshua B. Tenenbaum; Katherine Fairchild; Amit H. Bermano; Shari Liu",
    "corresponding_authors": "Yotam Erel; Shari Liu",
    "abstract": "Technological advances in psychological research have enabled large-scale studies of human behavior and streamlined pipelines for automatic processing of data. However, studies of infants and children have not fully reaped these benefits because the behaviors of interest, such as gaze duration and direction, still have to be extracted from video through a laborious process of manual annotation, even when these data are collected online. Recent advances in computer vision raise the possibility of automated annotation of these video data. In this article, we built on a system for automatic gaze annotation in young children, iCatcher, by engineering improvements and then training and testing the system (referred to hereafter as iCatcher+) on three data sets with substantial video and participant variability (214 videos collected in U.S. lab and field sites, 143 videos collected in Senegal field sites, and 265 videos collected via webcams in homes; participant age range = 4 months-3.5 years). When trained on each of these data sets, iCatcher+ performed with near human-level accuracy on held-out videos on distinguishing \"LEFT\" versus \"RIGHT\" and \"ON\" versus \"OFF\" looking behavior across all data sets. This high performance was achieved at the level of individual frames, experimental trials, and study videos; held across participant demographics (e.g., age, race/ethnicity), participant behavior (e.g., movement, head position), and video characteristics (e.g., luminance); and generalized to a fourth, entirely held-out online data set. We close by discussing next steps required to fully automate the life cycle of online infant and child behavioral studies, representing a key step toward enabling robust and high-throughput developmental research.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4366425751",
    "type": "article"
  },
  {
    "title": "Selective Hypothesis Reporting in Psychology: Comparing Preregistrations and Corresponding Publications",
    "doi": "https://doi.org/10.1177/25152459231187988",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Olmo R. van den Akker; Marcel A. L. M. van Assen; Manon Enting; Myrthe de Jonge; How Hwee Ong; Franziska Rüffer; Martijn Schoenmakers; Andrea H. Stoevenbelt; Jelte M. Wicherts; Marjan Bakker",
    "corresponding_authors": "Olmo R. van den Akker",
    "abstract": "In this study, we assessed the extent of selective hypothesis reporting in psychological research by comparing the hypotheses found in a set of 459 preregistrations with the hypotheses found in the corresponding articles. We found that more than half of the preregistered studies we assessed contained omitted hypotheses ( N = 224; 52%) or added hypotheses ( N = 227; 57%), and about one-fifth of studies contained hypotheses with a direction change ( N = 79; 18%). We found only a small number of studies with hypotheses that were demoted from primary to secondary importance ( N = 2; 1%) and no studies with hypotheses that were promoted from secondary to primary importance. In all, 60% of studies included at least one hypothesis in one or more of these categories, indicating a substantial bias in presenting and selecting hypotheses by researchers and/or reviewers/editors. Contrary to our expectations, we did not find sufficient evidence that added hypotheses and changed hypotheses were more likely to be statistically significant than nonselectively reported hypotheses. For the other types of selective hypothesis reporting, we likely did not have sufficient statistical power to test for a relationship with statistical significance. Finally, we found that replication studies were less likely to include selectively reported hypotheses than original studies. In all, selective hypothesis reporting is problematically common in psychological research. We urge researchers, reviewers, and editors to ensure that hypotheses outlined in preregistrations are clearly formulated and accurately presented in the corresponding articles.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4386851558",
    "type": "article"
  },
  {
    "title": "Calculating Repeated-Measures Meta-Analytic Effects for Continuous Outcomes: A Tutorial on Pretest–Posttest-Controlled Designs",
    "doi": "https://doi.org/10.1177/25152459231217238",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "David Skvarc; Matthew Fuller‐Tyszkiewicz",
    "corresponding_authors": "David Skvarc",
    "abstract": "Meta-analysis is a statistical technique that combines the results of multiple studies to arrive at a more robust and reliable estimate of an overall effect or estimate of the true effect. Within the context of experimental study designs, standard meta-analyses generally use between-groups differences at a single time point. This approach fails to adequately account for preexisting differences that are likely to threaten causal inference. Meta-analyses that take into account the repeated-measures nature of these data are uncommon, and so this article serves as an instructive methodology for increasing the precision of meta-analyses by attempting to estimate the repeated-measures effect sizes, with particular focus on contexts with two time points and two groups (a between-groups pretest–posttest design)—a common scenario for clinical trials and experiments. In this article, we summarize the concept of a between-groups pretest–posttest meta-analysis and its applications. We then explain the basic steps involved in conducting this meta-analysis, including the extraction of data and several alternative approaches for the calculation of effect sizes. We also highlight the importance of considering the presence of within-subjects correlations when conducting this form of meta-analysis.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4391620659",
    "type": "article"
  },
  {
    "title": "Understanding Meta-Analysis Through Data Simulation With Applications to Power Analysis",
    "doi": "https://doi.org/10.1177/25152459231209330",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Filippo Gambarota; Gianmarco Altoè",
    "corresponding_authors": "Filippo Gambarota",
    "abstract": "Meta-analysis is a powerful tool to combine evidence from existing literature. Despite several introductory and advanced materials about organizing, conducting, and reporting a meta-analysis, to our knowledge, there are no introductive materials about simulating the most common meta-analysis models. Data simulation is essential for developing and validating new statistical models and procedures. Furthermore, data simulation is a powerful educational tool for understanding a statistical method. In this tutorial, we show how to simulate equal-effects, random-effects, and metaregression models and illustrate how to estimate statistical power. Simulations for multilevel and multivariate models are available in the Supplemental Material available online. All materials associated with this article can be accessed on OSF ( https://osf.io/54djn/ ).",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4391790232",
    "type": "article"
  },
  {
    "title": "An Inception-Cohort Study Quantifying How Many Registered Studies Are Publicly Shared",
    "doi": "https://doi.org/10.1177/25152459241296031",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Eline N. F. Ensinck; Daniël Lakens",
    "corresponding_authors": "",
    "abstract": "We quantified how many studies registered on the OSF up to November 2017 are performed but not shared after at least 4 years. Examining a sample of 169 registered research studies, we found that 104 (61.5%) were publicly shared. We estimate that 5,316 out of 9,172 (58%) registered studies on the OSF are publicly shared. Researchers use registries to make unpublished studies public, and the OSF policy to open registrations after a 4-year embargo substantially increases the number of studies that become known to the scientific community. In responses to emails asking researchers why studies remained unpublished, logistical issues (e.g., lack of time, researchers changing jobs) were the most common cause, followed by null results and rejections during peer review. Our exploratory study shows that a substantial amount of studies that researchers perform remain unpublished.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406891205",
    "type": "article"
  },
  {
    "title": "ARIADNE: A Scientific Navigator to Find Your Way Through the Resource Labyrinth of Psychological Sciences",
    "doi": "https://doi.org/10.1177/25152459241297674",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Helena Hartmann; Çağatay Necati Gürsoy; Alexander Lischke; Marie Mueckstein; Matthias F. J. Sperl; Susanne Vogel; Yufang Yang; Gordon B. Feld; Alexandros Kastrinogiannis; Alina Koppold",
    "corresponding_authors": "",
    "abstract": "Performing high-quality research is a challenging endeavor, especially for early career researchers, in many fields of psychological science. Most research is characterized by experiential learning, which can be time-consuming, error-prone, and frustrating. Although most institutions provide selected resources to help researchers with their projects, these resources are often expensive, spread out, hard to find, and difficult to compare with one another in terms of reliability, validity, usability, and practicability. A comprehensive overview of resources that are useful for researchers in psychological science is missing. To address this issue, we created ARIADNE: a living and interactive resource navigator that helps to use and search a dynamically updated database of resources ( https://igor-biodgps.github.io/ARIADNE ). In this tutorial, we aim to guide researchers through a standard research project using ARIADNE along the way. The open-access database covers a growing list of resources useful for each step of a research project, from the planning and designing of a study, over the collection and analysis of the data, to the writing and disseminating of the findings. We provide (a) a step-by-step guide on how to perform a research project (in the fields of biological psychology and neuroscience as a case example but with broad application to neighboring fields) and (b) an overview of resources that are useful at different project steps. By explicitly highlighting open-access and open-source resources, we level the playing field for researchers from underprivileged countries or institutions, thereby facilitating open, fair, and reproducible research in the psychological sciences.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406981015",
    "type": "article"
  },
  {
    "title": "A Guide to Causal Inference in Life-Event Studies",
    "doi": "https://doi.org/10.1177/25152459241302015",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Mario Lawes; Stephen G. West; Michael Eid",
    "corresponding_authors": "",
    "abstract": "There is considerable interest in studying the impact of major life events (e.g., marriage, job loss) on people’s lives. This line of research is inherently causal: Its goal is to study whether life events cause changes in the examined outcomes. However, because major life events cannot be randomly assigned, studies in this area necessarily rely on longitudinal observational data. In this article, we provide guidelines for researchers interested in studying life events in an explicitly causal framework. Although focused on life-event studies for substantive context, many recommendations also apply to longitudinal observational studies more broadly. We begin by emphasizing the importance of clearly specifying the causal estimand and describe conditions in which the defined causal estimand can be identified. Then, we discuss the features and challenges of the two main analytical approaches to causal inference in life-event studies: difference-in-difference designs with a (matched) comparison group that attempt to separate event-related changes from normative changes and within-person designs that control for all time-invariant person-level confounders. We describe how the desired causal effect can be estimated in these designs and provide recommendations for when to apply each modeling strategy. In addition, we present methods for conducting sensitivity analysis, probing the robustness of the estimated causal effects, and evaluating the generalizability of the results. We conclude by describing how new specialized panel studies can be designed to examine the impact of various life events in more controlled settings.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4407778237",
    "type": "article"
  },
  {
    "title": "One App to Rule Them All: A One-Stop Calculator and Guide for 95 Effect-Size Variants for Two-Group Comparisons of Central Tendency, Variability, Overlap, Dominance, and Distributional Tails",
    "doi": "https://doi.org/10.1177/25152459251323186",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Marton L. Gyimesi; Victor Webersberger; Marco Del Giudice; Martin Voracek; Ulrich S. Tran",
    "corresponding_authors": "",
    "abstract": "The prevalence of effect-size (ES) reporting has risen significantly, yet studies comparing two groups tend to rely exclusively on the Cohen’s d family of ESs. In this article, we aim to broaden the readers’ horizon of known ESs by introducing various ES families for two-group comparisons, including indices of standardized differences in central tendency, overlap, dominance, and differences in variability and distributional tails. We describe parametric and nonparametric estimators in each ES family and present an interactive web application (R Shiny) for computing these ESs and facilitating their application. This one-stop calculator allows for the computation of 95 applications of 67 unique ESs and their confidence intervals and various plotting options and provides detailed descriptions for each ES, making it a valuable resource for both self-guided exploration and instructor-led teaching. With this comprehensive guide and its companion app, we aim to improve the clarity and accuracy of ES reporting in research design that involves two-group comparisons.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4409055931",
    "type": "article"
  },
  {
    "title": "How Not to Fool Ourselves About Heterogeneity of Treatment Effects",
    "doi": "https://doi.org/10.1177/25152459241304347",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Paul T. von Hippel; Brendan A. Schuetze",
    "corresponding_authors": "",
    "abstract": "Researchers across many fields have called for greater attention to heterogeneity of treatment effects—shifting focus from the average effect to variation in effects between different treatments, studies, or subgroups. True heterogeneity is important, but many reports of heterogeneity have proved to be false, nonreplicable, or overestimated. In this review, we catalog ways that past researchers fooled themselves about heterogeneity and recommend ways that we, as researchers, can stop fooling ourselves about heterogeneity in the future. We make 18 specific recommendations. The most common themes are to (a) seek heterogeneity only when the mechanism offers clear motivation and the data offer adequate power, (b) shy away from seeking “no-but” heterogeneity when there is no main effect, (c) separate the noise of estimation error from the signal of true heterogeneity, (d) shrink variation in estimates toward zero, (e) increase p values and widen confidence intervals when conducting multiple tests, (f) estimate interactions rather than subgroup effects, and (g) check whether findings of heterogeneity are sensitive to changes in model or measurement. We also resolve long-standing debates about centering interactions in linear models and estimating interactions in nonlinear models, such as logistic, ordinal, and interval regression. If researchers follow these recommendations, the search for heterogeneity should yield more trustworthy results in the future.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4409303753",
    "type": "article"
  },
  {
    "title": "A Primer for Evaluating Large Language Models in Social-Science Research",
    "doi": "https://doi.org/10.1177/25152459251325174",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Suhaib Abdurahman; Alireza S. Ziabari; Alexander Moore; Daniel M. Bartels; Morteza Dehghani",
    "corresponding_authors": "",
    "abstract": "Autoregressive large language models (LLMs) exhibit remarkable conversational and reasoning abilities and exceptional flexibility across a wide range of tasks. Subsequently, LLMs are being increasingly used in scientific research to analyze data, generate synthetic data, or even write scientific articles. This trend necessitates that authors follow best practices for conducting and reporting LLM research and that journal reviewers can evaluate the quality of works that use LLMs. We provide authors of social-scientific research with essential recommendations to ensure replicable and robust results using LLMs. Our recommendations also highlight considerations for reviewers, focusing on methodological rigor, replicability, and validity of results when evaluating studies that use LLMs to automate data processing or simulate human data. We offer practical advice on assessing the appropriateness of LLM applications in submitted studies, emphasizing the need for transparency in methodological reporting and the challenges posed by the nondeterministic and continuously evolving nature of these models. By providing a framework for best practices and critical review, in this primer, we aim to ensure high-quality, innovative research in the evolving landscape of social-science studies using LLMs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4409491228",
    "type": "article"
  },
  {
    "title": "Data: Sharing Is Caring",
    "doi": "https://doi.org/10.1177/2515245918758319",
    "publication_date": "2018-02-28",
    "publication_year": 2018,
    "authors": "Margaret C. Levenstein; Jared Lyle",
    "corresponding_authors": "Margaret C. Levenstein",
    "abstract": "Data sharing promotes scientific progress by permitting replication of prior scientific analyses and by increasing the return on the human and financial investments made in data collection. The costs of data sharing can be reduced through the implementation of best practices in data management across the research life cycle; this article provides specific guidance on these practices. The benefits of data sharing will be reaped when researchers who share their data are rewarded with citations and recognition of the intellectual value inherent in producing new scientific data.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2792295599",
    "type": "article"
  },
  {
    "title": "Reproducible Tables in Psychology Using the apaTables Package",
    "doi": "https://doi.org/10.1177/2515245918773743",
    "publication_date": "2018-07-31",
    "publication_year": 2018,
    "authors": "David Stanley; Jeffrey S. Spence",
    "corresponding_authors": "",
    "abstract": "Growing awareness of how susceptible research is to errors, coupled with well-documented replication failures, has caused psychological researchers to move toward open science and reproducible research. In this Tutorial, to facilitate reproducible psychological research, we present a tool that creates reproducible tables that follow the American Psychological Association’s (APA’s) style. Our tool, apaTables, automates the creation of APA-style tables for commonly used statistics and analyses in psychological research: correlations, multiple regressions (with and without blocks), standardized mean differences, N-way independent-groups analyses of variance (ANOVAs), within-subjects ANOVAs, and mixed-design ANOVAs. All tables are saved as Microsoft Word documents, so they can be readily incorporated into manuscripts without manual formatting or transcription of values.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2886602349",
    "type": "article"
  },
  {
    "title": "The Prior Odds of Testing a True Effect in Cognitive and Social Psychology",
    "doi": "https://doi.org/10.1177/2515245918767122",
    "publication_date": "2018-04-27",
    "publication_year": 2018,
    "authors": "Brent M. Wilson; John T. Wixted",
    "corresponding_authors": "",
    "abstract": "Efforts to increase replication rates in psychology generally consist of recommended improvements to methodology, such as increasing sample sizes to increase power or using a lower alpha level. However, little attention has been paid to how the prior odds ( R) that a tested effect is true can affect the probability that a significant result will be replicable. The lower R is, the less likely a published result will be replicable even if power is high. It follows that if R is lower in one set of studies than in another, then all else being equal, published results will be less replicable in the set with lower R. We illustrate this point by presenting an analysis of data from the social-psychology and cognitive-psychology studies that were included in the Open Science Collaboration’s (2015) replication project. We found that R was lower for the social-psychology studies than for the cognitive-psychology studies, which might explain why the rate of successful replications differed between these two sets of studies. This difference in replication rates may reflect the degree to which scientists in the two fields value risky but potentially groundbreaking (i.e., low- R) research. Critically, high- R research is not inherently better or worse than low- R research for advancing knowledge. However, if they wish to achieve replication rates comparable to those of high- R fields (a judgment call), researchers in low- R fields would need to use an especially low alpha level, conduct experiments that have especially high power, or both.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2802718367",
    "type": "article"
  },
  {
    "title": "Bayesian Inference and Testing Any Hypothesis You Can Specify",
    "doi": "https://doi.org/10.1177/2515245918773087",
    "publication_date": "2018-05-30",
    "publication_year": 2018,
    "authors": "Alexander Etz; Julia M. Haaf; Jeffrey N. Rouder; Joachim Vandekerckhove",
    "corresponding_authors": "Joachim Vandekerckhove",
    "abstract": "Hypothesis testing is a special form of model selection. Once a pair of competing models is fully defined, their definition immediately leads to a measure of how strongly each model supports the data. The ratio of their support is often called the likelihood ratio or the Bayes factor. Critical in the model-selection endeavor is the specification of the models. In the case of hypothesis testing, it is of the greatest importance that the researcher specify exactly what is meant by a “null” hypothesis as well as the alternative to which it is contrasted, and that these are suitable instantiations of theoretical positions. Here, we provide an overview of different instantiations of null and alternative hypotheses that can be useful in practice, but in all cases the inferential procedure is based on the same underlying method of likelihood comparison. An associated app can be found at https://osf.io/mvp53/ . This article is the work of the authors and is reformatted from the original, which was published under a CC-By Attribution 4.0 International license and is available at https://psyarxiv.com/wmf3r/ .",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2805070761",
    "type": "article"
  },
  {
    "title": "The Power of Replicated Measures to Increase Statistical Power",
    "doi": "https://doi.org/10.1177/2515245919849434",
    "publication_date": "2019-06-10",
    "publication_year": 2019,
    "authors": "Marc-André Goulet; Denis Cousineau",
    "corresponding_authors": "Marc-André Goulet",
    "abstract": "When running statistical tests, researchers can commit a Type II error, that is, fail to reject the null hypothesis when it is false. To diminish the probability of committing a Type II error (β), statistical power must be augmented. Typically, this is done by increasing sample size, as more participants provide more power. When the estimated effect size is small, however, the sample size required to achieve sufficient statistical power can be prohibitive. To alleviate this lack of power, a common practice is to measure participants multiple times under the same condition. Here, we show how to estimate statistical power by taking into account the benefit of such replicated measures. To that end, two additional parameters are required: the correlation between the multiple measures within a given condition and the number of times the measure is replicated. An analysis of a sample of 15 studies (total of 298 participants and 38,404 measurements) suggests that in simple cognitive tasks, the correlation between multiple measures is approximately .14. Although multiple measurements increase statistical power, this effect is not linear, but reaches a plateau past 20 to 50 replications (depending on the correlation). Hence, multiple measurements do not replace the added population representativeness provided by additional participants.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2952843659",
    "type": "article"
  },
  {
    "title": "Open Sharing of Data on Close Relationships and Other Sensitive Social Psychological Topics: Challenges, Tools, and Future Directions",
    "doi": "https://doi.org/10.1177/2515245917744281",
    "publication_date": "2018-01-09",
    "publication_year": 2018,
    "authors": "Samantha Joel; Paul W. Eastwick; Eli J. Finkel",
    "corresponding_authors": "Samantha Joel",
    "abstract": "This article reports on an adversarial (but friendly) collaboration examining the issues that lie at the intersection of confidentiality and open-data practices. We describe the process we followed to share our data for a speed-dating article we recently published in Psychological Science (Joel, Eastwick, &amp; Finkel, 2017) and provide a summary of the issues we considered and addressed along the way. As we drafted the present article, the third author became unsure, in retrospect, about some of the procedures we had followed, especially if our approach were to be perceived as a model for open-data decisions in other, more typical cases involving nonindependent data. This article addresses these concerns, but also identifies areas of consensus. All three authors agree that there remains an unmet need for guidelines and other resources to help researchers address the challenges of sharing data that cover sensitive topics, particularly nonindependent data collected from pairs and groups (e.g., romantic couples, work teams, therapy groups). We conclude with a discussion of new tools that could be developed to help scholars who have collected such data to increase the transparency of their research while simultaneously protecting the confidentiality of the participants.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2782848847",
    "type": "article"
  },
  {
    "title": "Improving Practices for Selecting a Subset of Important Predictors in Psychology: An Application to Predicting Pain",
    "doi": "https://doi.org/10.1177/2515245919885617",
    "publication_date": "2020-02-19",
    "publication_year": 2020,
    "authors": "Sierra A. Bainter; Thomas Granville McCauley; Tor D. Wager; Elizabeth A. Reynolds Losin",
    "corresponding_authors": "Sierra A. Bainter",
    "abstract": "Frequently, researchers in psychology are faced with the challenge of narrowing down a large set of predictors to a smaller subset. There are a variety of ways to do this, but commonly it is done by choosing predictors with the strongest bivariate correlations with the outcome. However, when predictors are correlated, bivariate relationships may not translate into multivariate relationships. Further, any attempts to control for multiple testing are likely to result in extremely low power. Here we introduce a Bayesian variable-selection procedure frequently used in other disciplines, stochastic search variable selection (SSVS). We apply this technique to choosing the best set of predictors of the perceived unpleasantness of an experimental pain stimulus from among a large group of sociocultural, psychological, and neurobiological (functional MRI) individual-difference measures. Using SSVS provides information about which variables predict the outcome, controlling for uncertainty in the other variables of the model. This approach yields new, useful information to guide the choice of relevant predictors. We have provided Web-based open-source software for performing SSVS and visualizing the results.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W3007917530",
    "type": "article"
  },
  {
    "title": "Simulation Studies as a Tool to Understand Bayes Factors",
    "doi": "https://doi.org/10.1177/2515245920972624",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Don van Ravenzwaaij; Alexander Etz",
    "corresponding_authors": "Don van Ravenzwaaij",
    "abstract": "When social scientists wish to learn about an empirical phenomenon, they perform an experiment. When they wish to learn about a complex numerical phenomenon, they can perform a simulation study. The goal of this Tutorial is twofold. First, it introduces how to set up a simulation study using the relatively simple example of simulating from the prior. Second, it demonstrates how simulation can be used to learn about the Jeffreys-Zellner-Siow (JZS) Bayes factor, a currently popular implementation of the Bayes factor employed in the BayesFactor R package and freeware program JASP. Many technical expositions on Bayes factors exist, but these may be somewhat inaccessible to researchers who are not specialized in statistics. In a step-by-step approach, this Tutorial shows how a simple simulation script can be used to approximate the calculation of the Bayes factor. We explain how a researcher can write such a sampler to approximate Bayes factors in a few lines of code, what the logic is behind the Savage-Dickey method used to visualize Bayes factors, and what the practical differences are for different choices of the prior distribution used to calculate Bayes factors.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W3139105153",
    "type": "article"
  },
  {
    "title": "A Guide to Posting and Managing Preprints",
    "doi": "https://doi.org/10.1177/25152459211019948",
    "publication_date": "2021-04-01",
    "publication_year": 2021,
    "authors": "Hannah Moshontz; Grace Binion; Haley Walton; Benjamin T. Brown; Moin Syed",
    "corresponding_authors": "Hannah Moshontz",
    "abstract": "Posting preprints online allows psychological scientists to get feedback, speed dissemination, and ensure public access to their work. This guide is designed to help psychological scientists post preprints and manage them across the publication pipeline. We review terminology, provide a historical and legal overview of preprints, and give guidance on posting and managing preprints before, during, or after the peer-review process to achieve different aims (e.g., get feedback, speed dissemination, achieve open access). We offer concrete recommendations to authors, such as post preprints that are complete and carefully proofread; post preprints in a dedicated preprint server that assigns DOIs, provides editable metadata, is indexed by GoogleScholar, supports review and endorsements, and supports version control; include a draft date and information about the paper’s status on the cover page; license preprints with CC BY licenses that permit public use with attribution; and keep preprints up to date after major revisions. Although our focus is on preprints (unpublished versions of a work), we also offer information relevant to postprints (author-formatted, post-peer-review versions of a work) and work that will not otherwise be published (e.g., theses and dissertations).",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W3162432007",
    "type": "article"
  },
  {
    "title": "Beyond Random Effects: When Small-Study Findings Are More Heterogeneous",
    "doi": "https://doi.org/10.1177/25152459221120427",
    "publication_date": "2022-10-01",
    "publication_year": 2022,
    "authors": "T. D. Stanley; Hristos Doucouliagos; John P. A. Ioannidis",
    "corresponding_authors": "T. D. Stanley",
    "abstract": "New meta-regression methods are introduced that identify whether the magnitude of heterogeneity across study findings is correlated with their standard errors. Evidence from dozens of meta-analyses finds robust evidence of this correlation and that small-sample studies typically have higher heterogeneity. This correlated heterogeneity violates the random-effects (RE) model of additive and independent heterogeneity. When small studies not only have inadequate statistical power but also high heterogeneity, their scientific contribution is even more dubious. When the heterogeneity variance is correlated with the sampling-error variance to the degree we find, simulations show that RE is dominated by an alternative weighted average, the unrestricted weighted least squares (UWLS). Meta-research evidence combined with simulations establish that UWLS should replace RE as the conventional meta-analysis summary of psychological research.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4307313709",
    "type": "article"
  },
  {
    "title": "Hybrid Experimental Designs for Intervention Development: What, Why, and How",
    "doi": "https://doi.org/10.1177/25152459221114279",
    "publication_date": "2022-07-01",
    "publication_year": 2022,
    "authors": "Inbal Nahum‐Shani; John J. Dziak; Maureen A. Walton; Walter Dempsey",
    "corresponding_authors": "Inbal Nahum‐Shani",
    "abstract": "Advances in mobile and wireless technologies offer tremendous opportunities for extending the reach and impact of psychological interventions and for adapting interventions to the unique and changing needs of individuals. However, insufficient engagement remains a critical barrier to the effectiveness of digital interventions. Human delivery of interventions (e.g., by clinical staff) can be more engaging but potentially more expensive and burdensome. Hence, the integration of digital and human-delivered components is critical to building effective and scalable psychological interventions. Existing experimental designs can be used to answer questions either about human-delivered components that are typically sequenced and adapted at relatively slow timescales (e.g., monthly) or about digital components that are typically sequenced and adapted at much faster timescales (e.g., daily). However, these methodologies do not accommodate sequencing and adaptation of components at multiple timescales and hence cannot be used to empirically inform the joint sequencing and adaptation of human-delivered and digital components. Here, we introduce the hybrid experimental design (HED)—a new experimental approach that can be used to answer scientific questions about building psychological interventions in which human-delivered and digital components are integrated and adapted at multiple timescales. We describe the key characteristics of HEDs (i.e., what they are), explain their scientific rationale (i.e., why they are needed), and provide guidelines for their design and corresponding data analysis (i.e., how can data arising from HEDs be used to inform effective and scalable psychological interventions).",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4295116042",
    "type": "article"
  },
  {
    "title": "A Practical Guide to Conversation Research: How to Study What People Say to Each Other",
    "doi": "https://doi.org/10.1177/25152459231183919",
    "publication_date": "2023-10-01",
    "publication_year": 2023,
    "authors": "Michael Yeomans; F. Katelynn Boland; Hanne K. Collins; Nicole Abi-Esber; Alison Wood Brooks",
    "corresponding_authors": "Michael Yeomans",
    "abstract": "Conversation—a verbal interaction between two or more people—is a complex, pervasive, and consequential human behavior. Conversations have been studied across many academic disciplines. However, advances in recording and analysis techniques over the last decade have allowed researchers to more directly and precisely examine conversations in natural contexts and at a larger scale than ever before, and these advances open new paths to understand humanity and the social world. Existing reviews of text analysis and conversation research have focused on text generated by a single author (e.g., product reviews, news articles, and public speeches) and thus leave open questions about the unique challenges presented by interactive conversation data (i.e., dialogue). In this article, we suggest approaches to overcome common challenges in the workflow of conversation science, including recording and transcribing conversations, structuring data (to merge turn-level and speaker-level data sets), extracting and aggregating linguistic features, estimating effects, and sharing data. This practical guide is meant to shed light on current best practices and empower more researchers to study conversations more directly—to expand the community of conversation scholars and contribute to a greater cumulative scientific understanding of the social world.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4388620701",
    "type": "article"
  },
  {
    "title": "Practices in Data-Quality Evaluation: A Large-Scale Review of Online Survey Studies Published in 2022",
    "doi": "https://doi.org/10.1177/25152459241236414",
    "publication_date": "2024-04-01",
    "publication_year": 2024,
    "authors": "Jaroslav Gottfried",
    "corresponding_authors": "Jaroslav Gottfried",
    "abstract": "In this study, I examine data-quality evaluation methods in online surveys and their frequency of use. Drawing from survey-methodology literature, I identified 11 distinct assessment categories and analyzed their prevalence across 3,298 articles published in 2022 from 200 psychology journals in the Web of Science Master Journal List. These English-language articles employed original data from self-administered online questionnaires. Strikingly, 55% of articles opted not to employ any data-quality evaluation, and 24% employed only one method despite the wide repertoire of methods available. The most common data-quality indicators were attention-control items (22%) and nonresponse rates (13%). Strict and unjustified nonresponse-based data-exclusion criteria were often observed. The results highlight a trend of inadequate quality control in online survey research, leaving results vulnerable to biases from automated response bots or respondents’ carelessness and fatigue. More thorough data-quality assurance is currently needed for online surveys.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4396883221",
    "type": "review"
  },
  {
    "title": "Validity and Transparency in Quantifying Open-Ended Data",
    "doi": "https://doi.org/10.1177/25152459241275217",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Clare Conry‐Murray; Tal Waltzer; Fiona C. DeBernardi; Jessica Fossum; Simona Haasova; Michael S. Matthews; Aoife O’Mahony; David Moreau; Myriam A. Baum; Veli‐Matti Karhulahti; Randy J. McCarthy; Helena Paterson; Kara McSweeney; Mahmoud Medhat Elsherif",
    "corresponding_authors": "",
    "abstract": "Quantitatively coding open-ended data (e.g., from videos, interviews) can be a rich source of information in psychological research, but reporting practices vary substantially. We provide strategies for improving validity and reliability of coding open-ended data and investigate questionable research practices in this area. First, we systematically examined articles in four top psychology journals ( N = 956) and found that 21% included open-ended data coded by humans. However, only about one-third of those articles reported sufficient details to replicate or evaluate the validity of the coding process. Next, we propose multiphase guidelines for transparently reporting on the quantitative coding of open-ended data, informed by concerns with replicability, content validity, and statistical validity. The first phase involves research design, including selecting data and identifying units reliably. The second phase includes developing a coding manual and training coders. The final phase outlines how to establish reliability. As part of this phase, we used data simulations to examine a common statistic for testing reliability on open-ended data, Cohen’s κ, and found that it can become inflated when researchers repeatedly test interrater reliability or manipulate categories, such as by including a missing-data category. Finally, to facilitate transparent and valid coding of open-ended data, we provide a preregistration template that reflects these guidelines. All of the guidelines and resources provided in this article can be adapted for different types of studies, depending on context.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4404401475",
    "type": "article"
  },
  {
    "title": "TaskMaster: A Tool for Determining When Subjects Are on Task",
    "doi": "https://doi.org/10.1177/2515245919838479",
    "publication_date": "2019-04-17",
    "publication_year": 2019,
    "authors": "Stephanie Permut; Matthew Fisher; Daniel M. Oppenheimer",
    "corresponding_authors": "Stephanie Permut",
    "abstract": "In this Tutorial, we introduce a tool that allows researchers to track subjects’ on- and off-task activity on Qualtrics’ online survey platform. Our TaskMaster tool uses JavaScript to create several arrayed variables representing the frequency with which subjects enter and leave an active survey window and how long they remain within a given window. We provide code and instructions that will allow researchers to both implement the TaskMaster and analyze its output. We detail several potential applications, such as in studies of persistence and cheating, and studies that require sustained attention to experimental outcomes. The TaskMaster is designed to be accessible to researchers who are comfortable designing studies in Qualtrics, but who may have limited experience using programming languages such as JavaScript.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2935892750",
    "type": "article"
  },
  {
    "title": "Minimizing Mistakes in Psychological Science",
    "doi": "https://doi.org/10.1177/2515245918801915",
    "publication_date": "2019-01-30",
    "publication_year": 2019,
    "authors": "Jeffrey N. Rouder; Julia M. Haaf; Hope K. Snyder",
    "corresponding_authors": "Jeffrey N. Rouder",
    "abstract": "Developing and implementing best practices in organizing a lab is challenging, especially in the face of new cultural norms, such as the open-science movement. Part of this challenge in today’s landscape is using new technologies, including cloud storage and computer automation. In this article, we discuss a few practices designed to increase the reliability of scientific labs, focusing on ways to minimize common, ordinary mistakes. We borrow principles from the theory of high-reliability organizations, which has been used to characterize operational practices in high-risk environments, such as aviation and health care. Guided by these principles, we focus on five strategies: (a) implementing a lab culture focused on learning from mistakes, (b) using computer automation in data and metadata collection whenever possible, (c) standardizing organizational strategies, (d) using coded rather than menu-driven analyses, and (e) developing expanded documents that record how analyses were performed.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2912512852",
    "type": "article"
  },
  {
    "title": "Failing Grade: 89% of Introduction-to-Psychology Textbooks That Define or Explain Statistical Significance Do So Incorrectly",
    "doi": "https://doi.org/10.1177/2515245919858072",
    "publication_date": "2019-06-27",
    "publication_year": 2019,
    "authors": "Scott A. Cassidy; Ralitza Dimova; Benjamin Giguère; Jeffrey S. Spence; David Stanley",
    "corresponding_authors": "Jeffrey S. Spence",
    "abstract": "Null-hypothesis significance testing (NHST) is commonly used in psychology; however, it is widely acknowledged that NHST is not well understood by either psychology professors or psychology students. In the current study, we investigated whether introduction-to-psychology textbooks accurately define and explain statistical significance. We examined 30 introductory-psychology textbooks, including the best-selling books from the United States and Canada, and found that 89% incorrectly defined or explained statistical significance. Incorrect definitions and explanations were most often consistent with the odds-against-chance fallacy. These results suggest that it is common for introduction-to-psychology students to be taught incorrect interpretations of statistical significance. We hope that our results will create awareness among authors of introductory-psychology books and provide the impetus for corrective action. To help with classroom instruction, we provide slides that correctly describe NHST and may be useful for introductory-psychology instructors.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2954642664",
    "type": "article"
  },
  {
    "title": "Web Scraping Using R",
    "doi": "https://doi.org/10.1177/2515245919859535",
    "publication_date": "2019-07-30",
    "publication_year": 2019,
    "authors": "A. Paul Bradley; Richard J. E. James",
    "corresponding_authors": "A. Paul Bradley",
    "abstract": "The ubiquitous use of the Internet in daily life means that there are now large reservoirs of data that can provide fresh insights into human behavior. One of the key barriers preventing more researchers from utilizing online data is that they do not have the skills to access the data. This Tutorial addresses this gap by providing a practical guide to scraping online data using the popular statistical language R. Web scraping is the process of automatically collecting information from websites. Such information can take the form of numbers, text, images, or videos. This Tutorial shows readers how to download web pages, extract information from those pages, store the extracted information, and do so across multiple pages of a website. A website has been created to assist readers in learning how to web-scrape. This website contains a series of examples that illustrate how to scrape a single web page and how to scrape multiple web pages. The examples are accompanied by videos describing the processes involved and by exercises to help readers increase their knowledge and practice their skills. Example R scripts have been made available at the Open Science Framework.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2965868575",
    "type": "article"
  },
  {
    "title": "Reexamining the Effect of Gustatory Disgust on Moral Judgment: A Multilab Direct Replication of Eskine, Kacinik, and Prinz (2011)",
    "doi": "https://doi.org/10.1177/2515245919881152",
    "publication_date": "2020-03-01",
    "publication_year": 2020,
    "authors": "Eric Ghelfi; Cody D. Christopherson; Heather L. Urry; Richie L. Lenne; Nicole Legate; Mary Ann Fischer; Fieke M. A. Wagemans; Brady Wiggins; Tamara Barrett; Michelle Bornstein; Bianca de Haan; Joshua Guberman; Nada Issa; Joan Kim; Elim Na; Justin OʼBrien; Aidan Paulk; Tayler Peck; Marissa Sashihara; Karen Sheelar; Justin Song; Hannah Steinberg; Dasan Sullivan",
    "corresponding_authors": "Eric Ghelfi",
    "abstract": "Eskine, Kacinik, and Prinz’s (2011) influential experiment demonstrated that gustatory disgust triggers a heightened sense of moral wrongness. We report a large-scale multisite direct replication of this study conducted by labs in the Collaborative Replications and Education Project. Subjects in each sample were randomly assigned to one of three beverage conditions: bitter (disgusting), control (neutral), or sweet. Then, subjects made a series of judgments about the moral wrongness of the behavior depicted in six vignettes. In the original study ( N = 57), drinking the bitter beverage led to higher ratings of moral wrongness than did drinking the control or sweet beverage; a contrast between the bitter condition and the other two conditions was significant among conservative ( n = 19) but not liberal ( n = 25) subjects. In the current project, random-effects meta-analyses across all subjects ( N = 1,137, k = 11 studies), conservative subjects ( n = 142, k = 5), and liberal subjects ( n = 635, k = 9) revealed standardized overall effect sizes across replications that were smaller than reported in the original study. Some were in the opposite of the predicted direction; all had 95% confidence intervals containing zero, and all were smaller than the effect size the original authors could have meaningfully detected. Results of linear mixed-effects regressions revealed that drinking the bitter beverage led to higher ratings of moral wrongness than did drinking the control beverage but not the sweet beverage. Bayes factor tests revealed greater relative support for the null than for the replication hypothesis. The overall pattern provides little to no support for the theory that physical disgust via taste perception harshens judgments of moral wrongness.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2953411324",
    "type": "article"
  },
  {
    "title": "Doing Better Data Visualization",
    "doi": "https://doi.org/10.1177/25152459211045334",
    "publication_date": "2021-10-01",
    "publication_year": 2021,
    "authors": "Eric Hehman; Sally Y Xie",
    "corresponding_authors": "",
    "abstract": "Methods in data visualization have rapidly advanced over the past decade. Although social scientists regularly need to visualize the results of their analyses, they receive little training in how to best design their visualizations. This tutorial is for individuals whose goal is to communicate patterns in data as clearly as possible to other consumers of science and is designed to be accessible to both experienced and relatively new users of R and ggplot2. In this article, we assume some basic statistical and visualization knowledge and focus on how to visualize rather than what to visualize. We distill the science and wisdom of data-visualization expertise from books, blogs, and online forum discussion threads into recommendations for social scientists looking to convey their results to other scientists. Overarching design philosophies and color decisions are discussed before giving specific examples of code in R for visualizing central tendencies, proportions, and relationships between variables.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3202884280",
    "type": "article"
  },
  {
    "title": "Citation Patterns Following a Strongly Contradictory Replication Result: Four Case Studies From Psychology",
    "doi": "https://doi.org/10.1177/25152459211040837",
    "publication_date": "2021-07-01",
    "publication_year": 2021,
    "authors": "Tom E Hardwicke; Dénes Szűcs; Robert T. Thibault; Sophia Crüwell; Olmo R. van den Akker; Michèle B. Nuijten; John P. A. Ioannidis",
    "corresponding_authors": "Tom E Hardwicke",
    "abstract": "Replication studies that contradict prior findings may facilitate scientific self-correction by triggering a reappraisal of the original studies; however, the research community’s response to replication results has not been studied systematically. One approach for gauging responses to replication results is to examine how they affect citations to original studies. In this study, we explored postreplication citation patterns in the context of four prominent multilaboratory replication attempts published in the field of psychology that strongly contradicted and outweighed prior findings. Generally, we observed a small postreplication decline in the number of favorable citations and a small increase in unfavorable citations. This indicates only modest corrective effects and implies considerable perpetuation of belief in the original findings. Replication results that strongly contradict an original finding do not necessarily nullify its credibility; however, one might at least expect the replication results to be acknowledged and explicitly debated in subsequent literature. By contrast, we found substantial citation bias: The majority of articles citing the original studies neglected to cite relevant replication results. Of those articles that did cite the replication but continued to cite the original study favorably, approximately half offered an explicit defense of the original study. Our findings suggest that even replication results that strongly contradict original findings do not necessarily prompt a corrective response from the research community.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W3200423834",
    "type": "article"
  },
  {
    "title": "Assessing Change in Intervention Research: The Benefits of Composite Outcomes",
    "doi": "https://doi.org/10.1177/2515245920931930",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "David Moreau; Kristina Wiebels",
    "corresponding_authors": "",
    "abstract": "Intervention research is often time- and resource-intensive, with numerous participants involved over extended periods of time. To maximize the value of intervention studies, multiple outcome measures are often included, either to ensure a diverse set of outcomes is being assessed or to refine assessments of specific outcomes. Here, we advocate for combining assessments, rather than relying on individual measures assessed separately, to better evaluate the effectiveness of interventions. Specifically, we argue that by pooling information from individual measures into a single outcome, composite scores can provide finer estimates of the underlying theoretical construct of interest while retaining important properties more sophisticated methods often forgo, such as transparency and interpretability. We describe different methods to compute, evaluate, and use composites depending on the goals, design, and data. To promote usability, we also provide a preregistration template that includes examples in the context of psychological interventions with supporting R code. Finally, we make a number of recommendations to help ensure that intervention studies are designed in a way that maximizes discoveries. A Shiny app and detailed R code accompany this article and are available at https://osf.io/u96em/ .",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3034236500",
    "type": "article"
  },
  {
    "title": "Multilab Direct Replication of Flavell, Beach, and Chinsky (1966): Spontaneous Verbal Rehearsal in a Memory Task as a Function of Age",
    "doi": "https://doi.org/10.1177/25152459211018187",
    "publication_date": "2021-04-01",
    "publication_year": 2021,
    "authors": "Emily M. Elliott; Candice C. Morey; Angela M. AuBuchon; Nelson Cowan; Christopher Jarrold; Eryn J. Adams; Meg Attwood; Büşra Bayram; Stefen Beeler-Duden; Taran Y. Blakstvedt; Gerhard Büttner; Thomas Castelain; Shari Cave; Davide Crepaldi; Eivor Fredriksen; Bret A. Glass; Andrew J. Graves; Dominic Guitard; Stefanie Hoehl; Alexis Hosch; Stéphanie Jeanneret; Tanya N. Joseph; Christopher Koch; Jarosław Roman Lelonkiewicz; Gary Lupyan; Amalia McDonald; Grace Meissner; Whitney Mendenhall; David Moreau; Thomas Ostermann; Asil Ali Özdoğru; Francesca Padovani; Sebastian Poloczek; Jan Phillip Röer; Christina Schonberg; Christian K. Tamnes; Martin J. Tomasik; Beatrice Valentini; Evie Vergauwe; Haley A. Vlach; Martin Voracek",
    "corresponding_authors": "Emily M. Elliott",
    "abstract": "Work by Flavell, Beach, and Chinsky indicated a change in the spontaneous production of overt verbalization behaviors when comparing young children (age 5) with older children (age 10). Despite the critical role that this evidence of a change in verbalization behaviors plays in modern theories of cognitive development and working memory, there has been only one other published near replication of this work. In this Registered Replication Report, we relied on researchers from 17 labs who contributed their results to a larger and more comprehensive sample of children. We assessed memory performance and the presence or absence of verbalization behaviors of young children at different ages and determined that the original pattern of findings was largely upheld: Older children were more likely to verbalize, and their memory spans improved. We confirmed that 5- and 6-year-old children who verbalized recalled more than children who did not verbalize. However, unlike Flavell et al., substantial proportions of our 5- and 6-year-old samples overtly verbalized at least sometimes during the picture memory task. In addition, continuous increase in overt verbalization from 7 to 10 years old was not consistently evident in our samples. These robust findings should be weighed when considering theories of cognitive development, particularly theories concerning when verbal rehearsal emerges and relations between speech and memory.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3174101565",
    "type": "article"
  },
  {
    "title": "Analyzing GPS Data for Psychological Research: A Tutorial",
    "doi": "https://doi.org/10.1177/25152459221082680",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Sandrine R. Müller; Joseph Bayer; Morgan Quinn Ross; Jerry Mount; Clemens Stachl; Gabriella M. Harari; Yung-Ju Chang; Huyen Le",
    "corresponding_authors": "Sandrine R. Müller",
    "abstract": "The ubiquity of location-data-enabled devices provides novel avenues for psychology researchers to incorporate spatial analytics into their studies. Spatial analytics use global positioning system (GPS) data to assess and understand mobility behavior (e.g., locations visited, movement patterns). In this tutorial, we provide a practical guide to analyzing GPS data in R and introduce researchers to key procedures and resources for conducting spatial analytics. We show readers how to clean GPS data, compute mobility features (e.g., time spent at home, number of unique places visited), and visualize locations and movement patterns. In addition, we discuss the challenges of ensuring participant privacy and interpreting the psychological implications of mobility behaviors. The tutorial is accompanied by an R Markdown script and a simulated GPS data set made available on the OSF.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4293108633",
    "type": "article"
  },
  {
    "title": "A Tutorial on Causal Inference in Longitudinal Data With Time-Varying Confounding Using G-Estimation",
    "doi": "https://doi.org/10.1177/25152459231174029",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Wen Wei Loh; Dongning Ren",
    "corresponding_authors": "Wen Wei Loh",
    "abstract": "In psychological research, longitudinal study designs are often used to examine the effects of a naturally observed predictor (i.e., treatment) on an outcome over time. But causal inference of longitudinal data in the presence of time-varying confounding is notoriously challenging. In this tutorial, we introduce g-estimation, a well-established estimation strategy from the causal inference literature. G-estimation is a powerful analytic tool designed to handle time-varying confounding variables affected by treatment. We offer step-by-step guidance on implementing the g-estimation method using standard parametric regression functions familiar to psychological researchers and commonly available in statistical software. To facilitate hands-on usage, we provide software code at each step using the open-source statistical software R. All the R code presented in this tutorial are publicly available online.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4386524709",
    "type": "article"
  },
  {
    "title": "The Chinese Open Science Network (COSN): Building an Open Science Community From Scratch",
    "doi": "https://doi.org/10.1177/25152459221144986",
    "publication_date": "2023-01-01",
    "publication_year": 2023,
    "authors": "Haiyang Jin; Qing Wang; Yufang Yang; Han Zhang; Mengyu Gao; Shuxian Jin; Yanxiu Chen; Ting Xu; Yuan-Rui Zheng; Chen Ji; Qinyu Xiao; Jinbiao Yang; Xindi Wang; Haiyang Geng; Jianqiao Ge; Weiwei Wang; Xi Chen; Lei Zhang; Xi‐Nian Zuo; Hu Chuan-Peng",
    "corresponding_authors": "Hu Chuan-Peng",
    "abstract": "Open Science is becoming a mainstream scientific ideology in psychology and related fields. However, researchers, especially early-career researchers (ECRs) in developing countries, are facing significant hurdles in engaging in Open Science and moving it forward. In China, various societal and cultural factors discourage ECRs from participating in Open Science, such as the lack of dedicated communication channels and the norm of modesty. To make the voice of Open Science heard by Chinese-speaking ECRs and scholars at large, the Chinese Open Science Network (COSN) was initiated in 2016. With its core values being grassroots-oriented, diversity, and inclusivity, COSN has grown from a small Open Science interest group to a recognized network both in the Chinese-speaking research community and the international Open Science community. So far, COSN has organized three in-person workshops, 12 tutorials, 48 talks, and 55 journal club sessions and translated 15 Open Science-related articles and blogs from English to Chinese. Currently, the main social media account of COSN (i.e., the WeChat Official Account) has more than 23,000 subscribers, and more than 1,000 researchers/students actively participate in the discussions on Open Science. In this article, we share our experience in building such a network to encourage ECRs in developing countries to start their own Open Science initiatives and engage in the global Open Science movement. We foresee great collaborative efforts of COSN together with all other local and international networks to further accelerate the Open Science movement.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4361012827",
    "type": "article"
  },
  {
    "title": "Capturing the Social Life of a Person by Integrating Experience-Sampling Methodology and Personal-Social-Network Assessments",
    "doi": "https://doi.org/10.1177/25152459241244745",
    "publication_date": "2024-04-01",
    "publication_year": 2024,
    "authors": "Marie Stadel; Laura F. Bringmann; Gert Stulp; Timon Elmer; Stijn Verdonck; Merijn Mestdagh; Marijtje A. J. van Duijn",
    "corresponding_authors": "Marie Stadel",
    "abstract": "The daily social life of a person can be captured with different methodologies. Two methods that are especially promising are personal-social-network (PSN) data collection and experience-sampling methodology (ESM). Whereas PSN data collections ask participants to provide information on their social relationships and broader social environment, ESM studies collect intensive longitudinal data on social interactions in daily life using multiple short surveys per day. In combination, the two methods enable detailed insights into someone’s social life, including information on interactions with specific interaction partners from the personal network. Despite many potential uses of such data integration, there are few studies to date using the two methods in conjunction. This is likely due to their complexity and lack of software that allows capturing the full social life of someone while keeping the burden for participants and researchers sufficiently low. In this article, we report on the development of methodology and software for an ESM/PSN integration within the established ESM tool m-Path. We describe results of a first study using the developed tool that illustrate the feasibility of the proposed method combination and show that participants consider the assessments insightful. We further outline study-design choices and ethical considerations when combining the two methodologies. We hope to encourage applications of the presented methods in research and practice across different fields.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4394805447",
    "type": "article"
  },
  {
    "title": "Interacting With Curves: How to Validly Test and Probe Interactions in the Real (Nonlinear) World",
    "doi": "https://doi.org/10.1177/25152459231207787",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Uri Simonsohn",
    "corresponding_authors": "Uri Simonsohn",
    "abstract": "Hypotheses involving interactions in which one variable modifies the association between another two are very common. They are typically tested relying on models that assume effects are linear, for example, with a regression like y = a + b x + c z + d x × z. In the real world, however, few effects are linear, invalidating inferences about interactions. For instance, in realistic situations, the false-positive rate can be 100% for detecting an interaction, and a probed interaction can reliably produce estimated effects of the wrong sign. In this article, I propose a revised toolbox for studying interactions in a curvilinear-robust manner, giving correct answers “even” when effects are not linear. It is applicable to most study designs and produces results that are analogous to those of current—often invalid—practices. The presentation combines statistical intuition, demonstrations with published results, and simulations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4392405378",
    "type": "article"
  },
  {
    "title": "On the Statistical Analysis of Experiments With Manipulation Checks",
    "doi": "https://doi.org/10.1177/25152459241297537",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Maya B. Mathur",
    "corresponding_authors": "Maya B. Mathur",
    "abstract": "Psychologists are often interested in the effect of an internal state, such as ego depletion, that cannot be directly assigned in an experiment. Instead, they assign participants to a manipulation intended to produce this state and use manipulation checks to assess the manipulation’s effectiveness. In this article, I discuss statistical analyses for experiments in which researchers are primarily interested in the average treatment effect (ATE) of the target internal state rather than that of the manipulation. Often, researchers estimate the association of the manipulation itself with the dependent variable, but this intention-to-treat (ITT) estimator is typically biased for the ATE of the target state, and the bias could be either toward the null (conservative) or away from the null. I discuss the fairly stringent assumptions under which this estimator is conservative. Given this, I argue against the status-quo practice of interpreting the ITT estimate as the effect of the target state without any explicit discussion of whether these assumptions hold. Under a somewhat weaker version of the same assumptions, one can alternatively use instrumental-variables (IVs) analysis to directly estimate the effect of the target state. IVs analysis complements ITT analysis by directly addressing the central question of interest. As a running example, I consider a multisite replication study on the ego-depletion effect, in which the manipulation’s partial effectiveness led to criticism and several reanalyses that arrived at varying conclusions. I use IVs analysis to directly account for the manipulation’s partial effectiveness; this corroborated the replication authors’ reported null results.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406516600",
    "type": "article"
  },
  {
    "title": "With Low Power Comes Low Credibility? Toward a Principled Critique of Results From Underpowered Tests",
    "doi": "https://doi.org/10.1177/25152459241296397",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Lukas Lengersdorff; Claus Lamm",
    "corresponding_authors": "",
    "abstract": "Researchers should be motivated to adequately power statistical tests because tests with low power have a low probability of detecting true effects. However, it is also often claimed that significant results obtained by underpowered tests are less likely to reflect a true effect. Here, we critically discuss this “low-power/low-credibility” (LPLC) critique from both frequentist and Bayesian perspectives. Although the LPLC critique is first and foremost a critique of frequentist tests, it is itself not consistent with frequentist theory. In particular, it demands that researchers have some information on the probability that a hypothesis is true before they test it. However, such prior probabilities are dismissed as meaningless in frequentist inference, and we demonstrate that they cannot be meaningfully introduced into frequentist thinking. Even when adopting a Bayesian perspective, however, significant results from tests with low power can provide a nonnegligible amount of support for the tested hypothesis. We conclude that even though low power reduces the chances to obtain significant findings, there is little justification to dismiss already obtained significant findings on the basis of low power alone. However, concerns about low power might often reflect suspicions that findings were produced by questionable research practices. If this is the case, we suggest that communicating these issues transparently rather than using low power as a proxy concern will be more appropriate. We conclude by providing suggestions on how results from tests with low power can be critiqued for the correct reasons and in a constructive manner.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406891217",
    "type": "article"
  },
  {
    "title": "Practical Problems Estimating and Reporting Power When Hypotheses Are Embedded in Complex Statistical Models",
    "doi": "https://doi.org/10.1177/25152459241302300",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "David A. Cole; George Abitante; Haidong Kan; Qimin Liu; Kristopher J. Preacher; Scott E. Maxwell",
    "corresponding_authors": "",
    "abstract": "In both grant proposals and published studies, many research hypotheses (represented by primary parameters) are tested in the context of complex statistical models. The power to detect these primary parameters depends on the values of multiple secondary model parameters that often go unexamined, unreported, and unjustified. The result is that many a priori power analyses are incomplete, ambiguous, potentially subjective, and nearly impossible for others to evaluate. In the current article, we encourage researchers to use plausible values for secondary parameters (PVSPs), in addition to the hoped-for effect sizes for their primary parameters, in their power calculations. More specifically, upper and lower bounds for power are generated based on the highest and lowest plausible values for secondary model parameters. In the article, we demonstrate how to conduct and describe such power estimates for four increasingly complex statistical methods. We further demonstrate how such analyses can inform decisions about resource allocation in ways that improve power in the context of complex statistical models, often revealing that power can be enhanced by methods other than increasing sample size. The PVSP approach can enhance power, improve the transparency of power analyses in grant proposals, reduce the likelihood of funding underpowered research, and alleviate at least one problem underlying the replication crisis.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406949801",
    "type": "article"
  },
  {
    "title": "Research With the Reluctant: Challenges and Strategies",
    "doi": "https://doi.org/10.1177/25152459241304775",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "E. B. Gross; Susan A. Gelman",
    "corresponding_authors": "",
    "abstract": "Although psychological research often relies on convenience samples, the most informative participants may be individuals who are reluctant to engage because of vulnerability, mistrust of the research process, and/or disagreement with a study’s goals. Although this concern is particularly urgent for certain research questions, it is relevant for all researchers because relying solely on easy-to-reach participants limits a study’s validity and generalizability and may substantially hollow out or even render unanswerable some research questions. We review a number of challenges for conducting research with “reluctant but informative” participants and strategies to meet and overcome these challenges. We argue that engaging reluctant participants requires attention at every phase of the research process, including study design and planning, participant recruitment and testing, data analysis and interpretation, and reporting and broader impacts. We also illustrate with our own recent experiences conducting research on children’s gender concepts in rural, conservative U.S. communities that expressed skepticism about the value of this research. Although every study is different, notable across all projects are the need to respect both individual participants and their communities and to balance competing desiderata. Finally, we discuss the importance of transparency around sample composition and constraints to generalizability and the potential utility of collaborative research both across research institutions and between researchers and participant communities.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407307685",
    "type": "article"
  },
  {
    "title": "VALID: A Checklist-Based Approach for Improving Validity in Psychological Research",
    "doi": "https://doi.org/10.1177/25152459241306432",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Susanne Kerschbaumer; Martin Voracek; Balázs Aczél; Samantha F. Anderson; Brandon M. Booth; Erin Michelle Buchanan; Rickard Carlsson; Daniel W. Heck; Anu Pauliina Hiekkaranta; Rink Hoekstra; Julian D. Karch; Ginette Lafit; Zhicheng Lin; Siwei Liu; David P. MacKinnon; Emma L. McGorray; David Moreau; Μαριέττα Παπαδάτου-Παστού; Helena Paterson; Robert A. Perera; Daniel J. Schad; David K. Sewell; Moin Syed; Louis Tay; Jorge N. Tendeiro; Michael D. Toland; Wolf Vanpaemel; Don van Ravenzwaaij; Lieke Voncken; Ulrich S. Tran",
    "corresponding_authors": "",
    "abstract": "In response to the replication and confidence crisis across various empirical disciplines, ensuring the validity of research has gained attention. High validity is crucial for obtaining replicable and robust study outcomes when both exploring new questions and replicating previous findings. In this study, we aimed to address this issue by developing a comprehensive checklist to assist researchers in enhancing and monitoring the validity of their research. After systematically analyzing previous findings on validity, a comprehensive list of potential checklist items was compiled. Over the course of three rounds, more than 30 interdisciplinary and psychological-science experts participated in a Delphi study. Experts rated items on their importance and were given the opportunity to propose novel items as well as improve existing ones. This process resulted in a final set of 91 items, organized according to common stages of a research project. The VALID checklist is accessible online ( https://www.validchecklist.com/ ) and provides researchers with an adaptable, versatile tool to monitor and improve the validity of their research and to suit their specific needs. By focusing on adaptiveness during its development, VALID encompasses 331 unique checklist versions, making it a one-stop solution suitable for a wide range of projects, designs, and requirements.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407779358",
    "type": "article"
  },
  {
    "title": "Sample-Size Planning in Item-Response Theory: A Tutorial",
    "doi": "https://doi.org/10.1177/25152459251314798",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Ulrich Schroeders; Timo Gnambs",
    "corresponding_authors": "",
    "abstract": "Although item-response-theory (IRT) models offer well-established psychometric advantages over traditional scoring methods, they remain underused in practice. Following a brief introduction to the IRT framework, we emphasize its major advantages and explore potential applications in various research areas. The main part of this tutorial provides a comprehensive, step-by-step guide to Monte Carlo simulation-based sample-size estimation in IRT, which is essential for obtaining precise estimates of item and person parameters, structural effects, and model fit. Accurate a priori sample-size estimation is also crucial for effective study planning, especially in preregistration and registered reports. We highlight 10 key decisions, organized into four areas: (a) determining the data-generation model, (b) defining the test design and the process of missing values, (c) selecting the IRT model and parameters of interest, and (d) setting up and running the Monte Carlo simulation. The procedure is illustrated with examples from educational, personality, and clinical psychology. An extensively annotated and easily customizable syntax is available in an online repository.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407892587",
    "type": "article"
  },
  {
    "title": "The Social-Relations Model for Asymmetric-Block-Design Data: A Tutorial With R",
    "doi": "https://doi.org/10.1177/25152459241279522",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Avraham N. Kluger; Robert A. Ackerman; David A. Kenny; Thomas E. Malloy; Paul W. Eastwick",
    "corresponding_authors": "",
    "abstract": "We provide a guide to estimating social-relations-model (SRM) parameters for data collected with the asymmetric block design using R. SRM estimates reflect how people differ in the social behaviors they emit and elicit from others and the extent to which they behave differently with each unique partner. Such analyses have proven useful in studies of the power of leaders, negotiation effectiveness, and personality perception, to name a few. The SRM parameters can be estimated for data collected with various designs; the most common design is the round-robin. A less used design in SRM studies is the asymmetric block design, which involves measurements taken from dyadic interactions of individuals from two different social categories (e.g., speed-dating studies in which different-gendered participants interact). In this guide, we show how to estimate SRM parameters for asymmetric block design by specifying mixed-effect models with two R packages: glmmTMB and brms. Specifically, we show how to calculate SRM estimates for (a) one continuous outcome, (b) multiple measures of a continuous outcome, (c) bivariate correlations for two continuous outcomes, (d) one dichotomous outcome, (e) bivariate and multivariate associations between continuous SRM variables and one dichotomous outcome (in supplementary work-around), and (f) data from mother and child interactions rather than the more common speed-dating data. In all analyses, we illustrate how to model both fixed and random effects to allow testing additional fixed effects, such as group characteristics and the order of interaction between participants.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408128986",
    "type": "article"
  },
  {
    "title": "Navigating Unmeasured Confounding in Nonexperimental Psychological Research: A Practical Guide to Computing and Interpreting E-Value",
    "doi": "https://doi.org/10.1177/25152459251326571",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Kaiwen Bi; Gabriel J. Merrin; Tianyu Li; Xianlin Sun; Yi Chai; Zékai Lu; Shuquan Chen",
    "corresponding_authors": "",
    "abstract": "Randomized experiments remain the “gold standard” for establishing causality, yet ethical and practical constraints in certain fields often require researchers to rely on observational data. Although psychologists recognize that correlation does not imply causality, the conventional cautionary statements regarding correlation typically found at the end of articles have not sufficiently advanced psychological science, particularly in subfields, such as developmental and personality psychology, that predominantly rely on observational data. Sensitivity analyses commonly used in biostatistics and epidemiology offer powerful tools to quantify the risk of unmeasured confounding in observational data analysis, essentially encouraging applied researchers to assess how strongly an unmeasured confounder must be associated with both the predictor and outcome to negate an observed predictor-outcome association (i.e., reduce the effect to null). In this tutorial, we explore the frequently overlooked but critical issue of unmeasured confounding in psychological research and introduce psychologists to the E-value, a novel and straightforward method for assessing the robustness of exposure-outcome associations to unmeasured confounding. We demonstrate the application of E-value using common psychological-research scenarios in R and discuss its strengths, limitations, and recommended best practices. Psychologists can more accurately assess and transparently report research findings, particularly in subfields relying primarily on observational data, by more explicitly considering unmeasured confounding and incorporating sensitivity-analysis techniques such as the E-value into their methodological tool kits.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409423144",
    "type": "article"
  },
  {
    "title": "Registered Replication Report: Study 3 From Trafimow and Hughes (2012)",
    "doi": "https://doi.org/10.1177/25152459251328334",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Sean C. Rife; Quinn Scott Lambert; Robert Calin‐Jageman; Matúš Adamkovič﻿﻿﻿; Gabriel Baník; Itxaso Barberia; Jennifer L Beaudry; Hanna Bernauer; Dustin P. Calvillo; William J. Chopik; Louise David; Ismay de Beijer; Thomas Rhys Evans; Andree Hartanto; Pavol Kačmár; Nicole Legate; Marcel Martončik; Karlijn Massar; Simon McCabe; David Moreau; Şevval Osmanoğlu; Asil Ali Özdoğru; Miriam Panning; Maximilian Primbs; John Protzko; Javier Rodríguez‐Ferreiro; Jan Philipp Röer; Ivan Ropovik; Simon Schindler; Willem W. A. Sleegers; Gill A. ten Hoor; Ulrich S. Tran; Hein T. van Schie; Martin Voracek; Brady Wiggins",
    "corresponding_authors": "",
    "abstract": "Terror-management theory (TMT) proposes that when people are made aware of their own death, they are more likely to endorse cultural values. TMT is a staple of social psychology, featured prominently in textbooks and the subject of much research. The implications associated with TMT are significant because its advocates claim it can partially explain cultural conflicts, intergroup antagonisms, and even war. However, considerable ambiguity regarding effect size exists, and no preregistered replication of death-thought-accessibility findings exists. Moreover, there is debate regarding the role of time delay between the manipulation of mortality salience and assessment of key measures. We present results from 22 labs in 11 countries (total N = 3,447) attempting to replicate and extend an existing study of TMT, Study 3 from Trafimow and Hughes, and the role of time-delay effects. We successfully replicate Trafimow and Hughes and demonstrate that it is possible to prime death-related thoughts and that priming is more effective when there is no delay between the priming and outcome measure. Implications for future research and TMT are discussed.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409672125",
    "type": "article"
  },
  {
    "title": "De-Identification When Making Data Sets Findable, Accessible, Interoperable, and Reusable (FAIR): Two Worked Examples From the Behavioral and Social Sciences",
    "doi": "https://doi.org/10.1177/25152459251336130",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Don van Ravenzwaaij; M. de Jong; Rink Hoekstra; Susanne Scheibe; Marijke Span; Ineke Wessel; Vera Ellen Heininga",
    "corresponding_authors": "",
    "abstract": "In recent years, the advancement of open science has led to data sharing becoming more common practice. Data availability has clear merits for science because it opens up possibilities for reuse of data sets by others, leading to less redundancy, more efficiency, and more transparency. The ideal is for scientific data to be as open as possible and findable, accessible, interoperable, and reusable (FAIR). Parallel to this development, recent times have seen more stringent guidelines with respect to data privacy, culminating in the General Data Protection Regulation law. Navigating the balance between protecting participants’ privacy and making one’s data set as open as possible can be challenging for researchers. In this article, we provide two worked examples with real data sets from the behavioral and social sciences on how to be as open as possible and as closed as necessary with the goal of maximally facilitating science while minimizing the risk of participant identification.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410383479",
    "type": "article"
  },
  {
    "title": "Bestiary of Questionable Research Practices in Psychology",
    "doi": "https://doi.org/10.1177/25152459251348431",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Tamás Nagy; Jane Hergert; Mahmoud Medhat Elsherif; Lukas Wallrich; Kathleen Schmidt; Talia Waltzer; Jason W. Payne; Biljana Gjoneska; Yashvin Seetahul; Yilin Andre Wang; Daniel Scharfenberg; Gabriella Tyson; Yufang Yang; Aleksandrina Skvortsova; Samuel Alarie; Katherine A. Graves; Lukas K. Sotola; David Moreau; Eva Rubínová",
    "corresponding_authors": "",
    "abstract": "Questionable research practices (QRPs) pose a significant threat to the quality of scientific research. However, historically, they remain ill-defined, and a comprehensive list of QRPs is lacking. In this article, we address this concern by defining, collecting, and categorizing QRPs using a community-consensus method. Collaborators of the study agreed on the following definition: QRPs are ways of producing, maintaining, sharing, analyzing, or interpreting data that are likely to produce misleading conclusions, typically in the interest of the researcher. QRPs are not normally considered to include research practices that are prohibited or proscribed in the researcher’s field (e.g., fraud, research misconduct). Neither do they include random researcher error (e.g., accidental data loss). Drawing from both iterative discussions and existing literature, we collected, defined, and categorized 40 QRPs for quantitative research. We also considered attributes such as potential harms, detectability, clues, and preventive measures for each QRP. The results suggest that QRPs are pervasive and versatile and have the potential to undermine all stages of the scientific enterprise. This work contributes to the maintenance of research integrity, transparency, and reliability by raising awareness for and improving the understanding of QRPs in quantitative psychological research.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4412134464",
    "type": "article"
  },
  {
    "title": "Bridging Traditional-Statistics and Machine-Learning Approaches in Psychology: Navigating Small Samples, Measurement Error, Nonindependent Observations, and Missing Data",
    "doi": "https://doi.org/10.1177/25152459251345696",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Rosa Lavelle-Hill; Gavin Smith; Kou Murayama",
    "corresponding_authors": "",
    "abstract": "In recent years, machine learning has propagated into different aspects of psychological research, and supervised machine-learning methods have increasingly been used as a tool for predicting human behavior or psychological characteristics when there is a large number of possible predictors. However, researchers often face practical challenges when using machine-learning methods on psychological data. In this article, we identify and discuss four key challenges that often arise when applying machine learning to data collected for psychological research. The four challenge areas cover (a) limited sample size, (b) measurement error, (c) nonindependent data, and (d) missing data. Such challenges are extensively discussed in the “traditional” statistical literature but are often not explicitly addressed, or at least not to the same extent, in the applied-machine-learning community. We present how each of these challenges is dealt with first from a traditional-statistics perspective and then from a machine-learning perspective and discuss the strengths and weaknesses of these solutions by comparing the approaches. We argue that the boundary between traditional statistics and machine learning is fluid and emphasize the need for cross-disciplinary collaboration to better tackle these core challenges and improve replicability.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4412530215",
    "type": "article"
  },
  {
    "title": "How Do Psychology Journals Handle Postpublication Critique? A Cross-Sectional Study of Policy and Practice",
    "doi": "https://doi.org/10.1177/25152459251366102",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Annie Whamond; Simine Vazire; Beth Clarke; Nicholas Moodie; Sarah R. Schiavone; Robert T. Thibault; Tom E Hardwicke",
    "corresponding_authors": "",
    "abstract": "Postpublication critique, such as letters to the editor, can contribute to the validity and trustworthiness of scientific research. We conducted a cross-sectional analysis of the policy and practice of postpublication critique in (a) randomly selected ( N = 100) and (b) prominent ( N = 100) psychology journals. In 2023, an explicit submission option for postpublication critique was available at 23% (95% confidence interval [CI] = [16%, 32%]) of randomly sampled psychology journals and 38% of the most prominent psychology journals. Journals sometimes imposed limits on the length and time allowed to submit critiques. We manually inspected two random samples of empirical articles published in 2020 (articles per sample: N = 101), estimating the prevalence of postpublication critique to be 0% (95% CI = [0%, 3.7%]) in psychology journals generally and 1% (95% CI = [0.2%, 5.4%]) in the most prominent psychology journals. The policy and practice of postpublication critique is seriously neglected in psychology journals.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4414311343",
    "type": "article"
  },
  {
    "title": "Fallibility in Science: Responding to Errors in the Work of Oneself and Others",
    "doi": "https://doi.org/10.1177/2515245918776632",
    "publication_date": "2018-07-03",
    "publication_year": 2018,
    "authors": "Dorothy Bishop",
    "corresponding_authors": "Dorothy Bishop",
    "abstract": "",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W4236356630",
    "type": "article"
  },
  {
    "title": "On the Interpretation of Parameters in Multivariate Multilevel Models Across Different Combinations of Model Specification and Estimation",
    "doi": "https://doi.org/10.1177/2515245919842770",
    "publication_date": "2019-08-02",
    "publication_year": 2019,
    "authors": "Lesa Hoffman",
    "corresponding_authors": "Lesa Hoffman",
    "abstract": "The increasing availability of software with which to estimate multivariate multilevel models (also called multilevel structural equation models) makes it easier than ever before to leverage these powerful techniques to answer research questions at multiple levels of analysis simultaneously. However, interpretation can be tricky given that different choices for centering model predictors can lead to different versions of what appear to be the same parameters; this is especially the case when the predictors are latent variables created through model-estimated variance components. A further complication is a recent change to Mplus (Version 8.1), a popular software program for estimating multivariate multilevel models, in which the selection of Bayesian estimation instead of maximum likelihood results in different lower-level predictors when random slopes are requested. This article provides a detailed explication of how the parameters of multilevel models differ as a function of the analyst’s decisions regarding centering and the form of lower-level predictors (i.e., observed or latent), the method of estimation, and the variant of program syntax used. After explaining how different methods of centering lower-level observed predictor variables result in different higher-level effects within univariate multilevel models, this article uses simulated data to demonstrate how these same concepts apply in specifying multivariate multilevel models with latent lower-level predictor variables. Complete data, input, and output files for all of the example models have been made available online to further aid readers in accurately translating these central tenets of multivariate multilevel modeling into practice.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2964606584",
    "type": "article"
  },
  {
    "title": "StudySwap: A Platform for Interlab Replication, Collaboration, and Resource Exchange",
    "doi": "https://doi.org/10.1177/2515245918808767",
    "publication_date": "2018-11-08",
    "publication_year": 2018,
    "authors": "Christopher R. Chartier; Amy Riegelman; Randy J. McCarthy",
    "corresponding_authors": "Christopher R. Chartier",
    "abstract": "The resources needed to conduct psychological research (i.e., time, access to participants, equipment, expertise, etc.) are sometimes distributed inefficiently: Resources going unused at one lab may be needed to complete projects at another lab. This inefficient distribution of resources can be an impediment to scientific progress and to individuals’ careers. StudySwap ( https://osf.io/view/StudySwap ) is an online platform where researchers can post brief de-scriptions of research resources that are available for use ( haves) or that they need and another researcher may have ( needs). This Tutorial provides instructions for posting haves and needs on StudySwap, responding to the posts of other researchers, and creating exchange agreements that define expectations of all collaborative parties prior to data collection or other resource exchanges. Ultimately, we hope that StudySwap can be used to increase the efficiency with which psychology’s collective research resources are being used.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2782343273",
    "type": "article"
  },
  {
    "title": "Curating Research Assets: A Tutorial on the Git Version Control System",
    "doi": "https://doi.org/10.1177/2515245918754826",
    "publication_date": "2018-04-11",
    "publication_year": 2018,
    "authors": "Matti Vuorre; James P. Curley",
    "corresponding_authors": "Matti Vuorre",
    "abstract": "Recent calls for improving reproducibility have increased attention to the ways in which researchers curate, share, and collaborate on their research assets. In this Tutorial, we explain how version control systems, such as the popular Git program, support these functions and then show how to use Git with a graphical interface in the RStudio program. This Tutorial is written for researchers with no previous experience using version control systems and covers both single-user and collaborative workflows. The online Supplemental Material provides information on advanced Git command-line functions. Git presents an elegant solution to specific challenges to curating, sharing, and collaborating on research assets and can be implemented in common workflows with little extra effort.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2788935317",
    "type": "article"
  },
  {
    "title": "Getting Started Creating Data Dictionaries: How to Create a Shareable Data Set",
    "doi": "https://doi.org/10.1177/2515245920928007",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Erin Michelle Buchanan; Sarah E Crain; Ari L. Cunningham; Hannah Johnson; Hannah Stash; Μαριέττα Παπαδάτου-Παστού; Peder Mortvedt Isager; Rickard Carlsson; Balázs Aczél",
    "corresponding_authors": "Erin Michelle Buchanan",
    "abstract": "As researchers embrace open and transparent data sharing, they will need to provide information about their data that effectively helps others understand their data sets’ contents. Without proper documentation, data stored in online repositories such as OSF will often be rendered unfindable and unreadable by other researchers and indexing search engines. Data dictionaries and codebooks provide a wealth of information about variables, data collection, and other important facets of a data set. This information, called metadata, provides key insights into how the data might be further used in research and facilitates search-engine indexing to reach a broader audience of interested parties. This Tutorial first explains terminology and standards relevant to data dictionaries and codebooks. Accompanying information on OSF presents a guided workflow of the entire process from source data (e.g., survey answers on Qualtrics) to an openly shared data set accompanied by a data dictionary or codebook that follows an agreed-upon standard. Finally, we discuss freely available Web applications to assist this process of ensuring that psychology data are findable, accessible, interoperable, and reusable.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2947495427",
    "type": "article"
  },
  {
    "title": "Comparing Analysis Blinding With Preregistration in the Many-Analysts Religion Project",
    "doi": "https://doi.org/10.1177/25152459221128319",
    "publication_date": "2023-01-01",
    "publication_year": 2023,
    "authors": "Alexandra Sarafoglou; Suzanne Hoogeveen; Eric‐Jan Wagenmakers",
    "corresponding_authors": "Alexandra Sarafoglou",
    "abstract": "In psychology, preregistration is the most widely used method to ensure the confirmatory status of analyses. However, the method has disadvantages: Not only is it perceived as effortful and time-consuming, but reasonable deviations from the analysis plan demote the status of the study to exploratory. An alternative to preregistration is analysis blinding, in which researchers develop their analysis on an altered version of the data. In this experimental study, we compare the reported efficiency and convenience of the two methods in the context of the Many-Analysts Religion Project. In this project, 120 teams answered the same research questions on the same data set, either preregistering their analysis ( n = 61) or using analysis blinding ( n = 59). Our results provide strong evidence (Bayes factor [BF] = 71.40) for the hypothesis that analysis blinding leads to fewer deviations from the analysis plan, and if teams deviated, they did so on fewer aspects. Contrary to our hypothesis, we found strong evidence (BF = 13.19) that both methods required approximately the same amount of time. Finally, we found no and moderate evidence on whether analysis blinding was perceived as less effortful and frustrating, respectively. We conclude that analysis blinding does not mean less work, but researchers can still benefit from the method because they can plan more appropriate analyses from which they deviate less frequently.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4313855994",
    "type": "article"
  },
  {
    "title": "The Epistemic Importance of Establishing the Absence of an Effect",
    "doi": "https://doi.org/10.1177/2515245918770407",
    "publication_date": "2018-05-30",
    "publication_year": 2018,
    "authors": "Fiona Fidler; Felix Singleton Thorn; Ashley Barnett; Steven Kambouris; Ariel Kruger",
    "corresponding_authors": "Fiona Fidler",
    "abstract": "",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2805261179",
    "type": "article"
  },
  {
    "title": "Writing Empirical Articles: Transparency, Reproducibility, Clarity, and Memorability",
    "doi": "https://doi.org/10.1177/2515245918754485",
    "publication_date": "2018-07-13",
    "publication_year": 2018,
    "authors": "Morton Ann Gernsbacher",
    "corresponding_authors": "Morton Ann Gernsbacher",
    "abstract": "This article provides recommendations for writing empirical journal articles that enable transparency, reproducibility, clarity, and memorability. Recommendations for transparency include preregistering methods, hypotheses, and analyses; submitting registered reports; distinguishing confirmation from exploration; and showing your warts. Recommendations for reproducibility include documenting methods and results fully and cohesively, by taking advantage of open-science tools, and citing sources responsibly. Recommendations for clarity include writing short paragraphs, composed of short sentences; writing comprehensive abstracts; and seeking feedback from a naive audience. Recommendations for memorability include writing narratively; embracing the hourglass shape of empirical articles; beginning articles with a hook; and synthesizing, rather than Mad Libbing, previous literature.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2883032466",
    "type": "article"
  },
  {
    "title": "Who Should Do Replication Labor?",
    "doi": "https://doi.org/10.1177/2515245918803619",
    "publication_date": "2018-10-24",
    "publication_year": 2018,
    "authors": "Felipe Romero",
    "corresponding_authors": "Felipe Romero",
    "abstract": "Scientists, for the most part, want to get it right. However, the social structures that govern their work undermine that aim, and this leads to nonreplicable findings in many fields. Because the social structure of science is a decentralized system, it is difficult to intervene. In this article, I discuss how we might do so, focusing on self-corrective-labor schemes (i.e., ways of distributing replication efforts within the scientific community). First, I argue that we need to implement a scheme that makes replication work outcome independent, systematic, and sustainable. Second, I use these three criteria to evaluate extant proposals, which place the responsibility for replication on original researchers, consumers of their research, students, or many labs. Third, on the basis of a philosophical analysis of the reward system of science and the benefits of the division of cognitive labor, I propose a scheme that satisfies the criteria better: the professional scheme. This scheme has two main components. First, the scientific community is organized into two groups: discovery researchers, who produce new findings, and confirmation researchers, whose primary function is to do confirmation work (i.e., replication, reproduction, meta-analysis). Second, a distinct reward system is established for confirmation researchers so that their career advancement is separated from whether they obtain positive experimental results.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2897425065",
    "type": "article"
  },
  {
    "title": "Advancing Meta-Analysis With Knowledge-Management Platforms: Using metaBUS in Psychology",
    "doi": "https://doi.org/10.1177/2515245919882693",
    "publication_date": "2019-11-26",
    "publication_year": 2019,
    "authors": "Frank A. Bosco; James G. Field; Kai R. Larsen; Yingyi Chang; Krista L. Uggerslev",
    "corresponding_authors": "Frank A. Bosco",
    "abstract": "In this article, we provide a review of research-curation and knowledge-management efforts that may be leveraged to advance research and education in psychological science. After reviewing the approaches and content of other efforts, we focus on the metaBUS project’s platform, the most comprehensive effort to date. The metaBUS platform uses standards-based protocols in combination with human judgment to organize and make readily accessible a database of research findings, currently numbering more than 1 million. It allows users to conduct rudimentary, instant meta-analyses, and capacities for visualization and communication of meta-analytic findings have recently been added. We conclude by discussing challenges, opportunities, and recommendations for expanding the project beyond applied psychology.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2990795924",
    "type": "article"
  },
  {
    "title": "Introducing <i>Advances in Methods and Practices in Psychological Science</i>",
    "doi": "https://doi.org/10.1177/2515245918757424",
    "publication_date": "2018-02-14",
    "publication_year": 2018,
    "authors": "Daniel J. Simons",
    "corresponding_authors": "Daniel J. Simons",
    "abstract": "",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2793639116",
    "type": "article"
  },
  {
    "title": "Corrigendum: Evaluating Effect Size in Psychological Research: Sense and Nonsense",
    "doi": "https://doi.org/10.1177/2515245920979282",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4245246094",
    "type": "erratum"
  },
  {
    "title": "Average Power: A Cautionary Note",
    "doi": "https://doi.org/10.1177/2515245920902370",
    "publication_date": "2020-05-28",
    "publication_year": 2020,
    "authors": "Blakeley B. McShane; Ulf Böckenholt; Karsten T. Hansen",
    "corresponding_authors": "Blakeley B. McShane",
    "abstract": "Replication is an important contemporary issue in psychological research, and there is great interest in ways of assessing replicability, in particular, retrospectively via prior studies. The average power of a set of prior studies is a quantity that has attracted considerable attention for this purpose, and techniques to estimate this quantity via a meta-analytic approach have recently been proposed. In this article, we have two aims. First, we clarify the nature of average power and its implications for replicability. We explain that average power is not relevant to the replicability of actual prospective replication studies. Instead, it relates to efforts in the history of science to catalogue the power of prior studies. Second, we evaluate the statistical properties of point estimates and interval estimates of average power obtained via the meta-analytic approach. We find that point estimates of average power are too variable and inaccurate for use in application. We also find that the width of interval estimates of average power depends on the corresponding point estimates; consequently, the width of an interval estimate of average power cannot serve as an independent measure of the precision of the point estimate. Our findings resolve a seeming puzzle posed by three estimates of the average power of the power-posing literature obtained via the meta-analytic approach.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3030344239",
    "type": "article"
  },
  {
    "title": "Analyzing Individual Differences in Intervention-Related Changes",
    "doi": "https://doi.org/10.1177/2515245920979172",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Tanja Könen; Julia Karbach",
    "corresponding_authors": "",
    "abstract": "Intervention studies can be expensive and time-consuming, which is why it is important to extract as much knowledge as possible. We discuss benefits and limitations of analyzing individual differences in intervention studies in addition to traditional analyses of average group effects. First, we present a short introduction to latent change modeling and measurement invariance in the context of intervention studies. Then, we give an overview on options for analyzing individual differences in intervention-related changes with a focus on how substantive information can be distinguished from methodological artifacts (e.g., regression to the mean). The main topics are benefits and limitations of predicting changes with baseline data and of analyzing correlated change. Both approaches can offer descriptive correlational information about individuals in interventions, which can inform future variations of experimental conditions. Applications increasingly emerge in the literature—from clinical, developmental, and educational psychology to occupational psychology—and demonstrate their potential across all of psychology.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3135664292",
    "type": "article"
  },
  {
    "title": "SampleSizePlanner: A Tool to Estimate and Justify Sample Size for Two-Group Studies",
    "doi": "https://doi.org/10.1177/25152459211054059",
    "publication_date": "2022-01-01",
    "publication_year": 2022,
    "authors": "Márton Kovács; Don van Ravenzwaaij; Rink Hoekstra; Balázs Aczél",
    "corresponding_authors": "Márton Kovács",
    "abstract": "Planning sample size often requires researchers to identify a statistical technique and to make several choices during their calculations. Currently, there is a lack of clear guidelines for researchers to find and use the applicable procedure. In the present tutorial, we introduce a web app and R package that offer nine different procedures to determine and justify the sample size for independent two-group study designs. The application highlights the most important decision points for each procedure and suggests example justifications for them. The resulting sample-size report can serve as a template for preregistrations and manuscripts.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4210783725",
    "type": "article"
  },
  {
    "title": "Journal N-Pact Factors From 2011 to 2019: Evaluating the Quality of Social/Personality Journals With Respect to Sample Size and Statistical Power",
    "doi": "https://doi.org/10.1177/25152459221120217",
    "publication_date": "2022-10-01",
    "publication_year": 2022,
    "authors": "R. Chris Fraley; Jia Y. Chong; Kyle A. Baacke; Anthony J. Greco; Hanxiong Guan; Simine Vazire",
    "corresponding_authors": "R. Chris Fraley",
    "abstract": "Scholars and institutions commonly use impact factors to evaluate the quality of empirical research. However, a number of findings published in journals with high impact factors have failed to replicate, suggesting that impact alone may not be an accurate indicator of quality. Fraley and Vazire proposed an alternative index, the N-pact factor, which indexes the median sample size of published studies, providing a narrow but relevant indicator of research quality. In the present research, we expand on the original report by examining the N-pact factor of social/personality-psychology journals between 2011 and 2019, incorporating additional journals and accounting for study design (i.e., between persons, repeated measures, and mixed). There was substantial variation in the sample sizes used in studies published in different journals. Journals that emphasized personality processes and individual differences had larger N-pact factors than journals that emphasized social-psychological processes. Moreover, N-pact factors were largely independent of traditional markers of impact. Although the majority of journals in 2011 published studies that were not well powered to detect an effect of ρ = .20, this situation had improved considerably by 2019. In 2019, eight of the nine journals we sampled published studies that were, on average, powered at 80% or higher to detect such an effect. After decades of unheeded warnings from methodologists about the dangers of small-sample designs, the field of social/personality psychology has begun to use larger samples. We hope the N-pact factor will be supplemented by other indices that can be used as alternatives to improve further the evaluation of research.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4305082791",
    "type": "article"
  },
  {
    "title": "Does Your Smartphone “Know” Your Social Life? A Methodological Comparison of Day Reconstruction, Experience Sampling, and Mobile Sensing",
    "doi": "https://doi.org/10.1177/25152459231178738",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Yannick Roos; Michael Dominik Krämer; David Richter; Ramona Schoedel; Cornelia Wrzus",
    "corresponding_authors": "Yannick Roos",
    "abstract": "Mobile sensing is a promising method that allows researchers to directly observe human social behavior in daily life using people’s mobile phones. To date, limited knowledge exists on how well mobile sensing can assess the quantity and quality of social interactions. We therefore examined the agreement among experience sampling, day reconstruction, and mobile sensing in the assessment of multiple aspects of daily social interactions (i.e., face-to-face interactions, calls, and text messages) and the possible unique access to social interactions that each method has. Over 2 days, 320 smartphone users (51% female, age range = 18–80, M = 39.53 years) answered up to 20 experience-sampling questionnaires about their social behavior and reconstructed their days in a daily diary. Meanwhile, face-to-face and smartphone-mediated social interactions were assessed with mobile sensing. The results showed some agreement between measurements of face-to-face interactions and high agreement between measurements of smartphone-mediated interactions. Still, a large number of social interactions were captured by only one of the methods, and the quality of social interactions is still difficult to capture with mobile sensing. We discuss limitations and the unique benefits of day reconstruction, experience sampling, and mobile sensing for assessing social behavior in daily life.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4385858798",
    "type": "article"
  },
  {
    "title": "A Tutorial on Analyzing Ecological Momentary Assessment Data in Psychological Research With Bayesian (Generalized) Mixed-Effects Models",
    "doi": "https://doi.org/10.1177/25152459241235875",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Jonas Dora; Connor McCabe; Caspar J. Van Lissa; Katie Witkiewitz; Kevin M. King",
    "corresponding_authors": "Jonas Dora",
    "abstract": "In this tutorial, we introduce the reader to analyzing ecological momentary assessment (EMA) data as applied in psychological sciences with the use of Bayesian (generalized) linear mixed-effects models. We discuss practical advantages of the Bayesian approach over frequentist methods and conceptual differences. We demonstrate how Bayesian statistics can help EMA researchers to (a) incorporate prior knowledge and beliefs in analyses, (b) fit models with a large variety of outcome distributions that reflect likely data-generating processes, (c) quantify the uncertainty of effect-size estimates, and (d) quantify the evidence for or against an informative hypothesis. We present a workflow for Bayesian analyses and provide illustrative examples based on EMA data, which we analyze using (generalized) linear mixed-effects models to test whether daily self-control demands predict three different alcohol outcomes. All examples are reproducible, and data and code are available at https://osf.io/rh2sw/. Having worked through this tutorial, readers should be able to adopt a Bayesian workflow to their own analysis of EMA data.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4393286134",
    "type": "article"
  },
  {
    "title": "Implementing <i>Statcheck</i> During Peer Review Is Related to a Steep Decline in Statistical-Reporting Inconsistencies",
    "doi": "https://doi.org/10.1177/25152459241258945",
    "publication_date": "2024-04-01",
    "publication_year": 2024,
    "authors": "Michèle B. Nuijten; Jelte M. Wicherts",
    "corresponding_authors": "Michèle B. Nuijten",
    "abstract": "We investigated whether statistical-reporting inconsistencies could be avoided if journals implement the tool statcheck in the peer-review process. In a preregistered pretest–posttest quasi-experiment covering more than 7,000 articles and more than 147,000 extracted statistics, we compared the prevalence of reported p values that were inconsistent with their degrees of freedom and test statistics in two journals that implemented statcheck in their peer-review process ( Psychological Science and Journal of Experimental and Social Psychology) and two matched control journals ( Journal of Experimental Psychology: General and Journal of Personality and Social Psychology) before and after statcheck was implemented. Preregistered multilevel logistic regression analyses showed that the decrease in both inconsistencies and decision inconsistencies around p = .05 is considerably steeper in statcheck journals than in control journals, offering preliminary support for the notion that statcheck can be a useful tool for journals to avoid statistical-reporting inconsistencies in published articles. We discuss limitations and implications of these findings.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4400067224",
    "type": "article"
  },
  {
    "title": "A Cautionary Note on Using Univariate Methods for Meta-Analytic Structural Equation Modeling",
    "doi": "https://doi.org/10.1177/25152459241274249",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Suzanne Jak; Mike W.‐L. Cheung",
    "corresponding_authors": "",
    "abstract": "Meta-analytic structural equation modeling (MASEM) is an increasingly popular technique in psychology, especially in management and organizational psychology. MASEM refers to fitting structural equation models (SEMs), such as path models or factor models, to meta-analytic data. The meta-analytic data, obtained from multiple primary studies, generally consist of correlations across the variables in the path or factor model. In this study, we contrast the method that is most often applied in management and organizational psychology (the univariate-r method) to several multivariate methods. “Univariate-r” refers to performing multiple univariate meta-analyses to obtain a synthesized correlation matrix as input in an SEM program. In multivariate MASEM, a multivariate meta-analysis is used to synthesize correlation matrices across studies (e.g., generalized least squares, two-stage SEM, one-stage MASEM). We conducted a systematic search on applications of MASEM in the field of management and organizational psychology and showed that reanalysis of the four available data sets using multivariate MASEM can lead to different conclusions than applying univariate-r. In two simulation studies, we show that the univariate-r method leads to biased standard errors of path coefficients and incorrect fit statistics, whereas the multivariate methods generally perform adequately. In the article, we also discuss some issues that possibly hinder researchers from applying multivariate methods in MASEM.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4403509409",
    "type": "article"
  },
  {
    "title": "Peer-Review Guidelines Promoting Replicability and Transparency in Psychological Science",
    "doi": "https://doi.org/10.1177/2515245918806489",
    "publication_date": "2018-11-02",
    "publication_year": 2018,
    "authors": "William E. Davis; Roger Giner‐Sorolla; D. Stephen Lindsay; Jessica P. Lougheed; Matthew C. Makel; Matt E. Meier; Jessie Sun; Leigh Ann Vaughn; John M. Zelenski",
    "corresponding_authors": "Roger Giner‐Sorolla",
    "abstract": "More and more psychological researchers have come to appreciate the perils of common but poorly justified research practices and are rethinking commonly held standards for evaluating research. As this methodological reform expresses itself in psychological research, peer reviewers of such work must also adapt their practices to remain relevant. Reviewers of journal submissions wield considerable power to promote methodological reform, and thereby contribute to the advancement of a more robust psychological literature. We describe concrete practices that reviewers can use to encourage transparency, intellectual humility, and more valid assessments of the methods and statistics reported in articles.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2891110192",
    "type": "article"
  },
  {
    "title": "Examining Psychological Science Through Systematic Meta-Method Analysis: A Call for Research",
    "doi": "https://doi.org/10.1177/2515245919863296",
    "publication_date": "2019-08-22",
    "publication_year": 2019,
    "authors": "Malte Elson",
    "corresponding_authors": "Malte Elson",
    "abstract": "Research synthesis is based on the assumption that when the same association between constructs is observed repeatedly in a field, the relationship is probably real, even if its exact magnitude can be debated. Yet the probability that the relationship is real is a function not only of recurring results, but also of the quality and consistency of the empirical procedures that produced those results and that any meta-analysis necessarily inherits. Standardized protocols in data collection, analysis, and interpretation are foundations of empiricism and a healthy sign of a discipline’s maturity. I propose that meta-analysis as typically applied in psychology will benefit from complementing aggregation of observed effect sizes with systematic examination of the standardization of the methodology that deterministically produced them. I describe potential units of analysis and offer two examples illustrating the benefits of such efforts. Ideally, this synergetic approach will advance theory by improving the quality of meta-analytic inferences.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2969571933",
    "type": "article"
  },
  {
    "title": "Should Psychology Journals Adopt Specialized Statistical Review?",
    "doi": "https://doi.org/10.1177/2515245919858428",
    "publication_date": "2019-07-03",
    "publication_year": 2019,
    "authors": "Tom E Hardwicke; Michael C. Frank; Simine Vazire; Steven N. Goodman",
    "corresponding_authors": "Tom E Hardwicke",
    "abstract": "Readers of peer-reviewed research may assume that the reported statistical analyses supporting scientific claims have been closely scrutinized and surpass a high-quality threshold. However, widespread misunderstanding and misuse of statistical concepts and methods suggests that suboptimal or erroneous statistical practice is routinely overlooked during peer review in psychology. Here, we explore whether psychology journals could ameliorate some of the field’s statistical ailments by adopting specialized statistical review: a focused technical assessment, performed by statistical experts, that addresses the analysis and presentation of quantitative information and supplements regular peer review. We discuss evidence from a recent survey of journal editors suggesting that specialized statistical review may be unusual in psychology journals and is regarded by many editors as unnecessary. We contrast these views with those in the biomedical domain, where statistical review has been considered a partial preventive measure against the improper use of statistics since the late 1970s. We suggest that the current “credibility revolution” presents an opportune occasion for psychology journals to consider adopting specialized statistical review.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2954987451",
    "type": "article"
  },
  {
    "title": "Metascience on Peer Review: Testing the Effects of a Study’s Originality and Statistical Significance in a Field Experiment",
    "doi": "https://doi.org/10.1177/2515245919895419",
    "publication_date": "2020-02-25",
    "publication_year": 2020,
    "authors": "Malte Elson; Markus Huff; Sonja Utz",
    "corresponding_authors": "Malte Elson",
    "abstract": "Peer review has become the gold standard in scientific publishing as a selection method and a refinement scheme for research reports. However, despite its pervasiveness and conferred importance, relatively little empirical research has been conducted to document its effectiveness. Further, there is evidence that factors other than a submission’s merits can substantially influence peer reviewers’ evaluations. We report the results of a metascientific field experiment on the effect of the originality of a study and the statistical significance of its primary outcome on reviewers’ evaluations. The general aim of this experiment, which was carried out in the peer-review process for a conference, was to demonstrate the feasibility and value of metascientific experiments on the peer-review process and thereby encourage research that will lead to understanding its mechanisms and determinants, effectively contextualizing it in psychological theories of various biases, and developing practical procedures to increase its utility.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3006810440",
    "type": "article"
  },
  {
    "title": "Leveraging Containers for Reproducible Psychological Research",
    "doi": "https://doi.org/10.1177/25152459211017853",
    "publication_date": "2021-04-01",
    "publication_year": 2021,
    "authors": "Kristina Wiebels; David Moreau",
    "corresponding_authors": "David Moreau",
    "abstract": "Containers have become increasingly popular in computing and software engineering and are gaining traction in scientific research. They allow packaging up all code and dependencies to ensure that analyses run reliably across a range of operating systems and software versions. Despite being a crucial component for reproducible science, containerization has yet to become mainstream in psychology. In this tutorial, we describe the logic behind containers, what they are, and the practical problems they can solve. We walk the reader through the implementation of containerization within a research workflow with examples using Docker and R. Specifically, we describe how to use existing containers, build personalized containers, and share containers alongside publications. We provide a worked example that includes all steps required to set up a container for a research project and can easily be adapted and extended. We conclude with a discussion of the possibilities afforded by the large-scale adoption of containerization, especially in the context of cumulative, open science, toward a more efficient and inclusive research ecosystem.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3164616207",
    "type": "article"
  },
  {
    "title": "PsyCalibrator: An Open-Source Package for Display Gamma Calibration and Luminance and Color Measurement",
    "doi": "https://doi.org/10.1177/25152459221151151",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Zhicheng Lin; Qi Ma; Yang Zhang",
    "corresponding_authors": "Zhicheng Lin",
    "abstract": "Studies in vision, psychology, and neuroscience often present visual stimuli on digital screens. Crucially, the appearance of visual stimuli depends on properties such as luminance and color, making it critical to measure them. Yet conventional luminance-measuring equipment is not only expensive but also onerous to operate (particularly for novices). Building on previous work, here we present an open-source integrated software package—PsyCalibrator ( https://github.com/yangzhangpsy/PsyCalibrator )—that takes advantage of consumer hardware (SpyderX, Spyder5) and makes luminance/color measurement and gamma calibration accessible and flexible. Gamma calibration based on visual methods (without photometers) is also implemented. PsyCalibrator requires MATLAB (or its free alternative, GNU Octave) and works in Windows, macOS, and Linux. We first validated measurements from SpyderX and Spyder5 by comparing them with professional, high-cost photometers (ColorCAL MKII Colorimeter and Photo Research PR-670 SpectraScan). Validation results show (a) excellent accuracy in linear correction and luminance/color measurement and (b) for practical purposes, low measurement variances. We offer a detailed tutorial on using PsyCalibrator to measure luminance/color and calibrate displays. Finally, we recommend reporting templates to describe simple (e.g., computer-generated shapes) and complex (e.g., naturalistic images and videos) visual stimuli.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4366516899",
    "type": "article"
  },
  {
    "title": "A Delphi Study to Strengthen Research-Methods Training in Undergraduate Psychology Programs",
    "doi": "https://doi.org/10.1177/25152459231213808",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Robert T. Thibault; Deborah Bailey‐Rodriguez; James E. Bartlett; Paul Blazey; Robin J. Green; Madeleine Pownall; Marcus R. Munafò",
    "corresponding_authors": "Robert T. Thibault",
    "abstract": "Psychology programs often emphasize inferential statistical tests over a solid understanding of data and research design. This imbalance may leave graduates underequipped to effectively interpret research and employ data to answer questions. We conducted a two-round modified Delphi to identify the research-methods skills that the UK psychology community deems essential for undergraduates to learn. Participants included 103 research-methods instructors, academics, students, and nonacademic psychologists. Of 78 items included in the consensus process, 34 reached consensus. We coupled these results with a qualitative analysis of 707 open-ended text responses to develop nine recommendations for organizations that accredit undergraduate psychology programs—such as the British Psychological Society. We recommend that accreditation standards emphasize (1) data skills, (2) research design, (3) descriptive statistics, (4) critical analysis, (5) qualitative methods, and (6) both parameter estimation and significance testing; as well as (7) give precedence to foundational skills, (8) promote transferable skills, and (9) create space in curricula to enable these recommendations. Our data and findings can inform modernized accreditation standards to include clearly defined, assessable, and widely encouraged skills that foster a competent graduate body for the contemporary world.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391300488",
    "type": "article"
  },
  {
    "title": "Reproducibility of Published Meta-Analyses on Clinical-Psychological Interventions",
    "doi": "https://doi.org/10.1177/25152459231202929",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Rubén López‐Nicolás; Daniël Lakens; José A López-López; María Rubio‐Aparicio; Alejandro Sandoval‐Lentisco; Carmen López‐Ibáñez; Desirée Blázquez-Rincón; Julio Sánchez‐Meca",
    "corresponding_authors": "Rubén López‐Nicolás",
    "abstract": "Meta-analysis is one of the most useful research approaches, the relevance of which relies on its credibility. Reproducibility of scientific results could be considered as the minimal threshold of this credibility. We assessed the reproducibility of a sample of meta-analyses published between 2000 and 2020. From a random sample of 100 articles reporting results of meta-analyses of interventions in clinical psychology, 217 meta-analyses were selected. We first tried to retrieve the original data by recovering a data file, recoding the data from document files, or requesting it from original authors. Second, through a multistage workflow, we tried to reproduce the main results of each meta-analysis. The original data were retrieved for 67% (146/217) of meta-analyses. Although this rate showed an improvement over the years, in only 5% of these cases was it possible to retrieve a data file ready for reuse. Of these 146, 52 showed a discrepancy larger than 5% in the main results in the first stage. For 10 meta-analyses, this discrepancy was solved after fixing a coding error of our data-retrieval process, and for 15 of them, it was considered approximately reproduced in a qualitative assessment. In the remaining meta-analyses (18%, 27/146), different issues were identified in an in-depth review, such as reporting inconsistencies, lack of data, or transcription errors. Nevertheless, the numerical discrepancies were mostly minor and had little or no impact on the conclusions. Overall, one of the biggest threats to the reproducibility of meta-analysis is related to data availability and current data-sharing practices in meta-analysis.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391548038",
    "type": "article"
  },
  {
    "title": "Tempered Expectations: A Tutorial for Calculating and Interpreting Prediction Intervals in the Context of Replications",
    "doi": "https://doi.org/10.1177/25152459231217932",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Jeffrey S. Spence; David Stanley",
    "corresponding_authors": "Jeffrey S. Spence",
    "abstract": "Over the last decade, replication research in the psychological sciences has become more visible. One way that replication research can be conducted is to compare the results of the replication study with the original study to look for consistency, that is to say, to evaluate whether the original study is “replicable.” Unfortunately, many popular and readily accessible methods for ascertaining replicability, such as comparing significance levels across studies or eyeballing confidence intervals, are generally ill suited to the task of comparing results across studies. To address this issue, we present the prediction interval as a statistic that is effective for determining whether a replication study is inconsistent with the original study. We review the statistical rationale for prediction intervals, demonstrate hand calculations, and provide a walkthrough using an R package for obtaining prediction intervals for means, d values, and correlations. To aid the effective adoption of prediction intervals, we provide guidance on the correct interpretation of results when using prediction intervals in replication research.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4392022385",
    "type": "article"
  },
  {
    "title": "The Incremental Propensity Score Approach for Diversity Science",
    "doi": "https://doi.org/10.1177/25152459241240681",
    "publication_date": "2024-04-01",
    "publication_year": 2024,
    "authors": "Wen Wei Loh; Dongning Ren",
    "corresponding_authors": "",
    "abstract": "Addressing core questions in diversity science requires quantifying causal effects (e.g., what drives social inequities and how to reduce them). Conventional approaches target the average causal effect (ACE), but ACE-based analyses suffer from limitations that undermine their relevance for diversity science. In this article, we introduce a novel alternative from the causal inference literature: the so-called incremental propensity score (IPS). First, we explain why the IPS is well suited for investigating core queries in diversity science. Unlike the ACE, the IPS does not demand conceptualizing unrealistic counterfactual scenarios in which everyone in the population is uniformly exposed versus unexposed to a causal factor. Instead, the IPS focuses on the effect of hypothetically shifting individuals’ chances of being exposed along a continuum. This allows seeing how the effect may be graded, offering a more realistic and policy-relevant quantification of the causal effect than a single ACE estimate. Moreover, the IPS does not require the positivity assumption, a necessary condition for estimating the ACE but which rarely holds in practice. Next, to broaden accessibility, we provide a step-by-step guide on estimating the IPS using R, a free and popular software. Finally, we illustrate the IPS using two real-world examples. The current article contributes to the methodological advancement in diversity science and offers researchers a more realistic, relevant, and meaningful approach.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4396782034",
    "type": "article"
  },
  {
    "title": "Opening the Black Box: A Multilevel Framework for Studying Group Processes",
    "doi": "https://doi.org/10.1177/2515245918823722",
    "publication_date": "2019-03-07",
    "publication_year": 2019,
    "authors": "Jonas W. B. Lang; Paul D. Bliese; Amy B. Adler",
    "corresponding_authors": "Jonas W. B. Lang",
    "abstract": "Over time, groups can change in at least two important ways. First, they can display different trajectories (e.g., increases or decreases) on constructs of interest. Second, the configuration of group members’ responses within a group can change, such that the members become more or less similar to each other. Psychologists have historically been interested in understanding changes in groups over time; however, there is currently no comprehensive quantitative framework for studying and modeling group processes over time. We present a multilevel framework for such research—the multilevel group-process framework (MGPF). The MGPF builds on a statistical approach developed to capture whether individual members of a group develop a shared climate over time, but we extend the core ideas in two important ways. First, we describe how researchers can gain insights into group phenomena such as group leniency, group learning, groupthink, group extremity, group forming, group freezing, and group adjourning through modeling change in latent mean levels and consensus. Second, we present a sequence of model-testing steps that enable researchers to systematically study and contrast different group processes. We describe how the MGPF can lead to novel research questions and illustrate its use in two example data sets.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2903497288",
    "type": "article"
  },
  {
    "title": "Models to Examine the Validity of Cluster-Level Factor Structure Using Individual-Level Data",
    "doi": "https://doi.org/10.1177/2515245919855039",
    "publication_date": "2019-08-21",
    "publication_year": 2019,
    "authors": "Laura M. Stapleton; Tessa L. Johnson",
    "corresponding_authors": "Laura M. Stapleton",
    "abstract": "When researchers model multilevel data, often a shared construct of interest is measured by individual-level observations, for example, students’ responses regarding how engaging their instructor’s teaching style is. In such cases, the construct of interest, “engaging teaching,” is shared at the cluster level across individuals, yet rarely are these shared constructs modeled as such. To address this gap, we discuss multilevel confirmatory factor analysis models that have been applied to item-level data obtained from multiple raters within given clusters, focusing particularly on measuring a characteristic at the cluster level. After discussing the parameters in each potential model, we make recommendations as to the appropriate modeling approach and the steps to be taken for model assessment given a set of data and hypothesized construct of interest. In particular, we encourage applied researchers not to use a model without constraints across the within-cluster level and the between-cluster level because such models assume that the average amount of the individual-level construct in a cluster does not differ across clusters. To illustrate this issue, we present simulation results and evaluate a series of models using empirical data from the Trends in International Mathematics and Science Study.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2969426311",
    "type": "article"
  },
  {
    "title": "Commentary on Hussey and Hughes (2020): Hidden Invalidity Among 15 Commonly Used Measures in Social and Personality Psychology",
    "doi": "https://doi.org/10.1177/2515245920957618",
    "publication_date": "2020-10-15",
    "publication_year": 2020,
    "authors": "Eunike Wetzel; Brent W. Roberts",
    "corresponding_authors": "Eunike Wetzel",
    "abstract": "",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3092754782",
    "type": "article"
  },
  {
    "title": "Boundary Conditions for the Practical Importance of Small Effects in Long Runs: A Comment on Funder and Ozer (2019)",
    "doi": "https://doi.org/10.1177/2515245920957607",
    "publication_date": "2020-10-07",
    "publication_year": 2020,
    "authors": "James D. Sauer; Aaron Drummond",
    "corresponding_authors": "James D. Sauer",
    "abstract": "Funder and Ozer (2019) argued that small effects can have\r\nimportant implications in cumulative long-run scenarios.\r\nWe certainly agree. However, some important caveats\r\nmerit explicit consideration. We elaborate on the previously acknowledged importance of preregistration (and\r\nopen-data practices) and identify two additional considerations for interpreting small effects in long-run scenarios: restricted extrapolation and construct validity",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3092407997",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Registered Replication of Vohs and Schooler (2008), Experiment 1",
    "doi": "https://doi.org/10.1177/2515245920917931",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Nicholas R. Buttrick; Balázs Aczél; Lena Fanya Aeschbach; Bence E. Bakos; Florian Brühlmann; Heather M. Claypool; Joachim Hüffmeier; Márton Kovács; Kurt Schuepfer; Péter Szécsi; Attila Szuts; Orsolya Szöke; Manuela Thomae; Ann-Kathrin Torka; Ryan J. Walker; Michael Wood",
    "corresponding_authors": "Nicholas R. Buttrick",
    "abstract": "Does convincing people that free will is an illusion reduce their sense of personal responsibility? Vohs and Schooler (2008) found that participants reading from a passage “debunking” free will cheated more on experimental tasks than did those reading from a control passage, an effect mediated by decreased belief in free will. However, this finding was not replicated by Embley, Johnson, and Giner-Sorolla (2015), who found that reading arguments against free will had no effect on cheating in their sample. The present study investigated whether hard-to-understand arguments against free will and a low-reliability measure of free-will beliefs account for Embley et al.’s failure to replicate Vohs and Schooler’s results. Participants ( N = 621) were randomly assigned to participate in either a close replication of Vohs and Schooler’s Experiment 1 based on the materials of Embley et al. or a revised protocol, which used an easier-to-understand free-will-belief manipulation and an improved instrument to measure free will. We found that the revisions did not matter. Although the revised measure of belief in free will had better reliability than the original measure, an analysis of the data from the two protocols combined indicated that free-will beliefs were unchanged by the manipulations, d = 0.064, 95% confidence interval = [−0.087, 0.22], and in the focal test, there were no differences in cheating behavior between conditions, d = 0.076, 95% CI = [−0.082, 0.22]. We found that expressed free-will beliefs did not mediate the link between the free-will-belief manipulation and cheating, and in exploratory follow-up analyses, we found that participants expressing lower beliefs in free will were not more likely to cheat in our task.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3104524549",
    "type": "article"
  },
  {
    "title": "Information Provision for Informed Consent Procedures in Psychological Research Under the General Data Protection Regulation: A Practical Guide",
    "doi": "https://doi.org/10.1177/25152459231151944",
    "publication_date": "2023-01-01",
    "publication_year": 2023,
    "authors": "Dara Hallinan; Franziska Boehm; Annika Külpmann; Malte Elson",
    "corresponding_authors": "Dara Hallinan",
    "abstract": "Psychological research often involves the collection and processing of personal data from human research participants. The European General Data Protection Regulation (GDPR) applies, as a rule, to psychological research conducted on personal data in the European Economic Area (EEA)—and even, in certain cases, to psychological research conducted on personal data outside the EEA. The GDPR elaborates requirements concerning the forms of information that should be communicated to research participants whenever personal data are collected directly from them. There is a general norm that informed consent should be obtained before psychological research involving the collection of personal data directly from research participants is conducted. The information required to be provided under the GDPR is normally communicated in the context of an informed consent procedure. There is reason to believe, however, that the information required by the GDPR may not always be provided. Our aim in this tutorial is thus to provide general practical guidance to psychological researchers allowing them to understand the forms of information that must be provided to research participants under the GDPR in informed consent procedures.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4322577109",
    "type": "article"
  },
  {
    "title": "Estimating Evidential Value From Analysis of Variance Summaries: A Comment on Ly et al. (2018)",
    "doi": "https://doi.org/10.1177/2515245919872960",
    "publication_date": "2019-09-13",
    "publication_year": 2019,
    "authors": "Thomas J. Faulkenberry",
    "corresponding_authors": "Thomas J. Faulkenberry",
    "abstract": "",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2972415491",
    "type": "article"
  },
  {
    "title": "Rock the MIC: The Matrix of Implied Causation, a Tool for Experimental Design and Model Checking",
    "doi": "https://doi.org/10.1177/2515245920922775",
    "publication_date": "2020-06-25",
    "publication_year": 2020,
    "authors": "Timothy R. Brick; Drew H. Bailey",
    "corresponding_authors": "Timothy R. Brick",
    "abstract": "Path modeling and the extended structural equation modeling framework are in increasingly common use for statistical analysis in modern behavioral science. Path modeling, including structural equation modeling, provides a flexible means of defining complex models in a way that allows them to be easily visualized, specified, and fitted to data. Although causality cannot be determined simply by fitting a path model, researchers often use such models as representations of underlying causal-process models. Indeed, causal implications are a vital characteristic of a model’s explanatory value, but these implications are rarely examined directly. When models are hypothesized to be causal, they can be differentiated from one another by examining their causal implications as defined by a combination of the model assumptions, data, and estimation procedure. However, the implied causal relationships may not be immediately obvious to researchers, especially for intricate or long-chain causal structures (as in longitudinal panel designs). We introduce the matrix of implied causation (MIC) as a tool for easily understanding and reporting a model’s implications for the causal influence of one variable on another. With examples from the literature, we illustrate the use of MICs in model checking and experimental design. We argue that MICs should become a routine element of interpretation when models with complex causal implications are examined, and that they may provide an additional tool for differentiating among models with otherwise similar fit.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3037665468",
    "type": "article"
  },
  {
    "title": "Corrigendum: Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results",
    "doi": "https://doi.org/10.1177/2515245918810511",
    "publication_date": "2018-10-24",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4234671895",
    "type": "erratum"
  },
  {
    "title": "A Cautionary Note on Estimating Effect Size",
    "doi": "https://doi.org/10.1177/2515245921992035",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Don van den Bergh; Julia M. Haaf; Alexander Ly; Jeffrey N. Rouder; Eric‐Jan Wagenmakers",
    "corresponding_authors": "Don van den Bergh",
    "abstract": "An increasingly popular approach to statistical inference is to focus on the estimation of effect size. Yet this approach is implicitly based on the assumption that there is an effect while ignoring the null hypothesis that the effect is absent. We demonstrate how this common null-hypothesis neglect may result in effect size estimates that are overly optimistic. As an alternative to the current approach, a spike-and-slab model explicitly incorporates the plausibility of the null hypothesis into the estimation process. We illustrate the implications of this approach and provide an empirical example.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3139330367",
    "type": "article"
  },
  {
    "title": "How Do We Choose Our Giants? Perceptions of Replicability in Psychological Science",
    "doi": "https://doi.org/10.1177/25152459211018199",
    "publication_date": "2021-04-01",
    "publication_year": 2021,
    "authors": "Manikya Alister; Raine Vickers-Jones; David K. Sewell; Timothy Ballard",
    "corresponding_authors": "Timothy Ballard",
    "abstract": "Judgments regarding replicability are vital to scientific progress. The metaphor of “standing on the shoulders of giants” encapsulates the notion that progress is made when new discoveries build on previous findings. Yet attempts to build on findings that are not replicable could mean a great deal of time, effort, and money wasted. In light of the recent “crisis of confidence” in psychological science, the ability to accurately judge the replicability of findings may be more important than ever. In this Registered Report, we examine the factors that influence psychological scientists’ confidence in the replicability of findings. We recruited corresponding authors of articles published in psychology journals between 2014 and 2018 to complete a brief survey in which they were asked to consider 76 specific study attributes that might bear on the replicability of a finding (e.g., preregistration, sample size, statistical methods). Participants were asked to rate the extent to which information regarding each attribute increased or decreased their confidence in the finding being replicated. We examined the extent to which each research attribute influenced average confidence in replicability. We found evidence for six reasonably distinct underlying factors that influenced these judgments and individual differences in the degree to which people’s judgments were influenced by these factors. The conclusions reveal how certain research practices affect other researchers’ perceptions of robustness. We hope our findings will help encourage the use of practices that promote replicability and, by extension, the cumulative progress of psychological science.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3167586170",
    "type": "article"
  },
  {
    "title": "Four Internal Inconsistencies in Tversky and Kahneman’s (1992) Cumulative Prospect Theory Article: A Case Study in Ambiguous Theoretical Scope and Ambiguous Parsimony",
    "doi": "https://doi.org/10.1177/25152459221074653",
    "publication_date": "2022-01-01",
    "publication_year": 2022,
    "authors": "Michel Regenwetter; Maria M. Robinson; Cihang Wang",
    "corresponding_authors": "Michel Regenwetter",
    "abstract": "Scholars heavily rely on theoretical scope as a tool to challenge existing theory. We advocate that scientific discovery could be accelerated if far more effort were invested into also overtly specifying and painstakingly delineating the intended purview of any proposed new theory at the time of its inception. As a case study, we consider Tversky and Kahneman (1992). They motivated their Nobel-Prize-winning cumulative prospect theory with evidence that in each of two studies, roughly half of the participants violated independence, a property required by expected utility theory (EUT). Yet even at the time of inception, new theories may reveal signs of their own limited scope. For example, we show that Tversky and Kahneman’s findings in their own test of loss aversion provide evidence that at least half of their participants violated their theory, in turn, in that study. We highlight a combination of conflicting findings in the original article that make it ambiguous to evaluate both cumulative prospect theory’s scope and its parsimony on the authors’ own evidence. The Tversky and Kahneman article is illustrative of a social and behavioral research culture in which theoretical scope plays an extremely asymmetric role: to call existing theory into question and motivate surrogate proposals.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4226107087",
    "type": "article"
  },
  {
    "title": "Evaluating Implementation of the Transparency and Openness Promotion Guidelines: Reliability of Instruments to Assess Journal Policies, Procedures, and Practices",
    "doi": "https://doi.org/10.1177/25152459221149735",
    "publication_date": "2023-01-01",
    "publication_year": 2023,
    "authors": "Sina Kianersi; Sean Grant; Kevin Naaman; Beate Henschel; David Thomas Mellor; Shruti Apte; Jessica E. Deyoe; Paul Eze; Cuiqiong Huo; Bethany L. Lavender; Nicha Taschanchai; Xinlu Zhang; Evan Mayo‐Wilson",
    "corresponding_authors": "",
    "abstract": "The Transparency and Openness Promotion (TOP) Guidelines describe modular standards that journals can adopt to promote open science. The TOP Factor quantifies the extent to which journals adopt TOP in their policies, but there is no validated instrument to assess TOP implementation. Moreover, raters might assess the same policies differently. Instruments with objective questions are needed to assess TOP implementation reliably. In this study, we examined the interrater reliability and agreement of three new instruments for assessing TOP implementation in journal policies (instructions to authors), procedures (manuscript-submission systems), and practices (journal articles). Independent raters used these instruments to assess 339 journals from the behavioral, social, and health sciences. We calculated interrater agreement (IRA) and interrater reliability (IRR) for each of 10 TOP standards and for each question in our instruments (13 policy questions, 26 procedure questions, 14 practice questions). IRA was high for each standard in TOP; however, IRA might have been high by chance because most standards were not implemented by most journals. No standard had “excellent” IRR. Three standards had “good,” one had “moderate,” and six had “poor” IRR. Likewise, IRA was high for most instrument questions, and IRR was moderate or worse for 62%, 54%, and 43% of policy, procedure, and practice questions, respectively. Although results might be explained by limitations in our process, instruments, and team, we are unaware of better methods for assessing TOP implementation. Clarifying distinctions among different levels of implementation for each TOP standard might improve its implementation and assessment (study protocol: https://doi.org/10.1186/s41073-021-00112-8 ).",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4361852460",
    "type": "article"
  },
  {
    "title": "When to Use Different Inferential Methods for Power Analysis and Data Analysis for Between-Subjects Mediation",
    "doi": "https://doi.org/10.1177/25152459231156606",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Jessica Fossum; Amanda Kay Montoya",
    "corresponding_authors": "Jessica Fossum",
    "abstract": "Several options exist for conducting inference on indirect effects in mediation analysis. Although methods that use bootstrapping are the preferred inferential approach for testing mediation, they are time-consuming when the test must be performed many times for a power analysis. Alternatives that are more computationally efficient are not as robust, meaning accuracy of the inferences from these methods is more affected by nonnormal and heteroskedastic data. Previous research has shown that different sample sizes are needed to achieve the same amount of statistical power for different inferential approaches with data that meet all the statistical assumptions of linear regression. By contrast, we explore how similar power estimates are at the same sample size, including when assumptions are violated. We compare the power estimates from six inferential methods for between-subjects mediation using a Monte Carlo simulation study. We varied the path coefficients, inferential methods for the indirect effect, and degree to which assumptions are met. We found that when the assumptions of linear regression are met, three inferential methods consistently perform similarly: the joint significance test, the Monte Carlo confidence interval, and the percentile bootstrap confidence interval. When the assumptions were violated, the nonbootstrapping methods tended to have vastly different power estimates compared with the bootstrapping methods. On the basis of these results, we recommend using the more computationally efficient joint significance test for power analysis only when no assumption violations are hypothesized a priori. We also recommend the joint significance test to pick an optimal starting sample size value for power analysis using the percentile bootstrap confidence interval when assumption violations are suspected.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4376127120",
    "type": "article"
  },
  {
    "title": "Improving Statistical Analysis in Team Science: The Case of a Bayesian Multiverse of Many Labs 4",
    "doi": "https://doi.org/10.1177/25152459231182318",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Suzanne Hoogeveen; Sophie Wilhelmina Berkhout; Quentin F. Gronau; Eric‐Jan Wagenmakers; Julia M. Haaf",
    "corresponding_authors": "Suzanne Hoogeveen",
    "abstract": "Team-science projects have become the “gold standard” for assessing the replicability and variability of key findings in psychological science. However, we believe the typical meta-analytic approach in these projects fails to match the wealth of collected data. Instead, we advocate the use of Bayesian hierarchical modeling for team-science projects, potentially extended in a multiverse analysis. We illustrate this full-scale analysis by applying it to the recently published Many Labs 4 project. This project aimed to replicate the mortality-salience effect—that being reminded of one’s own death strengthens the own cultural identity. In a multiverse analysis, we assess the robustness of the results with varying data-inclusion criteria and prior settings. Bayesian model comparison results largely converge to a common conclusion: The data provide evidence against a mortality-salience effect across the majority of our analyses. We issue general recommendations to facilitate full-scale analyses in team-science projects.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4386123375",
    "type": "article"
  },
  {
    "title": "Visualization of Composite Plots in R Using a Programmatic Approach and <i>smplot2</i>",
    "doi": "https://doi.org/10.1177/25152459241267927",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Seung Hyun Min",
    "corresponding_authors": "Seung Hyun Min",
    "abstract": "In psychology and human neuroscience, the practice of creating multiple subplots and combining them into a composite plot has become common because the nature of research has become more multifaceted and sophisticated. In the last decade, the number of methods and tools for data visualization has surged. For example, R, a programming language, has become widely used in part because of ggplot2, a free, open-source, and intuitive plotting library. However, despite its strength and ubiquity, it has some built-in restrictions that are most noticeable when one creates a composite plot, which currently involves a complex and repetitive process with steps that go against the principles of open science out of necessity. To address this issue, I introduce smplot2, an open-source R package that integrates ggplot2’s declarative syntax and a programmatic approach to plotting. The package aims to enable users to create customizable composite plots by linearizing the process of complex visualization. The documentation and code examples of the smplot2 package are available online ( https://smin95.github.io/dataviz ).",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402975488",
    "type": "article"
  },
  {
    "title": "Registered Replication Report: A Large Multilab Cross-Cultural Conceptual Replication of Turri et al. (2015)",
    "doi": "https://doi.org/10.1177/25152459241267902",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Braeden Hall; Kathleen Schmidt; Jordan Wagge; Savannah C Lewis; Sophia Christin Weißgerber; Felix Kiunke; Gerit Pfuhl; Stefan Stieger; Ulrich S. Tran; Krystian Barzykowski; Natalia Bogatyreva; Marta Kowal; Karlijn Massar; Felizitas Pernerstofer; Piotr Sorokowski; Martin Voracek; Christopher R. Chartier; Mark J. Brandt; Jon Grahe; Asil Ali Özdoğru; Michael R. Andreychik; Sau-Chin Chen; Thomas Rhys Evans; Caro Hautekiet; Hans IJzerman; Pavol Kačmár; Anthony J. Krafnick; Erica D. Musser; Evie Vergauwe; Kaitlyn M. Werner; Balázs Aczél; Patrí­cia Arriaga; Carlota Batres; Jennifer L Beaudry; Florián Cova; Simona Ďurbisová; Leslie D. Cramblet Alvarez; Gilad Feldman; Hendrik Godbersen; Jaroslav Gottfried; Gerald J. Haeffel; Andree Hartanto; Chris Isloi; Joseph P. McFall; Marina Milyavskaya; David Moreau; Ester Nosáľová; Kostas Papaioannou; Susana Ruiz Fernández; Jana Schrötter; Daniel Storage; Kévin Vezirian; Leonhard Volz; Yanna J. Weisberg; Qinyu Xiao; Dana Awlia; Hannah W. Branit; Megan R. Dunn; Agata Groyecka-Bernard; Ricky Haneda; Julita Kielińska; Caroline Kolle; Paweł Lubomski; Alexys M. Miller; Martin Jensen Mækelæ; Myrto Pantazi; Rafael Ramos Ribeiro; Robert M. Ross; Agnieszka Sorokowska; Christopher L. Aberson; Xanthippi Alexi Vassiliou; Bradley J. Baker; M. Bognár; Chin Wen Cong; Alex F. Danvers; William E. Davis; Vilius Dranseika; Andrei Dumbravă; Harry Farmer; Andy P. Field; Patrick S. Forscher; Aurélien Graton; Nándor Hajdú; Peter Howlett; Radosław Kabut; Emmett M. Larsen; Sean T. H. Lee; Nicole Legate; Carmel Levitan; Neil Levy; Jackson G. Lu; Michał Misiak; Roxana E. Morariu; Jennifer Novak; Ekaterina Pronizius; Irina Prusova; Athulya S. Rathnayake; Marina Romanova; Jan Philipp Röer; Waldir M. Sampaio",
    "corresponding_authors": "",
    "abstract": "According to the justified true belief (JTB) account of knowledge, people can truly know something only if they have a belief that is both justified and true (i.e., knowledge is JTB). This account was challenged by Gettier, who argued that JTB does not explain knowledge attributions in certain situations, later called “Gettier-type cases,” wherein protagonists are justified in believing something to be true, but their belief was correct only because of luck. Laypeople may not attribute knowledge to protagonists with justified but only luckily true beliefs. Although some research has found evidence for these so-called Gettier intuitions, Turri et al. found no evidence that participants attributed knowledge in a counterfeit-object Gettier-type case differently than in a matched case of JTB. In a large-scale, cross-cultural conceptual replication of Turri and colleagues’ Experiment 1 ( N = 4,724) using a within-participants design and three vignettes across 19 geopolitical regions, we did find evidence for Gettier intuitions; participants were 1.86 times more likely to attribute knowledge to protagonists in standard cases of JTB than to protagonists in Gettier-type cases. These results suggest that Gettier intuitions may be detectable across different scenarios and cultural contexts. However, the size of the Gettier intuition effect did vary by vignette, and the Turri et al. vignette produced the smallest effect, which was similar in size to that observed in the original study. Differences across vignettes suggest that epistemic intuitions may also depend on contextual factors unrelated to the criteria of knowledge, such as the characteristics of the protagonist being evaluated.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4403819045",
    "type": "article"
  },
  {
    "title": "Preprocessing Experience-Sampling-Method Data: A Step-by-Step Framework, Tutorial Website, R Package, and Reporting Templates",
    "doi": "https://doi.org/10.1177/25152459241256609",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Jordan Revol; Chiara Carlier; Ginette Lafit; Martine W. F. T. Verhees; Laura Sels; Eva Ceulemans",
    "corresponding_authors": "",
    "abstract": "Experience-sampling-method (ESM) studies have become a very popular tool to gain insight into the dynamics of psychological processes. Although the statistical modeling of ESM data has been widely studied, the preprocessing steps that precede such modeling have received relatively limited attention despite being a challenging phase. At the same time, adequate preprocessing of ESM data is crucial: It provides valuable information about the quality of the data and, importantly, helps to resolve issues in the data that may compromise the validity of statistical analyses. To support researchers in properly preprocessing ESM data, we have developed a step-by-step framework, a tutorial website that provides a gallery of R code, an R package, and templates to report the preprocessing steps. Particular attention is given to three different aspects in preprocessing: checking adherence to the study design (e.g., whether the momentary questionnaires were delivered according to the sampling scheme), examining participants’ response behaviors (e.g., compliance, careless responding), and describing and visualizing the data (e.g., examining distributions of variables).",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4403932529",
    "type": "article"
  },
  {
    "title": "How Statistical Challenges and Misreadings of the Literature Combine to Produce Unreplicable Science: An Example From Psychology",
    "doi": "https://doi.org/10.1177/25152459241276398",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Andrew Gelman; Nicholas J. L. Brown",
    "corresponding_authors": "",
    "abstract": "Given the well-known problems of replicability, how is it that researchers at respected institutions continue to publish and publicize studies that are fatally flawed in the sense of not providing evidence to support their strong claims? We argue that two general problems are (a) difficulties of analyzing data with multilevel structure and (b) misinterpretation of the literature. We demonstrate with the example of a recently published claim that altering patients’ subjective perception of time can have a notable effect on physical healing. We discuss ways of avoiding or at least reducing such problems, including comparing final results with simpler analyses, moving away from shot-in-the-dark phenomenological studies, and more carefully examining previous published claims. Making incorrect choices in multilevel modeling is just one way that things can go wrong, but this example also provides a window into more general problems with complicated designs, cutting-edge statistical methods, and the connections between substantive theory, experimental design, data collection, and replication.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4404608509",
    "type": "article"
  },
  {
    "title": "The Comedy of Measurement Errors: Standard Error of Measurement and Standard Error of Estimation",
    "doi": "https://doi.org/10.1177/25152459241285885",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "David T. Stanley; Jeffrey S. Spence",
    "corresponding_authors": "",
    "abstract": "Testing is used to inform a range of critical decisions that help structure much of contemporary society. An unavoidable aspect of testing is that test scores are not infallible. As a result, individual test scores should be accompanied by an interval that indicates the uncertainty surrounding the score. There are a number of different test-score intervals that can be created from different error terms. Unfortunately, there are pervasive misinterpretations of these errors and their intervals. Many of these interpretations can be found in authoritative sources on psychological measurement, which has resulted in stubborn and persistent confusion about what these intervals mean. In the current article, we clarify two important error terms and their intervals: (a) the Standard Error of Estimation and (b) the Standard Error of Measurement. We explicate the meaning and interpretation of these errors by examining their statistical foundations. Specifically, we detail how these terms are formulated from different statistical models and the implications of these models for their different interpretations. We use classical test theory, bivariate linear regression, R activities, and algebra to illustrate the key concepts and differences.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4404685516",
    "type": "article"
  },
  {
    "title": "Updating a Classic: A New Generation of Vignette Experiments Involving Iterative Decision Making",
    "doi": "https://doi.org/10.1177/2515245917742982",
    "publication_date": "2018-01-22",
    "publication_year": 2018,
    "authors": "Gili Freedman; Max Seidman; Mary Flanagan; Melanie C. Green; Geoff Kaufman",
    "corresponding_authors": "Gili Freedman",
    "abstract": "Although the vignette method is widely used in psychology, it is often implemented without the key feature of iterative decision making that can affect the eventual outcome of the vignettes. This Tutorial provides an explanation of how to use Twine, an interactive narrative platform, to create vignettes with iterative decision making. Twine is an especially useful tool for experiments involving branching narratives, spatial navigation, and resource allocation. We provide code for creating exemplar experiments in social and cognitive psychology, as well as behavioral economics, and explain how to integrate Twine projects with survey-management platforms, such as Qualtrics. After following this Tutorial, researchers will be able to use Twine in their experiments to update the classic vignette method by incorporating iterative decision-making tasks.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2784453701",
    "type": "article"
  },
  {
    "title": "A Simple, Principled Approach to Combining Evidence From Meta-Analysis and High-Quality Replications",
    "doi": "https://doi.org/10.1177/2515245918756858",
    "publication_date": "2018-04-04",
    "publication_year": 2018,
    "authors": "Evan C. Carter; Michael E. McCullough",
    "corresponding_authors": "Evan C. Carter",
    "abstract": "Recent discussions of the influence of publication bias and questionable research practices on psychological science have increased researchers’ interest in both bias-correcting meta-analytic techniques and preregistered replication. Both approaches have their strengths: For example, meta-analyses can quantitatively characterize the full body of work done in the field of interest, and preregistered replications can be immune to bias. Both approaches also have clear weaknesses: Decisions about which meta-analytic estimates to interpret tend to be controversial, and replications can be discounted for failing to address important methodological heterogeneity. Using the experimental literature on ego depletion as a case study, we illustrate a principled approach to combining information from meta-analysis with information from subsequently conducted high-quality replications. This approach (a) compels researchers to explicate their beliefs in meta-analytic conclusions (and also, when controversy arises, to defend the basis for those beliefs), (b) encourages consideration of practical significance, and (c) facilitates the process of planning replications by specifying the sample sizes necessary to have a reasonable chance of changing the minds of other researchers.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2796160738",
    "type": "article"
  },
  {
    "title": "Clarifying the Choice of Confidence Intervals in Psychological Testing: A Comment on Stanley and Spence (2024)",
    "doi": "https://doi.org/10.1177/25152459251328281",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Stefan C. Schmukle; Julia M. Rohrer",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409631953",
    "type": "article"
  },
  {
    "title": "Implications of the Death-Thought-Suppression-and-Rebound Assumption: Integrating the Findings of Rife et al. (2025) and Trafimow and Hughes (2012)",
    "doi": "https://doi.org/10.1177/25152459251328348",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "David Trafimow",
    "corresponding_authors": "David Trafimow",
    "abstract": "Researchers on terror-management theory (TMT) often obtain effects on dependent variables, such as worldview assertion, after a delay following mortality salience (contemplating death) but not immediately. As justification, TMT researchers invoked a post hoc assumption: Death thoughts are immediately suppressed following mortality salience but rebound after a delay. In contradiction, Trafimow and Hughes and Rife et al. found that death thoughts are more accessible immediately following mortality salience than after a delay. The contradiction is so problematic that ignoring it trends toward degenerative science. TMT research might exemplify a larger problem in psychology.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409672321",
    "type": "article"
  },
  {
    "title": "Corrigendum to “dockerHDDM: A User-Friendly Environment for Bayesian Hierarchical Drift-Diffusion Modeling”",
    "doi": "https://doi.org/10.1177/25152459251331976",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409718140",
    "type": "erratum"
  },
  {
    "title": "Multilab Replications Provide Theoretical and Methodological Insight but Not Necessarily About the Studies They Seek to Replicate: Comment on Rife et al. (2025)",
    "doi": "https://doi.org/10.1177/25152459251336134",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "David A. Lishner; Christopher L. Groves",
    "corresponding_authors": "",
    "abstract": "Rife et al. conducted a multilab replication of Trafimow and Hughes (Experiment 3) to examine whether mortality salience produces higher death-thought accessibility immediately after imagining one’s own death or after a time delay. Like Trafimow and Huges, Rife et al. found that thinking about death without delay produced higher death-thought accessibility than when thinking about death with delay or thinking about dental pain. This pattern occurred regardless of whether participants were randomly assigned the original Trafimow and Hughes word-generation death-thought-accessibility measure or assigned a word-fragment death-thought-accessibility measure more commonly used in the literature. However, we argue that regardless of whether multilab replications produce results consistent with the original replicated study, they offer weak insight into the integrity and empirical plausibility of the original study. Instead, multilab replications provide valuable theoretical and methodological information regarding the span of effect created by the procedural elements shared among the set of replication studies. This information, in turn, permits clearer theoretical inference when using similar procedural elements to investigate theoretically related phenomena. Moreover, multilab replications are often conceived as a defense against false-positive empirical results and theoretical interpretations, but Rife et al.’s results reveal they may be better suited for protecting against false-negative empirical results and theoretical interpretations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410287858",
    "type": "article"
  },
  {
    "title": "An Expert Guide to Planning Experimental Tasks For Evidence-Accumulation Modeling",
    "doi": "https://doi.org/10.1177/25152459251336127",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Russell J. Boag; Reilly James Innes; Niek Stevenson; Giwon Bahg; Jerome R. Busemeyer; Gregory E. Cox; Chris Donkin; Michael J. Frank; Guy E. Hawkins; Andrew Heathcote; Craig Hedge; Veronika Lerche; Simon D. Lilburn; Gordon D. Logan; Dóra Matzke; Steven Miletić; Adam F Osth; Thomas J. Palmeri; Per B. Sederberg; Henrik Singmann; Philip L. Smith; Tom Stafford; Mark Steyvers; Luke Strickland; Jennifer S. Trueblood; Konstantinos Tsetsos; Brandon M. Turner; Marius Usher; Leendert van Maanen; Don van Ravenzwaaij; Joachim Vandekerckhove; Andreas Voss; Emily Ruth Weichart; Gabriel Weindel; Corey N. White; Nathan J. Evans; Scott Brown; Birte U. Forstmann",
    "corresponding_authors": "",
    "abstract": "Evidence-accumulation models (EAMs) are powerful tools for making sense of human and animal decision-making behavior. EAMs have generated significant theoretical advances in psychology, behavioral economics, and cognitive neuroscience and are increasingly used as a measurement tool in clinical research and other applied settings. Obtaining valid and reliable inferences from EAMs depends on knowing how to establish a close match between model assumptions and features of the task/data to which the model is applied. However, this knowledge is rarely articulated in the EAM literature, leaving beginners to rely on the private advice of mentors and colleagues and inefficient trial-and-error learning. In this article, we provide practical guidance for designing tasks appropriate for EAMs, relating experimental manipulations to EAM parameters, planning appropriate sample sizes, and preparing data and conducting an EAM analysis. Our advice is based on prior methodological studies and the our substantial collective experience with EAMs. By encouraging good task-design practices and warning of potential pitfalls, we hope to improve the quality and trustworthiness of future EAM research and applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410768466",
    "type": "article"
  },
  {
    "title": "The Design of Current Replication Studies: A Systematic Literature Review on the Variation of Study Characteristics",
    "doi": "https://doi.org/10.1177/25152459251328273",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "J Hoffmann; Mathias Twardawski; Johanna M. Höhs; Anne Gast; Steffi Pohl; Marie‐Ann Sengewald",
    "corresponding_authors": "",
    "abstract": "In what aspects do replication studies differ from their primary studies? This question is central for providing insights into the reasons for the nonreplicability of psychological effects. So far, research on potential explanations for the nonreplicability of effects has mainly focused on publication bias and methodological challenges related to measurement error or statistical inference. The recently developed causal-replication framework directs attention toward controlling for differences in study characteristics, including variations in treatment conditions, outcome measures, recruitment, causal estimates, time, location, population, and setting. To contribute to this aim, we conducted a systematic literature review to investigate the design practices of current replication studies. We preregistered the assessment of study characteristics in a detailed review protocol and investigated the available information and intended or unintended variations across primary and replication studies. To do this, we compiled a database of studies that aimed to replicate a causal effect of a clearly stated primary study and that were published in impactful social- and cognitive-psychological journals between January 2017 and August 2022. Our review results highlight that compared with the primary study, authors of replication studies predominantly focus on controlling specific study characteristics in (i.e., methods, procedures, analysis) while often neglecting other study characteristics, such as population or setting. Furthermore, the results indicate that in most replication studies, multiple study characteristics are varied in the study comparison or are insufficiently reported. Accordingly, we discuss prevalent variations, reporting standards, and strategies for planning future replication studies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410884114",
    "type": "article"
  },
  {
    "title": "Does Truth Pay? Investigating the Effectiveness of the Bayesian Truth Serum With an Interim Payment: A Registered Report",
    "doi": "https://doi.org/10.1177/25152459251343043",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Claire Marie Neville; Matt N Williams",
    "corresponding_authors": "",
    "abstract": "Self-report data are vital in psychological research, but biases such as careless responding and socially desirable responding can compromise their validity. Although various methods are employed to mitigate these biases, they have limitations. The Bayesian truth serum (BTS) offers a survey scoring method to incentivize truthfulness by leveraging correlations between personal and collective opinions and rewarding “surprisingly common” responses. In this study, we evaluated the effectiveness of the BTS in mitigating socially desirable responding to sensitive questions and tested whether an interim payment could enhance its efficacy by increasing trust. In a between-subjects experimental survey, 877 participants were randomly assigned to one of three conditions: BTS, BTS with interim payment, and regular incentive (RI). Contrary to the hypotheses, participants in the BTS conditions displayed lower agreement with socially undesirable statements compared with the RI condition. The interim payment did not significantly enhance the BTS’s effectiveness. Instead, response patterns diverged from the mechanism’s intended effects, raising concerns about its robustness. As the second registered report to challenge its efficacy, this study’s results cast serious doubt on the BTS as a reliable tool for mitigating socially desirable responding and improving the validity of self-report data in psychological research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412076096",
    "type": "article"
  },
  {
    "title": "From Embeddings to Explainability: A Tutorial on Large-Language-Model-Based Text Analysis for Behavioral Scientists",
    "doi": "https://doi.org/10.1177/25152459251351285",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Rudolf Debelak; T. Koch; Matthias Aßenmacher; Clemens Stachl",
    "corresponding_authors": "",
    "abstract": "Large language models (LLMs) are transforming research in psychology and the behavioral sciences by enabling advanced text analysis at scale. Their applications range from the analysis of social media posts to infer psychological traits to the automated scoring of open-ended survey responses. However, despite their potential, many behavioral scientists struggle to integrate LLMs into their research because of the complexity of text modeling. In this tutorial, we aim to provide an accessible introduction to LLM-based text analysis, focusing on the Transformer architecture. We guide researchers through the process of preparing text data, using pretrained Transformer models to generate text embeddings, fine-tuning models for specific tasks such as text classification, and applying interpretability methods, such as Shapley additive explanations and local interpretable model-agnostic explanations, to explain model predictions. By making these powerful techniques more approachable, we hope to empower behavioral scientists to leverage LLMs in their research, unlocking new opportunities for analyzing and interpreting textual data.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412625114",
    "type": "article"
  },
  {
    "title": "The DECIDE Framework: Describing Ethical Choices in Digital-Behavioral-Data Explorations",
    "doi": "https://doi.org/10.1177/25152459251361013",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Heather Shaw; Olivia Brown; Joanne Hinds; Sophie J. Nightingale; John N. Towse; David A. Ellis",
    "corresponding_authors": "",
    "abstract": "Behavioral sciences now routinely rely on digital data, supported by digital technologies and platforms. This has resulted in an abundance of new ethical challenges for researchers and ethical-review boards. Several contemporary high-profile cases emphasize that ethical issues often surface after the research is published, once harm has already occurred. Consequently, implementing safeguards in digital-behavioral research is often reactionary and fails to adequately prevent harm. In response, we propose the DECIDE (Describing Ethical Choices in Digital-Behavioural Data Explorations) framework, which encourages ethical reflections and discussions throughout all stages of the research process. The framework presents several questions designed to help researchers view their work from new perspectives and uncover ethical issues they might not have anticipated. We provide several resources to support researchers with their ethical reflections and discussions, including (a) the DECIDE framework spreadsheet, (b) the DECIDE desktop app, (c) information documents, and (d) flowcharts. In this article, we provide suggestions on how to use each resource to encourage proactive discussions of how ethical issues may apply to specific research contexts. By promoting continuous ethical considerations, safeguards can be put in place throughout the research project, even after research commencement. The DECIDE framework shifts ethical reflection away from being reactive toward a more proactive endeavor, reducing the risk of harm and the misuse of digital-behavioral data.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413272007",
    "type": "article"
  },
  {
    "title": "Open Science in the Developing World: A Collection of Practical Guides for Researchers in Developing Countries",
    "doi": "https://doi.org/10.1177/25152459251357565",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Hu Chuan-Peng; Zhiqi Xu; Aleksandra Lazić; P. Bhattacharya; Leonardo Seda; Samiul Hossain; Alma Jeftić; Asil Ali Özdoğru; Olavo B. Amaral; Nadica Miljković; Zlatomira Ilchovska; Ljiljana B. Lazarević; Han‐Wu‐Shuang Bao; Nikita Ghodke; David Moreau; Mahmoud Medhat Elsherif; Chinchu Chithrangathan; Sakshi Ghai; Clarissa F. D. Carneiro; Danka Purić; Yin Wang; Mirela Zaneva; Felipe Vilanova; Iris Žeželj; Obrad Vučkovac; Saida Heshmati; Pooja Kulkarni; Nadia Saraí Corral-Frías; Juan Diego García‐Castro; Shubham Pandey; Jamal Amani Rad; T. M. Rajesh; Bita Vahdani; Saad Al-Majed; A. Ben Haj Amara; Leher Singh; Ali H. Al‐Hoorie; Marcelo C. Batistuzzo; Daniel Fatori; Frankie T. K. Fong; Zahra Khorami; Joseph Uy Almazan; Biljana Gjoneska; Meng Liu; Flávio Azevedo",
    "corresponding_authors": "",
    "abstract": "Over the past decade, the open-science movement has transformed the research landscape, although its impact has largely been confined to developed countries. Recently, researchers from developing countries have called for a redesign of open science to better align with their unique contexts. However, raising awareness alone is insufficient—practical actions are required to drive meaningful and inclusive change. In this work, we analyze the opportunities offered by the open-science movement and explore the macro- and micro-level barriers researchers in developing countries face when engaging with these practices. Drawing on these insights and aiming to inspire researchers in developing regions or other resource-constrained contexts to embrace open-science practices, we offer a four-level guide for gradual engagement: (a) foundation, using open resources to build a solid foundation for rigorous research; (b) growth, adopting low-cost, easily implementable practices; (c) community, contributing to open-science communities through actionable steps; and (d) leadership, taking on leadership roles or forming local communities to foster cultural change. We further discuss potential pitfalls of the current open-science practices and call for readaptation of these practices in developing countries’ settings. We conclude by outlining concrete recommendations for future action.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413273662",
    "type": "article"
  },
  {
    "title": "Three Sensitivity-Analysis Methods to Assess Unmeasured Pretreatment Confounding Bias in Experimental Mediation Analysis",
    "doi": "https://doi.org/10.1177/25152459251355586",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Diana Alvarez-Bartolo; David P. MacKinnon",
    "corresponding_authors": "",
    "abstract": "Statistical-mediation analysis is a widely used method in psychological research that helps understand the intermediate variables, known as mediators ( M ), by which an independent variable ( X ) causes an outcome variable ( Y ). A major contribution to statistical-mediation analysis has been the incorporation of causal methods because it allows a clear definition of the causal direct and mediated effects and the specification of the assumptions to interpret such effects as causal. Modern causal approaches to mediation analysis encourage routinely investigating the extent to which unobserved confounders may explain the observed mediated effects. The recommendation acknowledges that even when X represents random assignment, participants are not usually randomly assigned to levels of M ; hence, unobserved confounders may bias the M to Y relation ( b -path). In this article, we describe unobserved pretreatment confounding of the M to Y relation in experimental mediation studies and three sensitivity-analysis methods to assess unmeasured pretreatment confounding of the M to Y relation: the correlated-residuals method, the left-out-variables-error method, and the phantom-variable method. We report the results of a simulation study that compares the routine application of the three sensitivity-analysis methods. Results generally indicate that larger effect sizes of the population b -path are less susceptible to confounding bias for all sensitivity methods. Thus, an initial approach to investigating confounding bias in experimental mediation studies is to assess the effect size of the path relating M to Y , and more details can be obtained by applying one of the three sensitivity-analysis methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413357924",
    "type": "article"
  },
  {
    "title": "From Experiments to Policy Insights: Generalizing Causal Effects From Study Samples to Target Populations",
    "doi": "https://doi.org/10.1177/25152459251359623",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Wen Wei Loh; Dongning Ren",
    "corresponding_authors": "",
    "abstract": "Psychological science holds substantial promise for informing policy decisions but faces challenges in realizing its potential. One widely recognized challenge is bridging the gap between the nonrepresentative study samples commonly used to evaluate interventions and the broader populations that policymakers aim to serve. To address this challenge, we introduce causal effect generalizability, an approach from causal inference and epidemiology, in the form of an accessible, nontechnical tutorial for psychological and behavioral scientists. We use publicly available data from a real-world psychology intervention study to illustrate why causal effects in a nonrepresentative study sample may systematically differ from those in a broader population. We provide a step-by-step guide with user-friendly R functions, enabling researchers to generalize causal effects from a study sample back to the full target population. This approach allows researchers to assess intervention effects in broader populations, offering valuable insights to guide evidence-based policy development. We hope this nontechnical introductory material will assist scholars in enhancing the policy relevance and real-world impact of psychological science.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413389069",
    "type": "article"
  },
  {
    "title": "On Partial Versus Full Mediation and the Importance of Effect Sizes",
    "doi": "https://doi.org/10.1177/25152459251355585",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Thomas Ledermann; Myriam Rudaz; Matthew S. Fritz",
    "corresponding_authors": "",
    "abstract": "Theoretical models involving one or multiple intervening variables often posit whether a cause influences an outcome both directly and indirectly or only indirectly. In testing mediation, this distinction of partial and full mediation has become a subject of debate because of statistical issues. We extend the critique on this notion and provide insights into what a statistically significant direct effect between a cause and an outcome in a mediation model can mean. We also evaluate different effect size measures for direct and indirect effects and offer practical recommendations for assessing mediation mechanisms, which we illustrate using different examples. The broader relevance of these recommendations beyond mediation analysis is discussed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414041436",
    "type": "article"
  },
  {
    "title": "Bridging Null Hypothesis Testing and Estimation: A Practical Guide to Statistical Conclusion Drawing From Research in Psychology",
    "doi": "https://doi.org/10.1177/25152459251365960",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Henk A. L. Kiers; Jorge N. Tendeiro",
    "corresponding_authors": "",
    "abstract": "A well-known problem of null hypothesis significance testing is that it cannot be used to find support for the null hypothesis. A common solution for this is to replace the exact 0 value by an interval associated with values that are close to 0. This approach is denoted as equivalence testing and is a special case of procedures that test intervals of values against each other. Smiley et al. recently published a unified framework of statistical inference and suggested a straightforward method of testing all sorts of interval-based hypotheses in a unified way. In the present article, we discuss three alternative general approaches, based on Bayesian analysis, that have the advantage that the ensuing probabilities can be interpreted as probabilities of the population parameters rather than probabilities of the data (as is the case with frequentist methods). These methods (in some form) have been previously suggested, but here, we bring them together and show how they can be used for Smiley et al.’s full unified framework of statistical inference, now complementing it with three Bayesian counterparts. In particular, we show how each of the methods works in the analysis of a leading example data set involving a test on proportions. Subsequently, their relative pros and cons are discussed, and it is explained how the methods can be used for many statistical-analysis questions in practice using R and/or JASP. This is illustrated on an empirical data set for comparing means of two groups.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414041440",
    "type": "article"
  },
  {
    "title": "Citing Decisions in Psychology: A Roadblock to Cumulative and Inclusive Science",
    "doi": "https://doi.org/10.1177/25152459251351287",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Katherine M. Lawson; Brett A. Murphy; Jovani Azpeitia; Ella J. Lombard; Terrence Pope",
    "corresponding_authors": "",
    "abstract": "Citations are the main avenue through which scholarly contributions are recognized. However, decisions about what to cite (or not cite) are often made without much systematic thought. Suboptimal citing practices undermine psychological science. Yet psychological science as a field has yet to comprehensively discuss ways to improve authors’ citing decisions. We outline the importance of citing for promoting the cumulativeness of the scientific endeavor, which encompasses promoting diversity, equity, and inclusion in the field. We describe how psychologists make citing decisions and some negative consequences when citation decisions are negligent (or even fraudulent). Moreover, we describe how citations driven by insular professional networks can reinforce historical exclusion and result in reference sections that reflect a failure to meaningfully search and engage with existing literature. Then, we review some potential causes of problematic citing behaviors, which include factors that manifest at the level of the individual, such as a desire to elevate one’s own professional profile, and systemic factors, such as the exponential growth in published literature. Finally, we offer strategies for the field, journals, labs, and individuals to improve citations. In framing our arguments and recommendations, we refer to empirical data collected on citing decisions from editorial-board members ( N = 213) at 23 psychology journals.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414041456",
    "type": "article"
  },
  {
    "title": "A Practical Guide to Specifying Random Effects in Longitudinal Dyadic Multilevel Modeling",
    "doi": "https://doi.org/10.1177/25152459251351286",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Kareena del Rosario; Tessa V. West",
    "corresponding_authors": "",
    "abstract": "Analyzing over-time dyadic data can be challenging, particularly when using multilevel models with complex random-effect structures. In this tutorial, we discuss the best practices of model specification for longitudinal dyadic multilevel modeling, providing a practical guide to specifying (and respecifying) random effects with both theoretical and practical considerations in mind. We begin by defining random effects in the context of repeated-measures dyadic data and address common issues such as nonconvergence. Then, using two models—the dyadic growth-curve and the stability and influence model—we demonstrate how to apply these guidelines in both SAS and R. The dyadic growth-curve model provides a straightforward example, whereas the stability and influence model illustrates common challenges when dealing with complex random-effect structures and convergence issues. In the first exercise, we explain how to customize the variance-covariance matrix for these analyses in SAS. In the second exercise, we adapt these analyses for R and discuss how to implement the sum-and-difference approach for indistinguishable dyads. We conclude with a discussion of alternative models and go over the utility of data simulation during study design, helping readers plan and select the best approach for their research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414173379",
    "type": "article"
  },
  {
    "title": "A Window Into the State of the Science: Current Reporting Practices Related to Generalizability in MRI and Functional-MRI Studies",
    "doi": "https://doi.org/10.1177/25152459251372115",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Arianna M. Gard; Deena Shariq; A. Albrecht; Alaina Lurie; Hyung Cho Kim; Colter Mitchell; Luke W. Hyde",
    "corresponding_authors": "",
    "abstract": "Concerns for the replicability, reliability, and generalizability of MRI and functional MRI (fMRI) research have led to debates over the contributions of sample size, open-science practices, and recruitment methods, particularly in the psychological sciences. Key to understanding the state of a science is an assessment of reporting practices. In this structured review, we evaluated select reporting practices across three domains: (a) demographic (e.g., age), (b) methodological (e.g., inclusion/exclusion criteria), and (c) open science and generalizability (e.g., preregistration, target-population identification). Included were 919 published MRI and fMRI studies from 2019 in nine top-ranked journals. Reporting across domains was infrequent; participant racial-ethnic identity (14.8%), reasons for missing imaging data (31.2%), and identification of a target population (19.4%) were particularly low. Reporting likelihood varied by study characteristics (e.g., journal) and was correlated across domains. Finally, study sample size but not reporting frequency was positively associated with 2-year citation counts. Results call for recentering transparency in reporting practices in MRI and fMRI studies, with direct implications for study generalizability.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414191849",
    "type": "article"
  },
  {
    "title": "Identifying Careless Survey Respondents Through Machine Learning Using Responses to a Gibberish Scale",
    "doi": "https://doi.org/10.1177/25152459251378420",
    "publication_date": "2025-10-01",
    "publication_year": 2025,
    "authors": "Leah Bloy; Yehezkel S. Resheff; Avraham N. Kluger; Nechumi Malovicki-Yaffe",
    "corresponding_authors": "",
    "abstract": "Invalid responses pose a significant risk of distorting survey data, compromising statistical inferences, and introducing errors in conclusions drawn from surveys. Given the pivotal role of surveys in research, development, and decision-making, it is imperative to identify careless survey respondents. The existing literature on this subject comprises two primary categories of approaches: methods that rely on survey items and methods involving post hoc analyses. The latter, which does not demand preemptive preparation, predominantly incorporates statistical techniques or metadata analysis aimed at identifying distinct response patterns that are associated with careless responses. However, several inherent limitations limit the precise identification of careless respondents. One notable challenge is the lack of consensus concerning the thresholds to use for the various measures. Furthermore, each method is designed to detect a specific response pattern associated with carelessness, leading to conflicting outcomes. In this article, we seek to assess the efficacy of the existing methods using a novel survey methodology encompassing responses to both meaningful and meaningless gibberish scales in which the latter compels respondents to answer without considering item content. Using this approach, we propose the application of machine learning to identify careless survey respondents. Our findings underscore the efficacy of a methodology using supervised machine learning combined with unique gibberish data as a potent method for the identification of careless respondents, aligning with and outperforming other approaches in terms of effectiveness and versatility.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415005314",
    "type": "article"
  },
  {
    "title": "Enriching Meta-Analytic Models of Summary Data: A Thought Experiment and Case Study",
    "doi": "https://doi.org/10.1177/2515245919884304",
    "publication_date": "2019-11-26",
    "publication_year": 2019,
    "authors": "Blakeley B. McShane; Ulf Böckenholt",
    "corresponding_authors": "Blakeley B. McShane",
    "abstract": "Meta-analysis typically involves the analysis of summary data (e.g., means, standard deviations, and sample sizes) from a set of studies via a statistical model that is a special case of a hierarchical (or multilevel) model. Unfortunately, the common summary-data approach to meta-analysis used in psychological research is often employed in settings where the complexity of the data warrants alternative approaches. In this article, we propose a thought experiment that can lead meta-analysts to move away from the common summary-data approach to meta-analysis and toward richer and more appropriate summary-data approaches when the complexity of the data warrants it. Specifically, we propose that it can be extremely fruitful for meta-analysts to act as if they possess the individual-level data from the studies and consider what model specifications they might fit even when they possess only summary data. This thought experiment is justified because (a) the analysis of the individual-level data from the studies via a hierarchical model is considered the “gold standard” for meta-analysis and (b) for a wide variety of cases common in meta-analysis, the summary-data and individual-level-data approaches are, by a principle known as statistical sufficiency, equivalent when the underlying models are appropriately specified. We illustrate the value of our thought experiment via a case study that evolves across five parts that cover a wide variety of data settings common in meta-analysis.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2991459896",
    "type": "article"
  },
  {
    "title": "Why Bayesian “Evidence for <i>H</i><sub>1</sub>” in One Condition and Bayesian “Evidence for <i>H</i><sub>0</sub>” in Another Condition Does Not Mean Good-Enough Bayesian Evidence for a Difference Between the Conditions",
    "doi": "https://doi.org/10.1177/2515245920913019",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Bence Pálfi; Zoltán Dienes",
    "corresponding_authors": "",
    "abstract": "Psychologists are often interested in whether an independent variable has a different effect in condition A than in condition B. To test such a question, one needs to directly compare the effect of that variable in the two conditions (i.e., test the interaction). Yet many researchers tend to stop when they find a significant test in one condition and a nonsignificant test in the other condition, deeming this as sufficient evidence for a difference between the two conditions. In this Tutorial, we aim to raise awareness of this inferential mistake when Bayes factors are used with conventional cutoffs to draw conclusions. For instance, some researchers might falsely conclude that there must be good-enough evidence for the interaction if they find good-enough Bayesian evidence for the alternative hypothesis, H 1 , in condition A and good-enough Bayesian evidence for the null hypothesis, H 0 , in condition B. The case study we introduce highlights that ignoring the test of the interaction can lead to unjustified conclusions and demonstrates that the principle that any assertion about the existence of an interaction necessitates the direct comparison of the conditions is as true for Bayesian as it is for frequentist statistics. We provide an R script of the analyses of the case study and a Shiny app that can be used with a 2 × 2 design to develop intuitions on this issue, and we introduce a rule of thumb with which one can estimate the sample size one might need to have a well-powered design.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3041157995",
    "type": "article"
  },
  {
    "title": "Australian and Italian Psychologists’ View of Replication",
    "doi": "https://doi.org/10.1177/25152459211039218",
    "publication_date": "2021-07-01",
    "publication_year": 2021,
    "authors": "Franca Agnoli; Hannah Fraser; Felix Singleton Thorn; Fiona Fidler",
    "corresponding_authors": "Franca Agnoli",
    "abstract": "Solutions to the crisis in confidence in the psychological literature have been proposed in many recent articles, including increased publication of replication studies, a solution that requires engagement by the psychology research community. We surveyed Australian and Italian academic research psychologists about the meaning and role of replication in psychology. When asked what they consider to be a replication study, nearly all participants (98% of Australians and 96% of Italians) selected options that correspond to a direct replication. Only 14% of Australians and 8% of Italians selected any options that included changing the experimental method. Majorities of psychologists from both countries agreed that replications are very important, that more replications should be done, that more resources should be allocated to them, and that they should be published more often. Majorities of psychologists from both countries reported that they or their students sometimes or often replicate studies, yet they also reported having no replication studies published in the prior 5 years. When asked to estimate the percentage of published studies in psychology that are replications, both Australians (with a median estimate of 13%) and Italians (with a median estimate of 20%) substantially overestimated the actual rate. When asked what constitute the main obstacles to replications, difficulty publishing replications was the most frequently cited obstacle, coupled with the high value given to innovative or novel research and the low value given to replication studies.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3162424082",
    "type": "article"
  },
  {
    "title": "Understanding Ecological-Momentary-Assessment Data: A Tutorial on Exploring Item Performance in Ecological-Momentary-Assessment Data",
    "doi": "https://doi.org/10.1177/25152459241286877",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Björn S. Siepe; Carlotta Rieble; Rayyan Tutunji; Aljoscha Rimpler; Julius März; Ricarda K. K. Proppert; Eiko I. Fried",
    "corresponding_authors": "",
    "abstract": "The use of ecological-momentary-assessment (EMA) data to study individuals in their everyday lives is popular in many areas of social and life sciences. At the same time, EMA data sets are complex, the psychometric properties of EMA items are often not investigated systematically, and scales are often neither standardized nor validated beyond their face validity. Here, we present different descriptive statistics and data-visualization techniques to increase the understanding of the performance of EMA items. We apply these techniques to a wide range of items used in a large EMA data set (599 participants, 360 time points) collected in the WARN-D study to investigate their distributions, contextual influences, change over time, sources of variability, and relationship with classical static measures of psychopathology. We discuss the theoretical and substantive implications of our findings and provide researchers with R code that they can adapt to their own EMA data, as well as literature recommendations for each topic. We hope to inspire more researchers to share in-depth descriptive summaries of their experience-sampling data such that the field can move forward in understanding the performance of EMA measures across contexts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407307340",
    "type": "article"
  },
  {
    "title": "Measuring Variation in Gaze Following Across Communities, Ages, and Individuals: A Showcase of TANGO-CC (Task for Assessing iNdividual differences in Gaze understanding-Open-Cross-Cultural)",
    "doi": "https://doi.org/10.1177/25152459241308170",
    "publication_date": "2025-01-01",
    "publication_year": 2025,
    "authors": "Julia Christin Prein; Florian Markus Bednarski; Ardain Dzabatou; Michael C. Frank; Annette M. E. Henderson; Josefine Kalbitz; Patricia Kanngießer; Dilara Keşşafoğlu; Bahar Köymen; Maira Manrique-Hernandez; Shirley Magazi; Lizbeth Mújica-Manrique; Julia Ohlendorf; Damilola Olaoba; Wesley R. Pieters; Sarah M. Pope; Umay Sen; Katie E. Slocombe; Robert Zane Sparks; Roman Stengelin; Jahnavi Sunderarajan; Kirsten Sutherland; Florence Tusiime; Wilson Filipe da Silva Vieira; Zhen Zhang; Yufei Zong; Daniel B. M. Haun; Manuel Bohn",
    "corresponding_authors": "",
    "abstract": "Cross-cultural studies are crucial for investigating the cultural variability and universality of cognitive developmental processes. However, cross-cultural assessment tools in cognition across languages and communities are limited. In this article, we describe a gaze-following task designed to measure basic social cognition across individuals, ages, and communities (the Task for Assessing iNdividual differences in Gaze understanding-Open-Cross-Cultural; TANGO-CC). The task was developed and psychometrically assessed in one cultural setting and, with input of local collaborators, adapted for cross-cultural data collection. Minimal language demands and the web-app implementation allow fast and easy contextual adaptations to each community. TANGO-CC captures individual- and community-level variation and shows good internal consistency in a data set of 2.5- to 11-year-old children from 17 diverse communities. Within-communities variation outweighed between-communities variation. We provide an open-source website for researchers to customize and use the task ( https://ccp-odc.eva.mpg.de/tango-cc ). TANGO-CC can be used to assess basic social cognition in diverse communities and provides a roadmap for researching community-level and individual-level differences across cultures.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407314402",
    "type": "article"
  },
  {
    "title": "One Decade Into the Replication Crisis, How Have Psychological Results Changed?",
    "doi": "https://doi.org/10.1177/25152459251323480",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Paul C. Bogdan",
    "corresponding_authors": "Paul C. Bogdan",
    "abstract": "A psychology article’s p values say a lot about how its studies were conducted and whether its results are likely to replicate. Examining p values across the entire literature can, in turn, shed light on the state of psychology overall and how it has changed since the start of the replication crisis. In the present research, I investigate strong ( p &lt; .01) and weak (.01 ≤ p &lt; .05) p values reported across 240,355 empirical psychology articles from 2004 to 2024. Over this period and across every subdiscipline, the typical study has begun reporting markedly stronger p values. Nowadays, articles reporting strong p values are also more often published in top journals and receive more citations. Yet it also appears that robust research is still not correspondingly linked to career success given that researchers at the highest ranked universities tend to publish articles with the weakest p values. Investigating language usage suggests that two-thirds of this association can be explained by highly ranked universities preferring laborious, expensive, and subtle research topics even though these generally produce weaker results. Altogether, these findings point to the strength of most contemporary psychological research and suggest academic incentives have begun to promote such research. However, there remain key questions about the extent to which robustness is truly valued compared with other research aspects.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409250869",
    "type": "article"
  },
  {
    "title": "The Benefits of Reporting Critical-Effect-Size Values",
    "doi": "https://doi.org/10.1177/25152459251335298",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Ambra Perugini; Filippo Gambarota; Enrico Toffalini; Daniël Lakens; Massimiliano Pastore; Livio Finos; Gianmarco Altoè",
    "corresponding_authors": "",
    "abstract": "Critical-effect-size values represent the smallest detectable effect that can reach statistical significance given a specific sample size, alpha level, and test statistic. It can be useful to calculate the critical effect size when designing a study and evaluate whether such effects are plausible. Reporting critical-effect-size values may be useful when the sample size has not been planned a priori, there is uncertainty about the expected sample size that can be collected, or researchers plan to analyze the data with a statistical hypothesis test. To assist researchers in calculating critical-effect-size values, we developed an R package that allows researchers to report critical-effect-size values for group comparisons, correlations, linear regressions, and meta-analyses. Reflecting on critical-effect-size values could benefit researchers during the planning phase of the study by helping them to understand the limitations of their research design. Critical-effect-size values are also useful when evaluating studies performed by other researchers when a priori power analyses were not performed, especially when nonsignificant results are observed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410768167",
    "type": "article"
  },
  {
    "title": "On the Statistical Analysis of Studies With Attention Checks",
    "doi": "https://doi.org/10.1177/25152459251338041",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Maya B. Mathur",
    "corresponding_authors": "Maya B. Mathur",
    "abstract": "Attention checks are often used to identify and exclude participants who may be responding carelessly. There has been little statistical guidance on the analysis of such studies and on when it is indeed valid to simply exclude inattentive participants. To address this, I first formalize attention checks as measures intended to identify participants whose responses are free of measurement error. Measurement error could arise not only because of careless responding but also if some participants fail to receive the experimental manipulation in its intended form because they did not attend to its contents. I discuss the statistical assumptions under which it is valid to simply exclude inattentive participants. In randomized experiments, this standard analysis may lead to bias if (a) the dependent variable affects attentiveness or (b) there are variables that affect both attentiveness and the dependent variable. The latter assumption is stringent and is likely to be violated in many studies. I suggest a straightforward modification to the standard approach, that is, controlling for variables that affect both attentiveness and the dependent variable. This covariate-adjusted approach requires considerably less stringent assumptions. In two worked examples, I reanalyze previously published experiments on (a) a documentary intended to reduce consumption of meat and animal products and (b) flag-priming effects on political conservatism.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410879373",
    "type": "article"
  },
  {
    "title": "Mixed-Methods Research in Psychology: Rationales and Research Designs",
    "doi": "https://doi.org/10.1177/25152459251343919",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Moin Syed; Dulce Wilkinson Westberg",
    "corresponding_authors": "",
    "abstract": "Psychological science has long maintained a preference for quantitative methods over qualitative methods. The allegiance to one methodological family and the rejection of another means that at least, in part, the field’s methods are constraining the universe of research questions it is willing to ask. In this article, we provide an overview of mixed-methods research, which involves the use and integration of both qualitative and quantitative methods, and why psychology should do more of it. The focal audience is quantitatively oriented researchers who are interested in—and perhaps even skeptical of—the role of qualitative methods for their work. The article consists of three general sections: (a) a brief discussion of philosophical issues underlying the application of mixed-methods research in psychology, (b) a deeper examination of what constitutes “quantitative” and “qualitative” research, and (c) a description of four major mixed-methods-research designs that hold promise for psychology research. We provide researchers with broad conceptual foundations and concrete tools for how research questions in psychology can be mapped to different mixed-methods designs, helping correct for researchers’ lack of exposure and/or negative preconceptions that have inhibited uptake in the field.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411195402",
    "type": "article"
  },
  {
    "title": "Side Effects of Experience-Sampling Protocols: A Systematic Analysis of How They Affect Data Quality, Data Quantity, and Bias in Study Results",
    "doi": "https://doi.org/10.1177/25152459251347274",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Thomas Reiter; Sophia Sakel; Julian Scharbert; Julian ter Horst; Maarten van Zalk; Mitja D. Back; Markus Bühner; Ramona Schoedel",
    "corresponding_authors": "",
    "abstract": "In studies using the increasingly popular experience-sampling method (ESM), design decisions are often guided by theoretical or practical considerations. Yet limited empirical evidence exists on how these choices affect data quantity (e.g., response probabilities), data quality (e.g., response latency), and potential biases in study outcomes (e.g., characteristics of study variables). In a preregistered, 4-week study ( N = 395), we experimentally manipulated two key ESM protocol characteristics for sending ESM surveys: timing (fixed vs. varying times) and contingency (directly vs. indirectly after unlocking the smartphone). We evaluated the ESM protocols resulting from the combination of these two characteristics regarding different criteria: As hypothesized for contingency, indirect protocols resulted in higher response probabilities (increased data quantity). But they also led to higher response latencies (reduced data quality). Contrary to our expectations, the combined effect of contingency and timing did not significantly influence response probability. We also did not observe other effects of timing or contingency on data quality. In exploratory follow-up analyses, we discovered that timing significantly affected response probability and smartphone-usage behaviors, as measured by screen logs; however, these effects were likely attributable to time-of-day effects. Self-reported states showed no differences based on the chosen ESM protocol, and similar trends were found when correlating primary outcomes with external criteria, such as trait affect and well-being. Based on the study’s findings, we discuss the trade-offs that researchers should consider when choosing their ESM protocols to optimize data quantity, data quality, and biases in study outcomes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412723231",
    "type": "article"
  },
  {
    "title": "The Response-Process-Evaluation Method: A New Approach to Survey-Item Validation",
    "doi": "https://doi.org/10.1177/25152459251353135",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Melissa Gordon Wolf; Elliott Ihm; Andrew Maul; Ann Taves",
    "corresponding_authors": "",
    "abstract": "Pretesting survey items for interpretability and relevance is a commonly recommended practice in the social sciences. The goal is to construct items that are understood as intended by the population of interest and test if participants use the expected cognitive processes when responding to a survey item. Such evidence forms the basis for a critical source of validity evidence known as the “response process,” which is often neglected in favor of quantitative methods. This may be because existing methods of investigating item comprehension, such as cognitive interviewing and web probing, lack clear guidelines for retesting revised items and documenting improvements and can be difficult to implement in large samples. To remedy this, we introduce the response-process-evaluation (RPE) method, a standardized framework for pretesting multiple versions of a survey. This iterative, evidence-based approach to item development relies on feedback from the population of interest to quantify and qualify improvements in item interpretability across a large sample. The result is a set of item-validation reports that detail the intended interpretation and use of each item, the population it was validated on, the percentage of participants that interpreted the item as intended, examples of participant interpretations, and any common misinterpretations to be cautious of. We also include an empirical study that compares the RPE method with cognitive interviewing in terms of the quality of data gathered and the resources expended. Researchers may find that they have more confidence in the inferences drawn from survey data after engaging in rigorous item pretesting.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413143001",
    "type": "article"
  },
  {
    "title": "A Cross-Sectional Study of the Completeness of Preregistrations by Psychological Authors From German-Speaking Institutions",
    "doi": "https://doi.org/10.1177/25152459251357568",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "L. Albert Hahn; Andreas Glöckner; Mario Gollwitzer; Jens H. Hellmann; Jens Lange; Simon Schindler; Kai Sassenberg",
    "corresponding_authors": "",
    "abstract": "Preregistering confirmatory research aims at reducing researchers’ degrees of freedom and increasing transparency to ultimately increase replicability. Yet the extent to which preregistrations actually achieve these goals depends on the completeness of a preregistration. To scrutinize the completeness of current preregistrations, we coded all preregistrations mentioned in journal articles published by psychologists from institutions in German-speaking countries in 2020 as to whether they contain six procedural specifications: (a) the hypothesized pattern of results, (b) the measures, (c) planned sample size, (d) exclusion criteria, (e) planned analyses to test the hypotheses, and (f) a time stamp. In addition, we consider transparency-related elements. Our results show that the completeness of preregistration was associated with neither the journal’s impact factor nor its transparency and openness promotion factor. Approximately half of the preregistrations contained all six procedural specifications. Hence, in line with previous research, our findings indicate that when considering publications from diverse subdisciplines of psychology, there was room for improvement regarding the completeness of preregistrations in psychology. We discuss steps to improve preregistration completeness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413456909",
    "type": "article"
  },
  {
    "title": "Six Fallacies in Substituting Large Language Models for Human Participants",
    "doi": "https://doi.org/10.1177/25152459251357566",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Zhicheng Lin",
    "corresponding_authors": "Zhicheng Lin",
    "abstract": "Can artificial-intelligence (AI) systems, such as large language models (LLMs), replace human participants in behavioral and psychological research? Here, I critically evaluate the replacement perspective and identify six interpretive fallacies that undermine its validity. These fallacies are (a) equating token prediction with human intelligence, (b) treating LLMs as the average human, (c) interpreting alignment as explanation, (d) anthropomorphizing AI systems, (e) essentializing identities, and (f) substituting model data for human evidence. Each fallacy represents a potential misunderstanding about what LLMs are and what they can tell researchers about human cognition. In the analysis, I distinguish levels of similarity between LLMs and humans, particularly functional equivalence (outputs) versus mechanistic equivalence (processes), while highlighting both technical limitations (addressable through engineering) and conceptual limitations (arising from fundamental differences between statistical and biological intelligence). For each fallacy, specific safeguards are provided to guide responsible research practices. Ultimately, the analysis supports conceptualizing LLMs as pragmatic simulation tools—useful for role-play, rapid hypothesis testing, and computational modeling (provided their outputs are validated against human data)—rather than as replacements for human participants. This framework enables researchers to leverage language models productively while respecting the fundamental differences between machine intelligence and human thought.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413459188",
    "type": "article"
  },
  {
    "title": "Realistic Expectations for Replications: Expecting Too Little Is Just as Bad as Expecting Too Much",
    "doi": "https://doi.org/10.1177/25152459251369846",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Frieder Göppert; Kriti Bhatia; Sascha Meyen; Volker H. Franz",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414310080",
    "type": "article"
  },
  {
    "title": "Enabling Confirmatory Secondary Data Analysis by Logging Data Checkout",
    "doi": "https://doi.org/10.1177/2515245918815849",
    "publication_date": "2019-01-07",
    "publication_year": 2019,
    "authors": "Kimberly Scott; Melissa Kline",
    "corresponding_authors": "Kimberly Scott",
    "abstract": "As more researchers make their data sets openly available, the potential of secondary data analysis to address new questions increases. However, the distinction between primary and secondary data analysis is unnecessarily confounded with the distinction between confirmatory and exploratory research. We propose a framework, akin to library-book checkout records, for logging access to data sets in order to support confirmatory analysis when appropriate. This system would support a standard form of preregistration for secondary data analysis, allowing authors to demonstrate that their plans were registered prior to data access. We discuss the critical elements of such a system, its strengths and limitations, and potential extensions.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2909888300",
    "type": "article"
  },
  {
    "title": "Corrigendum: Two Lines: A Valid Alternative to the Invalid Testing of U-Shaped Relationships With Quadratic Regressions",
    "doi": "https://doi.org/10.1177/2515245919894972",
    "publication_date": "2019-12-01",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4255189630",
    "type": "erratum"
  },
  {
    "title": "A Practical Guide to Variable Selection in Structural Equation Modeling by Using Regularized Multiple-Indicators, Multiple-Causes Models:",
    "doi": "https://doi.org/10.17863/cam.39567",
    "publication_date": "2019-03-25",
    "publication_year": 2019,
    "authors": "Ross Jacobucci; Andreas M. Brandmaier; Rogier Kievit",
    "corresponding_authors": "",
    "abstract": "R. A. Kievit is supported by the Sir Henry Wellcome Trust (Grant 107392/Z/15/Z) and by an MRC Programme Grant (SUAG/014/RG91365). This project has also received funding from the European Union’s Horizon 2020 Research and Innovation program (Grant 732592).",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2929731722",
    "type": "article"
  },
  {
    "title": "Experiment-Wise Type I Error Control: A Focus on 2 × 2 Designs",
    "doi": "https://doi.org/10.1177/2515245920985137",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Andrew V. Frane",
    "corresponding_authors": "Andrew V. Frane",
    "abstract": "Factorial designs are common in psychology research. But they are nearly always used without control of the experiment-wise Type I error rate (EWER), perhaps because of a lack of awareness about viable procedures for that purpose and perhaps also because of a lack of appreciation for the problem of Type I error inflation. In this article, key concepts relating to Type I error inflation are discussed, with emphasis on the 2 × 2 factorial design. Simulations are used to evaluate various approaches in that context. I show that conventional approaches often do not control the EWER. Alternative approaches are recommended that reliably control the EWER and are simple to implement.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3139492395",
    "type": "article"
  },
  {
    "title": "Evaluating Response Shift in Statistical Mediation Analysis",
    "doi": "https://doi.org/10.1177/25152459211012271",
    "publication_date": "2021-04-01",
    "publication_year": 2021,
    "authors": "A. R. Georgeson; Matthew J. Valente; Oscar González",
    "corresponding_authors": "A. R. Georgeson",
    "abstract": "Researchers and prevention scientists often develop interventions to target intermediate variables (known as mediators) that are thought to be related to an outcome. When researchers target a mediating construct measured by self-report, the meaning of self-report measure could change from pretest to posttest for the individuals who received the intervention - which is a phenomenon referred to as response shift. As a result, any observed changes on the mediator measure across groups or across time might reflect a combination of true change on the construct and response shift. Although previous studies have focused on identifying the source and type of response shift in measures after an intervention, there has been limited research on how using sum scores in the presence of response shift affects the estimation of mediated effects via statistical mediation analysis, which is critical for explaining how the intervention worked. In this paper, we focus on recalibration response shift, which is a change in internal standards of measurement, which affects how respondents interpret the response scale. We provide background on the theory of response shift and the methodology used to detect response shift (i.e., tests of measurement invariance). Additionally, we use simulated datasets to provide an illustration of how recalibration in the mediator can bias estimates of the mediated effect and also impact type I error and power.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3160289996",
    "type": "article"
  },
  {
    "title": "Advancing Group-Based Disparities Research and Beyond: A Cautionary Note on Selection Bias",
    "doi": "https://doi.org/10.1177/25152459241260256",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Dongning Ren; Wen Wei Loh",
    "corresponding_authors": "",
    "abstract": "Obtaining an accurate understanding of group-based disparities is an important pursuit. However, unsound study designs can lead to erroneous conclusions that impede this crucial work. In this article, we highlight a critical methodological challenge to drawing valid causal inferences in disparities research: selection bias. We describe two commonly adopted study designs in the literature on group-based disparities. The first is outcome-dependent selection, when the outcome determines whether an observation is selected. The second is outcome-associated selection, when the outcome is associated with whether an observation is selected. We explain the methodological challenge each study design presents and why it can lead to selection biases when evaluating the actual disparity of interest. We urge researchers to recognize the complications that beset these study designs and to avoid the insidious impact of inappropriate selection. We offer practical suggestions on how researchers can improve the rigor and demonstrate the defensibility of their conclusions when investigating group-based disparities. Finally, we highlight the broad implications of selection mechanisms for psychological science.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402973496",
    "type": "article"
  },
  {
    "title": "From the Illusion of Choice to Actual Control: Reconsidering the Induced-Compliance Paradigm of Cognitive Dissonance",
    "doi": "https://doi.org/10.1177/25152459241265002",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Shiva Pauer; Roman Linne; Hans‐Peter Erb",
    "corresponding_authors": "",
    "abstract": "The induced-compliance paradigm is a fundamental pillar in the literature on cognitive dissonance. A recent failed replication by Vaidis et al. casts doubt on the widely used experimental method, thereby challenging the literature and prevailing theorizing about the role of perceived choice in cognitive dissonance. However, the nonreplication of the experimental effects could be attributable to methodological factors, such as laboratory settings and cross-temporal dynamics. We therefore reanalyzed the replication data to further explore the relationship between dissonant-attitude change and choice perceptions, employing self-report items instead of the traditional experimental manipulation of choice. Our analysis revealed a significant interaction effect between perceived choice and dissonant behavior (writing a counterattitudinal essay vs. a self-chosen essay) on attitude change: Participants who wrote a counterattitudinal essay aligned their attitudes only if they reported high (vs. low) freedom of choice. These findings suggest a crucial role of choice perceptions in dissonance reduction, consistent with the original theorizing. Future research can employ various methods and draw from adjacent fields, especially from the literature on control perceptions, to reconsider the induced-compliance paradigm and advance research on cognitive dissonance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404313280",
    "type": "article"
  },
  {
    "title": "But Did They Really Perceive No (Low) Choice? Comment on Vaidis et al. (2024)",
    "doi": "https://doi.org/10.1177/25152459241267915",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "David A. Lishner",
    "corresponding_authors": "David A. Lishner",
    "abstract": "",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404313340",
    "type": "article"
  },
  {
    "title": "Dissonance in the Induced-Compliance Paradigm: A Commentary on Vaidis et al. (2024)",
    "doi": "https://doi.org/10.1177/25152459241268308",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Eddie Harmon‐Jones; Cindy Harmon‐Jones",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404313409",
    "type": "article"
  },
  {
    "title": "Replicating the Effect of the Accessibility of Moral Standards on Dishonesty: Authors’ Response to the Replication Attempt",
    "doi": "https://doi.org/10.1177/2515245918769062",
    "publication_date": "2018-09-01",
    "publication_year": 2018,
    "authors": "On Amir; Nina Mažar; Dan Ariely",
    "corresponding_authors": "On Amir",
    "abstract": "",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2890434600",
    "type": "article"
  },
  {
    "title": "The Role of Human Fallibility in Psychological Research: A Survey of Mistakes in Data Management",
    "doi": "https://doi.org/10.1177/25152459211045930",
    "publication_date": "2021-10-01",
    "publication_year": 2021,
    "authors": "Márton Kovács; Rink Hoekstra; Balázs Aczél",
    "corresponding_authors": "Balázs Aczél",
    "abstract": "Errors are an inevitable consequence of human fallibility, and researchers are no exception. Most researchers can recall major frustrations or serious time delays due to human errors while collecting, analyzing, or reporting data. The present study is an exploration of mistakes made during the data-management process in psychological research. We surveyed 488 researchers regarding the type, frequency, seriousness, and outcome of mistakes that have occurred in their research team during the last 5 years. The majority of respondents suggested that mistakes occurred with very low or low frequency. Most respondents reported that the most frequent mistakes led to insignificant or minor consequences, such as time loss or frustration. The most serious mistakes caused insignificant or minor consequences for about a third of respondents, moderate consequences for almost half of respondents, and major or extreme consequences for about one fifth of respondents. The most frequently reported types of mistakes were ambiguous naming/defining of data, version control error, and wrong data processing/analysis. Most mistakes were reportedly due to poor project preparation or management and/or personal difficulties (physical or cognitive constraints). With these initial exploratory findings, we do not aim to provide a description representative for psychological scientists but, rather, to lay the groundwork for a systematic investigation of human fallibility in research data management and the development of solutions to reduce errors and mitigate their impact.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3165110265",
    "type": "article"
  },
  {
    "title": "Effective Maps, Easily Done: Visualizing Geo-Psychological Differences Using Distance Weights",
    "doi": "https://doi.org/10.1177/25152459221101816",
    "publication_date": "2022-07-01",
    "publication_year": 2022,
    "authors": "Tobias Ebert; Lars Mewes; Friedrich M. Götz; Thomas Brenner",
    "corresponding_authors": "Tobias Ebert",
    "abstract": "Psychologists of many subfields are becoming increasingly interested in the geographical distribution of psychological phenomena. An integral part of this new stream of geo-psychological studies is to visualize spatial distributions of psychological phenomena in maps. However, most psychologists are not trained in visualizing spatial data. As a result, almost all existing geo-psychological studies rely on the most basic mapping technique: color-coding disaggregated data (i.e., grouping individuals into predefined spatial units and then mapping out average scores across these spatial units). Although this basic mapping technique is not wrong, it often leaves unleveraged potential to effectively visualize spatial patterns. The aim of this tutorial is to introduce psychologists to an alternative, easy-to-use mapping technique: distance-based weighting (i.e., calculating area estimates that represent distance-weighted averages of all measurement locations). We outline the basic idea of distance-based weighting and explain how to implement this technique so that it is effective for geo-psychological research. Using large-scale mental-health data from the United States ( N = 2,058,249), we empirically demonstrate how distance-based weighting may complement the commonly used basic mapping technique. We provide fully annotated R code and open access to all data used in our analyses.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4292384610",
    "type": "article"
  },
  {
    "title": "Low Research-Data Availability in Educational-Psychology Journals: No Indication of Effective Research-Data Policies",
    "doi": "https://doi.org/10.1177/25152459231156419",
    "publication_date": "2023-01-01",
    "publication_year": 2023,
    "authors": "Markus Huff; Elke C. Bongartz",
    "corresponding_authors": "",
    "abstract": "Research-data availability contributes to the transparency of the research process and the credibility of educational-psychology research and science in general. Recently, there have been many initiatives to increase the availability and quality of research data. Many research institutions have adopted research-data policies. This increased awareness might have raised the sharing of research data in empirical articles. To test this idea, we coded 1,242 publications from six educational-psychology journals and the psychological journal Cognition (as a baseline) published in 2018 and 2020. Research-data availability was low (3.85% compared with 62.74% in Cognition) but has increased from 0.32% (2018) to 7.16% (2020). However, neither the data-transparency level of the journal nor the existence of an official research-data policy on the level of the corresponding author’s institution was related to research-data availability. We discuss the consequences of these findings for institutional research-data-management processes.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4361761075",
    "type": "article"
  },
  {
    "title": "Beyond the Mean: Can We Improve the Predictive Power of Psychometric Scales?",
    "doi": "https://doi.org/10.1177/25152459231177713",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Yngwie Asbjørn Nielsen; Isabel Thielmann; Stefan Pfattheicher",
    "corresponding_authors": "Yngwie Asbjørn Nielsen",
    "abstract": "Two participants completing a psychometric scale may leave wildly different responses yet attain the same mean score. Moreover, the mean score often does not represent the bulk of participants’ responses, which may be skewed, kurtotic, or bimodal. Even so, researchers in psychological science often aggregate item scores using an unweighted mean or a sum score, thereby neglecting a substantial amount of information. In the present contribution, we explore whether other summary statistics of a scale (e.g., the standard deviation, the median, or the kurtosis) can capture and leverage some of this neglected information to improve prediction of a broad range of outcome measures: life satisfaction, mental health, self-esteem, counterproductive work behavior, and social value orientation. Overall, across 32 psychometric scales and three data sets (total N = 8,376), we show that the mean is the strongest predictor of all five outcomes considered, with little to no additional variance explained by other summary statistics. These results provide justification for the current practice of relying on the mean score but hopefully inspire future research to explore the predictive power of other summary statistics for relevant outcomes. For this purpose, we provide a tutorial and example code for R.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4380361400",
    "type": "article"
  },
  {
    "title": "Keeping Meta-Analyses Alive and Well: A Tutorial on Implementing and Using Community-Augmented Meta-Analyses in PsychOpen CAMA",
    "doi": "https://doi.org/10.1177/25152459231197611",
    "publication_date": "2023-10-01",
    "publication_year": 2023,
    "authors": "Lisa Bucher; Tanja Burgard; Ulrich S. Tran; Gerhard M. Prinz; Michael Bošnjak; Martin Voracek",
    "corresponding_authors": "Lisa Bucher",
    "abstract": "Newly developed, web-based, open-repository concepts, such as community-augmented meta-analysis (CAMA), provide open access to fulfill the needs for transparency and timeliness of synthesized evidence. The main idea of CAMA is to keep meta-analyses up-to-date by allowing the research community to include new evidence continuously. In 2021, the Leibniz Institute for Psychology released a platform, PsychOpen CAMA, which serves as a publication format for CAMAs in all fields of psychology. The present work serves as a tutorial on implementing and using a CAMA in PsychOpen CAMA from a data-provider perspective, using six large-scale meta-analytic data sets on the dark triad of personality as a working example. First, the processes of data contribution and implementation of either new or updated existing data sets are summarized. Furthermore, a step-by-step tutorial on using and interpreting CAMAs guides the reader through the web application. Finally, the tutorial outlines the major benefits and the remaining challenges of CAMAs in PsychOpen CAMA.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4388089998",
    "type": "article"
  },
  {
    "title": "The Workings of Choosing and Rejecting: Commentary on Many Labs 2",
    "doi": "https://doi.org/10.1177/2515245918814812",
    "publication_date": "2018-12-01",
    "publication_year": 2018,
    "authors": "Eldar Shafir",
    "corresponding_authors": "Eldar Shafir",
    "abstract": "",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2906464580",
    "type": "article"
  },
  {
    "title": "Degrees of Freedom at the Start of the Second 100 Years: A Pedagogical Treatise",
    "doi": "https://doi.org/10.1177/2515245919882050",
    "publication_date": "2019-12-01",
    "publication_year": 2019,
    "authors": "Joseph Lee Rodgers",
    "corresponding_authors": "Joseph Lee Rodgers",
    "abstract": "Degrees of freedom is a critical core concept within the field of statistics. Virtually every introductory statistics class treats the topic, though textbooks and the statistical literature show mostly superficial treatment, weak pedagogy, and substantial confusion. Fisher first defined degrees of freedom in 1915, and Walker provided technical treatment of the concept in 1940. In this article, the history of degrees of freedom is reviewed, and the pedagogical challenges are discussed. The core of the article is a simple reconceptualization of the degrees-of-freedom concept that is easier to teach and to learn than the traditional treatment. This reconceptualization defines a statistical bank, into which are deposited data points. These data points are used to estimate statistical models; some data are used up in estimating a model, and some data remain in the bank. The several types of degrees of freedom define an accounting process that simply counts the flow of data from the statistical bank into the model. The overall reconceptualization is based on basic economic principles, including treating data as statistical capital and data exchangeability (fungibility). The goal is to stimulate discussion of degrees of freedom that will improve its use and understanding in pedagogical and applied settings.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2994154179",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Registered Replication of LoBue and DeLoache (2008)",
    "doi": "https://doi.org/10.1177/2515245920953350",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Ljiljana B. Lazarević; Danka Purić; Iris Žeželj; Radomir Belopavlović; Bojana Bodroža; Marija V. Čolić; Charles R. Ebersole; Máire B. Ford; Ana Orlić; Ivana Pedović; Boban Petrović; Ani N. Shabazian; Darko Stojilović",
    "corresponding_authors": "Ljiljana B. Lazarević",
    "abstract": "Across three studies, LoBue and DeLoache (2008) provided evidence suggesting that both young children and adults exhibit enhanced visual detection of evolutionarily relevant threat stimuli (as compared with nonthreatening stimuli). A replication of their Experiment 3, conducted by Cramblet Alvarez and Pipitone (2015) as part of the Reproducibility Project: Psychology (RP:P), demonstrated trends similar to those of the original study, but the effect sizes were smaller and not statistically significant. There were, however, some methodological differences (e.g., screen size) and sampling differences (the age of recruited children) between the original study and the RP:P replication study. Additionally, LoBue and DeLoache expressed concern over the choice of stimuli used in the RP:P replication. We sought to explore the possible moderating effects of these factors by conducting two new replications—one using the protocol from the RP:P and the other using a revised protocol. We collected data at four sites, three in Serbia and one in the United States (total N = 553). Overall, participants were not significantly faster at detecting threatening stimuli. Thus, results were not supportive of the hypothesis that visual detection of evolutionarily relevant threat stimuli is enhanced in young children. The effect from the RP:P protocol ( d = −0.10, 95% confidence interval = [−1.02, 0.82]) was similar to the effect from the revised protocol ( d = −0.09, 95% confidence interval = [−0.33, 0.15]), and the results from both the RP:P and the revised protocols were more similar to those found by Cramblet Alvarez and Pipitone than to those found by LoBue and DeLoache.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3100119188",
    "type": "article"
  },
  {
    "title": "The Unbearable Lightness of Attentional Cuing by Symbolic Magnitude: Commentary on the Registered Replication Report by Colling et al.",
    "doi": "https://doi.org/10.1177/2515245920902743",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Martin H. Fischer; Michael D. Dodd; Alan D. Castel; Jay Pratt",
    "corresponding_authors": "Martin H. Fischer",
    "abstract": "",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3034580050",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Registered Replication of Payne, Burkley, and Stokes (2008), Study 4",
    "doi": "https://doi.org/10.1177/2515245919885609",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Charles R. Ebersole; Luca Andrighetto; Erica Casini; Carlo Chiorri; Anna Dalla Rosa; Filippo Domaneschi; Ian Ferguson; Emily Fryberger; Mauro Giacomantonio; Jon Grahe; Jennifer A. Joy-Gaba; Eleanor V. Langford; Austin Nichols; Angelo Panno; Kimberly P. Parks; Emanuele Preti; Juliette Richetin; Michelangelo Vianello",
    "corresponding_authors": "Charles R. Ebersole",
    "abstract": "To rule out an alternative to their structural-fit hypothesis, Payne, Burkley, and Stokes (2008) demonstrated that correlations between implicit and explicit race attitudes were weaker when participants were put under high pressure to respond without bias than when they were placed under low pressure. This effect was replicated in Italy by Vianello (2015), although the replication effect was smaller than the original effect. In the current investigation, we examined the possibility that the source of a study’s sample moderates this effect. Teams from eight universities, four in the United States and four in Italy, replicated the original study (replication N = 1,103). Although we did detect moderation by the sample’s country, it was due to a reversal of the original effect in the United States and a lack of the original effect in Italy. We discuss this curious finding and possible explanations.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3103784926",
    "type": "article"
  },
  {
    "title": "Evaluating the Pedagogical Effectiveness of Study Preregistration in the Undergraduate Dissertation",
    "doi": "https://doi.org/10.1177/25152459231202724",
    "publication_date": "2023-10-01",
    "publication_year": 2023,
    "authors": "Madeleine Pownall; Charlotte R. Pennington; Emma Norris; Marie Juanchich; David Smailes; Pascale Sophie Russell; Debbie Gooch; Thomas Rhys Evans; Sofia Persson; Matthew HC Mak; Loukia Tzavella; Rebecca L. Monk; Thomas Gough; Christopher S. Y. Benwell; Mahmoud Medhat Elsherif; Emily K. Farran; Thomas Gallagher‐Mitchell; Luke T. Kendrick; Julia Bahnmueller; Emily Nordmann; Mirela Zaneva; Katie Anne Gilligan-Lee; Marina Bazhydai; Andrew Jones; Jemma Sedgmond; Iris J. Holzleitner; James P. Reynolds; Joanna Moss; Daniel Farrelly; Adam J Parker; Kait Clark",
    "corresponding_authors": "Madeleine Pownall",
    "abstract": "Research shows that questionable research practices (QRPs) are present in undergraduate final-year dissertation projects. One entry-level Open Science practice proposed to mitigate QRPs is “study preregistration,” through which researchers outline their research questions, design, method, and analysis plans before data collection and/or analysis. In this study, we aimed to empirically test the effectiveness of preregistration as a pedagogic tool in undergraduate dissertations using a quasi-experimental design. A total of 89 UK psychology students were recruited, including students who preregistered their empirical quantitative dissertation ( n = 52; experimental group) and students who did not ( n = 37; control group). Attitudes toward statistics, acceptance of QRPs, and perceived understanding of Open Science were measured both before and after dissertation completion. Exploratory measures included capability, opportunity, and motivation to engage with preregistration, measured at Time 1 only. This study was conducted as a Registered Report; Stage 1 protocol: https://osf.io/9hjbw (date of in-principle acceptance: September 21, 2021). Study preregistration did not significantly affect attitudes toward statistics or acceptance of QRPs. However, students who preregistered reported greater perceived understanding of Open Science concepts from Time 1 to Time 2 compared with students who did not preregister. Exploratory analyses indicated that students who preregistered reported significantly greater capability, opportunity, and motivation to preregister. Qualitative responses revealed that preregistration was perceived to improve clarity and organization of the dissertation, prevent QRPs, and promote rigor. Disadvantages and barriers included time, perceived rigidity, and need for training. These results contribute to discussions surrounding embedding Open Science principles into research training.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390071350",
    "type": "article"
  },
  {
    "title": "Beyond Statistics: Accepting the Null Hypothesis in Mature Sciences",
    "doi": "https://doi.org/10.1177/2515245918776023",
    "publication_date": "2018-06-01",
    "publication_year": 2018,
    "authors": "Richard D. Morey; Saskia Homer; Travis Proulx",
    "corresponding_authors": "Richard D. Morey",
    "abstract": "Scientific theories explain phenomena using simplifying assumptions—for instance, that the speed of light does not depend on the direction in which the light is moving, or that the shape of a pea plant’s seeds depends on a small number of alleles randomly obtained from its parents. These simplifying assumptions often take the form of statistical null hypotheses; hence, supporting these simplifying assumptions with statistical evidence is crucial to scientific progress, though it might involve “accepting” a null hypothesis. We review two historical examples in which statistical evidence was used to accept a simplifying assumption (that there is no luminiferous ether and that genetic traits are passed on in discrete forms) and one in which the null hypothesis was not accepted despite repeated failures (gravitational waves), drawing lessons from each. We emphasize the role of the scientific context in acceptance of the null: Accepting a null hypothesis is never a purely statistical affair.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2800633249",
    "type": "article"
  },
  {
    "title": "Some Reflections on the Many Labs 2 Replication of Norenzayan, Smith, Kim, and Nisbett’s (2002) Study 2: Cultural Preferences for Formal Versus Intuitive Reasoning",
    "doi": "https://doi.org/10.1177/2515245918817284",
    "publication_date": "2018-12-01",
    "publication_year": 2018,
    "authors": "Ara Norenzayan",
    "corresponding_authors": "Ara Norenzayan",
    "abstract": "",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2906019683",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Registered Replication of Albarracín et al. (2008), Experiment 5",
    "doi": "https://doi.org/10.1177/2515245920945963",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Christopher R. Chartier; Jack Arnal; Holly Arrow; Nicholas Bloxsom; Diane B. V. Bonfiglio; Claudia Chloe Brumbaugh; Katherine S. Corker; Charles R. Ebersole; Alexander Garinther; Steffen R. Giessner; Sean Hughes; Michael Inzlicht; Hause Lin; Brett Mercier; Mitchell M. Metzger; Derek Jay Rangel; Blair Saunders; Kathleen Schmidt; Daniel Storage; Carly Tocco",
    "corresponding_authors": "Christopher R. Chartier",
    "abstract": "In Experiment 5 of Albarracín et al. (2008), participants primed with words associated with action performed better on a subsequent cognitive task than did participants primed with words associated with inaction. A direct replication attempt by Frank, Kim, and Lee (2016) as part of the Reproducibility Project: Psychology (RP:P) failed to find evidence for this effect. In this article, we discuss several potential explanations for these discrepant findings: the source of participants (Amazon’s Mechanical Turk vs. traditional undergraduate-student pool), the setting of participation (online vs. in lab), and the possible moderating role of affect. We tested Albarracín et al.’s original hypothesis in two new samples: For the first sample, we followed the protocol developed by Frank et al. and recruited participants via Amazon’s Mechanical Turk ( n = 580). For the second sample, we used a revised protocol incorporating feedback from the original authors and recruited participants from eight universities ( n = 884). We did not detect moderation by protocol; patterns in the revised protocol resembled those in our implementation of the RP:P protocol, but the estimate of the focal effect size was smaller than that found originally by Albarracín et al. and larger than that found in Frank et al.’s replication attempt. We discuss these findings and possible explanations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3098973543",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Registered Replication of Shnabel and Nadler (2008), Study 4",
    "doi": "https://doi.org/10.1177/2515245920917334",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Erica Baranski; Ernest Baskin; Sean P. Coary; Charles R. Ebersole; Lacy E. Krueger; Ljiljana B. Lazarević; Jeremy K. Miller; Ana Orlić; Matthew R. Penner; Danka Purić; Sean C. Rife; Leigh Ann Vaughn; Aaron L. Wichman; Iris Žeželj",
    "corresponding_authors": "Erica Baranski",
    "abstract": "Shnabel and Nadler (2008) assessed a needs-based model of reconciliation suggesting that in conflicts, victims and perpetrators have different psychological needs that when satisfied increase the chances of reconciliation. For instance, Shnabel and Nadler found that after a conflict, perpetrators indicated that they had a need for social acceptance and were more likely to reconcile after their sense of social acceptance was restored, whereas victims indicated that they had a need for power and were more likely to reconcile after their sense of power was restored. Gilbert (2016), as a part of the Reproducibility Project: Psychology (RP:P), attempted to replicate these findings using different study materials but did not find support for the original effect. In an attempt to reconcile these discrepant findings, we conducted two new sets of replications—one using the RP:P protocol and another using modified materials meant to be more relatable to undergraduate participants. Teams from eight universities contributed to data collection ( N = 2,738). We did find moderation by protocol; the focal interaction from the revised protocol, but not from the RP:P protocol, replicated the interaction in the original study. We discuss differences in, and possible explanations for, the patterns of results across protocols.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3099850254",
    "type": "article"
  },
  {
    "title": "Performing Small-Telescopes Analysis by Resampling: Empirically Constructing Confidence Intervals and Estimating Statistical Power for Measures of Effect Size",
    "doi": "https://doi.org/10.1177/25152459241227865",
    "publication_date": "2024-01-01",
    "publication_year": 2024,
    "authors": "Samantha Costigan; John Ruscio; Jarret T. Crawford",
    "corresponding_authors": "John Ruscio",
    "abstract": "When new data are collected to check the findings of an original study, it can be challenging to evaluate replication results. The small-telescopes method is designed to assess not only whether the effect observed in the replication study is statistically significant but also whether this effect is large enough to have been detected in the original study. Unless both criteria are met, the replication either fails to support the original findings or the results are mixed. When implemented in the conventional manner, this small-telescopes method can be impractical or impossible to conduct, and doing so often requires parametric assumptions that may not be satisfied. We present an empirical approach that can be used for a variety of study designs and data-analytic techniques. The empirical approach to the small-telescopes method is intended to extend its reach as a tool for addressing the replication crisis by evaluating findings in psychological science and beyond. In the present tutorial, we demonstrate this approach using a Shiny app and R code and included an analysis of most studies (95%) replicated as part of the Open Science Collaboration’s Reproducibility Project in Psychology. In addition to its versatility, simulations demonstrate the accuracy and precision of the empirical approach to implementing small-telescopes analysis.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392720653",
    "type": "article"
  },
  {
    "title": "Robust Evidence for Knowledge Attribution and Luck: A Comment on Hall et al. (2024)",
    "doi": "https://doi.org/10.1177/25152459241268220",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Wesley Buckwalter; Ori Friedman",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403819046",
    "type": "article"
  },
  {
    "title": "Noise Versus Signal: What Can One Conclude When a Classic Finding Fails to Replicate?",
    "doi": "https://doi.org/10.1177/25152459241268293",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Wilson Cyrus-Lai; Warren Tierney; Eric Luis Uhlmann",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404313351",
    "type": "article"
  },
  {
    "title": "A Tutorial on Tailored Simulation-Based Sample-Size Planning for Experimental Designs With Generalized Linear Mixed Models",
    "doi": "https://doi.org/10.1177/25152459241287132",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Florian Pargent; T. Koch; Anne‐Kathrin Kleine; Eva Lermer; Susanne Gaube",
    "corresponding_authors": "",
    "abstract": "When planning experimental research, determining an appropriate sample size and using suitable statistical models are crucial for robust and informative results. The recent replication crisis underlines the need for more rigorous statistical methodology and adequately powered designs. Generalized linear mixed models (GLMMs) offer a flexible statistical framework to analyze experimental data with complex (e.g., dependent and hierarchical) data structures. However, available methods and software for a priori sample-size planning for GLMMs are often limited to specific designs. Tailored data-simulation approaches offer a more flexible alternative. Based on a practical case study in which we focus on a binomial GLMM with two random intercepts and discrete predictor variables, the current tutorial equips researchers with a step-by-step guide and corresponding code for conducting tailored a priori sample-size planning with GLMMs. We not only focus on power analysis but also explain how to use the precision of parameter estimates to determine appropriate sample sizes. We conclude with an outlook on the increasing importance of simulation-based sample-size planning.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405362605",
    "type": "article"
  },
  {
    "title": "Prevalence of Transparent Research Practices in Psychology: A Cross-Sectional Study of Empirical Articles Published in 2022",
    "doi": "https://doi.org/10.1177/25152459241283477",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Tom E Hardwicke; Robert T. Thibault; Beth Clarke; Nicholas Moodie; Sophia Crüwell; Sarah R. Schiavone; Sarah A. Handcock; Khanh An Nghiem; Fallon Mody; Tuomas Eerola; Simine Vazire",
    "corresponding_authors": "",
    "abstract": "More than a decade of advocacy and policy reforms have attempted to increase the uptake of transparent research practices in the field of psychology; however, their collective impact is unclear. We estimated the prevalence of transparent research practices in (a) all psychology journals (i.e., field-wide), and (b) prominent psychology journals, by manually examining two random samples of 200 empirical articles ( N = 400) published in 2022. Most articles had an open-access version (field-wide: 74%, 95% confidence interval [CI] = [67%, 79%]; prominent: 71% [64%, 77%]) and included a funding statement (field-wide: 76% [70%, 82%]; prominent: 76% [70%, 82%]) or conflict-of-interest statement (field-wide: 76% [70%, 82%]; prominent: 73% [67%, 79%]). Relatively few articles had a preregistration (field-wide: 7% [2.5%, 12%]; prominent: 14% [8.5%, 19%]), materials (field-wide: 16% [9%, 24%]; prominent: 19% [12%, 27%]), raw/primary data (field-wide: 14% [7%, 21%]; prominent: 16% [9.5%, 24%]), or analysis scripts (field-wide: 8.5% [4.5%, 13%]; prominent: 14% [9.5%, 19%]) that were immediately accessible without contacting authors or third parties. In conjunction with prior research, our results suggest transparency increased moderately from 2017 to 2022. Overall, despite considerable infrastructure improvements, bottom-up advocacy, and top-down policy initiatives, research transparency continues to be widely neglected in psychology.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405734082",
    "type": "article"
  },
  {
    "title": "Dynamic Data Visualizations to Enhance Insight and Communication Across the Life Cycle of a Scientific Project",
    "doi": "https://doi.org/10.1177/25152459231160103",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Kristina Wiebels; David Moreau",
    "corresponding_authors": "David Moreau",
    "abstract": "In scientific communication, figures are typically rendered as static displays. This often prevents active exploration of the underlying data, for example, to gauge the influence of particular data points or of particular analytic choices. Yet modern data-visualization tools, from animated plots to interactive notebooks and reactive web applications, allow psychologists to share and present their findings in dynamic and transparent ways. In this tutorial, we present a number of recent developments to build interactivity and animations into scientific communication and publications using examples and illustrations in the R language (basic knowledge of R is assumed). In particular, we discuss when and how to build dynamic figures, with step-by-step reproducible code that can easily be extended to the reader’s own projects. We illustrate how interactivity and animations can facilitate insight and communication across a project life cycle—from initial exchanges and discussions in a team to peer review and final publication—and provide a number of recommendations to use dynamic visualizations effectively. We close with a reflection on how the scientific-publishing model is currently evolving and consider the challenges and opportunities this shift might bring for data visualization.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4384023911",
    "type": "article"
  },
  {
    "title": "Modeling Cluster-Level Constructs Measured by Individual Responses: Configuring a Shared Approach",
    "doi": "https://doi.org/10.1177/25152459231182319",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Suzanne Jak; Terrence D. Jorgensen; Debby ten Hove; Barbara Nevicka",
    "corresponding_authors": "Suzanne Jak",
    "abstract": "When multiple items are used to measure cluster-level constructs with individual-level responses, multilevel confirmatory factor models are useful. How to model constructs across levels is still an active area of research in which competing methods are available to capture what can be interpreted as a valid representation of cluster-level phenomena. Moreover, the terminology used for the cluster-level constructs in such models varies across researchers. We therefore provide an overview of used terminology and modeling approaches for cluster-level constructs measured through individual responses. We classify the constructs based on whether (a) the target of measurement is at the cluster level or at the individual level and (b) the construct requires a measurement model. Next, we discuss various two-level factor models that have been proposed for multilevel constructs that require a measurement model, and we show that the so-called doubly latent model with cross-level invariance of factor loadings is appropriate for all types of constructs that require a measurement model. We provide two illustrations using empirical data from students and organizational teams on stimulating teaching and on conflict in organizational teams, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386123387",
    "type": "article"
  },
  {
    "title": "How Do Science Journalists Evaluate Psychology Research?",
    "doi": "https://doi.org/10.1177/25152459231183912",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Julia G. Bottesini; Christie Aschwanden; Mijke Rhemtulla; Simine Vazire",
    "corresponding_authors": "Julia G. Bottesini",
    "abstract": "What information do science journalists use when evaluating psychology findings? We examined this in a preregistered, controlled experiment by manipulating four factors in descriptions of fictitious behavioral-psychology studies: (a) the study’s sample size, (b) the representativeness of the study’s sample, (c) the p value associated with the finding, and (d) institutional prestige of the researcher who conducted the study. We investigated the effects of these manipulations on 181 real journalists’ perceptions of each study’s trustworthiness and newsworthiness. Sample size was the only factor that had a robust influence on journalists’ ratings of how trustworthy and newsworthy a finding was; larger sample sizes led to an increase of about two-thirds of 1 point on a 7-point scale. University prestige had no effect in this controlled setting, and the effects of sample representativeness and of p values were inconclusive, but any effects in this setting are likely quite small. Exploratory analyses suggest that other types of prestige might be more important (i.e., journal prestige) and that study design (experimental vs. correlational) may also affect trustworthiness and newsworthiness.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386495044",
    "type": "article"
  },
  {
    "title": "Impossible Hypotheses and Effect-Size Limits",
    "doi": "https://doi.org/10.1177/25152459231197605",
    "publication_date": "2023-10-01",
    "publication_year": 2023,
    "authors": "Wijnand A. P. van Tilburg; Lennert J. A. van Tilburg",
    "corresponding_authors": "Wijnand A. P. van Tilburg",
    "abstract": "Psychological science is moving toward further specification of effect sizes when formulating hypotheses, performing power analyses, and considering the relevance of findings. This development has sparked an appreciation for the wider context in which such effect sizes are found because the importance assigned to specific sizes may vary from situation to situation. We add to this development a crucial but in psychology hitherto underappreciated contingency: There are mathematical limits to the magnitudes that population effect sizes can take within the common multivariate context in which psychology is situated, and these limits can be far more restrictive than typically assumed. The implication is that some hypothesized or preregistered effect sizes may be impossible. At the same time, these restrictions offer a way of statistically triangulating the plausible range of unknown effect sizes. We explain the reason for the existence of these limits, illustrate how to identify them, and offer recommendations and tools for improving hypothesized effect sizes by exploiting the broader multivariate context in which they occur.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388880496",
    "type": "article"
  },
  {
    "title": "Structure and Goal Pursuit: Individual and Cultural Differences",
    "doi": "https://doi.org/10.1177/2515245918797130",
    "publication_date": "2018-12-01",
    "publication_year": 2018,
    "authors": "Kristin Laurin; Aaron C. Kay; Mark J. Landau",
    "corresponding_authors": "Kristin Laurin",
    "abstract": "",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2906653757",
    "type": "article"
  },
  {
    "title": "Linear Discriminant Analysis for Prediction of Group Membership: A User-Friendly Primer:",
    "doi": "https://doi.org/10.25384/sage.c.4571060.v2",
    "publication_date": "2019-07-09",
    "publication_year": 2019,
    "authors": "Peter Boedeker; Nathan T. Kearns",
    "corresponding_authors": "",
    "abstract": "In psychology, researchers are often interested in the predictive classification of individuals. Various models exist for such a purpose, but which model is considered a best practice is conditiona...",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2961939378",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Registered Replication of Förster, Liberman, and Kuschel’s (2008) Study 1",
    "doi": "https://doi.org/10.1177/2515245920916513",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Hans IJzerman; Ivan Ropovik; Charles R. Ebersole; Natasha Tidwell; Łukasz Markiewicz; Tiago Jessé Souza de Lima; Daniel Wolf; Sarah A. Novak; W. Matthew Collins; Madhavi Menon; Luana Elayne Cunha de Souza; Przemysław Sawicki; Leanne Boucher; Michał Białek; Katarzyna Idzikowska; Timothy S. Razza; Sue Kraus; Sophia Christin Weißgerber; Gabriel Baník; Sabina Kołodziej; Peter Babinčák; Astrid Schütz; Rolf Sternglanz; Katarzyna Gawryluk; Gavin Brent Sullivan; Chris Day",
    "corresponding_authors": "Hans IJzerman",
    "abstract": "In a test of their global-/local-processing-style model, Förster, Liberman, and Kuschel (2008) found that people assimilate a primed concept (e.g., “aggressive”) into their social judgments after a global prime (e.g., they rate a person as being more aggressive than do people in a no-prime condition) but contrast their judgment away from the primed concept after a local prime (e.g., they rate the person as being less aggressive than do people in a no prime-condition). This effect was not replicated by Reinhard (2015) in the Reproducibility Project: Psychology. However, the authors of the original study noted that the replication could not provide a test of the moderation effect because priming did not occur. They suggested that the primes might have been insufficiently applicable and the scenarios insufficiently ambiguous to produce priming. In the current replication project, we used both Reinhard’s protocol and a revised protocol that was designed to increase the likelihood of priming, to test the original authors’ suggested explanation for why Reinhard did not observe the moderation effect. Teams from nine universities contributed to this project. We first conducted a pilot study ( N = 530) and successfully selected ambiguous scenarios for each site. We then pilot-tested the aggression prime at five different sites ( N = 363) and found that it did not successfully produce priming. In agreement with the first author of the original report, we replaced the prime with a task that successfully primed aggression (hostility) in a pilot study by McCarthy et al. (2018). In the final replication study ( N = 1,460), we did not find moderation by protocol type, and judgment patterns in both protocols were inconsistent with the effects observed in the original study. We discuss these findings and possible explanations.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3100424266",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Registered Multisite Replication of the Tempting-Fate Effects in Risen and Gilovich (2008)",
    "doi": "https://doi.org/10.1177/2515245918785165",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Maya B. Mathur; Diane-Jo Bart-Plange; Balázs Aczél; Michael H. Bernstein; Antonia M. Ciunci; Charles R. Ebersole; Filipe Falcão; Kayla Ashbaugh; Rias A. Hilliard; Alan Jern; Danielle Kellier; Grecia Kessinger; Vanessa S. Kolb; Márton Kovács; Caio Ambrosio Lage; Eleanor V. Langford; Samuel Lins; Dylan Manfredi; Venus Meyet; Don A. Moore; Gideon Nave; Christian Nunnally; Anna Pálinkás; Kimberly P. Parks; Sebastiaan Pessers; Tiago Ramos; Kaylis Hase Rudy; Janos Salamon; Rachel L. Shubella; Rúben Silva; Sara Steegen; L. A. R. Stein; Barnabás Szászi; Péter Szécsi; Francis Tuerlinckx; Wolf Vanpaemel; Maria Vlachou; Bradford J. Wiggins; David Zealley; Márk Zrubka; Michael C. Frank",
    "corresponding_authors": "Maya B. Mathur",
    "abstract": "Risen and Gilovich (2008) found that subjects believed that “tempting fate” would be punished with ironic bad outcomes (a main effect), and that this effect was magnified when subjects were under cognitive load (an interaction). A previous replication study (Frank &amp; Mathur, 2016) that used an online implementation of the protocol on Amazon Mechanical Turk failed to replicate both the main effect and the interaction. Before this replication was run, the authors of the original study expressed concern that the cognitive-load manipulation may be less effective when implemented online than when implemented in the lab and that subjects recruited online may also respond differently to the specific experimental scenario chosen for the replication. A later, large replication project, Many Labs 2 (Klein et al. 2018), replicated the main effect (though the effect size was smaller than in the original study), but the interaction was not assessed. Attempting to replicate the interaction while addressing the original authors’ concerns regarding the protocol for the first replication study, we developed a new protocol in collaboration with the original authors. We used four university sites ( N = 754) chosen for similarity to the site of the original study to conduct a high-powered, preregistered replication focused primarily on the interaction effect. Results from these sites did not support the interaction or the main effect and were comparable to results obtained at six additional universities that were less similar to the original site. Post hoc analyses did not provide strong evidence for statistical inconsistency between the original study’s estimates and our estimates; that is, the original study’s results would not have been extremely unlikely in the estimated distribution of population effects in our sites. We also collected data from a new Mechanical Turk sample under the first replication study’s protocol, and results were not meaningfully different from those obtained with the new protocol at universities similar to the original site. Secondary analyses failed to support proposed substantive mechanisms for the failure to replicate.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3102574417",
    "type": "article"
  },
  {
    "title": "A Guide to Visualizing Trajectories of Change With Confidence Bands and Raw Data",
    "doi": "https://doi.org/10.1177/25152459211047228",
    "publication_date": "2021-10-01",
    "publication_year": 2021,
    "authors": "Howard E. Barbaree",
    "corresponding_authors": "Howard E. Barbaree",
    "abstract": "This tutorial is aimed at researchers working with repeated measures or longitudinal data who are interested in enhancing their visualizations of model-implied mean-level trajectories plotted over time with confidence bands and raw data. The intended audience is researchers who are already modeling their experimental, observational, or other repeated measures data over time using random-effects regression or latent curve modeling but who lack a comprehensive guide to visualize trajectories over time. This tutorial uses an example plotting trajectories from two groups, as seen in random-effects models that include Time × Group interactions and latent curve models that regress the latent time slope factor onto a grouping variable. This tutorial is also geared toward researchers who are satisfied with their current software environment for modeling repeated measures data but who want to make graphics using R software. Prior knowledge of R is not assumed, and readers can follow along using data and other supporting materials available via OSF at https://osf.io/78bk5/ . Readers should come away from this tutorial with the tools needed to begin visualizing mean trajectories over time from their own models and enhancing those plots with graphical estimates of uncertainty and raw data that adhere to transparent practices in research reporting.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3167015291",
    "type": "article"
  },
  {
    "title": "PsyBuilder: An Open-Source, Cross-Platform Graphical Experiment Builder for Psychtoolbox With Built-In Performance Optimization",
    "doi": "https://doi.org/10.1177/25152459211070573",
    "publication_date": "2022-01-01",
    "publication_year": 2022,
    "authors": "Zhicheng Lin; Zhe Yang; Chengzhi Feng; Yang Zhang",
    "corresponding_authors": "Yang Zhang",
    "abstract": "Psychtoolbox is among the most popular open-source software packages for stimulus presentation and response collection. It provides flexibility and power in the choice of stimuli and responses, in addition to precision in control and timing. However, Psychtoolbox requires coding in MATLAB (or its equivalent, e.g., Octave). Scripting is challenging to learn and can lead to timing inaccuracies unwittingly. It can also be time-consuming and error prone even for experienced users. We have developed the first general-purpose graphical experiment builder for Psychtoolbox, called PsyBuilder, for both new and experienced users. The builder allows users to graphically implement sophisticated experimental tasks through intuitive drag and drop without the need to script. The output codes have built-in optimized timing precision and come with detailed comments to facilitate customization. Because users can see exactly how the code changes in response to modifications in the graphical interface, PsyBuilder can also bolster the understanding of programming in ways that were not previously possible. In this tutorial, we first describe its interface, then walk the reader through the graphical building process using a concrete experiment, and finally address important issues from the perspective of potential adopters.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4226184467",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Registered Replication of Crosby, Monin, and Richardson (2008)",
    "doi": "https://doi.org/10.1177/2515245919870737",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Hugh Rabagliati; Martin Corley; Benjamin Dering; Peter Hancock; Josiah King; Carmel Levitan; Jia E. Loy; Ailsa E. Millen",
    "corresponding_authors": "Hugh Rabagliati",
    "abstract": "Crosby, Monin, and Richardson (2008) found that hearing an offensive remark caused subjects ( N = 25) to look longer at a potentially offended person, but only if that person could hear the remark. On the basis of this result, they argued that people use social referencing to assess the offensiveness. However, in a direct replication in the Reproducibility Project: Psychology, the result for Crosby et al.’s key effect was not significant. In the current project, we tested whether the size of the social-referencing effect might be increased by a peer-reviewed and preregistered protocol manipulation in which some participants were given context to understand why the remark was potentially offensive. Three labs in Europe and the United States ( N = 283) took part. The protocol manipulation did not affect the size of the social-referencing effect. However, we did replicate the original effect reported by Crosby et al., albeit with a much smaller effect size. We discuss these results in the context of ongoing debates about how replication attempts should treat statistical power and contextual sensitivity.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2989584893",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Replication of van Dijk, van Kleef, Steinel, and van Beest (2008)",
    "doi": "https://doi.org/10.1177/2515245920927643",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Lauren Skorb; Balázs Aczél; Bence E. Bakos; Lily Feinberg; Ewa Hałasa; Mathias Kauff; Márton Kovács; Karolina Krasuska; Katarzyna Kuchno; Dylan Manfredi; Andres Montealegre; Emilian Pękala; Damian Pieńkosz; Jonathan Ravid; Katrin Rentzsch; Barnabás Szászi; Stefan Schulz‐Hardt; Barbara Sioma; Péter Szécsi; Attila Szuts; Orsolya Szöke; Oliver Christ; Anna Fedor; William Jiménez‐Leal; Rafał Muda; Gideon Nave; Janos Salamon; Thomas Schultze; Joshua K. Hartshorne",
    "corresponding_authors": "Joshua K. Hartshorne",
    "abstract": "As part of the Many Labs 5 project, we ran a replication of van Dijk, van Kleef, Steinel, and van Beest’s (2008) study examining the effect of emotions in negotiations. They reported that when the consequences of rejection were low, subjects offered fewer chips to angry bargaining partners than to happy partners. We ran this replication under three protocols: the protocol used in the Reproducibility Project: Psychology, a revised protocol, and an online protocol. The effect averaged one ninth the size of the originally reported effect and was significant only for the revised protocol. However, the difference between the original and revised protocols was not significant.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3098606098",
    "type": "article"
  },
  {
    "title": "Many Labs 5: Registered Replication of Albarracín et al. (2008), Experiment 7",
    "doi": "https://doi.org/10.1177/2515245920925750",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Katherine S. Corker; Jack Arnal; Diane B. V. Bonfiglio; Paul Curran; Christopher R. Chartier; William J. Chopik; Rosanna E. Guadagno; Amanda M. Kimbrough; Kathleen Schmidt; Bradford J. Wiggins",
    "corresponding_authors": "Katherine S. Corker",
    "abstract": "Albarracín et al. (2008, Experiment 7) tested whether priming action or inaction goals (vs. no goal) and then satisfying those goals (vs. not satisfying them) would be associated with subsequent cognitive responding. They hypothesized and found that priming action or inaction goals that were not satisfied resulted in greater or lesser responding, respectively, compared with not priming goals ( N = 98). Sonnleitner and Voracek (2015) attempted to directly replicate Albarracín et al.’s (2008) study with German participants ( N = 105). They did not find evidence for the 3 × 2 interaction or the expected main effect of task type. The current study attempted to directly replicate Albarracín et al. (2008), Experiment 7, with a larger sample of participants ( N = 1,690) from seven colleges and universities in the United States. We also extended the study design by using a scrambled-sentence task to prime goals instead of the original task of completing word fragments, allowing us to test whether study protocol moderated any effects of interest. We did not detect moderation by protocol in the full 3 × 2 × 2 design (pseudo- r 2 = 0.05%). Results for both protocols were largely consistent with Sonnleitner and Voracek’s findings (pseudo- r 2 s = 0.14% and 0.50%). We consider these results in light of recent findings concerning priming methods and discuss the robustness of action-/inaction-goal priming to the implementation of different protocols in this particular context.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3102638009",
    "type": "article"
  },
  {
    "title": "Cultural Differences in Correspondence Bias Are Systematic and Multifaceted",
    "doi": "https://doi.org/10.1177/2515245918817076",
    "publication_date": "2018-12-01",
    "publication_year": 2018,
    "authors": "Yuri Miyamoto; Shinobu Kitayama",
    "corresponding_authors": "Yuri Miyamoto",
    "abstract": "",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4231080762",
    "type": "article"
  },
  {
    "title": "Assessing Ego-Centered Social Networks in formr: A Tutorial",
    "doi": "https://doi.org/10.1177/2515245920985467",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "Louisa M. Reins; Ruben C. Arslan; Tanja M. Gerlach",
    "corresponding_authors": "Louisa M. Reins",
    "abstract": "In psychological science, ego-centered social networks are assessed to investigate the patterning and development of social relationships. In this approach, a focal individual is typically asked to report the people they interact with in specific contexts and to provide additional information on those interaction partners and the relationships with them. Although ego-centered social networks hold considerable promise for investigating various interesting questions from psychology and beyond, their implementation can be challenging. This tutorial provides researchers with detailed instructions on how to set up a study involving ego-centered social networks online using the open-source software formr. By including a fully functional study template for the assessment of social networks and extensions to this design, we hope to equip researchers from different backgrounds with the tools necessary to collect social-network data tailored to their research needs.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3012984200",
    "type": "article"
  },
  {
    "title": "<i>StatBreak</i>: Identifying “Lucky” Data Points Through Genetic Algorithms",
    "doi": "https://doi.org/10.1177/2515245920917950",
    "publication_date": "2020-05-21",
    "publication_year": 2020,
    "authors": "Hannes Rosenbusch; Leon P. Hilbert; Anthony M. Evans; Marcel Zeelenberg",
    "corresponding_authors": "Hannes Rosenbusch",
    "abstract": "Sometimes interesting statistical findings are produced by a small number of “lucky” data points within the tested sample. To address this issue, researchers and reviewers are encouraged to investigate outliers and influential data points. Here, we present StatBreak, an easy-to-apply method, based on a genetic algorithm, that identifies the observations that most strongly contributed to a finding (e.g., effect size, model fit, p value, Bayes factor). Within a given sample, StatBreak searches for the largest subsample in which a previously observed pattern is not present or is reduced below a specifiable threshold. Thus, it answers the following question: “Which (and how few) ‘lucky’ cases would need to be excluded from the sample for the data-based conclusion to change?” StatBreak consists of a simple R function and flags the luckiest data points for any form of statistical analysis. Here, we demonstrate the effectiveness of the method with simulated and real data across a range of study designs and analyses. Additionally, we describe StatBreak’s R function and explain how researchers and reviewers can apply the method to the data they are working with.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3010248266",
    "type": "article"
  },
  {
    "title": "The Appropriateness of Outlier Exclusion Approaches Depends on the Expected Contamination: Commentary on André (2022)",
    "doi": "https://doi.org/10.1177/25152459231186577",
    "publication_date": "2023-07-01",
    "publication_year": 2023,
    "authors": "Daniel Villanova",
    "corresponding_authors": "Daniel Villanova",
    "abstract": "In a recent article, André (2022) addressed the decision to exclude outliers using a threshold across conditions or within conditions and offered a clear recommendation to avoid within-conditions exclusions because of the possibility for large false-positive inflation. In this commentary, I note that André’s simulations did not include the situation for which within-conditions exclusion has previously been recommended—when across-conditions exclusion would exacerbate selection bias. Examining test performance in this situation confirms the recommendation for within-conditions exclusion in such a circumstance. Critically, the suitability of exclusion criteria must be considered in relationship to assumptions about data-generating mechanisms.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385516590",
    "type": "article"
  },
  {
    "title": "When Alternative Analyses of the Same Data Come to Different Conclusions: A Tutorial Using <i>DeclareDesign</i> With a Worked Real-World Example",
    "doi": "https://doi.org/10.1177/25152459241267904",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Dorothy Bishop; Charles Hulme",
    "corresponding_authors": "",
    "abstract": "Recent studies in psychology have documented how analytic flexibility can result in different results from the same data set. Here, we demonstrate a package in the R programming language, DeclareDesign, that uses simulated data to diagnose the ways in which different analytic designs can give different outcomes. To illustrate features of the package, we contrast two analyses of a randomized controlled trial (RCT) of GraphoGame, an intervention to help children learn to read. The initial analysis found no evidence that the intervention was effective, but a subsequent reanalysis concluded that GraphoGame significantly improved children’s reading. With DeclareDesign, we can simulate data in which the truth is known and thus can identify which analysis is optimal for estimating the intervention effect using “diagnosands,” including bias, precision, and power. The simulations showed that the original analysis accurately estimated intervention effects, whereas selection of a subset of data in the reanalysis introduced substantial bias, overestimating the effect sizes. This problem was exacerbated by inclusion of multiple outcome measures in the reanalysis. Much has been written about the dangers of performing reanalyses of data from RCTs that violate the random assignment of participants to conditions; simulated data make this message clear and quantify the extent to which such practices introduce bias. The simulations confirm the original conclusion that the intervention has no benefit over “business as usual.” In this tutorial, we demonstrate several features of DeclareDesign, which can simulate observational and experimental research designs, allowing researchers to make principled decisions about which analysis to prefer.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402270279",
    "type": "article"
  },
  {
    "title": "Assessing the Generality of a Self-Administered Strategic-Resource-Use Intervention on Academic Performance: A Multisite, Preregistered Conceptual Replication of Chen et al. (2017)",
    "doi": "https://doi.org/10.1177/25152459241270604",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Peter P. J. L. Verkoeijen; Gabriela V. Koppenol‐Gonzalez; Lara M. van Peppen; Marloes Broeren; Anita Heijltjes; Renske E. Kuijpers; Janneke T. L. M. Nobelen; Marion Tillema; Marleen H. M. de Moor; Lidia R. Arends",
    "corresponding_authors": "",
    "abstract": "Chen et al. designed a novel strategic-resource-use (SRU) intervention that higher-education students could self-administer online. This intervention aimed to help students improve their performance by stimulating them to think about using learning resources for an exam preparation. The SRU intervention was tested in two undergraduate introductory-statistics courses. In the first experiment, students in the control condition received an email asking them to state their desired grade, how motivated they were to get that grade, how important it was to obtain the desired grade, and how confident they were in obtaining it. Participants in the experimental condition received the same mail and took the 15-min SRU intervention. On the final course exam, the SRU group outperformed the control group, yielding a small to medium effect size, a finding that was replicated in a second study. We conducted four preregistered conceptual replications of Chen and colleagues’ study in four undergraduate introductory-statistics courses at two Dutch higher-education institutions. In our study, the meta-analytic standardized effects on the final-exam scores in the intention-to-treat meta-analysis and the compliant-only analysis were small and not significantly different from 0, and the upper limits of the 95% confidence intervals of both meta-analyses were smaller than the effect sizes of the two studies reported by Chen and colleagues. Comparable results were obtained for the pass rates. Thus, the results of the present study failed to corroborate the previously demonstrated positive effect of the SRU intervention on final-exam scores and pass rates.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402932599",
    "type": "article"
  },
  {
    "title": "When Replication Fails: What to Conclude and Not to Conclude?",
    "doi": "https://doi.org/10.1177/25152459241268197",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Willem W. A. Sleegers; Florian van Leeuwen; Robert M. Ross; Kenneth G. DeMarree; Ilja van Beest; Daniel Priolo; Marie‐Amélie Martinie; Coby Morvinski; Bruno Verschuère; David C. Vaidis",
    "corresponding_authors": "",
    "abstract": "In this commentary, we examine the implications of the failed replication reported by Vaidis et al., which represents the largest multilab attempt to replicate the induced-compliance paradigm in cognitive-dissonance theory. We respond to commentaries on this study and discuss potential explanations for the null findings, including issues with the perceived choice manipulation and various post hoc explanations. Our commentary includes an assessment of the broader landscape of cognitive-dissonance research, revealing pervasive methodological limitations, such as underpowered studies and a lack of open-science practices. We conclude that our replication study and our examination of the literature raise substantial concerns about the reliability of the induced-compliance paradigm and highlight the need for more rigorous research practices in the field of cognitive dissonance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404361822",
    "type": "article"
  },
  {
    "title": "Testing Bayesian Informative Hypotheses in Five Steps With JASP and R",
    "doi": "https://doi.org/10.1177/25152459241260259",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Sara Garofalo; Gianluca Finotti; Matteo Orsoni; Sara Giovagnoli; Mariagrazia Benassi",
    "corresponding_authors": "",
    "abstract": "One of the most common applications of Bayes’s theorem for inferential purposes consists of computing the ratio between the probability of the alternative and the null hypotheses via a Bayes factor, which allows quantifying the most likely explanation of the data between the two. However, the actual scientific questions that researchers are interested in are rarely well represented by the classically defined alternative and null hypotheses. Bayesian informative-hypothesis testing offers a valid and easy way to overcome such limitations via a model-selection procedure that allows comparing highly specific hypotheses formulated in terms of equality (A = B) or inequality (A &gt; B) constraints among parameters. Although packages for testing informative hypotheses in the most used statistical software have been developed in recent years, they are still rarely used, possibly because their implementation and interpretation may not be straightforward. Starting from a brief theoretical overview of the Bayesian theorem and its applications in statistical inference (i.e., Bayes factor and Bayesian informative hypotheses), in this article, we provide two step-by-step tutorials illustrating how to test, interpret, and report in a scientific article Bayesian informative hypotheses using JASP and R/RStudio software for running a 2 × 2 analysis of variance and a multiple linear regression. The complete JASP files, R code, and data sets used in the article are freely available on the OSF page at https://osf.io/dez9b/ .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405632175",
    "type": "article"
  },
  {
    "title": "A Response to a Comment on Hall et al. (2024)",
    "doi": "https://doi.org/10.1177/25152459241268249",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Kathleen Schmidt; Gerald J. Haeffel; Neil Levy; David Moreau; Sean T. H. Lee; Erin Michelle Buchanan; Anthony J. Krafnick; Martin Voracek; Gerit Pfuhl; Krystian Barzykowski; Marta Kowal; Jordan Wagge",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403820134",
    "type": "article"
  },
  {
    "title": "A Guide to Prototype Analyses in Cross-Cultural Research: Purpose, Advantages, and Risks",
    "doi": "https://doi.org/10.1177/25152459241296401",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Yuning Sun; Elaine L. Kinsella; Eric R. Igou",
    "corresponding_authors": "",
    "abstract": "The prototype approach provides a theoretically supported basis for novel research, detailing “typical” cognitive representations of targets in question (e.g., groups, experiences). Fairly recently, in social and cognitive psychology, this approach has emerged to understand how people categorize and conceptualize everyday phenomena. Although this approach has previously been used to study everyday concepts, it has predominantly been overlooked in cross-cultural research. Prototype analyses are flexible enough to allow for the identification of both universal and culture-specific elements, offering a more comprehensive and nuanced understanding of the concept in question. We highlight theoretical, empirical, and practical reasons why prototype analyses offer an important tool in cross-cultural and interdisciplinary research while also addressing the potential for reducing construct bias in research that spans multiple cultural contexts. The advantages and risks of conducting prototype analyses are discussed in detail along with novel ways of integrating computational approaches with traditional prototype-analyses methods to assist in their implementation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405047812",
    "type": "article"
  },
  {
    "title": "A Methodological Framework for Stimuli Control: Insights From Numerical Cognition",
    "doi": "https://doi.org/10.1177/25152459241249185",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Yoel Shilat; Avishai Henik; Hanit Galili; Shir Wasserman; Alon Salzmann; Moti Salti",
    "corresponding_authors": "",
    "abstract": "The stimuli presented in cognitive experiments have a crucial role in the ability to isolate the underlying mechanism from other interweaved mechanisms. New ideas aimed at unveiling cognitive mechanisms are often realized through introducing new stimuli. This, in turn, raises challenges in reconciling results to literature. We demonstrate this challenge in the field of numerical cognition. Stimuli used in this field are designed to present quantity in a non symbolic manner. Physical properties, such as surface area and density, inherently correlate with quantity, masking the mechanism underlying numerical perception. Different generation methods (GMs) are used to control these physical properties. However, the way a GM controls physical properties affects numerical judgments in different ways, compromising comparability and the pursuit of cumulative science. Here, using a novel data-driven approach, we provide a methodological review of non symbolic stimuli GMs developed since 2000. Our results reveal that the field thrives and that a wide variety of GMs are tackling new methodological and theoretical ideas. However, the field lacks a common language and means to integrate new ideas into the literature. These shortcomings impair the interpretability, comparison, replication, and reanalysis of previous studies that have considered new ideas. We present guidelines for GMs relevant also to other fields and tasks involving perceptual decisions, including (a) defining controls explicitly and consistently, (b) justifying controls and discussing their implications, (c) considering stimuli statistical features, and (d) providing complete stimuli set, matching responses, and generation code. We hope these guidelines will promote the integration of findings and increase findings’ explanatory power.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405047934",
    "type": "article"
  },
  {
    "title": "Corrigendum: Journal N-Pact Factors From 2011 to 2019: Evaluating the Quality of Social/Personality Journals With Respect to Sample Size and Statistical Power",
    "doi": "https://doi.org/10.1177/25152459231175075",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4377293121",
    "type": "erratum"
  },
  {
    "title": "Corrigendum: Impossible Hypotheses and Effect-Size Limits",
    "doi": "https://doi.org/10.1177/25152459231224489",
    "publication_date": "2023-10-01",
    "publication_year": 2023,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390071351",
    "type": "erratum"
  },
  {
    "title": "Cultural Differences in Correspondence Bias Are Systematic and Multifaceted",
    "doi": "https://doi.org/10.25384/sage.c.4347038.v1",
    "publication_date": "2018-12-24",
    "publication_year": 2018,
    "authors": "Yuri Miyamoto; Shinobu Kitayama",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2906187452",
    "type": "article"
  },
  {
    "title": "Multilevel Modeling and Meta-Analysis: Epilogue to the Invited Forums",
    "doi": "https://doi.org/10.1177/2515245920905783",
    "publication_date": "2020-03-01",
    "publication_year": 2020,
    "authors": "Frederick L. Oswald; Jennifer L. Tackett",
    "corresponding_authors": "Frederick L. Oswald",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3010574499",
    "type": "article"
  },
  {
    "title": "Corrigendum: Simulation Studies as a Tool to Understand Bayes Factors",
    "doi": "https://doi.org/10.1177/25152459211061266",
    "publication_date": "2021-10-01",
    "publication_year": 2021,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4213148003",
    "type": "erratum"
  },
  {
    "title": "Acknowledgment",
    "doi": "https://doi.org/10.1177/2515245921993161",
    "publication_date": "2021-01-01",
    "publication_year": 2021,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4214496028",
    "type": "article"
  }
]